{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# <div style=\"color:#fff;display:fill;border-radius:10px;background-color:#000000;text-align:left;letter-spacing:0.1px;overflow:hidden;padding:20px;color:white;overflow:hidden;margin:0;font-size:100%\"> - | Notebook resume</div>\n\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">   \nGoing through Kaggle I have come across amazing notebooks and they gave me ideas to make a trainer class that also works for different competitions, in fact I'm using a variation of this class in the Tabulars Playground series. Hope you like it. Greetings to all. If you want you can also check my other titanic kernel with other trainer class <a href=\"https://www.kaggle.com/code/maxdiazbattan/titanic-competition-class-v2-updated\">[link]</a>. Any suggestions are welcome. Happy kaggling! </p>\n","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"color:#fff;display:fill;border-radius:10px;background-color:#000000;text-align:left;letter-spacing:0.1px;overflow:hidden;padding:20px;color:white;overflow:hidden;margin:0;font-size:100%\"> - | Table of contents</div>\n\n* [1-Libraries](#section-one)\n* [2-Data loading](#section-two)\n* [3-Preprocessing and Feature engineering](#section-three)\n* [4-Training](#section-four)\n* [5-Blending](#section-five)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# <div style=\"color:#fff;display:fill;border-radius:10px;background-color:#000000;text-align:left;letter-spacing:0.1px;overflow:hidden;padding:20px;color:white;overflow:hidden;margin:0;font-size:100%\"> 1 | Libraries</div>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import model_selection, metrics, impute, preprocessing","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-31T18:00:36.186181Z","iopub.execute_input":"2022-03-31T18:00:36.186827Z","iopub.status.idle":"2022-03-31T18:00:36.597037Z","shell.execute_reply.started":"2022-03-31T18:00:36.186696Z","shell.execute_reply":"2022-03-31T18:00:36.59587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nimport catboost as cb\nimport lightgbm as lgb\nfrom sklearn import linear_model","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:00:36.59898Z","iopub.execute_input":"2022-03-31T18:00:36.59928Z","iopub.status.idle":"2022-03-31T18:00:37.532589Z","shell.execute_reply.started":"2022-03-31T18:00:36.599245Z","shell.execute_reply":"2022-03-31T18:00:37.531517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# <div style=\"color:#fff;display:fill;border-radius:10px;background-color:#000000;text-align:left;letter-spacing:0.1px;overflow:hidden;padding:20px;color:white;overflow:hidden;margin:0;font-size:100%\"> 2 | Data loading</div>","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:00:37.534747Z","iopub.execute_input":"2022-03-31T18:00:37.535094Z","iopub.status.idle":"2022-03-31T18:00:37.540249Z","shell.execute_reply.started":"2022-03-31T18:00:37.535049Z","shell.execute_reply":"2022-03-31T18:00:37.539604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/titanic/train.csv')\ntest = pd.read_csv('../input/titanic/test.csv')\nsubmission = pd.read_csv('../input/titanic/gender_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:00:37.541487Z","iopub.execute_input":"2022-03-31T18:00:37.541996Z","iopub.status.idle":"2022-03-31T18:00:37.56808Z","shell.execute_reply.started":"2022-03-31T18:00:37.541964Z","shell.execute_reply":"2022-03-31T18:00:37.567354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Survived'] = -1","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:00:37.572283Z","iopub.execute_input":"2022-03-31T18:00:37.572642Z","iopub.status.idle":"2022-03-31T18:00:37.578723Z","shell.execute_reply.started":"2022-03-31T18:00:37.572605Z","shell.execute_reply":"2022-03-31T18:00:37.57774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df = pd.concat([train,test], axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:00:37.580077Z","iopub.execute_input":"2022-03-31T18:00:37.580704Z","iopub.status.idle":"2022-03-31T18:00:37.60095Z","shell.execute_reply.started":"2022-03-31T18:00:37.580671Z","shell.execute_reply":"2022-03-31T18:00:37.599873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# <div style=\"color:#fff;display:fill;border-radius:10px;background-color:#000000;text-align:left;letter-spacing:0.1px;overflow:hidden;padding:20px;color:white;overflow:hidden;margin:0;font-size:100%\"> 3 | Preprocessing and Feature engineering</div>","metadata":{}},{"cell_type":"code","source":"def preprocessing_inputs (df):\n    df = df.copy()\n    \n    # Feature Engineering:\n    # Name:\n    # Casting the Name feature and creating a new feature just with the title \n    df['Title'] = df['Name'].apply(lambda x: x.split('.')[0]).apply(lambda x : x.split(',')[1])\n    \n    # Last name extraction\n    df['LastName'] = df['Name'].str.extract('^(.+?),', expand = False)\n    # Last name count\n    \n    # Age:\n    df['Age'].fillna(df['Age'].median(), inplace = True)\n    df['AgeBin'] = pd.cut(df['Age'].astype(int), 5, labels=False)\n    \n    # Sib & Parch\n    # Math transform on Sib and Parch & binning\n    df['Family'] = df['SibSp'] + df['Parch'] + 1                        \n    \n    # Ticket:\n    # Ticket number\n    df['TicketNumber'] = df['Ticket'].apply(lambda x: x.split(' ')).apply(lambda x : x[1] if len (x) > 1 else x[0]).apply(lambda x: x[0])\n    df['TicketNumber'].replace({'LINE': -1, 'SC/AH Basle 541': -1, 'L':-1, 'B':-1}, inplace=True)\n    df['TicketNumber'] = df['TicketNumber'].astype(int)\n    \n    # Fare:\n    df['SocialClassByFare'] = df['Fare'].apply(lambda x : 'Rich' if x > df['Fare'].quantile(0.75) else ( 'Poor' if x < df['Fare'].quantile(0.25) else 'Midd' ))\n    df['FareBins'] = pd.qcut(df['Fare'], 3, labels=False)\n    \n    # Cabin:\n    df['CabinCode'] = df['Cabin'].apply(lambda x : str(x)).apply(lambda x: 'U' if x == 'nan' else x[0])\n    #df.drop('Cabin', inplace=True)\n    \n    # Embarked:\n    df['Embarked'] = df['Embarked'].apply(lambda x : str(x)).apply(lambda x: 'U' if x == 'nan' else x)\n    \n    # Split the dataframe\n    train = df.query(\"Survived != -1\").copy()\n    train['Survived'] = train['Survived'].astype(int)\n    \n    test = df.query(\"Survived == -1\").copy()\n    test.drop(['Survived'], axis = 1, inplace=True)\n    \n    return train, test","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:00:37.816169Z","iopub.execute_input":"2022-03-31T18:00:37.816699Z","iopub.status.idle":"2022-03-31T18:00:37.832432Z","shell.execute_reply.started":"2022-03-31T18:00:37.816663Z","shell.execute_reply":"2022-03-31T18:00:37.831279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = preprocessing_inputs(combined_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:00:38.250421Z","iopub.execute_input":"2022-03-31T18:00:38.251024Z","iopub.status.idle":"2022-03-31T18:00:40.030828Z","shell.execute_reply.started":"2022-03-31T18:00:38.250971Z","shell.execute_reply":"2022-03-31T18:00:40.029863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# <div style=\"color:#fff;display:fill;border-radius:10px;background-color:#000000;text-align:left;letter-spacing:0.1px;overflow:hidden;padding:20px;color:white;overflow:hidden;margin:0;font-size:100%\"> 4 | Trainer class</div>","metadata":{}},{"cell_type":"code","source":"class ModelTrain():\n            \n    \"\"\"\n    Trainer class which is responsible for imputing, encoding, scaling and training the models.\n    Args:\n        - train: Train dataframe.\n        - test: Test dataframe.\n        - sub: Submission dataframe.\n        - n_splits: Number of folds.\n        - num_feats: Numerical features (list).\n        - cat_feats: Categorical features (list).\n        - model_name: The corresponding model name to be used to identify it in the training process.\n        - model: Model to train.\n        - preprocessing: Preprocessing process (boolean).\n        - impute_type: Impute type for missing values.\n        - encode_type: Encoding type for categorical features.\n        - scale_type: Scale or transforming.\n    \"\"\"\n    \n    def __init__(self, train, test, sub, n_splits, features, num_feats, cat_feats, target, model_name, model,\n                 preprocessing=False, impute_type=False, encode_type=False, scale_type=False):\n        \n        self.train = train\n        self.test = test\n        self.sub = sub\n        self.n_splits = n_splits\n        self.num_feats = num_feats\n        self.cat_feats = cat_feats\n        self.target = target\n        self.model_name = model_name\n        self.model = model\n        self.preprocessing = preprocessing\n        self.impute_type = impute_type\n        self.encode_type = encode_type\n        self.scale_type = scale_type\n        \n        self.valid_preds = {}\n        self.test_preds = []\n        \n    def kfold(self):\n        \n        \"\"\"\n        Folds creation.\n        \"\"\"\n            \n        n_splits = self.n_splits\n        target = self.target\n        df = self.train.copy()\n        df['kfold'] = -1\n               \n        skf= model_selection.StratifiedKFold (n_splits=n_splits, shuffle=True, random_state=0)\n        for fold, (train_idx, valid_idx) in enumerate (skf.split(X=df, y=df[target].values)):\n            df.loc[valid_idx,'kfold'] = fold\n        return df\n    \n    def imputer_(self, xtrain, xvalid, xtest):\n        \n        \"\"\"\n        Impute the missing values.\n        Args:\n            - xtrain: Train dataframe.\n            - xvalid: Validation dataframe.\n            - xtest: Test dataframe.\n        \"\"\"\n            \n        self.xtrain = xtrain\n        self.xvalid = xvalid\n        self.xtest = xtest\n        \n        num_feats = self.num_feats\n        \n        if self.impute_type == 'SI':\n            si = impute.SimpleImputer()\n            self.xtrain[num_feats] = si.fit_transform(self.xtrain[num_feats])\n            self.xvalid[num_feats] = si.transform(self.xvalid[num_feats])\n            self.xtest[num_feats] = si.transform(self.xtest[num_feats])\n            return self.xtrain[num_feats], self.xvalid[num_feats], self.xtest[num_feats]\n        \n        elif self.impute_type == 'KNN':\n            knn = impute.KNNImputer()\n            self.xtrain[num_feats] = knn.fit_transform(self.xtrain[num_feats])\n            self.xvalid[num_feats] = knn.transform(self.xvalid[num_feats])\n            self.xtest[num_feats] = knn.transform(self.xtest[num_feats])\n            return self.xtrain[num_feats], self.xvalid[num_feats], self.xtest[num_feats]\n        \n        else:\n            raise Exception ('Impute type not supported, supported types SI or KNN.')\n    \n    def encoder_(self, xtrain, xvalid, xtest):\n        \n        \"\"\"\n        Encode categorical values.\n        Args:\n            - xtrain: Train dataframe.\n            - xvalid: Validation dataframe.\n            - xtest: Test dataframe.\n        \"\"\"\n        \n        self.xtrain = xtrain\n        self.xvalid = xvalid\n        self.xtest = xtest\n        cat_feats = self.cat_feats\n        \n        if self.encode_type == 'OHE':\n            ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown='ignore')\n            ohe.fit(self.xtrain[cat_feats])\n            encoded_cols = list(ohe.get_feature_names(cat_feats))\n            self.xtrain[encoded_cols] = ohe.fit_transform(self.xtrain[cat_feats].fillna('-9999'))\n            self.xvalid[encoded_cols] = ohe.transform(self.xvalid[cat_feats].fillna('-9999'))\n            self.xtest[encoded_cols] = ohe.transform(self.xtest[cat_feats].fillna('-9999'))\n            return self.xtrain, self.xvalid, self.xtest, encoded_cols\n        \n        elif self.encode_type == 'LBL':\n            lbl_ = preprocessing.LabelEncoder()\n            encoded_cols = list(self.train[cat_feats].columns)\n            for c in encoded_cols:\n                self.xtrain.loc[:, c] = lbl_.fit_transform(self.xtrain[c].fillna('-9999'))\n                self.xvalid.loc[:, c] = lbl_.transform(self.xvalid[c].fillna('-9999'))\n                self.xtest.loc[:, c] = lbl_.transform(self.xtest[c].fillna('-9999'))\n            return self.xtrain, self.xvalid, self.xtest, encoded_cols\n        \n        elif self.encode_type == 'ORD':\n            # Works with sklearn v 0.24 by setting the handle_unknown parameter\n            ord_ = preprocessing.OrdinalEncoder()\n            encoded_cols = list(self.train[cat_feats].columns)\n            self.xtrain[encoded_cols] = ord_.fit_transform(self.xtrain[cat_feats].fillna('-9999'))\n            self.xvalid[encoded_cols] = ord_.transform(self.xvalid[cat_feats].fillna('-9999'))\n            self.xtest[encoded_cols] = ord_.transform(self.xtest[cat_feats].fillna('-9999'))\n            return self.xtrain, self.xvalid, self.xtest, encoded_cols\n        \n        else:\n            raise Exception ('Encoded type not supported, supported types OHE or ORD.')\n            \n    def scaler_(self, xtrain, xvalid, xtest):\n        \n        \"\"\"\n        Scale the numerical values.\n        Args:\n            - xtrain: Train dataframe.\n            - xvalid: Validation dataframe.\n            - xtest: Test dataframe.\n        \"\"\"\n        \n        self.xtrain = xtrain\n        self.xvalid = xvalid\n        self.xtest = xtest\n        num_feats = self.num_feats\n        \n        if self.scale_type == 'STD':\n            std = preprocessing.StandardScaler()\n            self.xtrain[num_feats] = std.fit_transform(self.xtrain[num_feats])\n            self.xvalid[num_feats] = std.transform(self.xvalid[num_feats])\n            self.xtest[num_feats] = std.transform(self.xtest[num_feats])\n            return self.xtrain[num_feats], self.xvalid[num_feats], self.xtest[num_feats]\n        \n        elif self.scale_type == 'RBT':\n            rbt = preprocessing.RobustScaler()\n            self.xtrain[num_feats] = rbt.fit_transform(self.xtrain[num_feats])\n            self.xvalid[num_feats] = rbt.transform(self.xvalid[num_feats])\n            self.xtest[num_feats] = rbt.transform(self.xtest[num_feats])\n            return self.xtrain[num_feats], self.xvalid[num_feats], self.xtest[num_feats]\n        \n        else:\n            raise Exception ('Scaler type not supported, supported types STD or RBT.')\n    \n    def train_test (self):     \n        \n        self.train_df = self.kfold().copy()\n        scores = []\n               \n        for fold in range(self.n_splits):\n        \n            X_train = self.train_df[self.train_df.kfold != fold].reset_index(drop=True)\n            X_valid = self.train_df[self.train_df.kfold == fold].reset_index(drop=True)\n\n            X_test = self.test[self.num_feats+self.cat_feats].copy() #\n            \n            X_valid_ids = X_valid.PassengerId.values.tolist()\n\n            y_train = X_train[self.target]\n            y_valid = X_valid[self.target]\n\n            X_train = X_train[self.num_feats+self.cat_feats]\n            X_valid = X_valid[self.num_feats+self.cat_feats]\n            \n            # Preprocessing\n            if self.preprocessing:\n                if self.impute_type == False or self.scale_type == False:\n                    xtrain, xvalid, xtest = X_train, X_valid, X_test    \n                if self.impute_type != False:\n                    xtrain, xvalid, xtest = self.imputer_(X_train, X_valid, X_test)\n                if self.scale_type != False:\n                    xtrain, xvalid, xtest = self.scaler_(xtrain, xvalid, xtest)\n                if self.encode_type != False:\n                    xtrain_e, xvalid_e, xtest_e, cols = self.encoder_(X_train, X_valid, X_test)           \n                else:\n                    xtrain_e, xvalid_e, xtest_e, cols = X_train, X_valid, X_test, categoric_features             \n       \n                X_train = pd.concat([xtrain[self.num_feats], xtrain_e[cols]], axis=1).values\n                X_valid = pd.concat([xvalid[self.num_feats], xvalid_e[cols]], axis=1).values\n                X_test = pd.concat([xtest[self.num_feats], xtest_e[cols]], axis=1).values\n            \n            # Training & Predicting\n            model.fit(X_train, y_train) \n\n            preds_valid = model.predict(X_valid)\n            preds_test = model.predict(X_test)\n            \n            self.valid_preds.update(dict(zip(X_valid_ids,preds_valid )))\n            self.test_preds.append(preds_test)\n\n            acc = metrics.accuracy_score(y_valid, preds_valid)\n            scores.append(acc)\n            \n            print(f'Fold = {fold}, ACC = {acc:.10f}')\n        print(f'Mean score {self.model_name} = {np.mean(scores):.10f}')\n        print()\n        \n        valid_df = pd.DataFrame.from_dict(self.valid_preds, orient='index').reset_index().rename(columns = {'index':'PassengerId', 0:f'preds_{self.model_name}'})\n        \n        test_df = self.sub.copy()\n        test_df.drop(self.target, axis=1, inplace=True)\n        test_df.loc[:,f'preds_{self.model_name}'] = np.mean(np.column_stack (self.test_preds), axis=1 ).astype(int)\n        \n        return valid_df , test_df, self.train_df","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:00:40.033277Z","iopub.execute_input":"2022-03-31T18:00:40.033637Z","iopub.status.idle":"2022-03-31T18:00:40.087284Z","shell.execute_reply.started":"2022-03-31T18:00:40.03359Z","shell.execute_reply":"2022-03-31T18:00:40.085999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {\n          'XGB': xgb.XGBRFClassifier(random_state = 0, objective = 'reg:squarederror'),\n          'LGBM': lgb.LGBMClassifier(random_state = 0),\n          'CB': cb.CatBoostClassifier(random_state = 0, verbose=False),\n          'LR': linear_model.LogisticRegression(solver='liblinear'),\n          'RI': linear_model.RidgeClassifier()\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:00:40.08867Z","iopub.execute_input":"2022-03-31T18:00:40.089016Z","iopub.status.idle":"2022-03-31T18:00:40.1084Z","shell.execute_reply.started":"2022-03-31T18:00:40.088955Z","shell.execute_reply":"2022-03-31T18:00:40.107519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['PassengerId', 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Title', \n            'AgeBin', 'Family', 'TicketNumber', 'SocialClassByFare', 'FareBins', 'CabinCode']","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:00:40.111589Z","iopub.execute_input":"2022-03-31T18:00:40.112281Z","iopub.status.idle":"2022-03-31T18:00:40.126505Z","shell.execute_reply.started":"2022-03-31T18:00:40.112245Z","shell.execute_reply":"2022-03-31T18:00:40.12575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categoric_features = [feature for feature in train_df[features] if train_df[feature].dtype =='O']\nnumeric_features = [feature for feature in train_df[features] if feature not in categoric_features+['PassengerId','kfold','Survived']]        ","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:00:42.811544Z","iopub.execute_input":"2022-03-31T18:00:42.811914Z","iopub.status.idle":"2022-03-31T18:00:42.823047Z","shell.execute_reply.started":"2022-03-31T18:00:42.81188Z","shell.execute_reply":"2022-03-31T18:00:42.821616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs_valid = []\ndfs_test = []\nfor name, model in models.items():\n    titanic = ModelTrain(train_df, test_df, submission, 5, features , numeric_features, categoric_features, 'Survived', name, \n                        model, preprocessing=True, impute_type='KNN', scale_type='RBT', encode_type='OHE' )\n    df_valid, df_test, train_df = titanic.train_test()\n    dfs_valid.append(df_valid)\n    dfs_test.append(df_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:00:45.14498Z","iopub.execute_input":"2022-03-31T18:00:45.146272Z","iopub.status.idle":"2022-03-31T18:00:56.441071Z","shell.execute_reply.started":"2022-03-31T18:00:45.146213Z","shell.execute_reply":"2022-03-31T18:00:56.440019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_valid_df = pd.concat(dfs_valid, axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_test_df = pd.concat(dfs_test, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n# <div style=\"color:#fff;display:fill;border-radius:10px;background-color:#000000;text-align:left;letter-spacing:0.1px;overflow:hidden;padding:20px;color:white;overflow:hidden;margin:0;font-size:100%\"> 5 | Blending</div>\n\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em; margin-left:20px\">   \nIn this particular case I'm going to use a meta model to do the blend, a Logistic Regression model to be more precise. You can tried an weighted average of the predictions, usually performs better. </p>","metadata":{}},{"cell_type":"code","source":"df_valid = final_valid_df.iloc[:,[0,1,3,5,7,9]]\ndf_test = final_test_df.iloc[:,[0,1,3,5,7,9]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_blend = pd.merge(train_df , df_valid, on='PassengerId', how='left')\ntest_blend = pd.merge(test_df , df_test, on='PassengerId' , how='left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat =  ['preds_XGB','preds_LGBM','preds_CB','preds_LR', 'preds_RI']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Meta Model LR\n\ntrain = train_blend.copy()\ntest = test_blend.copy()\n\nscores = []\nvalid_preds_lr = {}\ntest_preds_lr = []\n\nfor fold in range(5):\n    \n    X_train = train[train.kfold != fold].reset_index(drop=True)\n    X_valid = train[train.kfold == fold].reset_index(drop=True)\n    \n    X_test = test[feat].copy() #\n    \n    X_valid_ids = X_valid.PassengerId.values.tolist()\n\n    y_train = X_train['Survived']\n    y_valid = X_valid['Survived']\n    \n    X_train = X_train[feat]\n    X_valid = X_valid[feat]\n    \n    # Model\n    model = linear_model.LogisticRegression()\n    model.fit(X_train, y_train)\n        \n    preds_valid = model.predict(X_valid)\n    preds_test = model.predict(X_test)\n    \n    valid_preds_lr.update(dict(zip(X_valid_ids,preds_valid )))\n    test_preds_lr.append(preds_test)\n    \n    acc = metrics.accuracy_score(y_valid, preds_valid)\n    scores.append(acc)\n    \n    print(f' Fold = {fold}, ACC = {acc:.10f}')\nprint(f'Mean score = {np.mean(scores):.10f} Std = {np.std(scores):.3f} ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"level1_valid_preds_lr = pd.DataFrame.from_dict(valid_preds_lr, orient='index').reset_index().rename(columns = {'index':'PassengerId', 0:'lr_pred_1'})\n\nlevel1_test_preds_lr = submission.copy()\nlevel1_test_preds_lr.Survived = np.mean(np.column_stack (test_preds_lr), axis=1 ).astype(int)\nlevel1_test_preds_lr.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; margin-left:20px\">   \nThanks for taking the time to read my notebook, greetings! </p>","metadata":{}}]}