{"cells":[{"metadata":{"_uuid":"6b07cd2a3a3c758dbdff5c4a571c3e4ee94e8793"},"cell_type":"markdown","source":"# Simple Deep Neural Networks with Keras\nIn this tutorial i am going to show how to implement Deep Neural Networks in keras and Also we will have a look on simple feature engineering to be able to classify the dataset efficiently","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a36183f4043627fec4f3650053c87c7ca53218d"},"cell_type":"markdown","source":"First let's start by importing our dataset into dataframes using pandas. Dataframes enables us to work easily with data usign its built in functions.","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\ntraining = pd.read_csv(\"../input/train.csv\");\n\nx_test = pd.read_csv(\"../input/test.csv\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97c4d91a27c09288157977984350cf16ffbe6838"},"cell_type":"markdown","source":"Now let's have a look on the dataset and search for important information manually.","execution_count":null},{"metadata":{"_uuid":"28840817ba08cfc35db8f2e57d86ff2b157a26b9","trusted":true},"cell_type":"code","source":"training.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training['n_new']= training['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training['n_new'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training['Title'] = 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training.loc[training[\"n_new\"] == 'Mr', 'Title'] = 1\ntraining.loc[training[\"n_new\"] == 'Miss', 'Title'] = 4\ntraining.loc[training[\"n_new\"] == 'Mrs', 'Title'] = 5\ntraining.loc[training[\"n_new\"] == 'Master', 'Title'] = 3\ntraining.loc[training[\"n_new\"] == 'Dr', 'Title'] = 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8bb539321270d3849d28c8718c017fa68d38a55"},"cell_type":"markdown","source":"Now I will explore the correlations of the featues relative to the target variable.\n#### Note: not all features are listed but only numeric features.","execution_count":null},{"metadata":{"_uuid":"7604110320e2365715dcc82ba4ad61dc2ddbfa7b","trusted":true},"cell_type":"code","source":"import seaborn as sns\n\n\nimport matplotlib.pyplot as plt\n\n\ncorr = training.corr()\nf, ax = plt.subplots(figsize=(25, 25))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, vmax=.1, center=0,\n            square=True, linewidths=.5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1b03833324ea57f1b212c4901336790861fd487"},"cell_type":"markdown","source":"I will explore the size of the dataset to compare it with the number of NANs in the dataset","execution_count":null},{"metadata":{"_uuid":"d9b24d2016dad11bb905008fba850ecb88555deb","trusted":true},"cell_type":"code","source":"print(\"The number of traning examples(data points) = %i \" % training.shape[0])\nprint(\"The number of features we have = %i \" % training.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9a40efd24290bbcf850312ca62a93379cd68491"},"cell_type":"markdown","source":"I will count the number of data examples in each class in the target to determine which metric to use while evaluationg performance.","execution_count":null},{"metadata":{"_uuid":"db74e9a52ccc648877bb40881c98328ab454b393","scrolled":true,"trusted":true},"cell_type":"code","source":"unique, count= np.unique(training[\"Survived\"], return_counts=True)\nprint(\"The number of occurances of each class in the dataset = %s \" % dict (zip(unique, count) ), \"\\n\" )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba43f7e67b107fbe128df6818e4c45dfdf424c19"},"cell_type":"markdown","source":"The numbers doesn't seem to be very far from each other, So i will use accuracy for performance eval..","execution_count":null},{"metadata":{"_uuid":"2b591db4e26212f186e884786819f3bcc9f92a70"},"cell_type":"markdown","source":"Now i will check the number of Null values. If most of a column's values are Nulls or NaNs i will drop it because filling it will not be accurate but if the number is small then i will fill it with the mean values.","execution_count":null},{"metadata":{"_uuid":"c414b828b9c785123d5890a1f9de373cea4e47ab","trusted":true},"cell_type":"code","source":"training.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d01947a20fc55d38ddf2569af9b5e9f558dcae8b"},"cell_type":"markdown","source":"From the results of correlation matrix and the Nan number and manual checking of dataset, I will drop \"Name\" ,\"Ticket\",\"Cabin\" and \"PassengerId\".\n<lb>I will also store the label column and drop it from training.\n<lb>Then i will engineer the catagorical features and map the strings to integers to be able to use it in the model later.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"training.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C = training.Cabin[training.Cabin.isna()]\nC_not = training.Cabin[training.Cabin.notna()]\nC.values[:] = 0\nC_not.values[:] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cabine_not = pd.concat([C, C_not]).sort_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"702243b3b8b99fae9b0acb01d0705a624d34a7a9","scrolled":true,"trusted":true},"cell_type":"code","source":"np.random.seed(0)\ntraining['sp'] = training.SibSp + training['Parch']\ntraining['cabine_n'] = cabine_not\ntraining.cabine_n = training.cabine_n.astype(int)\ntraining.drop([\"n_new\", \"Name\" ,\"Embarked\", \"PassengerId\",\"Ticket\",\"Cabin\"], inplace = True, axis = 1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = training\nrepCol9 = {1 : 3 ,   2 : 2 , 3 : 1  }\n\n\nx_train['IsAlone'] = 1\nx_train['IsAlone'].loc[x_train['sp'] > 0] = 0\n\nx_train.replace({'Pclass': repCol9} , inplace = True )\nx_train = pd.get_dummies(x_train)\n\ny_train = x_train[\"Survived\"]\nx_train = x_train.drop(['Survived'], axis = 1)\n\nprint(y_train.shape )\ny_train.head(20)\nx_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nimp = IterativeImputer(max_iter=10, random_state=0)\nx = imp.fit_transform(x_train)\nx_train = pd.DataFrame(x, columns = x_train.columns)\nx_train[[ 'Pclass', 'SibSp', 'Parch', 'Title', 'sp',\n       'cabine_n', 'IsAlone', 'Sex_female', 'Sex_male']] = x_train[[ 'Pclass', 'SibSp', 'Parch', 'Title', 'sp',\n       'cabine_n', 'IsAlone', 'Sex_female', 'Sex_male']].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b623a6ffeda73b4fd885b5085c6341da84ada00f","trusted":true},"cell_type":"code","source":"from scipy import stats\nimport numpy as np\n\nz = np.abs(stats.zscore(x_train))\nzee = (np.where(z > 3))[1]\n\nprint(\"number of data examples greater than 3 standard deviations = %i \" % len(zee))\n# x_train = x_train[(z < 2.5).all(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"297140108fe482363791fa2d647ddd0f03a99e15"},"cell_type":"markdown","source":"I will Plot some features to see if there is any pattern in the data.","execution_count":null},{"metadata":{"_uuid":"340a7f8fcb1206acbbfe59b01c41711e034cfd27","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.pairplot(training)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2f282a4678c54731c02b2b0e9cc47c6133bb004"},"cell_type":"markdown","source":"Since i dropped some features from the training set I have to drop the same featurres from the test set and do the same steps of feature eng.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test['n_new']= x_test['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test['Title'] = 0\n\nx_test.loc[x_test[\"n_new\"] == 'Mr', 'Title'] = 1\nx_test.loc[x_test[\"n_new\"] == 'Miss', 'Title'] = 4\nx_test.loc[x_test[\"n_new\"] == 'Mrs', 'Title'] = 5\nx_test.loc[x_test[\"n_new\"] == 'Master', \"Title\"] = 3\nx_test.loc[x_test[\"n_new\"] == 'Dr', 'Title'] = 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"786e9d4073b9e8717325297c67ee47c28f9be7f7","trusted":true},"cell_type":"code","source":"C = x_test.Cabin[x_test.Cabin.isna()]\nC_not = x_test.Cabin[x_test.Cabin.notna()]\nC.values[:] = 0\nC_not.values[:] = 1\n\nx_test['sp'] = x_test.SibSp + x_test['Parch']\nx_test['cabine_n'] = cabine_not\nx_test.cabine_n = x_test.cabine_n.astype(int)\n\nx_test['IsAlone'] = 1\nx_test['IsAlone'].loc[x_test['sp'] > 0] = 0\n\n\nx_test.drop([\"Name\", \"Embarked\", \"Ticket\", \"n_new\", \"Cabin\"], inplace = True, axis = 1 )\nx_test.replace({'Pclass': repCol9} , inplace = True )\nx_test = pd.get_dummies(x_test)\nx_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d16aae371a092c32ef88c6f7101387269e91839c"},"cell_type":"markdown","source":"Now it is time to design the ML Pipeline. I will use Deep Neural Networks in Keras to classify the dataset. The number of layers i am using is optmized using some error analys of the results.\n<lb> I will use early stopping to stop if the error is not decreasing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fact = y_train[y_train == 0].count() / y_train[y_train == 1].count()\n\nclass_weight = {1: fact, 0: 1.}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b033228ff01ff54d6c3e471f8cde84edc763da2","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import np_utils\nfrom keras import callbacks\nfrom keras import optimizers\nfrom keras.layers import BatchNormalization\n\n#y_train = np_utils.to_categorical(y_train)\n\nInputDimension = 11\nprint(y_train.shape )\n\nmodel = Sequential()\nmodel.add(Dense(25,input_dim=InputDimension, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\n\n\n\nmodel.add(Dense(2, activation='softmax'))\n\n\nearlystopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='min')\noptimizer = optimizers.Adam(lr=0.001, decay=0.0001)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\nhistory = model.fit(x_train, pd.get_dummies(y_train), epochs=2000, batch_size=60, validation_split=0.2, verbose=1, callbacks=[earlystopping], class_weight = class_weight)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6c9703ed468d0ec3351a25dc9fba4ac89483901"},"cell_type":"markdown","source":"Now let's see how good is my training with respect to validation accuracies.","execution_count":null},{"metadata":{"_uuid":"d2d7508d6d394d8c85d3c27c254b9e2b2ea12d79","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92f7a5cb757572ccb53ad62e551cf653e5262980"},"cell_type":"markdown","source":"Now i will normalize the test set as i did before and will fill null values in the dataset as well.","execution_count":null},{"metadata":{"_uuid":"361656fc7fed40b326be7c17a104a32ddafeef6b","trusted":true},"cell_type":"code","source":"id = x_test['PassengerId']\n\nx_test.drop(['PassengerId'], inplace = True, axis = 1)\n\nimp = IterativeImputer(max_iter=10, random_state=0)\nx = imp.fit_transform(x_test)\nx_test = pd.DataFrame(x, columns = x_train.columns)\nx_test[[ 'Pclass', 'SibSp', 'Parch', 'Title', 'sp',\n       'cabine_n', 'IsAlone', 'Sex_female', 'Sex_male']] = x_test[['Pclass', 'SibSp', 'Parch', 'Title', 'sp',\n       'cabine_n', 'IsAlone', 'Sex_female', 'Sex_male']].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"162726d19aaec49fe1e29599637424d79537b346"},"cell_type":"markdown","source":"Now every thing is okay with the dataset so i will predict the output values for submission","execution_count":null},{"metadata":{"_uuid":"dcd1fdc352632ae8cbd0988065bfa31e61915972","scrolled":true,"trusted":true},"cell_type":"code","source":"predictions = model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee468f31cc1b1677e91872db7035d6d375c26789"},"cell_type":"markdown","source":"Predictions will return probability between 0 and 1 for survived or non survived so i will take the argmax() of the array to get the max index for each test example","execution_count":null},{"metadata":{"_uuid":"15f383ea3ec3fd9ff4b8cf8040ff53fba6dd4ce4","scrolled":false,"trusted":true},"cell_type":"code","source":"predictions = np.argmax(predictions, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"186aeeef16eaaccd8f40ea9b863fca44f4768a61"},"cell_type":"markdown","source":"It is time to make submission.","execution_count":null},{"metadata":{"_uuid":"41d2e0b4d974f05b5a5b73df6bba8b7f612ce0b5","scrolled":true,"trusted":true},"cell_type":"code","source":"id.reset_index(drop=True, inplace=True)\nout = pd.DataFrame({'PassengerId': id, \"Survived\": predictions})\nout.to_csv('titanic-predictions.csv', index = False)\nout.head(100)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}