{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import the relevant libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-17T12:10:55.006257Z","iopub.execute_input":"2022-05-17T12:10:55.007013Z","iopub.status.idle":"2022-05-17T12:10:55.038621Z","shell.execute_reply.started":"2022-05-17T12:10:55.006918Z","shell.execute_reply":"2022-05-17T12:10:55.037744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n\nseed = 4092022","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:55.040125Z","iopub.execute_input":"2022-05-17T12:10:55.040338Z","iopub.status.idle":"2022-05-17T12:10:56.517644Z","shell.execute_reply.started":"2022-05-17T12:10:55.040312Z","shell.execute_reply":"2022-05-17T12:10:56.516699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## Concatenation","metadata":{}},{"cell_type":"code","source":"def concatenated_df(train_data, test_data):\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\ndf_train = pd.read_csv('/kaggle/input/titanic/train.csv')\ndf_test = pd.read_csv('/kaggle/input/titanic/test.csv')\ndf_all = concatenated_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Sets' \n\ndfs = [df_train, df_test]\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:56.519212Z","iopub.execute_input":"2022-05-17T12:10:56.519472Z","iopub.status.idle":"2022-05-17T12:10:56.572062Z","shell.execute_reply.started":"2022-05-17T12:10:56.51943Z","shell.execute_reply":"2022-05-17T12:10:56.571149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.info())\ndf_train.sample(3)\n# The Training Data has 891 rows and 12 columns. The extra column/feature is 'Survived.'\n# 'Age', 'Cabin', and 'Embarked' features have missing values.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:56.574298Z","iopub.execute_input":"2022-05-17T12:10:56.574702Z","iopub.status.idle":"2022-05-17T12:10:56.611145Z","shell.execute_reply.started":"2022-05-17T12:10:56.574657Z","shell.execute_reply":"2022-05-17T12:10:56.61056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test.info())\ndf_test.sample(3)\n# The Test Data has 418 rows and 11 columns.\n# 'Age', 'Fare', and 'Cabin' have missing values.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:56.612217Z","iopub.execute_input":"2022-05-17T12:10:56.61263Z","iopub.status.idle":"2022-05-17T12:10:56.638893Z","shell.execute_reply.started":"2022-05-17T12:10:56.612592Z","shell.execute_reply":"2022-05-17T12:10:56.637889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_missing(df):\n    for col in df.columns.tolist():\n        print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n# This prints column missing values and sums up the total number of missing values per category.\nfor df in dfs:\n    print('{}'.format(df.name))\n    display_missing(df)\n# To display the total number of Missing Values per column/feature in the Training, Validation, & Test Datasets.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:56.640627Z","iopub.execute_input":"2022-05-17T12:10:56.641031Z","iopub.status.idle":"2022-05-17T12:10:56.661031Z","shell.execute_reply.started":"2022-05-17T12:10:56.640981Z","shell.execute_reply":"2022-05-17T12:10:56.660231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dealing with Missing Values","metadata":{}},{"cell_type":"markdown","source":"### 'Age'","metadata":{}},{"cell_type":"code","source":"df_all_corr = df_all.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=True).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_all_corr[df_all_corr['Feature 1'] == 'Age']\n# To see the correlation of the 'Age' feature and other features.\n# Age is not correlated with 'PassengerId', so it doesn't have any effect on the target.\n# 'Survived' is the feature I'm trying to predict. ","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:56.66214Z","iopub.execute_input":"2022-05-17T12:10:56.662486Z","iopub.status.idle":"2022-05-17T12:10:56.687248Z","shell.execute_reply.started":"2022-05-17T12:10:56.662456Z","shell.execute_reply":"2022-05-17T12:10:56.686655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 'Passenger Class'","metadata":{}},{"cell_type":"code","source":"df_all_corr = df_all.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=True).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_all_corr[df_all_corr['Feature 1'] == 'Pclass']\n# It is noteworthy that 'Pclass' and 'Survived' features are correlated.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:56.688592Z","iopub.execute_input":"2022-05-17T12:10:56.688793Z","iopub.status.idle":"2022-05-17T12:10:56.705147Z","shell.execute_reply.started":"2022-05-17T12:10:56.688767Z","shell.execute_reply":"2022-05-17T12:10:56.70455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using the Medians of 'Passenger Class' and 'Sex' features to fill in the NaN in 'Age'","metadata":{}},{"cell_type":"code","source":"age_by_pclass_sex = df_all.groupby(['Sex', 'Pclass']).median()['Age']\n\nfor pclass in range(1, 4):\n    for sex in ['female', 'male']:\n        print('Median age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\nprint('Median age of all passengers: {}'.format(df_all['Age'].median()))\n# Median age of 'Pclass' groups is the best choice because of its high correlation with Age (0.408) and Survived (0.338). \n# It is also more logical to group ages by passenger classes instead of other features.\n# When 'Pclass' increases (1 being the highest), the median age for both males and females also increases. \n# However, females tend to have slightly lower median Age than males. \n\ndf_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n# To fill the missing values in Age with the medians of Sex and Pclass groups (28).","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:56.706226Z","iopub.execute_input":"2022-05-17T12:10:56.706584Z","iopub.status.idle":"2022-05-17T12:10:56.731237Z","shell.execute_reply.started":"2022-05-17T12:10:56.706549Z","shell.execute_reply":"2022-05-17T12:10:56.730296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 'Embarked'","metadata":{}},{"cell_type":"code","source":"df_all[df_all['Embarked'].isnull()]\n# To find out the 2 missing values of 'Embarked' feature.\n# After researching, I found out that Mrs. Stone embarked from Southampton 'S' with her maid Miss Icard.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:56.734765Z","iopub.execute_input":"2022-05-17T12:10:56.734988Z","iopub.status.idle":"2022-05-17T12:10:56.751745Z","shell.execute_reply.started":"2022-05-17T12:10:56.734961Z","shell.execute_reply":"2022-05-17T12:10:56.750927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all['Embarked'] = df_all['Embarked'].fillna('S')\n# To fill the missing values in 'Embarked' with S.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:56.753027Z","iopub.execute_input":"2022-05-17T12:10:56.75342Z","iopub.status.idle":"2022-05-17T12:10:56.762131Z","shell.execute_reply.started":"2022-05-17T12:10:56.75337Z","shell.execute_reply":"2022-05-17T12:10:56.761424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 'Fare'","metadata":{}},{"cell_type":"code","source":"df_all[df_all['Fare'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:56.763379Z","iopub.execute_input":"2022-05-17T12:10:56.764183Z","iopub.status.idle":"2022-05-17T12:10:56.783998Z","shell.execute_reply.started":"2022-05-17T12:10:56.764145Z","shell.execute_reply":"2022-05-17T12:10:56.783319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"median_fare = df_all.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n# Logically, 'Fare' is related to family size ('Parch' and 'SibSp') and 'Pclass' features.\ndf_all['Fare'] = df_all['Fare'].fillna(median_fare)\n# Therefore, the median 'Fare' value of a male with a third class ticket and no family is a logical choice to fill in this particular missing value.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:56.784986Z","iopub.execute_input":"2022-05-17T12:10:56.78533Z","iopub.status.idle":"2022-05-17T12:10:56.799052Z","shell.execute_reply.started":"2022-05-17T12:10:56.785296Z","shell.execute_reply":"2022-05-17T12:10:56.798095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 'Cabin'","metadata":{}},{"cell_type":"code","source":"df_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n# The 'Cabin' feature has many missing values and it cannot be simply ignored because some cabins might have higher survival rates.\n# I created the 'Deck' column, using the first letters of the values in the Cabin column. \n# M stands for Missing Value.\ndf_all_decks = df_all.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', \n                                                                        'Fare', 'Embarked', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\n# After looking at the datasets and doing some research, it turns out that the first letter of the 'Cabin' values are in fact the decks on which the cabins are located.\n\n\ndef get_pclass_dist(df):\n    \n    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n    decks = df.columns.levels[0]    \n    \n    for deck in decks:\n        for pclass in range(1, 4):\n            try:\n                count = df[deck][pclass][0]\n                deck_counts[deck][pclass] = count \n            except KeyError:\n                deck_counts[deck][pclass] = 0\n                \n    df_decks = pd.DataFrame(deck_counts)    \n    deck_percentages = {}\n    # Created a dictionary for every passenger class count in every deck\n\n    for col in df_decks.columns:\n        deck_percentages[col] = [(count / df_decks[col].sum()) * 100 for count in df_decks[col]]\n        \n    return deck_counts, deck_percentages\n    # Created a dictionary for every passenger class percentage in every deck\n\ndef display_pclass_dist(percentages):\n    \n    df_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.9\n    \n    pclass1 = df_percentages[0]\n    pclass2 = df_percentages[1]\n    pclass3 = df_percentages[2]\n    \n    plt.figure(figsize=(30, 15))\n    plt.bar(bar_count, pclass1, color='#87fbb9', edgecolor='black', width=bar_width, label='Passenger Class 1')\n    plt.bar(bar_count, pclass2, bottom=pclass1, color='#87c9fb', edgecolor='black', width=bar_width, label='Passenger Class 2')\n    plt.bar(bar_count, pclass3, bottom=pclass1 + pclass2, color='#f37200', edgecolor='black', width=bar_width, label='Passenger Class 3')\n\n    plt.xlabel('Deck', size=20, labelpad=30)\n    plt.ylabel('Passenger Class Percentage', size=20, labelpad=30)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 20})\n    plt.title('Passenger Class Distribution in Decks', size=24, y=1)   \n    \n    plt.show()    \n\nall_deck_count, all_deck_per = get_pclass_dist(df_all_decks)\ndisplay_pclass_dist(all_deck_per)\n# Decks A, B, C were only for Pclass 1. 100% of A, B and C decks are 1st class passengers\n# Deck D was for Pclasses 1 and 2. Deck D has 87% 1st class and 13% 2nd class passengers\n# Deck E was for all Pclasses. Deck E has 83% 1st class, 10% 2nd class and 7% 3rd class passengers\n# Deck F was only for Pclasses 2 and 3. Deck F has 62% 2nd class and 38% 3rd class passengers\n# Deck G was only for Pclass 3. 100% of G deck are 3rd class passengers\n# M contains all the passengers with missing values.\n# Deck T contains only one Pclass 1 passenger.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:56.80056Z","iopub.execute_input":"2022-05-17T12:10:56.800783Z","iopub.status.idle":"2022-05-17T12:10:57.257011Z","shell.execute_reply.started":"2022-05-17T12:10:56.800756Z","shell.execute_reply":"2022-05-17T12:10:57.25621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dect_regroup = df_all[df_all['Deck'] == 'T'].index\ndf_all.loc[dect_regroup, 'Deck'] = 'A'\n# Deck T has the closest resemblance to Deck A, so I decided to regroup that 1 passenger with Deck A.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:57.258249Z","iopub.execute_input":"2022-05-17T12:10:57.259014Z","iopub.status.idle":"2022-05-17T12:10:57.265129Z","shell.execute_reply.started":"2022-05-17T12:10:57.258975Z","shell.execute_reply":"2022-05-17T12:10:57.264505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all_decks_survived = df_all.groupby(['Deck', 'Survived']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n                                                                                   'Embarked', 'Pclass', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name':'Count'}).transpose()\n\ndef get_survived_dist(df):\n    \n    surv_counts = {'A':{}, 'B':{}, 'C':{}, 'D':{}, 'E':{}, 'F':{}, 'G':{}, 'M':{}}\n    decks = df.columns.levels[0]    \n\n    for deck in decks:\n        for survive in range(0, 2):\n            surv_counts[deck][survive] = df[deck][survive][0]\n            \n    df_surv = pd.DataFrame(surv_counts)\n    surv_percentages = {}\n\n    for col in df_surv.columns:\n        surv_percentages[col] = [(count / df_surv[col].sum()) * 100 for count in df_surv[col]]\n        \n    return surv_counts, surv_percentages\n    # Created a dictionary for every survival count in every deck\n    \ndef display_surv_dist(percentages):\n    \n    df_survived_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.9   \n\n    not_survived = df_survived_percentages[0]\n    survived = df_survived_percentages[1]\n    \n    plt.figure(figsize=(30, 15))\n    plt.bar(bar_count, not_survived, color='#aa99aa', edgecolor='black', width=bar_width, label=\"Not Survived\")\n    plt.bar(bar_count, survived, bottom=not_survived, color='#33cc33', edgecolor='black', width=bar_width, label=\"Survived\")\n \n    plt.xlabel('Deck', size=20, labelpad=30)\n    plt.ylabel('Survival Percentage', size=20, labelpad=30)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Survival Percentage in Decks', size=24, y=1.05)\n    \n    plt.show()\n\nall_surv_count, all_surv_per = get_survived_dist(df_all_decks_survived)\ndisplay_surv_dist(all_surv_per)\n# To count the survival rate per deck.\n# Every deck has different survival rates.\n# Decks B, C, D, E, and F have the highest survival rates. These decks were mostly occupied by Pclass 1.\n# M has the lowest survival rate, which was mostly occupied by 2nd and 3rd class passengers.\n# Though it might be that the cabin data of the victims in M is unretrievable, that's why it has the lowest survival rate.\n# Cabins used by 1st class passengers have higher survival rates than cabins used by 2nd and 3rd class passengers.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:57.266335Z","iopub.execute_input":"2022-05-17T12:10:57.266777Z","iopub.status.idle":"2022-05-17T12:10:57.627968Z","shell.execute_reply.started":"2022-05-17T12:10:57.266748Z","shell.execute_reply":"2022-05-17T12:10:57.627185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all['Deck'] = df_all['Deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_all['Deck'] = df_all['Deck'].replace(['D', 'E'], 'DE')\ndf_all['Deck'] = df_all['Deck'].replace(['F', 'G'], 'FG')\n\ndf_all['Deck'].value_counts()\n# I regrouped Decks 'A', 'B', and 'C' 'ABC' because all of them contain only 1st class passengers\n# Decks 'D' and 'E are regrouped as 'DE' because both of them have similar passenger class distribution and same survival rate. \n# Decks 'F' and 'G' are regrouped as 'FG' for the same reason above.\n# I didn't regroup Deck 'M' with other decks because it contains missing values that cannot possibly filled in accurately with available data and tools.\n# Deck 'M' has the lowest survival rate.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:57.629244Z","iopub.execute_input":"2022-05-17T12:10:57.629577Z","iopub.status.idle":"2022-05-17T12:10:57.643906Z","shell.execute_reply.started":"2022-05-17T12:10:57.629544Z","shell.execute_reply":"2022-05-17T12:10:57.643263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all.drop(['Cabin'], inplace=True, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:57.645018Z","iopub.execute_input":"2022-05-17T12:10:57.645713Z","iopub.status.idle":"2022-05-17T12:10:57.651599Z","shell.execute_reply.started":"2022-05-17T12:10:57.645676Z","shell.execute_reply":"2022-05-17T12:10:57.650606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_test = divide_df(df_all)\ndfs = [df_train, df_test]\n\nfor df in dfs:\n    display_missing(df)\n# Dropped 'Cabin' because 'Deck' exists already and to prevent multicollinearity.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:57.652796Z","iopub.execute_input":"2022-05-17T12:10:57.653028Z","iopub.status.idle":"2022-05-17T12:10:57.676159Z","shell.execute_reply.started":"2022-05-17T12:10:57.653001Z","shell.execute_reply":"2022-05-17T12:10:57.67555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Survival Distribution in the Training Dataset","metadata":{}},{"cell_type":"code","source":"survived = df_train['Survived'].value_counts()[1]\nnot_survived = df_train['Survived'].value_counts()[0]\nsurvived_per = survived / df_train.shape[0] * 100\nnot_survived_per = not_survived / df_train.shape[0] * 100\n\nprint('{} of {} passengers survived and it is the {:.2f}% of the training set.'.format(survived, df_train.shape[0], survived_per))\nprint('{} of {} passengers did not survive and it is the {:.2f}% of the training set.'.format(not_survived, df_train.shape[0], not_survived_per))\n\nplt.figure(figsize=(12, 10))\nsns.countplot(df_train['Survived'])\n\nplt.xlabel('Survival', size=20, labelpad=15)\nplt.ylabel('Passenger Count', size=20, labelpad=15)\nplt.xticks((0, 1), ['Not Survived ({0:.2f}%)'.format(not_survived_per), 'Survived ({0:.2f}%)'.format(survived_per)])\nplt.tick_params(axis='x', labelsize=16)\nplt.tick_params(axis='y', labelsize=16)\n\nplt.title('Training Set Survival Distribution', size=24, y=1.10)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:57.677176Z","iopub.execute_input":"2022-05-17T12:10:57.678004Z","iopub.status.idle":"2022-05-17T12:10:57.877403Z","shell.execute_reply.started":"2022-05-17T12:10:57.677956Z","shell.execute_reply":"2022-05-17T12:10:57.876766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation","metadata":{}},{"cell_type":"code","source":"df_train_corr = df_train.drop(['PassengerId'], axis=1).corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_train_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_train_corr.drop(df_train_corr.iloc[1::2].index, inplace=True)\ndf_train_corr_nd = df_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index)\n# Training Data\n\ndf_test_corr = df_test.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_test_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)\ndf_test_corr_nd = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)\n# Test Data","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:57.878297Z","iopub.execute_input":"2022-05-17T12:10:57.878884Z","iopub.status.idle":"2022-05-17T12:10:57.900102Z","shell.execute_reply.started":"2022-05-17T12:10:57.878853Z","shell.execute_reply":"2022-05-17T12:10:57.899253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = df_train_corr_nd['Correlation Coefficient'] > 0.1\ndf_train_corr_nd[corr]\n# The highest correlation among features in the training set is 0.549500 - 'Fare' and 'Pclass'.\n# There are 9 correlations in the training set that are higher than 0.1","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:57.901524Z","iopub.execute_input":"2022-05-17T12:10:57.901733Z","iopub.status.idle":"2022-05-17T12:10:57.91326Z","shell.execute_reply.started":"2022-05-17T12:10:57.901708Z","shell.execute_reply":"2022-05-17T12:10:57.91244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = df_test_corr_nd['Correlation Coefficient'] > 0.1\ndf_test_corr_nd[corr]\n# The highest correlation between features ('Pclass' and 'Fare') is 0.577 in the test set.\n# There are 6 correlations in the test set that are higher than 0.1.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:57.914625Z","iopub.execute_input":"2022-05-17T12:10:57.915024Z","iopub.status.idle":"2022-05-17T12:10:57.929119Z","shell.execute_reply.started":"2022-05-17T12:10:57.914994Z","shell.execute_reply":"2022-05-17T12:10:57.928343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Heatmap","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=2, figsize=(30, 30))\n\nsns.heatmap(df_train.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True, cmap='Blues', annot_kws={'size': 16})\nsns.heatmap(df_test.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True, cmap='Blues', annot_kws={'size': 16})\n\nfor i in range(2):    \n    axs[i].tick_params(axis='x', labelsize=16)\n    axs[i].tick_params(axis='y', labelsize=16)\n    \naxs[0].set_title('Training Set Correlations', size=24)\naxs[1].set_title('Test Set Correlations', size=24)\n\nplt.show()\n# This is a heatmap that visualizes the correlations of the features in the training and test sets.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:57.930275Z","iopub.execute_input":"2022-05-17T12:10:57.930549Z","iopub.status.idle":"2022-05-17T12:10:59.013029Z","shell.execute_reply.started":"2022-05-17T12:10:57.930508Z","shell.execute_reply":"2022-05-17T12:10:59.012213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Continuous Features","metadata":{}},{"cell_type":"code","source":"cont_features = ['Age', 'Fare']\nsurv = df_train['Survived'] == 1\n\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(25, 25))\nplt.subplots_adjust(right=1.75)\n\nfor i, feature in enumerate(cont_features):    \n    # Distribution of Survival in 'Age' and 'Fare' features\n    sns.distplot(df_train[~surv][feature], label='Not Survived', hist=True, color='#aa99aa', ax=axs[0][i])\n    sns.distplot(df_train[surv][feature], label='Survived', hist=True, color='#33cc33', ax=axs[0][i])\n    \n    # Distribution of 'Age' and 'Fare' features in the training and test sets\n    sns.distplot(df_train[feature], label='Training Set', hist=False, color='#3fbddf', ax=axs[1][i])\n    sns.distplot(df_test[feature], label='Test Set', hist=False, color='#ffcc00', ax=axs[1][i])\n    \n    axs[0][i].set_xlabel('')\n    axs[1][i].set_xlabel('')\n    \n    for j in range(2):        \n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n    \n    axs[0][i].legend(loc='upper right', prop={'size': 20})\n    axs[1][i].legend(loc='upper right', prop={'size': 20})\n    axs[0][i].set_title('Distribution of Survival in {}'.format(feature), size=20, y=1.05)\n\naxs[1][0].set_title('Distribution of {} Feature'.format('Age'), size=20, y=1.05)\naxs[1][1].set_title('Distribution of {} Feature'.format('Fare'), size=20, y=1.05)\n        \nplt.show()\n# Distribution of Age feature clearly shows that children younger than 15 has a higher survival rate than any of the other age groups\n# In distribution of Fare feature, the survival rate is higher on distribution tails. The distribution also has positive skew because of the extremely large outliers","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:10:59.014072Z","iopub.execute_input":"2022-05-17T12:10:59.014645Z","iopub.status.idle":"2022-05-17T12:11:00.267944Z","shell.execute_reply.started":"2022-05-17T12:10:59.014612Z","shell.execute_reply":"2022-05-17T12:11:00.267088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical Features","metadata":{}},{"cell_type":"code","source":"categorical_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck']\n\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20, 20))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(categorical_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.countplot(x=feature, hue='Survived', data=df_train)\n    \n    plt.xlabel('{}'.format(feature), size=20, labelpad=15)\n    plt.ylabel('Passenger Count', size=20, labelpad=15)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\n    plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)\n\nplt.show()\n# Every categorical feature has at least one class with high mortality rate. \n# Those classes are very helpful to predict whether the passenger is a survivor or victim.\n# The categorical features with the most homogenous distributions are are 'Pclass' and 'Sex'.\n# Passengers who boarded from Southampton has the lowest survival rate. More than half of the passengers boarded from Cherbourg had survived. Why? This observation could be related to 'Pclass' feature.\n# 'Parch' and 'SibSp' features show that passengers with only one family member has the highest survival rate","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:00.26915Z","iopub.execute_input":"2022-05-17T12:11:00.26941Z","iopub.status.idle":"2022-05-17T12:11:01.426774Z","shell.execute_reply.started":"2022-05-17T12:11:00.269375Z","shell.execute_reply":"2022-05-17T12:11:01.425837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checkpoint","metadata":{}},{"cell_type":"code","source":"df_all = concatenated_df(df_train, df_test)\ndf_all_i = df_all.copy()\ndfs_i = dfs.copy()\n# Most of the features are correlated with each other. This relationship can be used to create new features with feature transformation/engineering and feature interaction. \n# Created a new feature called 'Deck' and dropped 'Cabin' feature at the Exploratory Data Analysis part","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:01.428167Z","iopub.execute_input":"2022-05-17T12:11:01.428425Z","iopub.status.idle":"2022-05-17T12:11:01.440682Z","shell.execute_reply.started":"2022-05-17T12:11:01.428391Z","shell.execute_reply":"2022-05-17T12:11:01.439479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Advanced Feature Engineering","metadata":{}},{"cell_type":"code","source":"df_all_i['Fare'] = pd.qcut(df_all_i['Fare'], 13)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:01.445778Z","iopub.execute_input":"2022-05-17T12:11:01.446035Z","iopub.status.idle":"2022-05-17T12:11:01.460727Z","shell.execute_reply.started":"2022-05-17T12:11:01.446004Z","shell.execute_reply":"2022-05-17T12:11:01.459738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(24, 11))\nsns.countplot(x='Fare', hue='Survived', data=df_all_i)\n\nplt.xlabel('Fare', size=18, labelpad=20)\nplt.ylabel('Passenger Count', size=18, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Count in {} Feature'.format('Fare'), size=20, y=1.05)\n\nplt.show()\n# Fare feature is positively skewed and survival rate is extremely high on the right end. \n# 13 quantile-based bins are used for Fare feature. Even though the bins are too much, they provide decent amount of information gain.\n# There is an unusual group (15.742, 23.25] in the middle with high survival rate (survived/not survived) that is captured in this process.\n# The groups at the left side of the graph have the lowest survival rate and the groups at the right side of the graph have the highest survival rate.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:01.462108Z","iopub.execute_input":"2022-05-17T12:11:01.462879Z","iopub.status.idle":"2022-05-17T12:11:01.817996Z","shell.execute_reply.started":"2022-05-17T12:11:01.462832Z","shell.execute_reply":"2022-05-17T12:11:01.817412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all_i['Age'] = pd.qcut(df_all_i['Age'], 10)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:01.81916Z","iopub.execute_input":"2022-05-17T12:11:01.81948Z","iopub.status.idle":"2022-05-17T12:11:01.826847Z","shell.execute_reply.started":"2022-05-17T12:11:01.819451Z","shell.execute_reply":"2022-05-17T12:11:01.825723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(24, 11))\nsns.countplot(x='Age', hue='Survived', data=df_all_i)\n\nplt.xlabel('Age', size=18, labelpad=20)\nplt.ylabel('Passenger Count', size=18, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Count in {} Feature'.format('Age'), size=20, y=1.05)\n\nplt.show()\n# Age feature has a normal distribution with some spikes and bumps. \n# 10 quantile-based bins are used for Age. \n# The 1st bin has the highest survival rate while the 4th has the lowest survival rate.\n# There is also an unusual group (34.0, 40.0) with high survival rate that is captured in this process","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:01.828142Z","iopub.execute_input":"2022-05-17T12:11:01.828428Z","iopub.status.idle":"2022-05-17T12:11:02.155888Z","shell.execute_reply.started":"2022-05-17T12:11:01.828398Z","shell.execute_reply":"2022-05-17T12:11:02.155195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Frequency Encoding","metadata":{}},{"cell_type":"code","source":"df_all_i['Family_Size'] = df_all_i['SibSp'] + df_all_i['Parch'] + 1\n# Family_Size is created by adding SibSp, Parch and 1 - adding 1 at the end is the current passenger.\n\nfig, axs = plt.subplots(figsize=(24, 24), ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=df_all_i['Family_Size'].value_counts().index, y=df_all_i['Family_Size'].value_counts().values, ax=axs[0][0])\naxs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\nsns.countplot(x='Family_Size', hue='Survived', data=df_all_i, ax=axs[0][1])\naxs[0][1].set_title('Survival Counts in Family Size ', size=20, y=1.05)\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\n# Family Size with 1 are labeled as 'Alone'\n# Family Size with 2, 3 and 4 are labeled as 'Small'\n# Family Size with 5 and 6 are labeled as 'Medium'\n# Family Size with 7, 8 and 11 are labeled as 'Large'\ndf_all_i['Family_Size_Grouped'] = df_all_i['Family_Size'].map(family_map)\n\nsns.barplot(x=df_all_i['Family_Size_Grouped'].value_counts().index, y=df_all_i['Family_Size_Grouped'].value_counts().values, ax=axs[1][0])\naxs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)\nsns.countplot(x='Family_Size_Grouped', hue='Survived', data=df_all_i, ax=axs[1][1])\naxs[1][1].set_title('Survival Counts in Family Size After Grouping', size=20, y=1.05)\n\nfor i in range(2):\n    axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n        axs[i][j].set_xlabel('')\n        axs[i][j].set_ylabel('')\n\nplt.show()\n# Graphs show that family size is a predictor of survival because different values have different survival rates.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:02.157131Z","iopub.execute_input":"2022-05-17T12:11:02.157551Z","iopub.status.idle":"2022-05-17T12:11:03.161236Z","shell.execute_reply.started":"2022-05-17T12:11:02.15751Z","shell.execute_reply":"2022-05-17T12:11:03.160385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all_i['Ticket_Frequency'] = df_all_i.groupby('Ticket')['Ticket'].transform('count')\n# There are too many unique Ticket values to analyze, so grouping them up by their frequencies makes things easier.\n# 'Ticket_Frequency' is different from 'Family_Size' because many passengers travelled along with groups consisting of friends, nannies, maids, etc. - all of whom weren't counted as family but used the same ticket.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:03.162283Z","iopub.execute_input":"2022-05-17T12:11:03.162535Z","iopub.status.idle":"2022-05-17T12:11:03.170968Z","shell.execute_reply.started":"2022-05-17T12:11:03.162506Z","shell.execute_reply":"2022-05-17T12:11:03.170151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(14, 11))\nsns.countplot(x='Ticket_Frequency', hue='Survived', data=df_all_i)\n\nplt.xlabel('Ticket Frequency', size=20, labelpad=20)\nplt.ylabel('Passenger Count', size=20, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Ticket Frequency'), size=24, y=1.05)\n\nplt.show()\n# According to the graph below, groups with 2, 3, and 4 members had a higher survival rate, while those who traveled alone had the lowest survival rate. \n# After 4 group members, survival rate decreases drastically. \n# This pattern is very similar to Family_Size feature but, there are minor differences as mentioned in the previous block. \n# Ticket_Frequency values are not grouped like Family_Size because that would basically create the same feature with perfect correlation.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:03.172607Z","iopub.execute_input":"2022-05-17T12:11:03.173572Z","iopub.status.idle":"2022-05-17T12:11:03.460209Z","shell.execute_reply.started":"2022-05-17T12:11:03.173539Z","shell.execute_reply":"2022-05-17T12:11:03.459398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all_i['Title'] = df_all_i['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n# The 'Title' feature is created by extracting the prefix before the 'Name' feature.\n\ndf_all_i['Is_Married'] = 0\ndf_all_i['Is_Married'].loc[df_all_i['Title'] == 'Mrs'] = 1\n# 'Is_Married' is a binary feature based on the Mrs title. This title has the highest survival rate among other female titles and needs to be a feature because all female titles are grouped with each other.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:03.461601Z","iopub.execute_input":"2022-05-17T12:11:03.462015Z","iopub.status.idle":"2022-05-17T12:11:03.476886Z","shell.execute_reply.started":"2022-05-17T12:11:03.461983Z","shell.execute_reply":"2022-05-17T12:11:03.476276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=2, figsize=(20, 20))\nsns.barplot(x=df_all_i['Title'].value_counts().index, y=df_all_i['Title'].value_counts().values, ax=axs[0])\naxs[0].set_title('Title Feature Value Counts', size=24, y=1.05)\naxs[0].tick_params(axis='x', labelsize=10)\n\ndf_all_i['Title'] = df_all_i['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\ndf_all_i['Title'] = df_all_i['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr/Military/Noble/Clergy')\n\nsns.barplot(x=df_all_i['Title'].value_counts().index, y=df_all_i['Title'].value_counts().values, ax=axs[1])\naxs[1].set_title('Title Feature Value Counts After Grouping', size=24, y=1.05)\naxs[1].tick_params(axis='x', labelsize=15)\n\nfor i in range(2):    \n    axs[i].tick_params(axis='y', labelsize=15)\n\nplt.show()\n# According to first graph below, there are many titles that are occuring very few times. Some of those titles don't seem correct and they need to be replaced. \n# Miss, Mrs, Ms, Mlle, Lady, Mme, the Countess, Dona titles are replaced with Miss/Mrs/Ms because all of them are female. \n# Values like Mlle, Mme and Dona are actually the name of the passengers, but they were previously classified as titles because the 'Name' feature is split by comma. \n# Dr, Col, Major, Jonkheer, Capt, Sir, Don and Rev titles are replaced with Dr/Military/Noble/Clergy because those passengers have similar characteristics. \n# Master is a unique title. It is given to male passengers below age 26. They have the highest survival rate among all males.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:03.47776Z","iopub.execute_input":"2022-05-17T12:11:03.4785Z","iopub.status.idle":"2022-05-17T12:11:03.934525Z","shell.execute_reply.started":"2022-05-17T12:11:03.478465Z","shell.execute_reply":"2022-05-17T12:11:03.933662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target Encoding","metadata":{}},{"cell_type":"code","source":"def extract_surname(data):    \n    \n    families = []\n    \n    for i in range(len(data)):        \n        name = data.iloc[i]\n\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n            \n    return families\n# extract_surname function is used to extract surnames of passengers from the 'Name' feature. \n\ndf_all_i['Family'] = extract_surname(df_all_i['Name'])\ndf_train = df_all_i.loc[:890]\ndf_test = df_all_i.loc[891:]\n\ndfs_i = [df_train, df_test]\n# 'Family' feature is thus created using the extract_surname function. This feature is important in order to group passengers in the same family.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:03.935789Z","iopub.execute_input":"2022-05-17T12:11:03.936032Z","iopub.status.idle":"2022-05-17T12:11:03.974092Z","shell.execute_reply.started":"2022-05-17T12:11:03.936002Z","shell.execute_reply":"2022-05-17T12:11:03.973507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n# Created a list of families and tickets that are occurring in the training and test sets\n# A list of family names (non_unique_families), that are occurring in all sets is created.\n\ndf_family_survival_rate = df_train.groupby('Family')['Survived', 'Family','Family_Size'].median()\ndf_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n\nfamily_rates = {}\nticket_rates = {}\n\n\nfor i in range(len(df_family_survival_rate)):\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n    # Checked if a family exists in all sets, and has members more than 1\n\nfor i in range(len(df_ticket_survival_rate)):\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]\n    # Checked if a ticket exists in both training and test set, and has members more than 1","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:03.975121Z","iopub.execute_input":"2022-05-17T12:11:03.975442Z","iopub.status.idle":"2022-05-17T12:11:04.16705Z","shell.execute_reply.started":"2022-05-17T12:11:03.975415Z","shell.execute_reply":"2022-05-17T12:11:04.166197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_survival_rate = np.mean(df_train['Survived'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)        \n        \nfor i in range(len(df_test)):\n    if df_test['Family'].iloc[j] in family_rates:\n        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \ndf_train['Family_Survival_Rate'] = train_family_survival_rate\ndf_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ndf_test['Family_Survival_Rate'] = test_family_survival_rate\ndf_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n# Family_Survival_Rate is calculated from families in training set since there is no Survived feature in test set.\n# The survival rate is calculated for families with more than 1 members in that list, and stored in Family_Survival_Rate feature.\n# An extra binary feature Family_Survival_Rate_NA is created for families that are unique to the test set. \n# This extra feature is also necessary because there is no way to calculate those families' survival rate, and implies that family survival rate is not applicable to those passengers because there is no way to retrieve their survival rate.\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n# Ticket_Survival_Rate and Ticket_Survival_Rate_NA features are also created with the same method. \n\nfor i in range(len(df_train)):\n    if df_train['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \ndf_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\ndf_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ndf_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\ndf_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:04.168454Z","iopub.execute_input":"2022-05-17T12:11:04.169263Z","iopub.status.idle":"2022-05-17T12:11:04.22338Z","shell.execute_reply.started":"2022-05-17T12:11:04.169221Z","shell.execute_reply":"2022-05-17T12:11:04.222648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for df in [df_train, df_test]:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) / 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) / 2\n# Ticket_Survival_Rate and Family_Survival_Rate are averaged and become Survival_Rate.\n# Ticket_Survival_Rate_NA and Family_Survival_Rate_NA are also averaged and become Survival_Rate_NA.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:04.224477Z","iopub.execute_input":"2022-05-17T12:11:04.224688Z","iopub.status.idle":"2022-05-17T12:11:04.233714Z","shell.execute_reply.started":"2022-05-17T12:11:04.224662Z","shell.execute_reply":"2022-05-17T12:11:04.232786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Transformation","metadata":{}},{"cell_type":"code","source":"non_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']\n\nfor df in dfs_i:\n    for feature in non_numeric_features:\n        df[feature] = LabelEncoder().fit_transform(df[feature])\n# Embarked, Sex, Deck , Title and Family_Size_Grouped are object type.\n# Age and Fare features are category type. \n# LabelEncoder basically labels the classes from 0 to n, converting the aforementioned types into numerical type.\n# This process is necessary for the model to learn from these features.","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:04.234834Z","iopub.execute_input":"2022-05-17T12:11:04.235032Z","iopub.status.idle":"2022-05-17T12:11:04.265098Z","shell.execute_reply.started":"2022-05-17T12:11:04.235006Z","shell.execute_reply":"2022-05-17T12:11:04.264236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One-hot Encoding","metadata":{}},{"cell_type":"code","source":"categorical_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\nencoded_features = []\n\nfor df in dfs_i:\n    for feature in categorical_features:\n        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        n = df[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n# The categorical features (Pclass, Sex, Deck, Embarked, Title) are converted to one-hot encoded features with OneHotEncoder. \n# 'Age' and 'Fare' features are not converted because they are ordinal, unlike the other features. Label Encoding must be used for such ordinal features.\n\ndf_train = pd.concat([df_train, *encoded_features[:6]], axis=1)\ndf_test = pd.concat([df_test, *encoded_features[6:]], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:04.266601Z","iopub.execute_input":"2022-05-17T12:11:04.267297Z","iopub.status.idle":"2022-05-17T12:11:04.293938Z","shell.execute_reply.started":"2022-05-17T12:11:04.267266Z","shell.execute_reply":"2022-05-17T12:11:04.293091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all_i = concatenated_df(df_train, df_test)\ndrop_cols = ['Deck', 'Embarked', 'Family', 'Family_Size', 'Family_Size_Grouped', 'Survived',\n             'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Ticket', 'Title',\n            'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA']\n# 'Family_Size' is created by adding 'Parch' and 'SibSp' features and 1. \n# 'Ticket_Frequency' is created by counting the occurrence of Ticket values.\n# 'Name' is quite useful: First, 'Title' and 'Is_Married' features are created from the title prefix in the names.\n# Second, 'Family_Survival_Rate' and 'Family_Survival_Rate_NA' features are created by target encoding the surname of the passengers. \n# 'Ticket_Survival_Rate' is created by target encoding the Ticket feature. \n# 'Survival_Rate' feature is created by averaging the 'Family_Survival_Rate' and 'Ticket_Survival_Rate' features.\n\ndf_all_i.drop(columns=drop_cols, inplace=True)\ndf_all_i.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:04.295745Z","iopub.execute_input":"2022-05-17T12:11:04.295982Z","iopub.status.idle":"2022-05-17T12:11:04.338175Z","shell.execute_reply.started":"2022-05-17T12:11:04.295953Z","shell.execute_reply":"2022-05-17T12:11:04.337399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"X_train = StandardScaler().fit_transform(df_train.drop(columns=drop_cols))\ny_train = df_train['Survived'].values\nX_test = StandardScaler().fit_transform(df_test.drop(columns=drop_cols))\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))\nprint('X_test: {}'.format(X_test.shape))","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:04.339275Z","iopub.execute_input":"2022-05-17T12:11:04.3396Z","iopub.status.idle":"2022-05-17T12:11:04.360876Z","shell.execute_reply.started":"2022-05-17T12:11:04.339567Z","shell.execute_reply":"2022-05-17T12:11:04.360043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"single_best_model = RandomForestClassifier(criterion='gini', \n                                           n_estimators=1100,\n                                           # 1100\n                                           max_depth=5,\n                                           # 5\n                                           min_samples_split=4,\n                                           # 4\n                                           min_samples_leaf=5,\n                                           # 5\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=seed,\n                                           n_jobs=-1,\n                                           verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:11:04.361971Z","iopub.execute_input":"2022-05-17T12:11:04.362195Z","iopub.status.idle":"2022-05-17T12:11:04.367579Z","shell.execute_reply.started":"2022-05-17T12:11:04.362167Z","shell.execute_reply":"2022-05-17T12:11:04.366729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = 5\noob = 0\nprobs = pd.DataFrame(np.zeros((len(X_test), N * 2)), columns=['Fold_{}_Prob_{}'.format(i, j) for i in range(1, N + 1) for j in range(2)])\nimportances = pd.DataFrame(np.zeros((X_train.shape[1], N)), columns=['Fold_{}'.format(i) for i in range(1, N + 1)], index=df_all_i.columns)\nfprs, tprs, scores = [], [], []\n\nskf = StratifiedKFold(n_splits=N, random_state=N, shuffle=True)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n    print('Fold {}\\n'.format(fold))\n    \n    single_best_model.fit(X_train[trn_idx], y_train[trn_idx])\n\n    # Computing Train AUC score\n    trn_fpr, trn_tpr, trn_thresholds = roc_curve(y_train[trn_idx], single_best_model.predict_proba(X_train[trn_idx])[:, 1])\n    trn_auc_score = auc(trn_fpr, trn_tpr)\n      \n    scores.append(trn_auc_score)\n    \n    # X_test probabilities\n    probs.loc[:, 'Fold_{}_Prob_0'.format(fold)] = single_best_model.predict_proba(X_test)[:, 0]\n    probs.loc[:, 'Fold_{}_Prob_1'.format(fold)] = single_best_model.predict_proba(X_test)[:, 1]\n    importances.iloc[:, fold - 1] = single_best_model.feature_importances_\n        \n    oob += single_best_model.oob_score_ / N\n    print('Fold {} OOB Score: {}\\n'.format(fold, single_best_model.oob_score_))   \n    \nprint('Average OOB Score: {}'.format(oob))","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:14:31.802346Z","iopub.execute_input":"2022-05-17T12:14:31.803142Z","iopub.status.idle":"2022-05-17T12:14:56.392023Z","shell.execute_reply.started":"2022-05-17T12:14:31.803101Z","shell.execute_reply":"2022-05-17T12:14:56.391064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances['Mean_Importance'] = importances.mean(axis=1)\nimportances.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n\nplt.figure(figsize=(15, 20))\nsns.barplot(x='Mean_Importance', y=importances.index, data=importances)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.title('Random Forest Classifier Mean Feature Importance Between Folds', size=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:15:04.763911Z","iopub.execute_input":"2022-05-17T12:15:04.764888Z","iopub.status.idle":"2022-05-17T12:15:05.256601Z","shell.execute_reply.started":"2022-05-17T12:15:04.764844Z","shell.execute_reply":"2022-05-17T12:15:05.255325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc_curve(fprs, tprs):\n    \n    tprs_interp = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    f, ax = plt.subplots(figsize=(15, 15))\n    \n    # Plotting ROC for each fold and computing AUC scores\n    for i, (fpr, tpr) in enumerate(zip(fprs, tprs), 1):\n        tprs_interp.append(np.interp(mean_fpr, fpr, tpr))\n        tprs_interp[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        ax.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC Fold {} (AUC = {:.3f})'.format(i, roc_auc))\n        \n    # Plotting ROC for random guessing\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8, label='Random Guessing')\n    \n    mean_tpr = np.mean(tprs_interp, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    \n    # Plotting the mean ROC\n    ax.plot(mean_fpr, mean_tpr, color='b', label='Mean ROC (AUC = {:.3f} $\\pm$ {:.3f})'.format(mean_auc, std_auc), lw=2, alpha=0.8)\n    \n    # Plotting the standard deviation around the mean ROC Curve\n    std_tpr = np.std(tprs_interp, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='$\\pm$ 1 std. dev.')\n    \n    ax.set_xlabel('False Positive Rate', size=15, labelpad=20)\n    ax.set_ylabel('True Positive Rate', size=15, labelpad=20)\n    ax.tick_params(axis='x', labelsize=15)\n    ax.tick_params(axis='y', labelsize=15)\n    ax.set_xlim([-0.05, 1.05])\n    ax.set_ylim([-0.05, 1.05])\n\n    ax.set_title('ROC Curves of Folds', size=20, y=1.02)\n    ax.legend(loc='lower right', prop={'size': 13})\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:15:43.944446Z","iopub.execute_input":"2022-05-17T12:15:43.945293Z","iopub.status.idle":"2022-05-17T12:15:43.959519Z","shell.execute_reply.started":"2022-05-17T12:15:43.945251Z","shell.execute_reply":"2022-05-17T12:15:43.958639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_survived = [col for col in probs.columns if col.endswith('Prob_1')]\nprobs['1'] = probs[class_survived].sum(axis=1) / N\nprobs['0'] = probs.drop(columns=class_survived).sum(axis=1) / N\nprobs['pred'] = 0\npos = probs[probs['1'] >= 0.5].index\nprobs.loc[pos, 'pred'] = 1\n\ny_pred = probs['pred'].astype(int)\n\nsubmission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = df_test['PassengerId']\nsubmission_df['Survived'] = y_pred.values\nsubmission_df.to_csv('submissions.csv', header=True, index=False)\nsubmission_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T12:15:50.384875Z","iopub.execute_input":"2022-05-17T12:15:50.385186Z","iopub.status.idle":"2022-05-17T12:15:50.4143Z","shell.execute_reply.started":"2022-05-17T12:15:50.385146Z","shell.execute_reply":"2022-05-17T12:15:50.413683Z"},"trusted":true},"execution_count":null,"outputs":[]}]}