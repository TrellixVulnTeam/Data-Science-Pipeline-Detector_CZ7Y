{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTr0_0EtE1pjZl9xezJREPfPqCR0xKgHxvUGPDWPnMNh8wne_Rc)\n\n# This notebook only aims to introduce the \"pipeline\" concept. \n\n## So what is \"pipeline\" ? \n\n- Well, the pipeline is an abstract concept aiming to perform several different transformations before the final estimatior. The below is taken from [SKLEARN PIPELINE DOCUMENTATION](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n\n- The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a ‘__’, as in the example below. A step’s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting it to ‘passthrough’ or None.\n\n\n**Parameters**:\n\n- **steps**: (list) List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator.\n\n- **memory**: (None, str or object with the joblib.Memory interface, optional) Used to cache the fitted transformers of the pipeline. By default, no caching is performed. If a string is given, it is the path to the caching directory. Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. Use the attribute named_steps or steps to inspect estimators within the pipeline. Caching the transformers is advantageous when fitting is time consuming.\n\n- **verbose**: (bool, default=False) If True, the time elapsed while fitting each step will be printed as it is completed.\n\n**Attributes**:\n\n- **named_steps**: (bunch object, a dictionary with attribute access) Read-only attribute to access any step parameter by user given name. Keys are step names and values are steps parameters.\n\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## For this notebook, we will be using the Titanic Data\n\n![](https://res.cloudinary.com/dk-find-out/image/upload/q_80,w_1920,f_auto/MA_00079563_yvu84f.jpg)\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# importing the libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nfrom sklearn.pipeline      import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute        import SimpleImputer\nfrom sklearn.linear_model  import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# loading the files"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/titanic/train.csv')\ndf_test  = pd.read_csv('../input/titanic/test.csv')\ndf_sample= pd.read_csv('../input/titanic/gender_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# information about the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping unnecessary columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(['Name','Ticket','Cabin'],axis=1,inplace=True)\ndf_test.drop( ['Name','Ticket','Cabin'],axis=1,inplace=True)\n\n# We could create new features from these 3 but I aim to keep it simple and minimal","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We both have numerical and categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sex    = pd.get_dummies(df_train['Sex'],drop_first=True)\nembark = pd.get_dummies(df_train['Embarked'],drop_first=True)\n\ndf_train = pd.concat([df_train,sex,embark],axis=1)\ndf_test  = pd.concat([df_test ,sex,embark],axis=1)\n\ndf_train.drop(['Sex','Embarked'],axis=1,inplace=True)\ndf_test.drop(['Sex','Embarked'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Only numerical features left to feed our pipeline\n\n- Imputation\n- Standard Scaling\n- Predictive Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer  = SimpleImputer()\nscaler   = StandardScaler()\nclf      = LogisticRegression()\npipe     = make_pipeline(imputer,scaler,clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = df_train.drop('Survived',axis=1).columns\n\nX,y   = df_train[features], df_train['Survived']\ndf_test.fillna(df_test.mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting the pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe.fit(X,y)\ny_pred = pd.DataFrame(pipe.predict(df_test))\n\ny_pred['Survived'] = y_pred[0]\ny_pred.drop(0,axis=1,inplace=True)\ny_pred['PassengerId'] = df_test['PassengerId']\n\ny_pred.to_csv('titanic_pred_logistic.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook doesn't aim to achieve high score, it serves the very and only purpose\n\nIf you're looking to a more advanced one:\n\nhttps://www.kaggle.com/frtgnn/titanic-survival-classifier\n\nIf you're looking only to some visuals:\n\nhttps://www.kaggle.com/frtgnn/focusing-on-couple-of-visuals-using-titanic-data"},{"metadata":{},"cell_type":"markdown","source":"# thank you very much for checking\n# I try to update it frequently!\n\n![](https://natgeo.imgix.net/factsheets/thumbnails/RMSTitanic_TimelineofDisaster_Titanic.jpg?auto=compress,format&w=1600&h=900&fit=crop)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}