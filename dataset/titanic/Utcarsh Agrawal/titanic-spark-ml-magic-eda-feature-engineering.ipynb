{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Introduction**\nIn this notebook we will walkthrough detailed statistical analysis of Titanic data set along with Machine learning model implementation. This notebook will work as a tutorial for all the beginners who don't know much about sparkml as I have tried to explain each and every steps with simplicity. I have used many different types of plotting techniques so that you can understand how each and every column will affect the prediction score. Also, I will go through many different machine learning classifiers so that you get to know about it and youâ€™ll be able to solve any problem thrown your way. \n\n## <font color = 'red'> Please do an upvote if you find the kernel useful. </font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Table of Contents**\n* [Setting up the environment](#1)\n* [Importing Libraries](#2)\n* [Reading the data](#3)\n* [Exploratory Data Analysis](#4)\n* [Feature Engineering](#5)\n* [Spark ML Models](#6)\n* [Submitting the predictions](#7)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='1'></a>\n# **Setting up the environment**\nBefore starting we first have to change the java version of the notebook because if we will use version 11 which is already installed then we will get some errors and we will not be able to use pyspark properly. So we will delete java version 11 and install java version 8.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"! apt remove -y openjdk-11-jre-headless","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!apt-get update","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!apt install -y openjdk-8-jdk openjdk-8-jre","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**If you get this error \"E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/p/pulseaudio/libpulse0_11.1-1ubuntu7.5_amd64.deb  404  Not Found [IP: 91.189.88.142 80]\" while installing java version 8 then run \"!apt-get update\" this command first like I did and then install jdk8.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!java -version","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**After installing jdk8, we will now install pyspark.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='2'></a>\n# **Importing Libraries**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport re\nfrom pylab import *\nfrom pyspark.sql.functions import udf, concat, col, lit\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\n\n#Creating spark session\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\nfrom pyspark.sql import SparkSession\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()\nsqlContext = SQLContext(sc)\n\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.classification import LinearSVC\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='3'></a>\n# **Reading the data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.read_csv('../input/titanic/train.csv')\ndf2 = pd.read_csv('../input/titanic/test.csv')\nsub_df = pd.read_csv('../input/titanic/gender_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4'></a>\n# **Exploratory Data Analysis**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Let us first see how many people survived or not.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"survival = df1.groupby('Survived').count()['Name'].reset_index()\nsns.countplot(x='Survived', data=df1)\nprint(\"Number of passengers didn't Survived = {}\".format(survival['Name'][0]))\nprint(\"Number of passengers survived = {}\".format(survival['Name'][1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will observe how each factor has its impact on whether the person survived or not.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"val = ['Pclass', 'Sex', 'Embarked', 'SibSp', 'Parch']\nplt.figure(figsize=(15,15))\nplt.subplots_adjust(right=1.5)\nfor i in range(5):\n    plt.subplot(2,3,i+1), sns.countplot(x=val[i], hue='Survived', data = df1)\n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 10})\n    plt.title('Count of Survival in {} Feature'.format(val[i]), size=10, y=1.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"surv = df1['Survived'] == 1\n\nsns.distplot(df1[~surv]['Age'], label='Not Survived', hist=True, color='#e74c3c')\nsns.distplot(df1[surv]['Age'], label='Survived', hist=True, color='#2ecc71')\n\nplt.legend()\nplt.title('Distribution of Survival in Age')\n\n        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\nplt.plot(range(0,len(df1[~surv]['Fare'])), df1[~surv]['Fare'], color='blue', animated=True, linewidth=1)\nplt.plot(range(0,len(df1[surv]['Fare'])), df1[surv]['Fare'], color='red', animated=True, linewidth=1)\nplt.xlabel('PassengerID', fontsize=14)\nplt.ylabel('Fare', fontsize=14)\nplt.legend(['Not Survived', 'Survived'])\nplt.title('Distribution of Fare')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='5'></a>\n# **Feature Engineering**\nBasically, all machine learning algorithms use some input data to create outputs. This input data comprise features, which are usually in the form of structured columns. Algorithms require features with some specific characteristic to work properly. Here, the need for feature engineering arises. There maybe be many redundant features which should be eliminated. Also we can get or add new features by observing or extracting information from other features.\n\nWe will apply feature engineering steps to both our training and test data. Here we are going to concat them so that we don't have to apply each steps separately. Then later on after applying feature engineering process we will separate them.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df1,df2],ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will make a new column which will store the values of number of person in a family and another column which will tell whether the person is alone or not. Then, we will visualize it so that we can check if survival rate have anything to do with family size of the passengers.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Family'] = df['SibSp'] + df['Parch'] + 1\ndf['Alone'] = df['Family'].apply(lambda x : 0 if x>1 else 1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,ax=plt.subplots(2,2,figsize=(14,12))\nsns.barplot('Family','Survived',data=df,ax=ax[0][0])\nax[0][0].set_title('Family vs Survived')\nsns.pointplot('Family','Survived',data=df,ax=ax[0][1])\nax[0][1].set_title('Family vs Survived')\nsns.countplot('Alone',hue='Survived',data=df,ax=ax[1][0])\nax[1][0].set_title('Alone vs Survived')\nsns.pointplot('Alone','Survived',data=df,ax=ax[1][1])\nax[1][1].set_title('Alone vs Survived')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**After calculating family size, now we will go to Name column which we haven't seen yet. Although the whole name doesn't make any sense that it will affect the survival rate but the title like Mr., Mrs. etc can affect it. So we will make a new column which will store title of every name.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Title'] = df['Name'].apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Title'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, we will identify the social status of each title.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Title_Dictionary = {\n        \"Capt\":       \"Officer\",\n        \"Col\":        \"Officer\",\n        \"Major\":      \"Officer\",\n        \"Dr\":         \"Officer\",\n        \"Rev\":        \"Officer\",\n        \"Jonkheer\":   \"Royalty\",\n        \"Don\":        \"Royalty\",\n        \"Sir\" :       \"Royalty\",\n        \"Countess\":   \"Royalty\",\n        \"Dona\":       \"Royalty\",\n        \"Lady\" :      \"Royalty\",\n        \"Mme\":        \"Mrs\",\n        \"Ms\":         \"Mrs\",\n        \"Mrs\" :       \"Mrs\",\n        \"Mlle\":       \"Miss\",\n        \"Miss\" :      \"Miss\",\n        \"Mr\" :        \"Mr\",\n        \"Master\" :    \"Master\"\n                   }\n    \n# we map each title to correct category\ndf['Title'] = df['Title'].map(Title_Dictionary)\ndf['Title'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Title', hue='Survived', data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will fill the null values in the Age column. As we have observed, the Age column has many different values so we can fill it by the mean of all the ages but here I am just filling it with -0.5.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Age'] = df['Age'].fillna(-0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we have observed, the graph of Fare column shows that although Fare column have many different values but most of the values are around the median. So we will fill all the null values with the median of the Fare column. Since fare is also a continous feature, we need to convert it into ordinal value.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n#intervals to categorize\nquant = (-1, 0, 8, 15, 31, 600)\n\n#Labels without input values\nlabel_quants = ['NoInf','quart_1', 'quart_2', 'quart_3', 'quart_4']\n\n#doing the cut in fare and puting in a new column\ndf[\"Fare_cat\"] = pd.cut(df['Fare'], quant, labels=label_quants)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,ax=plt.subplots(1,2,figsize=(14,6))\nsns.barplot('Fare_cat','Survived',data=df,ax=ax[0])\nax[0].set_title('Fare_cat vs Survived')\nsns.pointplot('Fare_cat','Survived',data=df,ax=ax[1])\nax[1].set_title('Fare_cat vs Survived')\nplt.close(2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lastly we will fill all the null values in Embarked column with the value which has occured maximum in the data.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Embarked', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So as 'S' as occured most of the time so we will fill it with 'S'.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Embarked\"] = df[\"Embarked\"].fillna('S')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Also, we will drop the columns which we don't require.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Name','Ticket', 'Cabin'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now as we have applied all the feature engineering steps so now its time to separate our data back.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs = np.split(df, [len(df1)], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = dfs[0]\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = dfs[1]\ndel test['Survived']\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6'></a>\n# **Spark ML Models**\nSo now it's time to create our models. Spark ML is a package which aims to provide a uniform set of high-level APIs that help users create and tune practical machine learning pipelines. Spark ML standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow.<br> ***Here as we are using pyspark to create our models, we first have to convert our data from pandas frame to spark frame.***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = sqlContext.createDataFrame(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As some of the column contains values in string format so first we indexed them using StringIndexer. A StringIndexer will assign unique integer number to each unique string values.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"indexer = StringIndexer(inputCol='Sex',outputCol='label1')\nindexer2 = StringIndexer(inputCol='Embarked',outputCol='label2')\nindexer3 = StringIndexer(inputCol='Fare_cat',outputCol='label3')\nindexer4 = StringIndexer(inputCol='Title',outputCol='label4')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Then data is converted which are required to predict survival into vector form by using VectorAssembler as VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models.<br>Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm. It takes parameter p, which specifies the p-norm used for normalization. This normalization can help standardize your input data and improve the behavior of learning algorithms.  So then we normalize our data by using Normalizer.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vector = VectorAssembler(inputCols=['label1','Pclass','Age','label2','Family','label3','label4', 'Alone'],outputCol='features')\nnormalizer = Normalizer(inputCol='features',outputCol='features_norm', p=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Then we call our model and give the input and the output column. Here, the input column will be our normalized data and the output column is what we have to predict i.e. Survived.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lor = LogisticRegression(featuresCol='features_norm', labelCol='Survived', maxIter=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now it's time to call  pipeline. MLlib standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow. So we add all the commands which we have called till now and add them to pipeline.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline1 = Pipeline(stages=[indexer,indexer2,indexer3,indexer4,vector,normalizer,lor])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next, we will fit our pipeline and create a model. This method is called Estimator. An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data. Technically, an Estimator implements a method fit(), which accepts a DataFrame and produces a Model, which is a Transformer. For example, a learning algorithm such as LogisticRegression is an Estimator, and calling fit() trains a LogisticRegressionModel, which is a Model and hence a Transformer.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = pipeline1.fit(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finally we perform transform function which is known as transformer. A Transformer is an abstraction that includes feature transformers and learned models. Technically, a Transformer implements a method transform(), which converts one DataFrame into another, generally by appending one or more columns.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions1 = model1.transform(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For comparsion of different models we are initializing one list which will store accuracy of all the models.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using MulticlassClassificationEvaluator we will get the accuracy of our model.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eval = MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('Survived').setPredictionCol('prediction')\nprint(\"The accuracy is: \" + str(eval.evaluate(predictions1)))\naccuracy.append(eval.evaluate(predictions1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The second classification method is GBTClassifier. We have to almost repeat the same steps as we did previously and just have to change the name of model and pipeline and call the gbtclassifier and check the accuracy of the model.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gbt = GBTClassifier(featuresCol='features_norm',labelCol='Survived',maxIter=100)\npipeline2 = Pipeline(stages=[indexer,indexer2,indexer3,indexer4,vector,normalizer,gbt])\nmodel2 = pipeline2.fit(train)\npredictions2 = model2.transform(train)\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\neval = MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('Survived').setPredictionCol('prediction')\nprint(\"The accuracy is: \" + str(eval.evaluate(predictions2)))\naccuracy.append(eval.evaluate(predictions2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The third classification method is Linear Support Vector Classifier.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = LinearSVC(featuresCol='features_norm', labelCol='Survived', maxIter=10)\npipeline3 = Pipeline(stages=[indexer,indexer2,indexer3,indexer4,vector,normalizer,svc])\nmodel3 = pipeline3.fit(train)\npredictions3 = model3.transform(train)\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\neval = MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('Survived').setPredictionCol('prediction')\nprint(\"The accuracy is: \" + str(eval.evaluate(predictions3)))\naccuracy.append(eval.evaluate(predictions3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The fourth classification method is DecisionTreeClassifier.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(featuresCol='features_norm', labelCol='Survived')\npipeline4 = Pipeline(stages=[indexer,indexer2,indexer3,indexer4,vector,normalizer,dt])\nmodel4 = pipeline4.fit(train)\npredictions4 = model4.transform(train)\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\neval = MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('Survived').setPredictionCol('prediction')\nprint(\"The accuracy is: \" + str(eval.evaluate(predictions4)))\naccuracy.append(eval.evaluate(predictions4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The fifth classification method is RandomForestClassifier.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(featuresCol='features_norm', labelCol='Survived')\npipeline5 = Pipeline(stages=[indexer,indexer2,indexer3,indexer4,vector,normalizer,rfc])\nmodel5 = pipeline5.fit(train)\npredictions5 = model5.transform(train)\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\neval = MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('Survived').setPredictionCol('prediction')\nprint(\"The accuracy is: \" + str(eval.evaluate(predictions5)))\naccuracy.append(eval.evaluate(predictions5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The sixth classification method is  MultilayerPerceptronClassifier. This method is little different as here we have to provide it with layers also. For eg. here we give layers = [8, 5, 4, 2], where 8 demonstrate number of input features, 5 and 4 are genral middle layers and 2 is number of output classes. According to yor model you should define layers.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"layers = [8, 5, 4, 2]\ntrainer = MultilayerPerceptronClassifier(featuresCol='features_norm', labelCol='Survived', maxIter=100, layers=layers, blockSize=128, seed=1234)\npipeline6 = Pipeline(stages=[indexer,indexer2,indexer3,indexer4,vector,normalizer,trainer])\nmodel6 = pipeline6.fit(train)\npredictions6 = model6.transform(train)\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\neval = MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('Survived').setPredictionCol('prediction')\nprint(\"The accuracy is: \" + str(eval.evaluate(predictions6)))\naccuracy.append(eval.evaluate(predictions6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now let us compare our models through visualization.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['Logistic Regression', 'GBTClassifier', 'LinearSVC', 'DecisionTreeClassifier', 'RandomForestClassifier', 'MultilayerPerceptronClassifier']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,1, figsize=(15,10))\nsns.barplot(x=names, y=accuracy, ax=ax[0])\nsns.pointplot(x=names, y=accuracy, ax=ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we have observed that GBTClasifer method has the highest accuracy so now let us see some of the predictions through by GBTClassifer method.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions2.select(\"Survived\", \"prediction\").show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now its time to apply our trained model to our test data. We will do this by appling transform method and we will use GBTClassifier model because it has the highest accuracy.**","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"test = sqlContext.createDataFrame(test)\npredictions = model2.transform(test)\npredictions = predictions.toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model5.transform(test)\npredictions = predictions.toPandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='7'></a>\n# **Submitting the predictions**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df['Survived'] = predictions['prediction']\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So we come to an end. I hope I have tried to explain each and every thing. But if you still want to know anything do comment and I will definitely try to solve your doubt.<br> Also, if you want to know more about Spark ML and some other different techniques and methods you can view my another notebook: -** <a href = \"https://www.kaggle.com/utcarshagrawal/water-quality-prediction-using-sparkml/notebook\" class = \"btn btn-info btn-lg active\"  role = \"button\" style = \"color: white;\" data-toggle = \"popover\" title = \"Click\">Click here</a> \n### <font color = 'red'> Thanks a lot for having a look at this notebook. I would like to get an appreciation from you with an upvote. Please upvote if you liked the kernel.</font>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}