{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center><font size=\"6\"> Titanic: Detailed Feature Engineering, Encoding and Modeling</font></center></h1>\n\n<h2><center><font size=\"4\"> Titanic: Machine Learning from Disaster </font></center></h2>\n\n<img src=\"https://imagesvc.timeincapp.com/v3/foundry/image/?q=70&w=1920&url=http%3A%2F%2Fd254andzyoxz3f.cloudfront.net%2F0419160-titanic-real-time-hero.jpg\" width=\"600\"></img>\n\n<br>\n\n\nIn this kernel I am solving Titanic competition - dataset for beginners on Kaggle. \nThe solution provided in this kernel achieved 81.8% on the public leaderboard. The main objective of this kernel is to provide tutorial on feature engineering, encoding, data cleaning stretegies and basic model tuning. I hope you will like it. \n\n#### Why this kernel is useful?\n\nThis kernel describes:\n- theoretical strategies and knowledge from multiple resources and books (with provided links)\n- feature engineering and preparation for different model types  \n- how to choose best performing model   \n- different strategies for missing values imputation   \n- different strategies for feature encoding with pros and cons  \n- why leaderboard score is different from crossvalidation and how to deal with it  \n\n\n#### <font color=\"Blue\"><b> Thank you for your vote. Please, let me know if you find any misstakes in this Kernel, I will gladly correct them \n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Table of contents\n> ***\n- [Introduction](#introduction)\n- [Resources and useful links](#links)\n- [Part 1: Importing Libraries](#Importing_Libraries)\n- [Part 2: Importing Dataset](#Importing_Dataset)\n- [Part 3: Explore Dataset](#Explore_Dataset)\n- [Part 4: Explore Target Variable](#target)\n- [Part 5: Explore and Engineer Features](#features)\n    - [5.1. Filling Missing Values](#miss)\n        - [5.1.1. Missing Values: Fare and Embarked](#fare)\n        - [5.1.2. Missing Values:Age](#age)\n    - [5.2. Features with High Cardinality](#cardinal)\n        - [5.2.1. PassengerId](#pass)\n        - [5.2.2. Ticket](#ticket)\n        - [5.2.3. Cabin](#cabin)\n        - [5.2.4. Name](#name)\n    - [5.3. Features with Outliers](#outliers)\n    - [5.4. Features with Rare values](#rare)\n        - [5.4.1.Rare values: Parch and SibSp](#parch)  \n        - [5.4.2. Rare values: Pclass, Sex, Embarked](#pclass)\n    - [5.5. Predict Missing Age with KernelRidge](#miss_age) \n    - [5.6. Feature Correlation and Dependencies](#correlation)\n- [Part 6: Feature Encoding](#encoding)\n    - [6.1. One-hot Encoder](#onehot)\n    - [6.2. Label Encoder](#label)\n    - [6.3. Mean Encoder](#mean)\n    - [6.4. Frequency Encoder](#frequency)\n- [Part 7: Modeling](#modeling)\n    - [7.1. Random Forest](#RF)\n    - [7.2. Gradient Boosting](#GB)\n    - [7.3. KNN](#KNN)\n    - [7.4. SVC](#SVC)\n- [Part 8: Conclusions](#conc)    \n- [Part 9: Submission](#submission)   "},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n<a id=\"introduction\"></a>"},{"metadata":{},"cell_type":"markdown","source":"#### Objective\n\nPredict survival on Titanic \n\n#### Description of the challenge\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy."},{"metadata":{},"cell_type":"markdown","source":"## Resources and useful links\n<a id=\"links\"></a>"},{"metadata":{},"cell_type":"markdown","source":"#### Handle  Missing Data:\n1. Missing-data imputation [link](http://www.stat.columbia.edu/~gelman/arm/missing.pdf)\n2. How to Handle Missing Data [link](https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4)\n\n#### Feature Engineering:\n1. Discover Feature Engineering, How to Engineer Features and How to Get Good at It [link](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)\n2. Machine Learning with Kaggle: Feature Engineering [link](https://www.datacamp.com/community/tutorials/feature-engineering-kaggle)\n3. Categorical Data [link](https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63)\n\n#### Modeling:\n1. Top 10 Machine Learning Algorithms [link](https://www.dezyre.com/article/top-10-machine-learning-algorithms/202)\n2. Should I normalize/standardize/rescale the data [link](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html)    \n3. Efficient Back Prop [link](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n\n#### Awesome books:\n1. An Introduction to Statistical Learning [link](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf)\n2. The Elements of Statistical Learning [link](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n\n#### Kaggle Kernels:\n1. Introduction to Ensembling/Stacking in Python [link](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)\n2. A Comprehensive ML Workflow with Python [link](https://www.kaggle.com/mjbahmani/a-comprehensive-ml-workflow-with-python)\n3. How am I doing with my score? [link](https://www.kaggle.com/pliptor/how-am-i-doing-with-my-score)\n "},{"metadata":{},"cell_type":"markdown","source":"## Part 1: Importing Libraries\n<a id=\"Importing_Libraries\"></a>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression, Ridge, LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\nimport string\nimport math\nimport sys\n\n# disable warnings:\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 2: Importing Dataset\n<a id=\"Importing_Dataset\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#axtract data into Dataframe\ndata_train = pd.read_csv(\"../input/train.csv\")\ndata_test= pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Explore_Dataset\"></a>\n# Part 3: Explore Dataset"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(\"Training Data shape:\", data_train.shape)\nprint(\"Test Data shape:\", data_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Training dataset has 891 examples with 11 features and 1 target label in column 'Survived' \n- Test dataset has 418 samples with same 11 features as training set. And as expected it doesn't have survived column \n- Some columns have missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract target variable from train set\nlabel = data_train['Survived']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. There are 5 object features: Name, Sex, Ticket, Cabin and Embarked. \n    - Two of them: Cabin and Ticket are mixed type (numbers+ string)\n    - Sex is binary category\n    - Embarked is category\n    - Name is text\n\n2. There are 6 numeric features: Pclass, Age, PassengerID, SibSp, Parch, Fare. \n    - Age and Fare are continuous \n    - Pclass is ordinal category\n    - SibSp and Parch are discrete ordinal\n\n3. Target: Survived is binary categorical"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"target\"></a>\n# Part 4: Explore Target Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for unique values of labels:\nlabel.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for missing values:\nif label.isnull().sum()==0:\n    print(\"No missing values\")\nelse:\n    print(label.isnull().sum(), 'missing values found in dataset')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Historgam\nlabel.value_counts().plot.pie(autopct='%1.2f%%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Survived is binary category with 2 possible values: 1 - if person survived and 0 - if not\n- 61.6% of people did not survive in titanic catastrophy"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"features\"></a>\n# Part 5: Explore and Engineer Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check info for train and test dataset\ndata_train.info()\nprint(\"----------------------------------\")\ndata_test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There are a lot of missing values in Cabin column\n- more than 200 missing values in Age column \n- couple of missing values in Fare and Embarked columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many unique values each feature has:\nfor column in data_train.columns:\n    print(column, len(data_train[column].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Features PassengerId, Name, Ticket and Cabin are cardinal as they are categorical and have a lot of unique values\n- Age and Fare have a lot of unique values as well, but it is natural for continues numeric features\n- Sex in binary category\n- Pclass is ordinal category\n- Embarked is non-ordinal category\n- SibSp and Parch are discreat counts"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"miss\"></a>\n## 5.1: Filling Missing Values"},{"metadata":{},"cell_type":"markdown","source":"In the dataset Age, Embarked, Fare and Cabin features have missing values.  \nThere are miltiple ways to deal with missing values in dataset and chosen strategy highly depends on type feature type, amount of missing values, size of dataset and etc.  \n\nPossible solutions to deal with missing values in continues variables:\n\n1. Complete Case Analysis CCA (discard observations where any value is missing in any variable) \n    -doesn't work if there are many features missing or on small datasets\n2. Mean and median imputation (substitute missing values with mean feature value (for Gaussian features) and median (if a variable is not Gaussian) \n    - normally good strategy if data is missing completely at random\n    - but if there is a dependency between why data is missing and target it is better to additionally create a separate variable to capture NAs\n    - changes the original distribution of variable and can distort covariance with other variables --> if too much can influence prediction in a negative way\n3. Random sampling imputation (to fill NAs - take a random observation from available observation)\n    - preserves the original distribution of data, but can create randomness - treatment of the same sample differently or unfair\n4. Adding a separate variable to capture NAs \n    - useful in situations when data is not missing at random and there is some connection on why data is missing with a target or other variables\n    - expands feature space as creates an additional feature \n5. End of distribution imputation (replace NAs with values that are at the far end of the distribution)\n    - shows the importance of missingness\n    - but if missingness is not important can mask predictive power and distorts the original distribution\n6. Arbitrary value imputation (substitute with value away from other values in the variable -999, 999)\n    - captures the importance of missing value\n    - but creates outlier (it doesn't matter for trees)\n7. Build a model to impute missing values: KNN, linear regression, etc.\n    - a very good estimate of missing data\n    - but it can be a problem because prediction usually is too good and doesn't match reality\n\nPossible solutions to deal with missing values in categorical variables: \n1. CCA \n    - for pros and cons refer above\n2. Random sample imputation\n3. Adding a separate variable to capture NAs\n4. Adding a separate category to capture NAs\n\nFor more details on this topic refer to:   \n1. [Missing-data imputation](http://www.stat.columbia.edu/~gelman/arm/missing.pdf)\n2. [How to Handle Missing Data](https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"fare\"></a>\n### 5.1.1: Missing values: Fare and Embarked"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check variable Age for missing values:\nprint('Amount of missing data in Fare for train:', data_train.Fare.isnull().sum())\nprint('Amount of missing data in Fare for test:',data_test.Fare.isnull().sum())\nprint(\"--------------------------------------------------\")\n# check variable Age for missing values:\nprint('Amount of missing data in Embarked for train:',data_train.Embarked.isnull().sum())\nprint('Amount of missing data in Embarked for test:', data_test.Embarked.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Amount of missing data in both columns is insignificant. We will just fill them with most frequent value for Embarked and median value for Fare"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train['Embarked'] = data_train['Embarked'].fillna(\"S\") \ndata_test['Fare'] = data_test['Fare'].fillna(data_train['Fare'].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"age\"></a>\n### 5.1.2: Missing values: Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.Age.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maximum age is 80, and minimum age is 0.42, conseqeuntly there is no unusual values for this variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check variable Age for missing values:\nprint(data_train.Age.isnull().sum())\nprint(data_test.Age.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age feature has 177 missing values in train and 86 missing values in test set. \nIn order to understand which missing value imputation strategy to choose, we need to need to answer some questions about variable Age:  \n1. Are values in variabe Age missing at random? - if not, it is useful to have feature to capture information in which rows Age is missing\n2. We will choose 2 different strategies to capture missing values and compare model performance for them: mean/median imputation and build models to impute missing values. This 2 strategies are very different from each other:\n    - Mean/median imputation is easy and works well, however it distorts original variance of variable and covarience of variable with other features\n    - Building a model to predict values is normally a very good estimate of missing values, however, it tends to give too perfect values for real situation. It results in unrealistic prediction due to standard error is deflation.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. create feature to show rows with missing values of age:\ndata_train['Age_NA'] =np.where(data_train.Age.isnull(), 1, 0)\ndata_test['Age_NA'] =np.where(data_test.Age.isnull(), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # visualize Age_NA vs survival rate\nprint(data_train[\"Age_NA\"].value_counts())\nsns.factorplot('Age_NA','Survived', data=data_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the plot survival rate for people with missing Age is lower than for people that have age value. This information is representative as there is enough samples for both cases: 714 and 177, therefore we will keep Age_NA variable for future use. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. plot distribution of available Age vs survival rate\na = sns.FacetGrid(data_train, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data_train['Age'].max()))\na.add_legend()\n\nprint('Skew for train data:',data_train.Age.skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"data is a almost normal, we will fill missing values with mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create feature where missing age is imputed with mean of age values that are not missing\ndata_train['Age_mean'] =np.where(data_train.Age.isnull(), data_train['Age'].mean(), data_train['Age'])\ndata_test['Age_mean'] =np.where(data_test.Age.isnull(), data_test['Age'].mean(), data_test['Age'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot distribution of available Age_mean vs survival rate\na = sns.FacetGrid(data_train, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age_mean', shade= True )\na.set(xlim=(0 , data_train['Age_mean'].max()))\na.add_legend()\n\nprint('Skew for train data:',data_train.Age.skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, after we filled missing values with the mean disctribution of Age variable has changed.   \nTo avoid this phenomenon we will use prediction model to predict missing values of Age from other available variables. To constract the model we will need to use other features from dataset. As we didn't preprocessed other features yet, we will do it in section 5.5 of this chapter.  \n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"cardinal\"></a>\n## 5.2. Features with High Cardinality"},{"metadata":{},"cell_type":"markdown","source":"Features with high cardinality (a lot of categories) are very challenging for modeling and normally they are dropped prior to training. However, there can be some useful infomation hidden inside this features.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many unique values each feature has:\nfor column in data_train.columns:\n    print(column, len(data_train[column].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features PassengerId, Name, Ticket and Cabin are features with high cardinality. Normally this features can be dropped from dataset, however, sometimes they have usefull information hidden, and before dropping them, we need to extract this information.  \nHypothesis:\n1. PassengerId can be dropped as it is unique and represents ID for each passenger. Hypothesis: there is no hidden information\n2. Cabin:\n    - cabin_type (letter before number) infomation is most probably related with deck. Hypothesis: People on lower deck could potentially have less probability to survive. \n    - Cabin has also a lot of missing values. This can be also connected with survival rate. Hypothesis: people who did not survive could not provide information about cabin\n3. Ticket:\n    - ticket_type could potentantially be connected with survival rate. Hypothesis: ticket type can have hidden information about passenger status\n4. Name:\n    - title. Hypothesis: can have hidden information about passenger status. For example married women could have more chances to survive than single man\n    - family survival rate: a lot of people survived from specific family increases chances of person from same family to survive."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"pass\"></a>\n### 5.2.1. PassengerID"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = data_train.drop(['PassengerId'], axis=1)\ndata_test = data_test.drop(['PassengerId'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ticket\"></a>\n### 5.2.2. Ticket"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data_train.Ticket[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Variable in column Ticket has following structure:  \n    - ticket type : A/5, PC, etc.  Not every ticket has associated type\n    - ticket number: starting from 0 to 3101317. For most of the tickets first number is associated with Pclass, except tickets that are less than 5 digits long or tickets than have ticket type associated with them. There is not much additional information in ticket number, so it can be ommited"},{"metadata":{},"cell_type":"markdown","source":"#### Create variable ticket_type:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create function that takes ticket feature and returns list of ticket_types\ndef ticket_sep(data_ticket):\n    ticket_type = []\n\n    for i in range(len(data_ticket)):\n\n            ticket =data_ticket.iloc[i]\n\n            for c in string.punctuation:\n                ticket = ticket.replace(c,\"\")\n                splited_ticket = ticket.split(\" \")   \n            if len(splited_ticket) == 1:\n                ticket_type.append('NO')\n            else: \n                ticket_type.append(splited_ticket[0])\n    return ticket_type ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for train data create new column with ticket_type:\ndata_train[\"ticket_type\"] = ticket_sep(data_train.Ticket)\n\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for test data create new column with ticket_type:\ndata_test[\"ticket_type\"]= ticket_sep(data_test.Ticket)\n\ndata_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many samples are there for each ticket type and visualize:\nprint(data_train[\"ticket_type\"].value_counts())\nsns.factorplot('ticket_type','Survived', data=data_train,size=4,aspect=3)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that according to the plot there are too many ticket types that are not representative. It happens because of too less unique values for each type. In order to make data representative we will put all ticket types in train data that have less than 15 samples into separate type 'OTHER_T'. For test set we will also only leave ticket types that are left in training set and keep the rest under 'OTHER_T' type"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for those types that have less than 15 samples in training set, assign type to 'OTHER':\n\nfor t in data_train['ticket_type'].unique():\n    if len(data_train[data_train['ticket_type']==t]) < 15:\n        data_train.loc[data_train.ticket_type ==t, 'ticket_type'] = 'OTHER_T'\n       \n    \nfor t in data_test['ticket_type'].unique():\n    if t not in data_train['ticket_type'].unique():\n        data_test.loc[data_test.ticket_type ==t, 'ticket_type'] = 'OTHER_T'\n        \nprint(data_train['ticket_type'].unique())\nprint(data_test['ticket_type'].unique())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# visualize ticket_type vs survival rate\nprint(data_train[\"ticket_type\"].value_counts()/len(data_train))\nsns.barplot(x = 'ticket_type', y = 'Survived', data = data_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph it is visible that there is some pattern in probability of survival based on ticket type:\n- Passengers with PC ticket_type have higher probability to survive, and with A5 or SOTONOQ - lower probability to survive than average. \n- As A5 and SOTONOQ have similar distributions and are 'rare' categories, we can combine then together"},{"metadata":{"trusted":true},"cell_type":"code","source":"# where ticket_type is 'SOTONOQ' convert it to 'A5'\ndata_train[\"ticket_type\"] = np.where(data_train[\"ticket_type\"]=='SOTONOQ', 'A5', data_train[\"ticket_type\"])\ndata_test[\"ticket_type\"] = np.where(data_test[\"ticket_type\"]=='SOTONOQ', 'A5', data_test[\"ticket_type\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize ticket_type vs survival rate\nprint(data_train[\"ticket_type\"].value_counts()/len(data_train))\nsns.barplot(x = 'ticket_type', y = 'Survived', data = data_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop Ticket from dataset:\n\ndata_train = data_train.drop(['Ticket'], axis=1)\ndata_test = data_test.drop(['Ticket'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"cabin\"></a>\n### 5.2.3. Cabin"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Missing values in Train set:', data_train.Cabin.isnull().sum())\nprint('Missing values in Test set:', data_test.Cabin.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.Cabin[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Variable Cabin has a lot of missing values both in train and test set. As there are a lot of missing values it worth to separate missing values from all other cabin types and check how it relates with other variables and target.\n\n\n2. Variable in column Cabin has following structure:  \n    - deck : Letter in front of number \n    - cabin number \nDeck probably has impact on survival rate. "},{"metadata":{},"cell_type":"markdown","source":"#### Create 'cabin_type' feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create function that takes cabin from dataset and extracts cabin type for each cabin that is not missing.\n# If cabin is missing, leaves missing value:\n\ndef cabin_sep(data_cabin):\n    cabin_type = []\n\n    for i in range(len(data_cabin)):\n\n            if data_cabin.isnull()[i] == True: \n                cabin_type.append('NaN') \n            else:    \n                cabin = data_cabin[i]\n                cabin_type.append(cabin[:1]) \n            \n    return cabin_type","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply cabin sep on test and train set:\ndata_train['cabin_type'] = cabin_sep(data_train.Cabin)\ndata_test['cabin_type'] = cabin_sep(data_test.Cabin)\n\n\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize cabin_type vs survival rate:\nprint(data_train[\"cabin_type\"].value_counts())\nsns.factorplot('cabin_type','Survived', data=data_train,size=4,aspect=3)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to make data representative we will put all cabin types in train data that have less tan 15 samples into separate type 'OTHER_C'. For test set we will also only leave cabin types that are left in training set and keep the rest under 'OTHER_C' type"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for those types that have less than 15 samples in training set, assign type to 'OTHER_C':\n\nfor t in data_train['cabin_type'].unique():\n    if len(data_train[data_train['cabin_type']==t]) <= 15:\n        data_train.loc[data_train.cabin_type ==t, 'cabin_type'] = 'OTHER_C'\n       \n    \nfor t in data_test['cabin_type'].unique():\n    if t not in data_train['cabin_type'].unique():\n        data_test.loc[data_test.cabin_type ==t, 'cabin_type'] = 'OTHER_C'\n        \nprint(data_train['cabin_type'].unique())\nprint(data_test['cabin_type'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize cabin_type vs survival rate\nprint(data_train[\"cabin_type\"].value_counts()/len(data_train))\nsns.barplot(x = 'cabin_type', y = 'Survived', data = data_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop cabin from dataset:\n\ndata_train = data_train.drop(['Cabin'], axis=1)\ndata_test = data_test.drop(['Cabin'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"name\"></a>\n### 5.2.4. Name"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.Name[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature 'Name' has following structure:   \nFamily name--> \",\" --> Title --> Name --> \"(\" --> some name in bracket.  \nWe will separate this into new features and drop Name as it is high cardinal feature.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create function that take name and separates it into title, family name and deletes all puntuation from name column:\ndef name_sep(data):\n    families=[]\n    titles = []\n    new_name = []\n    #for each row in dataset:\n    for i in range(len(data)):\n        name = data.iloc[i]\n        # extract name inside brakets into name_bracket:\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(\",\")[0]\n        title = name_no_bracket.split(\",\")[1].strip().split(\" \")[0]\n        \n        #remove punctuations accept brackets:\n        for c in string.punctuation:\n            name = name.replace(c,\"\").strip()\n            family = family.replace(c,\"\").strip()\n            title = title.replace(c,\"\").strip()\n            \n        families.append(family)\n        titles.append(title)\n        new_name.append(name)\n            \n    return families, titles, new_name    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply name_sep on train and test set:\ndata_train['family'], data_train['title'], data_train['Name']  = name_sep(data_train.Name)\ndata_test['family'], data_test['title'], data_test['Name'] = name_sep(data_test.Name)\n\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Title"},{"metadata":{},"cell_type":"markdown","source":"Hypothesis: There is some correlation in title with survival rate"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# check how many samples we have for each title and visualize vs survival rate:\nprint(data_train[\"title\"].value_counts())\nsns.factorplot('title','Survived', data=data_train,size=4,aspect=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to make data representative we will put all title in train data that have less than 15 samples into separate type 'OTHER'. For test set we will also only leave titles that are left in training set and keep the rest under 'OTHER' type"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for those types that have less than 15 samples in training set, assign type to 'OTHER':\n\nfor t in data_train['title'].unique():\n    if len(data_train[data_train['title']==t]) <= 15:\n        data_train.loc[data_train.title ==t, 'title'] = 'OTHER'\n       \n    \nfor t in data_test['title'].unique():\n    if t not in data_train['title'].unique():\n        data_test.loc[data_test.title ==t, 'title'] = 'OTHER'\n        \nprint(data_train['title'].unique())\nprint(data_test['title'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize title vs survival rate:\nsns.barplot(x = 'title', y = 'Survived', data = data_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the plot there is definatelly correlation between survival rate and title. Most of this correlation comes from gender, however, gender information provides less information by itself. For example Mrs and Miss have the same gender, but it is visible that Mrs has higher survival rate than Miss. It is very logical, as Mrs probably had priorities to board surviving boats because they had children with them."},{"metadata":{},"cell_type":"markdown","source":"#### Family:"},{"metadata":{},"cell_type":"markdown","source":"We can see that there are a lot of family names in dataset. Before proceeding with preprocessing of this variable lets check how useful it is. First we will check if there are overlapping family names in train and test set, and if there are unique family names only for train or only for test set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# amount of overlapping family names in train and test set:\nlen([x for x in data_train.family.unique() if x in data_test.family.unique()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# amount of non overlapping with test set unique family names in train set:\nlen([x for x in data_train.family.unique() if x not in data_test.family.unique()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# amount of non overlapping with train set unique family names in test set:\nlen([x for x in data_test.family.unique() if x not in data_train.family.unique()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. There are 144 overlapping family names in train and test set\n2. There are 523 family names that are unique in train seat that not overlapping with test set\n3. there are 208 family names that are unique in test set and not overlapping with train set\n\nThis kind of variable is introducing noise rather than information and in machine learning model using this variable may tend to overfit. However, if survival rate for certain family is high in training set, it can influence probability of survival for person belonging to this family from test set. \nLet's look  at 144 families that overlap:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a list with all overlapping families\noverlap = [x for x in data_train.family.unique() if x in data_test.family.unique()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# introduce new column to data called family_size:\ndata_train['family_size'] = data_train.SibSp + data_train.Parch +1\ndata_test['family_size'] = data_test.SibSp + data_test.Parch +1\n\n# calculate survival rate for each family in train_set:\nrate_family = data_train.groupby('family')['Survived', 'family','family_size'].median()\nrate_family.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if family size is more than 1 and family name is in overlap list \noverlap_family ={}\nfor i in range(len(rate_family)):\n    if rate_family.index[i] in overlap and  rate_family.iloc[i,1] > 1:\n        overlap_family[rate_family.index[i]] = rate_family.iloc[i,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For all family names in test and train set that are not in overlap_family dictionary, we will keep family survival rate as train dataset mean survival rate, and for those families that are in dataset we will set family survival rate as the one we have calculated in overlap_family dictionary. Also to signalize that dataset is not overlapping we will make a separate varibale 'Family_survival_rate_NA'"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_survival_rate = np.mean(data_train.Survived)\nfamily_survival_rate = []\nfamily_survival_rate_NA = []\n\nfor i in range(len(data_train)):\n    if data_train.family[i] in overlap_family:\n        family_survival_rate.append(overlap_family[data_train.family[i]])\n        family_survival_rate_NA.append(1)\n    else:\n        family_survival_rate.append(mean_survival_rate)\n        family_survival_rate_NA.append(0)\n        \ndata_train['family_survival_rate']= family_survival_rate\ndata_train['family_survival_rate_NA']= family_survival_rate_NA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# repeat the same for test set:\nmean_survival_rate = np.mean(data_train.Survived)\nfamily_survival_rate = []\nfamily_survival_rate_NA = []\n\nfor i in range(len(data_test)):\n    if data_test.family[i] in overlap_family:\n        family_survival_rate.append(overlap_family[data_test.family[i]])\n        family_survival_rate_NA.append(1)\n    else:\n        family_survival_rate.append(mean_survival_rate)\n        family_survival_rate_NA.append(0)\ndata_test['family_survival_rate']= family_survival_rate\ndata_test['family_survival_rate_NA']= family_survival_rate_NA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop name and family from dataset:\ndata_train = data_train.drop(['Name', 'family'], axis=1)\ndata_test = data_test.drop(['Name', 'family'], axis=1)\n\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"outliers\"></a>\n## 5.3. Features with Outliers"},{"metadata":{},"cell_type":"markdown","source":"Outliers can shift decision boundry for linear models significanlty, thats why is it inportant to handle them. Tree models are not sensitive for ourliers, but shifting outliers will not effect them in any way, so we will perform shifting for all models.  \nThere are only 3 features that can potentially have outliers: Fare, Age and Age_mean. We will make box plots for Age and Fare to check if they have outliers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_train.Age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_train.Age_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_train.Fare)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All features have outliers. It can be dangerous for linear models.  \nThe way to handle ourliers depends on distribution type. If disctribution is close to normal --> bound values within 3*std range, but if disctribution is skewed --> bound between 3*IQR range. \n\nWe already know that Age is close to normal. We will check Fare and Age_mean:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Skew for Fare:',data_train.Fare.skew())\nprint('Skew for Age_mean:',data_train.Fare.skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fare and Age_mean arde highly skewed. We will use IQR for both variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate upper bound for Fair\nIQR = data_train.Fare.quantile(0.75) - data_train.Fare.quantile(0.25)\nupper_bound = data_train.Fare.quantile(0.75) + 3*IQR\n# for train and test sets convert all values in column Fair where age is more than upper_bound to upper_bound:\ndata_train.loc[data_train.Fare >upper_bound, 'Fare'] = upper_bound \ndata_test.loc[data_test.Fare >upper_bound, 'Fare'] = upper_bound\n\nmax(data_train.Fare)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate upper bound for Age_mean\nIQR = data_train.Age_mean.quantile(0.75) - data_train.Age_mean.quantile(0.25)\nupper_bound = data_train.Age_mean.quantile(0.75) + 3*IQR\n# for train and test sets convert all values in column Fair where age is more than upper_bound to upper_bound:\ndata_train.loc[data_train.Age_mean >upper_bound, 'Age_mean'] = upper_bound \ndata_test.loc[data_test.Age_mean >upper_bound, 'Age_mean'] = upper_bound\n\nmax(data_train.Age_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate upper bound for Age:\nupper_bound = data_train.Age.mean() + 3* data_train.Age.std()\n# for train and test sets convert all values in column Fair where age is more than upper_bound to upper_bound:\ndata_train.loc[data_train.Age >upper_bound, 'Age'] = upper_bound \ndata_test.loc[data_test.Age >upper_bound, 'Age'] = upper_bound\n\nmax(data_train.Age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"rare\"></a>\n## 5.4. Features with Rare values"},{"metadata":{},"cell_type":"markdown","source":"Rare values in categorical variables tend to overfit models, especially it is true for tree models. A big number of infrequent labels adds noise with little info. Moreover there are values that can only appear in train or in test set. If category appears only in test set, tree model will not know what to do with this category, as it was not trained to handle it. \n\nWe dropped high cardinal features already, so the only features we are interested in are Pclass, Sex, SibSp, Parch, and Embarked"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. check if all values from test set are in train set \ncolumns = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n\nfor column in columns:\n    print(column)\n    print(data_train[column].unique())\n    print(data_test[column].unique())\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"parch\"></a>\n### 5.4.1. Rare values: Parch and SibSp"},{"metadata":{},"cell_type":"markdown","source":"We can see that there is one unique value of 9 in test set for Parch. We will create separate variable family_size in order to overcome this issue.    \nHypothesis: family size is more representative feature vs survival rate, rather then by Parch and SibSp separatelly. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if all values from test set are in train set for family_size:\nprint(data_train['family_size'].unique())\nprint(data_test['family_size'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize SibSp, Parch and family size vs survival rate:\nsns.factorplot('SibSp','Survived', data=data_train)\nsns.factorplot('Parch','Survived', data=data_train)\nsns.factorplot('family_size','Survived', data=data_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Indeed, according to plots we can see that combining SibSp and Parch features together is better than to have them separatelly. Family size has much less variability, but still maintains proper correlation with survival rate "},{"metadata":{"trusted":true},"cell_type":"code","source":"# check family size for rare lables:\nprint(data_train[\"family_size\"].value_counts()/len(data_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"pclass\"></a>\n### 5.4.2. Rare values: Pclass, Sex, Embarked"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Pclass')\nprint(data_train[\"Pclass\"].value_counts()/len(data_train))\nprint(data_test[\"Pclass\"].value_counts()/len(data_train))\nprint(\"------------------------------\")\n\nprint('Sex')\nprint(data_train[\"Sex\"].value_counts()/len(data_train))\nprint(data_test[\"Sex\"].value_counts()/len(data_train))\nprint(\"------------------------------\")\n\nprint('Embarked')\nprint(data_train[\"Embarked\"].value_counts()/len(data_train))\nprint(data_test[\"Embarked\"].value_counts()/len(data_train))\nprint(\"------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No rare labels in Embarked, Sex and Pclass categories"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"miss_age\"></a>\n## 5.5. Predict Missing Age with KernelRidge"},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine train and test dataset\ndata = pd.concat([data_train.drop(['Survived'], axis=1), data_test], axis =0, sort = False)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode variables into numeric labels\nle = LabelEncoder()\n\ncolumns = ['Sex', 'Embarked', 'ticket_type', 'cabin_type', 'title']\n\nfor col in columns:\n    le.fit(data[col])\n    data[col] = le.transform(data[col])\n    \ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns that have information about age or are strongly correlated with other features\ndata = data.drop(['Age_mean', 'Age_NA'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(data.Age.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data, x_vars= ['Pclass', 'Sex','Fare','Embarked','ticket_type','cabin_type',\\\n                            'title', 'family_survival_rate'], y_vars='Age', size = 5, kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,8))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(data.corr(),linewidths=0.1,vmax=1.0, \n            square=True,linecolor='white',cmap=colormap, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_age = data.dropna().drop(['Age'], axis =1)\ny_train_age = data.dropna()['Age']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_age = data[pd.isnull(data.Age)].drop(['Age'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"parameters = {'gamma' : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n              'kernel': ['rbf', 'linear'],\n               'alpha' :[0.001, 0.01, 0.1, 1, 10, 100, 1000],\n              \n             }\n\nmodel_lin = KernelRidge()\nsearch_lin = GridSearchCV(model_lin, parameters, n_jobs = -1, cv = 10, scoring = 'r2',verbose=1)\nsearch_lin.fit(x_train_age, y_train_age)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lin = make_pipeline(StandardScaler(),KernelRidge())\nkfold = model_selection.KFold(n_splits=10, random_state=4, shuffle = True)\n#model_lin.get_params().keys()\nparameters = {'kernelridge__gamma' : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n              'kernelridge__kernel': ['rbf', 'linear'],\n               'kernelridge__alpha' :[0.001, 0.01, 0.1, 1, 10, 100, 1000],\n              \n             }\nsearch_lin = GridSearchCV(model_lin, parameters, n_jobs = -1, cv = kfold, scoring = 'r2',verbose=1)\nsearch_lin.fit(x_train_age, y_train_age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best parameters are:\", search_lin.best_params_)\nprint(\"Best accuracy achieved:\",search_lin.cv_results_['mean_test_score'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_age = search_lin.predict(x_test_age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['Age'].isnull(), 'Age'] = y_test_age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = int(data_train.shape[0])\ndata_train['Age'] = data.iloc[:idx].Age\ndata_test['Age'] = data.iloc[idx:].Age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot distribution of available Age vs survival rate\na = sns.FacetGrid(data_train, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data_train['Age'].max()))\na.add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the distribution of Age values filled with prediction model looks more similar to original disctribution than when we filled missing values with mean"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"correlation\"></a>\n## 5.6. Feature Correlation and Dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before constructing correlation matrix, we need to convert categorical features to number. We will do it using label encoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode 'cabin_type' into numeric labels\nle = LabelEncoder()\ndata_train_LE = data_train.copy()\ndata_test_LE = data_test.copy()\n\ncolumns = ['Sex', 'Embarked', 'ticket_type', 'cabin_type', 'title']\n\nfor col in columns:\n    le.fit(data_train_LE[col])\n    data_train_LE[col] = le.fit_transform(data_train_LE[col])\n    data_test_LE[col] = le.transform(data_test_LE[col])\n    \ndata_train_LE.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(data_train_LE.corr(),linewidths=0.1,vmax=1.0, \n            square=True,linecolor='white',cmap=colormap, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to correlation matrix:\n1. As expected features Age and Age_mean are highly correlated between each other. We will drop Age as is has less correlation with Survived\n2. SibSp and Parch are correlataed with family size, we will drop SipSp and Parch as discussed in the chapter 5.4.1 "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_LE.columns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for col in ['Pclass', 'Sex', 'SibSp', 'Parch','Embarked', 'Age_NA', 'ticket_type', 'cabin_type', 'title',\n       'family_size', 'family_survival_rate', 'family_survival_rate_NA']:\n    sns.barplot(x = col, y = 'Survived', data = data_train_LE)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_col = ['Age_mean', 'SibSp', 'Parch']\ndata_train_LE = data_train_LE.drop(drop_col, axis=1)\ndata_test_LE = data_test_LE.drop(drop_col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,10))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(data_train_LE.corr(),linewidths=0.1,vmax=1.0, \n            square=True,linecolor='white',cmap=colormap, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,10))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(data_test_LE.corr(),linewidths=0.1,vmax=1.0, \n            square=True,linecolor='white',cmap=colormap, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"encoding\"></a>\n# Part 6: Feature Encoding "},{"metadata":{},"cell_type":"markdown","source":"Performance of particular encoding strategy for categorical feratures depends on amount of instances and features in data set, variable type and chosen model.\n\nThere are multiple ways to perform encoding:\n\n1. One-hot encoding:  \nkeeps all information about variable, but create a lot of new features  \n\n2. Binary encoding:  \nkeeps all information about variable, creates new dimensions, but less than one-hot encoder        \n             \n3. Ordinal numbering encoding:    \nkeeps semantical info about variable, but treats 2-1 as 3-2 and can be missleading, doesn't create new dimensions  \n\n4. Frequency encoding:  \ndoesn't create new dimensions, but depends on frequency rather than relation with target and that can lead to wrong predictions  \n\n5. Target guided encoding: ordinal, mean, and probability ratio encoding, Weight of evidence:  \ncreates monotonic relationship between variable and target, but can cause overfitting   \n\nAll of this models have advantages and disadvantagies and performance of particular encoding strategy for categorical feratures depends on amount of instances and features in data set, variable type and chosen prediction model. For more information about this encoding strategies there are multiple research papers and useful tutorial for each of this strategies.\n\nWe will try 4 encoding strategies and  compare performance for each of them."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"onehot\"></a>\n## 6.1. One-hot Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_onehot = data_train.drop(drop_col, axis=1)\nX_test_onehot = data_test.drop(drop_col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train_onehot.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['cabin_type', 'title',  'Sex', 'Embarked', 'ticket_type', 'Pclass']\n\nfor col in columns:\n    #X_train = pd.concat([X_train, pd.get_dummies(data_train[col])], axis =1)\n    #X_test = pd.concat([X_test, pd.get_dummies(data_test[col])], axis =1)\n    X_train_onehot = pd.concat([X_train_onehot, pd.get_dummies(X_train_onehot[col], drop_first = True)], axis =1)\n    X_test_onehot = pd.concat([X_test_onehot, pd.get_dummies(X_test_onehot[col], drop_first = True)], axis =1)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_onehot = X_train_onehot.drop(columns, axis=1)\nX_test_onehot = X_test_onehot.drop(columns, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train_onehot.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.clustermap(X_train_onehot.corr(),linewidths=0.1,vmax=1.0, \n            square=True,linecolor='white',cmap=colormap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"label\"></a>\n## 6.2. Label Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lab = data_train.drop(drop_col, axis=1)\nX_test_lab = data_test.drop(drop_col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lab.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode 'cabin_type' into numeric labels\nle = LabelEncoder()\ncolumns = ['Sex', 'Embarked', 'ticket_type', 'cabin_type', 'title']\n\nfor col in columns:\n    le.fit(data_train[col])\n    X_train_lab[col] = le.transform(X_train_lab[col])\n    X_test_lab[col] = le.transform(X_test_lab[col])\n    \nX_test_lab.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.clustermap(X_train_lab.corr(),linewidths=0.1,vmax=1.0, \n            square=True,linecolor='white',cmap=colormap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"mean\"></a>\n## 6.3. Mean Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_mean = data_train.drop(drop_col, axis=1)\nX_test_mean = data_test.drop(drop_col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_train_mean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['cabin_type', 'title',  'Sex', 'Embarked', 'ticket_type']\n\nfor col in columns:\n    ordered_labels = X_train_mean.groupby([col])['Survived'].mean().to_dict()\n    X_train_mean[col] = X_train_mean[col].map(ordered_labels)\n    X_test_mean[col] = X_test_mean[col].map(ordered_labels)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_train_mean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.clustermap(X_train_mean.corr(),linewidths=0.1,vmax=1.0, \n            square=True,linecolor='white',cmap=colormap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"frequency\"></a>\n## 6.4. Frequency encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_freq = data_train.drop(drop_col, axis=1)\nX_test_freq = data_test.drop(drop_col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_train_freq.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['cabin_type', 'title',  'Sex', 'Embarked', 'ticket_type']\n\nfor col in columns:\n    ordered_labels = X_train_freq[col].value_counts().to_dict()\n    X_train_freq[col] = X_train_freq[col].map(ordered_labels)\n    X_test_freq[col] = X_test_freq[col].map(ordered_labels)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_train_freq.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.clustermap(X_train_freq.corr(),linewidths=0.1,vmax=1.0, \n            square=True,linecolor='white',cmap=colormap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"model\"></a>\n# Part 7: Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#separate dataset into X_train and Y_train:\ndef separate(X_train):\n    X = X_train.drop(columns= ['Survived'])\n    Y = X_train['Survived']\n    return X, Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_onehot, Y_onehot  = separate(X_train_onehot)\nX_lab, Y_lab  = separate(X_train_lab)\nX_mean, Y_mean  = separate(X_train_mean)\nX_freq, Y_freq  = separate(X_train_freq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modeling step Test differents algorithms \nrandom_state = 4\nclassifiers = []\n\nclassifiers.append(('SVC', make_pipeline(StandardScaler(),SVC(random_state=random_state))))\nclassifiers.append(('DecisionTree', DecisionTreeClassifier(random_state=random_state)))\nclassifiers.append(('AdaBoost', AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),\\\n                                                  random_state=random_state,learning_rate=0.1)))\nclassifiers.append(('RandomForest', RandomForestClassifier(random_state=random_state)))\nclassifiers.append(('GradientBoost', GradientBoostingClassifier(random_state=random_state)))\nclassifiers.append(('MPL', make_pipeline(StandardScaler(), MLPClassifier(random_state=random_state))))\nclassifiers.append(('KNN',make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=7))))\n\n# evaluate each model \nresults = []\nnames = []\nfor name, classifier in classifiers:\n    kfold = model_selection.KFold(n_splits= 3, random_state=random_state, shuffle = True)\n    cv_results = model_selection.cross_val_score(classifier, X_onehot, y = Y_onehot, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For further modeling we will chose four best performing models: SVC, Random Forest, Gradient Boost, and KNN"},{"metadata":{},"cell_type":"markdown","source":"#### <font color=\"Blue\"><b>Important Information \n\nAfter submitting first results to Kaggle I discovered that cross validation score is very different from results on public board. This is a known issue with Titanic dataset and there is a lot of information available to understand why it happens and how test set is divided into public and private. You can find more information on Titanic forum or here [link](https://www.kaggle.com/pliptor/how-am-i-doing-with-my-score)\n\nTo overcome this issue we will use public leaderboard score as additional validation score. We will submit results for each model + encoder pair and choose five best performing models to improve final result. This will provide better score on public and private leaderboards.\n\nThis strategy is not advisable for other competitions though, as result on private leaderboard can differ from results on public leaderboard. The general advice for future : trust your cross validation."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"RF\"></a>\n## 7.1. Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_forest(X, Y, X_test):\n    parameters = {'max_depth' : [2, 4, 5, 10], \n                  'n_estimators' : [200, 500, 1000, 2000], \n                  'min_samples_split' : [3, 4, 5], \n\n                 }\n    kfold = model_selection.KFold(n_splits=3, random_state=random_state, shuffle = True)\n    model_RFC = RandomForestClassifier(random_state = 4, n_jobs = -1)\n    search_RFC = GridSearchCV(model_RFC, parameters, n_jobs = -1, cv = kfold, scoring = 'accuracy',verbose=1)\n    search_RFC.fit(X, Y)\n    predicted= search_RFC.predict(X_test)\n    \n    print(\"Best parameters are:\", search_RFC.best_params_)\n    print(\"Best accuracy achieved:\",search_RFC.best_score_)\n    \n    return search_RFC.best_params_, model_RFC, search_RFC, predicted","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"param_RFC_onehot, model_RFC_onehot, search_RFC_onehot, predicted_cv_RFC_onehot = random_forest(X_onehot, Y_onehot, X_test_onehot)\nparam_RFC_lab, model_RFC_lab, search_RFC_lab, predicted_cv_RFC_lab = random_forest(X_lab, Y_lab, X_test_lab)\nparam_RFC_mean, model_RFC_mean, search_RFC_mean, predicted_cv_RFC_mean = random_forest(X_mean, Y_mean,  X_test_mean)\nparam_RFC_freq, model_RFC_freq, search_RFC_freq, predicted_cv_RFC_freq = random_forest(X_freq, Y_freq, X_test_freq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After submiting each of this solution to Kaggle, discovered that cross validation scores are higher than on public leaderboard, meaning we are overfitting to training set. To make generalization better --> decrease max_depth and increased n_estimators to make model more general."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_pred_RF(X, Y, X_test):\n\n    model_RFC = RandomForestClassifier(max_depth =2,  min_samples_split =3, n_estimators = 5000,\n                                     random_state = 4, n_jobs = -1)\n    model_RFC.fit(X, Y)\n    \n    predicted= model_RFC.predict(X_test)\n    \n    return predicted, model_RFC\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict lables for all encoding strategies:\npredicted_RFC_onehot, model_RFC_onehot = fit_pred_RF(X_onehot, Y_onehot, X_test_onehot)\npredicted_RFC_lab, model_RFC_lab = fit_pred_RF(X_lab, Y_lab, X_test_lab)\npredicted_RFC_mean, model_RFC_mean = fit_pred_RF(X_mean, Y_mean, X_test_mean)\npredicted_RFC_freq, model_RFC_freq = fit_pred_RF(X_freq, Y_freq, X_test_freq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(model_RFC_onehot.feature_importances_,X_onehot.columns).sort_values(ascending=True).plot.barh(width=0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"GB\"></a>\n## 7.2. Gradient Boosting "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def grad_boost(X, Y, X_test):\n\n    parameters = {'max_depth' : [2, 4, 10, 15], \n                  'n_estimators' : [10, 50, 100], \n                  'min_samples_split' : [5, 10, 15],\n                 }\n    kfold = model_selection.KFold(n_splits=3, random_state=random_state, shuffle = True)\n    model_GBC = GradientBoostingClassifier(random_state = 4)\n    search_GBC = GridSearchCV(model_GBC, parameters, n_jobs = -1, cv = kfold, scoring = 'accuracy',verbose=1)\n    search_GBC.fit(X, Y)\n    predicted= search_GBC.predict(X_test)\n    \n    print(\"Best parameters are:\", search_GBC.best_params_)\n    print(\"Best accuracy achieved:\",search_GBC.cv_results_['mean_test_score'].mean())\n    \n    return search_GBC.best_params_, model_GBC, search_GBC, predicted\n    ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"param_GBC_onehot, model_GBC_onehot, search_GBC_onehot, predicted_cv_GBC_onehot = grad_boost(X_onehot, Y_onehot, X_test_onehot)\nparam_GBC_lab, model_GBC_lab, search_GBC_lab, predicted_cv_GBC_lab = grad_boost(X_lab, Y_lab, X_test_lab)\nparam_GBC_mean, model_GBC_mean, search_GBC_mean, predicted_cv_GBC_mean = grad_boost(X_mean, Y_mean, X_test_mean)\nparam_GBC_freq, model_GBC_freq, search_GBC_freq, predicted_cv_GBC_freq = grad_boost(X_freq, Y_freq, X_test_freq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Optimal parameters found during cross validation are lower than on public leaderboard, meaning we are overfitting to training set--> decrease max_depth and decrease n_estimators to make model more general."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_pred_GBC(X, Y, X_test):\n\n    model_GBC = GradientBoostingClassifier(max_depth = 2, min_samples_split = 15, n_estimators = 10,\\\n                                 random_state = 4, max_features= 'auto')\n    model_GBC.fit(X, Y)\n    \n    predicted= model_GBC.predict(X_test)\n    \n    return predicted, model_GBC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict lables for all encoding strategies:\npredicted_GBC_onehot, model_GBC_onehot = fit_pred_GBC(X_onehot, Y_onehot, X_test_onehot)\npredicted_GBC_lab, model_GBC_lab = fit_pred_GBC(X_lab, Y_lab, X_test_lab)\npredicted_GBC_mean, model_GBC_mean = fit_pred_GBC(X_mean, Y_mean, X_test_mean)\npredicted_GBC_freq, model_GBC_freq = fit_pred_GBC(X_freq, Y_freq, X_test_freq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"KNN\"></a>\n## 7.3. KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mod_KNN(X, Y, X_test):\n    \n    model_KNN=make_pipeline(MinMaxScaler(),KNeighborsClassifier())\n    #KNN.get_params().keys()\n    kfold = model_selection.KFold(n_splits=3, random_state=random_state, shuffle = True)\n    parameters=[{'kneighborsclassifier__n_neighbors': [2,3,4,5,6,7,8,9,10]}]\n    search_KNN = GridSearchCV(estimator=model_KNN, param_grid=parameters, scoring='accuracy', cv=kfold)\n    scores_KNN=cross_val_score(search_KNN, X, Y,scoring='accuracy', cv=kfold, verbose=1)\n    search_KNN.fit(X, Y)\n    predicted= search_KNN.predict(X_test)\n    \n    print(\"Best parameters are:\", search_KNN.best_params_)\n    print(\"Best accuracy achieved:\",search_KNN.cv_results_['mean_test_score'].mean())\n    \n    return search_KNN.best_params_, model_KNN, search_KNN, predicted\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"param_KNN_onehot, model_KNN_onehot, search_KNN_onehot, predicted_cv_KNN_onehot = mod_KNN(X_onehot, Y_onehot, X_test_onehot)\nparam_KNN_lab, model_KNN_lab, search_KNN_lab, predicted_cv_KNN_lab = mod_KNN(X_lab, Y_lab, X_test_lab)\nparam_KNN_mean, model_KNN_mean, search_KNN_mean, predicted_cv_KNN_mean = mod_KNN(X_mean, Y_mean, X_test_mean)\nparam_KNN_freq, model_KNN_freq, search_KNN_freq, predicted_cv_KNN_freq = mod_KNN(X_freq, Y_freq, X_test_freq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Optimal parameters found during cross validation are lower than on public leaderboard, meaning we are overfitting to training set--> increase n-neighbors."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_pred_KNN(X, Y, X_test):\n\n    model_KNN = make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=11))\n    \n    model_KNN.fit(X, Y)\n    \n    predicted= model_KNN.predict(X_test)\n    \n    return predicted, model_KNN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict lables for all encoding strategies:\npredicted_KNN_onehot, model_KNN_onehot = fit_pred_KNN(X_onehot, Y_onehot, X_test_onehot)\npredicted_KNN_lab, model_KNN_lab = fit_pred_KNN(X_lab, Y_lab, X_test_lab)\npredicted_KNN_mean, model_KNN_mean = fit_pred_KNN(X_mean, Y_mean, X_test_mean)\npredicted_KNN_freq, model_KNN_freq = fit_pred_KNN(X_freq, Y_freq, X_test_freq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"SVC\"></a>\n## 7.4. SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mod_SVC(X, Y, X_test):\n\n    model_SVC=make_pipeline(StandardScaler(),SVC(random_state=1))\n    parameters=[{'svc__C': [0.0001,0.001,0.1,1, 10, 100], \n           'svc__gamma':[0.0001,0.001,0.1,1,10,50,100],\n           'svc__kernel':['rbf'],\n           'svc__degree' : [1,2,3,4]\n          }]\n    kfold = model_selection.KFold(n_splits=3, random_state=random_state, shuffle = True)\n    search_SVC = GridSearchCV(estimator=model_SVC, param_grid = parameters, scoring='accuracy', cv=kfold)\n    scores_SVC=cross_val_score(search_SVC, X, Y,scoring='accuracy', cv=kfold, verbose =1)\n    search_SVC.fit(X, Y)\n    predicted= search_SVC.predict(X_test)\n    \n    print(\"Best parameters are:\", search_SVC.best_params_)\n    print(\"Best accuracy achieved:\",search_SVC.cv_results_['mean_test_score'].mean())\n    \n    return search_SVC.best_params_, model_SVC, search_SVC, predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_SVC_onehot, model_SVC_onehot, search_SVC_onehot, predicted_cv_SVC_onehot = mod_SVC(X_onehot, Y_onehot, X_test_onehot)\nparam_SVC_lab, model_SVC_lab, search_SVC_lab, predicted_cv_SVC_lab = mod_SVC(X_lab, Y_lab, X_test_lab)\nparam_SVC_mean, model_SVC_mean, search_SVC_mean, predicted_cv_SVC_mean = mod_SVC(X_mean, Y_mean, X_test_mean)\nparam_SVC_freq, model_SVC_freq, search_SVC_freq, predicted_cv_SVC_freq = mod_SVC(X_freq, Y_freq, X_test_freq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_pred_SVC(X, Y, X_test):\n\n    model_SVC = make_pipeline(StandardScaler(),SVC(random_state=random_state, C= 1, gamma = 0.001, kernel = 'rbf', degree =1))\n    \n    model_SVC.fit(X, Y)\n    \n    predicted= model_SVC.predict(X_test)\n    \n    return predicted, model_SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict lables for all encoding strategies:\npredicted_SVC_onehot, model_SVC_onehot = fit_pred_SVC(X_onehot, Y_onehot, X_test_onehot)\npredicted_SVC_lab, model_SVC_lab = fit_pred_SVC(X_lab, Y_lab, X_test_lab)\npredicted_SVC_mean, model_SVC_mean = fit_pred_SVC(X_mean, Y_mean, X_test_mean)\npredicted_SVC_freq, model_SVC_freq = fit_pred_SVC(X_freq, Y_freq, X_test_freq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"conc\"></a>\n# Part 8: Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Top 5 models + encoding pairs achieved best results on public leaderboard are: \n    - Random forest with one-hot feature encoding \n    - Pipeline of MinMaxScaler and KNN with one hot feature encoding \n    - Pipeline of StandardScaling and SVC with frequency feature encoding\n    - Random forest with mean feature encoding\n    - Random forest with frequency feature encoding\nCombination of this models together allows to achieve 0.81818 public score. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"submission\"></a>\n# Part 9: Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = np.where(((predicted_SVC_mean + predicted_KNN_onehot+predicted_RFC_onehot+predicted_RFC_freq+ predicted_RFC_mean )/5) > 0.5, 1, 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test =pd.read_csv(\"../input/test.csv\")\nsubmission = pd.DataFrame({'PassengerId': test['PassengerId'],'Survived':predicted})\n\nsubmission.head()\n\nfilename = 'Titanic Predictions Public.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}