{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## First Steps on the ML Universe - Titanic Disaster!\n\n## First Steos on Kaggle and ML!\n\nDuring the past few months, I've been learning the basics of Machine Learning. This notebook is an attempt to show others like me, the insights I've came across so far. Hope you find it useful!\n\n### Table of Contents\n\n* [Exploratory Data Analysis](#eda)\n    * [Correlations, Distributions](#corr)\n* [Pre Processing](#prepro)\n    * [Some Feature Engineering](#fea)\n    * [Normalization - Scaling](#norm)\n    * [Imputation](#imp)\n        * [Observations](#obs)\n    * [+ Feature Enginering](#fea2)\n    * [Outliers](#out)\n        * [Observations](#obs2)\n* [Modeling](#mod)\n* [Final Thoughts](#final)\n\n\n### Understanding your problem and your data\n\nThe first step on your journey is to understand the problem you are trying to solve(**who survived the Titanic disaster?**), and the data you have to do it.\n\nLet's import your datasets and some of the libraries you will be using and see what the dataset looks like:","metadata":{}},{"cell_type":"code","source":"import numpy as np #Numpy is used for array manipulation and processing\nimport pandas as pd #Pandas is used for table manipulation and processing\nimport matplotlib.pyplot as plt\nimport seaborn as sns # A very useful library other than matplotlib for plotting\nfrom sklearn.impute import KNNImputer \nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier # Simple classification model\nfrom sklearn.model_selection import train_test_split #Splitting intro train and test\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV# Hyperparameter tunning\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix # Classification metrics\n\n##These lines of code import the datasets that are also available in the Code tab on the Kaggle's competition\n# You could also upload them via the Add data button on the right of your kaggle notebook.\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-13T16:38:12.235716Z","iopub.execute_input":"2021-08-13T16:38:12.23611Z","iopub.status.idle":"2021-08-13T16:38:12.247429Z","shell.execute_reply.started":"2021-08-13T16:38:12.236079Z","shell.execute_reply":"2021-08-13T16:38:12.246359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/titanic/train.csv') #Import our data\ndf.columns #This method returns the names of the columns of the dataset","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:12.34246Z","iopub.execute_input":"2021-08-13T16:38:12.34286Z","iopub.status.idle":"2021-08-13T16:38:12.356616Z","shell.execute_reply.started":"2021-08-13T16:38:12.342825Z","shell.execute_reply":"2021-08-13T16:38:12.355566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"eda\"></a>\n## Exploratory Data Analysis (EDA)\n\nWhen you are presented when a new problem, it might contain special vocabulary associated with the topic of the problem itself, so it's really important tu understand what are the variables(columns in this case) you have.\n\nIn this case, Kaggle provides information about it in the **Data** tab in the Titanic's competition page.\n\nBut it might happen you don't have this information(with most of the real world problems), so you will have to gather it by yourself. Usually, an expert of the topic can be a great place to start. **Research** it's a big part of solving a problem! \n\n### The columns\n\nMany of them are self-explanatory, but there are some that don't:\n\n* Embarked: Name of the port of embarkation\n* Parch: Number of parents/children aboard\n* SibSp: Number of siblings/spouses aboard\n\n### How do we continue?\n\nNow that you understand what are the data variables telling, you will try to figure out the relation of each variable with the target prediction(if the person survived or not). We will do some plotting and some further analysis.\n\nSome useful questions to guide youur analysis may be:\n\n* What type of distribution do the variables have?\n* Are there any outliers?\n* Are there any missing values? \n* What variables do you think will be of interest? Is there any that don't give too much information about the problem?","metadata":{}},{"cell_type":"code","source":"sns.pairplot(data = df, hue = 'Survived') #Plotting histograms and distributions of each variable.\n# hue parameter sets color by another variable, in this case, \"Survived\"","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:12.386961Z","iopub.execute_input":"2021-08-13T16:38:12.387318Z","iopub.status.idle":"2021-08-13T16:38:26.655871Z","shell.execute_reply.started":"2021-08-13T16:38:12.387288Z","shell.execute_reply":"2021-08-13T16:38:26.654785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(df.SibSp, hue = df.Survived)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:26.657493Z","iopub.execute_input":"2021-08-13T16:38:26.657891Z","iopub.status.idle":"2021-08-13T16:38:26.888103Z","shell.execute_reply.started":"2021-08-13T16:38:26.657857Z","shell.execute_reply":"2021-08-13T16:38:26.887052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/titanic/test.csv') #Import our data\n\nsns.pairplot(df_test,)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:26.889841Z","iopub.execute_input":"2021-08-13T16:38:26.890117Z","iopub.status.idle":"2021-08-13T16:38:36.821431Z","shell.execute_reply.started":"2021-08-13T16:38:26.890091Z","shell.execute_reply":"2021-08-13T16:38:36.820463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test.head(), df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:36.823081Z","iopub.execute_input":"2021-08-13T16:38:36.823374Z","iopub.status.idle":"2021-08-13T16:38:36.834807Z","shell.execute_reply.started":"2021-08-13T16:38:36.823345Z","shell.execute_reply":"2021-08-13T16:38:36.833664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df_test.corr(), annot = True)\nplt.title('Correlation between Features in Train')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:36.83656Z","iopub.execute_input":"2021-08-13T16:38:36.837078Z","iopub.status.idle":"2021-08-13T16:38:37.351702Z","shell.execute_reply.started":"2021-08-13T16:38:36.836971Z","shell.execute_reply":"2021-08-13T16:38:37.350587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df.corr(), annot = True)\nplt.title('Correlations between Features in Test')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:37.353252Z","iopub.execute_input":"2021-08-13T16:38:37.353667Z","iopub.status.idle":"2021-08-13T16:38:37.953053Z","shell.execute_reply.started":"2021-08-13T16:38:37.353622Z","shell.execute_reply":"2021-08-13T16:38:37.951936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Missing values in Train: ')\ndf.isna().sum() # We count the number of missing values on each column","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:37.954661Z","iopub.execute_input":"2021-08-13T16:38:37.955083Z","iopub.status.idle":"2021-08-13T16:38:37.965631Z","shell.execute_reply.started":"2021-08-13T16:38:37.95504Z","shell.execute_reply":"2021-08-13T16:38:37.964601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Missing values in Test: ')\ndf_test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:37.968872Z","iopub.execute_input":"2021-08-13T16:38:37.969475Z","iopub.status.idle":"2021-08-13T16:38:37.984446Z","shell.execute_reply.started":"2021-08-13T16:38:37.969429Z","shell.execute_reply":"2021-08-13T16:38:37.983449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Shape of Train: ', df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:37.986821Z","iopub.execute_input":"2021-08-13T16:38:37.987244Z","iopub.status.idle":"2021-08-13T16:38:38.010487Z","shell.execute_reply.started":"2021-08-13T16:38:37.9872Z","shell.execute_reply":"2021-08-13T16:38:38.009441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Shape of Test: ', df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:38.011742Z","iopub.execute_input":"2021-08-13T16:38:38.012051Z","iopub.status.idle":"2021-08-13T16:38:38.025913Z","shell.execute_reply.started":"2021-08-13T16:38:38.012023Z","shell.execute_reply":"2021-08-13T16:38:38.024708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x = df.Sex, y = df.Survived) #easy Barplot with seaborn ","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:38.027373Z","iopub.execute_input":"2021-08-13T16:38:38.027688Z","iopub.status.idle":"2021-08-13T16:38:38.278325Z","shell.execute_reply.started":"2021-08-13T16:38:38.027656Z","shell.execute_reply":"2021-08-13T16:38:38.277219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"corr\"></a>\n### Correlations\n\nTake some time to understand what's happening in the pairplots and heatmaps. There's plenty of information in those few lines. Parch and SibSp are **strongly correlatied**, so it gives you the chance to do some feature engineering. \nIn the Train Heatmap(which has Survived as a column) we see that Pclass is **strongly negative correlated**, and that's because first class passengers had a better chance to survive.\n\n### Distributions\n\nIn the KDEs you can percieve that depending on the number of SibSp, **the number of survivals is greater than the fatalities**, and, from my point of view, that's a very good reason to think on doing some Feature Enginerring with it. Something similar happens with the **Fare**\n\nThere's so many patterns going on that I'm missing, probably. You can dig in all you want and find others. This part of the process if crucial to get a better performance model.\n\n### Feature Importance\n\nThere are some columns that don't give you any information, throw them. Keep the ones that matter.\n\n","metadata":{}},{"cell_type":"markdown","source":"\n<a id = \"prepro\"></a>\n## Data Preprocessing\n\nDuring this step we want to leave the data ready for modeling. You will:\n\n* Impute missing values: A model can't have missing values. You have to do something with them. \n\n* Handle your Outliers: values that are too large or too small compared to the rest.\n\n* Normalize and scale: These steps are not always necessary. It depends on your Machine Learning Model. Some models are based on relations of distances(like K-Nearest Neighbors), so normalizing and scaling is indispensable to maintain an equally weighted importance of your features. Other models like Decision Trees are not designed the same way. Anyway, this is a good practice, so in this notebook you'll leasrn the basics on how to do it.\n\n\n### Some Feature Engineering\n\n**Label Encoding**: With the Sex column, Expensive Fare, and Small Family(see KDEs), all decisions based on the barplot, KDE and heatmap. \n\nI'll change Pclass from a descending rank to an ascending one. I don't know if this change something.\n ","metadata":{}},{"cell_type":"code","source":"df.Sex = df.Sex.map(dict(female=1, male=0)) #The map method uses a dictionary to transform values on a Series object.\ndf_test.Sex = df_test.Sex.map(dict(female=1, male=0))","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:38.280073Z","iopub.execute_input":"2021-08-13T16:38:38.280629Z","iopub.status.idle":"2021-08-13T16:38:38.290013Z","shell.execute_reply.started":"2021-08-13T16:38:38.280584Z","shell.execute_reply":"2021-08-13T16:38:38.288953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Pclass'] = df['Pclass'].replace([3,1],[1,3]) #Another method to replace values on column\ndf_test['Pclass'] = df_test['Pclass'].replace([3,1],[1,3])\nprint(df.head(), df_test.head())","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:38.291378Z","iopub.execute_input":"2021-08-13T16:38:38.291721Z","iopub.status.idle":"2021-08-13T16:38:38.314221Z","shell.execute_reply.started":"2021-08-13T16:38:38.291691Z","shell.execute_reply":"2021-08-13T16:38:38.313265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Small Family'] = np.where((df['SibSp'] <= 2) & (df['SibSp'] != 0), 1, 0)\ndf_test['Small Family'] = np.where((df_test['SibSp'] <= 2) & (df_test['SibSp'] != 0), 1, 0)\ndf_test['Lonely Child'] = np.where(df_test['Parch'] == 1, 1, 0)\ndf['Lonely Child'] = np.where(df['Parch'] == 1, 1, 0)\n\ndf['Family'] = df['SibSp'] + df['Parch']\ndf_test['Family'] = df_test['SibSp'] + df_test['Parch']\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:38.315455Z","iopub.execute_input":"2021-08-13T16:38:38.315778Z","iopub.status.idle":"2021-08-13T16:38:38.346227Z","shell.execute_reply.started":"2021-08-13T16:38:38.315748Z","shell.execute_reply":"2021-08-13T16:38:38.345203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df[['Age', 'Pclass', 'Fare', 'Family','Lonely Child', 'Sex', 'Small Family','PassengerId']]#Select features\nXt = df_test[['Age', 'Pclass', 'Fare', 'Family','Lonely Child', 'Sex', 'Small Family', 'PassengerId']]\ny = df.Survived #Select target","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:38.34746Z","iopub.execute_input":"2021-08-13T16:38:38.347772Z","iopub.status.idle":"2021-08-13T16:38:38.355671Z","shell.execute_reply.started":"2021-08-13T16:38:38.347742Z","shell.execute_reply":"2021-08-13T16:38:38.354897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"norm\"></a>\n### Normalization - Scaling\n\nScaling data means(in a non-exhaustive, non-academic way) **bringing all your feature values to a common scale, maintaining the individual relationships between each feature**, this is, brinding each column of your dataframe to, say, values between 0 and 1. This makes sense: if a column has values between 1000 and 100000, and another between 0 and 1, some algorithms may incorrectly decide that the first feature is much more important than the second one.\nNormalizing is, well, **changing the shape of your distributions to fit a normal one**, said in very simple terms. The **encoded features** like \"Sex\" and \"Pclass\" are not normalized.\n \n\n#### Side Notes\n\nThe truth is, you should make some Statistical proof about the relation between the variables for the imputation. We are simplifying things a big lot here. If you want more details, just let me know.\n\nI'm also learning so any feedback on improvements are very, very welcomed :)","metadata":{}},{"cell_type":"code","source":"X = X.append(Xt)\n\nfor i in range(4): # Run through all columns except encoded\n    X.iloc[:,i] = X.iloc[:,i] / X.iloc[:,i].max() # Scale each column dividing by maximum\n    mean = X.iloc[:,i].mean() # Calculate column mean\n    std = X.iloc[:,i].std() # Calculate column STD\n    X.iloc[:,i] = (X.iloc[:,i] - mean)/std # Normalie column\nprint(X.shape, X.head())","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:38.356555Z","iopub.execute_input":"2021-08-13T16:38:38.356892Z","iopub.status.idle":"2021-08-13T16:38:38.378994Z","shell.execute_reply.started":"2021-08-13T16:38:38.356863Z","shell.execute_reply":"2021-08-13T16:38:38.377984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"knn\"></a>\n### KNN Imputer\n\nThis kind of imputation uses KNNeighbors logic: it assings a value to the missing gaps depending on the nearests points values. It has it's own hyperparameters, but in this notebook I won't focus on those aspects. I don't want you to get too bored too quickly. Kaggle and StackOverflow are places to look up for amazing content.","metadata":{"execution":{"iopub.status.busy":"2021-08-10T04:29:43.030769Z","iopub.execute_input":"2021-08-10T04:29:43.031062Z","iopub.status.idle":"2021-08-10T04:29:43.035301Z","shell.execute_reply.started":"2021-08-10T04:29:43.031038Z","shell.execute_reply":"2021-08-10T04:29:43.034293Z"}}},{"cell_type":"code","source":"sns.kdeplot(Xt.Age) # plot first kde\nplt.title('Age Distribution without imputation')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:38.38037Z","iopub.execute_input":"2021-08-13T16:38:38.380715Z","iopub.status.idle":"2021-08-13T16:38:38.597229Z","shell.execute_reply.started":"2021-08-13T16:38:38.380684Z","shell.execute_reply":"2021-08-13T16:38:38.596149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputer = KNNImputer() # Create the imputer object\nX = pd.DataFrame(imputer.fit_transform(X),columns = X.columns) # Transform the data","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:38.598533Z","iopub.execute_input":"2021-08-13T16:38:38.598904Z","iopub.status.idle":"2021-08-13T16:38:38.648502Z","shell.execute_reply.started":"2021-08-13T16:38:38.598873Z","shell.execute_reply":"2021-08-13T16:38:38.647075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(Xt.Age) # plot first kde\nplt.title('Age Distribution without imputation')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:38.650264Z","iopub.execute_input":"2021-08-13T16:38:38.650702Z","iopub.status.idle":"2021-08-13T16:38:38.894909Z","shell.execute_reply.started":"2021-08-13T16:38:38.650648Z","shell.execute_reply":"2021-08-13T16:38:38.89386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<a id = \"obs\"></a>\n### Observations\n\nYou can see that Age distribution looks close enough with and without imputation. **This is crucial**. You want your data to look similar with the imputation as how it looked without it.\n\n<a id = \"fea2\"></a>\n## Feature Engineering(Child and Family Columns)\n\nYou can see in the above graphs that we could create a new feature using this \"two headed distribution\". Let's add a new column named Child with 0 if False and 1 if True","metadata":{}},{"cell_type":"code","source":"X['Child'] = np.where(X['Age']< -1.5, 1, 0)\nX['Expensive'] = np.where(X['Fare'] <= 0.14, 0, 1)\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:38.89674Z","iopub.execute_input":"2021-08-13T16:38:38.897178Z","iopub.status.idle":"2021-08-13T16:38:38.920037Z","shell.execute_reply.started":"2021-08-13T16:38:38.897133Z","shell.execute_reply":"2021-08-13T16:38:38.918941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"out\"></a>\n### Outliers\n\nWithout scaling and normalization, you could tend to get too many instances(each **row** of your dataset) when detecting outliers. Now that you have everything set up nicely, let's see how these new features look like, this time with **boxplots** ","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x = 'variable', y = 'value', data=pd.melt(X[['Age', 'Pclass', 'Fare', 'Family', 'Sex', 'Small Family']]))\nplt.title('Distribution of the features')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:38.921845Z","iopub.execute_input":"2021-08-13T16:38:38.922261Z","iopub.status.idle":"2021-08-13T16:38:39.200987Z","shell.execute_reply.started":"2021-08-13T16:38:38.922219Z","shell.execute_reply":"2021-08-13T16:38:39.199812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"obs2\"></a>\n### Observations\n\nAs you may expect, there are no such thing as an outlier on an encoded column like Sex or Pclass, and in **all** other features we can see a long-tailed distribution. **This must be kept on mind for the outlier detection**.\n\nFor an **excellent in-depth article about outliers** visit: \nhttps://www.kaggle.com/aimack/how-to-handle-outliers/notebook\nby Akash Dey. \n\nThe following steps follow his logic. Some of the code waws borrowed as well.\nSince there are not too many instances(each individual), less than 1000, **Capping method** will be used.","metadata":{}},{"cell_type":"code","source":"for i in [0, 2, 3]:\n    q1 = X.iloc[:,i].quantile(0.25) # Calculate 1 quartile\n    q3 = X.iloc[:,i].quantile(0.75) # Calculate 3 quartile\n\n    IQR = q3 - q1 # Interquartile range\n\n    #defining max and min limits\n    max_limit = q3 + (1.5 * IQR)\n    min_limit = q1 - (1.5 * IQR) \n\n    #capping\n    X.iloc[:,i] = pd.DataFrame(np.where(X.iloc[:,i] > max_limit, max_limit, \n             (np.where(X.iloc[:,i] < min_limit, min_limit,X.iloc[:,i] ))))","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:39.202777Z","iopub.execute_input":"2021-08-13T16:38:39.203223Z","iopub.status.idle":"2021-08-13T16:38:39.222698Z","shell.execute_reply.started":"2021-08-13T16:38:39.203178Z","shell.execute_reply":"2021-08-13T16:38:39.221609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x = 'variable', y = 'value', data=pd.melt(X[['Age', 'Pclass', 'Fare', 'Family', 'Sex', 'Small Family']]))\nplt.title('Distribution of the features')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:39.226856Z","iopub.execute_input":"2021-08-13T16:38:39.227188Z","iopub.status.idle":"2021-08-13T16:38:39.497377Z","shell.execute_reply.started":"2021-08-13T16:38:39.227157Z","shell.execute_reply":"2021-08-13T16:38:39.496299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"mod\"></a>\n### Modeling\n\nNow that you have both train and test sets preprocessed, let's train a Random Forest and a Support Vector Machine. My hypothesis is that SVC might perform better, since it can benefit from the scaling and normalizing.","metadata":{}},{"cell_type":"code","source":"Xtrain = X.iloc[:891, :]\nXtest = X.iloc[891:, :]","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:39.49943Z","iopub.execute_input":"2021-08-13T16:38:39.499891Z","iopub.status.idle":"2021-08-13T16:38:39.505299Z","shell.execute_reply.started":"2021-08-13T16:38:39.499847Z","shell.execute_reply":"2021-08-13T16:38:39.504287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = Xtrain[['Age', 'Pclass', 'Fare', 'Family','Lonely Child', 'Sex', 'Small Family', 'Expensive', 'Child']]\nX_test = Xtest[['Age', 'Pclass', 'Fare', 'Family','Lonely Child', 'Sex', 'Small Family', 'Expensive', 'Child']]","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:39.506994Z","iopub.execute_input":"2021-08-13T16:38:39.507428Z","iopub.status.idle":"2021-08-13T16:38:39.519786Z","shell.execute_reply.started":"2021-08-13T16:38:39.507388Z","shell.execute_reply":"2021-08-13T16:38:39.518727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtrain2, Xtest2, ytrain, ytest = train_test_split(X_train, y, train_size = 0.7, test_size = 0.3)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:39.521026Z","iopub.execute_input":"2021-08-13T16:38:39.521367Z","iopub.status.idle":"2021-08-13T16:38:39.534578Z","shell.execute_reply.started":"2021-08-13T16:38:39.521338Z","shell.execute_reply":"2021-08-13T16:38:39.533368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier()\ngrid_list = {\"n_estimators\": [100, 200, 300, 500, 1000],\n             'max_depth': [100, 30, 150, None],\n             'max_features':['auto', 'sqrt', 'log2']}\n\n\ngrid_search = GridSearchCV(rf, param_grid = grid_list, n_jobs = 4, cv = 5, scoring = 'accuracy') \ngrid_search.fit(Xtrain2, ytrain) \nprint(grid_search.best_params_, grid_search.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:38:39.536231Z","iopub.execute_input":"2021-08-13T16:38:39.536736Z","iopub.status.idle":"2021-08-13T16:40:33.875997Z","shell.execute_reply.started":"2021-08-13T16:38:39.536694Z","shell.execute_reply":"2021-08-13T16:40:33.874863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = SVC()\ngrid_list = {'C': [0.1,1, 10, 100], \n             'gamma': [1,0.1,0.01,0.001],\n             'kernel': ['rbf', 'poly', 'sigmoid']}\n\n\ngrid_search = GridSearchCV(svc, param_grid = grid_list, n_jobs = 4, cv = 5, scoring = 'accuracy') \ngrid_search.fit(Xtrain2, ytrain) \nprint(grid_search.best_params_, grid_search.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:40:33.877519Z","iopub.execute_input":"2021-08-13T16:40:33.877865Z","iopub.status.idle":"2021-08-13T16:40:52.392175Z","shell.execute_reply.started":"2021-08-13T16:40:33.877831Z","shell.execute_reply":"2021-08-13T16:40:52.390917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"final\"></a>\n### Final Thoughts\n\nIf you have any comment, suggestion, or idea, please let me know. Hope you enjoyed it =).\n\ndrK~\n","metadata":{}},{"cell_type":"code","source":"svc = SVC(C = 100, gamma = 0.01, kernel = 'rbf')\nsvc.fit(Xtrain2, ytrain)\npredictions = svc.predict(Xtest2)\naccuracy_score(ytest, predictions)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:40:52.393793Z","iopub.execute_input":"2021-08-13T16:40:52.394482Z","iopub.status.idle":"2021-08-13T16:40:52.429984Z","shell.execute_reply.started":"2021-08-13T16:40:52.394436Z","shell.execute_reply":"2021-08-13T16:40:52.428858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 300)\nrf.fit(Xtrain2,ytrain)\npredictions = rf.predict(Xtest2)\naccuracy_score(ytest,predictions)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:41:26.718292Z","iopub.execute_input":"2021-08-13T16:41:26.718864Z","iopub.status.idle":"2021-08-13T16:41:27.436036Z","shell.execute_reply.started":"2021-08-13T16:41:26.718815Z","shell.execute_reply":"2021-08-13T16:41:27.435053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = SVC(C = 100, gamma = 0.01, kernel = 'rbf')\nsvc.fit(X_train, y)\npredictions = svc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:41:38.027869Z","iopub.execute_input":"2021-08-13T16:41:38.028516Z","iopub.status.idle":"2021-08-13T16:41:38.082524Z","shell.execute_reply.started":"2021-08-13T16:41:38.02848Z","shell.execute_reply":"2021-08-13T16:41:38.081619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtest['PassengerId'] = Xtest['PassengerId'].astype(int)\n\noutput = pd.DataFrame({'PassengerId': Xtest.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:41:48.951206Z","iopub.execute_input":"2021-08-13T16:41:48.95159Z","iopub.status.idle":"2021-08-13T16:41:48.965255Z","shell.execute_reply.started":"2021-08-13T16:41:48.951543Z","shell.execute_reply":"2021-08-13T16:41:48.964294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}