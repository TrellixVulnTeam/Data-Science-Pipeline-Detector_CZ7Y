{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"top\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Content</h3>\n\n* [1. Reading the Data](#1)\n* [2. EDA: Exploring Insights](#2)\n    - [2.1 Survival Rate](#2.1)\n    - [2.2 Gender Analysis](#2.2)\n    - [2.3 Social Class Influence](#2.3)\n    - [2.4 Companion on the Ship](#2.4)\n    - [2.5 The Age Factor](#2.5)\n    - [2.6 Fare Paid by Passengers](#2.6)\n    - [2.7 Simultaneous Analysis](#2.7)\n* [3. Prep: Building Pipelines](#3) \n    - [3.1 Initial Pipeline](#3.1)\n        - [3.1.1 Custom Features](#3.1.1)\n        - [3.1.1 Candidate Features](#3.1.2)\n        - [3.1.2 Duplicated Data](#3.1.3)\n        - [3.1.3 Modifying Dtypes](#3.1.4)\n        - [3.1.4 Training and Validation Data](#3.1.5)\n    - [3.2 Numerical Pipeline](#3.2)\n        - [3.2.1 Null Data](#3.2.1)\n        - [3.2.2 Log Transformation](#3.2.2)\n        - [3.2.3 Normalization](#3.2.3)\n    - [3.3 Categorical Pipeline](#3.3)\n        - [3.3.1 Encoding](#3.3.1)\n    - [3.4 Complete Pipelines](#3.4)\n* [4. Modeling: Survival Prediction](#4)\n    - [4.1 Structuring Variables](#4.1)\n    - [4.2 Training Models](#4.2)\n    - [4.3 Evaluating Performance](#4.3)\n    - [4.4 Training Flow and Visual Analysis](#4.4)\n* [5. Tunning Hyperparameters](#5)\n* [6. Submitting: Prediction Pipeline](#6)\n* [7. References](#7)    ","metadata":{"toc":true}},{"cell_type":"markdown","source":"This notebook aims to allocate the development related to exploratory analysis of insights related to the dataset [Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic) taken from the Kaggle platform to improve skills in Data Science and Machine Learning. Also, this notebook uses the tools presented on [xplotter](https://github.com/ThiagoPanini/xplotter) and [mlcomposer](https://github.com/ThiagoPanini/mlcomposer) python packages made by myself and published on PyPI repository. This is a real good effort for coding useful functions for making the Exploratory Data Analysis and applying Machine Learning process a lot more easier for Data Scientists and Data Analysis through deliverying charts customization and matplotlib/seaborn plots with a little few lines of code. I really hope you all enjoy it!\n\n<div align=\"center\">\n    <img src=\"https://i.imgur.com/5XFP1Ha.png\" height=300 width=200 alt=\"xplotter Logo\">\n    <img src=\"https://i.imgur.com/MIcPH8g.png\" width=450 height=450 alt=\"mlcomposer logo\">\n</div>\n\n___\n**_Description and context:_**\n_The sinking of the Titanic is one of the most well-known events in the world. [...] While some factors related to \"luck\" were present among the survivors, apparently some specific groups of passengers and crew were more likely to survive than others. In this challenge, it is proposed to create a Machine Learning model capable of answering the following question: \"Which groups of people could be more able to survive the shipwreck?\"_","metadata":{}},{"cell_type":"code","source":"!pip install xplotter --upgrade\n!pip install mlcomposer --upgrade","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Project libraries\nimport pandas as pd\nimport os\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# Project variables\nDATA_PATH = '../input/titanic'\nTRAIN_FILENAME = 'train.csv'\nTEST_FILENAME = 'test.csv'","metadata":{"ExecuteTime":{"end_time":"2020-12-23T16:06:54.761918Z","start_time":"2020-12-23T16:06:53.851147Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>1. Reading the Data</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After importing the main libraries common to the project and also defining important variables for reading the data, it is possible to make the first contact with the database available for the development of the task.","metadata":{}},{"cell_type":"code","source":"# Reading training data\ndf = pd.read_csv(os.path.join(DATA_PATH, TRAIN_FILENAME))\ndf.head()","metadata":{"ExecuteTime":{"end_time":"2020-12-23T16:06:54.811464Z","start_time":"2020-12-23T16:06:54.773391Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In case you may ask about [metadata](https://www.kaggle.com/c/titanic/data), here is some useful points to consider:\n\n- **_PassengerId:_** reference of passenger id registered on the trip;\n- **_Survived:_** target variable indicating passenger's survival (1=yes, 0=no);\n- **_Pclass:_** passenger's social class (ticket's reference) (1=high, 2=medium ou 3=low);\n- **_Name:_** passenger's name;\n- **_Sex:_** passenger's gender;\n- **_Age:_** passenger's age;\n- **_SibSp:_** total of passenger's siblings / spouses aboard the ship;\n- **_Parch:_** total of passenger's parents / children aboard the ship;\n- **_Ticket:_** passenger's ticket reference number;\n- **_Fare:_** amount paid of passenger on the ticket bought;\n- **_Cabin:_** passenger's cabin reference number;\n- **_Embarked:_** port of embarkation(C=Cherbourg, Q=Queenstown, S=Southampton).","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>2. EDA: Exploring Insights</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"At this point, there is a well-defined context of the project's objective, in addition to a database already read and transformed into a DataFrame format of the pandas. From this moment on, a true scan of the data will be proposed for the application of a detailed descriptive analysis in order to gather relevant insights for the business context.\n\nUsing the homemade package [xplotter](https://github.com/ThiagoPanini/xplotter), whose construction was motivated exactly to facilitate the work of data scientists in the pillars of insights and exploratory data analysis. The next steps will be based on the tools provided from xplotter library to make beautiful charts in order to get a deep understand of our data. ","metadata":{}},{"cell_type":"markdown","source":"<div align=\"center\">\n    <img src=\"https://i.imgur.com/5XFP1Ha.png\" alt=\"xplotter logo\">\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n<font color=\"dimgrey\" size=+2.0><b>2.1 Survival Rate</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"code","source":"# Importing libraries\nfrom xplotter.insights import *\n\n# Survival rate\nsurvived_map = {1: 'Survived', 0: 'Not Survived'}\nsurvived_colors = ['crimson', 'darkslateblue']\nplot_donut_chart(df=df, col='Survived', label_names=survived_map, colors=survived_colors,\n                 title='Absolute Total and Percentual of Passengers \\nwho Survived Titanic Disaster')","metadata":{"ExecuteTime":{"end_time":"2020-12-23T16:06:56.906649Z","start_time":"2020-12-23T16:06:54.815171Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graph above clearly shows the proportion of survivors and victims of the 891 passengers and crew present in the database available for analysis. In it, it is possible to perceive a greater number of victims of the shipwreck, totaling 549 passengers or 61.6% of the total. The smallest number of survivors adds up to 342 passengers or 38.4% of the total.\n\nThe `Survived` variable can be given as the target variable for a possible predictive model to be trained in this notebook in the future. The main objective is to analyze the main factors or to identify the main groups of passengers with a greater chance of survival in view of the demographic and social characteristics present in the base.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n<font color=\"dimgrey\" size=+2.0><b>2.2 Gender Analysis</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"From the survival analysis performed above, it is possible to expand the views using other variables present in the base. In this session, we will check if there was any influence of the passenger gender on the issue of survival.","metadata":{}},{"cell_type":"code","source":"# Countplot for gender\ngender_colors = ['lightskyblue', 'lightcoral']\ngender_map = {'male': 'Male', 'female': 'Female'}\nplot_countplot(df=df, col='Sex', palette=gender_colors, label_names=gender_map,\n               title='Total Passengers by Its Gender')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph above, it can be seen that the public of travelers on the Titanic was made up of approximately 65% men and 35% women, and this can probably be considered a standard scenario for the time (after all, we are talking about something that occurred in the middle of 1911).\n\nKeeping in mind the total volumetry by gender is essential for further analysis of the survival rate by gender.","metadata":{}},{"cell_type":"code","source":"# Survival rate by gender\nplot_countplot(df=df, col='Survived', hue='Sex', label_names=survived_map, palette=gender_colors,\n               title=\"Could gender had some influence on\\nsurviving from Titanic shipwreck?\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graph above shows that, of the 549 recorded victims (61.6% of the total), we have 468 (or 52.5%) males and only 81 (or 9.1%) females. On the other hand, of the 342 (or 38.4%) survivors, we have 109 (or 12.2%) males and 233 (or 26.2%) females.\n\nIn other words, the graph shows that there was a possible rescue priority given to the women of the vessel, since their share of representativeness is more significant in the group of survivors than in the group of victims. To explain this scenario, it is possible to imagine the existence of a possible rescue protocol for prioritizing women and children in emergency cases.\n\nAnother way to analyze this representativeness of survival by gender can be found in the double donut chart below:","metadata":{}},{"cell_type":"code","source":"# Plotting a double donut chart\nplot_double_donut_chart(df=df, col1='Survived', col2='Sex', label_names_col1=survived_map, \n                        colors1=['crimson', 'navy'], colors2=['lightcoral', 'lightskyblue'],\n                        title=\"Did the passenger's gender influence \\nsurvival rate?\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both graphs show: female passengers had priority in the rescue during the shipwreck.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n<font color=\"dimgrey\" size=+2.0><b>2.3 Social Class Influence</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"The next topic of analysis takes into account the values contained in the `Pclass` variable, which, in turn, brings information regarding the social class of each of the passengers present on the vessel.","metadata":{}},{"cell_type":"code","source":"# Number of passengers for each class\npclass_map = {1: 'Upper Class', 2: 'Middle Class', 3: 'Lower Class'}\nplot_pie_chart(df=df, col='Pclass', colors=['brown', 'gold', 'darkgrey'],\n               explode=(0.03, 0, 0), label_names=pclass_map,\n               title=\"Total Passengers by Social Class\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the pie chart above, it is possible to see that the majority of the ship's passengers and crew were formed by members of the lower class, representing just over 55% of the total present. Next, we will analyze whether this variable could, in some way, have included passengers' survival","metadata":{}},{"cell_type":"code","source":"# Relação entre sobrevivência e classe social\nplot_countplot(df=df, col='Pclass', hue='Survived', label_names=pclass_map, palette=survived_colors,\n               title=\"Survival Analysis by Social Class\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From reading the bar graph above, it is possible to see how the red (victims) and blue (survivors) bars have different proportions for each of the social classes analyzed in the base. In general, we have:\n\n* Upper Class: there is a higher number of survivors than victims;\n* Middle Class: a more equal balance of survivors and victims;\n* Lower Class: formed mostly by victims.\n\nThis scenario indicates a clear situation: members of the so-called \"Upper Class\" had a greater chance of surviving the shipwreck than, for example, members of the \"Lower Class\". Probably some privilege was given to these passengers during the rescue or, in some way, these same passengers could be allocated in more comfortable / safe positions on the ship, thus facilitating the rescue.","metadata":{}},{"cell_type":"markdown","source":"Uma outra forma de analisar essa representatividade de sobrevivência por classe social pode ser dada a partir do gráfico de barras agrupadas abaixo:","metadata":{}},{"cell_type":"code","source":"plot_pct_countplot(df=df, col='Pclass', hue='Survived', palette='rainbow_r',\n                   title='Social Class Influence on Survival Rate')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.4\"></a>\n<font color=\"dimgrey\" size=+2.0><b>2.4 Companion on the Ship</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"There are two variables in the dataset that inform the presence of passengers' companions on the vessel, namely SibSp (number of siblings or spouses) and Parch (number of parents or children). In this session, we will analyze whether, in any way, the presence of companions influenced the survival of these passengers.","metadata":{}},{"cell_type":"code","source":"# Relationship between SibSp/Parch and Survived\nplot_countplot(df=df, col='SibSp', hue='Survived', orient='v', palette=survived_colors,\n               title='Survival Analysis by SibSp')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Relationship between SibSp/Parch and Survived\nplot_countplot(df=df, col='Parch', hue='Survived', orient='v', palette=survived_colors,\n               title='Survival Analysis by Parch')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing the two bar graphs above, it is possible to infer that passengers accompanied by 1 or 2 people, be they siblings, spouses, parents or children, prevent a more positive survival scenario in relation to the others.\n\nIt is likely that, in the chaotic rescue scenario, the presence of a not too high and not too low number of companions may have helped in the rescue in a possible situation of mutual help.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.5\"></a>\n<font color=\"dimgrey\" size=+2.0><b>2.5 The Age Factor</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Well, until this moment of the exploratory analysis, only categorical attributes present in the base were placed on the agenda. However, the available data set has at least two numerical invoices of probable importance for the project objective: Age and Average Ticket.\n\nIn this first moment, we will analyze how age may have influenced the survival of the ship's passengers.","metadata":{}},{"cell_type":"code","source":"# Distribution of age variable\nplot_distplot(df=df, col='Age', title=\"Passenger's Age Distribution\", hist=True)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above distribution, it is possible to have a general idea of the audience present on the ship in terms of age. The peak density occurs at approximately 25 years of age, indicating that the largest range of passengers was formed by a relatively young audience.\n\nWe will repeat the density analysis above and build a curve for survivors and another for victims.","metadata":{}},{"cell_type":"code","source":"plot_distplot(df=df, col='Age', hue='Survived', kind='kde', color_list=['crimson', 'darkslateblue'],\n              title=\"Is there any relationship between age distribution\\n from survivors and non survivors passengers?\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overall, the graph above does not show a clear relationship between the influence of age on passenger survival. However, it is possible to highlight some subtle points on the graph, for example, the elevation of the curve at ages \"close to 0\", thus indicating a possible rescue priority given to the youngest children.\n\nThis scenario is also repeated, even more subtly, in a portion of passengers over 80 years old.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.6\"></a>\n<font color=\"dimgrey\" size=+2.0><b>2.6 Fare Paid by Passengers</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"The `Fare` variable in the data set brings the amount paid by each passenger to board the Titanic. We saw, in some previous stages, that social class may have had a certain influence on the passenger's chance of survival. Does this scenario repeat itself for the amount paid for the entry ticket?","metadata":{}},{"cell_type":"code","source":"plot_distplot(df=df, col='Fare', title='Fare Distribution', hist=True)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see the distribution of Fare variable with some outliers on the right side of the curve (people who paid a really high amount for the ticket).","metadata":{}},{"cell_type":"code","source":"# Fare distribution by social class\nplot_distplot(df=df, col='Fare', hue='Pclass', kind='strip', label_names=pclass_map,\n              palette=['gold', 'silver', 'brown'],\n              title=\"Fare Distribution by Social Class\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graph above shows that, in general, the higher the passenger's social class, the higher the ticket paid to enter the ship. Something expected, but of utmost importance its graphic visualization in this study.","metadata":{}},{"cell_type":"code","source":"plot_distplot(df=df, col='Fare', hue='Sex', kind='boxen', palette=gender_colors,\n              title=\"What's the Fare distribution by Gender?\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is interesting to see that, in general, women paid higher amounts than men.","metadata":{}},{"cell_type":"code","source":"plot_distplot(df=df, col='Fare', hue='Survived', kind='kde', color_list=['crimson', 'darkslateblue'],\n              title=\"What's the relationship between the \\nfare paid and survivor?\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, the graph above shows a density distribution relating the average ticket paid per passenger and the database survival indicator. Here, it is possible to notice that the orange curve related to the survivors (Survived = 1) has a higher concentration in high values for the Fare variable, thus indicating that the ticket value may have had some influence on the passengers' survival.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.7\"></a>\n<font color=\"dimgrey\" size=+2.0><b>2.7 Simultaneous Analysis</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Within the tools of the `insights` module of the `xplotter` package, it is possible to find some functions built to facilitate analysis in several columns of a base simultaneously. For example, to view volumetries of a series of categorical variables, it would be possible to execute the function `plot_multiple_catplots()`, as shown below:","metadata":{}},{"cell_type":"code","source":"cat_cols = ['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\nplot_multiple_countplots(df=df, col_list=cat_cols, orient='v')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another possibility is to analyze categorical columns based on a numerical aggregation column. For this, the `xplotter` module brings with it the` plot_cat_aggreg_report() `function. The block below dynamically analyzes the behavior of the `Embarked` variable and the` Fare` variable together, answering questions such as \"what is the volume of passengers per port of departure?\" or \"what are the statistical averages of the amount paid for the ticket?\"","metadata":{}},{"cell_type":"code","source":"plot_cat_aggreg_report(df=df, cat_col='Embarked', value_col='Fare', title3='Statistical Analysis', \n                       desc_text=f'A statistical approach for Fare \\nusing the data available',\n                       stat_title_mean='Mean', stat_title_median='Median', stat_title_std='Std', \n                       stat_title_x_pos=.3, stat_x_pos=.3, inc_x_pos=10)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>3. Prep: Building Pipelines</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After a long journey in the exploratory analysis session, we have gathered valuable insights that can make modeling work much easier and more intuitive. Thus, the next topics in this notebook will deal with an extremely important step in the training of a machine learning model: the preparation of the database.\n\nUsing the homemade package pycomp, whose construction was motivated exactly to facilitate the work of data scientists in the pillars of insights, prep and modeling, for this second session is expected a full understanding of the set of available data and a clear idea of the steps required to be applied in the prep and in the modeling. To facilitate this work, a very powerful tool built within the `mlcomposer` package will be used. This is the `mlcomposer.transformers` module, which consists of ready-made classes capable of performing a series of activities and transformations in a database, in addition to a complete integration with` Pipelines` of prep data.\n\n<div align=\"center\">\n    <img src=\"https://i.imgur.com/MIcPH8g.png\" width=450 height=450 alt=\"mlcomposer logo\">\n</div>\n\nBefore starting the steps regarding these transformations, let's apply another equally powerful function of the `xplotter.insights` module capable of returning an overview of a database.","metadata":{}},{"cell_type":"code","source":"# Data overview from xplotter\ndata_overview(df=df)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.1 Initial Pipeline</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1.1\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.1.1 Custom Features</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"In this first session, we will propose the construction of customized features in our database, thinking about a possible extraction of value from columns that, in its raw format, probably do not bring relevant insights to a predictive model. Thus, the following blocks of code will focus on extracting new insights that may or may not be considered in the final model, as follows:\n\n* **_name_title_**: column that extracts the \"title\" information from the passenger's name (Mr, Mrs, etc.)\n* **_cabin_class_**: column responsible for bringing the \"category\" of the cabin (A, B, C, etc.)\n* **_ticket_class_**: column that extracts categorized information from the passenger's ticket\n* **_age_cat_**: column for categorizing age ranges\n* **_fare_cat_**: Fare variable track categorization column\n* **_family_size_**: column to inform the passenger's total family size\n\nTo contemplate the extraction of these new features, a class called `CustomFeaturesTitanic` will be built, which, in turn, contains extraction intelligence based on` RegEx` and other rules in its own structure for inclusion of this step in a possible pipeline for preparing the Dice. The main objective is to iterate this and other transformation classes in a search for the best combinations considered by a classification model.","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport re\n\nclass CustomFeaturesTitanic(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, name_title=True, ticket_class=True, cabin_class=True, name_length=True,\n                 age_cat=True, fare_cat=True, family_size=True, name_title_re='([a-zA-Z]+\\.)', \n                 ticket_class_re='[A-Z]+([/]{0,})([^\\s.]+)', cabin_class_re = '\\w',\n                 age_bins=[0, 10, 20, 40, 60, 999], age_labels=['0_10', '10_20', '20_40', '40_60', 'greater_60'],\n                 fare_bins=[0, 8, 15, 25, 50, 99999], fare_labels=['0_8', '8_15', '15_25', '25_50', 'greater_50']):\n        self.name_title = name_title\n        self.ticket_class = ticket_class\n        self.cabin_class = cabin_class\n        self.name_length = name_length\n        self.age_cat = age_cat\n        self.fare_cat = fare_cat\n        self.family_size = family_size\n        \n        self.age_bins = age_bins\n        self.age_labels = age_labels\n        self.fare_bins = fare_bins\n        self.fare_labels = fare_labels\n        self.name_title_re = name_title_re\n        self.ticket_class_re = ticket_class_re\n        self.cabin_class_re = cabin_class_re\n        \n    def fit(self, df, y=None):\n        return self\n    \n    def transform(self, df, y=None):\n        \n        # Extraindo feature relacionada ao título da pessoa\n        if self.name_title:\n            df['name_match'] = df['Name'].apply(lambda x: re.search(self.name_title_re, x))\n            df['name_regex'] = df['name_match'].apply(lambda x: ''.join(x.groups()) if x is not None else np.nan)\n            \n            def extract_name_title(x, other_tag='OTHER'):\n                if x not in ['Mr.', 'Miss.', 'Mrs.', 'Master.']:\n                    x = other_tag\n                return x\n                    \n            df['name_title'] = df['name_regex'].apply(lambda x: extract_name_title(x))\n            df.drop('name_match', axis=1, inplace=True)\n            df.drop('name_regex', axis=1, inplace=True)\n        \n        # Extraindo feature relacionada a classe do ticket\n        if self.ticket_class:\n            df['ticket_match'] = df['Ticket'].apply(lambda x: re.search(self.ticket_class_re, x))\n            df['ticket_regex'] = df['ticket_match'].apply(lambda x: x.group() if x is not None else 'all_numbers')\n            \n            # Definindo função para extração da classe\n            def extract_ticket_class(x, other_tag='OTHER'):\n                if x in ['A/5', 'A/4', 'A4', 'A/S']:\n                    x = 'A'\n                elif x in ['STON/O', 'SOTON/O', 'SOTON/OQ', 'STON/O2', 'SOTON/O2', 'SOTON']:\n                    x = 'STON_SOTON'\n                elif x in ['SC/PARIS', 'SC/Paris', 'SC/AH', 'SC']:\n                    x = 'SC'\n                elif x == 'PC':\n                    x = 'PC'\n                else:\n                    x = other_tag\n                return x\n\n            df['ticket_class'] = df['ticket_regex'].apply(lambda x: extract_ticket_class(x))\n            df.drop('ticket_match', axis=1, inplace=True)\n            df.drop('ticket_regex', axis=1, inplace=True)\n            \n        # Extraindo feature relacionada a classe da cabine\n        if self.cabin_class:\n            df['cabin_match'] = df['Cabin'].apply(lambda x: re.search(self.cabin_class_re, x) if x is not np.nan else np.nan)\n            df['cabin_regex'] = df['cabin_match'].apply(lambda x: x.group() if x is not np.nan else np.nan)\n            \n            # Definindo função para extração da classe\n            def extract_cabin_class(x):\n                if x in ['F', 'G', 'T']:\n                    x = 'FGT'\n                return x\n\n            df['cabin_class'] = df['cabin_regex'].apply(lambda x: extract_cabin_class(x))\n            df.drop('cabin_match', axis=1, inplace=True)\n            df.drop('cabin_regex', axis=1, inplace=True)\n            \n        # Extraindo feature relacionada ao tamanho do nome\n        if self.name_length:\n            df['name_length'] = df['Name'].apply(lambda x: len(x))\n            \n        # Extraindo feature relacionada a categoria de idade\n        if self.age_cat:\n            df['age_cat'] = pd.cut(df['Age'], bins=self.age_bins, labels=self.age_labels)\n        \n        # Extraindo feature relacionada a categoria de Fare\n        if self.fare_cat:\n            df['fare_cat'] = pd.cut(df['Fare'], bins=self.fare_bins, labels=self.fare_labels)\n            \n        # Extraindo feature relacionada a tamanho da família\n        if self.family_size:\n            df['family_size'] = df['Parch'] + df['SibSp'] + 1\n            \n        return df","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating object and applying transformation\nfeature_adder = CustomFeaturesTitanic(name_title=True, cabin_class=True, ticket_class=True)\ndf_custom = feature_adder.fit_transform(df)\ndf_custom.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After applying the transformation function, it would be interesting to see the results obtained from the new features created.","metadata":{}},{"cell_type":"code","source":"# New features\ncustom_features = ['name_title', 'ticket_class', 'cabin_class', 'age_cat', 'fare_cat', 'family_size']\nplot_multiple_countplots(df=df, col_list=custom_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After executing the `fit_transform()` method of the `feature_adder` object, it is possible to perceive, in the resulting base, the presence of the three new features previously considered. When training a predictive model, we will verify that these features are relevant to the modeling as a whole.\n\nIn addition, it is possible to analyze that, considering a predictive bias, not all columns of the base will be used to predict the survival of the ship's passengers. In practice, columns such as `Cabin`, `PassengerId`, `Ticket` and `Name`, for example, have no practical significance for predictive modeling, being responsible only for key passenger information or unique record indicators. Thus, as a next step, we will apply a feature selection process to choose only the candidate variables within a modeling context.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1.2\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.1.2 Candidate Features</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Having defined the variables present in the initial set of features (variable `INITIAL_FEATURES`), it is possible to use the `ColumnSelection()` class of the `mlcomposer.transformers` module to apply the feature selection process to pre-selected attributes.","metadata":{}},{"cell_type":"code","source":"# Importando classe\nfrom mlcomposer.transformers import ColumnSelection\n\n# Initial features\nTARGET = 'Survived'\nTO_DROP = ['PassengerId', 'Name', 'Ticket', 'Cabin']\nINITIAL_FEATURES = list(df.drop(TO_DROP, axis=1).columns)\n\n# Aplicando transformador\nselector = ColumnSelection(features=INITIAL_FEATURES)\ndf_slct = selector.fit_transform(df)\n\n# Resultados\nprint(f'Shape of original dataset: {df.shape}')\nprint(f'Shape of dataset after selection: {df_slct.shape}')\ndf_slct.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the resulting dimensions, it can be seen that the application of the FiltraColunas class resulted in the reduction of 4 columns from the original base.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1.3\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.1.3 Duplicated Data</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"The handling of duplicate data is an important step in preparing the basis for model training. This is because eliminating duplicates also means eliminating redundancy at the base, allowing machine learning models a faster convergence to local / global minimums.\n\nTo accomplish this task, the class DuplicatesDimplicates of the module pycomp.ml.transformers will be used, which, in turn, is responsible for simply eliminating duplicate records from a database passed as input.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.transformers import DropDuplicates\n\n# Applying transformer\ndup_dropper = DropDuplicates()\ndf_nodup = dup_dropper.fit_transform(df_slct)\n\n# Results\nprint(f'Total of duplicates before: {df_slct.duplicated().sum()}')\nprint(f'Total of duplicates after: {df_nodup.duplicated().sum()}')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.1.4\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.1.4 Modifying Dtypes</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"During the analyzes proposed in the exploratory phase of the project, it was possible to notice the presence of some numerical columns with categorical meaning at the base. In other words, they are columns that, in their origin, are persisted with numeric types but, in practice, represent categories related to some specific information as we can see in, for example, `Pclass`, `SibSp` and `Parch` (more specifically in `Pclass` than in the others and, at first, we can consider a first version working on the transformation only in the` Pclass` column).\n\nThus, an important step so that the preparation processes can be applied correctly, is the transformation of these numerical columns into strings.","metadata":{}},{"cell_type":"code","source":"# Importando classe\nfrom mlcomposer.transformers import DtypeModifier\n\n# Definições iniciais\ncat_custom_features = ['Pclass']\nmod_dict = {col: str for col in cat_custom_features}\nprint(f'Selected columns dtype before transformation:\\n')\nprint(df_nodup.dtypes[cat_custom_features])\n\n# Criando objeto e aplicando transformação\ndtype_mod = DtypeModifier(mod_dict=mod_dict)\ndf_mod = dtype_mod.fit_transform(df_nodup)\nprint(f'Selected columns dtype after transformation:\\n')\nprint(df_mod.dtypes[cat_custom_features])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.1.5\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.1.5 Training and Validation Data</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Finishing what we could call the initial pipeline of the project, we have an important step responsible for separating the database in training and testing. Thinking about a future modeling step, evaluating the result on different bases is extremely important to make decisions regarding the best practical solution to be put into production.\n\nFor this, we will use the SplitDados class also from the pycomp.ml.transformers module, which, in turn, applies this separation in the base and returns us with properly separated training and test data.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.transformers import DataSplitter\n\n# Applying transformer\nsplitter = DataSplitter(target='Survived')\nX_train, X_val, y_train, y_val = splitter.fit_transform(df_nodup)\n\n# Results\nprint(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of X_val: {X_val.shape}')\nprint(f'Shape of y_train: {y_train.shape}')\nprint(f'Shape of y_val: {y_val.shape}')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With that, we ended the first stage regarding the analysis of transformers to be applied in an initial pipeline of the base. The idea is to apply these common steps to the base as a whole.\n\nNext, specific pipelines will be built according to the primitive type of each column. Thus, we will have a numerical pipeline and a categorical pipeline that will compose the second transformation phase in the prep process.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.2 Numerical Pipeline</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Before starting the steps regarding the construction of a numerical pipeline, it is important to separate the attributes of our database according to their respective primitive types. For this, we will create two different lists containing, in each of them, the categorical and numeric columns of the base","metadata":{}},{"cell_type":"code","source":"# Splitting numerical features with categorical meaning\ncat_custom_features = ['Pclass']\n\n# Splitting features by dtype\nnum_features = list(X_train.select_dtypes(exclude=['object', 'category']).columns)\ncat_features = list(X_train.select_dtypes(include=['object', 'category']).columns)\n\n# Selecting datasets\nX_train_num = X_train[num_features]\nX_train_cat = X_train[cat_features]\n\nprint(f'Numerical features:\\n{num_features}')\nprint(f'\\nCategorical features:\\n{cat_features}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this way, we can then begin the steps of building specific transformers for the numerical and categorical types of the base.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2.1\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.2.1 Null Data</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"We saw, at the beginning of session 3. that some variables in the database have null data. It is necessary, in some way, to carry out the treatment of these data for a future insertion in predictive models. We will retrieve this analysis of the base overview, using now only the training data (attributes filtered and treated) in the initial pipeline.","metadata":{}},{"cell_type":"code","source":"# New overview\ndf_overview = data_overview(X_train)\nprint(f'Numerical data overview:')\ndata_overview(df=X_train).query('feature in @num_features')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The table above shows the need for handling null data in the Age column. Considering the centralization of transformations in numeric attributes in this session, we will consider filling in null data for the Age variable from sklearn's SimpleImputer class. As a statistical strategy, we can enter the median of this variable for completion.","metadata":{}},{"cell_type":"code","source":"# Importing library\nfrom sklearn.impute import SimpleImputer\n\n# Applying transformer\nimputer = SimpleImputer(strategy='median')\nX_train_num_imp = imputer.fit_transform(X_train_num)\nX_train_num_imp = pd.DataFrame(X_train_num_imp, columns=num_features)\n\n# Results\nprint(f'Null data before imputer: {X_train_num.isnull().sum().sum()}')\nprint(f'Null data after imputer: {X_train_num_imp.isnull().sum().sum()}')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! At first, we will not build other transformers besides filling in the null data in numeric attributes. In this way, we can start preparing the categorical data of the database","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2.2\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.2.2 Log Transformation</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"To validate the impact of the logarithmic transformation on candidate predictive models, we will optionally propose a step in the pipeline that applies this procedure to the numerical features present in the base. With this, we can validate whether the final performance of the model is sensitive to this type of transformation.","metadata":{}},{"cell_type":"code","source":"# Example\nlog_ex = 'Fare'\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(17, 7))\nplot_distplot(df=X_train_num, col=log_ex, ax=axs[0], hist=True,\n              title=f'Original {log_ex} Distribution')\n\ntmp_data = X_train_num.copy()\ntmp_data[log_ex] = tmp_data[log_ex].apply(lambda x: np.log1p(x))\nplot_distplot(df=tmp_data, col=log_ex, ax=axs[1], color='mediumseagreen', hist=True, \n              title=f'{log_ex} After Log Transformation')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Two highly relevant statistical measures for distribution analysis are `skew` and` kurtosis`. Through the [link](https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa) it is possible to have a clear idea on what each of these measures is and how to interpret continuous distributions through their values.\n\nThe logarithmic transformation helps to increase performance for distributions with positive skewness (asymmetric on the left). Thus, we will analyze the numerical features again and rank the main features with the opportunity for improvement through this type of transformation.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import skew, kurtosis\n\ntmp_ov = data_overview(df=X_train_num_imp)\ntmp_ov['skew'] = tmp_ov.query('feature in @num_features')['feature'].apply(lambda x: skew(X_train_num_imp[x]))\ntmp_ov['kurtosis'] = tmp_ov.query('feature in @num_features')['feature'].apply(lambda x: kurtosis(X_train_num_imp[x]))\ntmp_ov[~tmp_ov['skew'].isnull()].sort_values(by='skew', ascending=False).loc[:, ['feature', 'skew', 'kurtosis']]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The table above shows a list of features through their skewness and kurtosis measures of symmetry. In the code block below, we will execute the `DynamicLogTransformation` class, which, in turn, has the role of applying the logarithmic transformation in a database in a preparation pipeline. The advantage of this class is the previous definition of a list of features to which the transformation will be applied, which is defined by the user.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.transformers import DynamicLogTransformation\n\n# Setting parameters\nCOLS_TO_LOG = ['Fare']\nlog_tr = DynamicLogTransformation(num_features=num_features, cols_to_log=COLS_TO_LOG)\nX_train_num_ori = X_train_num_imp.copy()\nX_train_num_log = log_tr.fit_transform(X_train_num_imp)\n\n# Plotting results\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(17, 7))\n\nplot_distplot(df=df, col=COLS_TO_LOG[0], ax=axs[0], hist=True,\n              title=f'{COLS_TO_LOG[0]} Distribution $Before$ \\nLog Transformation')\nplot_distplot(df=X_train_num_log, col=COLS_TO_LOG[0], ax=axs[1], hist=True, color='mediumseagreen',\n              title=f'{COLS_TO_LOG[0]} Distribution $After$ \\nLog Transformation')    \n\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Additionally, it is worth mentioning that the class and `DynamicLogTransformation` have a Boolean attribute called `application` that can be used in the future for interactions in `GridSearch` or `RandomizedSearch`. Its objective is to enable performance analysis of models **with** or **without** the logarithmic transformation.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2.3\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.2.3 Normalization</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Another interesting way to apply a procedure that helps a given predictive model to converge to the optimal value more quickly is given by the `normalization` of the data. For the context of machine learning, it is possible to use ready-made sklearn classes, for example, `MinMaxScaler` or` StandardScaler`.\n\nThis type of standardization / normalization can optionally be applied directly to the numerical pipeline. Below, an example of how this transformation can be applied to our numerical database will be demonstrated.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.transformers import DynamicScaler\n\nscaler = DynamicScaler(scaler_type='Standard')\nX_train_num_scaled = scaler.fit_transform(X_train_num_log)\nX_train_num_scaled = pd.DataFrame(X_train_num_scaled, columns=num_features)\nX_train_num_scaled.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With that, we ended the preparation steps in the numerical pipeline of the project. In the future, we will consolidate each of these _steps_ into a single preparation block using the `sklearn` class `Pipeline`. As next steps, let's look at the categorical part of the set.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.3\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.3 Categorical Pipeline</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"For this session, we know that there is already a need to fill in null data from the Embarked column. In the analysis carried out in section 3.2.1 above, we saw that there are only 2 null records for the column that informs the port of departure of each passenger.\n\nAlthough relatively irrelevant, we will use the same SimpleImputer applied previously, but with a strategy that takes into account the most common entry present in the column as a way to fill in nulls.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.3.1\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.3.2 Encoding</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"An extremely important step involving categorical attributes of the database is the application of a process known as data encoding. The importance of this step is due to the inability of most machine learning models to read categorical entries from a database provided as input. This is because, in the backgorund of the most common models, several numerical calculations are performed in order to minimize a cost function and reach a minimum error, which in fact is impossible in the presence of categorical variables.\n\nFor this, the encoding process exists to apply a kind of \"encoding\" to categorical data, often separating them into different columns, one for each different entry. In this context, we can use the class DummiesEncoding () of the pycomp.ml.transformers module, which, in turn, is responsible for applying this process automatically.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.transformers import DummiesEncoding\n\n# Applying transformer\nencoder = DummiesEncoding(cat_features_ori=cat_features, dummy_na=True)\nX_train_cat_enc = encoder.fit_transform(X_train_cat)\n\n# Results\nprint(f'Shape before encoding: {X_train_cat.shape}')\nprint(f'Shape after encoding: {X_train_cat_enc.shape}')\nX_train_cat_enc.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.4\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.4 Complete Pipelines</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After going through 3 large blocks involving the configuration and application of specific classes for the preparation of our database, it is possible to consolidate these steps in native objects from sklearn's Pipeline class.\n\nIn this session, we will build pipelines capable of consolidating all the steps detailed so far into single objects of preparation. Such objects will be extremely important in the reproduction of this flow and also in the standardized application of the steps in new data to be received (test data, for example). So, considering the previous topics, we will build:\n\n* **_initial_pipeline:_** pipeline responsible for receiving a raw database and applying transformations common to all attributes;\n* **_prep_pipeline:_** pipeline responsible for consolidating the transformations applied to numerical and categorical data in a single object.","metadata":{}},{"cell_type":"code","source":"# Importing libraries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\n\n# Setting global variables\nTARGET = 'Survived'\n\nINITIAL_FEATURES = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'name_title', \n                    'ticket_class', 'cabin_class', 'name_length', 'age_cat', 'fare_cat', 'family_size']\nINITIAL_PRED_FEATURES = [col for col in INITIAL_FEATURES if col not in TARGET]\n\nDTYPE_MODIFICATION_DICT = {'Pclass': str}\n\nNUM_FEATURES = ['Age', 'SibSp', 'Parch', 'Fare', 'name_length', 'family_size']\nCAT_FEATURES = ['Pclass', 'Sex', 'Embarked', 'name_title', 'ticket_class', 'cabin_class', 'age_cat', 'fare_cat']\n\nCAT_FEATURES_FINAL = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Pclass_nan', 'Sex_female', 'Sex_male', 'Sex_nan',\n                      'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Embarked_nan', 'name_title_Master.', \n                      'name_title_Miss.', 'name_title_Mr.', 'name_title_Mrs.', 'name_title_OTHER',\n                      'name_title_nan', 'ticket_class_A', 'ticket_class_OTHER', 'ticket_class_PC', \n                      'ticket_class_SC', 'ticket_class_STON_SOTON', 'ticket_class_nan', 'cabin_class_A', \n                      'cabin_class_B', 'cabin_class_C', 'cabin_class_D', 'cabin_class_E', 'cabin_class_FGT', \n                      'cabin_class_nan', 'age_cat_0_10', 'age_cat_10_20', 'age_cat_20_40', 'age_cat_40_60',\n                      'age_cat_greater_60', 'age_cat_nan', 'fare_cat_0_8', 'fare_cat_15_25', \n                      'fare_cat_25_50', 'fare_cat_8_15', 'fare_cat_greater_50', 'fare_cat_nan']\n\nMODEL_FEATURES = NUM_FEATURES + CAT_FEATURES_FINAL\n\n# Variables for numerical pipeline\nNUM_STRATEGY_IMPUTER = 'median'\nSCALER_TYPE = None\nLOG_APPLICATION = False\nCOLS_TO_LOG = ['Fare', 'Age']\n\n# Variables for categorical pipeline\nENCODER_DUMMY_NA = True\nNAME_TITLE = True\nCABIN_CLASS = True\nTICKET_CLASS = True\nNAME_LENGTH = True\nAGE_CAT = True\nFARE_CAT = True\nFAMILY_SIZE = True\n\n# Building initial pipelines (train and prediction)\ninitial_train_pipeline = Pipeline([\n    ('feature_adder', CustomFeaturesTitanic(name_title=NAME_TITLE, cabin_class=CABIN_CLASS, \n                                            ticket_class=TICKET_CLASS, name_length=NAME_LENGTH,\n                                            age_cat=AGE_CAT, fare_cat=FARE_CAT, family_size=FAMILY_SIZE)),\n    ('col_filter', ColumnSelection(features=INITIAL_FEATURES)),\n    ('dtype_modifier', DtypeModifier(mod_dict=DTYPE_MODIFICATION_DICT)),\n    ('dup_dropper', DropDuplicates())\n])\n\ninitial_pred_pipeline = Pipeline([\n    ('feature_adder', CustomFeaturesTitanic(name_title=NAME_TITLE, cabin_class=CABIN_CLASS, \n                                            ticket_class=TICKET_CLASS, name_length=NAME_LENGTH,\n                                            age_cat=AGE_CAT, fare_cat=FARE_CAT, family_size=FAMILY_SIZE)),\n    ('col_filter', ColumnSelection(features=INITIAL_PRED_FEATURES)),\n    ('dtype_modifier', DtypeModifier(mod_dict=DTYPE_MODIFICATION_DICT))\n])\n\n# Building a numerical pipeline\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy=NUM_STRATEGY_IMPUTER)),\n    ('log_transformer', DynamicLogTransformation(application=LOG_APPLICATION, num_features=NUM_FEATURES, \n                                                 cols_to_log=COLS_TO_LOG)),\n    ('scaler', DynamicScaler(scaler_type=SCALER_TYPE))\n])\n\n# Building a categorical pipeline\ncat_pipeline = Pipeline([\n    ('encoder', DummiesEncoding(dummy_na=ENCODER_DUMMY_NA, cat_features_final=CAT_FEATURES_FINAL))\n])\n\n# Building a complete pipeline\nprep_pipeline = ColumnTransformer([\n    ('num', num_pipeline, NUM_FEATURES),\n    ('cat', cat_pipeline, CAT_FEATURES)\n])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading raw data\ndf = pd.read_csv(os.path.join(DATA_PATH, TRAIN_FILENAME))\n\n# Executing initial training pipeline\ndf_prep = initial_train_pipeline.fit_transform(df)\n\n# Splitting data into training and testing\nX_train, X_val, y_train, y_val = train_test_split(df_prep.drop(TARGET, axis=1), df_prep[TARGET].values,\n                                                  test_size=.20, random_state=42)\n\n# Executing preparation pipeline\nX_train_prep = prep_pipeline.fit_transform(X_train)\nX_val_prep = prep_pipeline.fit_transform(X_val)\n\n# Results\nprint(f'Shape of X_train_prep: {X_train_prep.shape}')\nprint(f'Shape of X_val_prep: {X_val_prep.shape}')\nprint(f'\\nTotal features considered: {len(MODEL_FEATURES)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perfect! At this point in the project, we are ready to start the predictive modeling stage in search of building a model capable of returning the Titanic passengers' probability of survival against the variables considered for analysis. Before diving into the topics related to survival prediction, let's use the final base prepared to plot a correlation matrix in relation to the model's target variable (`Survived`). The purpose of this study is to have a prior idea of the most important variables present in the database.","metadata":{}},{"cell_type":"code","source":"# Preparing a final DataFrame after transformation\ndf_prep = pd.DataFrame(X_train_prep, columns=MODEL_FEATURES)\ndf_prep['Survived'] = y_train\n\n# Plotting a correlation matrix\nplot_corr_matrix(df=df_prep, corr_col='Survived', figsize=(12, 12), cbar=False, n_vars=15)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_corr_matrix(df=df_prep, corr='negative', corr_col='Survived', figsize=(12, 12), cbar=False, n_vars=15)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>4. Modeling: Predicting Survival</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Finally, the long-awaited session where we finally conducted the training of machine learning models in search of the best algorithm capable of returning the probability of survival of the ship's passengers and crew against the wreck.\n\nTo reach this stage, we went through a dense exploratory analysis session that helped us greatly in gathering insights and in a complete understanding of the available database, in addition to a rich session where it was possible to build definitive pipelines for preparing the dataset.\n\nIn this way, we can separate the modeling step into:\n\n1. **_Initial definitions_**, where we will define the fundamental blocks for the beginning of the training, as for example, the models used and their respective search hyperparameters;\n2. **_Training_**, where we will apply, in fact, the training of the models selected in the previous step. For this, we will use, once again, powerful features of the pycomp package from its pycomp.ml.trainer module;\n3. **_Evaluation_**, where, after due training of the candidate models, we will analyze the individual performances of each one against the proposed prediction task.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.1\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.1 Structuring Variables</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"As mentioned earlier, this is the stage where we prepare the structures for the start of training. Here, the objective is to import and prepare the classification models to be used to predict the survival of passengers.","metadata":{}},{"cell_type":"code","source":"# Importing models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\n# Model objects\ndtree = DecisionTreeClassifier()\nforest = RandomForestClassifier()\nlgbm = LGBMClassifier()\nxgb = XGBClassifier()\nadaboost = AdaBoostClassifier()\ngradboost = GradientBoostingClassifier()\n\n# Creating set_classifiers dict\nmodel_obj = [dtree, forest, lgbm, xgb, adaboost, gradboost]\nmodel_names = [type(model).__name__ for model in model_obj]\nset_classifiers = {name: {'model': obj, 'params': {}} for (name, obj) in zip(model_names, model_obj)}\n\nprint(f'Classifiers that will be trained on next steps: \\n\\n{model_names}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.2 Training Models</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Once the modeling structure has been prepared from specific objects, such as the set_classifiers dictionary, it is now possible to import the ClassifierBinary class present in the pycomp.ml.trainer module to perform all training and evaluation of the candidate models.\n\nThis class was developed in order to greatly facilitate the work of the analyst / scientist in terms of implementing codes to train, evaluate and optimize predictive models for binary classification. Its methods include powerful features that perform various actions with just one call.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.trainer import BinaryClassifier\n\n# Creating an object and starting training\ntrainer = BinaryClassifier()\ntrainer.fit(set_classifiers, X_train_prep, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `fit()` method of the created trainer object is responsible for training the models encapsulated in the `set_classifiers` dictionary created in the initial definitions stage.\n\nBy configuring the method to also apply the `RandomizedSearchCV` process (random search for the best hyperparameters of each algorithm), it is possible to build models optimized according to the search space passed in the `set_classifiers` dictionary.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.3\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.3 Evaluating Performance</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Once the candidate models are trained through the `fit()` method, it is then possible to evaluate the performance obtained in each case, thus returning the main classification metrics capable of indicating the best direction for the given task.\n\nTo perform this process, we can use the `evaluate_performance()` or `plot_metrics()` methods of the trainer object. In the first case, the return is an analytical DataFrame containing the result of the evaluation of each model against the main metrics. In the second case, the return is a visual analysis of the metrics for each of the models.","metadata":{}},{"cell_type":"code","source":"# Analytical of training results\nmetrics = trainer.evaluate_performance(X_train_prep, y_train, X_val_prep, y_val)\nmetrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As mentioned earlier, the `evaluate_performance()` method returns an analytical table containing the performance of each model (in training and testing) for the main metrics for evaluating classification models. From this table, it is possible to point out that, in terms of accuracy, the `RandomForest` model performed slightly better than the others, despite the high time required to perform the calculations.\n\nThinking about setting, in fact, an optimization objective to choose the best predictive model, let's consider the accuracy as the metric to be used for this decision. Another way to analyze the performance of candidate models is from the `plot_metrics()` method. Its result can be seen below:","metadata":{}},{"cell_type":"code","source":"# Graphical analysis of performance\ntrainer.plot_metrics()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Graphically, it is possible to visualize the superiority of the `RandomForest` model in terms of accuracy. In the first line, there is a boxplot analysis considering the `k` folds used in the cross validation and, from there, it is possible to visualize the dispersion of each evaluation round for each candidate model. In the second line of the plot, there is an analysis of the average per metric and per candidate model.","metadata":{}},{"cell_type":"markdown","source":"___\n* **_Most relevant features_**\n___","metadata":{}},{"cell_type":"markdown","source":"By further improving the analysis of the results, it is possible to return the importance that each feature of the base had in the final result of the prediction. Let's look at the most important features considered by each model from the `plot_feature_importance()` method","metadata":{}},{"cell_type":"code","source":"# Feature importances\ntrainer.plot_feature_importance(features=MODEL_FEATURES)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing the method log, it is noticed that the LogisticRegression model does not have the `feature_importances_()` method and, thus, it is impossible to calculate and return this front to this model. Then considering the `DecisionTrees` and `RandomForest` models, the bar graphs above indicate the main features of greatest relevance for predicting the survival of passengers to the sinking of the Titanic.","metadata":{}},{"cell_type":"markdown","source":"___\n* **_Confusion matrix_**\n___","metadata":{}},{"cell_type":"markdown","source":"In binary classification models, it is common to perform individual analyzes on True Positives, True Negatives, False Positives and False Negatives, that is, in basically all the indicators that make up the main classification metrics previously viewed. For this, the tool used is the confusion matrix, which, in turn, aims to consolidate all these indicators in a visual matrix format.\n\nTo perform this analysis, it is possible to execute the trainer object's `plot_confusion_matrix()` method. The result will be given by two confusion matrices for each model used (one for the training data and the other using the test data). Let's see:","metadata":{}},{"cell_type":"code","source":"# Plotting confusion matrix for the models\ntrainer.plot_confusion_matrix()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n* **_ROC Curve_**\n___","metadata":{}},{"cell_type":"markdown","source":"In some cases, datasets with an unbalanced target variable, that is, an extremely small amount of records for a given class, may present high biases for the classical metrics for the evaluation of classification models. Thus, a possible solution is to analyze the results in terms of `score` or` probability` instead of fixed metrics.\n\nIn these cases, it is possible to analyze the ROC curve of the models to see if they are performing well in view of the business objective to be achieved. For that, we can execute the `plot_roc_curve()` method of the `trainer` object.","metadata":{}},{"cell_type":"code","source":"# Plotting ROC Curve\ntrainer.plot_roc_curve()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing the above training and test curves, as well as the metric `roc_auc` (Area Under the Curve) in the legend of each series, it is noticed that the` RandomForest` and `LogisticRegression` models present a better performance in relation to the model `DecisionTrees`. In the future, if the metric `roc_auc` becomes the optimization goal for this task, we can take the above analysis into consideration to decide the best algorithm.","metadata":{}},{"cell_type":"markdown","source":"___\n* **_Probabilities Distribution_**\n___","metadata":{}},{"cell_type":"markdown","source":"Complementing the analysis and comments above, in some cases it is extremely relevant to observe how the scores (or probabilities) of the models are distributed in relation to the whole. Such an analysis is important to verify whether the classifiers are, in fact, separating the positive and negative classes well, thus providing a clear view of the possible errors in each score range returned.\n\nThis view can be returned using the `plot_score_distribution()` method of the `trainer` object.","metadata":{}},{"cell_type":"code","source":"# Plotting score distribution\ntrainer.plot_score_distribution()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The curves above represent the distribution of the scores of each of the trained models, considering the training and test data. The break by the positive and negative classes present in the base (`y = 1` and` y = 0`) indicates how each algorithm performed the separation of the scores and how good each one can be in this differentiation considering the probability of each class between 0 and 1.\n\nIn practice, the curves are read by the density of elements of each of the classes when the probability score is in the ranges determined by the x-axis. Exemplifying from the curves for the `RandomForest` model (last line of the figure), it is possible to notice that, when the probability returned by the model is between 0 and 0.4, there is a high density of elements of the negative class (blue curve), the which is a good sign. On the other hand, when the model returns high probabilities, the density of elements of the positive class is extremely higher (red curve).","metadata":{}},{"cell_type":"markdown","source":"___\n* **_Probabilities Distribution on Bins_**\n___","metadata":{}},{"cell_type":"markdown","source":"Another way to analyze the distribution of the probability score of the models is through their separation into specific analysis ranges. Thus, it is possible to analyze the volumetry of each score range considering the different classes present in the base, opening the possibility to verify, in fact, if the models are separating the classes in a discrete approach.\n\nFor this, we can use the `plot_score_bins()` method of the `trainer` object as shown below:","metadata":{}},{"cell_type":"code","source":"# Plotting score distribution on bins\ntrainer.plot_score_bins()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen in the continuous distribution previously, the purpose of this band analysis is also to verify that the models are properly separating classes from bands. Performing the result rule for the `RandomForest` model (bottom line of the figure), there is an increase in the volume of records belonging to the positive class (`y = 1`) according to the growth of the x-axis bands, thus indicating that high scores are, in general, related to positive class (expected scenario).","metadata":{}},{"cell_type":"markdown","source":"___\n* **_Learning Curves_**\n___","metadata":{}},{"cell_type":"markdown","source":"An extremely powerful tool to check if the models used suffer from some kind of problem related to bias or variance (underfitting or overfitting) are the \"learning curves\". With them, it is possible to verify the evolution of the errors obtained by the models, in training and in validation, from specific numbers of samples used for such calculations.\n\nIn practice, a sample evolution \"step\" is defined and, sequentially, the model is trained using an N + \"step\" number of samples and their respective errors are computed in training and validation. The result are evolution curves showing the error behavior related to the increase in the number of samples used in the calculation. To perform this analysis, we have the `plot_learning_curve()` method of the `trainer` object.","metadata":{}},{"cell_type":"code","source":"# Plotting learning curves\ntrainer.plot_learning_curve()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing the curves, it is possible to conclude that the models do not suffer from serious overfitting problems, given that the error in the training and validation data is very similar at the end of the 500 samples. To analyze other possible scenarios, the reference link can be very useful: https://www.dataquest.io/blog/learning-curves-machine-learning/","metadata":{}},{"cell_type":"markdown","source":"___\n* **_Shap Analysis_**\n___","metadata":{}},{"cell_type":"markdown","source":"Finally, concluding the performance analysis block of the trained models, there is the `shap` analysis as a powerful way to analyze the impact of each of the base features on a given predictive model. Unlike the feature importances analysis performed earlier, the shap analysis allows you to view the impact of features according to their absolute value. In other words, it can be seen whether a high (or extremely low) value for a given feature can significantly impact the model's output.\n\nFor this rich analysis, we can use the `plot_shap_analysis()` method of the `trainer` object, passing, as main argument, the name of a model already trained by the class and present in the `classifiers_info` attribute.","metadata":{}},{"cell_type":"code","source":"# Shap Analysis\ntrainer.plot_shap_analysis(model_name='LGBMClassifier', features=MODEL_FEATURES)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As mentioned earlier, the shap analysis allows to cross the value of each feature and its respective in the final answer of the model. As a reading example, the result obtained for the variable `Sex_male` indicates that the higher its value (red spot), the less its impact on the model result (left side of the x axis). In a more technical and summarized analysis, it is possible to say that the higher the value of `Sex_male` (that is, the closer to the male gender), the less the value of the probability (low chance of survival).\n\nSimilarly, the reading for the variable `Sex_female` indicates the opposite: female passengers had a greater chance of survival, which, in fact, was investigated and surveyed in the exploratory phase of the project.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.4\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.4 Training Flow and Visual Analysis</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Throughout the entire project, and especially in this modeling session, the powerful tools of the `mlcomposer` package were used, thus allowing a quick and rich analysis of candidate models, from training to performance evaluation and several other relevant parameters for a more assertive decision on the best model for a given task.\n\nThroughout sections 4.2 and 4.2, each method of the `mlcomposer.trainer` module was executed individually in order to provide a specific analysis on each of the fronts involved. In practical terms, it is not always desirable to separate the analysis individually for each of the available methods and graphs, but to perform all analyzes at once.\n\nWith that in mind, two methods responsible for carrying out all the steps above in a consolidated manner were built within the classifierBinary class. They are: `training_flow()` and `visual_analysis()`. The big difference is due to the need to inform only a few essential arguments so that training, evaluation and generation of analysis graphs can be carried out. Results can be saved in specific user-defined directories.\n\nIn the cell below, we will simulate the execution of these two methods.","metadata":{}},{"cell_type":"code","source":"# Creating a new object\nfull_trainer = BinaryClassifier()\n\n# Training and evaluating models all at once\nfull_trainer.training_flow(set_classifiers, X_train_prep, y_train, X_val_prep, y_val, \n                           features=MODEL_FEATURES, random_search=True, scoring='accuracy', n_jobs=3)\n\n# Generating visual analysis for the models all at once\nfull_trainer.visual_analysis(features=MODEL_FEATURES, model_shap='LGBMClassifier')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After executing the methods `training_flow()` and `visual_analysis()` specified above, there is a series of generated outputs that can be accessed in the future for a more detailed analysis.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>5. Hyperparameters Tunning</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After a long exploratory and preparation journey in the database available for this task, we have enough inputs to decide between some candidate models in order to propose an improvement in their performances through the tuning of hyperparameters.\n\nSo far, training has been carried out considering a 'default' configuration in all algorithms, which, in fact, does not allow extracting the full potential of the model. In this stage, we will separate some candidate models that presented reasonable performance in this initial stage or that, in a way, may present a promising proposal after tuning hyperparameters. After this initial analysis, search dictionaries will be defined considering specific hyperparameters for each of these pre-selected models. At the end, a complete analysis will be proposed, through `RandomizedSearchCV` to find the best combination among all possible combinations of the candidate models.\n\nThus, we will then review the results obtained by the candidate models.","metadata":{}},{"cell_type":"code","source":"# Model metrics\nmetrics = pd.read_csv('output/metrics/metrics.csv')\nmetrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In a general balance, considering the `accuracy` as an optimization objective, it is possible to consider the `RandomForest`, `LightGBM` and `XGBoost` models as good candidates for tuning hyperparameters. In practice, there is a good possibility of improving performance for these models and, thus, we will build some blocks capable of assisting us in this search and improvement.\n\nThe first step to be taken is the definition of dictionaries containing the search hyperparameters for each of the selected models. With this dictionary in hand, we will build a new pipeline with:\n\n1. Data preparation with `prep_pipeline`;\n2. Application of feature selection with `FeatureSelection`;\n3. Model training\n\nThus, it will be possible to use the hyperparameter dictionaries to apply a random search in order to return the best possible combination, be it in relation to the modeling parameters or the base preparation parameters (such as the imputer strategy, the logarithm transformation application, the number of features to be considered, among others).","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"# Parâmetros de busca pro modelo RandomForest\nforest_tunning_grid = {\n    'bootstrap': [True, False],\n    'class_weight': [None, 'balanced'],\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [5, 6, 7, 9, 10],\n    #'max_features': [None, 'auto', 'sqrt', 'log2'],\n    #'max_leaf_nodes': np.arange(3, 50, 2),\n    #'min_impuriti_decrease': np.linspace(0, 1, 50),\n    #'min_samples_leaf': np.arange(1, 100, 1),\n    #'min_samples_split': np.arange(2, 100, 1),\n    #'min_weight_fraction_leaf': np.linspace(0, 1, 50),\n    'n_estimators': np.arange(300, 600, 50),\n    #'oob_score': [True, False],\n    'random_state': [42]\n}\n\n# Parâmetros de busca pro modelo LightGBM\nlgbm_tunning_grid = {\n    'boosting_type': ['gbdt'],\n    'class_weight': [None, 'balanced'],\n    #'colsample_bytree': np.linspace(.5, 1, 50),\n    #'importance_type': ['split', 'gain'],\n    'learning_rate': [0.003, 0.01, 0.03, 0.1, 0.3, 1, 3],\n    'max_depth': np.arange(-1, 100, 2),\n    #'min_child_samples': np.arange(10, 50, 1),\n    #'min_child_weight': np.linspace(1e-4, 1, 100),\n    'n_estimators': np.arange(300, 700, 50),\n    'num_leaves': [5, 10, 15, 20],\n    'objective': ['binary'],\n    'random_state': [42],\n    'reg_alpha': np.linspace(.0, 25, 15)\n}\n\n# Parâmetros de busca pro modelo XGBoost\nxgboost_tunning_grid = {\n    #'base_score': np.linspace(0.3, 0.7, 50),\n    'booster': ['gbtree'],\n    'max_depth': [3, 4, 6, 7],\n    'learning_rate': [0.003, 0.01, 0.03, 0.1, 0.3, 1, 3],\n    'n_estimators': np.arange(300, 700, 50),\n    'objective': ['binary:logistic'],\n    'seed': [42],\n    'reg_alpha': np.linspace(.0, 25, 15),\n    'reg_lambda': np.linspace(.0, 25, 15),\n    'colsample_bylevel': [0.5, 0.7, 0.9]\n}\n\n# Parâmetros de busca pro modelo AdaBoost\nadaboost_tunning_grid = {\n    'base_estimator': [DecisionTreeClassifier(max_depth=7)],\n    'n_estimators': np.arange(50, 700, 50),\n    'learning_rate': [0.003, 0.01, 0.03, 0.1, 0.3, 1, 3],\n    'algorithm': ['SAMME', 'SAMME.R'],\n    'random_state': [42]\n}\n\n# Parâmetros de busca pro modelo GradientBoost\ngradboost_tunning_grid = {\n    'n_estimators': np.arange(50, 700, 50),\n    'max_depth': [3, 6, 7, 8, 9, 10, 15],\n    'max_features': [None, 'auto', 'sqrt', 'log2'],\n    'max_leaf_nodes': np.arange(3, 50, 2),\n    #'min_impuriti_decrease': np.linspace(0, 1, 50),\n    #'min_samples_leaf': np.arange(1, 100, 1),\n    #'min_samples_split': np.arange(2, 100, 1),\n    #'min_weight_fraction_leaf': np.linspace(0, 1, 50),\n    'random_state': [42]\n}","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom datetime import datetime\n\ndef clf_cv_performance(model, X, y, cv=5, model_name=None):\n    \"\"\"\n    Função responsável por calcular as principais métricas de um modelo de classificação\n    utilizando validação cruzada\n    \n    Parâmetros\n    ----------\n    :param model: estimator do modelo preditivo [type: estimator]\n    :param X: dados de entrada do modelo [type: np.array]\n    :param y: array de target do modelo [type: np.array]\n    :param cv: número de k-folds utilizado na validação cruzada [type: int, default=5]\n        \n    Retorno\n    -------\n    :return df_performance: DataFrame contendo as principais métricas de classificação [type: pd.DataFrame]\n    \n    Aplicação\n    ---------\n    results = clf_cv_performance(model=model, X=X, y=y)\n    \"\"\"\n\n    # Computing metrics using cross validation\n    t0 = time.time()\n    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n    precision = cross_val_score(model, X, y, cv=cv, scoring='precision').mean()\n    recall = cross_val_score(model, X, y, cv=cv, scoring='recall').mean()\n    f1 = cross_val_score(model, X, y, cv=cv, scoring='f1').mean()\n\n    # Probas for calculating AUC\n    try:\n        y_scores = cross_val_predict(model, X, y, cv=cv, method='decision_function')\n    except:\n        # Tree based models don't have 'decision_function()' method, but 'predict_proba()'\n        y_probas = cross_val_predict(model, X, y, cv=cv, method='predict_proba')\n        y_scores = y_probas[:, 1]\n    auc = roc_auc_score(y, y_scores)\n\n    # Creating a DataFrame with metrics\n    t1 = time.time()\n    delta_time = t1 - t0\n    train_performance = {}\n    if model_name is None:\n        train_performance['model'] = model.__class__.__name__\n    else:\n        train_performance['model'] = model_name\n    train_performance['approach'] = 'Final Model'\n    train_performance['acc'] = round(accuracy, 4)\n    train_performance['precision'] = round(precision, 4)\n    train_performance['recall'] = round(recall, 4)\n    train_performance['f1'] = round(f1, 4)\n    train_performance['auc'] = round(auc, 4)\n    train_performance['total_time'] = round(delta_time, 3)\n    df_performance = pd.DataFrame(train_performance, index=train_performance.keys()).reset_index(drop=True).loc[:0, :]\n\n    # Adding information of measuring and execution time\n    cols_performance = list(df_performance.columns)\n    df_performance['anomesdia'] = datetime.now().strftime('%Y%m%d')\n    df_performance['anomesdia_datetime'] = datetime.now()\n    df_performance = df_performance.loc[:, ['anomesdia', 'anomesdia_datetime'] + cols_performance]\n\n    return df_performance","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing libraries\nfrom mlcomposer.transformers import FeatureSelection\n#from pycomp.ml.trainer import clf_cv_performance\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Preparing variables for a complete search\ntunning_models_keys = ['RandomForestClassifier', 'LGBMClassifier', 'XGBClassifier', 'AdaBoostClassifier',\n                       'GradientBoostingClassifier']\ntunning_param_grids = [forest_tunning_grid, lgbm_tunning_grid, xgboost_tunning_grid, adaboost_tunning_grid,\n                       gradboost_tunning_grid]\ntunned_pipelines = {}\ngeneral_metrics = pd.DataFrame({})\npipe_model_key = 'model'\n\n# Preparing dataset for training (train + validation)\nX = X_train.append(X_val)\ny = np.concatenate((y_train, y_val))\n\n# Iterating over each model and param grid\nfor model_name, param_grid in zip(tunning_models_keys, tunning_param_grids):\n    \n    # Returning informations about the model\n    baseline_model = trainer.get_estimator(model_name)\n    feature_importance = baseline_model.feature_importances_\n    general_metrics = general_metrics.append(trainer.get_metrics(model_name))\n    \n    # Creating a pipeline for preparation and prediction\n    tunning_pipeline = Pipeline([\n        ('prep', prep_pipeline),\n        ('selector', FeatureSelection(feature_importance, k=len(MODEL_FEATURES))),\n        (pipe_model_key, baseline_model)\n    ])\n    \n    # Preparing model grid for joining with pipeline grid\n    model_param_grid = {pipe_model_key + '__' + k: v for k, v in param_grid.items()}\n    tunning_param_grid = {\n        'prep__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n        'prep__num__log_transformer__application': [True, False],\n        'prep__num__scaler__scaler_type': [None, 'Standard', 'MinMax'],\n        'selector__k': np.arange(5, len(MODEL_FEATURES) + 1, 2)\n    } \n    tunning_param_grid.update(model_param_grid)\n    \n    # Applying search with RandomizedSearch\n    tunning_search = RandomizedSearchCV(tunning_pipeline, tunning_param_grid, scoring='accuracy', cv=5,\n                                        n_jobs=-1, verbose=-1, random_state=42)\n    tunning_search.fit(X, y)\n    print(f'\\nBest hyperparameters for {model_name} found by RandomizedSearch:\\n')\n    for k, v in tunning_search.best_params_.items():\n        print(f'{k}: {v}')\n\n    # Returning objects for the best combination found\n    final_pipeline = tunning_search.best_estimator_\n    tunned_pipelines[model_name] = final_pipeline\n    \n    # Computing metrics with the best combination pipeline\n    final_metrics = clf_cv_performance(final_pipeline, X, y, model_name=model_name)\n    model_metrics = trainer.get_metrics(model_name=model_name)\n    metrics_cols = general_metrics.columns\n    final_metrics = final_metrics.loc[:, metrics_cols]\n    general_metrics = general_metrics.append(final_metrics)\n    \n# Final result\ngeneral_metrics","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, in view of the best combinations obtained for each of the models and the resulting final metrics, it is possible to make a more assertive decision about the final model to be considered for the survival forecast in the official test base. Looking at the metrics dataset, it can be seen that, in practically all cases, there was an increase in the final performance of the candidate models after applying the search for hyperparameters. Another relevant point is that, in this case, the training base was used completely (without the separation between training and validation).\n\nAs a final model, we will select the `RandomForestClassifier` due to the good accuracy obtained in the cross validation. In the future, it is possible to consider other more robust algorithms to verify a possible improvement in performance.","metadata":{}},{"cell_type":"code","source":"# Returning final pipeline\nFINAL_MODEL = 'RandomForestClassifier'\nfinal_pipeline = tunned_pipelines[FINAL_MODEL]","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>6. Submitting: Prediction Pipeline</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Finally, it is then possible to read the official test base available for final validation of the predictive model. The goal is to simulate the execution of the data pre-processing and consumption pipelines of the trained model. Additionally, it is possible to study the possibility of consolidating a single pipeline capable of receiving a validation base, executing the prep pipelines and consuming the final model, thus generating a base containing the score and other parameters that may be included and necessary in the outlet base.","metadata":{}},{"cell_type":"code","source":"# Reding test dataset\ndf_test = pd.read_csv(os.path.join(DATA_PATH, TEST_FILENAME))\nprint(f'Shape of test dataset: {df_test.shape}')\ndf_test.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After reading the official test base, we will import the `ModelResults` class, which, in turn, is responsible for receiving the information from an entry base and enriching it with the results of a predictive model already trained. Once the best combination final pipeline obtained with the `Random Forest` model is returned, it is possible to provide this object as an input parameter for the consumption class of the model, thus opening the possibility of creating a definitive prediction pipeline to be applied in databases of test.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.transformers import ModelResults\n\n# Creating object and building a prediction pipeline\nmodel_consumer = ModelResults(model=final_pipeline, features=INITIAL_PRED_FEATURES)\n\nprediction_pipeline = Pipeline([\n    ('initial', initial_pred_pipeline),\n    ('prediction', model_consumer)\n])\n\n# Executing pipeline\ndf_pred = prediction_pipeline.fit_transform(df_test)\ndf_pred.head()","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a last step, we will prepare the final prediction base to submit the results in the Kaggle competition.","metadata":{}},{"cell_type":"code","source":"# Preparing submission dataset\ndf_sub = df_test.merge(df_pred, how='left', left_index=True, right_index=True)\ndf_sub = df_sub.loc[:, ['PassengerId', 'y_pred']]\ndf_sub.columns = ['PassengerId', 'Survived']\ndf_sub.to_csv('output/submission.csv', index=False)\ndf_sub.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>7. References</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"One great inspiration for creating custom features on this project came from the excelent notebook from Awwal Malhi: https://www.kaggle.com/awwalmalhi/titanic-eda-and-feature-engineering\n\nGERON, A.Hands-On Machine Learning with Scikit-Learn and TensorFlow. [S.l.]:O’Reilly, 2017. ISBN 978149196229.\n\nNG, A.Machine Learning. Available at: <https://www.coursera.org/learn/machine-learning/home/welcome>","metadata":{}},{"cell_type":"markdown","source":"Please tell me what do you think about `xplotter` and `mlcomposer` packages and leave here a comment or a upvote. Your opinion is really important and I'm really excited to show you new implementations on those packages.\n\n* **xplotter on Github:** https://github.com/ThiagoPanini/xplotter\n* **xplotter on PyPI:** https://pypi.org/project/xplotter/\n\n\n* **mlcomposer on Github:** https://github.com/ThiagoPanini/mlcomposer\n* **mlcomposer on PyPI:** https://pypi.org/project/mlcomposer/\n___\n\n<font size=\"+1\" color=\"black\"><b>You can also visit my other kernels by clicking on the buttons</b></font><br>\n\n<a href=\"https://www.kaggle.com/thiagopanini/presenting-xplotter-and-mlcomposer-on-tps-may21\" class=\"btn btn-primary\" style=\"color:white;\">TPS May 2021</a>\n<a href=\"https://www.kaggle.com/thiagopanini/pycomp-exploring-and-modeling-housing-prices\" class=\"btn btn-primary\" style=\"color:white;\">Housing Prices</a>\n<a href=\"https://www.kaggle.com/thiagopanini/predicting-restaurant-s-rate-in-bengaluru\" class=\"btn btn-primary\" style=\"color:white;\">Bengaluru's Restaurants</a>\n<a href=\"https://www.kaggle.com/thiagopanini/sentimental-analysis-on-e-commerce-reviews\" class=\"btn btn-primary\" style=\"color:white;\">Sentimental Analysis E-Commerce</a>","metadata":{}}]}