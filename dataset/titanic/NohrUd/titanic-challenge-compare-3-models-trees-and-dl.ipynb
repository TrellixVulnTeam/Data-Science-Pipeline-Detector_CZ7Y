{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<HR>","metadata":{}},{"cell_type":"markdown","source":"## Let's get started with Titanic Machine Learning Competition by using <font color=\"Green\">XGBoost</font> and <font color=\"Blue\">Random Forest Classifier</font> and <font color=\"gold\">Deep Learning</font>. Which one is better in this case?","metadata":{}},{"cell_type":"markdown","source":"<HR>","metadata":{}},{"cell_type":"markdown","source":"### Step1 : Preparation for prediction","metadata":{}},{"cell_type":"code","source":"#import libraries\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport xgboost as xgb\nimport warnings\nwarnings.simplefilter('ignore')\n#read train data and test data\ndata = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-14T15:34:38.168109Z","iopub.execute_input":"2022-03-14T15:34:38.168436Z","iopub.status.idle":"2022-03-14T15:34:38.187863Z","shell.execute_reply.started":"2022-03-14T15:34:38.168407Z","shell.execute_reply":"2022-03-14T15:34:38.187045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2022-03-14T15:54:35.186391Z","iopub.execute_input":"2022-03-14T15:54:35.186731Z","iopub.status.idle":"2022-03-14T15:54:35.211504Z","shell.execute_reply.started":"2022-03-14T15:54:35.186695Z","shell.execute_reply":"2022-03-14T15:54:35.210557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.bar(data.query('Survived==1').loc[:,[\"Sex\"]])","metadata":{"execution":{"iopub.status.busy":"2022-03-14T15:57:56.805558Z","iopub.execute_input":"2022-03-14T15:57:56.805912Z","iopub.status.idle":"2022-03-14T15:57:56.873738Z","shell.execute_reply.started":"2022-03-14T15:57:56.805877Z","shell.execute_reply":"2022-03-14T15:57:56.872808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"graph2=px.scatter(data.query('Survived==1').loc[:,[\"Age\"]])\ngraph2","metadata":{"execution":{"iopub.status.busy":"2022-03-14T15:40:03.89727Z","iopub.execute_input":"2022-03-14T15:40:03.897588Z","iopub.status.idle":"2022-03-14T15:40:03.966091Z","shell.execute_reply.started":"2022-03-14T15:40:03.897557Z","shell.execute_reply":"2022-03-14T15:40:03.965323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data[\"Name\"].str.contains(\"Masa\")]","metadata":{"execution":{"iopub.status.busy":"2022-03-14T15:56:57.297798Z","iopub.execute_input":"2022-03-14T15:56:57.298165Z","iopub.status.idle":"2022-03-14T15:56:57.312107Z","shell.execute_reply.started":"2022-03-14T15:56:57.298131Z","shell.execute_reply":"2022-03-14T15:56:57.311188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step2 : Remodel datasets.¶","metadata":{}},{"cell_type":"code","source":"#fill missing values drastically \ndata = data.fillna(method='bfill')\ndata = data.fillna(method='ffill')\ntest = test.fillna(method='bfill')\ntest = test.fillna(method='ffill')\n#make LabeleEncoder into le as an instance\nle = LabelEncoder()\n#LabelEncode the row named \"Sex\"\nle = le.fit(data['Sex'])\ndata['Sex'] = le.transform(data['Sex'])\ntest['Sex'] = le.transform(test['Sex'])\n#Change Cabin numbers like C-100 ,A-200 into A , C.  \ndata['Cabin']=data['Cabin'].str[0:1]\ntest['Cabin']=test['Cabin'].str[0:1]\n#Encode 3 columns into categorical features. \nle = LabelEncoder()\nle = le.fit(data['Sex'])\ndata['Sex'] = le.transform(data['Sex'])\ntest['Sex'] = le.transform(test['Sex'])\nle = le.fit(data['Cabin'])\ndata['Cabin'] = le.transform(data['Cabin'])\ntest['Cabin'] = le.transform(test['Cabin'])\nle = le.fit(data['Embarked'])\ndata['Embarked'] = le.transform(data['Embarked'])\ntest['Embarked'] = le.transform(test['Embarked'])","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:26:50.324568Z","iopub.execute_input":"2021-10-02T14:26:50.324944Z","iopub.status.idle":"2021-10-02T14:26:50.352153Z","shell.execute_reply.started":"2021-10-02T14:26:50.324909Z","shell.execute_reply":"2021-10-02T14:26:50.351022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train= data.iloc[:,[2,4,5,6,7,9,10,11]].astype(\"int64\")\ny_train= data.iloc[:,[1]].astype(\"int64\")","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:26:51.082086Z","iopub.execute_input":"2021-10-02T14:26:51.082402Z","iopub.status.idle":"2021-10-02T14:26:51.08944Z","shell.execute_reply.started":"2021-10-02T14:26:51.082371Z","shell.execute_reply":"2021-10-02T14:26:51.088609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:26:51.69507Z","iopub.execute_input":"2021-10-02T14:26:51.695386Z","iopub.status.idle":"2021-10-02T14:26:51.711783Z","shell.execute_reply.started":"2021-10-02T14:26:51.695355Z","shell.execute_reply":"2021-10-02T14:26:51.710692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:26:52.360114Z","iopub.execute_input":"2021-10-02T14:26:52.360439Z","iopub.status.idle":"2021-10-02T14:26:52.370212Z","shell.execute_reply.started":"2021-10-02T14:26:52.360407Z","shell.execute_reply":"2021-10-02T14:26:52.369382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step3: Check the correlation in the dataset.","metadata":{}},{"cell_type":"code","source":"check_df=pd.concat([x_train,y_train],axis=1)\ncheck_df.corr()","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:26:53.618943Z","iopub.execute_input":"2021-10-02T14:26:53.619271Z","iopub.status.idle":"2021-10-02T14:26:53.640513Z","shell.execute_reply.started":"2021-10-02T14:26:53.619241Z","shell.execute_reply":"2021-10-02T14:26:53.639416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Ummm. I decided to use whole columns.¶","metadata":{}},{"cell_type":"code","source":"x_test=test.iloc[:,[1,3,4,5,6,8,9,10]]\nx_test","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:26:55.052853Z","iopub.execute_input":"2021-10-02T14:26:55.053326Z","iopub.status.idle":"2021-10-02T14:26:55.087526Z","shell.execute_reply.started":"2021-10-02T14:26:55.053279Z","shell.execute_reply":"2021-10-02T14:26:55.086414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normalize data from 0 to 1 by using StandardScaler\nx_train = StandardScaler().fit_transform(x_train)\nx_test = StandardScaler().fit_transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:26:55.762634Z","iopub.execute_input":"2021-10-02T14:26:55.763067Z","iopub.status.idle":"2021-10-02T14:26:55.784318Z","shell.execute_reply.started":"2021-10-02T14:26:55.763027Z","shell.execute_reply":"2021-10-02T14:26:55.783357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step4 :Let's fit this remodeled dataset into 3 models.","metadata":{}},{"cell_type":"code","source":"# XGBoost Regressor\nxgbm = xgb.XGBRegressor(max_depth=50)\nxgbm.fit(x_train, y_train)\nmodel_score1=xgbm.score(x_train, y_train)\nprint(\"The score on traing data of this machine learning model is \",model_score1,\"!\")","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:26:57.128739Z","iopub.execute_input":"2021-10-02T14:26:57.129061Z","iopub.status.idle":"2021-10-02T14:26:57.305064Z","shell.execute_reply.started":"2021-10-02T14:26:57.12903Z","shell.execute_reply":"2021-10-02T14:26:57.304112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's fit and predict.\nforest = RandomForestClassifier(n_estimators=200, random_state=2, max_depth=300)\nforest.fit(x_train, y_train)\nmodel_score2=forest.score(x_train, y_train)\nprint(\"The score on traing data of this machine learning model is \",model_score2,\"!\")","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:26:57.919924Z","iopub.execute_input":"2021-10-02T14:26:57.920274Z","iopub.status.idle":"2021-10-02T14:26:58.351146Z","shell.execute_reply.started":"2021-10-02T14:26:57.920239Z","shell.execute_reply":"2021-10-02T14:26:58.350279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_i=pd.DataFrame([\"Random Forest\",\"XGBoost\"])\ncompare_c=pd.DataFrame([model_score2,model_score1])\npd.concat([compare_i,compare_c],axis=1)\n             ","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:26:58.74783Z","iopub.execute_input":"2021-10-02T14:26:58.748151Z","iopub.status.idle":"2021-10-02T14:26:58.759859Z","shell.execute_reply.started":"2021-10-02T14:26:58.748121Z","shell.execute_reply":"2021-10-02T14:26:58.758965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Then,Random Forest Classifier is better?!<HR>","metadata":{}},{"cell_type":"markdown","source":"#### How about Deep Learning models? Okay, give it a try. ","metadata":{}},{"cell_type":"markdown","source":"<HR>","metadata":{}},{"cell_type":"code","source":"#import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras as kr\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Activation, Flatten,BatchNormalization, Activation,MaxPool2D,Dropout\nfrom tensorflow.keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:27:02.774172Z","iopub.execute_input":"2021-10-02T14:27:02.774552Z","iopub.status.idle":"2021-10-02T14:27:02.782993Z","shell.execute_reply.started":"2021-10-02T14:27:02.774515Z","shell.execute_reply":"2021-10-02T14:27:02.781894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(y_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:27:04.1484Z","iopub.execute_input":"2021-10-02T14:27:04.148792Z","iopub.status.idle":"2021-10-02T14:27:04.154356Z","shell.execute_reply.started":"2021-10-02T14:27:04.14876Z","shell.execute_reply":"2021-10-02T14:27:04.153409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = x_train.astype('float32')/255\ny_train = kr.utils.to_categorical(y_train, 2)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:27:05.04785Z","iopub.execute_input":"2021-10-02T14:27:05.048194Z","iopub.status.idle":"2021-10-02T14:27:05.05288Z","shell.execute_reply.started":"2021-10-02T14:27:05.048147Z","shell.execute_reply":"2021-10-02T14:27:05.051823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Build an ordinary \"Deep Learning\" model with CNN and maxpooling by using Keras.\nmodel3 = Sequential()\nmodel3.add(Dense(512, input_dim=x_train.shape[1], activation='relu'))\nmodel3.add(Dense(256, activation='relu'))\nmodel3.add(Dense(128, activation='relu'))\nmodel3.add(Dense(64, activation='relu'))\nmodel3.add(Dense(32, activation='relu'))\nmodel3.add(Dropout(0.5))\nmodel3.add(Dense(2, activation='sigmoid'))\nmodel3.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nmodel3.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:27:06.08523Z","iopub.execute_input":"2021-10-02T14:27:06.085697Z","iopub.status.idle":"2021-10-02T14:27:06.196558Z","shell.execute_reply.started":"2021-10-02T14:27:06.085652Z","shell.execute_reply":"2021-10-02T14:27:06.193726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model fitting\ndl_result1=model3.fit(x_train, y_train,batch_size=50, epochs=20)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:28:13.755604Z","iopub.execute_input":"2021-10-02T14:28:13.755946Z","iopub.status.idle":"2021-10-02T14:28:14.773711Z","shell.execute_reply.started":"2021-10-02T14:28:13.755913Z","shell.execute_reply":"2021-10-02T14:28:14.772939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the deep learning model by using two metrics, loss and accuracy.\nmetrics = ['accuracy']\n#show the evaluation result by using matoplot.\nplt.figure(figsize=(10, 5))\n#Use \"For Loop\".\nfor i in range(len(metrics)):\n    metric = metrics[i]\n    #set subplots to show the result\n    plt.subplot(1, 2, i+1)\n    #Titles of subplots are \"loss\" and \"accuracy\"\n    plt.title(metric) \n    plt_result1 = dl_result1.history[metric] \n    #plot them all\n    plt.plot(plt_result1, label='Deep Learning model') \n    plt.legend() \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:28:18.786466Z","iopub.execute_input":"2021-10-02T14:28:18.786845Z","iopub.status.idle":"2021-10-02T14:28:18.926386Z","shell.execute_reply.started":"2021-10-02T14:28:18.786814Z","shell.execute_reply":"2021-10-02T14:28:18.925646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Deep Learning model works a little bit in this case. \n### Anyway, Random Forest Classifier is the best model in 3 models. ","metadata":{"execution":{"iopub.status.busy":"2021-07-10T22:28:08.707726Z","iopub.execute_input":"2021-07-10T22:28:08.708047Z","iopub.status.idle":"2021-07-10T22:28:08.757219Z","shell.execute_reply.started":"2021-07-10T22:28:08.708017Z","shell.execute_reply":"2021-07-10T22:28:08.756458Z"}}},{"cell_type":"markdown","source":"<HR>","metadata":{}},{"cell_type":"markdown","source":"### Step5 :Let's make a prediction!?","metadata":{}},{"cell_type":"code","source":"y_pred=forest.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:22:43.149777Z","iopub.status.idle":"2021-10-02T14:22:43.150335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred2= pd.DataFrame(y_pred)\ny_pred2[\"Survived\"]= y_pred2[0]\ny_pred2=y_pred2.iloc[:,[1]].astype(\"int\")\ny_pred2\n","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:22:43.151753Z","iopub.status.idle":"2021-10-02T14:22:43.15231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step6 :Let's compile a submission file.","metadata":{}},{"cell_type":"code","source":"#Connect test data and prediction data \nresult=pd.concat([test,y_pred2],axis=1)\nresult=result.iloc[:,[0,11]]\nresult","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:22:43.153695Z","iopub.status.idle":"2021-10-02T14:22:43.154254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#make an output csv file as submission.csv\nresult.to_csv('submission_rfc.csv', index=False)\nprint('submission_file was saved!')","metadata":{"execution":{"iopub.status.busy":"2021-10-02T14:22:43.155727Z","iopub.status.idle":"2021-10-02T14:22:43.156298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<HR>","metadata":{}},{"cell_type":"markdown","source":"\n#### Thanks for reading my notebook. Feel free to comment :-)","metadata":{}},{"cell_type":"markdown","source":"<HR>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}