{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"for the first timr when i started data science i took this dataste just becase of it is related to **Titanic**...:)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"The Objective of this notebook is to give an idea how is the workflow in any predictive modeling problem. How do we check features, how do we add new features and some Machine Learning Concepts. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it."},{"metadata":{},"cell_type":"markdown","source":"**Load the DataSet :**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=pd.read_csv(\"../input/titanic/train.csv\")\ntest_df=pd.read_csv(\"../input/titanic/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature analysis Feature engineering Modeling are the three main imp works we have "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets understand all features in this \n\nsurvival Survival 0 = No, 1 = Yes\n\npclass Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n\nsex Male or Female\n\nAge Age in years\n\nsibsp # of siblings / spouses aboard the Titanic\n\nparch # of parents / children aboard the Titanic\n\nticket Ticket number\n\nfare Passenger fare\n\ncabin Cabin number\n\nembarked Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\nVariable Notes pclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fiancÃ©s were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missingdata(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n    ms=pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    ms= ms[ms[\"Percent\"] > 0]\n    f,ax =plt.subplots(figsize=(8,6))\n    plt.xticks(rotation='90')\n    fig=sns.barplot(ms.index, ms[\"Percent\"],color=\"green\",alpha=0.8)\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title('Percent missing data by feature', fontsize=15)\n    return ms","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets  handel the missing missing values "},{"metadata":{"trusted":true},"cell_type":"code","source":"missingdata(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missingdata(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Age'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Fare'].fillna(test_df['Fare'].median(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_column = ['Cabin']# with more missing values \ntrain_df.drop(drop_column, axis=1, inplace = True)\ntest_df.drop(drop_column,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Age'].fillna(test_df['Age'].median(), inplace = True)\ntrain_df['Age'].fillna(train_df['Age'].median(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('check the nan value in train data')\nprint(train_df.isnull().sum())\nprint('___'*30)\nprint('check the nan value in test data')\nprint(test_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## combine test and train as single to apply some function\nall_data=[train_df,test_df]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in all_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in all_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in all_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', \n                                                 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create bin for age features\nfor dataset in all_data:\n    dataset['Age_bin'] = pd.cut(dataset['Age'], bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create bin for fare features\nfor dataset in all_data:\n    dataset['Fare_bin'] = pd.cut(dataset['Fare'], bins=[0,7.91,14.45,31,120], labels=['Low_fare','median_fare',\n                                                                                      'Average_fare','high_fare'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### for our reference making a copy of both DataSet start working for copy of dataset\ntraindf=train_df\ntestdf=test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_dat=[traindf,testdf]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in all_dat:\n    drop_column = ['Age','Fare','Name','Ticket']\n    dataset.drop(drop_column, axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_column = ['PassengerId']\ntraindf.drop(drop_column, axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf = pd.get_dummies(traindf, columns = [\"Sex\",\"Title\",\"Age_bin\",\"Embarked\",\"Fare_bin\"],\n                             prefix=[\"Sex\",\"Title\",\"Age_type\",\"Em_type\",\"Fare_type\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdf = pd.get_dummies(testdf, columns = [\"Sex\",\"Title\",\"Age_bin\",\"Embarked\",\"Fare_bin\"],\n                             prefix=[\"Sex\",\"Title\",\"Age_type\",\"Em_type\",\"Fare_type\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets see how it varys "},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.pairplot(data=train_df, hue='Survived', palette = 'seismic',\n                 size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as it is a classification model we have so many ways to fit it but lets chose the best one "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split #for split the data\nfrom sklearn.metrics import accuracy_score  #for accuracy_score\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nall_features = traindf.drop(\"Survived\",axis=1)\nTargeted_feature = traindf[\"Survived\"]\nX_train,X_test,y_train,y_test = train_test_split(all_features,Targeted_feature,test_size=0.3,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LogisticRegression **"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression # Logistic Regression\n\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\nprediction_lr=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Logistic Regression is',round(accuracy_score(prediction_lr,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_lr=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Logistic REgression is:',round(result_lr.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Support Vector Machines******"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC, LinearSVC\n\nmodel = SVC()\nmodel.fit(X_train,y_train)\nprediction_svm=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Support Vector Machines Classifier is',round(accuracy_score(prediction_svm,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_svm=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Support Vector Machines Classifier is:',round(result_svm.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nmodel= DecisionTreeClassifier(criterion='gini', \n                             min_samples_split=10,min_samples_leaf=1,\n                             max_features='auto')\nmodel.fit(X_train,y_train)\nprediction_tree=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the DecisionTree Classifier is',round(accuracy_score(prediction_tree,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_tree=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Decision Tree classifier is:',round(result_tree.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nmodel= GradientBoostingClassifier()\nmodel.fit(X_train,y_train)\nprediction_gbc=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Gradient Boosting Classifier is',round(accuracy_score(prediction_gbc,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_gbc=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoostClassifier is:',round(result_gbc.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(criterion='gini', n_estimators=700,\n                             min_samples_split=10,min_samples_leaf=1,\n                             max_features='auto',oob_score=True,\n                             random_state=1,n_jobs=-1)\nmodel.fit(X_train,y_train)\nprediction_rm=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Random Forest Classifier is',round(accuracy_score(prediction_rm,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_rm=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Random Forest Classifier is:',round(result_rm.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines','Logistic Regression',  \n              'Gradient Decent','Decision Tree','Random forest'],\n    'Score': [result_svm.mean(),result_lr.mean(),result_gbc.mean(), result_tree.mean(),result_rm.mean()]})\nmodels.sort_values(by='Score',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it seem good but we need somthing more .. ok then lets use we use **Hyper-Parameters Tuning** i.e the machine learning models are like a Black-Box. There are some default parameter values for this Black-Box, which we can tune or change the learning rate of the algorithm and get a better model. This is known as Hyper-Parameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = traindf.drop(\"Survived\", axis=1)\ntrain_Y=traindf[\"Survived\"]\ntest_X  = testdf.drop(\"PassengerId\", axis=1).copy()\ntrain_X.shape, train_Y.shape, test_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel = GradientBoostingClassifier()\nparam_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300,400],\n              'learning_rate': [0.1, 0.05, 0.01,0.001],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.2,0.1] \n              }\n\nmodelf = GridSearchCV(model,param_grid = param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\nmodelf.fit(train_X,train_Y)\nmodelf.best_score_\nmodelf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelf.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we are with the same .. \n\nlets check with svm"},{"metadata":{"trusted":true},"cell_type":"code","source":"model= SVC()\nparam_grid = {'kernel': ['rbf','linear'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\nmodelsvm = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nmodelsvm.fit(train_X,train_Y)\n\nprint(modelsvm.best_estimator_)\n\n# Best score\nprint(modelsvm.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier()\nn_estim=range(100,1000,100)\nparam_grid = {\"n_estimators\" :n_estim}\nmodel_rf = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nmodel_rf.fit(train_X,train_Y)\n\nprint(model_rf.best_score_)\nmodel_rf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forests\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nrandom_forest.fit(train_X, train_Y)\nY_pred_rf = random_forest.predict(test_X)\nrandom_forest.score(train_X,train_Y)\nacc_random_forest = round(random_forest.score(train_X, train_Y) * 100, 2)\n\nprint(\"Important features\")\npd.Series(random_forest.feature_importances_,train_X.columns).sort_values(ascending=True).plot.barh(width=0.8)\nprint('__'*30)\nprint(acc_random_forest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred_rf})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**If you like this notebook just upvote which can keep me movitvation ...:)**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}