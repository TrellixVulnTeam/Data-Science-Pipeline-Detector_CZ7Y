{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#                            Titanic Dataset \n**Hello and welcome üëã to titanic project. If you have recently been learning about data analysis, then this is the post you need for your journey. If you searched for exercises to practice on data analysis or data cleaning stuff, most of your searches will take you to either one of these datasets.**\n\n\n\n\n**If you browse the dataset page on kaggle you will notice that the page gives information about the details of the passengers aboard the titanic and a column on survival of the passengers. Those who survived are represented as ‚Äú1‚Äù while those who did not survive are represented as ‚Äú0‚Äù. The goal of this exercise is to determine if with the other features/information about the passengers it is possible to determine those who are likely to survive.**\n\n**We would divide the processing of this dataset into 4 parts:-**\n\n**1.Preprocessing**\n\n**2.EDA**\n\n**3.Feature Extraction**\n\n**4.Modelling**\n\n![](https://fort-russ.com/wp-content/uploads/2016/05/obama-economy-jobs-debt-deficit-political-cartoon-titanic-jobs-plan.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\n\n# Modeling\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing:-"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Loading Dataset\n\ntrain=pd.read_csv('/kaggle/input/titanic/train.csv')  # Loading the train dataset\ntest=pd.read_csv('/kaggle/input/titanic/test.csv')    # Loading the test dataset\n\ntarget=train['Survived']\n\n#detect outliers\n\ndef detect_outlier(df,n,cols):\n    outlier_indices = []\n    for i in cols:\n        Q1 = np.percentile(df[i], 25)\n        Q3 = np.percentile(df[i], 75)\n        IQR = Q3 - Q1\n        outlier_step = 1.5*IQR\n        outlier_index_list = df[(df[i] < Q1-outlier_step) | (df[i] > Q3+outlier_step)].index\n        outlier_indices.extend(outlier_index_list)\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(k for k,v in outlier_indices.items() if v>n)  \n    return multiple_outliers\n\noutliers_to_drop = detect_outlier(train,3,['Age', 'SibSp', 'Parch', 'Fare'])\ntrain = train.drop(outliers_to_drop, axis = 0).reset_index(drop=True)\n\n#Basic info about the dataset\n\nprint('Shape of train dataset:-',train.shape)\nprint('Shape of test dataset:-' ,test.shape)\n\n#Info about datatype and statistical model\n\nprint('\\n')\nprint(train.info())\ntrain.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combining both train and test dataset\ntotal=pd.concat([train.drop('Survived',axis=1),test])\ntarget=train['Survived']\n\ntotal.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA(Exploratory Data Analysis):-"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.heatmap(total.drop('PassengerId',axis=1).corr(),annot=True) #we are seeing the correlation of column with each other:-\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pclass vs Survival rate\nsns.catplot(x='Pclass',y='Survived',data=train,kind='bar',hue='Sex')\nprint(train.groupby(['Pclass','Sex'])['Survived'].mean()*100,'\\n','\\nIf women is from 1st and 2nd class then it has a lot chance to survive about 92%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Embarked vs Survived\nsns.catplot(x='Embarked',y='Survived',data=train,kind='bar',hue='Sex')\nprint(train.groupby(['Embarked','Sex'])['Survived'].mean())\nprint(train.groupby(['Embarked','Sex','Pclass'])['Survived'].mean())\nprint('If a female is from Southmpton it has 87% chance of survival and\\n if she is from Q it has 75% chance of survival')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SibSp vs Survival\nsns.catplot(x='SibSp',y='Survived',data=train,kind='bar')\nprint(train.groupby('SibSp')['Survived'].mean(),'\\n\\nFamily with more than 2 sibling has less chance to survive')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['Age'].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Manupulation and Extraction:-"},{"metadata":{},"cell_type":"markdown","source":"**Removing null values:-**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(total.isnull().sum())\n\ntotal['Age'] = total.groupby('Pclass')['Age'].transform(lambda x: x.fillna(x.median()))  #removing null value of Age with help of pclass\ntotal['Fare'] = total.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median())) #removing null value of Fare with help of pclass\ntotal['Embarked'].fillna('S',inplace=True)  #removing null value of Embarked with most common S\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Handling Categorical values:-**"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder=LabelEncoder()\ntotal['Sex']=encoder.fit_transform(total['Sex'])\ntotal['Embarked']=encoder.fit_transform(total['Embarked'])\ntotal=pd.get_dummies(total,columns=['Pclass','Embarked'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total['Fare_1_S']=total['Embarked_2']*total['Pclass_1']*total['Sex']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Building new features:-**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extracting Title from name\ntotal['Title'] =total['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntotal['Title'] =total['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntotal['Title'] =total['Title'].replace('Mlle', 'Miss')\ntotal['Title'] =total['Title'].replace('Ms', 'Miss')\ntotal['Title'] =total['Title'].replace('Mme', 'Mrs')\n#Mapping titles to numerical data\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 2, \"Master\": 3, \"Rare\": 4}\ntotal['Title'] =total['Title'].map(title_mapping)\ntotal['Title'] =total['Title'].fillna(0)\n\n\n#Extracting common ages into group\ntotal['Age_cat'] = pd.qcut(total['Age'],q=[0, .16, .33, .49, .66, .83, 1], labels=False, precision=1)\n\n#Fare group\ndef fare_category(fr): \n    if fr <= 7.91:\n        return 1\n    elif fr <= 14.454 and fr > 7.91:\n        return 2\n    elif fr <= 31 and fr > 14.454:\n        return 3\n    return 4\ntotal['Fare_cat'] =total['Fare'].apply(fare_category) \n\n\n#Family group\ntotal['FamilySize'] =total['SibSp'] + total['Parch'] + 1\ntotal['FamilySize_cat'] =total['FamilySize'].map(lambda x: 1 if x == 1 \n                                                            else (2 if 5 > x >= 2 \n                                                                  else (3 if 8 > x >= 5 \n                                                                       else 4 )    \n                                                                 ))   \n\n#Other columns to make \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dropping Useless columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"total.drop(['Name','Ticket','Cabin'],axis=1,inplace=True)\n\n#Dummy variable\ntotal=pd.get_dummies(total,columns=['SibSp','Parch','Age_cat','Title','FamilySize','Fare_cat','FamilySize_cat'])\n\ntotal['Age']=total['Age'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=total[:len(train)]\ntest=total[len(train):]\n\n\nnp.random.seed(42)\n# Split into train & test set\nX_train, X_test, y_train, y_test = train_test_split(train,target, test_size = 0.25) \n\n\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(max_iter=10000), \n          \"Random Forest\": RandomForestClassifier(),\n          \"SVC\" : SVC(probability=True),\n          \"DecisionTreeClassifier\" : DecisionTreeClassifier(),\n          \"AdaBoostClassifier\" : AdaBoostClassifier(),\n          \"GradientBoostingClassifier\" : GradientBoostingClassifier(),\n          \"GaussianNB\" : GaussianNB(),\n          \"LinearDiscriminantAnalysis\" : LinearDiscriminantAnalysis(),\n          \"QuadraticDiscriminantAnalysis\" : QuadraticDiscriminantAnalysis()}\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n\n    # Random seed for reproducible results\n    np.random.seed(42)\n    # Make a list to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Predicting target values\n        y_pred = model.predict(X_test)\n        # Evaluate the model and append its score to model_scores\n        #model_scores[name] = model.score(X_test, y_test)\n        model_scores[name] = roc_auc_score(y_pred, y_test)\n    return model_scores\nmodel_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\nmodel_scores\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nleaks = {\n897:1,\n899:1, \n930:1,\n932:1,\n949:1,\n987:1,\n995:1,\n998:1,\n999:1,\n1016:1,\n1047:1,\n1083:1,\n1097:1,\n1099:1,\n1103:1,\n1115:1,\n1118:1,\n1135:1,\n1143:1,\n1152:1, \n1153:1,\n1171:1,\n1182:1,\n1192:1,\n1203:1,\n1233:1,\n1250:1,\n1264:1,\n1286:1,\n935:0,\n957:0,\n972:0,\n988:0,\n1004:0,\n1006:0,\n1011:0,\n1105:0,\n1130:0,\n1138:0,\n1173:0,\n1284:0,\n}\n\nmodel=GradientBoostingClassifier()\nmodel.fit(train,target)\nsub = pd.DataFrame()\nsub['PassengerId'] = test['PassengerId']\nsub['Survived'] = model.predict(test)\nsub['Survived'] = sub['Survived'].apply(lambda x: 1 if x>0.8 else 0)\nsub['Survived'] = sub.apply(lambda r: leaks[int(r['PassengerId'])] if int(r['PassengerId']) in leaks else r['Survived'], axis=1)\nsub.to_csv('sub_titan.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**If you liked the approach,do upvote it,and thank you for visiting..**\n\n\n**Lets end with a smile**\n![](https://th.bing.com/th/id/OIP.3Wb7emYEtt9RhdPdaK_uQgHaHa?w=183&h=183&c=7&o=5&dpr=1.25&pid=1.7)"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}