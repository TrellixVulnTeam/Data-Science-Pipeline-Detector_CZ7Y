{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"RMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early hours of 15 April 1912, after colliding with an iceberg during her maiden voyage from Southampton to New York City. There were an estimated 2,224 passengers and crew aboard, and more than 1,500 died, making it one of the deadliest commercial peacetime maritime disasters in modern history. RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line.","metadata":{}},{"cell_type":"markdown","source":"\n<img src=\"https://miro.medium.com/max/750/0*wKmr2Sffqkr9FimI.gif\" alt=\"vac\" border=\"0\"></a>","metadata":{}},{"cell_type":"markdown","source":"The “Titanic: Machine Learning from Disaster” is a classical problem for beginners in Machine Learning.\nThe challenge is to predict, based on a set of training data, which people would survive the disaster and which would not. Obviously, this is just a challenge to try discover a correlation between the features of the people who survived and not and use it to make predictions.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Understanding problem/ Discovering Data\nIn this notebook, we will build a predictive model that is going to predict what sort of people were more likely to survive in the shipwreck Titatic. The data is provided to us in 2 sets : train dataset and test set.","metadata":{}},{"cell_type":"code","source":"#titanic_features = pd.read_csv('train.csv')\ndf = pd.read_csv(\"../input/titanic/train.csv\")\ndf_test = pd.read_csv(\"../input/titanic/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis:\nThe Beginning When you download the data from the Kaggle, they make available 2 datasets. One corresponds to train, with data to train your model, this dataset already have the answer of who survived and who not. And before you ask me, I don’t know if this is true. But we have some problems in this data …","metadata":{}},{"cell_type":"markdown","source":"# Checking missing values in the dataset","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Embarked feature has only 2 missing values, which can easily be filled. It will be much more tricky, to deal with the ‘Age’ feature, which has 177 missing values. The ‘Cabin’ feature needs further investigation.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(df.isnull(), yticklabels = False, cmap=\"YlGnBu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/660/0*aF7XRizN9CFij6cK.gif\" alt=\"vac\" border=\"0\"></a>","metadata":{}},{"cell_type":"markdown","source":"We have few data for this problem. But that doesn’t mean we can’t solve the problem!","metadata":{}},{"cell_type":"markdown","source":"# Features:\n+ PassengerId: Unique Id of a passenger\n+ Survival: Survival\n+ Pclass: Ticket class\n+ Name:\n+ Sex: Sex\n+ Age: Age in years\n+ Sibsp: # of siblings / spouses aboard the Titanic\n+ Parch: # of parents / children aboard the Titanic\n+ Ticket: Ticket number\n+ Fare: Passenger fare\n+ Cabin: Cabin number\n+ Embarked: Port of Embarkation\n\nFrom the table above, we can note a few things:\n+ We have a few categorical variabes that need to be either converted to numerical or one-hot encoded, so that the machine learning algorithms can process them.\n+ The features have widely different ranges, and we will need to convert into roughly the same scale.\n+ Some features contain missing values (NaN = not a number), that we need to deal with.","metadata":{}},{"cell_type":"code","source":"df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the observation above, we already see Age and Cabin have numbers of missing data. Depends on the dataset we would consider how to deal with these missing data. Since our dataset is not a big data, dropping columns might not be a good idea in this case but also depends on how much that data meaningful contribute to our dataset. Now, check in detail ...","metadata":{}},{"cell_type":"markdown","source":"Overall look on Survived data we have in our training set as we know the main point of this analyzation is about who were more likely to survive and who were not.","metadata":{}},{"cell_type":"code","source":"sns.set_style('ticks')\nsns.countplot(x='Survived', data = df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Usign countplot helps us to see the overall of Survived / Not survived and as we can see the number of people who were not survived is higher than people survived :(","metadata":{}},{"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.countplot(x='Survived', hue='Sex', data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see one more feature, Pclass and see how Pclass tells us about who were survived or not.","metadata":{}},{"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.countplot(x='Survived', hue='Pclass', data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So obviously we could give a unfailry conclude here that people in 1st class were likely have more chance to survive and there were many people in the 3rd class died.","metadata":{}},{"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.countplot(x='SibSp', data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\ntemp = df.groupby(by=\"Cabin\").count()\nname = temp.Survived.index\nval = temp.Survived.values\n\nfig = px.scatter_polar(temp, r=val, theta=name,color=name, symbol=val, \n                       size=val,color_discrete_sequence=px.colors.sequential.Plasma_r, title='Survived by Cabin')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nt0 = df.iloc[(df[\"Survived\"]==0).values]\nt1 = df.iloc[(df[\"Survived\"]==1).values]\n\ntemp0 = t0.groupby(by=\"Embarked\").count()\nname0 = temp0.Survived.index\nval0 = temp0.Survived.values\n\ntemp1 = t1.groupby(by=\"Embarked\").count()\nname1 = temp1.Survived.index\nval1 = temp1.Survived.values\n\nfig = px.scatter_polar(r=val1+val0, theta=name1,color=np.round(val1/(val0+val1)*100,0), \n                       symbol=val0, size=np.round(val1/(val0+val1)*100,0),\n                       color_discrete_sequence=px.colors.sequential.Plasma_r, title='Survived by Cabin')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+ Over 72% of the passengers embarked from the port ‘Southampton’, 18% from the port ‘Cherbourg’ and the rest from the port ‘Queenstown’.\n+ Passengers from port ‘Southampton’ have a low survival rate of 34%, while those from the port ‘Cherbourg’ have a survival rate of 55%.","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nman = df.iloc[((df[\"Survived\"]==1)&(df[\"Sex\"]==\"male\")).values]\nwoman = df.iloc[((df[\"Survived\"]==1)&(df[\"Sex\"]==\"female\")).values]\n\nM = man.groupby(by=\"Embarked\").count()\nname = M.Survived.index\nMn = M.Survived.values\n\nF = woman.groupby(by=\"Embarked\").count()\nFn = F.Survived.values\n\nfig = px.scatter_polar(r=Mn + Fn, theta=name,color=np.round(Mn/(Mn+Fn)*100,0), \n                       symbol=Fn, size=np.round(Fn/(Mn+Fn)*100,0),\n                       color_discrete_sequence=px.colors.sequential.Plasma_r, title='Survived by Cabin')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+ Women have a survival rate of 65%, while men have a survival rate of about 35%.\n+ Women on port Q and on port S have a higher chance of survival (90%). The inverse is true, if they are at port C. \n+ Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S.","metadata":{}},{"cell_type":"code","source":"S = df.iloc[(df[\"Survived\"]==1).values]\ntemp = S.groupby(by=\"Pclass\").count()\nname = temp.Survived.index\nval = temp.Survived.values\nfig = px.pie(temp, values=val, names=name, title='Survived by Pclass')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"S = df.iloc[(df[\"Survived\"]==1).values]\ntemp = S.groupby(by=\"Sex\").count()\nname = temp.Survived.index\nval = temp.Survived.values\nfig = px.pie(temp, values=val, names=name, title='Survived by Sex')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So among more than thousands of people in the ship, lets see what was the Age of these people through distribution plot.\n+ You can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully. \n+ For women the survival chances are higher between 14 and 40.\n+ For men the probability of survival is very low between the age of 5 and 18, but that isn’t true for women. \n+ Another thing to note is that infants also have a little bit higher probability of survival.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\ngrid = sns.FacetGrid(df, col='Survived', row='Pclass', size=3.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot above confirms our assumption about pclass 1, but we can also spot a high probability that a person in pclass 3 will not survive.","metadata":{}},{"cell_type":"markdown","source":"# Adapting the Data and solving Inconsistencies\nI’ll just resume the problems of these data and how we can solve it, the focus of this post is build a model who will works to our problem.","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df.drop(columns=\"Survived\")\ndf_train.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ntrain = df_train.shape[0]\nntest = df_test.shape[0]\ny = df.Survived\n\ndata = pd.concat((df_train, df_test)).reset_index(drop=True) \ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, it's time to deal with our cabin column. As we saw above, Cabin column is the column contains many NaN values. As I mentioned this is not only a messy data but also not a huge big data with a lot of features, so fill in the gaps by leverage the data instead of losing observations. I will fill the missing values by character \"U\" for unknow in Cabin column.\n\nThe rest values contain a character and numbers follow behind that character, I decide to just extract the first letter of the Cabin column value to get a general information out of this big missing value column.","metadata":{}},{"cell_type":"markdown","source":"We will use the Name feature to extract the Titles from the Name, so that we can build a new feature out of that.","metadata":{}},{"cell_type":"code","source":"data['Title'] = data['Name'].apply(lambda x: x.split(\",\")[1].split(\".\")[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a reminder, we have to deal with Cabin (687), Embarked (2) and Age (177). First I thought, we have to delete the ‘Cabin’ variable but then I found something interesting. A cabin number looks like ‘C123’ and the letter refers to the deck. Therefore we’re going to extract these and create a new feature, that contains a persons deck. Afterwords we will convert the feature into a numeric variable. ","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Cabin'] = data['Cabin'].fillna(\"C\")\n\n#Turning cabin number into Deck\ndata['Deck'] = data['Cabin'].str[:1]\n\ndata[\"Cabin\"] = data[\"Cabin\"].factorize()[0]\n\ndata['Age'] = data['Age'].fillna(data.Age.mean())\ndata['Embarked'] = data['Embarked'].fillna(\"S\")\ndata['Fare'] = data['Fare'].fillna(-999)\ndata[\"Fare\"] = (data[\"Fare\"] - data[\"Fare\"].min()) / (data[\"Fare\"].max() - data[\"Fare\"].min())\n\ndata[\"Avg_Fare\"] = data[\"Fare\"] / (1 + data[\"SibSp\"] + data[\"Parch\"])\ndata[\"relatives\"] = data[\"SibSp\"] + data[\"Parch\"]\n\ndata[\"Sex\"] = data[\"Sex\"].factorize()[0]\ndata[\"Embarked\"] = data[\"Embarked\"].factorize()[0]\ndata[\"Name\"] = data[\"Name\"].factorize()[0]\ndata[\"Avg_Age\"] = (data[\"Age\"] - data[\"Age\"].mean()) / data[\"Age\"].std()\ndata[\"Ticket\"] = data[\"Ticket\"].factorize()[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Considering theory that Name, PassengerId are not having any significant contribution for our data, we decide to drop them.","metadata":{}},{"cell_type":"code","source":"data = data.drop(columns = [\"PassengerId\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import norm, skew\n\nnumeric_feats = data.dtypes[data.dtypes != 'object'].index\nskewed_feats = data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_feats[abs(skewed_feats) > 0.5]\nhigh_skew","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We converted these \"high skew\" columns to \"normal\" by taking logarith:","metadata":{}},{"cell_type":"code","source":"for feature in high_skew.index:\n    data[feature] = np.log1p(np.abs(data[feature]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fill null values with random numbers, which are computed based on the mean age value in regards to the standard deviation.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\n\n# build dictionary function\ncols=np.array(data.columns[data.dtypes != object])\nd = defaultdict(LabelEncoder)\n\n# only for categorical columns apply dictionary by calling fit_transform \ntrainL = data.apply(lambda x: d[x.name].fit_transform(x))\ntrainL[cols] = data[cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainL.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.preprocessing import StandardScaler\n# Capture all the numerical features so that we can scale them later\n#numerical_features = list(trainL.select_dtypes(include=['int64', 'float64', 'int32']).columns)\n#numerical_features\n\n# Feature scaling - Standard scaler\n#SS = StandardScaler()\n#dataS = pd.DataFrame(data = trainL)\n#dataS[numerical_features] = SS.fit_transform(dataS[numerical_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataS = trainL\ndataS.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataS.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ML Modelling","metadata":{}},{"cell_type":"markdown","source":"Now, we finished the worse part of your problem. It’s time to make Machine Learning, baby!\n\n<img src=\"https://i.gifer.com/3Y0.gif\" alt=\"vac\" border=\"0\"></a>","metadata":{}},{"cell_type":"code","source":"X = dataS[:ntrain].values\nZ = dataS[ntrain:].values\ny = df['Survived'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom numpy import where\nimport collections\n\ncounter = collections.Counter(y)\nprint(counter)\n\nsmt = SMOTE(random_state=0)\nX, y = smt.fit_sample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\ncounter = collections.Counter(y)\nprint(counter)\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we creat a function to calculate the deviations and to illustrate the results:","metadata":{}},{"cell_type":"code","source":"def Models(models, title, X_train, X_test, y_train, y_test, X, y):\n    model = models\n    model.fit(X_train,y_train)\n    \n    train_matrix = pd.crosstab(y_train, model.predict(X_train), rownames=['Actual'], colnames=['Predicted'])    \n    test_matrix = pd.crosstab(y_test, model.predict(X_test), rownames=['Actual'], colnames=['Predicted'])\n    matrix = pd.crosstab(y, model.predict(X), rownames=['Actual'], colnames=['Predicted'])\n    \n    f,(ax1,ax2,ax3) = plt.subplots(1,3,sharey=True, figsize=(15, 2))\n    \n    g1 = sns.heatmap(train_matrix, annot=True, fmt=\".1f\", cbar=False,annot_kws={\"size\": 18},ax=ax1)\n    g1.set_title(title)\n    g1.set_ylabel('Total = {}'.format(y_train.sum()), fontsize=14, rotation=90)\n    g1.set_xlabel('Accuracy score (TrainSet): {}'.format(accuracy_score(model.predict(X_train), y_train)))\n    g1.set_xticklabels(['Not Survived','Survived'],fontsize=12)\n\n    g2 = sns.heatmap(test_matrix, annot=True, fmt=\".1f\",cbar=False,annot_kws={\"size\": 18},ax=ax2)\n    g2.set_ylabel('Total = {}'.format(y_test.sum()), fontsize=14, rotation=90)\n    g2.set_xlabel('Accuracy score (TestSet): {}'.format(accuracy_score(model.predict(X_test), y_test)))\n    g2.set_xticklabels(['Not Survived','Survived'],fontsize=12)\n\n    g3 = sns.heatmap(matrix, annot=True, fmt=\".1f\",cbar=False,annot_kws={\"size\": 18},ax=ax3)\n    g3.set_ylabel('Total = {}'.format(y.sum()), fontsize=14, rotation=90)\n    g3.set_xlabel('Accuracy score (Total): {}'.format(accuracy_score(model.predict(X), y)))\n    g3.set_xticklabels(['Not Survived','Survived'],fontsize=12)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And other function to show the precision and recovery curves of each model:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\ndef ROCs(model):\n    y_scores = model.predict_proba(X_train)\n    y_scores = y_scores[:,1]\n    precision, recall, threshold = precision_recall_curve(y_train, y_scores)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_scores)\n\n    f,(ax1,ax2,ax3) = plt.subplots(1,3,sharey=True, figsize=(25, 7))\n\n    ax1.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=2)\n    ax1.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=2)\n    ax1.legend(loc=\"upper right\", fontsize=14)\n    ax1.set_xlabel(\"threshold\", fontsize=14)\n    ax1.axis([0, 1., 0, 1.])\n\n    ax2.plot(recall, precision, \"g--\", linewidth=2)\n    ax2.set_ylabel(\"recall\", fontsize=14, rotation=90)\n    ax2.set_xlabel(\"precision\", fontsize=14)\n    ax2.axis([0, 1., 0, 1.])\n\n    ax3.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    ax3.plot([0, 1], [0, 1], 'r', linewidth=4)\n    ax3.axis([0, 1, 0, 1])\n    ax3.set_xlabel('False Positive Rate (FPR)', fontsize=14)\n    ax3.set_ylabel('True Positive Rate (TPR)', fontsize=14)\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We start first with the CatBoostClassifier model:","metadata":{}},{"cell_type":"code","source":"%%time\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\nparams = {'loss_function':'Logloss','eval_metric':'AUC','verbose': False,\n          'learning_rate': 0.05,'depth': 2,'l2_leaf_reg': 1,'n_estimators': 100}\nCBC = CatBoostClassifier(**params)\nModels(CBC, \"CatBoostClassifier\", X_train, X_test, y_train, y_test, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.gifer.com/4j.gif\" alt=\"vac\" border=\"0\"></a>","metadata":{}},{"cell_type":"markdown","source":"# Precision Recall Curve\nFor each person the model has to classify, it computes a probability based on a function and it classifies the person as survived (when the score is bigger the than threshold) or as not survived (when the score is smaller than the threshold). That’s why the threshold plays an important part. We will plot the precision and recall with the threshold:","metadata":{}},{"cell_type":"code","source":"ROCs(CBC)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+ Above you can clearly see that the recall is falling of rapidly at a precision of around 82%. Because of that you may want to select the precision/recall tradeoff before that.\n+ You are now able to choose a threshold, that gives you the best precision/recall tradeoff for your current machine learning problem. If you want for example a precision of 80%, you can easily look at the plots and see that you would need a threshold of around 0.4. Then you could train a model with exactly that threshold and would get the desired accuracy.\n\nAnother way is to plot the precision and recall against each other:","metadata":{}},{"cell_type":"markdown","source":"# ROC AUC Curve\nAnother way to evaluate and compare your binary classifier is provided by the ROC AUC Curve. This curve plots the true positive rate (also called recall) against the false positive rate (ratio of incorrectly classified negative instances), instead of plotting the precision versus the recall.\n\n+ The red line in the middel represents a purely random classifier (e.g a coin flip) and therefore your classifier should be as far away from it as possible. Our  model seems to do a good job.\n+ We also have a tradeoff here, because the classifier produces more false positives, the higher the true positive rate is.\n\n# ROC AUC Score\n+ The ROC AUC Score is the corresponding score to the ROC AUC Curve. It is simply computed by measuring the area under the curve, which is called AUC.\n+ A classifiers that is 100% correct, would have a ROC AUC Score of 1 and a completely random classiffier would have a score of 0.5.","metadata":{}},{"cell_type":"markdown","source":"# Gradient Boost","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import fbeta_score, make_scorer\n\nparam_test1 = {\n    'n_estimators': [120, 130,140],\n    'max_depth': [1, 2, 3],\n    'subsample':[0.3, 0.5, 0.7, 1],\n    'learning_rate': [0.006, 0.008, 0.01],\n    'max_features': [0.3, 0.5, 0.7, 1]\n}\n\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n\ngsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(), \n                       param_grid = param_test1, scoring=scoring, iid=False, cv=3, verbose = 5, refit='Accuracy')\ngsearch1.fit(X_train, y_train)\ngsearch1.best_params_, gsearch1.best_score_","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GBC = GradientBoostingClassifier(learning_rate=0.008, n_estimators=130,max_depth= 3,subsample=.7, max_features=.3)\nModels(GBC, \"GradientBoostingClassifier\", X_train, X_test, y_train, y_test, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROCs(GBC)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import fbeta_score, make_scorer\n\nparam_test1 = {'n_estimators': [80,90,100],'max_depth': [1, 2],\n               'min_child_weight': [1,2],'subsample':[1],\n               'colsample_bytree':[1],'reg_alpha':[0],\n               'learning_rate': [0.013, 0.015, 0.017]}\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n\ngsearch1 = GridSearchCV(estimator = XGBClassifier(), \n                       param_grid = param_test1, \n                       scoring=scoring, iid=False, cv=3, verbose = 5, refit='Accuracy')\ngsearch1.fit(X_train, y_train)\ngsearch1.best_params_, gsearch1.best_score_","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XGBC = XGBClassifier(learning_rate=0.017, n_estimators=90, max_depth= 2, min_child_weight= 1, colsample_bytree= 1.,reg_alpha= 0,subsample= 1)\nModels(XGBC, \"GradientBoostingClassifier\", X_train, X_test, y_train, y_test, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROCs(XGBC)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\nparams = {'loss_function':'Logloss','eval_metric':'AUC','verbose': False,\n          'learning_rate': 0.025,'depth': 2,'l2_leaf_reg': 1,'n_estimators': 110}\nCBC = CatBoostClassifier(**params)\nModels(CBC, \"CatBoostClassifier\", X_train, X_test, y_train, y_test, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CBC.fit(X_train, y_train)\nresult = CBC.predict(Z)\nsub = pd.DataFrame()\nsub = pd.DataFrame({'PassengerId':df_test.PassengerId,'Survived':result}) \nsub.to_csv('my_submission.csv', index=False)\nsub.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice ! We think that score is good enough to submit the predictions for the test-set to the Kaggle leaderboard.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion:\nThrough out the data visualization with the help of the dataset, we could se that some groups of people were more likely to survive such as women, children and upper class. CatBoostClassifier, XGBoost, GradientBoostingClassifier yielded similar result in this case on the test set.","metadata":{}}]}