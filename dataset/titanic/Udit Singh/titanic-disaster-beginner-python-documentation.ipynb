{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Simple Breakdown of Titanic Dataset- EDA,Comparisons and Predictions\n\n![](https://drive.google.com/uc?export=download&id=13no8f5E2ffXcBHqftiFrPls6gbQVnV50)"},{"metadata":{},"cell_type":"markdown","source":"<a></a>\n\n# Introduction\nWelcome aboard my Titanic Kernel. In this Kernel I'll be using the 'OG' [Titanic Dataset](https://www.kaggle.com/c/titanic/data) to perform-\n- Starter Exploratory Data Analysis using Distribution Plots,Box Plots and Count Plots.\n- DataCleaning/Data Analysis and Feature Engineering\n- Trainging our Dataset on:\n> 1. XGBoost\n> 2. Random Forest\n> 3. LightGBM\n> 4. CatBOOST\n> 5. AdaBoost\n> 6. Logistic Regression\n- Evaluating and Comparing the Predictions\n\n# UPDATE!!!!\n\n**After some thorough scribbling I was able to improve my submission score from** 0.78468 to 0.78947.**I was able to improve my feature engineering section by better handling of Age and Fare column Features as well as introducing a new Feature column 'TravelAlone'.**"},{"metadata":{},"cell_type":"markdown","source":"# Imports\n\n\n> 1. Let's get our environment ready with the libraries we'll need and then import the relevant ones beforehand!\n> 2. Pandas is one of the most widely used python libraries in data science. It provides high-performance, easy to use structures and data analysis tools.\n> 3. Matplotlib is a plotting library for the Python programming language\n> 4. Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#core imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fetching the Data\n> Using Pandas to load the dataset into this notebook. Using pandas we can read our datafile train.csv with the line below. Data-set loaded will be assigned to the respective variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"#load training dataset and assign it to a variable\ntrain=pd.read_csv(\"../input/titanic/train.csv\")\ntest=pd.read_csv(\"../input/titanic/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check out the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#use the 'head' method to show the first five rows of the table as well as their names. \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of the Titanic train dataframe\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Above is a concise summary of our dataframe returning columns' data-type,index data-type and number of non-null values !"},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis(EDA)\n> Let's create some simple plots to analyze and identify patterns in our data."},{"metadata":{},"cell_type":"markdown","source":"> > # 1. Continuous Features\n> > > ## a) Age\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Avgerage. Age : ',train['Age'].mean())\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nfig = sns.distplot(train['Age'], color=\"darkorange\")\nfig.set_xlabel(\"Age\",size=15)\nfig.set_ylabel(\"Density of Passengers\",size=15)\nplt.title('Passenger Age Distribution',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***We can observe that the probability for the age to be between 20-30 was high for a passenger onboard the Titanic***"},{"metadata":{},"cell_type":"markdown","source":"> > > ## b) Fare"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Avgerage. Fare : ',train['Fare'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nfig = sns.distplot(train['Fare'], color=\"darkorange\")\nfig.set_xlabel(\"Fare\",size=15)\nfig.set_ylabel(\"Density of Passengers\",size=15)\nplt.title('Titanic Fare Distribution',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Clearly, the probability that a passenger was travelling with a cheaper ticket was quite high on Titanic***"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nfig = sns.distplot(train[train['Fare']<=150]['Fare'], color=\"darkorange\")\nfig.set_xlabel(\"Fare\",size=15)\nfig.set_ylabel(\"Density of Passengers\",size=15)\nplt.title('Titanic Fare(Scaled) Distribution',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Fare price less than $25 the probability of finding a buyer was predominantly ***high***"},{"metadata":{},"cell_type":"markdown","source":"> > # 2. Continuous vs Categorical\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#WE PLOT THE PASSENGER AGE DISTRIBUTION VS PASSENGER CLASS ON TITANIC\nplt.figure(figsize=(8,4))\nfig=sns.boxplot(train['Pclass'],train['Age'],palette='Blues')\nfig.set_xlabel(\"Passenger Class\",size=15)\nfig.set_ylabel(\"Age of Passenger\",size=15)\nplt.title('Age Distribution/Pclass',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PClass 1 was predominantly occupied by ***older citizens***"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#WE PLOT THE PASSENGER AGE DISTRIBUTION VS PASSENGER Gender ON TITANIC\nplt.figure(figsize=(8,4))\nfig=sns.boxplot(train['Sex'],train['Age'],palette='Blues')\nfig.set_xlabel(\"Gender\",size=15)\nfig.set_ylabel(\"Age of Passenger\",size=15)\nplt.title('Age Distribution/Gender',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ***Male presence*** OnBoard Titanic was considerably ***older*** than the ***female***."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#WE PLOT THE PASSENGER AGE DISTRIBUTION VS SURVIVED ON TITANIC\nplt.figure(figsize=(8,4))\nfig=sns.boxplot(train['Survived'],train['Age'],palette='Blues',labels=[\"No\"])\nfig.set_xlabel(\"Survived\",size=15)\nfig.set_ylabel(\"Age of Passenger\",size=15)\nplt.title('Age Distribution/Survived',size = 20)\nfig.set(xticklabels=[\"0-No\",\"1-Yes\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Youth Presence onBoard was largely able to ***survive the disaster***"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#WE PLOT THE FARE DISTRIBUTION VS PASSENGER CLASS ON TITANIC\nplt.figure(figsize=(8,4))\nfig=sns.boxplot(train['Pclass'],train['Fare'],palette='Reds')\nfig.set_xlabel(\"Passenger Class\",size=15)\nfig.set_ylabel(\"Fare\",size=15)\nplt.title('Fare Distribution/Pclass',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We narrow our observation range for Fare i.e(fare<=300) to get a better visualization of the distribution."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#WE PLOT THE FARE DISTRIBUTION VS PASSENGER CLASS ON TITANIC\nplt.figure(figsize=(8,4))\nfig=sns.boxplot(train['Pclass'],train[train['Fare']<=300]['Fare'],palette='Reds')\nfig.set_xlabel(\"Passenger Class\",size=15)\nfig.set_ylabel(\"Fare\",size=15)\nplt.title('Fare(Scaled) Distribution/Pclass',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On Titanic, passenger class '3' was the ***most affordable*** and cheap with class '1' being the ***most expensive***"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#WE PLOT THE FARE DISTRIBUTION VS PASSENGER CLASS ON TITANIC\nplt.figure(figsize=(8,4))\nfig=sns.boxplot(train['Survived'],train[train['Fare']<=300]['Fare'],palette='Reds')\nfig.set_xlabel(\"Survived\",size=15)\nfig.set_ylabel(\"Fare\",size=15)\nfig.set(xticklabels=[\"0-No\",\"1-Yes\"])\nplt.title('Fare(Scaled) Distribution/Survived',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Passengers with ***high-priced tickets***, mostly ended up ***SURVIVING***"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nfig=sns.violinplot(train[\"Age\"],train[\"Sex\"], hue=train[\"Survived\"],split=True,palette='Reds')\nfig.set_ylabel(\"Sex\",size=15)\nfig.set_xlabel(\"Age\",size=15)\nplt.title('Age and Sex vs Survived',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Young Passengers(10<age<35) seems to have a good survival rate irrespective of the gender.***"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nfig=sns.violinplot(train[\"Pclass\"],train['Age'], hue=train[\"Survived\"],split=True,palette='Blues')\nfig.set_xlabel(\"Pclass\",size=15)\nfig.set_ylabel(\"Age\",size=15)\nplt.title('Age and Pclass vs Survived',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. for Pclass 1 and 2 survival rate is generally high for **aged 15-40***\n2. for Pclass 3 we see a high survival rate amongst ***children i.e  0<age<10***"},{"metadata":{},"cell_type":"markdown","source":"> # 3. Categorical Features"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"bg_color = (0.25, 0.25, 0.25)\nsns.set(rc={\"font.style\":\"normal\",\n            \"axes.facecolor\":bg_color,\n            \"figure.facecolor\":bg_color,\"text.color\":\"white\",\n            \"xtick.color\":\"white\",\n            \"ytick.color\":\"white\",\n            \"axes.labelcolor\":\"white\"})\nplt.figure(figsize=(8,4))\nfig=sns.countplot(train['Survived'],hue=train['Pclass'],palette='Blues',saturation=0.8)\nfig.set_xlabel(\"Survived\",size=15)\nfig.set_ylabel(\"#\",size=15)\nfig.set(xticklabels=[\"0-No\",\"1-Yes\"])\nplt.title('# of Survived/PClass',size = 20)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ***Passengers from class 3(cheapest Fare) suffered the most!***\n2. ***Pclass 1 reported the highest survival count*** "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nfig=sns.countplot(train['Survived'],hue=train['Sex'],palette='Oranges',saturation=0.8)\nfig.set_xlabel(\"Survived\",size=15)\nfig.set_ylabel(\"#\",size=15)\nfig.set(xticklabels=[\"0-No\",\"1-Yes\"])\nplt.title('# of Survived/Sex',size = 20)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ***Female presence*** largely survived the disaster.\n2. ***Highest Casualty*** was reported from the ***Male side***."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nfig=sns.countplot(train['Survived'],hue=train['SibSp']>0,palette='Blues',saturation=0.8)\nfig.set_xlabel(\"Survived\",size=15)\nfig.set_ylabel(\"#\",size=15)\nfig.set(xticklabels=[\"0-No\",\"1-Yes\"])\nplt.title('# of Survived per Siblings/Spouses Onboard',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Passengers without Siblings/Spouse onboard ***mostly survived***\n2. Interesting observation was the passengers travelling without a siblings/spouse onboard mostly ended up on the casualty side"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nfig=sns.countplot(train['Survived'],hue=train['Parch']>0,palette='Oranges',saturation=0.8)\nfig.set_xlabel(\"Survived\",size=15)\nfig.set_ylabel(\"#\",size=15)\nfig.set(xticklabels=[\"0-No\",\"1-Yes\"])\nplt.title('# of Survived per Parents/Children Onboard',size = 20)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.set(rc={\"font.style\":\"normal\",\n            \"axes.facecolor\":\"white\",\n            \"figure.facecolor\":\"white\",\"text.color\":\"black\",\n            \"xtick.color\":\"black\",\n            \"ytick.color\":\"black\",\n            \"axes.labelcolor\":\"black\"})\nplt.figure(figsize=(8,4))\nfig=sns.countplot(y=train['Pclass'],hue=train['SibSp']>0,palette='Blues',saturation=1.0)\nfig.set_xlabel(\"#\",size=15)\nfig.set_ylabel(\"Passenger Class\",size=15)\nplt.title('# of Pclass per Siblings/Spouses Onboard',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Passenger Class 1,2 and 3*** mostly had passengers ***accompanied neither by their Spouses nor Siblings***."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nfig=sns.countplot(y=train['Pclass'],hue=train['Parch']>0,palette='Reds',saturation=0.6)\nfig.set_ylabel(\"Pclass\",size=15)\nfig.set_xlabel(\"#\",size=15)\nplt.title('# of Pclass per Parents/Children Onboard',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introducing 'TravelAlone' Feature\n\n- 'True': Travelling Alone\n- 'False': Has Company(Either Sibling/Spouse/Parents/Children)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def func(x):\n    x1=x[0]\n    x2=x[1]\n    if x1>0 or x2>0:\n        return False\n    else:\n        return True\ntrain['TravelAlone']=train[['SibSp','Parch']].apply(func,axis=1)\ntest['TravelAlone']=test[['SibSp','Parch']].apply(func,axis=1)\n\nplt.figure(figsize=(8,4))\nfig=sns.countplot(y=train['Survived'],hue=train['TravelAlone'],palette='RdBu',saturation=2.0)\nfig.set_ylabel(\"Survived\",size=15)\nfig.set_xlabel(\"#\",size=15)\nplt.title('# of Survived/Onboard Alone',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Passengers ***travelling alone*** mostly ***lost*** their lives in the disaster\n2. Passengers accompanied by their Siblings/Spouses/Parents/Children ***mostly survived***."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nfig=sns.countplot(y=train['Pclass'],hue=train['TravelAlone'],palette='Reds',saturation=0.8)\nfig.set_ylabel(\"Passenger Class\",size=15)\nfig.set_xlabel(\"#\",size=15)\nplt.title('# of Pclass/Onboard Alone',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Passenger Class 1,2 and 3*** on Titanic predominantly had passengers ***travelling alone***."},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the status of null values inside our training Dataframe\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the status of null values inside our testing Dataframe\ntest.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **Cabin** feature-column contains **NULL values**\n- We will drop **'PassengerId'** feature since it consists of unique ID having no influence on the model training result.\n- We will drop **'Cabin','Ticket' and 'Name'** Feature Columns as we are not using any NLP to process the information for model training from non numerical columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(columns=['PassengerId','Name','Ticket','Cabin'],inplace=True)\n#train.head()\n\n#same processing for test dataset\ntest.drop(columns=['PassengerId','Name','Ticket','Cabin'],inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 'TravelAlone' Feature column was introduced to combine the information of 'SibSp' and 'Parch' feature columns\n- SibSp and Parch can now be dropped from our test/train datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(columns=['SibSp','Parch'],inplace=True)\n#train.head()\n\n#same processing for test dataset\ntest.drop(columns=['SibSp','Parch'],inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n- We will use **one-hot encoding technique** for our **Categorical Features-'Sex','Embarked' and 'TravelAlone'** to use them in model training for better predictions(Converts Categorical data to Integer Data(0 or 1)).\n- We will get rid of **null values** inside the dataframe by filling them with an appropriate replacement,also taking care of **outliers**."},{"metadata":{},"cell_type":"markdown","source":"\n- We get rid of **Null Values** inside the Age Feature Column\n- We replace every Null Value by the median Age value for every Pclass passenger.\n- The median Age is Calculcated per Passenger Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"#median age for Pclass 1\na=train.groupby('Pclass').median()['Age'].iloc[0]\n#median age for Pclass 2\nb=train.groupby('Pclass').median()['Age'].iloc[1] \n#median age for Pclass 3\nc=train.groupby('Pclass').median()['Age'].iloc[2] \ndef fillAge_train(x):\n    age=x[0]\n    pclass=x[1]\n    if pd.isnull(age):\n        if pclass==1:\n            return a\n        elif pclass==2:\n            return b\n        else:\n            return c\n    else:\n        return age\n#median age for Pclass 1\na_test=test.groupby('Pclass').median()['Age'].iloc[0]\n#median age for Pclass 2\nb_test=test.groupby('Pclass').median()['Age'].iloc[1] \n#median age for Pclass 3\nc_test=test.groupby('Pclass').median()['Age'].iloc[2]\ndef fillAge_test(x):\n    age=x[0]\n    pclass=x[1]\n    if pd.isnull(age):\n        if pclass==1:\n            return a_test\n        elif pclass==2:\n            return b_test\n        else:\n            return c_test\n    else:\n        return age\n    \n#replacing null Age values in training dataset\ntrain['Age']=train[['Age','Pclass']].apply(fillAge_train,axis=1)\n#replacing null Age values in Test Dataset\ntest['Age']=test[['Age','Pclass']].apply(fillAge_test,axis=1)\n     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We now get rid of Null Fare values in our test dataset.\n- We replace null with Median Fare value for every Pclass."},{"metadata":{"trusted":true},"cell_type":"code","source":"a=test.groupby('Pclass').median()['Fare'].iloc[0]\nb=test.groupby('Pclass').median()['Fare'].iloc[1]\nc=test.groupby('Pclass').median()['Fare'].iloc[2]\ndef fillFare(x):\n    fare=x[0]\n    pclass=x[1]\n    if pd.isnull(fare):\n        if pclass==1:\n            return a\n        elif pclass==2:\n            return b\n        else:\n            return c\n    else:\n        return fare\n\n#replace Fare value using built in Pandas Functions\ntest['Fare']=test[['Fare','Pclass']].apply(fillFare,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We now get rid of **Null Values** inside **Embarked** Feature Column.\n- We replace the Null value with the **Mode value of the column.**\n- We fill Mode since it is a non-numerical Column where median is not applicable."},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill null values for feature column- Embarked using Pandas built-in function\ntrain['Embarked'].fillna(train['Embarked'].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert Categorical Column to Integer Column using One-hot Encoding\ntrain=pd.get_dummies(train,columns=['Sex','Embarked','TravelAlone'],drop_first=True)\n#train.head()\n\n#Same conversion for test dataset\ntest=pd.get_dummies(test,columns=['Sex','Embarked','TravelAlone'],drop_first=True)\n#test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling Age and Fare Continuous Data Columns\n\n- We divide data from 'Age Column' in 5 bands using Pandas' built in cut()/qcut() method and map the categories to either of these values- 0/1/2/3/4"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['AgeCateg'] = pd.cut(train['Age'], 5)\ntrain[['AgeCateg', 'Survived']].groupby(['AgeCateg'], as_index=False).mean().sort_values(by='AgeCateg', ascending=True)\n\ntrain['FareCateg'] = pd.qcut(train['Fare'], 4)\ntrain[['FareCateg', 'Survived']].groupby(['FareCateg'], as_index=False).mean().sort_values(by='FareCateg', ascending=True)\n\n#func to process values\ndef encodeAgeFare(train):\n    train.loc[train['Age'] <= 16, 'Age'] = 0\n    train.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = 1\n    train.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age'] = 2\n    train.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age'] = 3\n    train.loc[ (train['Age'] > 48) & (train['Age'] <= 80), 'Age'] = 4\n    \n    train.loc[train['Fare'] <= 7.91, 'Fare'] = 0\n    train.loc[(train['Fare'] > 7.91) & (train['Fare'] <= 14.454), 'Fare'] = 1\n    train.loc[(train['Fare'] > 14.454) & (train['Fare'] <= 31.0), 'Fare'] = 2\n    train.loc[(train['Fare'] > 31.0) & (train['Fare'] <= 512.329), 'Fare'] = 3\n    \nencodeAgeFare(train)\nencodeAgeFare(test)\n\n#dropping AgeCateg and FareCateg columns\ntrain.drop(columns=['AgeCateg','FareCateg'],inplace=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train/Test Data Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We Seperate and assign our Target Variable Column to a new variable\nX = train.drop('Survived',axis=1)\n# Dropped 'SibSp' and 'Parch' from Input Feature Data because we have Alone_True Column\n\ny = train['Survived']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We're splitting up our data set into groups called 'train' and 'test'\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(0)\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Core Imports for Model Training\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tuning/Training Models\n\n> ## 1. XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialize our object\nxgbclassifier=XGBClassifier()\n\n#fit train data\nxgbclassifier.fit(X_train,y_train)\n\n#predictions\npred_xgb=xgbclassifier.predict(X_test)\n\n# print(accuracy_score(y_test,pred_xgb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## 2. RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"randomfc = RandomForestClassifier(n_estimators=100)\n\n#fit train data\nrandomfc.fit(X_train,y_train)\n\n#predictions\npred_rf=randomfc.predict(X_test)\n\n# print(accuracy_score(y_test,pred_rf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## 3. LighGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"lightgb=LGBMClassifier()\n\n#fit train data\nlightgb.fit(X_train,y_train)\n\n#predictions\npred_lgb=lightgb.predict(X_test)\n\n# print(accuracy_score(y_test,pred_lgb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## 4. AdaBOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"ada=AdaBoostClassifier(n_estimators=50,learning_rate=1)\n\n#fit train data\nada.fit(X_train,y_train)\n\n#predictions\npred_ada=ada.predict(X_test)\n\n# print(accuracy_score(y_test,pred_ada))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## 5. CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"cbc=CatBoostClassifier(verbose=0, n_estimators=100)\n\n#fit train data\ncbc.fit(X_train,y_train)\n\n#predictions\npred_cbc=cbc.predict(X_test)\n\n# print(accuracy_score(y_test,pred_cbc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## 6. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"log=LogisticRegression(max_iter=1000)\n\n#fit train data\nlog.fit(X_train,y_train)\n\n#predictions\npred_log=log.predict(X_test)\n\n# print(accuracy_score(y_test,pred_log))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluations/Comparisons"},{"metadata":{},"cell_type":"markdown","source":"## 1. Confidence Score"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('XGBoost:', round(xgbclassifier.score(X_train, y_train) * 100, 2), '%.\\t\\t\\t RandomForest:', round(randomfc.score(X_train, y_train) * 100, 2), '%.')\nprint('LightGBM:', round(lightgb.score(X_train, y_train) * 100, 2), '%.\\t\\t\\t AdaBoost:', round(ada.score(X_train, y_train) * 100, 2), '%.')\nprint('CatBoost:', round(cbc.score(X_train, y_train) * 100, 2), '%.\\t\\t\\t LogisticRegression:', round(log.score(X_train, y_train) * 100, 2), '%.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**\n- ***Classification confidence scores are designed to measure the accuracy of the model when predicting class assignment.***\n- Random Forest,XgBoost and LightGBM nearly top the chart with a very high Confidence Score"},{"metadata":{},"cell_type":"markdown","source":"## 2. Accuracy Score"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('XGBoost:', round(accuracy_score(y_test,pred_xgb) * 100, 2), '%.\\t\\t\\t RandomForest:', round(accuracy_score(y_test,pred_rf) * 100, 2), '%.')\nprint('LightGBM:', round(accuracy_score(y_test,pred_lgb) * 100, 2), '%.\\t\\t\\t AdaBoost:', round(accuracy_score(y_test,pred_ada) * 100, 2), '%.')\nprint('CatBoost:', round(accuracy_score(y_test,pred_cbc) * 100, 2), '%.\\t\\t\\t LogisticRegression:', round(accuracy_score(y_test,pred_log) * 100, 2), '%.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Confusion Matrix"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(3, 2,figsize=(20,12))\n\nsns.heatmap(confusion_matrix(y_test,pred_xgb), annot=True,ax=axs[0][0])\naxs[0, 0].set_title('XGBoost')\n\nsns.heatmap(confusion_matrix(y_test,pred_rf), annot=True,ax=axs[0][1])\naxs[0, 1].set_title('RandomForest')\n\nsns.heatmap(confusion_matrix(y_test,pred_lgb), annot=True,ax=axs[1][0])\naxs[1, 0].set_title('LightGBM')\n\nsns.heatmap(confusion_matrix(y_test,pred_ada), annot=True,ax=axs[1][1])\naxs[1, 1].set_title('XgBoost')\n\nsns.heatmap(confusion_matrix(y_test,pred_cbc), annot=True,ax=axs[2][0])\naxs[2, 0].set_title('CatBoost')\n\nsns.heatmap(confusion_matrix(y_test,pred_log), annot=True,ax=axs[2][1])\naxs[2, 1].set_title('Logistic Regression')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true},"cell_type":"markdown","source":"## 4. Hyperparameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"- Tuned Random Forest model was used for submission to achieve a score of top14%.\n- We will now use RandomizedSearchCV to tune our **RandomForest model**"},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Tuned- Model used for Submission- 0.78947"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"#assigning X_train,X_test and y_train vatiables\nX_train=train.drop('Survived',axis=1)\nX_test=test.copy()\ny_train = train['Survived']\n\n#declaring and inititalizing params attribute\nfrom scipy.stats import uniform, truncnorm, randint\nmodel_params = {\n    # randomly sample numbers from 4 to 204 estimators\n    'n_estimators': randint(4,200),\n    # normally distributed max_features, with mean .25 stddev 0.1, bounded between 0 and 1\n    'max_features': truncnorm(a=0, b=1, loc=0.25, scale=0.1),\n    # uniform distribution from 0.01 to 0.2 (0.01 + 0.199)\n    'min_samples_split': uniform(0.01, 0.199)\n}\n\nrf_model = RandomForestClassifier()\n\n# set up random search meta-estimator\n# this will train 100 models over 5 folds of cross validation (500 models total)\nclf = RandomizedSearchCV(rf_model, model_params, n_iter=100, cv=5, random_state=1)\n\n# train the random search meta-estimator to find the best model out of 100 candidates\nmodel = clf.fit(X_train, y_train)\n\n# print winning set of hyperparameters\nfrom pprint import pprint\npprint(model.best_estimator_.get_params())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"testData=test.copy()\npredictions_rf=model.predict(testData)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm=pd.read_csv('../input/titanic/gender_submission.csv')\nSurvived=pd.Series(predictions_rf)\nsubm.drop(columns=['Survived'],inplace=True)\nsubm['Survived']=pd.Series(predictions_rf)\nsubm.to_csv(r'RandomForestSubmission.csv', index = False)\nprint(\"Submitted\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thank You!\n\n- Any suggestions for improvements in the score and accuracy are most welcome.\n- Do mention the irregularities found in the comments below."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}