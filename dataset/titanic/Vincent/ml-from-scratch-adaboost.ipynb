{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src=https://pictures.topspeed.com/IMG/crop/201909/the-bugatti-chiron-h-21_800x0w.jpg alt=\"a different kind of boost\"></center>\n<center>img source: topspeed.com</center>\n\n# <center><b>Adaboost‚öôÔ∏è</b></center>\n\n**What you can expect from this notebook:** Since I did a notebook on bagging(random forests) [here](https://www.kaggle.com/code/vincentbrunner/ml-from-scratch-random-forests), I thought doing one about boosting would fit quite good at this point. So this notebook covers the theory behind Adaboost as well as code implementation and test.\n\n<div class=\"alert alert-block alert-info\">üëâIf you're just interested in the complete, with comments documented implementation of an Adaboost classifier using just numpy and the copy module, feel free to click on show hidden code: </div>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"#  Adaboost implementation including the regulating learning rate parameter and a samme algorithm implementation\nclass AdaBoostClassifier():\n    def __init__(self, base_estimator=True, n_estimators=50, learning_rate=1):\n        if base_estimator:\n            self.base_estimator = DecisionTreeClassifier(max_depth=1, max_leaf_nodes=2)\n        else:\n            self.base_estimator = base_estimator\n            \n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        \n        self.estimators = None\n        self.estimator_weights = None\n        \n        #  to track performance, not nesseccary for the algorithm\n        self.total_errors = None\n        self.training_error = None\n        self.validation_error = None\n        \n    def fit(self, X, y, X_val=None, y_val=None):\n        #  resetting lists \n        self.estimators = []\n        self.estimator_weights = []\n        self.total_errors = []\n        self.training_error = []\n        self.validation_error = []\n        lr = self.learning_rate\n        \n        #  0) initialise equal weights\n        sample_weights = np.full(len(X), 1/len(X))\n        \n        for est_i in range(self.n_estimators):\n            #  1) fit weak learner\n            estimator = copy.copy(self.base_estimator)\n            estimator.fit(X, y, sample_weights)\n            \n            #  2) calculate total error\n            prediction = estimator.predict(X)\n            total_error = np.where(prediction != y, sample_weights, 0).sum() \n            \n            #  3) determine weight / amount of say in final prediction\n            amount_of_say = lr * 0.5 * np.log((1 - total_error)/(total_error + 1e-10))\n\n            #  3.5) save estimator and it's weight before going into the next iteration\n            self.estimators.append(estimator)\n            self.estimator_weights.append(amount_of_say)\n            \n            #  4) update weights\n            sample_weights = np.where(prediction != y, sample_weights * np.exp(amount_of_say), sample_weights * np.exp(-1 * amount_of_say))\n            \n            #  5) renormalize weights\n            sample_weights = sample_weights / sample_weights.sum()\n            \n            #  5.5) keep track of total- and training-error over iterations for documentation purposes\n            self.total_errors.append(total_error)\n            self.training_error.append(np.where(self.predict(X) != y, 1, 0).sum()/len(X))\n            if type(X_val) != \"NoneType\":\n                self.validation_error.append(np.where(self.predict(X_val) != y_val, 1, 0).sum()/len(X_val))\n    \n    def predict(self, X, verbose=False):\n        predictions = np.stack([estimator.predict(X) for estimator in self.estimators], axis=1) \n        weighted_majority_vote = lambda x: np.unique(x)[np.argmax([np.where(x==categ, self.estimator_weights, 0).sum() for categ in np.unique(x)])]\n        return np.apply_along_axis(weighted_majority_vote, axis=1, arr=predictions)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-25T15:20:32.851171Z","iopub.execute_input":"2022-06-25T15:20:32.851603Z","iopub.status.idle":"2022-06-25T15:20:32.869517Z","shell.execute_reply.started":"2022-06-25T15:20:32.851569Z","shell.execute_reply":"2022-06-25T15:20:32.868459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****\n\n# <b><span style=\"color:#ebd1a4\">|</span> Table of Contents üìÑ</b>\nThis notebook goes through all the principles neccessary to understand and code a classification or regression tree. The resulting models will be tested on actual datasets (no pseudo datasets this time^^).\n\n<p style=\"font-family: Arial; font-size: 16px; font-weight: bold; letter-spacing: 2px; line-height:1.3\"><a href=\"#1.\" style=\"color:#940000\">1. Adaboost intuition</a></p>\n\n<p style=\"font-family: Arial; font-size: 16px; font-weight: bold; letter-spacing: 2px; line-height:1.3\"><a href=\"#2.\" style=\"color:#940000\">2. Background knowledge:</a></p>\n\n<p style=\"text-indent:10px; font-family: Arial; font-size: 14px; letter-spacing: 2px; line-height:1.3\"><a href=\"#2.1.\" style=\"color:#940000\">2.1. Boosting</a></p>\n\n<p style=\"text-indent:10px; font-family: Arial; font-size: 14px; letter-spacing: 2px; line-height:1.3\"><a href=\"#2.2.\" style=\"color:#940000\">2.2. Exponential loss</a></p>\n\n<p style=\"font-family: Arial; font-size: 16px; font-weight: bold; letter-spacing: 2px; line-height:1.3\"><a href=\"#3.\" style=\"color:#940000\">3. The algorithm step for step</a></p>\n\n<p style=\"font-family: Arial; font-size: 16px; font-weight: bold; letter-spacing: 2px; line-height:1.3\"><a href=\"#4.\" style=\"color:#940000\">4. Implementation</a></p>\n\n<p style=\"font-family: Arial; font-size: 16px; font-weight: bold; letter-spacing: 2px; line-height:1.3\"><a href=\"#5.\" style=\"color:#940000\">5. Fitting and evaluation</a></p>\n\n<p style=\"font-family: Arial; font-size: 16px; font-weight: bold; letter-spacing: 2px; line-height:1.3\"><a href=\"#6.\" style=\"color:#940000\">6. Adaboost characteristics based on example</a></p>\n\n<br>\n\n##### <div class=\"alert alert-block alert-info\">‚ö†Ô∏è <strong>Important:</strong> since making own visualisations would be too time-consuming for a first notebook I mainly embedded images from <strong>google image search</strong>. If you should find you're image here and <strong>want it to be removed</strong> please leave a comment or <strong>contact me</strong>. </div>","metadata":{}},{"cell_type":"markdown","source":"<p id=\"1.\"></p>\n\n****\n\n# <b>1 <span style=\"color:#ebd1a4\">|</span> Adaboost intuition</b>\n\nThe main idea behind the Adaboost algorithm is to **combine multiple weak estimators** to create a **better estimator** than the individual estimators, reducing the bias (and variance compared to models with similarly low bias).\n\n<center><img src=https://www.researchgate.net/profile/Zhuo-Wang-36/publication/288699540/figure/fig9/AS:668373486686246@1536364065786/Illustration-of-AdaBoost-algorithm-for-creating-a-strong-classifier-based-on-multiple.png></center>\n<center> image source: researchgate.net </center>\n<br>\n\nThe idea is relatively straightforward:\n* train the estimators **one after the other**:\n    * identify the mistakes the estimator made as well as its overall performance\n    * fit the next estimator in a way that it **puts weight on correctly classifying samples the previous estimators didn't** and repeat\n* for a prediction, pass the data through every single model and **aggregate all the results weighted by the performance** of the origin model the results came from\n\nThis way the weak **estimators \"support each other\"** and create a **final model with lower bias**, while the variance, in the best case, get's kept low too.","metadata":{}},{"cell_type":"markdown","source":"<p id=\"2.1.\"></p>\n\n****\n\n# <b>2.1 <span style=\"color:#ebd1a4\">|</span> Boosting: the ensembling method Adaboost is based on</b>\n\n**Main goal: reducing bias**<br>\n**Combining estimators (usualy) by summation + based on weights -> weighted aggregation**<br>\n\nSteps:\n* Fit weak learner on a weighted dataset(***weak learner: estimator that makes predictions slightly better than random chance -> low variance, high bias***)\n* After each iteration **boost** the sample weights of falsely classified samples\n* Repeat till max number of estimators is reached or some other terminal condition is met\n* The output consists of the prediction of each estimator usually weighted by the previously determined accuracy/performance of the estimator and added up\n\nBias reducing aspect of Boosting:\n* Since the estimator aren't trained under the same conditions but to supplement the already trained ones, they **\"add\" to the capability of the model to make good predictions**\n* Combined with the fact, that the final prediction takes the performance of the individual estimators into account, this reduces the bias in comparison to the weak learners\n\nVariance reducing aspect of Boosting:\n* This is rather situationally dependant \n* But in a way, the summation of the weighted predictions of the estimators can be seen as a sample statistic\n* So adding more estimators does have a variance reducing aspect even tho the complexity every single estimator adds usually leads to an overall increase in variance\n* In comparison to the base estimators the variance still grows, but **in comparison to other models with similarly low bias, boosting often achieves a lower variance**\n\n***Boosting algorithms take weak learner with relatively high bias and combines them into a strong learner***","metadata":{}},{"cell_type":"markdown","source":"<p id=\"2.2.\"></p>\n\n****\n\n# <b>2.2 <span style=\"color:#ebd1a4\">|</span> The exponential loss: the loss function Adaboost minimizes</b>\n\nThe exponential loss is one of the most common classification losses but for the sake of completeness it's going to be explained quickly:\n* the ground truth $\\large y_i$ is a set of lenght n where $\\large y\\epsilon\\{1, -1\\}$ -> **binary classification problem**\n* an estimator $\\large h(x)$ is fit on a set of training data $\\large \\{(x_1, y_1), ..., (x_n, y_n)\\}$\n* the exponential loss is given by:<br>\n\n    $\\Large e^{-y_ih(x_i)}$\n    \n**interpretation:**<br>\n**the goal is for the output of h(x) to be clearly positive(or negative) if the ground trouth is 1(or -1)**\n\nLet's first take a look at the inner term $\\large y_ih(x_i)$:\n* **if the target y_1 = -1 the term increases as h(x) decreases vica verca**\n* **if the target y_1 = 1 the term increases as h(x) increases vica verca**\n\nThe outer term $e^{-x}$ looks like the following:<br>\n<center><img src=\"https://qph.fs.quoracdn.net/main-qimg-a1eb1cddee1b74e3457e36543bbf8971\"></center>\n<center>img source: quora.com</center>\n\n* when the $y_ih(x_i)$ term gets smaller, the loss get's exponentialy higher \n* -> **in order to decrease the loss, $y_ih(x_i)$ has to be as high as possible**\n\n**Conclusion:** The exponential loss is reduced by the output of h(x) being as clear positive/negative as possible, depending on the sign of the ground truth","metadata":{}},{"cell_type":"markdown","source":"<p id=\"3.\"></p>\n\n****\n\n# <b>3 <span style=\"color:#ebd1a4\">|</span> The algorithm step for step</b>\n\n### derivation for binary classification:\n#### **on an abstract lvl:**\n\nAs already mentioned in the boosting section, the output of this Algorithm looks like the following:<br>\n\n$\\large C(x) = \\sum_{m=1}^M\\alpha_mh(x)_m\\>\\>\\>\\>\\>\\>$where $\\alpha_i$ is the weight of the estimator $h(x)_i$ and $M$ is the total amount of ensembled estimators.\n\nAt every iteration m an estimator $h_m$ is added to the ensemble weighted by a factor $\\alpha_m$:<br>\n\n$\\large C(x)_m = C(x)_{m-1} + \\alpha_mh(x)_m$\n\nto minimize the sum of the exponential loss for all data points:<br>\n\n$\\Large \\sum_{i=1}^ne^{-y_i(C(x_i)_{m-1} + \\alpha_mh(x_i)_m)}$\n\nsince this term can be split into 2 parts(due to the summation in the exponent), h has to be trained to minimize:\n\n$\\Large \\sum_{i=1}^ne^{-y_i\\alpha_mh(x_i)_m}$\n\nand with sample weights:<br>\n\n$\\Large \\sum_{i=1}^nw_ie^{-y_i\\alpha_mh(x_i)_m}$\n\nthe term $y_ih(x_i)_m$ is positive for data points correctly classified and negative for incorrectly classified data points. Assuming $h(x)\\epsilon{-1, 1}$ the whole term can just take on 2 values: -1(for incorrectly classified data points) and 1(for correctly classified data points). Therefore it can be removed from the exponent by splitting the sum based on the value $y_ih(x_i)_m$ takes on:<br>\n\n$\\Large \\sum_{y_i\\neq h(x_i)_m}w_ie^{\\alpha_m} + \\sum_{y_i=h(x_i)_m}w_ie^{-\\alpha_m}$\n\n**finding $\\alpha$:**\n* The idea is to find an alpha that minimizes the loss above -> by finding a minimum of the loss -> a point where its derivative is equal to 0\nthis derivative is given by:<br>\n\n$\\Large \\frac{\\partial L}{\\partial \\alpha_m} = \\sum_{y_i\\neq h(x_i)_m}w_ie^{\\alpha_m} -\\sum_{y_i=h(x_i)_m}w_ie^{-\\alpha_m}$\n\nsetting it equal to 0 and solving for $\\alpha_m$ the following term is obtained:<br>\n\n$\\Large \\alpha_m = \\frac{1}{2}ln(\\frac{\\sum_{y_i= h(x_i)_m}w_i}{\\sum_{y_i\\neq h(x_i)_m}w_i})$ \n\nwhen normalizing w after each iteration, its total sum is equal to 1, so it can be written as:<br>\n\n$\\Large \\alpha_m = \\frac{1}{2}ln(\\frac{1 - \\sum_{y_i\\neq h(x_i)_m}w_i}{\\sum_{y_i\\neq h(x_i)_m}w_i}) = \\frac{1}{2}ln(\\frac{1 - e}{e})$ where $e = \\sum_{y_i\\neq h(x_i)_m}w_i$ is the sum of all **sample weights of incorrectly classified samples** which in this context is often refered to as the ***total error***\n\n<br>\n\n#### **Quick recap/summary:**\nThis means that after having fitted an estimator h(x) to the weighted training data we can add it to the ensemble multiplied by its factor:<br>\n\n$\\Large \\alpha_m = \\frac{1}{2}ln(\\frac{1 - e}{e})$\n\nby doing this, the exponential error get's minimized.<br>\nThe step left, is to **update the sample weights**, boosting the ones of incorrectly classified samples and renormalizing afterwards.\n\n#### **updating the sample weights:**\nThe weights are **updated by the exponential loss** calculated over the corresponding sample:\n\n$\\Large w_i\\leftarrow w_ie^{-y_i\\alpha_mh(x_i)_m}$\n\nThis way the weights of **the weights of incorrectly classified samples are boosted** and the rest is kept low.<br>\nSince this doesn't result in weights that sum up to 1 but this assumption was made to come up with the formula for determining the optimal $\\alpha_m$, the obtained new sample weights have to be **renormalized**:\n\n$\\Large w_i = \\frac{w_i}{\\sum_{i=1}^nw_i}$\n\n<br>\n\n### the resulting algorithm:\n**initialize sample weights** $\\large w_i = 1/n, i=1,2,...,n$\n* **for $\\large m ... M$:**\n\n    1. **fit estimator $\\large h(x)_m$ to training set utalising the weights W**\n    2. **compute $\\large e = \\sum_{y_i \\neq h(x)_m}w_i$**\n    3. **compute $\\large \\alpha_m = \\frac{1}{2}ln(\\frac{1 - e}{e})$**\n    4. **set $\\large w_i\\leftarrow w_ie^{-y_i\\alpha_mh(x_i)_m}$**\n    5. **renormalize $\\large w_i = \\frac{w_i}{\\sum_{i=1}^nw_i}$**\n    \n$\\large C(x) = \\sum_{m=1}^M\\alpha_mh(x)_m$\n\n<br>\n\n### in words:\n**initialize sample weights** $\\large w_i = 1/n, i=1,2,...,n$\n* **repeat till max amount of estimators is reached:**\n\n    1. **fit estimator $\\large h(x)_m$ to training set utalising the weights W**\n    2. & 3. **determine the best factor for $\\large h(x)_m$ when adding to ensemble**\n    4. **update weights based on the error: more emphasis on incorrectly classified samples**\n    5. **renormalize the weights to sum up to 1**\n    \n**the final estimator consists of all the weak learners multiplied by their weights and summed up**\n\n<br>\n\n### generalise for problems where y isn't -1 and 1:\nNote: the indicator function I ouputs 1 if the condition is met and 0 otherwise. \n\n**initialize sample weights** $\\large w_i = 1/n, i=1,2,...,n$\n* **for $\\large m ... M$:**\n\n    1. **fit estimator $\\large h(x)_m$ to training set utilising the weights W**\n    2. **compute $\\large e = \\sum_{i=1}^nw_i\\mathbb{I}(y_i \\neq h(x)_m)$**\n    3. **compute $\\large \\alpha_m = \\frac{1}{2}ln(\\frac{1 - e}{e})$**\n    4. **set $\\large w_i\\leftarrow w_i * \\left\\{ \\begin{array}{ c l }e^{-\\alpha_m} & \\quad \\textrm{if } y_i = h(x)_m \\\\ e^{\\alpha_m} & \\quad \\textrm{if } y_i \\neq h(x)_m \\end{array}\\right.$**\n    5. **renormalize $\\large w_i = \\frac{w_i}{\\sum_{i=1}^nw_i}$**\n    \n$\\large C(x) = \\underset{k}{\\operatorname{argmax}}\\sum_{m=1}^M\\alpha_m\\mathbb{I}(h(x)_m=k)$ -> weighted majority vote\n\n<br>\n\n##### <div class=\"alert alert-block alert-info\">‚ö†Ô∏è <strong>Important:</strong> This is just the base Adaboost variant (discrete Adaboost). Today newer variants like the samme algorithm get used more frequent especialy for multiclass classification. If there is interest in this, I will <strong>update</strong> this notebook with the corresponding background and implementations.</div>\n","metadata":{}},{"cell_type":"markdown","source":"<p id=\"4.\"></p>\n\n****\n\n# <b>4 <span style=\"color:#ebd1a4\">|</span> Python implementation</b>\n\n**Note:** for Adaboost **every type of weak learner** can be used in theory but since they are often used and the most simple, this implementation will have a decision tree stomp as the default base estimator.","metadata":{}},{"cell_type":"code","source":"#  used for implementing the algorithm\nimport numpy as np # linear algebra \nimport copy # deep copies of objects -> estimators\n\n#  estimator to ensemble with Adaboost:\nfrom sklearn.tree import DecisionTreeClassifier\n\n#  used for data handeling and visualisation\nimport pandas as pd # loading and transforming data\nimport matplotlib.pyplot as plt # visualisations\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # creating & visualising confusion matrices\nfrom sklearn.model_selection import train_test_split # splitting data in train/test set","metadata":{"execution":{"iopub.status.busy":"2022-06-25T15:20:34.220358Z","iopub.execute_input":"2022-06-25T15:20:34.220851Z","iopub.status.idle":"2022-06-25T15:20:34.226696Z","shell.execute_reply.started":"2022-06-25T15:20:34.220811Z","shell.execute_reply":"2022-06-25T15:20:34.225689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_data = pd.read_csv(\"../input/titanic/train.csv\", usecols=[\"Survived\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]).dropna()\ntitanic_data[\"Sex\"] = titanic_data[\"Sex\"].astype(\"category\").cat.codes\ntitanic_data[\"Embarked\"] = titanic_data[\"Embarked\"].astype(\"category\").cat.codes\n\nfeatures = titanic_data.loc[:, titanic_data.columns!=\"Survived\"].to_numpy() # select everything but the target\nlabels = titanic_data.loc[:, \"Survived\"].to_numpy() # select the target\n\nX_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2)\n\ntitanic_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T15:20:34.571115Z","iopub.execute_input":"2022-06-25T15:20:34.571771Z","iopub.status.idle":"2022-06-25T15:20:34.601435Z","shell.execute_reply.started":"2022-06-25T15:20:34.571719Z","shell.execute_reply":"2022-06-25T15:20:34.600711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Adaboost implementation including the regulating learning rate parameter and a samme algorithm implementation\nclass AdaBoostClassifier():\n    def __init__(self, base_estimator=True, n_estimators=50, learning_rate=1):\n        if base_estimator:\n            self.base_estimator = DecisionTreeClassifier(max_depth=1, max_leaf_nodes=2)\n        else:\n            self.base_estimator = base_estimator\n            \n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        \n        self.estimators = None\n        self.estimator_weights = None\n        \n        #  to track performance, not nesseccary for the algorithm\n        self.total_errors = None\n        self.training_error = None\n        self.validation_error = None\n        \n    def fit(self, X, y, X_val=None, y_val=None):\n        #  resetting lists \n        self.estimators = []\n        self.estimator_weights = []\n        self.total_errors = []\n        self.training_error = []\n        self.validation_error = []\n        lr = self.learning_rate\n        \n        #  0) initialise equal weights\n        sample_weights = np.full(len(X), 1/len(X))\n        \n        for est_i in range(self.n_estimators):\n            #  1) fit weak learner\n            estimator = copy.copy(self.base_estimator)\n            estimator.fit(X, y, sample_weights)\n            \n            #  2) calculate total error\n            prediction = estimator.predict(X)\n            total_error = np.where(prediction != y, sample_weights, 0).sum() \n            \n            #  3) determine weight / amount of say in final prediction\n            amount_of_say = lr * 0.5 * np.log((1 - total_error)/(total_error + 1e-10))\n\n            #  3.5) save estimator and it's weight before going into the next iteration\n            self.estimators.append(estimator)\n            self.estimator_weights.append(amount_of_say)\n            \n            #  4) update weights\n            sample_weights = np.where(prediction != y, sample_weights * np.exp(amount_of_say), sample_weights * np.exp(-1 * amount_of_say))\n            \n            #  5) renormalize weights\n            sample_weights = sample_weights / sample_weights.sum()\n            \n            #  5.5) keep track of total- and training-error over iterations for documentation purposes\n            self.total_errors.append(total_error)\n            self.training_error.append(np.where(self.predict(X) != y, 1, 0).sum()/len(X))\n            if type(X_val) != \"NoneType\":\n                self.validation_error.append(np.where(self.predict(X_val) != y_val, 1, 0).sum()/len(X_val))\n    \n    def predict(self, X, verbose=False):\n        \"\"\"\n        * every estimator makes his predictions in the shape (len(X)) -> [a, b, ..., len(X)]\n        * stack prediction of estimators to have them row wise(each row corresponds to a sample) -> [[a1, a2], [b1, b2], ..., len(X)]\n        * at each row apply the weighted majority vote previously discussed\n        \"\"\"\n        predictions = np.stack([estimator.predict(X) for estimator in self.estimators], axis=1) \n        weighted_majority_vote = lambda x: np.unique(x)[np.argmax([np.where(x==categ, self.estimator_weights, 0).sum() for categ in np.unique(x)])]\n        return np.apply_along_axis(weighted_majority_vote, axis=1, arr=predictions)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T15:20:34.850577Z","iopub.execute_input":"2022-06-25T15:20:34.851217Z","iopub.status.idle":"2022-06-25T15:20:34.867805Z","shell.execute_reply.started":"2022-06-25T15:20:34.851168Z","shell.execute_reply":"2022-06-25T15:20:34.866708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p id=\"5.\"></p>\n\n# <b>5 <span style=\"color:#ebd1a4\">|</span> Fitting and evaluation</b>","metadata":{}},{"cell_type":"code","source":"#  let's use a slightly complexer decision tree as base estimator:\nbase = DecisionTreeClassifier(max_depth=2, max_leaf_nodes=4)\n\n#  fit Adaboost classifier with 100 estimators\nadaboost = AdaBoostClassifier(base_estimator=base, n_estimators=500, learning_rate=1)\nadaboost.fit(X_train, y_train, X_val, y_val)\n\n#  make predictions:\npredictions = adaboost.predict(X_val)\n\n#  confusion matrix\ncm = confusion_matrix(y_val, predictions, labels=[0, 1])\ncm_displ = ConfusionMatrixDisplay(cm)\ncm_displ.plot()\nplt.show()\n\n#  calculate accuracy:\naccuracy = np.mean(predictions==y_val)\n\n#  calculate recall:\nrecall = cm[1, 1]/cm[1, :].sum() # of the total actual positives, how much were classified correctly\n\n#  calculate precision:\nprecision = cm[1, 1]/cm[:, 1].sum() # of all predicted positives, how much were True positives\n\n#  not that neccessary for this problem, but for the completeness:\nf1 = 2 * ((recall * precision)/(recall + precision)) \n\nprint(f\"accuracy = {accuracy},\\nrecall = {recall},\\nprecision = {precision},\\nf1-score = {f1}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-25T15:20:35.681391Z","iopub.execute_input":"2022-06-25T15:20:35.681986Z","iopub.status.idle":"2022-06-25T15:21:19.831709Z","shell.execute_reply.started":"2022-06-25T15:20:35.681935Z","shell.execute_reply":"2022-06-25T15:21:19.830642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p id=\"6.\"></p>\n\n# <b>6 <span style=\"color:#ebd1a4\">|</span> Adaboost characteristics</b>","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(14, 8))\n\nplt.plot(range(len(adaboost.training_error)), adaboost.training_error, color=\"red\", label=\"training error\")\nplt.plot(range(len(adaboost.validation_error)), adaboost.validation_error, color=\"green\", label=\"validation error\")\nplt.plot(range(len(adaboost.estimator_weights)), adaboost.estimator_weights, color=\"black\", label=\"amount of say\")\nplt.plot(range(len(adaboost.total_errors)), adaboost.total_errors, color=\"blue\", label=\"total error\")\n\nplt.xlabel(\"iteration\")\nplt.ylabel(\"error, amount of say\")\n\nplt.title(\"adaboost classifier performance summary\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T15:21:19.833905Z","iopub.execute_input":"2022-06-25T15:21:19.834339Z","iopub.status.idle":"2022-06-25T15:21:20.086814Z","shell.execute_reply.started":"2022-06-25T15:21:19.834299Z","shell.execute_reply":"2022-06-25T15:21:20.085854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Important things to take away:**\n* Adaboost is likely to overfit, this can be avoided with many regularisation techniques but has to be kept in mind\n* Adaboost performs extremely different on a problem depending on the base estimator used\n* Adaboost puts the most weight on estimators from early iterations since later estimators \"fix\" just small mistakes and don't have a good alone standing performance\n\n**That's all for this notebook, have a great day and happy learning!üëã**\n\nPapers I oriented this notebook at:<br>\n[1](https://www.sciencedirect.com/science/article/pii/S002200009791504X?via%3Dihub) <br>\n[2](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjh79Ka5Mj4AhW0g_0HHQEaCM4QFnoECAYQAQ&url=https%3A%2F%2Fhastie.su.domains%2FPapers%2Fsamme.pdf&usg=AOvVaw2IeMtma-dd6YlB0Au3R6YC) <br>\n[3](www.inf.fu-berlin.de/inst/ag-ki/adaboost4.pdf) <br>","metadata":{}}]}