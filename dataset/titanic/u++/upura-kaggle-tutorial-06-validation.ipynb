{"cells":[{"metadata":{"_cell_guid":"e12020f7-4f94-4ecc-9007-9b7a6e7458a6","_uuid":"1fecb0980d8d422ec0f005c4bfd6225385c2c60f"},"cell_type":"markdown","source":"This Notebook is a kaggle tutorial for Japanese kaggle beginners writen in Japanese.\n\n# 6. submitのその前に！ 「Cross Validation」の大切さを知ろう","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"3〜5つ目のNotebookでは、特徴量エンジニアリング・機械学習アルゴリズム・ハイパーパラメータの面で、スコアを上げていく方法を学びました。\n\nこの[Notebook](https://www.kaggle.com/sishihara/upura-kaggle-tutorial-06-validation)では、機械学習モデルの性能を見積もる「validation」について解説します。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 特徴量の準備\n\nimport numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv(\"../input/titanic/train.csv\")\ntest = pd.read_csv(\"../input/titanic/test.csv\")\ngender_submission = pd.read_csv(\"../input/titanic/gender_submission.csv\")\n\ndata = pd.concat([train, test], sort=False)\n\ndata['Sex'].replace(['male','female'], [0, 1], inplace=True)\ndata['Embarked'].fillna(('S'), inplace=True)\ndata['Embarked'] = data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ndata['Fare'].fillna(np.mean(data['Fare']), inplace=True)\ndata['Age'].fillna(data['Age'].median(), inplace=True)\ndata['FamilySize'] = data['Parch'] + data['SibSp'] + 1\ndata['IsAlone'] = 0\ndata.loc[data['FamilySize'] == 1, 'IsAlone'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"delete_columns = ['Name', 'PassengerId', 'Ticket', 'Cabin']\ndata.drop(delete_columns, axis=1, inplace=True)\n\ntrain = data[:len(train)]\ntest = data[len(train):]\n\ny_train = train['Survived']\nX_train = train.drop('Survived', axis=1)\nX_test = test.drop('Survived', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"特徴量の準備が完了しました。\n\n# ホールドアウト検証\n\n実は既に、LightGBMを利用する段階で「ホールドアウト検証」と呼ばれる一種のvalidationを実施していました。学習用データセットを分割した上でLightGBMを学習させていたことを思い出してください。\n\nこの検証用データセットは、自分で学習用データセットから切り出しているため、目的変数を含めて全容を把握できています。全体像の見えていないpublic LBのスコアに比べて、信頼性の高いスコアを得られる可能性を秘めています。\n\n検証用データセットに対する性能、すなわちvalidationスコアは、提出することなく手元で確認可能です。自分の気の済むだけ試行錯誤を回し、良いスコアを得た場合に実際にKaggleに提出するような運用が可能です。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=0, stratify=y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# カテゴリ変数の指定\ncategorical_features = ['Embarked', 'Pclass', 'Sex']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'objective': 'binary',\n    'max_bin': 300,\n    'learning_rate': 0.05,\n    'num_leaves': 40\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\n\nlgb_train = lgb.Dataset(X_train, y_train, categorical_feature=categorical_features)\nlgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train, categorical_feature=categorical_features)\n\nmodel = lgb.train(\n    params, lgb_train,\n    valid_sets=[lgb_train, lgb_eval],\n    verbose_eval=10,\n    num_boost_round=1000,\n    early_stopping_rounds=10\n)\n\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = (y_pred > 0.5).astype(int)\ny_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = gender_submission\n\nsub['Survived'] = y_pred\nsub.to_csv(\"submission_lightgbm_holdout.csv\", index=False)\n\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"この予測値は、私の環境で0.77033というスコアが出ています（前回のKernel参照）。\n\n# 交差検証（Cross Validation）\n\nさて、ここで「Cross Validation」を実行すると、ホールドアウト検証の例よりも、更に汎用的に性能を確認できます。交差検証（Cross Validation）とは、複数回にわたって異なる方法でデータセットを分割し、それぞれでホールドアウト検証を実行する方法です。そのスコアの平均を確認することで、1回のホールドアウト検証で生じうる偏りに対する懸念を弱めることができます。\n\n`train_test_split` を複数回書いて実装することもできそうですが、より便利なパッケージが用意されています。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 先にtrain_test_splitしてしまっているので、改めて特徴量の準備\n\ntrain = pd.read_csv(\"../input/titanic/train.csv\")\ntest = pd.read_csv(\"../input/titanic/test.csv\")\ngender_submission = pd.read_csv(\"../input/titanic/gender_submission.csv\")\n\ndata = pd.concat([train, test], sort=False)\n\ndata['Sex'].replace(['male','female'], [0, 1], inplace=True)\ndata['Embarked'].fillna(('S'), inplace=True)\ndata['Embarked'] = data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ndata['Fare'].fillna(np.mean(data['Fare']), inplace=True)\ndata['Age'].fillna(data['Age'].median(), inplace=True)\ndata['FamilySize'] = data['Parch'] + data['SibSp'] + 1\ndata['IsAlone'] = 0\ndata.loc[data['FamilySize'] == 1, 'IsAlone'] = 1\n\ndelete_columns = ['Name', 'PassengerId', 'Ticket', 'Cabin']\ndata.drop(delete_columns, axis=1, inplace=True)\n\ntrain = data[:len(train)]\ntest = data[len(train):]\n\ny_train = train['Survived']\nX_train = train.drop('Survived', axis=1)\nX_test = test.drop('Survived', axis=1)\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\n\ny_preds = []\nmodels = []\noof_train = np.zeros((len(X_train),))\ncv = KFold(n_splits=5, shuffle=True, random_state=0)\n\ncategorical_features = ['Embarked', 'Pclass', 'Sex']\n\nparams = {\n    'objective': 'binary',\n    'max_bin': 300,\n    'learning_rate': 0.05,\n    'num_leaves': 40\n}\n\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(X_train)):\n    X_tr = X_train.loc[train_index, :]\n    X_val = X_train.loc[valid_index, :]\n    y_tr = y_train[train_index]\n    y_val = y_train[valid_index]\n\n    lgb_train = lgb.Dataset(X_tr, y_tr, categorical_feature=categorical_features)\n    lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train, categorical_feature=categorical_features)\n\n    model = lgb.train(\n        params, lgb_train,\n        valid_sets=[lgb_train, lgb_eval],\n        verbose_eval=10,\n        num_boost_round=1000,\n        early_stopping_rounds=10\n    )\n\n    oof_train[valid_index] = model.predict(X_val, num_iteration=model.best_iteration)\n    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n    y_preds.append(y_pred)\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross Validationを実施した際は、全体の平均をvalidationスコアと見なすことが多いです。このスコアのことを「CVスコア」、省略して単に「CV」とも呼びます。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(oof_train).to_csv('oof_train_kfold.csv', index=False)\n\nscores = [\n    m.best_score['valid_1']['binary_logloss'] for m in models\n]\nscore = sum(scores) / len(scores)\nprint('===CV scores===')\nprint(scores)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n\ny_pred_oof = (oof_train > 0.5).astype(int)\naccuracy_score(y_train, y_pred_oof)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds[0][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_sub = sum(y_preds) / len(y_preds)\ny_sub = (y_sub > 0.5).astype(int)\ny_sub[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['Survived'] = y_sub\nsub.to_csv(\"submission_lightgbm_kfold.csv\", index=False)\n\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"この予測値を提出すると、私の環境で0.76555というスコアが出ました。ホールドアウト検証の時よりも悪いスコアになってしまいました。その原因の一つは、データセットの分割方法だと推察されます。最後に、この部分を掘り下げて解説していきます。","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# データセットの分割方法\n\nデータセットの分割に当たっては、データセットや課題設定の特徴を意識するのが大切です。","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"ここまで使っていたKFoldは、特にデータセットや課題設定の特徴を考慮することなくデータセットを分割します。例えば、学習用・検証用データセット内の`y==1`の予測値の割合を見てみると次のようになりました。fold: 2, 4などで顕著に割合が異なっていると分かります。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\n\ncv = KFold(n_splits=5, shuffle=True, random_state=0)\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(X_train)):\n    X_tr = X_train.loc[train_index, :]\n    X_val = X_train.loc[valid_index, :]\n    y_tr = y_train[train_index]\n    y_val = y_train[valid_index]\n\n    print(f'fold: {fold_id}')\n    print(f'y_tr y==1 rate: {sum(y_tr)/len(y_tr)}')\n    print(f'y_val y==1 rate: {sum(y_val)/len(y_val)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"繰り返しになりますが、Kaggleの目的は未知のデータセットであるLBに対する性能を高めることです。未知のデータセットにおける`y==1`の割合は誰にも正確には分からないですが、既存のデータセットである学習用データセットと同様の割合だと近似するのが一般的です。つまり、データセットは`y==1`の割合を保つように分割するのが理想的です。\n\n`y==1`の割合が均等でない場合、`y==1`を重要視したり逆に軽視したりと、機械学習アルゴリズムの学習がうまくいかない傾向にあります。このような状況では適切に特徴を学習できず、未知のデータセットに対する性能が劣化してしまう可能性があります。KFoldを用いた場合にスコアが悪化した原因もここにあると考えられます。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, y_train)):\n    X_tr = X_train.loc[train_index, :]\n    X_val = X_train.loc[valid_index, :]\n    y_tr = y_train[train_index]\n    y_val = y_train[valid_index]\n\n    print(f'fold: {fold_id}')\n    print(f'y_tr y==1 rate: {sum(y_tr)/len(y_tr)}')\n    print(f'y_val y==1 rate: {sum(y_val)/len(y_val)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"この分割を用いて学習・予測を実行しましょう。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\n\ny_preds = []\nmodels = []\noof_train = np.zeros((len(X_train),))\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n\ncategorical_features = ['Embarked', 'Pclass', 'Sex']\n\nparams = {\n    'objective': 'binary',\n    'max_bin': 300,\n    'learning_rate': 0.05,\n    'num_leaves': 40\n}\n\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, y_train)):\n    X_tr = X_train.loc[train_index, :]\n    X_val = X_train.loc[valid_index, :]\n    y_tr = y_train[train_index]\n    y_val = y_train[valid_index]\n\n    lgb_train = lgb.Dataset(X_tr, y_tr, categorical_feature=categorical_features)\n    lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train, categorical_feature=categorical_features)\n\n    model = lgb.train(\n        params, lgb_train,\n        valid_sets=[lgb_train, lgb_eval],\n        verbose_eval=10,\n        num_boost_round=1000,\n        early_stopping_rounds=10\n    )\n\n    oof_train[valid_index] = model.predict(X_val, num_iteration=model.best_iteration)\n    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n    y_preds.append(y_pred)\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(oof_train).to_csv('oof_train_skfold.csv', index=False)\nprint(oof_train[:10])\n\nscores = [\n    m.best_score['valid_1']['binary_logloss'] for m in models\n]\nscore = sum(scores) / len(scores)\nprint('===CV scores===')\nprint(scores)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n\ny_pred_oof = (oof_train > 0.5).astype(int)\naccuracy_score(y_train, y_pred_oof)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_sub = sum(y_preds) / len(y_preds)\ny_sub = (y_sub > 0.5).astype(int)\ny_sub[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['Survived'] = y_sub\nsub.to_csv(\"submission_lightgbm_skfold.csv\", index=False)\n\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"この予測値を提出すると、私の環境で0.77511というスコアが出ました。`KFold`の時よりも、ホールドアウト検証の時よりも良いスコアが出ています。","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}