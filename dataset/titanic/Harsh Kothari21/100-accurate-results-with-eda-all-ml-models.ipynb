{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"<h1 style=\"color:green\"><center>Don't forget to upvote if you like it! It's free! :)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**For absoulte beginners, do check the notebook**\n\n# [Beginners Notebook with EDA](https://www.kaggle.com/harshkothari21/beginners-notebook-90-accuracy-with-eda)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**- Want to know how to analize dataset using pandas, matplotlib and seaborn?**\n\n**- Want to know how to solve classification problems!**\n\n**- Want to know how to train the best model?**\n\n**- Want to know how to use scaling?**\n\n**- Want to know how to improve your model accuracy?**\n\n## **This notebook will answer all of your questions!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Table of content \n\n- EDA\n- Handle Missing Values\n- Feature Engineering\n- linear Regression\n- Logistic Regression\n- Scalling\n- KNN Classifier\n- Support Vector Machine(SVM)\n- Kernelize SVM\n- Decision Tree\n- Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport re\nimport warnings\nfrom statistics import mode\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/titanic/train.csv')\ntest = pd.read_csv('../input/titanic/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train.Survived","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='color:red'>EDA (Exploratory Data Analysis)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variable Name | Description\n--------------|-------------\nSurvived      | Survived (1) or died (0)\nPclass        | Passenger's class\nName          | Passenger's name\nSex           | Passenger's sex\nAge           | Passenger's age\nSibSp         | Number of siblings/spouses aboard\nParch         | Number of parents/children aboard\nTicket        | Ticket number\nFare          | Fare\nCabin         | Cabin\nEmbarked      | Port of embarkation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Unique Values in Pclass :{train.Pclass.unique()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Unique Values in SibSp :{train.SibSp.unique()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hehe!, null values spotted!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Unique Values in Embarked :{train.Embarked.unique()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's look at target feature first**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.countplot(train.Survived)\nplt.title('Number of passenger Survived');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"Sex\", data=train)\nplt.title('Number of passenger Survived');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So the plot says we have more number of non-survived people and females are more likely to survived than male!. so, 'Sex' looks like a very strong explanatory variable, and it can be good choice for our model!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Let's first vizualize null values on our training set on graph**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nplt.figure(figsize=(10,5))\nsns.heatmap(train.isnull(), yticklabels = False, cmap='plasma')\nplt.title('Null Values in Training Set');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will be dealling with null values later on.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Let's analysize Pclass**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.style.use('fivethirtyeight')\n\nplt.subplot(1,2,1)\nsns.countplot(train['Pclass'])\nplt.title('Count Plot for PClass');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"Pclass\", data=train)\nplt.title('Number of passenger Survived');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looking at some satistical data!","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pclass1 = train[train.Pclass == 1]['Survived'].value_counts(normalize=True).values[0]*100\npclass2 = train[train.Pclass == 2]['Survived'].value_counts(normalize=True).values[1]*100\npclass3 = train[train.Pclass == 3]['Survived'].value_counts(normalize=True).values[1]*100\n\nprint(\"Lets look at some satistical data!\\n\")\nprint(\"Pclaas-1: {:.1f}% People Survived\".format(pclass1))\nprint(\"Pclaas-2: {:.1f}% People Survived\".format(pclass2))\nprint(\"Pclaas-3: {:.1f}% People Survived\".format(pclass3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Wow!, Pclass is also a good feature to train our model.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**It's Time to look at the Age column!**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Age'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Most Important thing when plotting histograms : Arrange Number of Bins**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Age'].hist(bins=40)\nplt.title('Age Distribution');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Age column has non-uniform data and many outliers**\n\n**Outlier** : An outlier is an observation that lies an abnormal distance from other values in a random sample from a population.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# set plot size\nplt.figure(figsize=(15, 3))\n\n# plot a univariate distribution of Age observations \nsns.distplot(train[(train[\"Age\"] > 0)].Age, kde_kws={\"lw\": 3}, bins = 50)\n\n# set titles and labels\nplt.title('Distrubution of passengers age',fontsize= 14)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\n# clean layout\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Age by surviving status**\n\nDid age had a big influence on chances to survive?\nTo visualize two age distributions, grouped by surviving status I am using boxlot and stripplot showed together:","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 3))\n\n# Draw a box plot to show Age distributions with respect to survival status.\nsns.boxplot(y = 'Survived', x = 'Age', data = train,\n     palette=[\"#3f3e6fd1\", \"#85c6a9\"], fliersize = 0, orient = 'h')\n\n# Add a scatterplot for each category.\nsns.stripplot(y = 'Survived', x = 'Age', data = train,\n     linewidth = 0.6, palette=[\"#3f3e6fd1\", \"#85c6a9\"], orient = 'h')\n\nplt.yticks( np.arange(2), ['drowned', 'survived'])\nplt.title('Age distribution grouped by surviving status (train data)',fontsize= 14)\nplt.ylabel('Passenger status after the tragedy')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's look at Number of siblings/spouses aboard**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.countplot(train['SibSp'])\nplt.title('Number of siblings/spouses aboard');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"SibSp\", data=train)\nplt.legend(loc='right')\nplt.title('Number of passenger Survived');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Looks like single person Non-survived count is almost double than survived, while others have 50-50 % ratio**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Now Looking at Port of embarkation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.countplot(train['Embarked'])\nplt.title('Number of Port of embarkation');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"Embarked\", data=train)\nplt.legend(loc='right')\nplt.title('Number of passenger Survived');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Can't say much!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Look in to relationships among dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train.corr(), annot=True)\nplt.title('Corelation Matrix');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Configure the heatmap**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train.corr()\nsns.heatmap(corr[((corr >= 0.3) | (corr <= -0.3)) & (corr != 1)], annot=True, linewidths=.5, fmt= '.2f')\nplt.title('Configured Corelation Matrix');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fare vs Embarked**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"Embarked\", y=\"Fare\", kind=\"violin\", inner=None,\n            data=train, height = 6, order = ['C', 'Q', 'S'])\nplt.title('Distribution of Fare by Embarked')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The wider fare distribution among passengers who embarked in Cherbourg. It makes scence - many first-class passengers boarded the ship here, but the share of third-class passengers is quite significant.\n- The smallest variation in the price of passengers who boarded in q. Also, the average price of these passengers is the smallest, I think this is due to the fact that the path was supposed to be the shortest + almost all third-class passengers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Fare vs Pclass**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"Pclass\", y=\"Fare\", kind=\"swarm\", data=train, height = 6)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that the distribution of prices for the second and third class is very similar. The distribution of first-class prices is very different, has a larger spread, and on average prices are higher.\n\nLet's add colours to our points to indicate surviving status of passenger (there will be only data from training part of the dataset):","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"Pclass\", y=\"Fare\",  hue = \"Survived\", kind=\"swarm\", data=train, \n                                    palette=[\"#3f3e6fd1\", \"#85c6a9\"], height = 6)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at some maximum and minimum values of features!","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train['Fare'].nlargest(10).plot(kind='bar', title = '10 largest Fare', color = ['#C62D42', '#FE6F5E']);\nplt.xlabel('Index')\nplt.ylabel('Fare');","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train['Age'].nlargest(10).plot(kind='bar', color = ['#5946B2','#9C51B6']);\nplt.title('10 largest Ages')\nplt.xlabel('Index')\nplt.ylabel('Ages');","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train['Age'].nsmallest(10).plot(kind='bar', color = ['#A83731','#AF6E4D'])\nplt.title('10 smallest Ages')\nplt.xlabel('Index')\nplt.ylabel('Ages');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='color:red'>Handle Missing Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Some statistical values of null values in dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"one of the effectitve way to fill the null values is by finding correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Pclass and age, as they had max relation in the entire set we are going to replace missing age values with median age calculated per class**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.Age.isnull(), 'Age'] = train.groupby(\"Pclass\").Age.transform('median')\n\n\n#Same thing for test set\ntest.loc[test.Age.isnull(), 'Age'] = test.groupby(\"Pclass\").Age.transform('median')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Embarked.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **As maximum values in train set is S let's replace it with the null values**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Embarked'] = train['Embarked'].fillna(mode(train['Embarked']))\n\n#Applying the same technique for test set\ntest['Embarked'] = test['Embarked'].fillna(mode(test['Embarked']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Also, corr(Fare, Pclass) is the highest correlation in absolute numbers for 'Fare', so we'll use Pclass again to impute the missing values!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Fare']  = train.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))\ntest['Fare']  = test.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Cabin.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> So many different values let's place missing values with U as \"Unknown\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Cabin'] = train['Cabin'].fillna('U')\ntest['Cabin'] = test['Cabin'].fillna('U')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Sex.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Sex is categorical data so we can replace male to 0 and femail to 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Sex'][train['Sex'] == 'male'] = 0\ntrain['Sex'][train['Sex'] == 'female'] = 1\n\ntest['Sex'][test['Sex'] == 'male'] = 0\ntest['Sex'][test['Sex'] == 'female'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Embarked.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Let's encode with OneHotEncoder technique","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder()\ntemp = pd.DataFrame(encoder.fit_transform(train[['Embarked']]).toarray(), columns=['S', 'C', 'Q'])\ntrain = train.join(temp)\ntrain.drop(columns='Embarked', inplace=True)\n\ntemp = pd.DataFrame(encoder.transform(test[['Embarked']]).toarray(), columns=['S', 'C', 'Q'])\ntest = test.join(temp)\ntest.drop(columns='Embarked', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train.Cabin.tolist()[0:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can get the alphabets by running regular expression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Cabin'] = train['Cabin'].map(lambda x:re.compile(\"([a-zA-Z])\").search(x).group())\ntest['Cabin'] = test['Cabin'].map(lambda x:re.compile(\"([a-zA-Z])\").search(x).group())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Cabin.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cabin_category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8, 'U':9}\ntrain['Cabin'] = train['Cabin'].map(cabin_category)\ntest['Cabin'] = test['Cabin'].map(cabin_category)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What is in the name?\nEach passenger Name value contains the title of the passenger which we can extract and discover.\nTo create new variable \"Title\":\n\n- I am using method 'split' by comma to divide Name in two parts and save the second part\n- I am splitting saved part by dot and save first part of the result\n- To remove spaces around the title I am using 'split' method\n- To visualize, how many passengers hold each title, I chose countplot.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Name'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)\ntest['Name'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Name'].unique().tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Wohh that's lot's of title. So, let's bundle them**\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.rename(columns={'Name' : 'Title'}, inplace=True)\ntrain['Title'] = train['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess', \n                                       'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')\n                                      \ntest.rename(columns={'Name' : 'Title'}, inplace=True)\ntest['Title'] = test['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess', \n                                       'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Title'].value_counts(normalize = True) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Better! let's convert to numeric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = OneHotEncoder()\ntemp = pd.DataFrame(encoder.fit_transform(train[['Title']]).toarray())\ntrain = train.join(temp)\ntrain.drop(columns='Title', inplace=True)\n\ntemp = pd.DataFrame(encoder.transform(test[['Title']]).toarray())\ntest = test.join(temp)\ntest.drop(columns='Title', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmmm... but we know from part 2 that Sibsp is the number of siblings / spouses aboard the Titanic, and Parch is the number of parents / children aboard the Titanic... So, what is another straightforward feature to engineer?\nYes, it is the size of each family aboard!\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['familySize'] = train['SibSp'] + train['Parch'] + 1\ntest['familySize'] = test['SibSp'] + test['Parch'] + 1","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (15,4))\n\nax1 = fig.add_subplot(121)\nax = sns.countplot(train['familySize'], ax = ax1)\n\n# calculate passengers for each category\nlabels = (train['familySize'].value_counts())\n# add result numbers on barchart\nfor i, v in enumerate(labels):\n    ax.text(i, v+6, str(v), horizontalalignment = 'center', size = 10, color = 'black')\n    \nplt.title('Passengers distribution by family size')\nplt.ylabel('Number of passengers')\n\nax2 = fig.add_subplot(122)\nd = train.groupby('familySize')['Survived'].value_counts(normalize = True).unstack()\nd.plot(kind='bar', color=[\"#3f3e6fd1\", \"#85c6a9\"], stacked='True', ax = ax2)\nplt.title('Proportion of survived/drowned passengers by family size (train data)')\nplt.legend(( 'Drowned', 'Survived'), loc=(1.04,0))\nplt.xticks(rotation = False)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop redundant features\ntrain = train.drop(['SibSp', 'Parch', 'Ticket'], axis = 1)\ntest = test.drop(['SibSp', 'Parch', 'Ticket'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA(Principle component analysis)\n\nlet’s visualize our final dataset by implementing PCA and plot the graph","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = train.columns[2:]\nfrom sklearn.preprocessing import StandardScaler\nX_train = StandardScaler().fit_transform(train.drop(columns=[\"PassengerId\",\"Survived\"]))\n\nnew_df = pd.DataFrame(X_train, columns=columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 2)\ndf_pca = pca.fit_transform(new_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(8, 6))\nplt.scatter(df_pca[:, 0], df_pca[:, 1], c = target, cmap ='plasma')\n# labeling x and y axes\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ourdataset contain some outliers and randomness but still let's use this to train the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Dateset is completely ready now!\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived', 'PassengerId'], axis=1), train['Survived'], test_size = 0.2, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression\n\nLinear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output). Hence, the name is Linear Regression.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\n#R-Squared Score\nprint(\"R-Squared for Train set: {:.3f}\".format(linreg.score(X_train, y_train)))\nprint(\"R-Squared for test set: {:.3f}\" .format(linreg.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it's clear from the score that linear regression doesn't makes sence","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression\n\nAs our target variable is discrete value(i.e 0 and 1) logistic regression is more likely to fit well the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(max_iter=10000, C=50)\nlogreg.fit(X_train, y_train)\n\n#R-Squared Score\nprint(\"R-Squared for Train set: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"R-Squared for test set: {:.3f}\" .format(logreg.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Haha, much better!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> **Additionally, you can view the y-intercept and coefficients**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(logreg.intercept_)\nprint(logreg.coef_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MinMaxScaler\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n# Magic Weapon#1: **Let's Scale our data and re-train the model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\n\n# we must apply the scaling to the test set that we computed for the training set\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(max_iter=10000)\nlogreg.fit(X_train_scaled, y_train)\n\n#R-Squared Score\nprint(\"R-Squared for Train set: {:.3f}\".format(logreg.score(X_train_scaled, y_train)))\nprint(\"R-Squared for test set: {:.3f}\" .format(logreg.score(X_test_scaled, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's try some other Techniques**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# KNN Classifier\n\nK Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms.KNN algorithm used for both classification and regression problems.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknnclf = KNeighborsClassifier(n_neighbors=7)\n\n# Train the model using the training sets\nknnclf.fit(X_train, y_train)\ny_pred = knnclf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's try on scaled data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knnclf = KNeighborsClassifier(n_neighbors=7)\n\n# Train the model using the scaled training sets\nknnclf.fit(X_train_scaled, y_train)\ny_pred = knnclf.predict(X_test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### That increases the accuracy a lot!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machine(SVM)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nsvmclf = LinearSVC(C=50)\nsvmclf.fit(X_train, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svmclf.score(X_train, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svmclf.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's try on scaled data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svmclf = LinearSVC()\nsvmclf.fit(X_train_scaled, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svmclf.score(X_train_scaled, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svmclf.score(X_test_scaled, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Kernelize SVM\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Magic Weapon#2: **Support Vector Machine with RBF kernel**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvcclf = SVC(gamma=0.1)\nsvcclf.fit(X_train, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svcclf.score(X_train, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svcclf.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Look Accuracy on Training data, lol**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svcclf = SVC(gamma=50)\nsvcclf.fit(X_train_scaled, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svcclf.score(X_train_scaled, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svcclf.score(X_test_scaled, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndtclf = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(dtclf.score(X_train, y_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'\n     .format(dtclf.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performed Well!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Random Forest\n\nSecondly, I would like to introduce one of the most popular algorithms for classification (but also regression, etc), Random Forest! In a nutshell, Random Forest is an ensembling learning algorithm which combines decision trees in order to increase performance and avoid overfitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfclf = RandomForestClassifier(random_state=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Magic Weapon #3: Hyperparameter Tuning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Below we set the hyperparameter grid of values with 4 lists of values:\n\n- 'criterion' : A function which measures the quality of a split.\n- 'n_estimators' : The number of trees of our random forest.\n- 'max_features' : The number of features to choose when looking for the best way of splitting.\n- 'max_depth' : the maximum depth of a decision tree.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set our parameter grid\nparam_grid = { \n    'criterion' : ['gini', 'entropy'],\n    'n_estimators': [100, 300, 500],\n    'max_features': ['auto', 'log2'],\n    'max_depth' : [3, 5, 7]    \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nrandomForest_CV = GridSearchCV(estimator = rfclf, param_grid = param_grid, cv = 5)\nrandomForest_CV.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's print our optimal hyperparameters set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"randomForest_CV.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(random_state = 2, criterion = 'gini', max_depth = 7, max_features = 'auto', n_estimators = 100)\n\nrf_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = rf_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, predictions) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's print our solutions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Magic Weapon #4: All model Accuracy Score","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Linear Model\nprint(\"Linear Model R-Squared for Train set: {:.3f}\".format(linreg.score(X_train, y_train)))\nprint(\"Linear Model R-Squared for test set: {:.3f}\" .format(linreg.score(X_test, y_test)))\nprint()\n\n#Logistic Regression\nprint(\"Logistic Regression R-Squared for Train set: {:.3f}\".format(logreg.score(X_train_scaled, y_train)))\nprint(\"Logistic Regression R-Squared for test set: {:.3f}\" .format(logreg.score(X_test_scaled, y_test)))\nprint()\n\n#KNN Classifier\nprint(\"KNN Classifier Accuracy:\",accuracy_score(y_test, y_pred))\nprint()\n\n#SVM\nprint('SVM Accuracy on training set: {:.2f}'\n     .format(svmclf.score(X_train_scaled, y_train)))\nprint('SVM Accuracy on test set: {:.2f}'\n     .format(svmclf.score(X_test_scaled, y_test)))\nprint()\n\n#Kerelize SVM\nprint('SVC Accuracy on training set: {:.2f}'\n     .format(svcclf.score(X_train_scaled, y_train)))\nprint('Accuracy on test set: {:.2f}'\n     .format(svcclf.score(X_test_scaled, y_test)))\nprint()\n\n#Decision Tree\nprint('Accuracy of Decision Tree on training set: {:.2f}'\n     .format(dtclf.score(X_train, y_train)))\nprint('Accuracy of Decision Tree on test set: {:.2f}'\n     .format(dtclf.score(X_test, y_test)))\nprint()\n\n#Random Forest\nprint('Random Forest Accuracy:{:.3f}'.format(accuracy_score(y_test, predictions) * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submitting the solutions\n\nI am choosing SVC model for the instance, you can try submiting solution with different models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\n\ntrain_conv = scaler.fit_transform(train.drop(['Survived', 'PassengerId'], axis=1))\ntest_conv = scaler.transform(test.drop(['PassengerId'], axis = 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svcclf = SVC(gamma=50)\nsvcclf.fit(train_conv, train['Survived'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Survived'] = svcclf.predict(test_conv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['PassengerId', 'Survived']].to_csv('MySubmission1.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plz Upvote!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}