{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Titanic: Machine Learning from Disaster"},{"metadata":{},"cell_type":"markdown","source":"### In this notebook we are going to analyze the given titanic dataset, apply feature engineering and use different machine learning models with advanced hyperparameter techniques.\n\n### We will also understand what each hyperparamter in models means."},{"metadata":{},"cell_type":"markdown","source":"### *Top score achieved by this approach is 0.79186 ( Top 8% )*\n![bhh.JPG](attachment:bhh.JPG)","attachments":{"bhh.JPG":{"image/jpeg":"/9j/4AAQSkZJRgABAQEAeAB4AAD/4RDuRXhpZgAATU0AKgAAAAgABAE7AAIAAAAMAAAISodpAAQAAAABAAAIVpydAAEAAAAYAAAQzuocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG5pdGlzaCBodWxrAAAFkAMAAgAAABQAABCkkAQAAgAAABQAABC4kpEAAgAAAAM4NAAAkpIAAgAAAAM4NAAA6hwABwAACAwAAAiYAAAAABzqAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMjAyMDoxMDoxOCAxNTo0MDozNgAyMDIwOjEwOjE4IDE1OjQwOjM2AAAAbgBpAHQAaQBzAGgAIABoAHUAbABrAAAA/+ELHmh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8APD94cGFja2V0IGJlZ2luPSfvu78nIGlkPSdXNU0wTXBDZWhpSHpyZVN6TlRjemtjOWQnPz4NCjx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iPjxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iLz48cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0idXVpZDpmYWY1YmRkNS1iYTNkLTExZGEtYWQzMS1kMzNkNzUxODJmMWIiIHhtbG5zOnhtcD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLyI+PHhtcDpDcmVhdGVEYXRlPjIwMjAtMTAtMThUMTU6NDA6MzYuODQyPC94bXA6Q3JlYXRlRGF0ZT48L3JkZjpEZXNjcmlwdGlvbj48cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0idXVpZDpmYWY1YmRkNS1iYTNkLTExZGEtYWQzMS1kMzNkNzUxODJmMWIiIHhtbG5zOmRjPSJodHRwOi8vcHVybC5vcmcvZGMvZWxlbWVudHMvMS4xLyI+PGRjOmNyZWF0b3I+PHJkZjpTZXEgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj48cmRmOmxpPm5pdGlzaCBodWxrPC9yZGY6bGk+PC9yZGY6U2VxPg0KCQkJPC9kYzpjcmVhdG9yPjwvcmRmOkRlc2NyaXB0aW9uPjwvcmRmOlJERj48L3g6eG1wbWV0YT4NCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgPD94cGFja2V0IGVuZD0ndyc/Pv/bAEMABwUFBgUEBwYFBggHBwgKEQsKCQkKFQ8QDBEYFRoZGBUYFxseJyEbHSUdFxgiLiIlKCkrLCsaIC8zLyoyJyorKv/bAEMBBwgICgkKFAsLFCocGBwqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKv/AABEIAK0DdQMBIgACEQEDEQH/xAAfAAABBQEBAQEBAQAAAAAAAAAAAQIDBAUGBwgJCgv/xAC1EAACAQMDAgQDBQUEBAAAAX0BAgMABBEFEiExQQYTUWEHInEUMoGRoQgjQrHBFVLR8CQzYnKCCQoWFxgZGiUmJygpKjQ1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4eLj5OXm5+jp6vHy8/T19vf4+fr/xAAfAQADAQEBAQEBAQEBAAAAAAAAAQIDBAUGBwgJCgv/xAC1EQACAQIEBAMEBwUEBAABAncAAQIDEQQFITEGEkFRB2FxEyIygQgUQpGhscEJIzNS8BVictEKFiQ04SXxFxgZGiYnKCkqNTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqCg4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2dri4+Tl5ufo6ery8/T19vf4+fr/2gAMAwEAAhEDEQA/AJNd8Q6zD4i1KKHV7+ONLqVURLlwFAcgADPAqh/wkuu/9BrUf/At/wDGk8Rf8jRqn/X5N/6Gaza+wp04ci0Wx+bVq1X2kvee76mn/wAJLrv/AEGtR/8AAt/8aP8AhJdd/wCg1qP/AIFv/jWZRV+zh2Rl7ar/ADP7zT/4SXXf+g1qP/gW/wDjR/wkuu/9BrUf/At/8azKKPZw7IPbVf5n95p/8JLrv/Qa1H/wLf8Axo/4SXXf+g1qP/gW/wDjWZRR7OHZB7ar/M/vNP8A4SXXf+g1qP8A4Fv/AI0f8JLrv/Qa1H/wLf8AxrMoo9nDsg9tV/mf3mn/AMJLrv8A0GtR/wDAt/8AGj/hJdd/6DWo/wDgW/8AjWZRR7OHZB7ar/M/vNP/AISXXf8AoNaj/wCBb/40f8JLrv8A0GtR/wDAt/8AGsyij2cOyD21X+Z/eaf/AAkuu/8AQa1H/wAC3/xo/wCEl13/AKDWo/8AgW/+NZlFHs4dkHtqv8z+80/+El13/oNaj/4Fv/jR/wAJLrv/AEGtR/8AAt/8azKKPZw7IPbVf5n95p/8JLrv/Qa1H/wLf/Gj/hJdd/6DWo/+Bb/41mUUezh2Qe2q/wAz+80/+El13/oNaj/4Fv8A40f8JLrv/Qa1H/wLf/Gsyij2cOyD21X+Z/eaf/CS67/0GtR/8C3/AMaP+El13/oNaj/4Fv8A41mUUezh2Qe2q/zP7zT/AOEl13/oNaj/AOBb/wCNH/CS67/0GtR/8C3/AMazKKPZw7IPbVf5n95p/wDCS67/ANBrUf8AwLf/ABo/4SXXf+g1qP8A4Fv/AI1mUUezh2Qe2q/zP7zT/wCEl13/AKDWo/8AgW/+NH/CS67/ANBrUf8AwLf/ABrMoo9nDsg9tV/mf3mn/wAJLrv/AEGtR/8AAt/8aP8AhJdd/wCg1qP/AIFv/jWZRR7OHZB7ar/M/vNP/hJdd/6DWo/+Bb/40f8ACS67/wBBrUf/AALf/Gsyij2cOyD21X+Z/eaf/CS67/0GtR/8C3/xo/4SXXf+g1qP/gW/+NZlFHs4dkHtqv8AM/vNP/hJdd/6DWo/+Bb/AONH/CS67/0GtR/8C3/xrMoo9nDsg9tV/mf3mn/wkuu/9BrUf/At/wDGj/hJdd/6DWo/+Bb/AONZlFHs4dkHtqv8z+80/wDhJdd/6DWo/wDgW/8AjR/wkuu/9BrUf/At/wDGsyij2cOyD21X+Z/eaf8Awkuu/wDQa1H/AMC3/wAaP+El13/oNaj/AOBb/wCNZlFHs4dkHtqv8z+80/8AhJdd/wCg1qP/AIFv/jR/wkuu/wDQa1H/AMC3/wAazKKPZw7IPbVf5n95p/8ACS67/wBBrUf/AALf/Gj/AISXXf8AoNaj/wCBb/41mUUezh2Qe2q/zP7zT/4SXXf+g1qP/gW/+NH/AAkuu/8AQa1H/wAC3/xrMoo9nDsg9tV/mf3mn/wkuu/9BrUf/At/8aP+El13/oNaj/4Fv/jWZRR7OHZB7ar/ADP7zT/4SXXf+g1qP/gW/wDjR/wkuu/9BrUf/At/8azKKPZw7IPbVf5n95p/8JLrv/Qa1H/wLf8Axo/4SXXf+g1qP/gW/wDjWZRR7OHZB7ar/M/vNP8A4SXXf+g1qP8A4Fv/AI0f8JLrv/Qa1H/wLf8AxrMoo9nDsg9tV/mf3mn/AMJLrv8A0GtR/wDAt/8AGj/hJdd/6DWo/wDgW/8AjWZRR7OHZB7ar/M/vNP/AISXXf8AoNaj/wCBb/40f8JLrv8A0GtR/wDAt/8AGsyij2cOyD21X+Z/eaf/AAkuu/8AQa1H/wAC3/xo/wCEl13/AKDWo/8AgW/+NZlFHs4dkHtqv8z+80/+El13/oNaj/4Fv/jR/wAJLrv/AEGtR/8AAt/8azKKPZw7IPbVf5n95p/8JLrv/Qa1H/wLf/Gj/hJdd/6DWo/+Bb/41mUUezh2Qe2q/wAz+80/+El13/oNaj/4Fv8A40f8JLrv/Qa1H/wLf/Gsyij2cOyD21X+Z/eaf/CS67/0GtR/8C3/AMaP+El13/oNaj/4Fv8A41mUUezh2Qe2q/zP7zT/AOEl13/oNaj/AOBb/wCNH/CS67/0GtR/8C3/AMazKKPZw7IPbVf5n95p/wDCS67/ANBrUf8AwLf/ABo/4SXXf+g1qP8A4Fv/AI1mUUezh2Qe2q/zP7zT/wCEl13/AKDWo/8AgW/+NH/CS67/ANBrUf8AwLf/ABrMoo9nDsg9tV/mf3mn/wAJLrv/AEGtR/8AAt/8aP8AhJdd/wCg1qP/AIFv/jWZRR7OHZB7ar/M/vNP/hJdd/6DWo/+Bb/40f8ACS67/wBBrUf/AALf/Gsyij2cOyD21X+Z/eaf/CS67/0GtR/8C3/xo/4SXXf+g1qP/gW/+NZlFHs4dkHtqv8AM/vNP/hJdd/6DWo/+Bb/AONH/CS67/0GtR/8C3/xrMoo9nDsg9tV/mf3mn/wkuu/9BrUf/At/wDGj/hJdd/6DWo/+Bb/AONZlFHs4dkHtqv8z+80/wDhJdd/6DWo/wDgW/8AjR/wkuu/9BrUf/At/wDGsyij2cOyD21X+Z/eaf8Awkuu/wDQa1H/AMC3/wAaP+El13/oNaj/AOBb/wCNZlFHs4dkHtqv8z+80/8AhJdd/wCg1qP/AIFv/jR/wkuu/wDQa1H/AMC3/wAazKKPZw7IPbVf5n95p/8ACS67/wBBrUf/AALf/Gj/AISXXf8AoNaj/wCBb/41mUUezh2Qe2q/zP7zT/4SXXf+g1qP/gW/+NH/AAkuu/8AQa1H/wAC3/xrMoo9nDsg9tV/mf3mn/wkuu/9BrUf/At/8aP+El13/oNaj/4Fv/jWZRR7OHZB7ar/ADP7zT/4SXXf+g1qP/gW/wDjR/wkuu/9BrUf/At/8azKKPZw7IPbVf5n95p/8JLrv/Qa1H/wLf8Axo/4SXXf+g1qP/gW/wDjWZRR7OHZB7ar/M/vNP8A4SXXf+g1qP8A4Fv/AI0f8JLrv/Qa1H/wLf8AxrMoo9nDsg9tV/mf3mn/AMJLrv8A0GtR/wDAt/8AGj/hJdd/6DWo/wDgW/8AjWZRR7OHZB7ar/M/vNP/AISXXf8AoNaj/wCBb/40f8JLrv8A0GtR/wDAt/8AGsyij2cOyD21X+Z/eaf/AAkuu/8AQa1H/wAC3/xorMoo9nDsg9tV/mf3ml4i/wCRo1T/AK/Jv/QzWbWl4i/5GjVP+vyb/wBDNZtFP4F6BW/iy9WFdHr3h2x0vw/p+oWOoG+NzLJHI6ptjBXH3c8kc4yeuK5ytq91i3ufB+maSiSie0nlkkYgbSHxjBznt6UqnNePL31LoeztNTXTT1Lvg/wkniRbx7m5a2jiCxwsAP3kzZ2rz9P5Vnab4Z1TVFma3hREgkETvPKsS+YeiAsRlj6VsaZ40j0Hw7ZWOl2FvPcLO1xcSXsAdRJkBCmG4IA68VZv/EXhzV0vbW4S/s7aa/GoRtFEjNvKBZEI3dyCQc9+RWDlWU5O2nT5f57nVGnhZUoq/vLfpe92tXppovm/lnR+DLmPSVvr1wjrqIspbQSRrJ74Jb72eAuOnzdOaraj4Xv4NS8u0s5vJmvGtLcSyRs5cHG1tpwDz9O/StG38R6QmmSWbx30ax6qt/bH5ZCygbdrkkc47jNXbLx5ZWuoa1cPbTyrPcveacCq/upirLluemGHTPSlz1027X/pf8EtU8I4pOVvPf8Am8vT7zCg8Ga5cRMyWsYId40ja4jDSshIcIM/PjB6Z9qr+HtFGtak8M1wLW3t4XnuZiufLjXqQO56Cul0nxrZwaLp8VzPd21zp6sv7i0glM2WLKQ8gJQgnng/nXPeHtci0vVriW+hee0vYZLe5RCA5R+pB6Z6GqUqzU01tsZOGFTptN62v+HbXTbz6E99pOi3dmk3hm9upZ/OWE2V4qLM5boybTyM8Y6io7jwbrdtLBGbaORpphbjyriNwsp/5ZsQcK31q7BqvhvQrqyudFtry8uYbpJ3nu8R7UH8CqrEEnuT+FaFr4r0PR5B/Zov7iO41GO9uDNGqmNVJOxfmO5snqcVPPVj8Kb9f6/Mfs6ErubSfk9OvrfotNtzKXwBr7FNsFud7FAftcWPMHWPO77/ALVXs/Bmt30KyxW0aI8jQp5txHGXkU4KAFs7s9q0ovFlilvZRmK4zb64dRbCrzHkcDn73t0962JdR0WTQNJ1HU5LyNF1O5uoUgRWZv3gbawJGCeOee/1qXVrxtdb+X/BNY4fCzvyt6ea/wAjk7LwhrN/GzQ2ypiVoVWaZI2kkXqihiCxHtV2z8HO2l6fqF1IWW6vvsr2kUkayjkDjc33snG0jjqeK24fH9ndRg3b3NhJDdzTr9ntIZzIrvuChpB8jA9xxWbaeLLD7Fai+W8Nxa6x/aAKojCRSylgxyuGwD0GCcdKfPiHurf1/XoS6WDi/dlff+v63IJfBpis7e/MpaGbUjaG1EsXnBQwXruwX5xjHHU8VB/whup3d5dDTrYpBHcyW8K3U8aSSOpOUHOGYY525FXj4q0yWzjSWO7WS31ltQi2opDozAkH5uDgHpkZxzVl/FWh6hcW0+oi/hbT9Rnu7dYUVvOSSXzArZYbWBxzyMZpKddLVfh/Vx+zwstL/j5vrbQw7HwZrmoW6TQWqKsjOkQlnRGkdThkCk53DB4x2rDdGjdkdSrKcEEcg117+MbSfUdEu5YJlaxv7i7uFUA5EkocBeecAHriuWv51utSubiMEJLK7qG6gEk810U5VW/fX9XOSvChFfu3f9dPToyvRRRW5yhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAGl4i/5GjVP+vyb/wBDNZtdFrvh/WZvEWpSw6RfyRvdSsjpbOQwLkgg45FUP+Ea13/oC6j/AOAj/wCFYU6kORarY6q1Gr7SXuvd9DMorT/4RrXf+gLqP/gI/wDhR/wjWu/9AXUf/AR/8Kv2kO6MvY1f5X9xmUVp/wDCNa7/ANAXUf8AwEf/AAo/4RrXf+gLqP8A4CP/AIUe0h3Qexq/yv7jMorT/wCEa13/AKAuo/8AgI/+FH/CNa7/ANAXUf8AwEf/AAo9pDug9jV/lf3GZRWn/wAI1rv/AEBdR/8AAR/8KP8AhGtd/wCgLqP/AICP/hR7SHdB7Gr/ACv7jMorT/4RrXf+gLqP/gI/+FH/AAjWu/8AQF1H/wABH/wo9pDug9jV/lf3GZRWn/wjWu/9AXUf/AR/8KP+Ea13/oC6j/4CP/hR7SHdB7Gr/K/uMyitP/hGtd/6Auo/+Aj/AOFH/CNa7/0BdR/8BH/wo9pDug9jV/lf3GZRWn/wjWu/9AXUf/AR/wDCj/hGtd/6Auo/+Aj/AOFHtId0Hsav8r+4zKK0/wDhGtd/6Auo/wDgI/8AhR/wjWu/9AXUf/AR/wDCj2kO6D2NX+V/cZlFaf8AwjWu/wDQF1H/AMBH/wAKP+Ea13/oC6j/AOAj/wCFHtId0Hsav8r+4zKK0/8AhGtd/wCgLqP/AICP/hR/wjWu/wDQF1H/AMBH/wAKPaQ7oPY1f5X9xmUVp/8ACNa7/wBAXUf/AAEf/Cj/AIRrXf8AoC6j/wCAj/4Ue0h3Qexq/wAr+4zKK0/+Ea13/oC6j/4CP/hR/wAI1rv/AEBdR/8AAR/8KPaQ7oPY1f5X9xmUVp/8I1rv/QF1H/wEf/Cj/hGtd/6Auo/+Aj/4Ue0h3Qexq/yv7jMorT/4RrXf+gLqP/gI/wDhR/wjWu/9AXUf/AR/8KPaQ7oPY1f5X9xmUVp/8I1rv/QF1H/wEf8Awo/4RrXf+gLqP/gI/wDhR7SHdB7Gr/K/uMyitP8A4RrXf+gLqP8A4CP/AIUf8I1rv/QF1H/wEf8Awo9pDug9jV/lf3GZRWn/AMI1rv8A0BdR/wDAR/8ACj/hGtd/6Auo/wDgI/8AhR7SHdB7Gr/K/uMyitP/AIRrXf8AoC6j/wCAj/4Uf8I1rv8A0BdR/wDAR/8ACj2kO6D2NX+V/cZlFaf/AAjWu/8AQF1H/wABH/wo/wCEa13/AKAuo/8AgI/+FHtId0Hsav8AK/uMyitP/hGtd/6Auo/+Aj/4Uf8ACNa7/wBAXUf/AAEf/Cj2kO6D2NX+V/cZlFaf/CNa7/0BdR/8BH/wo/4RrXf+gLqP/gI/+FHtId0Hsav8r+4zKK0/+Ea13/oC6j/4CP8A4Uf8I1rv/QF1H/wEf/Cj2kO6D2NX+V/cZlFaf/CNa7/0BdR/8BH/AMKP+Ea13/oC6j/4CP8A4Ue0h3Qexq/yv7jMorT/AOEa13/oC6j/AOAj/wCFH/CNa7/0BdR/8BH/AMKPaQ7oPY1f5X9xmUVp/wDCNa7/ANAXUf8AwEf/AAo/4RrXf+gLqP8A4CP/AIUe0h3Qexq/yv7jMorT/wCEa13/AKAuo/8AgI/+FH/CNa7/ANAXUf8AwEf/AAo9pDug9jV/lf3GZRWn/wAI1rv/AEBdR/8AAR/8KP8AhGtd/wCgLqP/AICP/hR7SHdB7Gr/ACv7jMorT/4RrXf+gLqP/gI/+FH/AAjWu/8AQF1H/wABH/wo9pDug9jV/lf3GZRWn/wjWu/9AXUf/AR/8KP+Ea13/oC6j/4CP/hR7SHdB7Gr/K/uMyitP/hGtd/6Auo/+Aj/AOFH/CNa7/0BdR/8BH/wo9pDug9jV/lf3GZRWn/wjWu/9AXUf/AR/wDCj/hGtd/6Auo/+Aj/AOFHtId0Hsav8r+4zKK0/wDhGtd/6Auo/wDgI/8AhR/wjWu/9AXUf/AR/wDCj2kO6D2NX+V/cZlFaf8AwjWu/wDQF1H/AMBH/wAKP+Ea13/oC6j/AOAj/wCFHtId0Hsav8r+4zKK0/8AhGtd/wCgLqP/AICP/hR/wjWu/wDQF1H/AMBH/wAKPaQ7oPY1f5X9xmUVp/8ACNa7/wBAXUf/AAEf/Cj/AIRrXf8AoC6j/wCAj/4Ue0h3Qexq/wAr+4zKK0/+Ea13/oC6j/4CP/hR/wAI1rv/AEBdR/8AAR/8KPaQ7oPY1f5X9xmUVp/8I1rv/QF1H/wEf/Cj/hGtd/6Auo/+Aj/4Ue0h3Qexq/yv7jMorT/4RrXf+gLqP/gI/wDhR/wjWu/9AXUf/AR/8KPaQ7oPY1f5X9xmUVp/8I1rv/QF1H/wEf8Awo/4RrXf+gLqP/gI/wDhR7SHdB7Gr/K/uMyitP8A4RrXf+gLqP8A4CP/AIUf8I1rv/QF1H/wEf8Awo9pDug9jV/lf3GZRWn/AMI1rv8A0BdR/wDAR/8ACj/hGtd/6Auo/wDgI/8AhR7SHdB7Gr/K/uMyitP/AIRrXf8AoC6j/wCAj/4Uf8I1rv8A0BdR/wDAR/8ACj2kO6D2NX+V/cZlFaf/AAjWu/8AQF1H/wABH/wo/wCEa13/AKAuo/8AgI/+FHtId0Hsav8AK/uMyitP/hGtd/6Auo/+Aj/4Uf8ACNa7/wBAXUf/AAEf/Cj2kO6D2NX+V/cZlFaf/CNa7/0BdR/8BH/wo/4RrXf+gLqP/gI/+FHtId0Hsav8r+4zKK0/+Ea13/oC6j/4CP8A4Uf8I1rv/QF1H/wEf/Cj2kO6D2NX+V/cZlFaf/CNa7/0BdR/8BH/AMKP+Ea13/oC6j/4CP8A4Ue0h3Qexq/yv7jMorT/AOEa13/oC6j/AOAj/wCFH/CNa7/0BdR/8BH/AMKPaQ7oPY1f5X9xmUVp/wDCNa7/ANAXUf8AwEf/AAo/4RrXf+gLqP8A4CP/AIUe0h3Qexq/yv7jMorT/wCEa13/AKAuo/8AgI/+FH/CNa7/ANAXUf8AwEf/AAo9pDug9jV/lf3GZRWn/wAI1rv/AEBdR/8AAR/8KP8AhGtd/wCgLqP/AICP/hR7SHdB7Gr/ACv7jMorT/4RrXf+gLqP/gI/+FH/AAjWu/8AQF1H/wABH/wo9pDug9jV/lf3GZRWn/wjWu/9AXUf/AR/8KP+Ea13/oC6j/4CP/hR7SHdB7Gr/K/uMyitP/hGtd/6Auo/+Aj/AOFFHtId0Hsav8r+4+mLb/j1i/3B/KpKjtv+PWL/AHB/KpK+MP04KKKbJIsUTySHCIpZjjoBQA6iqA1vT2GRccf7jf4Uja7py4zcdf8AYb/CgDQorEk8Y6FE5SS+wwOCPJc/+y1dtta0+7iEtvcb0PQ7GGfzFAF6ioBe256Sf+OmnfaYsZ3foaAJaKiN1COr/oaYb62HWT/x00AWKKr/AG+2/wCen/jpqaORZUDocqehoAdRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBHbf8esX+4P5VJUdt/wAesX+4P5VJQAVX1A40y6J7Qv8A+gmrFVNVONHvT/07v/6CaAONNwojA4GBWbPqcfmeVnn+VUbu6Yfdb9ay9PkM/iJICQd/3s+nWgCobs3ev3XO1UwAR3Ndr4euTbeZYStuMR3J7qen5V5d4a1i1uviZdaZkIk8x2ZPBK16Rqtq9tq2n6nE7ILcmKZB0ZGx1H1AoA7KKUEA1N5lZkM4cjbwfQ1aZsR5zQBaJPeqcgIkbnvmqF/4jsdLTN1cIpwT1riNQ+KcKuzWkPnEngjhcfU0AehiQqTmt3TjnT4yff8Ama888J+Ip/EkFxJNCsaRhcFWzya9B0oY0yLnPX+ZoAt0UUUAFFFefat458X6b4qtNEj8FWc76g05s5DrgXzEiwSzDyTtJDA4yfrR1sB6DRXL3PjA6PFdT+JI9P0+Kz0xLy4jXUFeWN2YqU2lVBXIAV8jcTjFWrbxt4bn8MWviCTWrC2025A2XE91GqBj/AWzjcDkEZ6g0B/X9feb1FZj+JdDi0+3v5NZsEs7oEwXDXKCOUBSx2tnBwFJ47A1UuPFVjPo0GpaBf6VqFvLdxW/nG/VYvmcKQHAYF+eF7nAo62Dpc3qKy7zxPoOn6rFpl/rWn21/NjyrWa6RJXz0whOTntV6K8tZrqe2huYZLi32+dEkgLxbhldw6jI5GetAE1FcnaePIJ/ijfeC5rNoZbe1S4hujJlZyQCyYxwQCD1ORnpikHj2CT4rf8ACEwWZkdLFrqa88zCxuCv7rbjk7XVic8bhxzSTvbz/T/hg7+X9fqdbRWVp/ijQNW1GXT9L1vTry9hz5tvb3SSSJg4OVByMHilh8T6Dc60+j2+tafLqaZ3WSXSGZcdcoDnj6UwNSiuU8OfELRtcSOK7vLHTtRmup7eGwkvEMsvlyvGGVTgndsJxj25xWjB4jt0m1D+1brTbSG1vVtInF6rFmZVKq4IGxyW4Tk4we9H9f194G1RWdpfiLRdbtZrnRtXsb+CA4lltblJFjOM/MVJA455pNJ8R6Jr/m/2FrFhqXknEv2S5SXYffaTigDSorGPiDHjhfDv2b72nG++0eZ6SBNm3HvnOfwq3q2uaToNqtzrmp2enQM21ZLudYlJ9AWI5ovpf+t7fmHW39dy9RWddeIdGstHTVbzVrGDTpACl5LcosTZ6YcnBz25pv8Awkuhf2XBqX9s6f8AYLltkN19qTypG5OFbOCflP5H0oA06KxJ/Gnhe2tUubnxFpUUEkaypLJexqrIxIVgSeQSpAPfBp7a9F/bFrDFPp72NxZSXfn/AGwbyqlcMqYwyYbJfOBx60AbFFZNh4r8PapqAsdM13Tby7MYlFvBdo8hQgENtBzjBBz6EUkni7w5FqM1hLr+mJeW+POt2u4xJHkhRuXORyyjnuR60eQGvRXP+N/F1p4K8LXOr3jQFkwkEM04iE0h6KGP4k4B4BOKbYeMdJ/syyk1nWtEt7u6iSQJb6grxuGYqpjZtpYEjAOOTkUbhsdFRWWfE+grrY0Y63p41QnAsjdJ52cZxsznpz0ps/izw9a6gthda7psN403kLbyXaLIZMA7NpOd2GXjr8w9aNwNais/TPEGj63JcJo2q2WoPbNsnW1uElMR54baTjoevpXP654w1qz8bL4d0Dw5DqsgsFvpZZdRFtsUyMmADG2T8ueo60dbB0b7f8MdhRXJ2vjf7bpkjrZR6fqFrqEVje2Wp3Sw+Szsv3XG4PlWBTH3sgcHitibxPoNtrSaPca1p8WpyYCWT3SCZs9MITnn6UAalFZV54o0DT9Wi0u/1vTrbUJtvl2k10iSvngYQnJz29aW/wDE+g6VqMOn6nrWn2d5Pjyre4ukSR8nAwpOTk8UAalFc/b+NtDuvG114VjvYv7UtoUlaMyp8+7cSijO4soXJGOAynvVyy8TaFqWqTabp+tafdX0GfNtYLpHkTHByoORigDUorOsPEWi6peNaaZq1jeXCIXaGC4R3VQxUkgHIAYEfUYrRoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiikZgq5NAC0VyvjHx/ovgm0WfW5pQz/AHLe3j3yN+ZAH4kV5d/wv7w1faszxX3iDSt5GGlijmgXAA5QMTjjsD3pX7EVHOEbxjf7v1Pe6K5rw34sj1VYIrma3lNwm+1u7Zsw3S98Z6MO6nkV0tCdyaVWNVXj/wAFeoUUUUzUKKKKACisK61HU7fxhp9m32YafdiUBQrGQlEDbi3AHJxjB9c84FTxFq+p6TciRb2yjEkqJZ2RjLPdZIDbmyNpyeMcDgknOALUO51FFYXiDUdT06705rT7MtpNdwwS71LSNvbBA6BcDnPOfQd5dcl1G3je4t760sLOCFpJJZojIzP2GMgBf1OeMd1fS4dbGxRWH/al9Pb6ErRfZbu+KyXERGfLQR7nHPvtX8ab/aWpx+OINOuPswsbi1mljVFYyAo0YyzE453ngDjHU1Vtbf1oK+lzeornNTuNatNasre21G2l+2XPFubQ5SFeXJbf2GBnHUis+/8AFVyNYvY7W9t7eKynWDZLaSyI7YUsZZlyIgd2AT6E89KS1G9Ds6K5DWfEd9a3msGG7tLVdKiSRLaaPc13ld2QdwwCfkGB1B69Kfq/iK7S4sXtJZLWzns/tLzDTZbvBJXAIT7owSSTR/X9fcD0/r0/zOsoqGymW4sYJknS4WSNWE0YwsmR94deDU1N6MNwooopAFFFFAEdt/x6xf7g/lUlR23/AB6xf7g/lUlABVTVhnRb0etvJ/6Cat1U1XnRr3/r3k/9BNAHk9zYtw5f6VlaVcwp8Q2hJG5YdyKTjJxzx+dauqStBbEtIFVRknPSuS1XT/K1621y3kbOFYFe/GD+lAEHiPQotC8VPqNlGFkMguImx0PcfnXfSa7DqdnaSIQUuUG4DnBrlvF1/b3GlRzOx3oN649O4rjLXxDdQxpDCSkKSEoSOuaAPZ4PEFvaw7ryZIvJBVyxxkiuc8S/ENWUxWp8tQOWLcH3/wDrV5tfamA7zXDGSQjOC3euavru4u7gyOSFA4FAG3r+vzalcEiR5P8AabgfgO1VtMgudQnjtk3O8jBQvqTVCzjebCmu+8Drb6fr0EkyEggqj44Vz0oA9a0KxtvC/h6KwjAabAMhA+83eu10STzdHgfGM7v/AEI1xrsXPzDk12Gg8aHb5/2v/QjQBo0UUUAFch4gsrqb4peD7uG2mktreG/E0yRkpEWSMLuboM4OM9cV19FHW4HAazpN/d+PPErwWczR3PhhbaGXYQjy75vkDdM/MOM9xWY+qagNF8INHpWoWFvBaPBcXp0OS4vLOZERAiRlGKK3z5faVO0etepUUkrf15t/qEtXf+tkv0PGPD3h7U8+F01DS7+RLfxTqFzIbu1AZEKTFJXCqEUFiCCAFyRir2t6Ffm58QC00u5KTeK9MuoxFbth0UW/mSDA5AKtlugwc9K9Zoqk7NPtb8Lf5A9b+d/xv/meZRn+xbvxPpet+GL/AFq41bU2uLcxWTSw3kTBQitLgpHsxg7yMbcjrXcadJaNr2pxw6RLaXMYh8+9a2CJd5X5dsnWTaODnp0rVopLRJdtPuB7t/13PKtf0XVU8XeJvEOnadcTXemS2F7YhYm/0sJE6zRIcfMSjMuBnkrUWj6BrNj4o0bV59Mkn1C90rUry+3xny1uZmgZIHbGBhVCAHsh9K9aopW0/rqrf5fcH9fc7/5/eeM6Pb6xqPifwTOsGqQrZG4ju7caJ9itNOZrVxsTKBiu7AB3MnTnJFbPg1xYaNoPhjU/Ct/Nq+n3BNxcSWbCCKQFi10LgjY27ORtJY7sEDmvTaKq+tw6Hj48O3qfCG4gTR7gag3iM3QjFs3mkf2jkSYxnHl87v7vPSjV9A1C7n1qN9JupoZ/GthcBTbMyyQKsAeTpygw2W6DBr2CiiOjT9PwcX/7aD1T+f43/wAzy3xh4a1XUtW8bRaLaSxtf6NZLG6x7VuXSWYvGGPylimF57MM8Vc8JWx1Dxpb6mZfEMr2dg9uXv8ASUsYkVmU+UfkRnIK5G0MoweeefRqKFo7/wBdf8wev9en+RwOu6g+hfFmDVLjTNWurN9Fa382w06a6Ak84NtPlqccDPNZ/iV7288VaL4ptv7astMFhNbh4NIM1xbStIpy1u8bOoYLjcF7DPBr06ipSsl5X/G/+YdW+9vwt/keSxW17oWg6IbSx1RLKbULu6mvbnRluLywL5KmO3jU+WHJbnblQcEDPGfZ+HdTuNPsYr7TNRuYz45F4/2yzAZoDGT5zqqhQuT1wAD15r2qiqWkub+tGn+gnqrev4pr9TjItIaT4y39/Pp5a2/sKGCO4eHKbjNIWQMRjONuR6YrjdC0HVLddFR9LvIlg8NanbkG3cCN2nTy4+nBKjgdwOK9loqHG8eX1/G/+ZV/e5vT8Lf5HmGlaBc2ug/CwR6VLDNYFBdgW5VrcNZSB9/Hy5fAOcfNjvXP6rBcaT8MP+Ef1TQLoX9prEUs180Q8mQteqROsnRiwYDA+YZIIAFe31zf/Cv/AA5/aIvDZzHFz9qFsbyY2wmzu8wQb/L3Z5zt689a1lLmnzPvf8bkpWVv66/5jPiNaXN94A1G3sreW4nfytsUKF2bEqE4A56DNU/7Kkk+OD6nLYu1vH4fSGK7aElFk+0MSofGN2MHGc4rs6KhaO/9bWDo13t+DT/Q8ck069/4Qqfwb/Yl+fED6s1wt79kcwEm680Xf2jGwYTnG7dkbcVJ4g8O3tz4Q+KKJo9xNc398GtQLVme5UQw4KcZYBg2MZ5zXr9FC0Xyt+X+RV9b+d/z/wAzkrbTZIPir9ris3jtm0JYWmWIhC6zZCZxjIBPHpWTrGoyaB8Yn1S50rV7qzm0FLdZbDTZrlfME7ttJRSAcc8+teh0ULS3lf8AG/8AmSlo13t+Fv8AI8j1XR9a1W3vtel0i7tn1PXtLeCyKb5o7eCVB5kirnaT8zEfwgDPepNbhvdP8T6hH4f0/U7ma91SGebS9Q0gT2VycoDOlyBiLCruG5yQy/dGRXrFFOPu2t/W3+Q3r/Xr/meK+P49c1LTvGGm2+n6jbXMsm62tdL0bct+iqhWWS5KEM3BGFZXG0AAmtPxjDd2Gt6xNodjqs2oahDCH0+40f7bYaoyrtUGQD91j7p3OoGM7T1Pq9FJaJLsNu5wM8Gox/EXV1WzurdtX0KC3tbuKFnhimjM5YNIowmN64zjPauc8I6PctN4U02//wCEijutEYM8B0mOG3tWWMq2bjYokRskfIzFsgkdSPYAwLFQRkdRnpS007O/9aXt+ZLWlv62S/Q4z4V6XLpXgdY7uxezuZL68kkWWExu2bmQqxBAJyu3B9MV2dFFIfVsKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACqWq3gsNPubtgGFtA82098DIFXaz9a09dV0i80+Rti3ltJblv7u5SM/rSd7aDVr6nz54umfxfNZWruWEkzNPIBu3L14/Ks7TrbwpfXFxpd5p/lgKVSYxrxjvkHIrQ8VaNd/DfTrSVrhWl84blUfJnHJ9an0/wAYW13ai5GiwzIyOlw5wuR0JUjJP0xXkNTPap8j2L/wu0aaLWL/AEi01NZNL3LcWskYyYbhDncPquQfw9K99U5UH1FeffD/AEOO3hXU7OBbeyljJgjIIYlu/PbGa9BAwoHoK9KjdwvI8etTpxrSlBb/AKC0UUVsQFFFFAGfeaX9r1rTr/ztn2Lzf3e3O/eu3rnjH41Q1jQNQ1Zbu0OrhNNvBtmge2DSIMYIjfcAufdWwScVpG/lF6Lb7G+4qXB3rjaDjPX3qdLy2kuGgS4iaZfvRhwWH4UeYFXVdK/tKGzjE3lC2uorjJXdu2HO3r39apa7omparqNpPa6lbQ29t84tbizaZXkzw5xImcdgeAeeuMa0V7azyeXBcwyPjdtSQE49cfjSXN0YZI4o4jLLLnaoIAAHUkmjYClb6fdnXI72/eGUwWfko8alAzs2ZDtJO0fKmOT3qSbSvO8SWmq+dj7NbSweVs+9vZDnOeMbOmO9Pk1REtEnETHdu3KSAV253fjwasw3CzySCMEqhwW7E9x+FPb+v67gVYdM2a5c6nNN5skkawwrtwIUHJA55JbknjoB2rK1DwrNdyajDbagtvp+qvvvYDb73Y7QrbH3DbuVQDlW9RiukopbAYGr+HLnU7ptmoRxWssXlPG9qHkjXGG8qTIKZHByG9sVbvtO1Fmi/sfUo7JFi8popbbzlx2ZfmUhh7kj2rUooAq6Xp8Wk6Ta6fblmitoljUseSAMZNWqKKNwCiiigAooooAjtv8Aj1i/3B/KpKjtv+PWL/cH8qkoAKq6opfR7xF6tA4Gf901aqtqTKmlXbv91YHJx6bTQB4Xr919ouzpLxPuljyjjkE56VBA6ReFHF0Cot5fLTcOT7D37Vtano7XzbrWZPtETiSL5huA7r/L86z9asp76wZZ3tkZHMmEkALsT6evNAHC6hcG4uFMv/HtGP3cZ9fesueVmy0abSOmR1Fdc3hecwm4lZVjUEgbs5NYU+hXI3tMCiqcAY6/SgDBjDSsC4yT2PrVk2TP0G8njitI6Z5Kk7NpGMZ6nn0rQjgV4mCuIzkZ2nrQBzrwSWc6hoypyMrjmun01pXjhRSyRAmQgnGDx29av2Ph83wXaYowDy8r4ro7LwfaROpmvVn55RGCigDrNJjddJg892lcoCWbrXcaJ/yB4P8AgX/oRri7aaLzAhb92owFQ8mu00TH9jwbSSPm5P8AvGgC/RRRQAVydqdPi8RARPZ3V3LdSZdcx3kOd2Q46ug6DOBjbwcA11lFHUOhxkWpS2mjW1udZmFw0k+6aR4F2srY8tiyHnkHaFLfQCr9nc3+q2l7I15LFi0j2RwogwzwhieVJzk8f1rpKKlq6sC0aZzczGbwFCyXjXLMkP78lWJO9fQAcHj8OahvNSvrEzW0uoOIo7sRtey+UjIpiDgEldg+Y4yV9upzXVUVT1k5AtFY5u1vtRuJrIC73j7JNcEQhCLgq6hMtt6EHnbjnpxVCLULjUtPuFk1CG5TZE8geWIGF/MX5flA2jrwxLZFdnRRvp/XX+vkK39fd/XzMDxHa/a7/S4vsNpffvJD5N22E+4efutz+FQRLqOnMbWKbm1smuFtoVBRmLPtjyRnaBhRjHQdOldNRSt/XyGclba1dNYXcqamlzEixb5w0Ttb7nw5OxQAAvzYbJGDnimRXZ+1PJa3S6mP7QURzMiHzP8ARieCAFznjKgenrXYUU3qNaHIXF611oUrweIpJZFMEk5iWMfZv3g3AkLhRjOVbJGOT1rZ1u6kttLhkgv47cM6gzu6KWGD0ZlKAnjqMHpxkVrUUMSOVm8RtDZSSteGPzNM82185UDySjfkgDhj904GR36GrYuNRkXV7iK6kY2k+2K3CJtKqqOR0zlssM57/jW/RQHb+uhzH2y81CS2uLSVAbpLh7NmiU7FAUIQSM4bG4/73sKJNdkn0v7d9pks4Lq4WKBsRr5YCncWZwQMsGHIPQYHNdPR160ragc34fu5b3UlnuHEkhsypcfxbZnUHgAcgdgPoKRdVuDqoQahuuDcyRSadtT91EA22TpvHAU5Jwd2Mciulootpb1/F3D+vwsc/oV5eyT2QvLt7n7Zp4uWDoiiNgUyF2qODv756Vn3izG81eF9QmLnVLQxRMUPlIfJ+ZRjOM7hzkZX1znsKKq+t/63uCurnKXOq3trDJBNfskcV60D3rmJGC+WrqCSuwElsZ29sdTmtqxQapoMI1RILtZVy2VDpIM/K2MYORg8DGelaNFLoI5PS7S1sdC0c2VrbwNPcoJGSBAW4fB6dffrUEWpS2mjW1udZmFw0k+6aR4F2srY8tiyHnkHaFLfQCuzoqVFpWv/AFp/kV1ucodV1GaGS7F20QgtLW48lI02yM+dwbIJxx2IIo/tEws0c+pJpNuZ7o+eqRrvdZcBfmUgnBJ6bj68Guroq3vclJpWZk6xqBtNJgc3LwNM6J56qibSRnJ8zIQHGOQTzjGayINS1G+ghVdRaIra3MhlhSNvMMcoRTkpjBHXAGe2K62il/X4DOPvdVe6XFxqAt5TJbGKxCr++RjGxfBG88lhkHA289DW1rN21vPaxyXx062k3b7kBPvDG1MuCozk9v4cCtaoLm0S6VQ8k0ZU5BilZPbnB5/Gh+QLY5OfXdQ+y2xe/W2d7USRvJ5cYuJdzAqVdSSBhflXDfN9K2tKMg8Qask97I7Fo3S2crhFKLyoABxnI9OPWtS2torS2jt7ddscahVGScD6nk/WpaFoBx5lOmm/c6jNC91qLJ5zeQgTamQC7LgDAxyGPHHc0SavqcmlvcpfNG9vp6XOEjQrI+5wd2VzghR0x7YrsOtFC0t/XQHrf+upzFzLcRyfZ7i4N35WoeWsk0Sbgpty3ZQAck8gA44qjY6ibTR0Xz/su9LWMTosSbcwBvmdwRjjGSCecAV2tHXrS6fd+APVp+TOc0S/vdRurcTXjeWkDOwjVCJiJXQEnb0woPG3n0HFLrWpahZ6kba1f78YuU+QH5I8mVenf5B6/Px0roqKYepxup3N5e6VLLFKyzXGlXk8DxRqJFXchjAOMj5SM+/uBh7zTRXd9fWWqSSrDaWr71EbLcDc/wB47emP7uK6+infSwnrYwvEsEk0mmNb58+C5aaIA43MsTnb9CMj8a5++uRqE2pasHYW9zpFwsAbIAiRk+bB6ZJY/TFd7RSWjuN6q39b3K1lqFnqEZawu4LpU4YwyBwp9Dg1zGl6sbWzi8rUWuyltI89oiITbBVyCAAG5OB8x5zxiuwopPcEcfFrN21pdKurxsiNCVuXmhyA27K71j8sH5RjIPXGeRh51eRxHcxzmNHtIt93LFHvRTKVZyQNuMc55X+LGK63HGO1FPqn/W1g6W/rc5W/1ZoNOglg14Swb5M3AMKSTYA4Usvlvjnptz68HPTwv5sEb8jcoPzDB5HcdqfiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKRlDLg0tFAGB4m8I6R4ssRaa9ZfaY1OUdWKuv4iud0j4PeFtHbFvHqE8W7d5E9x+7J9wAPyr0Gilyp9ClKSVkyGC3SGNERFREGERBhVHsKmoopkhRRRQAUUUUAVTDIdXWbb+7EDIWz33A/0qnBbXAW0t3t2X7K+5pyy4fgjIwc5Oecgd61qKOlgMLSUlmsNKVbUxJAokMu5cMNhGBg5yc85A71d1O3eaaBzA9zDHuLRxMFbccYbJI4xu796vqoVQqgADoAOlLTeo76mbBazJpMduYVDbyEDkN5K5OD7kD68/nTrGG5guGiYMLZQwXJXB54xjnpnOe9aFFIQUUUUAFFFFABRRRQAUUUUAFFFFAEdt/wAesX+4P5VJUdt/x6xf7g/lUlABWZ4mCHwlq4lbZGbGbc3oPLOTWnWV4qi87wdrMQkMW+wnXeBkrmNucUAfN+mz6jp8E9sht1DEsLot09Mnr0zUGj67cFpIr+6VYvMA4UklAOo9M4/WkvLXEdpGblvIkG0mQhWYr7njPP1qhZWM1xdXEF7byRttb5lG0/XntQBuXHiKxvtTtHk0+RbNQ0ThXDfMeATU+o6RZ3Vik+mlmiJI3hSdpHbpWDpz2tgrLFMrXDqVKyRk4PYYxgGur8Iz2en6Xcrqt7IjyKGjiRiM+pPvQByKaZMTMrJucKQkj/KB9adCH+yHe6LHGuCiPznPJrrbi10bxBEsa31xGzAll5IX6npms+DwRZRN+5laUA43HpQAxHmuvLkWVoxvOVC5AH1H+FWXeWyiWRZPP5OcIQR+JrVtNIvrCHEMkYTHAxmo7m51GNcJbrMx6fL3oAoQalqZmj8trhm3jIEYUBfY5r3XwS0z+D7JrkkyMZCc4zjzGx+mK8MvrrXRaKlrEUkLA5UK2K9u8ANcN4G09r0ETnzN4Ix/y0agDo6KKKACsVryb7Ob57uSJfOZBCIdyBVYr8xClgTjrkDmtqqjaZatOZSjZLbyokYIW9SucE/hS6mc1JrQrprBkvfJjgUoXaMNvOcjPJG3AGRjOfwqC31G/lWyMkUIE0bs5WQ54xyPl6+1aK2Ful156q4fJbHmNtyep25xn8KRdOtlWMKjARsSn7xuM9R16e3SlZ2FyzvuUI9ZkFuH8jekcEUsrvLhsPnoAuCeParI1KUzJi3XyJJjCsnm/NkZBO3HTIPephploIXiEXyPGsbDceVXoOvvVf8AspjqQuC6BFl80Kobk4x0LbQfUgZNPW5KVRLcXUXukmUo88VuEJLwIrkN/tAgkjHoKji1KYXF0X8h7eG3jlWQPt3FgeeRgA49ePfNXLnT4LqQPKJA23aTHKyZHodpGRSNp1q+7MWA0YiIViAVHQYB7dj1FGtinGfNdMpx6zLKyxpaoZWl8sfvWC/c3ZyVB7elJ/a1xD9q8+KI7LnyYsSEfwg8/Lx9efpV2LTbaGRZFRi6tv3vIzHONuck+hxSyadbSGUsjAysHYrIw+YdCMHg+4o1/r5C5alt9f8Ah/Iqx6rLOFSC2UylXZg0hVQFIGQdvOc8cCrenyvPpttLKcu8Ssxx3IpjaXatGilZPkzhhM4Y56gtnJB9DViGJIIUiiXbHGoVR6AUIqKnfVixyCRdyhgMkfMpB4OO9Z8ktw8NzdJctGIGcLFtXaQv94kZ5x2I7VpVWksLeWYyOjZYgsokYKx91BwfxFM0Iv7RfzyPIURLKsTOX+bLAEfLj/aA601dTYKsssAWB42kRlkydoGeRgYJHuatG1hO7KfekEh5PLDGD+gqtDpuy68xyhQBgEAbHzexJAHsOtHQOokt9MilZ4vKf5GXy5N2QXA6leDz6fjQuqMAZJoAkJEm1g+WOwnORj2Pep00+2RWUKzBtuS8jMeDkDJPAz2p32OEKoVB8pYrnJALZz9etAdRlvczPcGG4gWNtgcFJNwxnGOg5q1VKysDazNIzqxKBABuOAD6sSce3QVdoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigCO2/49Yv9wfyqSo7b/j1i/3B/KpKACsrxUM+DdZAGf8AQJ+M4/5Zt3rVpk0MVzbyQXEaSwyqUkjkUMrqRggg9QR2oA+TItNvY42YTSbJHBCp8xU85yenpWvHFLBbNDPdxxMzhl/idznHQdK+i4/DGgQqFi0PTUUdAtpGB/Kkbwt4fc5fQtNY+9nGf6UAfONxdWTK6iES3cbcXDdCR0yPQVSvNdvDptxaywRzrNgkFeUOOcV9MDwj4bC4Hh/SgPT7FH/8TSHwd4ZPXw7pJ/7cYv8A4mgD508KeKj4c0WSO8sQ0Jk3lt2G/wDr1b07xq89xd3oj3wzyfJbytwg9sCvoE+D/DTKVbw7pJB6g2MfP/jtIPB3hlcbfDmkjHTFjFx/47QB5HBrGl3ce+PesgOCAMAH0z3rE1Wyl1S7Z7fUnVjxsX5QB9K9+j8N6HCMQ6Np8Y9FtUH9KX/hH9G3bv7IsN3r9mT/AAoA8L0/TL+1to4/7T+VemOTXs3ghGTwbYq8hlYeZlyOT+8atE6JpJxnTLM46f6OnH6VaggitoVitokhjXOEjUKBznoKAJKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAI7b/j1i/3B/KpKjtv+PWL/cH8qkoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAI7b/j1i/wBwfyqSo7b/AI9Yv9wfyqSgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//2Q=="}}},{"metadata":{},"cell_type":"markdown","source":"## Whole Process in detail:\n\n* Importing both train and test data and combing the data into a single dataframe so that when can make change to both the dataframes at once and later we can split them \n\n* visualizing and selecting the columns which need processing \n\n* visualizing to see how the data is distributed and deciding which machine learning algorithms to use\n\n* Converting some columns to categorical and other feature engineering stuff\n\n* scaling data\n\n* Applying different machine learning models without hyperparameters\n\n* Applying different machine learning models with hyperparameters\n\n* Using soft voting classifier,hard voting classifer and predicting the output with all the combinations of different models\n\n* Choosing the best predictions out of all combinations used with soft and hard voting classifier"},{"metadata":{},"cell_type":"markdown","source":"### If you like the kernel, please UpVote It! which motivates me to upload more notebooks\n\n## Let's start playing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n\n\ntraining = pd.read_csv('../input/titanic/train.csv')\ntest = pd.read_csv('../input/titanic/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we are including train_test column so that we can divide the training and test set from the whole_df later\n\ntraining['train_test'] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\nwhole_df = pd.concat([training,test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at the datatypes and null values\ntraining.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# understand better about the numeric data\n\ntraining.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print which columns are numeric \n\ntraining.select_dtypes(include=['int','float']).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize Insights of the Categorical Data and Overall Distribution of numerical Data"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# lets use pairplot and see how the data is distributed and the blue and orange colour represents the survived data\n\ndf_plot = training[training.describe().columns]\nsns.pairplot(df_plot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As you can see above the data is spreaded in such a way that we cannot use linear models , so we will eliminate using linear regression and try random forest , logistic regression , knn and xgboost"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.barplot(training['Cabin'].value_counts().index,training['Cabin'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.barplot(training['Ticket'].value_counts().index,training['Ticket'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" As you can see above the columns ticket and cabin needs to be processed properly "},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create all categorical variables to whole_df \n# later we can divide the training and testing set with the help of train_test columns\n\nwhole_df['cabin_multiple'] = whole_df.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nwhole_df['cabin_adv'] = whole_df.Cabin.apply(lambda x: str(x)[0])\nwhole_df['numeric_ticket'] = whole_df.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nwhole_df['ticket_letters'] = whole_df.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\nwhole_df['name_title'] = whole_df.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling null values with median\nwhole_df.Age = whole_df.Age.fillna(training.Age.median())\nwhole_df.Fare = whole_df.Fare.fillna(training.Fare.median())\n\n# drop null. 2 instances of this in training and 0 in test \nwhole_df.dropna(subset=['Embarked'],inplace = True)\n\n# The reason we are applying log to the columns below because, after we applied log the dustribution was changed near to normal distribution\nwhole_df['norm_sibsp'] = np.log(whole_df.SibSp+1)\n\n# log norm of fare (used)\nwhole_df['norm_fare'] = np.log(whole_df.Fare+1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converted fare to category for pd.get_dummies()\n\nwhole_df.Pclass = whole_df.Pclass.astype(str)\n\n#created dummy variables from categories (also can use OneHotEncoder)\nwhole_dummies = pd.get_dummies(whole_df[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','cabin_adv','cabin_multiple','numeric_ticket','name_title','train_test']])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split to train test again\n\nX_train = whole_dummies[whole_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = whole_dummies[whole_dummies.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = whole_df[whole_df.train_test==1].Survived","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale data \n\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\n\nwhole_dummies_scaled = whole_dummies.copy()\nwhole_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(whole_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\nwhole_dummies_scaled\n\nX_train_scale = whole_dummies_scaled[whole_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scale = whole_dummies_scaled[whole_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = whole_df[whole_df.train_test==1].Survived","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_scale.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_scale.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The below are the algorithms we are going to apply : \n\n* Native Bayes theorm\n\n* Logistic Regression \n\n* Decision Tree\n\n* Knearest Neighbors\n\n* Random Forest \n\n* Support Vector Classifier\n\n* XGBoost "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = GaussianNB()\nlr = LogisticRegression(max_iter = 2000)\ndt = tree.DecisionTreeClassifier(random_state = 1)\nknn = KNeighborsClassifier()\nrf = RandomForestClassifier(random_state = 1)\nsvc = SVC(probability = True)\nxgb = XGBClassifier(random_state =1)\n\nmodel_list = [gnb,lr,dt,knn,rf,svc,xgb]\nfor model in model_list:\n    print('\\n')\n    print(model)\n    cv = cross_val_score(model,X_train,y_train,cv=10)\n    print(cv)\n    print(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> #### The above results are without hyperparameters and in them logistic regression performs the best "},{"metadata":{},"cell_type":"markdown","source":"\n## Read below to understand the hyperparameters of gridsearch and randomized search "},{"metadata":{},"cell_type":"markdown","source":"### **Grid Search hyperparameters**\n\n* verbose = watch the performance of the already-tried combinations of parameters during the execution ( the more number u give the more details u get ) \n\n* n_jobs = no of jobs to run in parallel (-1 means the execution uses all the parallel processors for speeding up the excecution)\n \n* cv = cross validation ( 3 means the data is folded into 3 partitions and in each fold a test and train set it taken, at last it finds the average accuracy of all the fold ) \n\n### Randomized search hyperparameters\n\n* n_iters =  no of iterations for trying the different random combinations of hyperparameters\n \n* verbose = watch the performance of the already-tried combinations of parameters during the execution ( the more number u give the more details u get ) \n\n* n_jobs = no of jobs to run in parallel (-1 means the execution uses all the parallel processor for speeding up the excecution)\n\n* cv = cross validation ( 3 means the data is folded into 3 partitions and in each fold a test and train set it taken, at last it finds the average accuracy of all the fold )"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV \n\n# create a function that automatically returns best score and best hyperparameters\ndef performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train_scale,y_train)\nperformance(best_clf_lr,'Logistic Regression')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNeighborsClassifier hyperparameters \n\n#### The main parameters of the class sklearn.neighbors.KNeighborsClassifier are:\n\n* weights = uniform (all weights are equal), distance (the weight is inversely proportional to the distance from the test sample), or any other user-defined function\n\n* algorithm (optional) = brute, ball_tree, KD_tree, or auto. In the first case, the nearest neighbors for each test case are computed by a grid search over the training set. In the second and third cases, the distances between the examples are stored in a tree to accelerate finding nearest neighbors. If you set this parameter to auto, the right way to find the neighbors will be automatically chosen based on the training set.\n\n* leaf_size (optional) = threshold for switching to grid search if the algorithm for finding neighbors is BallTree or KDTree;\n* metric: minkowski, manhattan, euclidean, chebyshev, or other.\n\n* p = Power parameter for the Minkowski metric. When p = 1, this is\n* equivalent to using manhattan_distance (l1), and euclidean_distance\n* (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n* n_neighbors = no of neighbors to take (default = 5)"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train_scale,y_train)\nperformance(best_clf_knn,'KNN')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVC hyperparameters\n\n#### kernel in svc is used to tranform the data into higher dimensions and apply svc on the transformed data"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train_scale,y_train)\nperformance(best_clf_svc,'SVC')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest classifier hyperparameters\n\n* n_esitmators = no of trees created while bagging\n\n* max_features = max number of features considered for splitting the node\n\n* max_depth = max number of levels in the each decision trees\n\n* min_samples_split = minimum no of data points allowed in a leaf node \n\n* boot strap = method for sampling data ( with or without replacement)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [400,450,500,550],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n\nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train_scale,y_train)\nperformance(best_clf_rf,'Random Forest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGB classifier hyperparameters \n\n* n_estimators = no of trees created in XGB\n\n* colsample_bytree =  percentage of columns you want to select from a tree for helping overfitting and speeding up the process\n\n* max_depth = depth of each tree \n\n* alpha = learning rate ( used when getting the predicted values )\n\n* lambda = regularization parameter \n\n* gamma = it is a user defined penality (it encourages pruning the trees)\n\n* min_child_weight = For regression, that is the minimum number of observations that go to a leaf. For classification, it is the minimum of the hessian"},{"metadata":{"trusted":true},"cell_type":"code","source":"# doing grid search on all these parameters takes lots of time so i did random search\n# based on the result we  will do gridsearch on limited parameters \n\nxgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [20, 50, 100, 250, 500,1000],\n    'colsample_bytree': [0.2, 0.5, 0.7, 0.8, 1],\n    'max_depth': [2, 5, 10, 15, 20, 25, None],\n    'reg_alpha': [0, 0.5, 1],\n    'reg_lambda': [1, 1.5, 2],\n    'subsample': [0.5,0.6,0.7, 0.8, 0.9],\n    'learning_rate':[.01,0.1,0.2,0.3,0.5, 0.7, 0.9],\n    'gamma':[0,.01,.1,1,10,100],\n    'min_child_weight':[0,.01,0.1,1,10,100],\n    'sampling_method': ['uniform', 'gradient_based']\n}\n\n\nclf_xgb_rnd = RandomizedSearchCV(xgb, param_distributions = param_grid, n_iter = 1000, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb_rnd = clf_xgb_rnd.fit(X_train_scale,y_train)\nperformance(best_clf_xgb_rnd,'XGB')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [500,550,600,650],\n    'colsample_bytree': [0.75,0.8,0.85],\n    'max_depth': [None],\n    'reg_alpha': [1],\n    'reg_lambda': [2, 5, 10],\n    'subsample': [0.55, 0.6, .65,0.9],\n    'learning_rate':[0.5],\n    'gamma':[.5,1,2],\n    'min_child_weight':[0.01],\n    'sampling_method': ['uniform']\n}\n\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train_scale,y_train)\nperformance(best_clf_xgb,'XGB')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the best parameters we got from grid search is \nbest_clf_xgb.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets use soft voting classifier and hard voting classifier "},{"metadata":{"trusted":true},"cell_type":"code","source":"best_lr = best_clf_lr.best_estimator_\nbest_knn = best_clf_knn.best_estimator_\nbest_svc = best_clf_svc.best_estimator_\nbest_rf = best_clf_rf.best_estimator_\nbest_xgb = best_clf_xgb.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets try the all the combinations by using soft voting classsifier and hard voting classifier "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# here we are trying all the combinations of models using both hard and soft classifier and see which one is performing better \n\nfrom itertools import combinations\nfrom sklearn.ensemble import VotingClassifier\ncomb_list = []\nhard = []\nsoft = []\nlist_models = [('lr',best_lr),('knn',best_knn),('svc',best_svc),('rf',best_rf),('xgb',best_xgb)]\nfor r in range(3,6):\n    soft_comb = combinations(list_models,r)\n    for i in soft_comb:\n        i = list(i)\n        models_list1 = []\n        for k in i:\n            models_list1.append(k[0])\n        print('\\n')\n        print('Voting classifier for {}'.format(models_list1))\n        comb_list.append(models_list1)\n        vc = VotingClassifier(estimators = i, voting = 'hard') \n#       print('voting_hard :',cross_val_score(vc,X_train,y_train,cv=5))\n        print('voting_hard mean :',cross_val_score(vc,X_train,y_train,cv=5).mean())\n        hard.append(cross_val_score(vc,X_train,y_train,cv=5).mean())\n        vc1 = VotingClassifier(estimators = i, voting = 'soft')\n#       print('voting_soft :',cross_val_score(vc1,X_train,y_train,cv=5))\n        print('voting_soft mean :',cross_val_score(vc1,X_train,y_train,cv=5).mean())\n        soft.append(cross_val_score(vc1,X_train,y_train,cv=5).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert the above results to a dataframe so that we can analyze better"},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_list1 = comb_list\nhard1 = hard\nsoft1 = soft ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_list1 = [str(x) for x in comb_list1]\ncomb_list1 = pd.Series(comb_list1)\ndf = pd.DataFrame([hard,soft],columns=comb_list1)\ndf = df.T\ndf = df.rename(columns = {0:'hard',1:'soft'})\ndf = df.sort_values('soft',ascending = False)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Even though we are getting 80% + accuracy ,there will minor overfitting with these models\n### we will write a code which saves all predictions got by the above model\n### later we will randomly select and submit the predictions to kaggle "},{"metadata":{"trusted":true},"cell_type":"code","source":"list_models = [('lr',best_lr),('knn',best_knn),('svc',best_svc),('rf',best_rf),('xgb',best_xgb)]\nfor r in range(3,6):\n    soft_comb = combinations(list_models,r)\n    for i in soft_comb:\n        i = list(i)\n        models_list1 = []\n        for k in i:\n            models_list1.append(k[0])\n            \n        # hard voting classifier\n        \n        vc = VotingClassifier(estimators = i, voting = 'hard') \n        vc.fit(X_train_scale, y_train)\n        predictions_hard = vc.predict(X_test_scale).astype(int)\n        final_data_hard = {'PassengerId': test.PassengerId, 'Survived': predictions_hard}\n        submission_hard = pd.DataFrame(data=final_data_hard)\n        filename = ''\n        for k in range(len(i)):\n            filename+=(i[k][0]+'_')\n            \n        submission_hard.to_csv('hard_{}.csv'.format(filename),index=False)\n        \n        # soft voting classifier\n        \n        vc = VotingClassifier(estimators = i, voting = 'soft') \n        vc.fit(X_train_scale, y_train)\n        predictions_soft = vc.predict(X_test_scale).astype(int)\n        final_data_soft = {'PassengerId': test.PassengerId, 'Survived': predictions_soft}\n        submission_soft = pd.DataFrame(data=final_data_soft)\n        filename = ''\n        for k in range(len(i)):\n            filename+=(i[k][0]+'_')\n            \n        submission_soft.to_csv('soft_{}.csv'.format(filename),index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now you will have all the submissions No need to try all the submissions because there are many, just choose randomly by analyzing the below table\n   \n### I got 0.79186 with soft voting with the combination of knn,rf,svc. The file name of submission is (soft_knn_svc_rf_.csv)\n\n### This is my first notebook in kaggle , so if there are any possible improvments please comment below\n\n### Will include more visualizations in future "},{"metadata":{},"cell_type":"markdown","source":"## If you have any doubts please do comment and if you like please upvote :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}