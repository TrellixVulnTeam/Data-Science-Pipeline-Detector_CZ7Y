{"cells":[{"metadata":{},"cell_type":"markdown","source":"## About the problem"},{"metadata":{},"cell_type":"markdown","source":"The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy."},{"metadata":{},"cell_type":"markdown","source":"**Key Take Aways**\n\n1) Exploratory Data Analysis\n\n2) Feature Engineering\n\n3) Advanced Machine Learning Techinques\n\nThe important of all is that you will get familiar with how the data science competition works. I hope this kernal will help people to prepare themselves for the data science competitions and people can use this kernal to brush up their skills."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport missingno as msno  #visualize the missing values from the data\nimport seaborn as sns #visualization\nimport matplotlib.pyplot as plt #visualization","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Loading the train and test datasets\ntrain_df=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df=pd.read_csv(\"/kaggle/input/titanic/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finding the missing nature of the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"isna_train = train_df.isnull().sum().sort_values(ascending=False)\nisna_test = test_df.isnull().sum().sort_values(ascending=False)\nplt.subplot(2,1,1)\nplt_1=isna_train.plot(kind='bar')\nplt.ylabel('Train Data')\nplt.subplot(2,1,2)\nisna_test.plot(kind='bar')\nplt.ylabel('Test Data')\nplt.xlabel('Number of features which are NaNs')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:**\n\n1)From this figure it is clear that missing nature of the data is same in both train and test data \n\n2)We have to figure out a common way to fill the missing data in both train and test. \n\n3)For that let's go for exploratory Data Analysis."},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"**Understanding the Survival Nature of Titanic**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.pie(train_df.Survived.groupby(train_df.Sex).sum(), explode=(0,0.1), labels=[0,1], colors=['green', 'red'],\nautopct='%1.1f%%', shadow=True, startangle=140)\n\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of Survival Age-wise\ng = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Survival of people on gender\ngrid = sns.FacetGrid(train_df, col='Survived', row='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Survival of people on gender\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"**Imputing the missing values:**\n\nAs name is nominal column and it has title's for each name (eg.**Mr.** Donald) and it is one of the best key to impute age."},{"metadata":{"trusted":true},"cell_type":"code","source":"Title_train=[]\nfor name in train_df.Name:\n    Title_train.append(name.split('.')[0].split(',')[1])\n    \nTitle_test=[]\nfor name in test_df.Name:\n    Title_test.append(name.split('.')[0].split(',')[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using the titles obtained we'll fill the age**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Title']=Title_train\ntest_df['Title']=Title_test\n\nTitle=list(set(Title_train))\nfor title in Title:\n    train_df.loc[train_df[\"Title\"]==title,'Age']=train_df.loc[train_df[\"Title\"]==title,'Age'].fillna(train_df.loc[train_df[\"Title\"]==title,'Age'].median())\nTitle=list(set(Title_test))\nfor title in Title:\n    test_df.loc[test_df[\"Title\"]==title,'Age']=test_df.loc[test_df[\"Title\"]==title,'Age'].fillna(test_df.loc[test_df[\"Title\"]==title,'Age'].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Explanation:**\n\n1) Subgrouping age based on title.\n\n2) Filling the missing values based on median so that distribution of data remains the same after filling Na's"},{"metadata":{},"cell_type":"markdown","source":"**Imputing Fare**\n\nThere is a single value missing from fare in test data let's impute that value with mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Fare']=test_df.groupby('Title')['Fare'].transform(lambda x: x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Imputing the Embarked**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab([train_df.Embarked,train_df.Pclass],[train_df.Sex,train_df.Survived],margins=True).style.background_gradient(cmap='winter_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since age and sex seem's to be a good parameter we'll try to localize the Embarked based on those values and impute them"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Embarked in trian has two missing values we'll drop those rows\ntrain_df['Embarked']=train_df.groupby(['Age','Sex'])['Embarked'].transform(lambda x: x.fillna(x.mode()))\ntest_df['Embarked']=train_df.groupby(['Age','Sex'])['Embarked'].transform(lambda x: x.fillna(x.mode()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating a new feature based on Age**\n\n1) Since age is from 0-100 we can form a new column based on age\n\n        Age Group    Class\n        0-18      -  Minor 0\n        18-40     -  Adult 1\n        40-60     -  Middle age 2\n        60-100    -  Old 3\n        "},{"metadata":{"trusted":true},"cell_type":"code","source":"def AgeGroup(Age):\n    Age_group=[]\n    for age in Age:\n        if (age)  < 18:\n            Age_group.append(0)\n        elif (age) >= 18 and (age) < 40:\n            Age_group.append(1)\n        elif (age) >= 40 and (age) < 60:\n            Age_group.append(2)\n        elif (age) >= 60 and (age) < 100:\n            Age_group.append(3)\n        else:\n            Age_group.append(2)\n    return Age_group\n\ntrain_df[\"AgeGroup\"]=AgeGroup(train_df['Age'])\ntest_df[\"AgeGroup\"]=AgeGroup(test_df['Age'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating a feature based on Sibsp and parch**\n\nsibsp\t- of siblings / spouses aboard the Titanic\t\nparch\t- of parents / children aboard the Titanic\t\n\nSince these two features represent Family we create new Column Family"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['FamilySize']=train_df['SibSp']+train_df['Parch']+1\ntest_df['FamilySize']=test_df['SibSp']+train_df['Parch']+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pylab \nimport scipy.stats as stats\n\nstats.probplot(train_df.Fare, dist=\"norm\", plot=pylab)\npylab.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Fare'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fare Category**\n\nSince Fare is From 0 - 512 we'll form a new variable Fare Category based on the quartile's it is distributed\n \n       FareCategory  - Quartile\n            0        -  0-25%\n            1        -  25-50%\n            2        -  50-75%\n            3        -  75-100%"},{"metadata":{"trusted":true},"cell_type":"code","source":"quartile_1=np.percentile(train_df.Fare, 25)\nquartile_2=np.percentile(train_df.Fare, 50)\nquartile_3=np.percentile(train_df.Fare, 75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Far_Cat(Fare):\n    FarCat=[]\n    for i in Fare:\n        if i >=0 and i< quartile_1:\n            FarCat.append(0)\n        if i >=quartile_1 and i< quartile_2:\n            FarCat.append(1)\n        if i >=quartile_2 and i< quartile_3:\n            FarCat.append(2)\n        if i >=quartile_3:\n            FarCat.append(3)\n    return FarCat\ntrain_df['FarCat']=Far_Cat(train_df.Fare)\ntest_df['FarCat']=Far_Cat(test_df.Fare)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['FamilySize']=test_df['FamilySize'].fillna(0)\ntest_df['Age']=test_df['Age'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dropping Columns**\n\n**Name**        -     Unique so not needed\n\n**Age**         -     Since we have AgeGroup,we'll delete this.\n\n**Ticket**      -     Unique so not needed\n\n**Fare**        -      Since we have FareCat,we'll delete this \n\n**Cabin**       -       Many Nan so imputing might lead to bais\n\n**PassengerId** - Cannot be categorised"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pid=test_df.PassengerId\ntrain_df=train_df.drop(['Ticket','Name','Age','Fare','Cabin','PassengerId'],axis=1)\ntest_df=test_df.drop(['Ticket','Name','Age','Fare','Cabin','PassengerId'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Label encoding categorical columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the columns whether they are categorical or numerical\ncols = train_df.columns\nnum_cols = train_df._get_numeric_data().columns\nprint(\"Numerical Columns\",num_cols)\ncat_cols=list(set(cols) - set(num_cols))\nprint(\"Categorical Columns:\",cat_cols)\n\nfrom sklearn.preprocessing import LabelEncoder\nfor i in cat_cols:\n    train_df[i]=LabelEncoder().fit_transform(train_df[i].astype(str)) \n    test_df[i]=LabelEncoder().fit_transform(test_df[i].astype(str)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Selection**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(train_df.corr(),ax=ax,annot= False,linewidth= 0.02,linecolor='black',fmt='.2f',cmap = 'Blues_r')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Distribution with respect Survived**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0, len(train_df.columns), 5):\n    sns.pairplot(data=train_df,\n                x_vars=train_df.columns[i:i+5],\n                y_vars=['Survived'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=train_df.loc[:,train_df.columns!='Survived']\nY=train_df.Survived","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(penalty=\"l1\", C=1).fit(X, Y)\nprediction = lr.predict(test_df)\npred_lr = pd.DataFrame()\npred_lr['PassengerId']=pid\npred_lr['Survived'] = prediction\npred_lr.to_csv(\"../working/submission_lr.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'submission_lr.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SVM Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nsvc = svm.SVC(\n    C=5,\n    kernel=\"rbf\",\n    degree=3,\n    gamma=\"auto\",\n    coef0=0.0,\n    shrinking=True,\n    probability=False,\n    tol=0.001,\n    cache_size=200,\n    class_weight=None,\n    verbose=False,\n    max_iter=-1,\n    decision_function_shape=\"ovr\",\n    random_state=None,\n)\nmodel = svc.fit(X, Y)\nprediction = model.predict(test_df)\npred_svc = pd.DataFrame()\npred_svc['PassengerId']=pid\npred_svc['Survived'] = prediction\npred_svc.to_csv(\"../working/submission_svc.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Knn**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel=KNeighborsClassifier(n_neighbors=3)\nmodel = model.fit(X,Y)\nprediction = model.predict(test_df)\npred_knn = pd.DataFrame()\npred_knn['PassengerId']=pid\npred_knn['Survived'] = prediction\npred_knn.to_csv(\"../working/submission_svc.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nclf = tree.DecisionTreeClassifier(\n    criterion=\"gini\",\n    splitter=\"best\",\n    max_depth=5,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features=None,\n    random_state=None,\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    class_weight=None,\n    presort=False,\n)\n\nmodel = clf.fit(X,Y)\nprediction = model.predict(test_df)\npred_dt = pd.DataFrame()\npred_dt['PassengerId']=pid\npred_dt['Survived'] = prediction\npred_dt.to_csv(\"../working/submission_svc.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import ensemble\nrf = ensemble.RandomForestClassifier(\n    n_estimators=800,\n    criterion=\"gini\",\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features=\"auto\",\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=1,\n    random_state=None,\n    verbose=0,\n    warm_start=False,\n    class_weight=None,\n)\nrf.fit(X, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = rf.predict(test_df)\npred_rf = pd.DataFrame()\npred_rf['PassengerId']=pid\npred_rf['Survived'] = prediction\npred_rf.to_csv(\"../working/submission_rf.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gradient Boosting Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(\n    loss=\"deviance\",\n    learning_rate=0.1,\n    n_estimators=200,\n    subsample=1.0,\n    criterion=\"friedman_mse\",\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_depth=3,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    init=None,\n    random_state=None,\n    max_features=None,\n)\nmodel = clf.fit(X, Y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = rf.predict(test_df)\npred_GB = pd.DataFrame()\npred_GB['PassengerId']=pid\npred_GB['Survived'] = prediction\npred_GB.to_csv(\"../working/submission_gb.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**XGB Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nmodel_xgb = xgb.XGBClassifier(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_xgb.fit(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model_xgb.predict(test_df)\npred_xgb = pd.DataFrame()\npred_xgb['PassengerId']=pid\npred_xgb['Survived'] = prediction\npred_xgb.to_csv(\"../working/submission_xgb.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensembling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statistics import mode\nfinal_pred = np.array([])\nfor i in range(0,len(test_df)):\n    final_pred = np.append(final_pred, mode([pred_lr['Survived'][i],pred_dt['Survived'][i],pred_knn['Survived'][i],pred_svc['Survived'][i],pred_rf['Survived'][i], pred_GB['Survived'][i], pred_xgb['Survived'][i]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model_xgb.predict(test_df)\npred_ensemble = pd.DataFrame()\npred_ensemble['PassengerId']=pid\npred_ensemble['Survived'] = prediction\npred_ensemble.to_csv(\"../working/submission_ensemble.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks **Manav Sehgal** for your kernel"},{"metadata":{},"cell_type":"markdown","source":"**Don't Forget to upvote :)**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}