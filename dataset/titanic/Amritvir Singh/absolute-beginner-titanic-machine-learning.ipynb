{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Titanic: Machine Learning from Disaster\n\nThis notebook is detailed walkthrough of all the steps I took to make my first Kaggle submission. The Challenges I faced in the process are not unique to you or someone who is just starting with Kaggle. Keeping in mind I have provided a detailed description of steps along with references and links. Taking reference from various internet blogs and videos, I finally put my code to work, I hope you find it useful and a kickstart for your ML journey. As you explore through more and more projects and get your hands dirty, you will gain more knowlege. The tough part is to get started, and believe me to just take a leap of faith. and its okay if you dont understand the code completely, with repetions the things form a clear picture of concepts.So lets dive in!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Best Model: Random Forest Classifier\n#### Best Score on train data: 80.92\n#### Score on submission: 74.16","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Contents:\n\n1. Project Skeleton\n2. Loading dataset (Downloaded from Kaggle)\n3. Exploratory Data Analysis\n    -   Exploring Missing Values\n    -  Data Interpretation and Visualization\n    - Count Plot for Features\n    -  Feature Relationships\n4. Data Pre-processing and Feature Engineering\n    -   Check Feature Data Types\n    -  Walk Through Each Feature One by One\n    - Re-Check Datasets\n5. Modelling\n    -  Try Different Models\n    - Survival Prediction on Test Data\n6. Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Project Skeleton\n\nBefore starting out any project, we must first plan our steps and have clarity on what type of problem we are tackling and what tools can be used and what cannot be used and why not?. This \"why not\" question will help you gain more insights on your ML journey. The following are key points I took into consideration.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Staircase\n1. What kind of ML problem statement is it? Try to define it\n2. Understand the type of data?\n3. Plot the values counts\n4. Check what data is missing (and also how can it be filled?)\n5. Relationships between various features\n6. Try your intuition about the field:\n    i. Are people with family more likely to survive?\n    ii. Are richer people more likely to survive\n    iii. Location of cabin and how the ship actually sank, did the front part sink first?\n7. Feature Engineering\n8. Pre-processing and scaling features\n9. Apply some base models and move to more advanced ones\n10. Cross validate on different subsets of data\n11. Chose the best one and predict on Test data\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Loading Datasets\nHere I have basically downloaded data into my repository and loaded them into pandas dataframe. Nothing too fancy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing base libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#magic function to view plot inside jupyter notebook\n%matplotlib inline \n\n#loading datasets\ntrain=pd.read_csv(\"../input/titanic/train.csv\")\ntest=pd.read_csv(\"../input/titanic/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## 3. Exploratory Data Analysis\nMajor portion of time while solving a ML problem will go into EDA and feature engineering. Applying a model is really 2-4 lines of code and easy part. So be patient here and use your own intuition. if you have strong foundation in stats, this can be really easy for you. Here we will first explore misisng data, then visualize the features and generate correlation to have an idea about how features are inter-related to each other.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### I. Exploring Missing Values\n\nReference for msno: https://towardsdatascience.com/visualize-missing-values-with-missingno-ad4d938b00a1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno #for visualizing missing data\n\n#train dataset missing data and preview\nprint(\"Train data rows x columns: \",train.shape,\"\\n\")\nprint(train.isnull().sum())\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test dataset missing data and preview\nprint(\"Test data rows x columns: \",train.shape,\"\\n\")\nprint(test.isnull().sum())\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize missing data in train as matrix\nmsno.matrix(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize missing data in train as matrix\nmsno.matrix(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize missing data in train as bar chart\nmsno.bar(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize missing data in test as bar chart\nmsno.bar(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if more than 40% of information is missing in columns for train data\nprint(train.isnull().sum()>int(0.40*train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if more than 40% of information is missing in columns for test data\nprint(train.isnull().sum()>int(0.40*train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### II. Data Interpretation and Visualization\nReference for seaborn: https://towardsdatascience.com/data-visualization-using-seaborn-fc24db95a850","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data is mssing in Age columns. Histogram plot for train data.\nsns.distplot(train['Age'].dropna(),hist=True, kde=True,rug=True, bins=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histogram plot for test data\nsns.distplot(test['Age'].dropna(),hist=True, kde=True, bins=40, rug=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### III. Count Plot for Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"Survived\",data=train,palette=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"Survived\",hue=\"Pclass\",data=train,palette=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### IV. Feature Relationships\nTo get an idea about how features are related to each other, we can generate a correlation matrix and check the pearson correlation coefficient\n\nReference: https://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr =train.corr()\nsns.heatmap(corr,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Data Pre-processing and Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### I. Check Feature Data Types","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the data type of each feature\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### II. Walk Through Each Feature One by One\nHere we explore each feature one by one and check if we can directly drop it, use it or extract some information from it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#intantiate label encoder\nle=LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combining both datasets\ndataset=[train,test]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### a. PassengerId\nThis is just the serial number identification for passengers and doe not contain much infromation. we can drop it directly","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the PassengerId column only in train as we equire passengerid in our test dataset for submission\ntrain.drop(\"PassengerId\",axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### b. Name\nName looks like it does not contain much information but, we can still create new feature \"Title\" and extract titles from each name in test and train data set which can be informative going further. To create this new feature we can use regular expressions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Name\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for data in dataset:\n    data[\"Title\"]=data[\"Name\"].str.extract('([A-Za-z]+)\\.',expand=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the Titles that were extracted","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Title\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"Title\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since most of the values are in Mr, Miss, Mrs, we can include the others in separate category and hence have four categories. We could have used label encoder but lot of small categories can be clubbed together into single category","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#create mapping\ntitle_mapping={\"Mr\":0,\"Miss\":1,\"Mrs\":2,\"Master\":3,\"Col\":3,\"Rev\":3,\"Ms\":3,\"Dr\":3,\"Dona\":3,\"Major\":3,\n         \"Mlle\":3,\"Countess\":3,\"Sir\":3,\"Jonkheer\":3,\"Don\":3,\"Mme\":3,\"Capt\":3,\"Lady\":3}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for data in dataset:\n    data[\"Title\"]=data[\"Title\"].map(title_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the count of survived according ot titles\nsns.countplot(x=\"Survived\",hue=\"Title\",data=train,palette=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the column Names as it is no longer required\nfor data in dataset:\n    data.drop(\"Name\",axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### c. Sex \nSince sex is a categorical variable we need to encode it using some sort of encoding, maybe manually using a dictionary or by the use of label encoder. Here I have used Label Encoder.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for data in dataset:\n    data[\"Sex\"]=le.fit_transform(data[\"Sex\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### d. Age\nSince has lots of missing values lets check how we can fill it. Lets first check the skewness","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Skewness = 3*(train[\"Age\"].mean()-train[\"Age\"].median())/train[\"Age\"].std()\nSkewness","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Skewness is positive so we can use medium to fill the missing values. But in our dataset we can do something better. We have feature \"Names\" from which we have extract titles like Mr,Mrs, master, miss, and now we can use the median of each group to fill the missing values for better accuracy of our model. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for data in dataset:\n    data[\"Age\"].fillna(data.groupby(\"Title\")[\"Age\"].transform(\"median\"),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### e. SibSp\n\nit contains information about the family, So we will combine this feature along with Parch to have a new feature called \"Family Size\"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### f. Parch\nThis also contains information about no of parents and children so let us combine it with SibSip to have new feature as \"Family Size\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for data in dataset:\n    data[\"Family Size\"]=data[\"SibSp\"]+data[\"Parch\"]+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the Parch and SibSp columns\nfor data in dataset:\n    data.drop([\"SibSp\",\"Parch\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### g. Ticket\nThis feature does not contain much data so we can drop it directly","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping Ticket column\nfor data in dataset:\n    data.drop([\"Ticket\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### h. Fare\nSince Fare has very few missing values, we can replace fare with its mean value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for data in dataset:\n    data[\"Fare\"].fillna(data[\"Fare\"].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### i. Cabin\nSince more than 40% of value in cabin data is missing, we can drop it directly if we want. But here I will extract the first letter of each cabin name and apply a numerical mapping on it and hence fill the missing values later. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"Cabin\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for data in dataset:\n    data[\"Cabin\"] = data[\"Cabin\"].str[:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cabin_mapping={\"A\":0,\"B\":0.4,\"C\":0.8,\"D\":1.2,\"E\":1.6,\"F\":2,\"G\":2.4,\"T\":2.8} #This is called feature scaling, please explore more on this advanced topic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for data in dataset:\n    data[\"Cabin\"] = data[\"Cabin\"].map(cabin_mapping)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fill the missing value by grouping by Pclass, since cabins are related to class of booking","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for data in dataset:\n    data[\"Cabin\"].fillna(data.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### j. Embarked\nSince Embarked has we missing values and is Categorical, we will just simply fill it by its mode from values S,C,Q.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for data in dataset:\n    data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label encoding Embarked\nfor data in dataset:\n    data[\"Embarked\"]=le.fit_transform(data[\"Embarked\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### III. Re-Check Datasets \nit is good practice to re-check datasets before actually applying models. Check if both train and test datasets have no null values. Here we must also separate labels and features from train dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Checking for null values if any:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#for train dataset\nsns.heatmap(train.isnull(),cmap = 'magma' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For test dataset\nsns.heatmap(test.isnull(),cmap = 'magma' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#separating labels\ny_train = train[\"Survived\"]\n#separating features\ntrain.drop(\"Survived\", axis=1,inplace=True)\nX_train=train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking train dataset\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking test dataset after removing passengerid as a copy of test data, as we reuire passengerid in final submission on Kaggle\nX_test = test.drop(\"PassengerId\",axis=1).copy()\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finally check the shapes\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Modelling\nWe will start with some basic models and then go through more advanced ensemble models. Lets drive straight into it!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import classifiers\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import libraries and functions for cross validation and metrics for accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score\n#set values for K-folds\nfolds= KFold(n_splits=10,shuffle=True,random_state=0)\nmetric=\"accuracy\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I. Try Different Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### a. K-nearest neighbor Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn=KNeighborsClassifier(n_neighbors=10)\nscore= cross_val_score(knn,X_train,y_train,cv=folds,n_jobs=1,scoring=metric)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mean score rounded to 2 decimal points\nround(np.mean(score)*100,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### b. Decision Tree Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc=DecisionTreeClassifier()\nscore= cross_val_score(dtc,X_train,y_train,cv=folds,n_jobs=1,scoring=metric)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mean score rounded to 2 decimal points\nround(np.mean(score)*100,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### c. Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc=RandomForestClassifier()\nscore= cross_val_score(rfc,X_train,y_train,cv=folds,n_jobs=1,scoring=metric)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mean score rounded to 2 decimal points\nround(np.mean(score)*100,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### d. Gaussian NB Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb=GaussianNB()\nscore= cross_val_score(gnb,X_train,y_train,cv=folds,n_jobs=1,scoring=metric)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mean score rounded to 2 decimal points\nround(np.mean(score)*100,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### e. Support Vector Machine Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svmcl=SVC()\nscore= cross_val_score(svmcl,X_train,y_train,cv=folds,n_jobs=1,scoring=metric)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mean score rounded to 2 decimal points\nround(np.mean(score)*100,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### II. Survival Prediction on Test Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#using the classifier that gave highest average accuracy on train dataset\nclf=RandomForestClassifier()\nclf.fit(X_train,y_train)\ny_test = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#wrapping up into a submission dataframe\nsubmission=pd.DataFrame({\"PassengerId\": test[\"PassengerId\"],\"Survived\":y_test})\n\n#converting to submission csv\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.read_csv(\"submission.csv\")\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Conclusion\n\nThe output csv is generated in the right pane in the kernel under folder \"output\" You can submit it on Kaggle using the following link:\n\nhttps://www.kaggle.com/c/titanic\n\nI am new to ML path, so this may not be the finest implementation but I tried to get my hands on with the knowledge I had. Feel free to comment on techniques I can use to improve the score of the models and dont forget to Upvote!\n\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}