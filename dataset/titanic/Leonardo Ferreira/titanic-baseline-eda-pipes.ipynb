{"cells":[{"metadata":{"_cell_guid":"adfd17d9-7925-49bd-94bf-854780a835c0","_uuid":"cdbc543b754b4cb269704504cca86466954846f2"},"cell_type":"markdown","source":"<h1> Welcome to my Titanic Kernel! </h1>\nThis kernel covers: Data Exploration and Visualization, data handling and modelling, Features preprocessing, ML pipeline, automated hyper parameter with HyperOpt and prediction of a dependent variable ('survived').  \n\nWhen I started on Data Science field, my second work on Kaggle was on titanic Dataset and now, I want to improve my work here.\n\nI will use a easy code that maybe could be useful to many people that are starting on Data Science or PyData libraries."},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"red\">If this kernel were useful for you, please <b>UPVOTE</b> the kernel =)</font>\nAlso, don't forget to give me your feedback, it's many important to me"},{"metadata":{"_cell_guid":"f47e97ab-eb3e-49c7-8ea6-25a6d9f8312d","_uuid":"10bb3506c6e83b4a8938f81620474ae1e214a544"},"cell_type":"markdown","source":"If you want many other simple kernels with pythonic code <a href=\"https://www.kaggle.com/kabure/kernels\">CLICK HERE</a> <br>\n\n"},{"metadata":{"_cell_guid":"4247239a-11d0-4a90-8431-c410409d7a1c","_uuid":"6008e6d18320ba0ff790618af888d3a4c25839bf"},"cell_type":"markdown","source":"<i>*I'm from Brazil, so english is not my first language, sorry about some mistakes</i>"},{"metadata":{"_cell_guid":"45132ff6-d170-4229-a9f9-62cfc3d6f33b","_uuid":"fa61bfbef5735a7c6a83747d15724da0fc73e7ad"},"cell_type":"markdown","source":"# Table of Contents:\n\n**1. [Introduction](#Introduction)** <br>\n**2. [Librarys](#Librarys)** <br>\n**3. [Knowning the data](#Known)** <br>\n**4. [Exploring some Variables](#Explorations)** <br>\n**5. [Preprocessing](#Prepocess)** <br>\n**6. [Modelling](#Model)** <br>\n**7. [Validation](#Validation)** <br>\n"},{"metadata":{"_cell_guid":"61a2256a-c100-405b-a6a7-7382ec84ef90","_uuid":"23440568688f65c1549aaa44c364e2a8aebeeb86"},"cell_type":"markdown","source":"<a id=\"Introduction\"></a> <br> \n# **1. Introduction:** \n<h3> The data have 891 entries on train dataset and 418 on test dataset</h3>\n- 10 columns in train_csv and 9 columns in train_test\n"},{"metadata":{"_cell_guid":"543344f8-6da7-4119-a60d-e4059d4aaf60","_uuid":"25b0f965e63fe79715087729784ece8d424820ad"},"cell_type":"markdown","source":"<h2>Competition Description: </h2>\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy."},{"metadata":{"_cell_guid":"9b61b570-3c40-4fe9-a6dd-cd010c2d55c4","_uuid":"73cc777de74395eb8d0e6ced15f8783c085bcc21"},"cell_type":"markdown","source":"<h3>Data Dictionary</h3><br>\nVariable\tDefinition\tKey<br>\n<b>survival</b>\tSurvival\t0 = No, 1 = Yes<br>\n<b>pclass</b>\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd<br>\n<b>sex</b>\tSex\t<br>\n<b>Age</b>\tAge in years\t<br>\n<b>sibsp</b>\t# of siblings / spouses aboard the Titanic\t<br>\n<b>parch</b>\t# of parents / children aboard the Titanic\t<br>\n<b>ticket</b>\tTicket number\t<br>\n<b>fare</b>\tPassenger fare\t<br>\n<b>cabin</b>\tCabin number\t<br>\n<b>embarked\t</b>Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton<br>\n<h3>Variable Notes</h3><br>\n<b>pclass: </b>A proxy for socio-economic status (SES)<br>\n1st = Upper<br>\n2nd = Middle<br>\n3rd = Lower<br>\n<b>age: </b>Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br>\n<b>sibsp:</b> The dataset defines family relations in this way...<br>\n- <b>Sibling </b>= brother, sister, stepbrother, stepsister<br>\n- <b>Spouse </b>= husband, wife (mistresses and fianc√©s were ignored)<br>\n\n<b>parch: </b>The dataset defines family relations in this way...<br>\n- <b>Parent</b> = mother, father<br>\n- <b>Child </b>= daughter, son, stepdaughter, stepson<br>\n\nSome children travelled only with a nanny, therefore parch=0 for them.<br>"},{"metadata":{"_cell_guid":"e5b2eb99-c1fb-4a8a-bb2b-fbb159358ffd","_uuid":"2d1b974a41c9b8ae24da08ba4793afec3d516ecd"},"cell_type":"markdown","source":"I am using the beapproachs as possible but if you think I can do anything another best way, please, let me know."},{"metadata":{"_cell_guid":"ab631e54-b2cc-45f1-bbcd-295e26f97a32","_uuid":"cd0487ef702d4ac4e7154af34346ddf9b856ab28"},"cell_type":"markdown","source":"<a id=\"Librarys\"></a> <br> \n# **2. Importing Librarys:** "},{"metadata":{"_cell_guid":"d018a79d-a8da-4dab-b289-afd411c89231","_uuid":"eb9b1cb179aae5859c84388844468c5225567241","trusted":true},"cell_type":"code","source":"#This librarys is to work with matrices\nimport pandas as pd \n# This librarys is to work with vectors\nimport numpy as np\n# This library is to create some graphics algorithmn\nimport seaborn as sns\n# to render the graphs\nimport matplotlib.pyplot as plt\n# import module to set some ploting parameters\nfrom matplotlib import rcParams\n# Library to work with Regular Expressions\nimport re\nimport gc\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, accuracy_score, make_scorer\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n## Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\nfrom scipy import stats\n\n# This function makes the plot directly on browser\n%matplotlib inline\n\n# Seting a universal figure size \nrcParams['figure.figsize'] = 12,5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Datasets"},{"metadata":{"_cell_guid":"61a77a63-b9ed-455b-b3d2-fde823676ed8","_uuid":"688fd0574f7f65b0fdaa5774cf6fc7626bd24d01","trusted":true},"cell_type":"code","source":"# Importing train dataset\ndf_train = pd.read_csv(\"../input/titanic/train.csv\")\n\n# Importing test dataset\ndf_test = pd.read_csv(\"../input/titanic/test.csv\")\n\nsubmission = pd.read_csv(\"../input/titanic/gender_submission.csv\", index_col='PassengerId')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6149e4cd-1340-4db1-9d3f-04c67e780aa3","_uuid":"d87340ef76b49cd3a6d346390c38cb4d140220ae"},"cell_type":"markdown","source":"<a id=\"Known\"></a> <br> \n# **3. First look at the data:** \n- I will implement a function to we summary all columns in a meaningful table."},{"metadata":{"trusted":true},"cell_type":"code","source":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary of df train"},{"metadata":{"trusted":true},"cell_type":"code","source":"resumetable(df_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1ed66b34-9219-49a2-a1f4-04fd9d00777e","_uuid":"069f3ccadab3fcc3af53c594e1b14c91bd801b85"},"cell_type":"markdown","source":"Cool!! We can see very important information about all our data. <br>\nOur target is \"Survived\" column that informs if the passenger survived or not the disaster"},{"metadata":{},"cell_type":"markdown","source":"## Summary of df test"},{"metadata":{"trusted":true},"cell_type":"code","source":"resumetable(df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In df test, we have missing values only on "},{"metadata":{"_cell_guid":"cc879cc5-0cc7-4fc0-8e0f-826b299270a9","_uuid":"612fec9e24c56a509b472266db3122560b90aa07"},"cell_type":"markdown","source":"<a id=\"Known\"></a> <br> \n# **4. Exploring the data:** \n- Different of the other Kernel, as now I am more experienced in data science, I will start by the target distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Survived'].replace({0:'No', 1:'Yes'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = len(df_train)\nplt.figure(figsize=(12,7))\n#plt.subplot(121)\ng = sns.countplot(x='Survived', data=df_train, color='green')\ng.set_title(f\"Passengers alive or died Distribution \\nTotal Passengers: {total}\", \n            fontsize=22)\ng.set_xlabel(\"Passenger Survived?\", fontsize=18)\ng.set_ylabel('Count', fontsize=18)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=15) \ng.set_ylim(0, total *.70)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool. We can see that only 38.38% of passengers survived. <br>\nLet's explore our other featuers and try to find some patterns"},{"metadata":{},"cell_type":"markdown","source":"# Let's start exploring Age Column\nI will start by the \"simplest\" columns that are columns that don't need some transformations or have only few unique values.\nThe first objective is to:\n- Explore the features\n- Imput missing values\n- See the distribution of numerical and categorical features\n- Understand the difference between groups that survived and not"},{"metadata":{"trusted":true},"cell_type":"code","source":"#First I will look my distribuition without NaN's\n#I will create a df to look distribuition \nage_high_zero_died = df_train[(df_train[\"Age\"] > 0) & \n                              (df_train[\"Survived\"] == 'No')]\nage_high_zero_surv = df_train[(df_train[\"Age\"] > 0) & \n                              (df_train[\"Survived\"] == 'Yes')]\n\n#figure size\nplt.figure(figsize=(16,5))\n\nplt.subplot(121)\nplt.suptitle('Age Distributions', fontsize=22)\nsns.distplot(df_train[(df_train[\"Age\"] > 0)][\"Age\"], bins=24)\nplt.title(\"Distribuition of Age\",fontsize=20)\nplt.xlabel(\"Age Range\",fontsize=15)\nplt.ylabel(\"Probability\",fontsize=15)\n\nplt.subplot(122)\n\nsns.distplot(age_high_zero_surv[\"Age\"], bins=24, color='r', label='Survived')\nsns.distplot(age_high_zero_died[\"Age\"], bins=24, color='blue', label='Not Survived')\nplt.title(\"Distribution of Age by Target\",fontsize=20)\nplt.xlabel(\"Age\",fontsize=15)\nplt.ylabel(\"Probability\",fontsize=15)\nplt.legend()\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting! A big part of all passengers has between 20 to 40 old years. <br>\nWhen we analyze the distribution by the target we can note that youngest adults has a highest density in not survived passengers.\n\nI will continue working in Age feature but before, I will try to understand the other columns. Maybe it could work well together "},{"metadata":{},"cell_type":"markdown","source":"# Gender Column\nUnderstanding Gender distribution and distribution by target"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_categoricals(df, col=None, cont='Age', binary=None, dodge=True):\n    tmp = pd.crosstab(df[col], df[binary], normalize='index') * 100\n    tmp = tmp.reset_index()\n\n    plt.figure(figsize=(16,12))\n\n    plt.subplot(221)\n    g= sns.countplot(x=col, data=df, order=list(tmp[col].values) , color='green')\n    g.set_title(f'{col} Distribuition', \n                fontsize=20)\n    g.set_xlabel(f'{col} Values',fontsize=17)\n    g.set_ylabel('Count Distribution', fontsize=17)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.0f}'.format(height),\n                ha=\"center\", fontsize=15) \n    g.set_ylim(0,max(sizes)*1.15)\n\n    plt.subplot(222)\n    g1= sns.countplot(x=col, data=df, order=list(tmp[col].values),\n                     hue=binary,palette=\"hls\")\n    g1.set_title(f'{col} Distribuition by {binary} ratio %', \n                fontsize=20)\n    gt = g1.twinx()\n    gt = sns.pointplot(x=col, y='Yes', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False)\n    gt.set_ylim(0,tmp['Yes'].max()*1.1)\n    gt.set_ylabel(\"Survived %Ratio\", fontsize=16)\n    g1.set_ylabel('Count Distribuition',fontsize=17)\n    g1.set_xlabel(f'{col} Values', fontsize=17)\n    \n    sizes = []\n    \n    for p in g1.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g1.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\", fontsize=10) \n    g1.set_ylim(0,max(sizes)*1.15)\n\n    plt.subplot(212)\n    g2= sns.swarmplot(x=col, y=cont, data=df, dodge=dodge, order=list(tmp[col].values),\n                     hue=\"Survived\",palette=\"hls\")\n    g2.set_title(f'{cont} Distribution by {col} and {binary}', \n                fontsize=20)\n    g2.set_ylabel(f'{cont} Distribuition',fontsize=17)\n    g2.set_xlabel(f'{col} Values', fontsize=17)\n\n\n    plt.suptitle(f'{col} Distributions', fontsize=22)\n    plt.subplots_adjust(hspace = 0.4, top = 0.90)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals(df_train, col='Sex', cont='Age', binary='Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool. Now we can see meaningful informations about the passengers. <br>\nThe distribution of Ages and Gender by Survived can show us a interesting pattern in people who survived and who not"},{"metadata":{},"cell_type":"markdown","source":"# PClass\n- Other feature that I think that could be important to understand the passenger's survivors\n- Let's understand distributions of Pclass and how it is distributed in considering our target feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals(df_train, col='Pclass', cont='Age', binary='Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that 55% of passengers are in the 3rd Class and also, is the Class where more people died. <br>\nLet's use the powerful <b>pd.crosstab</b> to see the distribution of Pclass by Sex and get the ratio of survivors"},{"metadata":{"trusted":true},"cell_type":"code","source":"(round(pd.crosstab(df_train['Survived'], [df_train['Pclass'], df_train['Sex']], \n             normalize='columns' ) * 100,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I noted one interesting thing. <br>\nIn first and second class the female have 92%+ of survivors, and in 3rd class the ratio is 50% in female survivors<br>\nAnother interesting information is that in first class the percent of male survivors are almost 37% and in 2 and 3 class the ratio is 15.7 and 13.5 respectivelly."},{"metadata":{},"cell_type":"markdown","source":"# Embarked Feature\n- Exploring the Distributions of the feature\n- Filling Na's values (we have only 2 missing values in this feature)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals(df_train, col='Embarked', cont='Age', binary='Survived')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another interesting information. We can see that 72.4% of all passengers embarked in \"S\" (Southampton). <br>\nAlso, 47% of all died passengers is from S."},{"metadata":{},"cell_type":"markdown","source":"### Let's fill na's in Embarked"},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets input the NA's with the highest frequency\ndf_train[\"Embarked\"] = df_train[\"Embarked\"].fillna('S')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Crossing Embarked by PClass and Survived"},{"metadata":{"trusted":true},"cell_type":"code","source":"(round(pd.crosstab(df_train['Survived'], [df_train['Embarked'], df_train['Pclass']], \n             normalize='columns' ) * 100,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## Crossing Embarked by Sex and Survived"},{"metadata":{"trusted":true},"cell_type":"code","source":"(round(pd.crosstab(df_train['Survived'], [df_train['Embarked'], df_train['Sex']], \n             normalize='columns' ) * 100,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we have we have different ratios when considering the Embarked place and Sex. It could be useful to build some features. "},{"metadata":{},"cell_type":"markdown","source":"# Fare Column"},{"metadata":{},"cell_type":"markdown","source":"## Looking quantiles of Fare "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Fare'].quantile([.01, .1, .25, .5, .75, .9, .99]).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Geting the Fare Log "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Fare_log'] = np.log(df_train['Fare'] + 1)\ndf_test['Fare_log'] = np.log(df_test['Fare'] + 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ploting Fare Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seting the figure size\nplt.figure(figsize=(16,10))\n\n# Understanding the Fare Distribuition \nplt.subplot(221)\nsns.distplot(df_train[\"Fare\"], bins=50 )\nplt.title(\"Fare Distribuition\", fontsize=20)\nplt.xlabel(\"Fare\", fontsize=15)\nplt.ylabel(\"Density\",fontsize=15)\n\nplt.subplot(222)\nsns.distplot(df_train[\"Fare_log\"], bins=50 )\nplt.title(\"Fare LOG Distribuition\", fontsize=20)\nplt.xlabel(\"Fare (Log)\", fontsize=15)\nplt.ylabel(\"Density\",fontsize=15)\n\nplt.subplot(212)\ng1 = plt.scatter(range(df_train[df_train.Survived == 'No'].shape[0]),\n                 np.sort(df_train[df_train.Survived == 'No']['Fare'].values), \n                 label='No Survived', alpha=.5)\ng1 = plt.scatter(range(df_train[df_train.Survived == 'Yes'].shape[0]),\n                 np.sort(df_train[df_train.Survived == 'Yes']['Fare'].values), \n                 label='Survived', alpha=.5)\ng1= plt.title(\"Fare ECDF Distribution\", fontsize=18)\ng1 = plt.xlabel(\"Index\")\ng1 = plt.ylabel(\"Fare Amount\", fontsize=15)\ng1 = plt.legend()\n\nplt.suptitle('Fare Distributions', fontsize=22)\nplt.subplots_adjust(hspace = 0.4, top = 0.90)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool. We can note that the big part of passengers paid less than USD 100. We can't see some difference between the survived or not group. <br>\nI will try cross Fare by other features and try to find some interesting patterns"},{"metadata":{"_cell_guid":"b8df1880-dd5f-4c98-a4a3-a3fee8950eaa","_uuid":"410262c53a3e6b2754834ced213cf5b342734ab6"},"cell_type":"markdown","source":"<br>\nDescription of Fare variable<br>\n- Min: 0<br>\n- Median: 14.45<br>\n- Mean: 32.20<br>\n- Max: 512.32<br> \n- Std: 49.69<br>\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Categorical features by Fare"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ploting_cat_group(df, col):\n    plt.figure(figsize=(14,6))\n    tmp = pd.crosstab(df['Survived'], df[col], \n                      values=df['Fare'], aggfunc='mean').unstack(col).reset_index().rename(columns={0:'FareMean'})\n    g = sns.barplot(x=col, y='FareMean', hue='Survived', data=tmp)\n    g.set_xlabel(f'{col} values', fontsize=18)\n    g.set_ylabel('Fare Mean', fontsize=18)\n    g.set_title(f\"Fare Distribution by {col} \", fontsize=20)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fare mean by Pclass"},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cat_group(df_train, 'Pclass')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The fist class passengers has highest Fare mean, that make many sense. But we can't see a high difference between second and third class Fare mean"},{"metadata":{},"cell_type":"markdown","source":"## Fare mean by Embarked"},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cat_group(df_train, 'Embarked')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People of C has a highest Fare mean."},{"metadata":{},"cell_type":"markdown","source":"## Fare mean by Sex"},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cat_group(df_train, 'Sex')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that We can infer that pooverty people had more probability to die."},{"metadata":{},"cell_type":"markdown","source":"## Fare by Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\ng = sns.scatterplot(x='Age', y='Fare_log', data=df_train, hue='Survived')\ng.set_title('Fare Distribution by Age', fontsize= 22)\ng.set_xlabel('Age Distribution', fontsize=18)\ng.set_ylabel(\"Fare Log Distribution\", fontsize=18)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool! We can see see the Fare distribution by Age and confirm and infer some questions. For example, the first class, probably has a highest age mean. Let's confirm that. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby(['Survived', 'Pclass'])['Age'].mean().unstack('Survived').reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool!!! Exactly what I tought. We can see that in 3 class we have a smallest Age mean"},{"metadata":{},"cell_type":"markdown","source":"# Names Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Name'].unique()[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can note that all names have the titles of passengers. Let's use regex to extract titles of passengers."},{"metadata":{},"cell_type":"markdown","source":"### Let's see the extracted titles"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting the prefix of all Passengers\ndf_train['Title'] = df_train.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\ndf_test['Title'] = df_test.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\n\n(df_train['Title'].value_counts(normalize=True) * 100).head(5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mapping the titles"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now, I will identify the social status of each title\n\nTitle_Dictionary = {\n        \"Capt\":       \"Officer\",\n        \"Col\":        \"Officer\",\n        \"Major\":      \"Officer\",\n        \"Dr\":         \"Officer\",\n        \"Rev\":        \"Officer\",\n        \"Jonkheer\":   \"Royalty\",\n        \"Don\":        \"Royalty\",\n        \"Sir\" :       \"Royalty\",\n        \"the Countess\":\"Royalty\",\n        \"Dona\":       \"Royalty\",\n        \"Lady\" :      \"Royalty\",\n        \"Mme\":        \"Mrs\",\n        \"Ms\":         \"Mrs\",\n        \"Mrs\" :       \"Mrs\",\n        \"Mlle\":       \"Miss\",\n        \"Miss\" :      \"Miss\",\n        \"Mr\" :        \"Mr\",\n        \"Master\" :    \"Master\"\n}\n    \n# we map each title to correct category\ndf_train['Title'] = df_train.Title.map(Title_Dictionary)\ndf_test['Title'] = df_test.Title.map(Title_Dictionary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ploting Title Distributions"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals(df_train, col='Title', cont='Age', binary='Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very interesting information. The data has 18 Officer's and we can note that Officer's have the highest Age mean. <br>\nCuriously we can note that \"Master\" has a very low Age distribution. It sounds very strange to me. "},{"metadata":{},"cell_type":"markdown","source":"# Let's use some features to help us fill Age NaN's "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's group the median age by sex, pclass and title, to have any idea and maybe input in Age NAN's\nage_group = df_train.groupby([\"Sex\",\"Pclass\",\"Title\"])[\"Age\"]\n\n#printing the variabe that we created by median\nage_group.median().unstack('Pclass').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"87ba3b6f-9a4c-4d0d-b1f6-2cf0ef45311d","_uuid":"3af4bfdefd77e3099be92daa789455fc0a34e345","trusted":true},"cell_type":"code","source":"#inputing the values on Age Na's \n# using the groupby to transform this variables\ndf_train.loc[df_train.Age.isnull(), 'Age'] = df_train.groupby(['Sex','Pclass','Title']).Age.transform('median')\ndf_test.loc[df_train.Age.isnull(), 'Age'] = df_test.groupby(['Sex','Pclass','Title']).Age.transform('median')\n\n# printing the total of nulls in Age Feature\nprint(df_train[\"Age\"].isnull().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"09c0ec2f-4271-4f72-a2ef-4ea407752751","_uuid":"525d53d8b8adc49d04c6db8a9ac778427f28a086","trusted":true},"cell_type":"code","source":"#df_train.Age = df_train.Age.fillna(-0.5)\n\n#creating the intervals that we need to cut each range of ages\ninterval = (0, 5, 12, 18, 25, 35, 60, 120) \n\n#Seting the names that we want use to the categorys\ncats = ['babies', 'Children', 'Teen', 'Student', 'Young', 'Adult', 'Senior']\n\n# Applying the pd.cut and using the parameters that we created \ndf_train[\"Age_cat\"] = pd.cut(df_train.Age, interval, labels=cats)\ndf_test[\"Age_cat\"] = pd.cut(df_test.Age, interval, labels=cats)\n\n# Printing the new Category\ndf_train[\"Age_cat\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ploting Age Cat Distributions - Fare"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals(df_train, col='Age_cat', cont='Fare', binary='Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Sibsp\tfeature\nthis feature refers to siblings / spouses aboard the Titanic\t\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals(df_train, col='SibSp', cont='Age', binary='Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Parch\tfeature\nThe feature refers to parents / children aboard the Titanic\t<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals(df_train, col='Parch', cont='Age', binary='Survived', dodge=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Creating the Family Size feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a new column and sum the Parch + SibSp + 1 that refers the people self\ndf_train[\"FSize\"] = df_train[\"Parch\"] + df_train[\"SibSp\"] + 1\ndf_test[\"FSize\"] = df_test[\"Parch\"] + df_test[\"SibSp\"] + 1\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', \n              5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large',\n              11: 'Large'}\n\ndf_train['FSize'] = df_train['FSize'].map(family_map)\ndf_test['FSize'] = df_test['FSize'].map(family_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ploting Family Size"},{"metadata":{"_cell_guid":"52abbfa0-8ba6-4b4e-998e-7d7f34e7491f","_uuid":"928b946783769488edba7961a728a4d265fe2721","scrolled":false,"trusted":true},"cell_type":"code","source":"plot_categoricals(df_train, col='FSize', cont='Fare', binary='Survived', dodge=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool!!! We can see that 60% of passengers are traveling alone. Curiosly, we can note that these people have a highest ratio of Not survived Passengers. <br>\nThe chance to survive is highest to people with small families on the boat. \n"},{"metadata":{},"cell_type":"markdown","source":"# Keep thinking about Familys\n### Extracting Sur Names \n- Taking advantage that we are dealing with family features, lets extract sur name from Name Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"## I saw this code in another kernel and it is very useful\n## Link: https://www.kaggle.com/gunesevitan/advanced-feature-engineering-tutorial-with-titanic\nimport string\n\ndef extract_surname(data):    \n    \n    families = []\n    \n    for i in range(len(data)):        \n        name = data.iloc[i]\n\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n            \n    return families","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Family'] = extract_surname(df_train['Name'])\ndf_test['Family'] = extract_surname(df_test['Name'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ticket feature\n- Understanding and creading new feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Ticket'].value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ticket is a very sparse data, with many values. So, lets try associate it with Family's and feed our model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Ticket_Frequency'] = df_train.groupby('Ticket')['Ticket'].transform('count')\ndf_test['Ticket_Frequency'] = df_test.groupby('Ticket')['Ticket'].transform('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's see the means of Fare "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby(['Survived', 'FSize'])['Fare'].mean().unstack('FSize').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5af6dec2-b158-4464-a575-0bbb0e34597c","_uuid":"97b914a88fe66c7ff5655cf9c70f3eef5c57a1c5"},"cell_type":"markdown","source":"Now it look's better and clearly"},{"metadata":{"_cell_guid":"0f80bee7-c523-4eae-bad1-3267413b8f71","_uuid":"9f997138277822312c4ba718313d16ede922787e","trusted":true},"cell_type":"code","source":"#Filling the NA's with -0.5\ndf_train.Fare = df_train.Fare.fillna(-1)\ndf_test.Fare = df_test.Fare.fillna(-1)\n#intervals to categorize\nquant = (-1, 0, 12, 30, 80, 100, 200, 600)\n\n#Labels without input values\nlabel_quants = ['NoInf', 'quart_1', 'quart_2', 'quart_3', 'quart_4', 'quart_5', 'quart_6']\n\n#doing the cut in fare and puting in a new column\ndf_train[\"Fare_cat\"] = pd.cut(df_train.Fare, quant, labels=label_quants)\ndf_test[\"Fare_cat\"] = pd.cut(df_test.Fare, quant, labels=label_quants)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals(df_train, col='Fare_cat', cont='Age', binary='Survived', dodge=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Excellent implementation from: \n# https://www.kaggle.com/franjmartin21/titanic-pipelines-k-fold-validation-hp-tuning\n\ndef cabin_extract(df):\n    return df['Cabin'].apply(lambda x: str(x)[0] if(pd.notnull(x)) else str('M'))\n\ndf_train['Cabin'] = cabin_extract(df_train)\ndf_test['Cabin'] = cabin_extract(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals(df_train, col='Cabin', cont='Age', binary='Survived', dodge=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Seting Cabin into Groups"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df_train['Cabin'], df_train['Pclass'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on informations of the boat and the confirmation of te crosstab.\n- ABC cabins are to first class\n- DE cabins are to first and second class\n- FG are majority to third class\n- M are the missing values\n- On the Boat Deck there were **6** rooms labeled as **T, U, W, X, Y, Z**, but only the **T** cabin is present in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Cabin'] = df_train['Cabin'].replace(['A', 'B', 'C'], 'ABC')\ndf_train['Cabin'] = df_train['Cabin'].replace(['D', 'E'], 'DE')\ndf_train['Cabin'] = df_train['Cabin'].replace(['F', 'G'], 'FG')\n# Passenger in the T deck is changed to A\ndf_train.loc[df_train['Cabin'] == 'T', 'Cabin'] = 'A'\n\ndf_test['Cabin'] = df_test['Cabin'].replace(['A', 'B', 'C'], 'ABC')\ndf_test['Cabin'] = df_test['Cabin'].replace(['D', 'E'], 'DE')\ndf_test['Cabin'] = df_test['Cabin'].replace(['F', 'G'], 'FG')\ndf_test.loc[df_test['Cabin'] == 'T', 'Cabin'] = 'A'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# End of EDA"},{"metadata":{},"cell_type":"markdown","source":"# Modelling\n- To a better understanding of the modelling part, I will delete df and train and "},{"metadata":{},"cell_type":"markdown","source":"## Dropping unecessary features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.api.types import CategoricalDtype \nfamily_cats = CategoricalDtype(categories=['Alone', 'Small', 'Medium', 'Large'], ordered=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.FSize = df_train.FSize.astype(family_cats)\ndf_test.FSize = df_test.FSize.astype(family_cats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Age_cat = df_train.Age_cat.cat.codes\ndf_train.Fare_cat = df_train.Fare_cat.cat.codes\ndf_test.Age_cat = df_test.Age_cat.cat.codes\ndf_test.Fare_cat = df_test.Fare_cat.cat.codes\ndf_train.FSize = df_train.FSize.cat.codes\ndf_test.FSize = df_test.FSize.cat.codes","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5fc23992-efec-4b03-9b76-e7cb271866a0","_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"3b0759682656ce4b4ca00919385353090e9ef207","trusted":true},"cell_type":"code","source":"#Now lets drop the variable Fare, Age and ticket that is irrelevant now\ndf_train.drop([ 'Ticket', 'Name'], axis=1, inplace=True)\ndf_test.drop(['Ticket', 'Name', ], axis=1, inplace=True)\n#df_train.drop([\"Fare\", 'Ticket', 'Age', 'Cabin', 'Name', 'SibSp', 'Parch'], axis=1, inplace=True)\n#df_test.drop([\"Fare\", 'Ticket', 'Age', 'Cabin', 'Name', 'SibSp', 'Parch'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a528235e-1ce4-4ec7-bb2d-18f8e1a897c7","_uuid":"b74cefa05a294976b2cc005f613cfb9427190f9f"},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"_cell_guid":"f810cb12-1ac6-467f-9ed8-f96ba7af5c94","_uuid":"840ce4a7f3e53e5a2939625273f14e81b879f3d3"},"cell_type":"markdown","source":"Now we might have information enough to think about the model structure"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['Survived'] = 'test'\ndf = pd.concat([df_train, df_test], axis=0, sort=False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding and getting Dummies of categorical features"},{"metadata":{},"cell_type":"markdown","source":"### Encoding\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['Family'] = le.fit_transform(df['Family'].astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Dummies"},{"metadata":{"_cell_guid":"93854fb1-9a60-412d-8eca-4c3e8631a148","_uuid":"9a83734eca0f33ff1a356d2d3e54ef8acc0f1992","trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, columns=['Sex', 'Cabin', 'Embarked', 'Title'],\\\n                          prefix=['Sex', \"Cabin\", 'Emb', 'Title'], drop_first=True)\n\ndf_train, df_test = df[df['Survived'] != 'test'], df[df['Survived'] == 'test'].drop('Survived', axis=1)\ndel df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Survived'].replace({'Yes':1, 'No':0}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Train shape: {df_train.shape}')\nprint(f'Train shape: {df_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(['Age', 'Fare','Fare_log','Family', 'SibSp', 'Parch'], axis=1, inplace=True)\ndf_test.drop(['Age', 'Fare','Fare_log','Family', 'SibSp', 'Parch'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting X and Y"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train.drop([\"Survived\",\"PassengerId\"],axis=1)\ny_train = df_train[\"Survived\"]\n\nX_test = df_test.drop([\"PassengerId\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resumetable(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5963f209-f2d8-407a-90f0-41f8cf51cae0","_uuid":"649f305a1329ee5fd008781d05037063c8f38ef5"},"cell_type":"markdown","source":"<a id=\"Model\"></a> <br> \n# **6. Modelling Pipeline of models to find the algo that best fit our problem ** "},{"metadata":{"_uuid":"6c3069a85900822d42baebb3d7bb9d10311049b9","trusted":true},"cell_type":"code","source":"#Importing the auxiliar and preprocessing librarys \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.metrics import accuracy_score\n\n#Models\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e2a6cd84-ef92-4f11-ae13-5e5e7e984d4e","_uuid":"d6ae353cac40213ee9b1c129550832de4e679d12","trusted":true},"cell_type":"code","source":"clfs = []\nseed = 3\n\nclfs.append((\"LogReg\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBClassifier())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsClassifier())]))) \n\nclfs.append((\"DecisionTreeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeClassifier())]))) \n\nclfs.append((\"RandomForestClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestClassifier(n_estimators=100))]))) \n\nclfs.append((\"GradientBoostingClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingClassifier(n_estimators=100))]))) \n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\nclfs.append((\"BaggingRidgeClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"BaggingClassifier\", BaggingClassifier())])))\n\nclfs.append((\"ExtraTreesClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreesClassifier())])))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'accuracy'\nn_folds = 7\n\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, \n                                 cv= 5, scoring=scoring,\n                                 n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20)\nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very cool! Based on the result of our CVLet's try model LogReg and XGBClassifier to predict who will survive or not"},{"metadata":{},"cell_type":"markdown","source":"# HyperOpt - Automated Bayeasian Hyperparameter serach. "},{"metadata":{},"cell_type":"markdown","source":"# HyperOpt with Random Forest"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import time\n\ndef objective(params):\n    time1 = time.time()\n    params = {\n        'max_depth': params['max_depth'],\n        'max_features': params['max_features'],\n        'n_estimators': params['n_estimators'],\n        'min_samples_split': params['min_samples_split'],\n        'criterion': params['criterion']\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 10\n    count=1\n\n    skf = StratifiedKFold(n_splits=FOLDS, random_state=42, shuffle=True)\n\n    kf = KFold(n_splits=FOLDS, shuffle=False, random_state=42)\n\n    score_mean = 0\n    for tr_idx, val_idx in kf.split(X_train, y_train):\n        clf = RandomForestClassifier(\n            random_state=4, \n            verbose=0,  n_jobs=-1, \n            **params\n        )\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr)\n        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n        #print(y_pred_train)\n        score = make_scorer(accuracy_score)(clf, X_vl, y_vl)\n        # plt.show()\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n    gc.collect()\n    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    return -(score_mean / FOLDS)\n\nrf_space = {\n    'max_depth': hp.choice('max_depth', range(2,8)),\n    'max_features': hp.choice('max_features', range(1,X_train.shape[1])),\n    'n_estimators': hp.choice('n_estimators', range(100,500)),\n    'min_samples_split': hp.choice('min_samples_split', range(5,35)),\n    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Running the HyperOpt to get the best params"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"best = fmin(fn=objective,\n            space=rf_space,\n            algo=tpe.suggest,\n            max_evals=40, \n            # trials=trials\n           )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best params "},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = space_eval(rf_space, best)\nbest_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting the X_test with Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(\n        **best_params, random_state=4,\n        )\n\nclf.fit(X_train, y_train)\n\ny_preds= clf.predict(X_test)\n\nsubmission['Survived'] = y_preds.astype(int)\nsubmission.to_csv('Titanic_rf_model_pred.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_______________________________________________\n# Predicting X_test with Logreg and HyperOpt"},{"metadata":{},"cell_type":"markdown","source":"### Calculating the class_weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                  np.unique(y_train),\n                                                  y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Objective LogReg"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective_logreg(params):\n    time1 = time.time()\n    params = {\n        'tol': params['tol'],\n        'C': params['C'],\n        'solver': params['solver'],\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 10\n    count=1\n\n    skf = StratifiedKFold(n_splits=FOLDS, random_state=42, shuffle=True)\n\n    kf = KFold(n_splits=FOLDS, shuffle=False, random_state=42)\n\n    score_mean = 0\n    for tr_idx, val_idx in kf.split(X_train, y_train):\n        clf = LogisticRegression(\n            random_state=4,  \n            **params\n        )\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr)\n        score = make_scorer(accuracy_score)(clf, X_vl, y_vl)\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n    gc.collect()\n    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    return -(score_mean / FOLDS)\n\nspace_logreg = {\n    'tol' : hp.uniform('tol', 0.00001, 0.001),\n    'C' : hp.uniform('C', 0.001, 2),\n    'solver' : hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Running LogReg HyperOpt"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"best = fmin(fn=objective_logreg,\n            space=space_logreg,\n            algo=tpe.suggest,\n            max_evals=45, \n            # trials=trials\n           )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = space_eval(space_logreg, best)\nbest_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(\n        **best_params, random_state=4,\n        )\n\nclf.fit(X_train, y_train)\n\ny_preds= clf.predict(X_test)\n\nsubmission['Survived'] = y_preds.astype(int)\nsubmission.to_csv('Titanic_logreg_model_pred.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b73fdd3c2dcc987e86725c0dfce527941f3c323c"},"cell_type":"markdown","source":"# Stay tuned and don't forget to votesup this kernel =)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}