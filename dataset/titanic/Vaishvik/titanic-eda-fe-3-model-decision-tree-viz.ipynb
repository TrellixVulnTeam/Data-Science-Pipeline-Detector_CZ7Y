{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><font size=\"10\">üö¢üèäüèª‚Äç‚ôÄÔ∏èTitanic Start Here: A GENTLE Introduction</font></center>\n<br>\n<center><font size=\"3\">Introdution</font></center>\n> In this Kernel we will see 3 approaches to the classification task in detail.\n> 1. [Import Data & Python Packages](#1-bullet) <br>\n> 2. [Missing Value Handling](#2-bullet)<br>\n>     * [2.1 Age - Missing Values](#2.1-bullet) <br>\n>     * [2.2 Embarked - Missing Values](#2.2-bullet) <br>\n>     * [2.3 Final Adjustments to Data](#2.3-bullet) <br>\n>     * [2.4 Additional Variables](#2.4-bullet) <br> \n> 3. [Exploratory Data Analysis](#3-bullet) <br>\n> 4. [Alternate Approach 1 :Logistic Regression](#4-bullet) <br>\n> 5. [Alternate Approach 2 : Random Forest Estimation](#5-bullet) <br>\n> 6. [Alternate Approach 3: Decision Tree](#6-bullet) <br>\n> 7. [Ensemble](#7-bullet)\n> 8. [TOP 1% Solution GA](#8-bullet)","metadata":{"_cell_guid":"ea25668d-9c1c-4897-89b8-61fcb47dfb2b","_uuid":"b65f81c1bdb84faefc3e77c8f621d4a6e2d326bb"}},{"cell_type":"markdown","source":"## 1. Import Data & Python Packages <a class=\"anchor\" id=\"1-bullet\"></a>","metadata":{"_cell_guid":"33c91cae-2ff8-45a6-b8cb-671619e9c933","_uuid":"0a395fd25f20834b070ef55cb8987c8c1f9b55f9"}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport sys\n\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nimport seaborn as sns\nsns.set(style=\"dark\") #white background style for seaborn plots\nsns.set(style=\"darkgrid\", color_codes=True)\nRED   = \"\\033[1;31m\"  \nBLUE  = \"\\033[1;34m\"\nCYAN  = \"\\033[1;36m\"\nGREEN = \"\\033[0;32m\"\n#sklearn imports source: https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8","metadata":{"_cell_guid":"de05512e-6991-44df-9599-da92a7e459ac","_uuid":"d8bdd5f0320e244e4702ed8ec1c2482b022c51cd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get titanic & test csv files as a DataFrame\n\n#developmental data (train)\ntitanic_df = pd.read_csv(\"../input/titanic/train.csv\")\n\n#cross validation data (hold-out testing)\ntest_df    = pd.read_csv(\"../input/titanic/test.csv\")\n\n# preview developmental data\ntitanic_df.head(5)\n","metadata":{"_cell_guid":"e0a17223-f682-45fc-89a5-667af9782bbe","_uuid":"7964157913fbcff581fc1929eed487708e81ac9c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head(5)","metadata":{"_cell_guid":"1d969b76-ea88-4d32-a58e-f22a070258bf","_uuid":"bff38fcf31baf67493513c06f0c2f6e50576ff09","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data Quality & Missing Value Assessment <a class=\"anchor\" id=\"2-bullet\"></a>","metadata":{"_cell_guid":"6578c0da-7bcf-433d-9f28-a66d8dfa6fa3","_uuid":"8660e63a62c2fcdb4f7633380166438caf5edae9"}},{"cell_type":"markdown","source":"### 2.1    Age - Missing Values <a class=\"anchor\" id=\"2.1-bullet\"></a>","metadata":{"_cell_guid":"7776faeb-6a8f-4460-a367-4b087d2cc089","_uuid":"696b428bd3ca49421f650665267ce7ca1b358814"}},{"cell_type":"code","source":"a=sum(pd.isnull(titanic_df['Age'])) # COUNT Missing Values in age\nb=round(a/(len(titanic_df[\"PassengerId\"])),4) # proportion of \"Age\" missing in percent\nsys.stdout.write(GREEN)\nprint(\"Count of missing Values : {} , The Proportion of this values with dataset is {}\\n\".format(a,b*100))\nsys.stdout.write(CYAN)\nprint(\"visualization AGE\")\nax = titanic_df[\"Age\"].hist(bins=15, color='#34495e', alpha=0.9)\nax.set(xlabel='Age', ylabel='Count')\nplt.show()","metadata":{"_cell_guid":"81c65784-d043-47f4-bd1f-18ed772d902b","_uuid":"233138d201b240f1e689a6cb9c63e527cb1cd929","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Since \"Age\" is (right) skewed, using the mean might give us biased results by filling in ages that are older than desired.  To deal with this, we'll use the median to impute the missing values. ","metadata":{"_cell_guid":"e62d6951-d968-43ba-aabf-add90524d042","_uuid":"24c201948b9c8c8076ab01271a4790d9db9096b5"}},{"cell_type":"code","source":"m1=titanic_df[\"Age\"].median(skipna=True)\nm2=titanic_df[\"Age\"].mean(skipna=True)\nsys.stdout.write(CYAN)\nprint(\"Median: {} and Mean: {} | Median age is 28 as compared to mean which is ~30\".format(m1,m2))","metadata":{"_cell_guid":"76fa4f61-cbd1-4638-8c36-b73430f4f40c","_uuid":"92c608b8f196e3eb7f75183a0d4ed61f3f914806","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Embarked - Missing Values <a class=\"anchor\" id=\"2.2-bullet\"></a>","metadata":{"_cell_guid":"0e696cff-ca80-4cb5-862c-ee80f4b1ab1f","_uuid":"d575319b1f528c7a153d8ab680282048cb163b14"}},{"cell_type":"code","source":"# proportion of \"Embarked\" missing\na=round(2/len(titanic_df[\"PassengerId\"]),4)\nsys.stdout.write(CYAN)\nprint('proportion of \"Embarked\" missing is {}'.format(a*100))","metadata":{"_cell_guid":"f21c2b55-2126-439d-8b1d-e96dafc97d81","_uuid":"92ab9e62fb62f2a0fb9972baf6ada444187540e6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.stdout.write(CYAN)\nprint('visualization Embarked')\nsns.countplot(x='Embarked',data=titanic_df,palette='Set1')\nplt.show()","metadata":{"_cell_guid":"7f3b91bc-f388-4fa3-b908-61c82292eab0","_uuid":"7561cc74de105488d349832545367d13d562b67d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Final Adjustments to Data (Train & Test) <a class=\"anchor\" id=\"2.3-bullet\"></a>\n\n> Based on my assessment of the missing values in the dataset, I'll make the following changes to the data:\n> * If \"Age\" is missing for a given row, I'll impute with 28 (median age).\n> * If \"Embark\" is missing for a riven row, I'll impute with \"S\" (the most common boarding port).\n> * I'll ignore \"Cabin\" as a variable.  There are too many missing values for imputation.  Based on the information available, it appears that this value is associated with the passenger's class and fare paid.","metadata":{"_cell_guid":"684c308f-25ae-4039-9332-ddb58953a054","_uuid":"3609e785d210d5a8110f7ce550e61007d066449b"}},{"cell_type":"code","source":"train_data = titanic_df\ntrain_data[\"Age\"].fillna(28, inplace=True)\ntrain_data[\"Embarked\"].fillna(\"S\", inplace=True)\ntrain_data.drop('Cabin', axis=1, inplace=True)","metadata":{"_cell_guid":"bc0d7121-1008-4890-9043-07eba1524e15","_uuid":"feeed4b6775f88edf5de12b0ee6ee73c16eba61d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Additional Variables <a class=\"anchor\" id=\"2.4-bullet\"></a>\n\n> According to the Kaggle data dictionary, both SibSp and Parch relate to traveling with family.  For simplicity's sake (and to account for possible multicollinearity), we will combine the effect of these variables into one categorical predictor: whether or not that individual was traveling alone.","metadata":{"_cell_guid":"6925fcc2-977b-4369-85e1-77a9210326a7","_uuid":"d8280757e6bc627821fb0540c87ccd6ca110f1e0"}},{"cell_type":"code","source":"## Create categorical variable for traveling alone\ntrain_data['TravelBuds']=train_data[\"SibSp\"]+train_data[\"Parch\"]\ntrain_data['TravelAlone']=np.where(train_data['TravelBuds']>0, 0, 1)\n\ntrain_data.drop('SibSp', axis=1, inplace=True)\ntrain_data.drop('Parch', axis=1, inplace=True)\ntrain_data.drop('TravelBuds', axis=1, inplace=True)\n\n#create categorical variable for Pclass || ONE HOT ENCODING\ntrain2 = pd.get_dummies(train_data, columns=[\"Pclass\"])\n\ntrain3 = pd.get_dummies(train2, columns=[\"Embarked\"])\n\ntrain4=pd.get_dummies(train3, columns=[\"Sex\"])\ntrain4.drop('Sex_female', axis=1, inplace=True)\n\n#Drop Unwanted\ntrain4.drop('PassengerId', axis=1, inplace=True)\ntrain4.drop('Name', axis=1, inplace=True)\ntrain4.drop('Ticket', axis=1, inplace=True)\ntrain4.head(5)\ndf_final = train4","metadata":{"_cell_guid":"759c3c8e-8db6-41d9-a1a2-058a15b338a6","_uuid":"d1f5815ba663f7e8cc17d7efcff73653af5b1bdb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply the same changes to the test data. <br>\n","metadata":{"_cell_guid":"768cf074-6ecb-47eb-9b42-8b9079ffb811","_uuid":"6a8e533e77c7f1f1a68d136119f93972447a31cf"}},{"cell_type":"code","source":"test_df[\"Age\"].fillna(28, inplace=True)\ntest_df[\"Fare\"].fillna(14.45, inplace=True)\ntest_df.drop('Cabin', axis=1, inplace=True)","metadata":{"_cell_guid":"8b9ef076-3669-4339-8d10-0d8783a92e07","_uuid":"145675b90aa2befa533c640aaedd4bf8069b12d4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['TravelBuds']=test_df[\"SibSp\"]+test_df[\"Parch\"]\ntest_df['TravelAlone']=np.where(test_df['TravelBuds']>0, 0, 1)\n\ntest_df.drop('SibSp', axis=1, inplace=True)\ntest_df.drop('Parch', axis=1, inplace=True)\ntest_df.drop('TravelBuds', axis=1, inplace=True)\n\ntest2 = pd.get_dummies(test_df, columns=[\"Pclass\"])\ntest3 = pd.get_dummies(test2, columns=[\"Embarked\"])\n\ntest4=pd.get_dummies(test3, columns=[\"Sex\"])\ntest4.drop('Sex_female', axis=1, inplace=True)\n\ntest4.drop('PassengerId', axis=1, inplace=True)\ntest4.drop('Name', axis=1, inplace=True)\ntest4.drop('Ticket', axis=1, inplace=True)\nfinal_test = test4","metadata":{"_cell_guid":"f848ce97-6644-485d-b6e9-ed18f79b34ce","_uuid":"746e1803237da93829db0c6bf926a83b7dada86a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_test.head(5)","metadata":{"_cell_guid":"760a79a0-4fa2-4f20-8745-b38b8015f919","_uuid":"bb3cc11f5e5b138f31a58fd7e242a43f73e14f0f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Exploratory Data Analysis <a class=\"anchor\" id=\"3-bullet\"></a>","metadata":{"_cell_guid":"1430d510-1c8d-4544-8009-3911fff7afbb","_uuid":"4e26c19bf719b7086addc0e1981c00836a19f189"}},{"cell_type":"markdown","source":"## 3.1 Exploration of Age <a class=\"anchor\" id=\"3.1-bullet\"></a>","metadata":{"_cell_guid":"2655428b-d69d-4c0f-85ff-e31ada8e37b9","_uuid":"32e9c04a3281fb1aa8c77e1406c56cd820459202"}},{"cell_type":"code","source":"sys.stdout.write(GREEN)\nprint(\"Density Plot of Age for Surviving Population and Deceased Population\")\nplt.figure(figsize=(15,8))\nsns.kdeplot(titanic_df[\"Age\"][df_final.Survived == 1], color=\"darkturquoise\", shade=True)\nsns.kdeplot(titanic_df[\"Age\"][df_final.Survived == 0], color=\"lightcoral\", shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Age for Surviving Population and Deceased Population')\nplt.show()\n","metadata":{"_cell_guid":"9f9ca9e5-50a0-4487-ba53-815dda90af1c","_uuid":"790e8d7ca89d19e276b3398e299c42893a796b79","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The age distribution for survivors and deceased is actually very similar.  One notable difference is that, of the survivors, a larger proportion were children.  The passengers evidently made an attempt to save children by giving them a place on the life rafts. ","metadata":{"_cell_guid":"8e304d72-27f3-41cf-863f-63872f4c37df","_uuid":"6c5625b454f5e01dd6b6d843d801851c14c64d1e"}},{"cell_type":"code","source":"plt.figure(figsize=(25,8))\navg_survival_byage = df_final[[\"Age\", \"Survived\"]].groupby(['Age'],as_index=False).mean()\ng = sns.barplot(x='Age', y='Survived', data=avg_survival_byage, color=\"LightSeaGreen\")\n","metadata":{"_cell_guid":"d2aa9f59-c433-4258-b8db-225b63a5eab6","_uuid":"9cf1794d9db2fdc314c20ca97a76e9470e81a354","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Considering the survival rate of passengers under 16, I'll also include another categorical variable in my dataset: \"Minor\"","metadata":{"_cell_guid":"0b636440-ab38-46a8-8cc9-9421683d5c0b","_uuid":"67051cf653243b3103c9f8015c501d89d92bd3bc"}},{"cell_type":"code","source":"df_final['IsMinor']=np.where(train_data['Age']<=16, 1, 0)","metadata":{"_cell_guid":"1655b49b-b33f-4236-8b31-d995ef26c6f6","_uuid":"8918defa6e17b83c700ea45357ebd67a3a22f02f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_test['IsMinor']=np.where(final_test['Age']<=16, 1, 0)","metadata":{"_cell_guid":"45a48287-86eb-4853-be6a-e5be295ce9ed","_uuid":"8773bbf9277fd1baec3e4becd11d7b5bf32c2463","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Exploration of Fare <a class=\"anchor\" id=\"3.2-bullet\"></a>","metadata":{"_cell_guid":"a643b196-91c6-4b12-9463-0f984fbfc91a","_uuid":"337b3ced0c6423cf1d126f23a7e60c0181af6a47"}},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.kdeplot(df_final[\"Fare\"][titanic_df.Survived == 1], color=\"#e74c3c\", shade=True)\nsns.kdeplot(df_final[\"Fare\"][titanic_df.Survived == 0], color=\"#3498db\", shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Fare for Surviving Population and Deceased Population')\n# limit x axis to zoom on most information. there are a few outliers in fare. \nplt.xlim(-20,200)\nplt.show()","metadata":{"_cell_guid":"9f31ffe1-7cd8-4169-b193-ed44e56d0bd4","_uuid":"4a1c521f08460f6983eca0c4e01294fb7c86e4f9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Exploration of Passenger Class <a class=\"anchor\" id=\"3.3-bullet\"></a>","metadata":{"_cell_guid":"cf585311-4029-4be4-8af2-3eea8258801a","_uuid":"4524affda51265ea23fa923e2ea7f93d7bb91875"}},{"cell_type":"code","source":"sns.barplot('Pclass', 'Survived', data=titanic_df, color=\"#2ecc71\")\nplt.show()","metadata":{"_cell_guid":"676548e8-6dd4-4180-800c-7b164acb3877","_uuid":"08fd677214959e0b938a0f8a94b63ab548673ea5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Unsurprisingly, being a first class passenger was safest.","metadata":{"_cell_guid":"193233f8-b220-4cae-aa0f-f822316d5623","_uuid":"8ddb19191253a6e09dfcb0beff2b3690f1052d52"}},{"cell_type":"markdown","source":"## 3.4 Exploration of Embarked Port <a class=\"anchor\" id=\"3.4-bullet\"></a>","metadata":{"_cell_guid":"c59f8e8f-e8c2-40fb-b9c8-12dddd6d318f","_uuid":"2fc06b75321946b721852f78431435f9ba5fef39"}},{"cell_type":"code","source":"sns.barplot('Embarked', 'Survived', data=titanic_df, color=\"#2ecc71\")\nplt.show()","metadata":{"_cell_guid":"6e5bec50-2f5e-433e-9130-c56956fddad3","_uuid":"a9f0598701c7c5224eaa73dafa869af73beffe18","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.5 Exploration of Traveling Alone vs. With Family <a class=\"anchor\" id=\"3.5-bullet\"></a>","metadata":{"_cell_guid":"9e6dc87e-ba59-4004-8145-79709328fe27","_uuid":"92bacce85a7dec5509217b9570bc2a2fea6a8452"}},{"cell_type":"code","source":"sns.barplot('TravelAlone', 'Survived', data=df_final, color=\"#2ecc71\")\nplt.show()","metadata":{"_cell_guid":"67017a88-93d4-412b-9adf-8b4d1d9b9db0","_uuid":"e0c3dc16292ef0bcabf0fc680d821ef654084ab4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Individuals traveling without family were more likely to die in the disaster than those with family aboard.  Given the era, it's likely that individuals traveling alone were likely male.","metadata":{"_cell_guid":"e9e68cef-5e74-46aa-8343-39afbbf00efe","_uuid":"f160bd7399e024ae669d55f09caf6e7902768851"}},{"cell_type":"markdown","source":"## 4. Logistic Regression <a class=\"anchor\" id=\"4-bullet\"></a>","metadata":{"_cell_guid":"c833cbf5-74db-44ff-90fa-b600ff0a09d7","_uuid":"39dbc095f99dcec6d25a7a4561e81bb641078622"}},{"cell_type":"code","source":"cols=[\"Age\", \"Fare\", \"TravelAlone\", \"Pclass_1\", \"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"] \nX=df_final[cols]\nY=df_final['Survived']","metadata":{"_cell_guid":"fdc40c40-81fb-4bab-bbb3-562d5416dc57","_uuid":"69ff389cd4ae7702c907ba252080ac81350ca477","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statsmodels.api as sm\nfrom scipy import stats\nstats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\nlogit_model=sm.Logit(Y,X)\nresult=logit_model.fit()\nsys.stdout.write(GREEN)\nprint(result.summary())","metadata":{"_cell_guid":"9f2a9b35-d553-4321-8e64-4d8a21cba48d","_uuid":"81fbdbb236fba1be540ecb6903505a2b6457f855","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Nearly all variables are significant at the 0.05 alpha level, but we'll run the model again without Fare and TravelAlone (removed one at a time, results didn't change much.  In the end removed both).  I also removed \"IsMinor\" from this regression, as the information provided is redundant to the Age variable.","metadata":{"_cell_guid":"b1b3b56f-2f5f-47d6-9375-62c11e49ce79","_uuid":"e9d52d5b182c0a01218982e844e53d5278e0d98a"}},{"cell_type":"code","source":"cols2=[\"Age\", \"Pclass_1\", \"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\"]  \nX2=df_final[cols2]\nY=df_final['Survived']\n\nlogit_model=sm.Logit(Y,X2)\nresult=logit_model.fit()\nsys.stdout.write(GREEN)\nprint(result.summary())","metadata":{"_cell_guid":"febd185a-bc4b-4210-b1f6-a10954bbdfaa","_uuid":"9d87357aa13931e9f0a0379a480d262b198a227f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X2, Y)\n\nprint(\"Model Accuracy : {:.2f}%\".format(logreg.score(X2, Y)*100))","metadata":{"_cell_guid":"9cef2c68-0004-4581-9474-391318843cec","_uuid":"e0b25290f527ae75c125fe07d175504ae762bf84","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 Using 75-25 Split for Cross Validation <a class=\"anchor\" id=\"4.3-bullet\"></a>","metadata":{"_cell_guid":"fec69951-fd95-45d8-ba9c-8735721831b1","_uuid":"907d61b84f6f76ef3f8efb9a62f68a03d5de04b8"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df_final, test_size=0.25)","metadata":{"_cell_guid":"8cb5c46c-41c4-4748-971c-f820bde62e55","_uuid":"7ee14c32600b2a42f365d671f146d30ebe474452","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#re-fit logistic regression on new train sample\n\ncols2=[\"Age\", \"Pclass_1\", \"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\"] \nX3=train[cols2]\nY3=train['Survived']\nlogit_model3=sm.Logit(Y3,X3)","metadata":{"_cell_guid":"677e898c-6efb-43cd-a31c-12e219cae53b","_uuid":"0f05e45e32cb8e81cd14db4e33bd532d011f8a75","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlogreg = LogisticRegression()\nlogreg.fit(X3, Y3)\nsys.stdout.write(GREEN)\nprint(\"Model Accuracy : {:.2f}%\".format(logreg.score(X3, Y3)*100))","metadata":{"_cell_guid":"9010d0f8-a9a7-4174-984c-ee948581bab8","_uuid":"14f0d5843c0e7d9685538436ae9a65745cdc41cf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The score for the new training sample (75% of original) is very close to the original performance, which is good!<br>\n> Let's assess how well it scores on the 25% hold-out sample.","metadata":{"_cell_guid":"85a23b6a-7506-4c0b-9e88-c782543672f3","_uuid":"e2d06f6339c48f50f4cd42cbe8186597b8c5b59e"}},{"cell_type":"code","source":"from sklearn import metrics\nlogreg.fit(X3, Y3)\n\nX3_test = test[cols2]\nY3_test = test['Survived']\n\nY3test_pred = logreg.predict(X3_test)\nsys.stdout.write(GREEN)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X3_test, Y3_test)*100))","metadata":{"_cell_guid":"f9246036-0c71-4875-b5fb-2e6db5b17076","_uuid":"837ef204f480adcebad0ccfdc5508bcad82ca3bd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.4 Out-of-sample Assessment <br> <a class=\"anchor\" id=\"4.4-bullet\"></a>","metadata":{"_cell_guid":"cd1b53cf-5e59-4a86-b034-4a205b949915","_uuid":"9d57dfdb1b26ab2447ad391d70af8ef91e573e46"}},{"cell_type":"markdown","source":"### Assessing the model's performance based on Cross Validation ROC/AUC ","metadata":{"_cell_guid":"f485ca4c-2172-4383-979e-21e78a66192c","_uuid":"ec44fbaddbc23f03a8ac470391f41ce35f40cf72"}},{"cell_type":"code","source":"# Model's in sample AUC\n\nfrom sklearn.metrics import roc_auc_score\nlogreg.fit(X3, Y3)\nY3_pred = logreg.predict(X3)\n\ny_true = Y3\ny_scores = Y3_pred\nsys.stdout.write(GREEN)\nprint(\"Model ROC_AUC : {:.2f}%\".format(roc_auc_score(y_true, y_scores)))","metadata":{"_cell_guid":"5404f653-701a-48a6-a0e0-624a40149d74","_uuid":"354d8663b9219459737c651e57a968bcd97e7843","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing the model's ROC curve (**source for graph code given below the plot)\nfrom sklearn.metrics import roc_curve, auc\nlogreg.fit(X3, Y3)\n\ny_test = Y3_test\nX_test = X3_test\n \n# Determine the false positive and true positive rates\nFPR, TPR, _ = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n \n# Calculate the AUC\nsys.stdout.write(GREEN)\nroc_auc = auc(FPR, TPR)\nprint ('ROC AUC: %0.3f' % roc_auc )\n \n# Plot of a ROC curve\nplt.figure(figsize=(10,10))\nplt.plot(FPR, TPR, label='ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (Test Sample Performance)')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"_cell_guid":"85546d4a-b9ed-4427-b565-445fe9219ab5","_uuid":"a9172d2bffa265ea5fadfbf548f70d7e13f84e8b","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> An AUC score of 0.5 is effectively as good as the flip of a coin, and means that the model really has no classification power at all between the positive and negative occurences. The AUC for both the test and train samples when run on my logistic regression demonstrates relatively strong power of separation between positive and negative occurences (survived - 1, died - 0).","metadata":{"_cell_guid":"68b656b0-66f2-4418-9013-f243dc386bfc","_uuid":"fb2a0c6234fb538fa0be9c090a58b4a277a7ce5a"}},{"cell_type":"markdown","source":"# 4.5 Logistic Regression Conclusion<br> <a class=\"anchor\" id=\"4.5-bullet\"></a>\n> Based on my analysis, if you were to be aboard the Titanic, your chances of survival were best if you fit the following criteria:<br>\n> * Female\n> * Young\n> * In First Class \n> * Embarked in Cherbourg France\n","metadata":{"_cell_guid":"f8225667-0fe0-4641-98a5-fbba2eaa3d6f","_uuid":"f29821f4daae995de78bfa338688fba748a9980c"}},{"cell_type":"markdown","source":"## 5. Random Forest Estimation <a class=\"anchor\" id=\"5-bullet\"></a>","metadata":{"_cell_guid":"82d95e6d-a700-4c77-bf29-04f9d58777ea","_uuid":"740bc96266c96981ada150cc9d50a4056267aca1"}},{"cell_type":"markdown","source":"> Our Logistic Regression is effective and easy to interpret, but there are other ML techniques which could provide a more accurate prediction.  Random forests, a tree-based machine learning technique, often provide more accurate results than Logistic Regression classifier models.  With respect to tree growth, performance tends to taper off after a certain number of trees are grown. <br> <br>\n> A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. ","metadata":{"_cell_guid":"ec2960f5-cf81-4668-a79c-5e9c167a8be0","_uuid":"44338a78cc26f5fa823ab1f29e04eae415e66d52"}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\ncols=[\"Age\", \"Fare\", \"TravelAlone\", \"Pclass_1\", \"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"] \nX=df_final[cols]\nY=df_final['Survived']\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X, Y)\nsys.stdout.write(GREEN)\nprint ('ROC AUC: %0.3f' % random_forest.score(X, Y) )","metadata":{"_cell_guid":"f3cc0e9e-b508-4fb2-8159-af6541137bfe","_uuid":"f25ecf956a5b9447f743c8901b9359299fea1ed8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final RF Submission","metadata":{"_cell_guid":"57d1a119-6949-4724-817a-6578ff9080b2","_uuid":"0ca07fa68c901ead873cb17b91a11ee409e624b9"}},{"cell_type":"code","source":"final_test_RF=final_test[cols]\nY_pred_RF = random_forest.predict(final_test_RF)","metadata":{"_cell_guid":"73e9ad95-6696-475b-a2eb-27650f8e58e0","_uuid":"d3686a38c1e0859fe53bc070d602151a75a9890c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred_RF\n    })\nsubmission.to_csv('titanic_RF.csv', index=False)","metadata":{"_cell_guid":"419771ee-93ed-4dfa-b1b8-2f18db7a16d9","_kg_hide-output":true,"_uuid":"f92595b431f087b36d88401424663a6f78a70dba","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Decision Tree <a class=\"anchor\" id=\"6-bullet\"></a>","metadata":{"_cell_guid":"35ccbe8e-4391-4004-b938-519c58ba0e07","_uuid":"9cc581f268a52a688d7b78176f5542a48139d981"}},{"cell_type":"markdown","source":"> Let's try another method- a decision tree.  There is a tradeoff for the additional complexity of utilizing a decision tree as compared to a logistic regression: growing your number of trees too much can subject your model to overfitting and reduce the predictive power of the model.  I've set parameters within the DecisionTreeClassifier from sklearn to help make sure my model is not overfit (too many branches based on the train data).  Some trial and error went into this to determine the optimal number of branches to \"prune\" to achieve strong out-of-sample results.<br><br>\n> *Note*: Again, I used the same variables for the decision tree as I did in my first logistic regression and in my random forest.","metadata":{"_cell_guid":"609a1cc9-4f88-4690-aa44-58922e84231a","_uuid":"692b2cdfc66aae2f960b3f4f96d59bba7317f751"}},{"cell_type":"code","source":"from sklearn import tree\nimport graphviz\ntree1 = tree.DecisionTreeClassifier(criterion='gini', splitter='best',max_depth=3, min_samples_leaf=20)","metadata":{"_cell_guid":"19657193-e509-48a9-be7f-ab2227df9634","_uuid":"0b0aa153618008898e516cadba8de78fd1a546b1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols=[\"Age\", \"Fare\", \"TravelAlone\", \"Pclass_1\", \"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"] \nX_DT=df_final[cols]\nY_DT=df_final['Survived']\n\ntree1.fit(X_DT, Y_DT)","metadata":{"_cell_guid":"f1ecec9c-ec77-449c-8120-35e9fbef83db","_uuid":"0be7d6737b3c5207389c6e9b947a0df3b4dbd34b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import graphviz \ntree1_view = tree.export_graphviz(tree1, out_file=None, feature_names = X_DT.columns.values, rotate=True) \ntree1viz = graphviz.Source(tree1_view)\ntree1viz","metadata":{"_cell_guid":"fbaba624-42ca-49db-97a1-a3f4d3442842","_uuid":"39c5670f722187de9de0a188d9ab6abfdb2a1726","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_test_DT=final_test[cols]\nY_pred_DT = tree1.predict(final_test_DT)\n\nsubmission = pd.DataFrame({\n       \"PassengerId\": test_df[\"PassengerId\"],\n       \"Survived\": Y_pred_DT\n   })\nsubmission.to_csv('titanic_DT.csv', index=False)","metadata":{"_cell_guid":"043246c0-0eab-4292-9eab-5865bbddb607","_uuid":"a5ba38bfb4b537427b95be3e206093f3da32c46d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Ensemble <a class=\"anchor\" id=\"7-bullet\"></a>\n\n> Ensemble Learning is a process using which multiple machine learning models (such as classifiers) are strategically constructed to solve a particular problem.\n> \n> Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred_RF* 0.8 + Y_pred_DT*0.2 #ensmeble of two models\n    })\nsubmission.to_csv('titanic_ensemble.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. TOP 1% Solution Genetic Algorithm <a class=\"anchor\" id=\"8-bullet\"></a>","metadata":{}},{"cell_type":"code","source":"from sympy import simplify, cos, sin, Symbol, Function, tanh, pprint, init_printing, exp\nfrom sympy.functions import Min,Max","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the winner variables with former values after the hashtag\nA = 0.058823499828577    \nB = 0.841127 # 0.885868\nC = 0.138462007045746 \nD = 0.31830988618379069\nE = 2.810815 # 2.675679922103882 \nF = 0.63661977236758138\nG = 5.428569793701172   \nH = 3.1415926535897931\nI = 0.592158 #0.623655974864960\nJ = 4.869778 #  2.770736 # 2.212120056152344\nK = 0.063467 # 1.5707963267948966\nL = -0.091481 # 0.094339601695538 \nM = 0.0821533 \nN = 0.720430016517639\nO = 0.230145 \nP = 9.89287 \nQ = 785 \nR = 1.07241 \nS = 281\nT = 734\nU = 5.3\nV = 67.0\nW = 2.484848\nX = 8.48635 \nY = 63\nZ = 12.6275 \nAA = 0.735354 # 0.7\nAB = 727\nAC = 2.5\nAD = 2.6 \nAE = 0.3\nAF = 3.0\nAG = 0.226263 #0.1\nAH = 2.0\nAI = 12.4148\nAJ = 96\nAK = 0.130303 # 0.2\nAL = 176\nAM = 3.2\nBIG = [A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,AA,AB,AC,AD,AE,AF,AG,AH,AI,AJ,AK,AL,AM]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now may I present: The winning gen function, Inspired by Akshat's notebook:\n# https://www.kaggle.com/akshat113/titanic-dataset-analysis-level-2\ndef GeneticFunction(data,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,AA,AB,AC,AD,AE,AF,AG,AH,AI,AJ,AK,AL,AM):\n    return ((np.minimum( ((((A + data[\"Sex\"]) - np.cos((data[\"Pclass\"] / AH))) * AH)),  ((B))) * AH) +\n            np.maximum( ((data[\"SibSp\"] - AC)),  ( -(np.minimum( (data[\"Sex\"]),  (np.sin(data[\"Parch\"]))) * data[\"Pclass\"]))) +\n            (AG * ((np.minimum( (data[\"Sex\"]),  (((data[\"Parch\"] / AH) / AH))) * data[\"Age\"]) - data[\"Cabin\"])) +\n            np.minimum( ((np.sin((data[\"Parch\"] * ((data[\"Fare\"] - AA) * AH))) * AH)),  ((data[\"SibSp\"] / AH))) +\n            np.maximum( (np.minimum( ( -np.cos(data[\"Embarked\"])),  (C))),  (np.sin(((data[\"Cabin\"] - data[\"Fare\"]) * AH)))) +\n            -np.minimum( ((((data[\"Age\"] * data[\"Parch\"]) * data[\"Embarked\"]) + data[\"Parch\"])),  (np.sin(data[\"Pclass\"]))) +\n            np.minimum( (data[\"Sex\"]),  ((np.sin( -(data[\"Fare\"] * np.cos((data[\"Fare\"] * W)))) / AH))) +\n            np.minimum( ((O)),  (np.sin(np.minimum( (((V / AH) * np.sin(data[\"Fare\"]))),  (D))))) +\n            np.sin((np.sin(data[\"Cabin\"]) * (np.sin((Z)) * np.maximum( (data[\"Age\"]),  (data[\"Fare\"]))))) +\n            np.sin(((np.minimum( (data[\"Fare\"]),  ((data[\"Cabin\"] * data[\"Embarked\"]))) / AH) *  -data[\"Fare\"])) +\n            np.minimum( (((AD * data[\"SibSp\"]) * np.sin(((AJ) * np.sin(data[\"Cabin\"]))))),  (data[\"Parch\"])) +\n            np.sin(np.sin((np.maximum( (np.minimum( (data[\"Age\"]),  (data[\"Cabin\"]))),  ((data[\"Fare\"] * AK))) * data[\"Cabin\"]))) +\n            np.maximum( (np.sin(((AI) * (data[\"Age\"] / AH)))),  (np.sin((-AF * data[\"Cabin\"])))) +\n            (np.minimum( (np.sin((((np.sin(((data[\"Fare\"] * AH) * AH)) * AH) * AH) * AH))),  (data[\"SibSp\"])) / AH) +\n            ((data[\"Sex\"] - data[\"SibSp\"]) * (np.cos(((data[\"Embarked\"] - AA) + data[\"Age\"])) / AH)) +\n            ((np.sin(data[\"Cabin\"]) / AH) - (np.cos(np.minimum( (data[\"Age\"]),  (data[\"Embarked\"]))) * np.sin(data[\"Embarked\"]))) +\n            np.minimum( (AE),  ((data[\"Sex\"] * (J * (N - np.sin((data[\"Age\"] * AH))))))) +\n            (np.minimum( (np.cos(data[\"Fare\"])),  (np.maximum( (np.sin(data[\"Age\"])),  (data[\"Parch\"])))) * np.cos((data[\"Fare\"] / AH))) +\n            np.sin((data[\"Parch\"] * np.minimum( ((data[\"Age\"] - K)),  ((np.cos((data[\"Pclass\"] * AH)) / AH))))) +\n            (data[\"Parch\"] * (np.sin(((data[\"Fare\"] * (I * data[\"Age\"])) * AH)) / AH)) +\n            (D * np.cos(np.maximum( ((0.5 * data[\"Fare\"])),  ((np.sin(N) * data[\"Age\"]))))) +\n            (np.minimum( ((data[\"SibSp\"] / AH)),  (np.sin(((data[\"Pclass\"] - data[\"Fare\"]) * data[\"SibSp\"])))) * data[\"SibSp\"]) +\n            np.tanh((data[\"Sex\"] * np.sin((U * np.sin((data[\"Cabin\"] * np.cos(data[\"Fare\"]))))))) +\n            (np.minimum( (data[\"Parch\"]),  (data[\"Sex\"])) * np.cos(np.maximum( ((np.cos(data[\"Parch\"]) + data[\"Age\"])),  (AM)))) +\n            (np.minimum( (np.tanh(((data[\"Cabin\"] / AH) + data[\"Parch\"]))),  ((data[\"Sex\"] + np.cos(data[\"Age\"])))) / AH) +\n            (np.sin((np.sin(data[\"Sex\"]) * (np.sin((data[\"Age\"] * data[\"Pclass\"])) * data[\"Pclass\"]))) / AH) +\n            (data[\"Sex\"] * (np.cos(((data[\"Sex\"] + data[\"Fare\"]) * ((X) * (Y)))) / AH)) +\n            np.minimum( (data[\"Sex\"]),  ((np.cos((data[\"Age\"] * np.tanh(np.sin(np.cos(data[\"Fare\"]))))) / AH))) +\n            (np.tanh(np.tanh( -np.cos((np.maximum( (np.cos(data[\"Fare\"])),  (L)) * data[\"Age\"])))) / AH) +\n            (np.tanh(np.cos((np.cos(data[\"Age\"]) + (data[\"Age\"] + np.minimum( (data[\"Fare\"]),  (data[\"Age\"])))))) / AH) +\n            (np.tanh(np.cos((data[\"Age\"] * ((-AH + np.sin(data[\"SibSp\"])) + data[\"Fare\"])))) / AH) +\n            (np.minimum( (((S) - data[\"Fare\"])),  (np.sin((np.maximum( ((AL)),  (data[\"Fare\"])) * data[\"SibSp\"])))) * AH) +\n            np.sin(((np.maximum( (data[\"Embarked\"]),  (data[\"Age\"])) * AH) * (((Q) * H) * data[\"Age\"]))) +\n            np.minimum( (data[\"Sex\"]),  (np.sin( -(np.minimum( ((data[\"Cabin\"] / AH)),  (data[\"SibSp\"])) * (data[\"Fare\"] / AH))))) +\n            np.sin(np.sin((data[\"Cabin\"] * (data[\"Embarked\"] + (np.tanh( -data[\"Age\"]) + data[\"Fare\"]))))) +\n            (np.cos(np.cos(data[\"Fare\"])) * (np.sin((data[\"Embarked\"] - ((T) * data[\"Fare\"]))) / AH)) +\n            ((np.minimum( (data[\"SibSp\"]),  (np.cos(data[\"Fare\"]))) * np.cos(data[\"SibSp\"])) * np.sin((data[\"Age\"] / AH))) +\n            (np.sin((np.sin((data[\"SibSp\"] * np.cos((data[\"Fare\"] * AH)))) + (data[\"Cabin\"] * AH))) / AH) +\n            (((data[\"Sex\"] * data[\"SibSp\"]) * np.sin(np.sin( -(data[\"Fare\"] * data[\"Cabin\"])))) * AH) +\n            (np.sin((data[\"SibSp\"] * ((((G + V) * AH) / AH) * data[\"Age\"]))) / AH) +\n            (data[\"Pclass\"] * (np.sin(((data[\"Embarked\"] * data[\"Cabin\"]) * (data[\"Age\"] - (R)))) / AH)) +\n            (np.cos((((( -data[\"SibSp\"] + data[\"Age\"]) + data[\"Parch\"]) * data[\"Embarked\"]) / AH)) / AH) +\n            (D * np.sin(((data[\"Age\"] * ((data[\"Embarked\"] * np.sin(data[\"Fare\"])) * AH)) * AH))) +\n            ((np.minimum( ((data[\"Age\"] * A)),  (data[\"Sex\"])) - F) * np.tanh(np.sin(data[\"Pclass\"]))) +\n            -np.minimum( ((np.cos(((AB) * ((data[\"Fare\"] + data[\"Parch\"]) * AH))) / AH)),  (data[\"Fare\"])) +\n            (np.minimum( (np.cos(data[\"Fare\"])),  (data[\"SibSp\"])) * np.minimum( (np.sin(data[\"Parch\"])),  (np.cos((data[\"Embarked\"] * AH))))) +\n            (np.minimum( (((data[\"Fare\"] / AH) - E)),  (C)) * np.sin((K * data[\"Age\"]))) +\n            np.minimum( ((M)),  (((np.sin(data[\"Fare\"]) + data[\"Embarked\"]) - np.cos((data[\"Age\"] * (P)))))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CleanData(data):\n    # Sex\n    data.drop(['Ticket', 'Name'], inplace=True, axis=1)\n    data.Sex.fillna('0', inplace=True)\n    data.loc[data.Sex != 'male', 'Sex'] = 0\n    data.loc[data.Sex == 'male', 'Sex'] = 1\n    # Cabin\n    data.Cabin.fillna('0', inplace=True)\n    data.loc[data.Cabin.str[0] == 'A', 'Cabin'] = 1\n    data.loc[data.Cabin.str[0] == 'B', 'Cabin'] = 2\n    data.loc[data.Cabin.str[0] == 'C', 'Cabin'] = 3\n    data.loc[data.Cabin.str[0] == 'D', 'Cabin'] = 4\n    data.loc[data.Cabin.str[0] == 'E', 'Cabin'] = 5\n    data.loc[data.Cabin.str[0] == 'F', 'Cabin'] = 6\n    data.loc[data.Cabin.str[0] == 'G', 'Cabin'] = 7\n    data.loc[data.Cabin.str[0] == 'T', 'Cabin'] = 8\n    # Embarked\n    data.loc[data.Embarked == 'C', 'Embarked'] = 1\n    data.loc[data.Embarked == 'Q', 'Embarked'] = 2\n    data.loc[data.Embarked == 'S', 'Embarked'] = 3\n    data.Embarked.fillna(0, inplace=True)\n    data.fillna(-1, inplace=True)\n    return data.astype(float)\n\n# This function rounds values to either 1 or 0, because the GeneticFunction below returns floats and no\n# definite values\ndef Outputs(data):\n    return np.round(1.-(1./(1.+np.exp(-data))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_train = pd.read_csv('../input/titanic/train.csv')\nraw_test = pd.read_csv('../input/titanic/test.csv')\n\ncleanedTrain = CleanData(raw_train)\ncleanedTest = CleanData(raw_test)\n# run a check on the Training dataset. See section \"Programm your own gen. algorithm\" below on how to \n# construct your own genetic algorithm\nthisArray = BIG.copy()\ntestPredictions = Outputs(GeneticFunction(cleanedTrain,thisArray[0],thisArray[1],thisArray[2],thisArray[3],thisArray[4],thisArray[5],thisArray[6],thisArray[7],thisArray[8],thisArray[9],thisArray[10],thisArray[11],thisArray[12],thisArray[13],thisArray[14],thisArray[15],thisArray[16],thisArray[17],thisArray[18],thisArray[19],thisArray[20],thisArray[21],thisArray[22],thisArray[23],thisArray[24],thisArray[25],thisArray[26],thisArray[27],thisArray[28],thisArray[29],thisArray[30],thisArray[31],thisArray[32],thisArray[33],thisArray[34],thisArray[35],thisArray[36],thisArray[37],thisArray[38]))\npdcheck = pd.DataFrame({'Survived': testPredictions.astype(int)})\nret = pdcheck.Survived.where(pdcheck[\"Survived\"].values==cleanedTrain[\"Survived\"].values).notna()\nt,f = ret.value_counts()\nscore = 100/(t+f)*t\nprint(\"Training set score: \",score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testPredictions = Outputs(GeneticFunction(cleanedTest,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,AA,AB,AC,AD,AE,AF,AG,AH,AI,AJ,AK,AL,AM))\npdtest = pd.DataFrame({'PassengerId': cleanedTest.PassengerId.astype(int),\n                        'Survived': testPredictions.astype(int)})\npdtest.to_csv('submission_GA.csv', index=False)\npdtest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}