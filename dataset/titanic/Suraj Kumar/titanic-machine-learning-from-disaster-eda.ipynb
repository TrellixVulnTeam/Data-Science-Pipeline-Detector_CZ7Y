{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><h1 style=\"color:blue\">Don't forget to Upvote if you like it.ðŸ˜Š</h1></center>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Introduction**\n\n\nIn this kernal we will going through the whole process of creating a Machine Learning on the Titanic dataset. It provides us a glance over the fate of the passenger onboard the \"Unsinkable\" ship which sinked. The dataset categorizes the passanger based on their economic status, sex, age and their survival. In this kernel, we will be analyzing, cleaning and visulizing the data in different forms to obtain hidden insights. Also, we'll create different ML models and depending upon their accuracies, use the most suitable model for the prediction.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **RMS Titanic**\n\nRMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters. RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. She was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, chief naval architect of the shipyard at the time, died in the disaster.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![Titanic](https://media.nationalgeographic.org/assets/photos/000/273/27302.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing Dataset","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#data processing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\n\n#algorithms\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\n\n#dataframe display settings\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotly Libraris\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.colors import n_colors\nfrom plotly.subplots import make_subplots\n# Minmax scaler\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After saving the training and testing dataset in \"train_data\" and \"test_data\" dataframes repectively, we'll replace the missing values in \"Age\" and \"Fare\" columns with the average values of the respective columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntrain_data[\"Age\"] = train_data[\"Age\"].fillna(train_data.describe()[\"Age\"][\"mean\"])\ntest_data[\"Age\"] = test_data[\"Age\"].fillna(test_data.describe()[\"Age\"][\"mean\"])\ntrain_data['Age']=train_data['Age'].astype(np.float64)\ntrain_data[\"Fare\"] = train_data[\"Fare\"].fillna(train_data.describe()[\"Fare\"][\"mean\"])\ntest_data[\"Fare\"] = test_data[\"Fare\"].fillna(test_data.describe()[\"Fare\"][\"mean\"])\ntrain_data['Fare']=train_data['Fare'].astype(int)\ntrain_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#women survival \nwomen = train_data[train_data.Sex == 'female'][\"Survived\"]\n#men survival \nmen = train_data[train_data.Sex == 'male'][\"Survived\"]\nprint(\"Survival rate for women is {:.2f} and for men is {:.2f}\".format((sum(women)/len(women))*100, (sum(men)/len(men))*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_data\ndata['Died'] = 1 - data['Survived']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration/Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>1. Gender and no. of Passangers</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[\n    go.Bar(name='Survived', x=data['Sex'], y=[data[data.Sex==data['Sex'][0]]['Survived'].agg(sum), data[data.Sex==data['Sex'][1]]['Survived'].agg(sum)], marker_color='#EB89B5', opacity=0.75),\n    go.Bar(name='Died', x=data['Sex'], y=[data[data.Sex==data['Sex'][0]]['Died'].agg(sum), data[data.Sex==data['Sex'][1]]['Died'].agg(sum)], marker_color='#330C73', opacity=0.75)\n])\nfig.update_layout(barmode='stack', width=600, title_text='Survivors categorized as Male/Female', xaxis_title_text='Sex', yaxis_title_text='Number of passangers')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>2. Survival based on Fare, Embarked and Pclass</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"facet_data = data[['Sex', 'Survived', 'Embarked', 'Pclass', 'Fare']].groupby(['Sex', 'Survived', 'Embarked', 'Pclass']).agg('sum').reset_index()\nfig  = px.bar(facet_data, x='Sex', y='Fare', facet_row='Survived', facet_col='Pclass', color='Embarked', barmode='group')\nfig.update_layout(title_text='Different view of Passenger survival with respect to Fare, Embarked and Pclass')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>3. Survival by Age of passangers</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Violin(x=data['Sex'][data['Survived']==1], y=data['Age'][data['Survived']==1],\n                         name='Survived', side='negative', legendgroup='Survived', scalegroup='Survived', line_color='blue'))\nfig.add_trace(go.Violin(x=data['Sex'][data['Died']==1], y=data['Age'][data['Died']==1],\n                         name='Died', side='positive', legendgroup='Died', scalegroup='Died', line_color='dark orange'))\nfig.update_traces(meanline_visible=True, width=1)\nfig.update_layout(violingap=0, violinmode='overlay', width=700, title_text='Survivors categorized as Male/Female related to their Age', xaxis_title_text='Sex', yaxis_title_text='Age of passangers')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>4. Survival by Gender and Age of pasangers</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Survived'] = data['Survived'].astype(str)\nfig_scatter = px.scatter(data, x='Fare', y='Age', color='Survived', facet_col='Sex')\nfig_scatter.update_layout(width=800, title_text='Survivors categorized as Male/Female', yaxis_title_text='Age of passangers')\nfig_scatter.show()\n\ndata['Survived'] = data['Survived'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>5. Survival by Fare</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_hist = go.Figure()\nfig_hist.add_trace(go.Histogram(x=data[data['Survived']==1]['Fare'], name='Survived', marker_color='#EB89B5', opacity=0.75))\nfig_hist.add_trace(go.Histogram(x=data[data['Died']==1]['Fare'], name='Died', marker_color='#330C73', opacity=0.75))\nfig_hist.update_layout(barmode='stack', title_text='Histogram representing relation between Survivor/Dead with Fare',\n                       xaxis_title_text='Fare', yaxis_title_text='Number of Passangers', bargap=0.2)\nfig_hist.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>6. Survival by Age and Fare</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_scatter_plane = go.Figure()\nfig_scatter_plane.add_trace(go.Scatter(x=data[data['Survived']==1]['Age'], y=data[data['Survived']==1]['Fare'],\n                                       mode='markers', name='Survived', marker=dict(size=data[data['Survived']==1]['Fare']/8)))\nfig_scatter_plane.add_trace(go.Scatter(x=data[data['Died']==1]['Age'], y=data[data['Died']==1]['Fare'],\n                                       mode='markers', name='Died', marker=dict(size=data[data['Died']==1]['Fare']/8)))\nfig_scatter_plane.update_layout(title_text='Scatter representing relation between Fare and Age of passangers',\n                       xaxis_title_text='Age', yaxis_title_text='Fare')\nfig_scatter_plane.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>7. Average Fare for each Pclass</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_bar_avgfare = go.Figure()\nfig_bar_avgfare.add_trace(go.Bar(x=np.sort(data['Pclass'].unique()), y=data.groupby('Pclass').mean()['Fare'], marker_color=np.sort(data['Pclass'].unique())))\nfig_bar_avgfare.update_traces(width=0.5)\nfig_bar_avgfare.update_layout(width=600, title_text='Bar chart representing Class and Avg. Fare relation',\n                       xaxis_title_text='Pclass', yaxis_title_text='Avg. Fare')\nfig_bar_avgfare.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating New Categories","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating combined dataframe\nx_train = train_data.drop(['Survived', 'Died'], axis=1)\ny_train = train_data['Survived']\nx_test = test_data\n#cobining the dataframes\ndf_combined = x_train.append(x_test)\ndf_combined.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>1. Creating categories based on Family size.</h2>\n\n<t>We'll calculate the family size for each passanger based on the values of \"Parch\" and \"SibSp\" and then categorize the individuals based on the value of family size into, Singleton(for family_size=1), SmallFamily(for family_size=2 to 4) and largeFamily(for family_size=5 to 8).</t>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating funtion for creating categories based on family size\ndef family_size():\n    global df_combined\n    df_combined['FamilySize'] = df_combined['Parch'] + df_combined['SibSp'] + 1\n    #feature to decide family size\n    df_combined['Singleton'] = df_combined['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n    df_combined['SmallFamily'] = df_combined['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\n    df_combined['LargeFamily'] = df_combined['FamilySize'].map(lambda s: 1 if 5 <= s <= 8 else 0)\n    return df_combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_combined = family_size()\ndf_combined.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>2. Creating categories based on Embarkment location</h2>\n\nNow, we'll be categorizing the passangers based on the port of embarkation. From the dataset, we know that there are three values in \"Embark\" column, C(for Cherbourg), Q(for Queenstown) and  S(for Southampton). For the missing values in the dataset, we'll assume that these passagers onboarded at the beginning of the journey. Since the Titanic set sail from Southhampton, we'll fill \"S\" in place of missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating function for creating categories based on embarked location\ndef embarked():\n    global df_combined\n    df_combined['Embarked'].fillna('S', inplace=True)\n    df_dummies = pd.get_dummies(df_combined['Embarked'], prefix='Embarked')\n    df_combined = pd.concat([df_combined, df_dummies], axis=1)\n    df_combined.drop('Embarked', axis=1, inplace=True)\n    return df_combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_combined = embarked()\ndf_combined.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>3. Creating categories based on the cabin of the passangers</h2>\n\nFrom the dataset, we can see that the cabin columns has values in format of \"C123\", which means the 123th cabin in the C deck(please refer the image). Assuming that passangers from each cabin in a particular deck has equal probabilty of survival, e.g:- a passanger from C123 and C20 each, has same chance of survival. And, a passanger from B83 and B12 each, has same chance of survival, you get the point, we'll remove the cabin number and just keep the deck of the cabin. For missing values of cabin, we'll use \"U\" for unknown.\nAfter getting the deck values for all the passangers, we'll categorize them in deck A, B, C, D, E, F, G, T and U.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![Cabin Structure](http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Titanic_cutaway_diagram.png/330px-Titanic_cutaway_diagram.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating function for creating categories based on the cabin of passengers\ndef cabin():\n    global df_combined\n    #replacing missing cabin values with U for unknown\n    df_combined['Cabin'].fillna('U', inplace=True)\n    \n    df_combined['Cabin'] = df_combined['Cabin'].map(lambda ca: ca[0])\n    #dummy encoding\n    cabin_dummies = pd.get_dummies(df_combined['Cabin'], prefix='Cabin')\n    df_combined = pd.concat([df_combined, cabin_dummies], axis=1)\n    df_combined.drop('Cabin', inplace=True, axis=1)\n    return df_combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_combined = cabin()\ndf_combined.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>4. Creating categories based on the Age of passangers</h2>\n\nNow, we'll segregate passangers based on their age. Since the spread of age property is high and ranges from 0.4 to 80 years, we'll create smaller ranges of Age to categorize passangers. Here, we'he chosen 0(for less than 11), 1(12 to 18), 2(19 to 22), 3(23 to 27), 4(28 to 33), 5(34 to 46), 6(47 to 66), 7(67 and above).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#comverting age decimal values to integer\ndf_combined['Age'] = df_combined['Age'].astype(int)\n#Converting the age range into categories\ndf_combined.loc[df_combined['Age'] <= 11, 'Age'] = 0\ndf_combined.loc[(df_combined['Age'] > 11) & (df_combined['Age'] <= 18), 'Age'] = 1\ndf_combined.loc[(df_combined['Age'] > 18) & (df_combined['Age'] <= 22), 'Age'] = 2\ndf_combined.loc[(df_combined['Age'] > 22) & (df_combined['Age'] <= 27), 'Age'] = 3\ndf_combined.loc[(df_combined['Age'] > 27) & (df_combined['Age'] <= 33), 'Age'] = 4\ndf_combined.loc[(df_combined['Age'] > 33) & (df_combined['Age'] <= 46), 'Age'] = 5\ndf_combined.loc[(df_combined['Age'] > 46) & (df_combined['Age'] <= 66), 'Age'] = 6\ndf_combined.loc[df_combined['Age'] >= 66, 'Age'] = 7\n#distribution\ndf_combined['Age'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>5. Creating categories based on Fare of passangers</h2>\n\nFinally, now we'll tag passangers based on their economic status or the fare they paid for the journey. Again, since the spread of prices is very high and ranges from 0 to 512. So, again we'll segment passangers based on the fare in smaller bins of 0(less than 7), 1(8 to 14), 2(15 to 31), 3(32 to 99), 4(100 to 250), 5(251 and above).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting fare to interger values and then into categories\ndf_combined.loc[df_combined['Fare'] <= 7, 'Fare'] = 0\ndf_combined.loc[(df_combined['Fare'] > 7) & (df_combined['Fare'] <= 14), 'Fare'] = 1\ndf_combined.loc[(df_combined['Fare'] > 14) & (df_combined['Fare'] <= 31), 'Fare'] = 2\ndf_combined.loc[(df_combined['Fare'] > 31) & (df_combined['Fare'] <= 99), 'Fare'] = 3\ndf_combined.loc[(df_combined['Fare'] > 99) & (df_combined['Fare'] <= 250), 'Fare'] = 4\ndf_combined.loc[df_combined['Fare'] >= 250, 'Fare'] = 5\ndf_combined['Fare'] = df_combined['Fare'].astype(int)\n#distribution\ndf_combined['Fare'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>6. Creating categories based on the Title of the passangers</h2>\n\nNow, it is to categorize based on Title of passangers. This is not a straight-forward value which we can analyze mathmatically, as the title of a person wouldn't mean anything to the iceberg that hit Titanic or to the chilling water that drowned anyone it could. But we know that passangers registered with Mr, Mrs, Master and other royal names would probably be rich and would have been given preference while saving and transferring using life boats. So, we'll categorize them only into Mr, Mrs, Master, Miss and Rare(which contains all other royal and rare titles used by very few passangers).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"###Get title from name\ncommon_titles = [\"Mr\", \"Mrs\", \"Miss\", \"Master\"]\ntitles = []\nfor name in df_combined['Name']:\n    title = name.split(',')[1].split('.')[0].strip()\n    if title in common_titles:\n        titles.append(title)\n    elif title==\"Mlle\":\n        titles.append(\"Miss\")\n    elif title==\"Mme\":\n        titles.append(\"Mrs\")\n    else:\n        titles.append(\"Rare\")\ndf_titles = pd.DataFrame(titles, columns=['Titles'])\ntitle_dummies = pd.get_dummies(df_titles['Titles'], prefix='Title')\ntitle_dummies = title_dummies.reset_index(drop=True)\ndf_combined = df_combined.reset_index(drop=True)\ndf_combined = pd.concat([df_combined, title_dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_combined.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Creating categories for Male and Female, and dropping few uneccessary columns</h3>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#dummies for gender\ngender_dummies = pd.get_dummies(df_combined['Sex'])\ndf_combined = pd.concat([df_combined, gender_dummies], axis=1)\ndf_combined.drop('Sex', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping down Name and ticket columns, and passengerId cloumn\ndf_combined.drop(['PassengerId',\"Name\",\"Ticket\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_combined.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Building and Training the Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Separating the train and test dataframes\nx_train = df_combined[:891].copy()\nx_test = df_combined[891:].copy()\nx_test.reset_index(inplace=True, drop=True)\nx_train.shape, x_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>1. Random Forest Model</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier()\nrandom_forest.fit(x_train, y_train)\n\ny_pred_random_forest = random_forest.predict(x_test)\n\nrandom_forest_accuracy = round(random_forest.score(x_train, y_train)*100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>2. Logistic Regression Model</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"log_regres = LogisticRegression()\nlog_regres.fit(x_train, y_train)\n\ny_pred_log_regres = log_regres.predict(x_test)\n\nlog_regres_accuracy = round(log_regres.score(x_train, y_train)*100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>3. K-nearest Neighbors</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train, y_train)\n\ny_pred_knn = knn.predict(x_test)\n\nknn_accuracy = round(knn.score(x_train, y_train)*100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>4. Gaussian Naive Bayes Model</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\n\ny_pred_gaussian = gaussian.predict(x_test)\n\ngaussian_accuracy = round(gaussian.score(x_train, y_train)*100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>5. Perceptrons Model</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"perceptron = Perceptron()\nperceptron.fit(x_train, y_train)\n\ny_pred_perceptron = perceptron.predict(x_test)\n\nperceptron_accuracy = round(perceptron.score(x_train, y_train)*100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>6. Support Vector Machine Model</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = LinearSVC()\nsvc.fit(x_train, y_train)\n\ny_pred_svc = svc.predict(x_test)\n\nsvc_accuracy = round(svc.score(x_train, y_train)*100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>7. Decision Tree Model</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeClassifier()\ntree.fit(x_train, y_train)\n\ny_pred_tree = tree.predict(x_train)\n\ntree_accuracy = round(tree.score(x_train, y_train)*100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>8. XGBoost Model</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming data into Dmatrix form\nd_x_train = xgb.DMatrix(x_train, label=y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setting the parameters\nparam = {\n    'eta': 0.5, \n    'max_depth': 16,  \n    'objective': 'multi:softprob',  \n    'num_class': 3} \n\nsteps = 20  # The number of training iterations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training the model\nxgb_model = xgb.train(param, d_x_train, steps)\n \ny_pred_xgb = xgb_model.predict(d_x_train)\n#Converting the prediction intp numpy array\ny_pred_xgb_new = np.asarray([np.argmax(line) for line in y_pred_xgb])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model_accuracy = round(accuracy_score(y_train, y_pred_xgb_new)*100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Which is the best model for prediction?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = pd.DataFrame({\n    'Model': ['Random Forest','Logistic Regression','K-Nearest Neighbour','Gaussian Naive Bayes','Perceptron','SVM','Decision Tree','XGBoost'], \n    'Result': [random_forest_accuracy,log_regres_accuracy,knn_accuracy,gaussian_accuracy,perceptron_accuracy,svc_accuracy,tree_accuracy,xgb_model_accuracy]})\nbest_model.sort_values(by=['Result'], ascending=False, inplace=True)\nbest_model.reset_index(inplace=True, drop=True)\nfig = go.Figure(data=[go.Table(\n    header=dict(values=list(best_model.columns),\n                fill_color='paleturquoise',\n                align='left'),\n    cells=dict(values=[best_model.Model, best_model.Result],\n               fill_color='lavender',\n               align='left'))\n])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Fold Cross Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nscores = cross_val_score(rf, x_train, y_train, cv=10, scoring='accuracy')\n\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This score shows that our model has an average accuracy of 83% with a standard deviation of around 4%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Feature Importance</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = pd.DataFrame({'Feature':x_train.columns,'Importance':np.round(random_forest.feature_importances_,3)})\nimportances_df = importances.sort_values('Importance', ascending=False).reset_index(drop=True)\nimportances_plot = importances.sort_values('Importance', ascending=False).set_index('Feature')\nimportances_plot.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the Features importances\nfig_bar_feature = go.Figure()\nfig_bar_feature.add_trace(go.Bar(x=importances_df['Feature'], y=importances_df['Importance']))\nfig_bar_feature.update_layout(width=800, title_text='Bar chart representing Importances of Features',\n                       xaxis_title_text='Features', yaxis_title_text='Importance', xaxis_tickangle=-45)\nfig_bar_feature.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph we can see that all the categories after \"Title_Rare\" and onwards doesn't play an important role in the prediction, hence dropping those categories.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(23, importances_df.shape[0]):\n    column = importances_df['Feature'][i]\n    df_combined.drop([column], inplace=True, axis=1)\n\ndf_combined.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Training Random Forest Model Again</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Separate the train and test dataframes\nx_train = df_combined[:891].copy()\nx_test = df_combined[891:].copy()\nx_test.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier()\nrandom_forest.fit(x_train, y_train)\ny_prediction = random_forest.predict(x_test)\n\n\naccuracy_random_forest = round(random_forest.score(x_train, y_train)*100, 2)\nprint(\"The accuracy after removing least important Features is {}, which is same as before removing.\".format(accuracy_random_forest))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Training Decision Tree Model Again</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeClassifier()\ntree.fit(x_train, y_train)\n\ny_pred_tree = tree.predict(x_test)\n\ntree_accuracy = round(tree.score(x_train, y_train)*100, 2)\nprint(\"The accuracy after removing least important Features is {}, which is same as before removing.\".format(tree_accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Training XGBoost Model Again</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming data into Dmatrix form\nd_x_train = xgb.DMatrix(x_train, label=y_train)\nd_x_test = xgb.DMatrix(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = xgb.train(param, d_x_train, steps)\ny_pred_xgb = xgb_model.predict(d_x_test)\ny_pred_xgb_new = np.asarray([np.argmax(line) for line in y_pred_xgb])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'PassengerID': test_data.PassengerId,'Survived':y_pred_xgb_new})\noutput.to_csv(\"my_submission.csv\", index=False)\nprint(\"Submission successfully saved!!!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\nThis project enhanced my Machine Learning concepts significantly and I have developed a structured approach towards analyzing and applying ML concepts, which I learned from different sources, on real world problems. \nI gained experience in using different visualization tools like \"Matplotlib\", which I used in earlier versions of this kernel, but later used \"Plotly\" due to the improved interactivity that it provides with plots.\nAlso I created multiple ML models, to choose the best suitable one and on the route to that learned the key concepts about building and training models.Also, gained crucial knowledge about hyperparameter tuning, which undoubtely one of the most important aspect of getting most accurate prediction from a model.\n\nOfcourse, there is still scope for improvement, either by removing noisy features or by performing extensive hyperparameter tuning which I'll try to perform in future.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}