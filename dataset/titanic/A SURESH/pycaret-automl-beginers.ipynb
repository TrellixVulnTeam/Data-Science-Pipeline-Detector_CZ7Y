{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=color:blue align=\"left\"> Table of Conents </h2>\n\n#### 1) What is PyCaret and Why Should you Use it?\n#### 2) Installing PyCaret on your Machine\n#### 3) Accessing Data\n>    3.1) Loading a Dataframe with Pandas\n\n>    3.2) Using the Data Repository\n\n>    3.3) Experiment Setup\n\n#### 4) Compare Baseline Models\n\n#### 5) Train and tune specific models\n\n#### 6) Combine Models ( Optional )\n\n#### 7) AutoML ( Optional ) \n\n#### 8) Classification Example\n>    8.1) Dataset: Diabetes\n\n#### 9) Regression Example\n\n>    9.1) Dataset: Bostan\n\n#### 10) Import Dataset: juice\n\n>    10.1) Setting up Environment\n\n>    10.2) Compare Models\n\n>         10.2.1) Default\n\n>         10.2.2) Sorted Method \n\n>         10.2.3) n_select parameter\n\n>         10.2.4) Whitelist parameter\n\n>    10.3) Create Model\n\n>    10.4) Tune Model\n\n>    10.5) Building Ensemble Models using PyCaret\n\n>    10.6) Blend Models\n\n>    10.7) Analyze Model \n\n>    10.8) Evaluate our Model\n\n>    10.9) Interpret Model\n\n>    10.10) Make Predictions\n\n>    10.11) Save and load the model\n\n>    10.12) Deploy Model","metadata":{}},{"cell_type":"markdown","source":"<h2 style=color:blue align=\"left\"> Reference </h2>\n\n> https://pycaret.org/compare-models/\n\n> https://www.kaggle.com/discussion/234790\n\n> https://www.youtube.com/watch?v=jlW5kRBwcb0\n\n> https://www.youtube.com/watch?v=BjcpOVQhNlc&t=74s\n\n> https://www.youtube.com/watch?v=TXOLlgzAdxM&t=4s","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:180%; text-align:center; border-radius: 15px 50px;\"> 1) What is PyCaret and Why Should you Use it? </h1>","metadata":{}},{"cell_type":"markdown","source":"- PyCaret is an open-source, machine learning library in Python that helps you from data preparation to model deployment. It is easy to use and you can do almost every data science project task with just one line of code.\n\n- PyCaret, being a low-code library, makes you more productive. You can spend less time on coding and can do more experiments\n\n- It is an easy to use machine learning library that will help you perform end-to-end machine learning experiments, whether that’s imputing missing values, encoding categorical data, feature engineering, hyperparameter tuning, or building ensemble models","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:180%; text-align:center; border-radius: 15px 50px;\"> 2) Installing PyCaret on your Machine </h1>","metadata":{}},{"cell_type":"code","source":"# run this cell to install pycaret in Google Colab\n# !pip install pycaret ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If you are using jupyter notebook, you can pip install pycaret using jupyter notebook or command line\n# pip install pycaret","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pycaret","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pycaret.utils import version\nversion()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pycaret\nprint('Using PyCaret Version', pycaret.__version__)\nprint('Path to PyCaret: ', pycaret.__file__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 3) Accessing Data </h1>\n\n#### There are two ways to register your data into PyCaret:\n\n> Loading a Dataframe with Pandas\n\n> Using the Data Repository\n\n<h3 style=color:green align=\"left\"> 3.1) Loading a Dataframe with Pandas </h3>\n- The first way to get data into PyCaret is simply to load up a Pandas dataframe and then pass it to PyCaret.\n\n        data = pd.read_csv(data_path)\n        data.head()\n        \n<h3 style=color:green align=\"left\"> 3.2) Using the Data Repository </h3>\n- The second way of getting data, which is used in the PyCaret tutorials, is to pull in a curated dataset from the PyCaret Data Repository. The repository helpfully includes popular sample datasets for classification, regression, clustering, NLP, etc.\n\n        all_datasets = pycaret.datasets.get_data('index')","metadata":{}},{"cell_type":"code","source":"# The repository contained 56 datasets\nfrom pycaret.datasets import get_data\nall_datasets = pycaret.datasets.get_data('index')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_datasets = pycaret.datasets.get_data('index')\ndataset_name = 'heart_disease' # Replace with your desired dataset.\ndata = pycaret.datasets.get_data(dataset_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=color:green align=\"left\"> 3.3) Experiment Setup </h3>\n\n- Many often-tedious preprocessing steps are taken care of automatically in PyCaret, which standardizes and conveniently packages fundamental data preparation steps into repeatable time-saving workflows.  Users are able to **automate cleaning (e.g. handling missing values with various imputation methods available), splitting into train and test sets, as well as some aspects of feature engineering and training.**  While many of the objects created in this process aren’t explicitly shown to the user (such as train and test sets, or label vectors), they are accessible if needed or desired by more experienced practitioners. ","metadata":{}},{"cell_type":"code","source":"from pycaret.classification import *\n\nclf1 = setup(data=data, \n             target = 'Disease',                # Use your target variable.\n             session_id=123, \n             log_experiment=True, \n             experiment_name='experiment1',     # Use any experiment name.\n             silent=True                        # Runs the command without user input. \n            )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 4) Compare Baseline Models </h1>\n\n- In a single line of code, we can train and compare baseline versions of all available models on our dataset:\n\n       best_model = compare_models()\n       \n\n- This trains a baseline version of each available model type and yields a detailed comparison of metrics for the trained models, and highlights the best results across models.\n\n- Note that we did not have to do any data preparation by hand — we just needed to make the data available as a CSV, and run the setup function.  Behind the scenes of those two setup steps, the data was passed into PyCaret and transformed to the extent necessary to train and evaluate the available models.  To see what models PyCaret knows about, we can run which returns a dataframe of all available models, their proper names, the reference package that they’re drawn from (e.g. sklearn.linear_model._logistic.LogisticRegression), and whether Turbo is supported (a mode that limits the model training time, which may be desirable for rapid comparisons).\n\n      models()","metadata":{}},{"cell_type":"code","source":"models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 5) Train and tune specific models </h1>\n\n- From **compare_models**, we were easily able to see the **best baseline models for each metric**, and select those for further investigation.\n\n- For example, if we were looking for the model with the **highest AUC** above, we would have elected to continue with **random forest**.  We can then save and fine tune our model using the **create_model and tune_model** functions. ","metadata":{}},{"cell_type":"code","source":"rf = create_model('rf', fold = 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tuned_rf = tune_model(rf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 6) Combine Models ( Optional ) </h1>\n\n- We can combine our trained models in various ways.  First, we can create ensemble models with methods such as **bagging (bootstrap aggregating) and boosting.**  Both bagging and boosting are invoked with the ensemble_model function.  We can further apply **blending and stacking methods** to combine diverse models, or estimators — a list of estimators can be passed to blend_models or stack_models.  If desired, one could create ensemble models and combine them via blending or stacking, all in a single line of code.  For clarity, we’ll show an example in which each of these four methods is shown sequentially in its own cell, which also allows us to see the default output from PyCaret when each of these methods is used.  ","metadata":{}},{"cell_type":"code","source":"dt = create_model('dt', fold = 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a bagged decision tree ensemble model\nbagged_dt = ensemble_model(dt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a boosted decision tree ensemble model\nboosted_dt = ensemble_model(dt, method='Boosting')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Blending estimators\nblender = blend_models(estimator_list = [boosted_dt, bagged_dt, Tuned_rf], method = 'soft')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stacking bagged, boosted, and tuned estimators\nstacker = stack_models(estimator_list = [boosted_dt,bagged_dt,Tuned_rf], meta_model=rf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 7) AutoML ( Optional ) </h1>\n\n- Quick and painless tuning for a particular metric can be accomplished using the AutoML feature.\n\n- AutoML techniques generally reduce the human oversight of the model selection process, which may not be ideal or appropriate in many contexts, they can be a useful tool to quickly identify the highest performing option for a particular purpose. ","metadata":{}},{"cell_type":"code","source":"# Select the best model based on the chosen metric\nbest = automl(optimize = 'AUC')\nbest","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 8) Classification Example </h1>","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:yellow; font-family:newtimeroman; font-size:180%; text-align:left;\"> 8.1) Dataset: Diabetes </h1>","metadata":{}},{"cell_type":"markdown","source":"        # Importing dataset\n          from pycaret.datasets import get_data\n          diabetes = get_data('diabetes')\n\n        # Importing module and initializing setup\n          from pycaret.classification import *\n          clf1 = setup(data = diabetes, target = 'Class variable')\n\n        # return best model\n          best = compare_models()\n\n        # return top 3 models based on 'Accuracy'\n          top3 = compare_models(n_select = 3)\n\n        # return best model based on AUC\n          best = compare_models(sort = 'AUC') #default is 'Accuracy'\n\n        # compare specific models\n          best_specific = compare_models(include = ['dt','rf','xgboost'])\n\n        # blacklist certain models\n          best_specific = compare_models(exclude = ['catboost', 'svm'])","metadata":{}},{"cell_type":"code","source":"# Importing dataset\ndiabetes = get_data('diabetes')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing module and initializing setup\nfrom pycaret.classification import *\nclf1 = setup(data = diabetes, target = 'Class variable')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# return best model\nbest = compare_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# return top 3 models based on 'Accuracy'\ntop3 = compare_models(n_select = 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# return best model based on AUC\nbest = compare_models(sort = 'AUC') # default is 'Accuracy'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare specific models\nbest_specific = compare_models(include = ['dt','rf','xgboost'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# blacklist certain models\nbest_specific = compare_models(exclude = ['catboost', 'svm'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 9) Regression Example </h1>","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:yellow; font-family:newtimeroman; font-size:180%; text-align:left;\"> 9.1) Dataset: Bostan </h1>","metadata":{}},{"cell_type":"markdown","source":"        # Importing dataset\n          from pycaret.datasets import get_data\n          boston = get_data('boston')\n\n        # Importing module and initializing setup\n          from pycaret.regression import *\n          reg1 = setup(data = boston, target = 'medv')\n\n        # return best model\n          best = compare_models()\n\n        # return top 3 models based on 'R2'\n          top3 = compare_models(n_select = 3)\n\n        # return best model based on MAPE\n          best = compare_models(sort = 'MAPE') #default is 'R2'\n\n        # compare specific models\n          best_specific = compare_models(include = ['dt','rf','xgboost'])\n\n        # blacklist certain models\n          best_specific = compare_models(exclude = ['catboost', 'svm'])","metadata":{}},{"cell_type":"code","source":"# Importing dataset\nfrom pycaret.datasets import get_data\nboston = get_data('boston')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing module and initializing setup\nfrom pycaret.regression import *\nreg1 = setup(data = boston, target = 'medv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# return best model\nbest = compare_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# return top 3 models based on 'R2'\ntop3 = compare_models(n_select = 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# return best model based on MAPE\nbest = compare_models(sort = 'MAPE') #default is 'R2'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare specific models\nbest_specific = compare_models(include = ['dt','rf','xgboost'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# blacklist certain models\nbest_specific = compare_models(exclude = ['catboost', 'svm'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:180%; text-align:center; border-radius: 15px 50px;\"> 10) Import Dataset: juice </h1>","metadata":{}},{"cell_type":"code","source":"from pycaret.datasets import get_data\ndata = get_data('juice')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All available datasets in PyCaret\nget_data('index')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 10.1) Setting up Environment </h1>","metadata":{}},{"cell_type":"markdown","source":"### Importing a Module:\n- Depending upon the type of problem you are going to solve, you first need to import the module.\n- In the first version of PyCaret, 6 different modules are available:\n\n> 1) Regression\n\n> 2) Classification\n\n> 3) Clustering\n\n> 4) Natural language processing (NLP)\n\n> 5) Anomaly detection\n\n> 6) Associate mining rule.\n- In this article, we will solve a **classification problem** and hence we will import the classification module\n\n### Initializing the Setup:\n- In this step, PyCaret performs some basic **preprocessing** tasks:\n\n> Ignoring the IDs and Date Columns\n\n> Imputing the missing values\n\n> Encoding the categorical variables\n\n> Splitting the dataset into the train-test split for the rest of the modeling steps.\n- When you run the setup function, it will first confirm the data types, and then if you press enter, it will create the environment for you to go ahead","metadata":{}},{"cell_type":"code","source":"from pycaret.classification import *\nclf1 = setup(data, target='Purchase', session_id=786)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 10.2) Compare Models </h1>\n\n##### This is another useful function of the PyCaret library. If you do not want to try the different models one by one, you can use the compare models function and it will train and compare common evaluation metrics for all the available models in the library of the module you have imported.\n\n#### This function is only available in:\n> pycaret.classification\n\n> pycaret.regression","metadata":{}},{"cell_type":"markdown","source":"- This function trains **all the models in the model library** using **default hyperparameters** and evaluates performance metrics using cross-validation. It returns the trained model object. The evaluation metrics used are:\n\n - **Classification:** Accuracy, AUC, Recall, Precision, F1, Kappa, MCC\n\n - **Regression:** MAE, MSE, RMSE, R2, RMSLE, MAPE\n \n- The output of the function is a table showing the averaged score of all models across the folds. The number of folds can be defined using the **fold** parameter within the **compare_models** function. By default, the **fold is set to 10.** The table is sorted (highest to lowest) by the metric of choice and can be defined using the **sort** parameter. By default, the table is sorted by **Accuracy** for classification experiments and R2 for regression experiments. Certain models are prevented from the comparison because of their longer run-time. In order to bypass this prevention, the turbo parameter can be set to False.","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:DeepSkyBlue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 10.2.1) Default </h1>","metadata":{}},{"cell_type":"code","source":"# compare performance of different classification models\ncompare_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = compare_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### In default compare models, which model highlights more yellow is the best model.","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:DeepSkyBlue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 10.2.2) Sorted Method </h1>","metadata":{}},{"cell_type":"code","source":"best_model = compare_models(sort='Recall')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now Recall is the priority, so whichever model shows high recall value that becomes best model","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:DeepSkyBlue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 10.2.3) n_select parameter </h1>","metadata":{}},{"cell_type":"code","source":"top5 = compare_models(n_select=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:DeepSkyBlue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 10.2.4) Whitelist parameter </h1>","metadata":{}},{"cell_type":"code","source":"# This will only train 4 models as mentioned in whitelist parameter.\nw = compare_models(include = ['dt', 'rf', 'xgboost', 'lightgbm'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 10.3) Create Model </h1>","metadata":{}},{"cell_type":"markdown","source":"### Training a Model\n- Training a model in PyCaret is quite simple. You just need to use the create_model function that takes just the one parameter – the model abbreviation as a string. Here, we are going to first train a **decision tree model** for which we have to pass **“dt”** and it will return a **table with k-fold cross-validated scores** of common evaluation metrics used for classification models.\n\n- Here’s q quick reminder of the evaluation metrics used for supervised learning:\n\n - **Classification:** Accuracy, AUC, Recall, Precision, F1, Kappa\n - **Regression:** MAE, MSE, RMSE, R2, RMSLE, MAPE","metadata":{}},{"cell_type":"code","source":"# build the Logistic Regression model\n# default number of folds =10\nlr = create_model('lr')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for number of folds =5\nlr = create_model('lr', fold=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build the decision tree model\ndt = create_model('dt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build the Naive Bayes model\nnb = create_model('nb')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 10.4) Tune Model </h1>","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameter Tuning\n- We can tune the hyperparameters of a machine learning model by just using the **tune_model** function which takes one parameter – the model abbreviation string (the same as we used in the create_model function).\n\n- PyCaret provides us a lot of flexibility. For example, we can define the number of folds using the **fold** parameter within the **tune_model** function. Or we can change the number of iterations using the **n_iter** parameter. Increasing the **n_iter** parameter will obviously increase the training time but will give a much better performance.","metadata":{}},{"cell_type":"code","source":"# build and tune the Decision Tree model\ntuned_dt = tune_model(dt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### we can compare below two parameters for with default parameter & hyperparameters ","metadata":{}},{"cell_type":"code","source":"dt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_dt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_nb = tune_model(nb, optimize='AUC')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_nb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 10.5) Building Ensemble Models using PyCaret </h1>\n\n#### Ensemble models in machine learning combine the decisions from multiple models to improve the overall performance.\n\n### In PyCaret, we can create bagging, boosting, blending, and stacking ensemble models with just one line of code.","metadata":{}},{"cell_type":"code","source":"# default n_estimators=10\nbagged_dt = ensemble_model(dt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set n_estimators=25\nbagged_dt = ensemble_model(dt, n_estimators=25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensemble boosting\nboosted_dt = ensemble_model(dt, method='Boosting')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 10.6) Blend Models </h1>","metadata":{}},{"cell_type":"code","source":"lr = create_model('lr', verbose=False)\nlda = create_model('lda', verbose=False)\ngbc = create_model('gbc', verbose=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensemble: blending\nblender = blend_models(estimator_list=[lr, lda, gbc], method='soft')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blender.estimators_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 10.7) Analyze Model </h1>\n\n##### Now, after training the model, the next step is to analyze the results. This especially useful from a business perspective, right? Analyzing a model in PyCaret is again very simple. Just a single line of code and you can do the following:\n\n### Plot Model Results:\n- Analyzing model performance in PyCaret is as simple as writing **plot_model.**\n\n- You can plot **decision boundaries, precision-recall curve, validation curve, residual plots, etc..**\n\n- **clustering** models, you can plot the **elbow plot and silhouette plot.**\n\n- **text data**, you can plot **word clouds, bigram and trigram frequency plots,** etc.\n\n### Interpret Results:\n- Interpreting model results helps in debugging the model by analyzing the important features. This is a crucial step in industry-grade machine learning projects. In PyCaret, we can interpret the model by **SHAP values and correlation plot** with just one line of code.","metadata":{}},{"cell_type":"code","source":"# AUC-ROC plot\n# plot_model(blender) (or) plot_model(blender, plot = 'auc') --> both gives same result\nplot_model(blender, plot = 'auc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(blender, plot='confusion_matrix')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(blender, plot='threshold')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Precision Recall Curve\nplot_model(blender, plot='pr')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation Curve\nplot_model(tuned_dt, plot='vc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Boundary for \"Decision tree\"\nplot_model(dt, plot='boundary')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Boundary for \"Naive Baeys\"\nplot_model(tuned_nb, plot='boundary')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Boundary for \"Blending\"\nplot_model(blender, plot='boundary')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 10.8) Evaluate our Model </h1>\n\n- If you do not want to plot all these visualizations individually, then the PyCaret library has another amazing function: **evaluate_model**. In this function, you just need to pass the model object and PyCaret will create an interactive window for you to see and analyze the model in all the possible ways:","metadata":{}},{"cell_type":"code","source":"# evaluate model\nevaluate_model(boosted_dt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 10.9) Interpret Model </h1>\n\n- Interpreting complex models is very important in most machine learning projects. It helps in debugging the model by analyzing what the model thinks is important. In PyCaret, this step is as simple as writing **interpret_model** to get the Shapley values.","metadata":{}},{"cell_type":"code","source":"xgboost = create_model('xgboost')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# interpret_model: SHAP\ninterpret_model(xgboost)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# interpret model : Correlation\ninterpret_model(xgboost, plot='correlation')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interpret_model(xgboost, plot='reason', observation=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interpret_model(xgboost, plot='reason')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 10.10) Make Predictions </h1>","metadata":{}},{"cell_type":"markdown","source":"#### read the test data\ntest_data_classification = pd.read_csv('datasets/loan_test_data.csv')\n\n#### make predictions\npredictions = classification.predict_model(classification_dt, data=test_data_classification)\n\n#### view the predictions\npredictions","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 10.11) Save and Load the Model </h1>","metadata":{}},{"cell_type":"markdown","source":"- Now, once the model is built and tested, we can save this in the pickle file using the save_model function. Pass the model to be saved and the file name and that’s it:","metadata":{}},{"cell_type":"markdown","source":"<h2 style=color:green align=\"left\"> save the model </h2>\nclassification.save_model(classification_dt, 'decision_tree_1')","metadata":{}},{"cell_type":"markdown","source":"- We can load this model later on and predict labels on the unseen data:","metadata":{}},{"cell_type":"code","source":"save_model(xgboost, 'abc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=color:green align=\"left\"> load model </h2>\ndt_model = classification.load_model(model_name='decision_tree_1')","metadata":{}},{"cell_type":"code","source":"l = load_model('abc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 10.12) Deploy Model </h1>","metadata":{}},{"cell_type":"markdown","source":"deploy_model(xgboost, model_name='xgboost-for-aws', authentication={'bucket':'pycaret-test'})","metadata":{}}]}