{"cells":[{"metadata":{"_uuid":"c9652aaeef5172df82d3723cec676a66f4188da0"},"cell_type":"markdown","source":"# Preamble\nThis is my second data science project and my first competition here on Kaggle. In first place I will analyse the single features. Therefore it starts with a null-value analysis and the restoration of missing data. This includes the creation of new features as well. Right after that, there will be a feature distribution as well as a survivor distribution to get a closer view of the dataset. For exercise, I have tried a lot around with Matplotlib to visualize several different charts. This sometimes ends in a lot of lines of code which I had to hide to not overload this notebook. In the end of this notebook there will be a crossvaildation of the trained model as well as the execution on the test dataset.\n\n**I am looking forward to your feedback and comments. If you like my work please upvote it.**\n"},{"metadata":{"_uuid":"59b82c97be6b7627096d182de65da72a451d01a9"},"cell_type":"markdown","source":"# Import and Preparation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nfrom matplotlib.ticker import PercentFormatter\nimport seaborn as sns\n\n# scipy.special for sigmoid function\nimport scipy.special\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv', index_col='PassengerId')\ndf_test = pd.read_csv('../input/test.csv', index_col='PassengerId')\ndf_gender_sub = pd.read_csv(\"../input/gender_submission.csv\", index_col='PassengerId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93f454dfae6bfa406b1525dc1e94630919fba987","scrolled":true},"cell_type":"code","source":"# Storing the target separately\nSurvived = df_train.loc[:,'Survived']\ndf_train = df_train.drop(['Survived'], axis=1).copy()\n\n# Saving index for train test split \ntrain_index = df_train.index\ntest_index = df_test.index\n\n# Concate the two datasets\ndf_all = pd.concat([df_train, df_test])\n\n# dont needed anymore\n##del df_train\n##del df_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e51e3d019489965faf9fe7a6eb0b623ee25bc642"},"cell_type":"markdown","source":"# Missing Values"},{"metadata":{"trusted":true,"_uuid":"33f50d0c8daccc602d4aa5222607f9fb2ff37e89"},"cell_type":"code","source":"# Function for nullanalysis\ndef nullAnalysis(df):\n    tab_info=pd.DataFrame(df.dtypes).T.rename(index={0:'column type'})\n\n    tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n    tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()/df.shape[0]*100)\n                         .T.rename(index={0:'null values (%)'}))\n    return tab_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f61fd8d0c8ceedf3b11d071e9c8e82ad9c227349"},"cell_type":"code","source":"# Show the null values\nnullAnalysis(df_all)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e732d45cfadbef6b465ab5adcf2f0ddc6a5d6dc"},"cell_type":"markdown","source":"## Age"},{"metadata":{"trusted":true,"_uuid":"26f1af74618dcee236d9d03db84cf971f06f1053"},"cell_type":"code","source":"# First 10 datarows where age is null\ndf_all[df_all.loc[:,'Age'].isnull()].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d73825103f4a469b6b4af373f7a974bc414350b","scrolled":true},"cell_type":"code","source":"# Average age overall\nprint(\"Average age of a passengers: \", round(df_all.loc[:,'Age'].agg('mean'),0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eca8902f0668b36441e87c4f8e1a39dbb8419713"},"cell_type":"markdown","source":"### Average Age per Class"},{"metadata":{"trusted":true,"_uuid":"b9abde06c946280939cd677a07c83082782954da","scrolled":true},"cell_type":"code","source":"# Average age per class\ndf_all.groupby('Pclass')['Age'].agg('mean')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee17d17a68c835ab1a8168975cfd159a8cf435e6"},"cell_type":"markdown","source":"We will use the average age of the corresponding class to fill the missing passenger ages. This would be the best fit for that problem."},{"metadata":{"trusted":true,"_uuid":"862144b2cf973b660791ddf05f98e8e493cd796c"},"cell_type":"code","source":"# Setting the average age of each class for the missing values inside the corresponding class\ndf_all.loc[(df_all['Age'].isnull()) & (df_all['Pclass'] == 1), ['Age']] = round(df_all.groupby('Pclass')['Age'].agg('mean')[1],0)\ndf_all.loc[(df_all['Age'].isnull()) & (df_all['Pclass'] == 2), ['Age']] = round(df_all.groupby('Pclass')['Age'].agg('mean')[2],0)\ndf_all.loc[(df_all['Age'].isnull()) & (df_all['Pclass'] == 3), ['Age']] = round(df_all.groupby('Pclass')['Age'].agg('mean')[3],0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e9e6fd8702ac0c818ba367856016783e8112f9c"},"cell_type":"markdown","source":"## Fare\n"},{"metadata":{"trusted":true,"_uuid":"bb3630fad919ec1b8778472b5507547cb4f99434"},"cell_type":"code","source":"df_all[df_all['Fare'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72f2e3727caa724d0afa43c73aad7666871c5537"},"cell_type":"markdown","source":"Only one passenger has not payed his Ticket, or is even not recorded. To fill the gab I will use the mean ticket price for his third class ticket: 13.30 "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"1f72b972dddf129302de5b22bd1106d0ed223d99"},"cell_type":"code","source":"df_all.groupby('Pclass', as_index=False)['Fare'].agg('mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"887e0f5121e468493409a7a6e143b63d2e199081"},"cell_type":"code","source":"# Setting Fare to mean fare of pclass\ndf_all.loc[1044,['Fare']] = 13.30","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ca201dfc10bc37ea0e3026a7c7244bb14082a74"},"cell_type":"markdown","source":"## Cabin"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d16640e13e6d3276d8addf9ab11bdba894dbbaca"},"cell_type":"code","source":"# Count all Cabins with NaN data\nprint(\"Count of cabins with nan data: \")\ndf_all.loc[(df_all.loc[:,'Cabin'].isnull()) == True]['Name'].count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d226a5fad1f989261ec4dd99aa64412c32ff4917"},"cell_type":"markdown","source":"The *Cabin* situation looks pretty bad. From the 1309 data points are only roundabout 300 filled with *Cabin* information, the other 1000 data points are empty. I assumed with the cabin information we could rather conclude on a passengers survival. I don't want to use the fillna() - method with this sparely filled feature, that would not make any sense and will probably affect my results badly.  Let's look into the *Cabin* distribution compared to the *Pclass*."},{"metadata":{"_uuid":"48d5b5087036d5e7368c4eb394a886a0d85879bb"},"cell_type":"markdown","source":"### Cabin and Pclass Distribution"},{"metadata":{"trusted":true,"_uuid":"46dcda86e2eff648075c0a41a742e45a21308509"},"cell_type":"code","source":"# Group by Pclasses\ndf_all.groupby('Pclass').agg('count')[['Name','Cabin']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e5e36327dd9f97729d2095520505281d2535c39"},"cell_type":"markdown","source":"The roundabout 300 filled examples for the *Cabin* information are fairly one-sided distributed to the first class (*Pclass*). The *Name* shows the total distribution for the cabin class, so 256 passengers of 323 first class passengers have a *Cabin* information available. Only 39 *Cabin* values in total are filled in the second and third class (*Pclass*). A comparison to the number of passengers does not make it better, there are 709 passengers in the third, 277 in the second class and 323 in the first class. \nThe completeness of the data in relation to the number of passengers per class is as follows:"},{"metadata":{"trusted":true,"_uuid":"c15d6f904cd5b10d38058efe1d14380e903d0545","scrolled":true},"cell_type":"code","source":"(df_all.groupby('Pclass').agg('count')['Cabin'] / df_all.groupby('Pclass').agg('count')['Name'])*100","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f270ab1fbebd8ec6cb0069c50c43d270bfacfb0b"},"cell_type":"markdown","source":"It clearly shows that the second and third class are sparsely filled in comparison to their passenger's count. The first class looks with nearly 80% pretty good. We could consider applying the **fillNa** only to the first class and using different methods for the second and third class.\n\nWhile analysing the *Cabin* feature I realize there are sometimes multiple values inside the *Cabin* feature. The following table shows these multiple *Cabin* values:"},{"metadata":{"trusted":true,"_uuid":"abcfb09b85c1fd263872db699c9ac9f0d5647451"},"cell_type":"code","source":"df_all[df_all['Cabin'].str.contains(' ', regex=False) == True].sort_values(by='Cabin')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e785a04722cca85d5ef67c77d24897760bc8ac9"},"cell_type":"markdown","source":"There is a way to split the string values easily but the information gain from the 41 examples in comparison to the roundabout 1000 missing values are pretty low:"},{"metadata":{"trusted":true,"_uuid":"fd53c7ddecf19da3b3e10230f452212aa96daab1"},"cell_type":"code","source":"# Creating a data frame for the Cabin values to split multiple values into seperate columns\ndf_cabin_expand = df_all.loc[:,'Cabin'].str.split(' ', expand=True)\n\n# Group all doubled values by the first value\ndf_cabin_expand[df_cabin_expand.loc[:,1].isnull() == False].groupby([0]).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1936fe7e7a3f86a27922221fb259c398a1b8254f"},"cell_type":"markdown","source":"The *Cabin* feature will be dropped at least. There is no way of restoring the information from this column, the duplicate values are not enough to restore all missing values and the other features do not supply enough information to restore this based of them."},{"metadata":{"_uuid":"86fae520da7dd26972bb0253e972a9ba9de1aa73"},"cell_type":"markdown","source":"## Embark\nThe *Embark* feature has two null values. For this small amount of missing values, I will use the fillna-function to fill the gaps."},{"metadata":{"trusted":true,"_uuid":"1feadfe984636bf3f513f30a432af0980b447754"},"cell_type":"code","source":"df_all[df_all['Embarked'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fbadb4eab4a5e4f3ea7d8bdb435531e29b03b36"},"cell_type":"code","source":"# Fill the two missing Embarked features by using the next valid value\ndf_all['Embarked'] = df_all['Embarked'].fillna(method='bfill')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d22146637a4c7810de00991655ef44455e22c3a"},"cell_type":"markdown","source":"# Feature Creation"},{"metadata":{"_uuid":"5e08e4c1079d9921276ffbada2c8989854b936e5"},"cell_type":"markdown","source":"## Salutation\nWith analyzing the *Cabin* feature I realized there exist different salutations in the passengers *Name* feature. The string split function will be useful here, mentioned in the *Cabin* feature analysis. The values differ among others as follows:"},{"metadata":{"trusted":true,"_uuid":"f9e61a3a00cd6122f33e217998d1e947cf702f4b","scrolled":false},"cell_type":"code","source":"# Split Name feature strings into several columns\ndf_name_salutation = df_all.loc[:,'Name'].str.split(' ', expand=True).copy()\ndf_name_salutation.groupby(1).count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"880156e1bd90160d2db8507f39497d41e67b9b13"},"cell_type":"markdown","source":"Here is still something to clean but you can clearly see, that the most values in the split column \"1\" refers to the passenger's salutation. Moreover, I recognized that the salutations strings always ends with a dot.:\n\n- Capt. - Captain\n- Col. - Colonel\n- Don. - Don\n- Dr. - Doctor\n- Major.\n- Master. - \"a way of addressing politely a boy ... too young to be called 'Mister'.\" - *Leslie Dunkling*  \n- Miss. \n- Mlle. - Mademoiselle\n- Mme. - Madame\n- Mr.\n- Mrs.\n- Ms.\n- Countess.\n- ...\n\nNext I will retriev the single salutations from column *\"1\"* and *\"2\"* based on the string value \".\" (dot) and concatenate the two columns to one salutation column and append these to the main dataframe. "},{"metadata":{"trusted":true,"_uuid":"f604840d656a83df4c5ba5b04d0e1dcb4086b589","scrolled":false},"cell_type":"code","source":"# Extract Salutation from every column based on the '.'\ndf_newsal_1 = df_name_salutation[df_name_salutation[1].str.contains('.', regex=False)][1]\ndf_newsal_2 = df_name_salutation[df_name_salutation[2].str.contains('.', regex=False)][2]\ndf_newsal_3 = df_name_salutation[(df_name_salutation[3].isnull() == False) & (df_name_salutation[3].str.contains('.', regex=False))][3]\n\n# Rename column for append \ndf_newsal_2 = df_newsal_2.rename(1)\ndf_newsal_3 =  df_newsal_3.rename(1)\n\n# Append both salutations results to one column and rename them\ndf_newsal = df_newsal_1.append([df_newsal_2, df_newsal_3])\ndf_newsal = df_newsal.rename('Salutation')\n\n# Concatenate them to the main dataframe\ndf_all = pd.concat([df_all,df_newsal],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"736561fa345bc8b5f8b597d816cb275a83761e0f"},"cell_type":"markdown","source":"The following list shows all the salutation distribution. Nothing is leftover:"},{"metadata":{"trusted":true,"_uuid":"b254d9af134a0b8f2576f64465f9f68bb91e8ee9"},"cell_type":"code","source":"df_all.groupby('Salutation').count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1a66f51ba285cfd5949aca956b3d4a83c16772f"},"cell_type":"markdown","source":"## Family\nCreating a ***Family*** true / false information based on the count of SibSp and Parch.\n"},{"metadata":{"trusted":true,"_uuid":"50c08d3eb1c04434166f9f552a4b43bcfa3c3ce9"},"cell_type":"code","source":"# Create new feature Family true/false\ndf_all.loc[:,'Family'] = ((df_all['SibSp'] > 0) | (df_all['Parch'] > 0)).replace(True, 1, inplace=False)\ndf_all.loc[:,'Family'] = df_all.loc[:,'Family'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"901c08b0dff3cea490e78f6f58c88e4adcb55394"},"cell_type":"markdown","source":"### Family Name Extraction"},{"metadata":{"trusted":true,"_uuid":"a3e9ec0fc369fd95c9ca915a926b566e6e297932"},"cell_type":"code","source":"# Splitting the name feature into seperate strings\ndf_familynames = df_all.loc[:,'Name'].str.split(' ', expand=True).copy()\n\n# Families with single last name\nl_singleLastname = df_familynames[(df_familynames[0].str.contains(',', regex=False)==True)].index\n\n# Families with double last name\nl_doubleLastname = df_familynames[(df_familynames[0].str.contains(',', regex=False)==False) \n                                  & (df_familynames[1].str.contains(',', regex=False)==True)].index\n\n# Families with double last name and more\nl_doubleLastnameSpec = df_familynames[(df_familynames[0].str.contains(',', regex=False)==False) \n                                      & (df_familynames[1].str.contains(',', regex=False)==False) \n                                      & (df_familynames[2].str.contains(',', regex=False)==True)].index\n\n# Create all last names for the single named, double named and multiple named passengers\ndf_singleLastname = df_familynames.loc[l_singleLastname,0]\ndf_doubleLastname = (df_familynames.loc[l_doubleLastname,0] \n                     + ' ' + df_familynames.loc[l_doubleLastname,1])\ndf_doubleLastnameSpec = (df_familynames.loc[l_doubleLastnameSpec,0] \n                         + ' ' + df_familynames.loc[l_doubleLastnameSpec,1] \n                         + ' ' + df_familynames.loc[l_doubleLastnameSpec,2])\n\n\n# Rename column for append \ndf_singleLastname = df_singleLastname.rename('Lastname')\ndf_doubleLastname = df_doubleLastname.rename('Lastname')\ndf_doubleLastnameSpec =  df_doubleLastnameSpec.rename('Lastname')\n\n# Creating a Lastname column in initial dataframe\ndf_familynames['Lastname'] = df_singleLastname.append([df_doubleLastname, df_doubleLastnameSpec])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"296bc30a778a10b1982170c252f6aac30022cedf"},"cell_type":"markdown","source":"### Family Member Count"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"43018e9b05ab437e0bb7b39564a15ede326bf2c5"},"cell_type":"code","source":"df_lastname_count = df_familynames.groupby('Lastname', as_index=False).count()\n\n# dropping all columns except for the first one\ndf_lastname_count = df_lastname_count.drop([1,2,3,4,5,6,7,8,9,10,11,12,13], axis=1)\ndf_familynames = df_familynames.drop([0,1,2,3,4,5,6,7,8,9,10,11,12,13], axis=1)\n\n# Joining the grouped by lastname counts to the PassengersId\ndf_familynames = df_familynames.join(df_lastname_count.set_index('Lastname'), on='Lastname')\n\n#renaming column\ndf_familynames = df_familynames.rename(columns = {0: \"Number_of_Familymembers\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d89bb3d2dd195de28e1995c7e7aa2a391b94ba6"},"cell_type":"code","source":"# Mergin the Lastnames and their family member count to original df\ndf_all = pd.merge(df_all , df_familynames, right_index=True, left_index=True)\n# Removing the trailing comma\ndf_all['Lastname'] = df_all['Lastname'].str.rstrip(',')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"652a1d1a8fdfcf8f2778087b42446aefedf38111"},"cell_type":"markdown","source":"Everyone who is travelling alone according to the ***Family*** feature, will be set with 1 for ***Number_of_Familymembers*** feature."},{"metadata":{"trusted":true,"_uuid":"456c03a54ce841a0b7e077f96f2b43735b088afd"},"cell_type":"code","source":"# Set Number_of_Familymembers = 0 when traveling alone\ndf_all.loc[df_all['Family'] == 0,'Number_of_Familymembers'] = 1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55020ebdd9bb21e61cdfac2e3f27dca14735b117"},"cell_type":"markdown","source":"# Feature Distribution"},{"metadata":{"_uuid":"50e3ec0f63afa947d42fbcb97c6e32804a75ac31"},"cell_type":"markdown","source":"## Gender Distribution"},{"metadata":{"trusted":true,"_uuid":"92f3a1c3d02355f655c7e452fb22f31e6ea52748","_kg_hide-input":true},"cell_type":"code","source":"# Gender distribution\ndf_all.groupby(['Parch']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72219f07d4928ff88b9205d044b34c9d16d40eac"},"cell_type":"markdown","source":"The feature **Cabin** seems to have some missing values."},{"metadata":{"_uuid":"9feacfd5ba2bc12dae3da0a52a31f5dd70af9e44"},"cell_type":"markdown","source":"## Gender Distribution by Ticket Class\nFollowing will show the gender distribution by ticket class."},{"metadata":{"trusted":true,"_uuid":"68baf6b3e3f5a26e9152a195334b58da9feca64e","_kg_hide-input":true},"cell_type":"code","source":"# Visualizing pie chart\n\nfig, ax = plt.subplots(figsize=(10,7))\n\n# Size and explsion\nsize_out = 3\nsize_in = 1\nexplode_out = (0.2,0.2)\nexplode_in = (0.3,0.3,0.3,0.3,0.3,0.3)\n\ncmap = plt.get_cmap('tab20c')\n\nouter_colors = cmap(np.array([8,0]))\ninner_colors = cmap(np.array([11,10,9,3,2,1]))\n\npatches1, texts1, autotexts1 = ax.pie(df_all.groupby(['Sex']).count().Name, radius=3, colors=outer_colors,\n       labels=df_all.groupby(['Sex']).count().Name.index,autopct='%1.1f%%',pctdistance=0.85,\n       wedgeprops=dict(width=size_out, edgecolor='black'),\n       explode = explode_out)\n\npatches2, texts2, autotexts2 = ax.pie(df_all.groupby(['Sex','Pclass']).count().Name, radius=2, colors=inner_colors,\n       labels=[1,2,3,1,2,3],autopct='%1.1f%%', labeldistance=0.88,pctdistance=0.55,\n       wedgeprops=dict(width=size_in, edgecolor='black'),\n      explode = explode_in)\n\n# Centre Cirle\ncentre_circle = plt.Circle((0,0),1.5,color='black', fc='white',linewidth=0)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n#plt.rcParams['font.size'] = 10.0\n#plt.rc_context\n\n\n# Define the labels on the outer plot\nfor t in texts1:\n    t.set_size('large')\nfor t in autotexts1:\n    t.set_size('large')\n#autotexts1[0].set_color('y')\n\n\n# Define the labels on the inner plot\nfor t in texts2:\n    t.set_size('large')\nfor t in autotexts2:\n    t.set_size('large')\n#autotexts2[0].set_color('d')\n\n\n# Setting legend\nax.legend(loc='lower right', bbox_to_anchor=(0.7, 0., 0.5, 0.5), shadow=1,title='Legend',\n          handletextpad=1, labelspacing=0.5 , fontsize='12', labels=['female','male','1. class','2. class', '3. class','1. class','2. class', '3. class'])\n\n\nax.set(aspect=\"equal\", title='Gender Distribution')\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4adda7f3e4de7c9cededc244cfb403fff49532b6"},"cell_type":"markdown","source":"Here you can see the different gender distributions to the ticket classes 1 to 3."},{"metadata":{"trusted":true,"_uuid":"dca910364c16bcf3daf307ba31859b22fcfdbc50"},"cell_type":"markdown","source":"## Age Distribution by Ticket Class\nNow the age distribution per ticket class will be visualized to get a better overview about the ages in each ticket class."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"36bd58c103f360c41c3806bfb0b763f78683d430"},"cell_type":"code","source":"# Sclicing the three classes\ndf_firstclass_ages = df_all[df_all.loc[:,'Pclass'] == 1]['Age'].copy()\ndf_secondclass_ages = df_all[df_all.loc[:,'Pclass'] == 2]['Age'].copy()\ndf_thirdclass_ages = df_all[df_all.loc[:,'Pclass'] == 3]['Age'].copy()\n\n# Combining all classes in an array\ndf_all_class_ages =[df_firstclass_ages.values,\n                    df_secondclass_ages.values,\n                    df_thirdclass_ages.values]\n\n# Font dictionary\nfont = {'color':  'black',\n        'weight': 'normal',\n        'size': 18,\n}\n\n# Building the figure and the axes for the plot\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 6) )\n\n# plot violin plot\nparts = axes.violinplot(df_all_class_ages\n                   ,showmeans=False,\n                    showmedians=True)\naxes.set_title('Age Distribution per Class', fontdict=font, fontsize=25)\n\n# Styling every violin in the graph\nfor pc in parts['bodies']:\n    pc.set_facecolor('#FF8C00')\n    pc.set_edgecolor('#000000')\n    pc.set_linewidth(2)\n    pc.set_alpha(0.7)\n\n\n# adding horizontal grid lines\naxes.yaxis.grid(True)\naxes.set_xticks([y + 1 for y in range(len(df_all_class_ages))])\naxes.set_xlabel('Class',fontdict=font, labelpad=20, size=20)\naxes.set_ylabel('Age', fontdict=font,labelpad=20, size=20)\n\n\naxes.vlines(1, df_firstclass_ages.describe()['25%'], df_firstclass_ages.describe()['75%'], color=['#000000'], linestyle='-', lw=5)\naxes.vlines(2, df_secondclass_ages.describe()['25%'], df_secondclass_ages.describe()['75%'], color=['#000000'], linestyle='-', lw=5)\naxes.vlines(3, df_thirdclass_ages.describe()['25%'], df_thirdclass_ages.describe()['75%'], color=['#000000'], linestyle='-', lw=5)\n#axes.vlines(2, whiskersMin, whiskersMax, color='k', linestyle='-', lw=1)\n\n# add x-tick labels\nplt.setp(axes, xticks=[y + 1 for y in range(len(df_all_class_ages))],\n         xticklabels=['First', 'Second','Third'])\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4ac00273d417b85bf0c16858991fd3ed51e02ca7"},"cell_type":"markdown","source":"The first class has a wide range of passengers with a mean age of nearly 40. This looks different in the second class. The passengers have at least the same range of ages but the mean age here is more in the area of 30 years. The same is with third class but here is the age range not so high as with the other classes. It's even much smaller outside the mean age of 25. The most passengers in the third class are around 25 years old. "},{"metadata":{"_uuid":"c2544de3cc946022d79d9e7f05339d2887cf7867"},"cell_type":"markdown","source":"## Salutation Distribution\n"},{"metadata":{"trusted":true,"_uuid":"5c453b4a8d98086598c083c1a01b8928f5f1d04d","scrolled":false},"cell_type":"code","source":"# Grouping by and reset index\ndf_sal_distr = df_all.groupby('Salutation').count()\ndf_sal_distr.reset_index(level=0, inplace=True)\ndf_sal_distr = df_sal_distr[['Salutation','Pclass']]\n#rename the column\ndf_sal_distr = df_sal_distr.rename(columns = {'Pclass':\"Salutation_Count\"})\n                                              \ndf_sal_distr","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8c7c70ee6278e3ead51fd9673c2e8662197950b"},"cell_type":"markdown","source":"# Survivor Distribution\nIn this chapter, we will focus on the survivor distribution. In comparison to the **Feature Distribution** we will only focus on the training dataset because we only have valid survival information in this dataset. The training dataset only includes 891 and not 1309 as in the **Feature Distribution** chapter, therefore the distribution will look here slightly different.\n"},{"metadata":{"_uuid":"548a3354ab7f5fee3c9af73752c94885577d29e1"},"cell_type":"markdown","source":"## Male and Female Survivor\nAccording to the distribution of 64.5% man and 35.6% women, we will now show the distribution of survivors broken down by gender. This distribution only shows the survival data for 891 passengers (from training data) and not for all ship passengers!\n"},{"metadata":{"trusted":true,"_uuid":"b620dd81d78c7298a976da9c0444c341206a5146"},"cell_type":"code","source":"# The dataset with survived information\ndf_survivalinfo = pd.concat([df_all.loc[train_index,:], Survived], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53bcf0490484879594121af831707e1a2504a871","_kg_hide-input":true},"cell_type":"code","source":"# Survival distribution per Sex\ngp_survived_gender = df_survivalinfo.groupby(['Survived','Sex'])['Name'].count()[1]\n\n# Gender Survival\ngp_gender_survived = df_survivalinfo.groupby(['Sex','Survived']).count()['Name']\n\n# Survival distribution Y/N \ngp_survived_yn = df_survivalinfo.groupby(['Survived']).agg('count')['Name']\n\n# Survival total female / male \ngp_survival_total = df_survivalinfo.groupby(['Sex','Survived']).count().xs('Name', axis=1)\n\n\n# Labels and size based on survival group by (df_survivalinfo)\n#labels_suvinf = ['not suvived','survived']\nsizes_suvinf = [gp_survived_yn[y] for y in range(len(gp_survived_yn))]\nexplode_suvinf = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\n# Labels and size based on gender group by (gp_sex_survived)\nlabels_sexinf = [gp_survived_gender.index[y] for y in range(len(gp_survived_gender.index))]\nsizes_sexinf = [gp_survived_gender[y] for y in range(len(gp_survived_gender))]\nexplode_sexinf = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\n# Labels and size MALE Survivor Distribution\nlabels_maleinf = [gp_gender_survived['male'].index[y] for y in range(len(gp_gender_survived['male'].index))]\nsizes_maleinf = [gp_gender_survived['male'][y] for y in range(len(gp_gender_survived['male']))]\nexplode_maleinf = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\n# Labels and size FEMALE Survivor Distribution\nlabels_femaleinf = [gp_gender_survived['female'].index[y] for y in range(len(gp_gender_survived['female'].index))]\nsizes_femaleinf = [gp_gender_survived['female'][y] for y in range(len(gp_gender_survived['female']))]\nexplode_femaleinf = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\n# Labels and size total survival data\nlabels_totalsuvinf = [gp_survival_total.index[y] for y in range(len(gp_gender_survived.index))]\nsizes_totalsuvinf = [gp_survival_total[y] for y in range(len(gp_survival_total))]\nexplode_totalsuvinf = (0.3, 0.0, 0.1, 0.0)  # not survived (fm), survived(fm), not survived(m), survived(m)\n\n\n# Font dictionary\nfont = {'color':  'black',\n        'weight': 'normal',\n        #'size': 15,\n        'fontsize':15\n}\n\n# Color maps for the pies\ncmap = plt.get_cmap('tab20c')\nsurvivedcolor = cmap(np.array([5,1]))\nsurvivorallcolor = cmap(np.array([9,10,0,1]))\n\n# Figure and axes of the plot / 4 * 2 plots \ngridsize = (4,2)\nfig1 = plt.figure(figsize=(14,10))\nax1 = plt.subplot2grid(gridsize, (0,0))\nax2 = plt.subplot2grid(gridsize, (0,1))\nax3 = plt.subplot2grid(gridsize, (1,0))\nax4 = plt.subplot2grid(gridsize, (1,1))\nax5 = plt.subplot2grid(gridsize, (2,0), colspan= 2, rowspan= 2)\n\n## fig1 configs\nfig1.suptitle('Distributions of 891 Passengers (Trainingset)', fontsize=25)\n\n## ax1 \n# Define first pie for survival true falls\nax1.pie(sizes_suvinf, \n        explode=explode_suvinf,\n        #labels=labels_suvinf,\n        autopct='%1.1f%%',\n        shadow=True, startangle=90,\n        colors=survivedcolor,\n        labeldistance=1.15,\n        pctdistance=0.55\n       )\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n#ax1.fontdict=font\nax1.legend(loc='upper left',fontsize='12',labels=('not survived', 'survived'))\nax1.set_title('Survivor Distribution', fontdict=font, fontsize=20)\n\n## ax2 \n# Define second pie for sex to survival\nax2.pie(sizes_sexinf, \n        explode=explode_sexinf,\n        #labels=labels_sexinf,\n        autopct='%1.1f%%',\n        shadow=True, startangle=90,\n        colors=outer_colors,\n        labeldistance=1.15,\n        pctdistance=0.55)\nax2.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n#ax2.fontdict=font\nax2.legend(loc='upper right', fontsize='12', labels=labels_sexinf )\nax2.set_title('Gender Survivors', fontdict=font, fontsize=20)\n\nax3.pie(sizes_maleinf, \n        explode=explode_maleinf,\n        #labels=labels_sexinf,\n        autopct='%1.1f%%',\n        shadow=True, startangle=90,\n        colors=survivedcolor,\n        labeldistance=1.15,\n        pctdistance=0.55)\n#ax3.fontdict=font\nax3.legend(loc='lower left', fontsize='12', labels=['not survived','survived'] )\nax3.set_title('Male Survivors', fontdict=font, fontsize=20)\nax3.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n\nax4.pie(sizes_femaleinf, \n        explode=explode_femaleinf,\n        #labels=labels_sexinf,\n        autopct='%1.1f%%',\n        shadow=True, startangle=90,\n        colors=survivedcolor,\n        labeldistance=1.15,\n        pctdistance=0.55)\n#ax4.fontdict=font\nax4.legend(loc='lower right', fontsize='12', labels=['not survived','survived'] )\nax4.set_title('Female Survivors', fontdict=font, fontsize=20)\nax4.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n\nax5.pie(sizes_totalsuvinf, \n        explode=explode_totalsuvinf,\n        #labels=labels_totalsuvinf,\n        autopct='%1.1f%%',\n        shadow=True, startangle=90,\n        colors=survivorallcolor,\n        labeldistance=1.15,\n        pctdistance=0.55)\nax5.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n#ax4.fontdict=font\nax5.legend(loc='lower right', fontsize='12', labels=['female not survived','female survived','male not survived','male survived'] )\nax5.set_title('Overall Survivors', fontdict=font, fontsize=20)\nax5.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be68f05bc883b9d175a672bb342233ccd9cea03a"},"cell_type":"markdown","source":"In the total of the 891 passengers, there are 62% that did not make it out of the tragedy and died in the open water.  The survivors are divided into 68.1% women and 31.9% man. It is clearly visible that a lot of female passengers have been rescued. If we look closer into the male passenger's data (third pie chart: \"Male Survivors), 81% of the male passengers did not survive. In contrast, 74% of female passengers survived. \n\nThe last pie in the middle shows an overall view of the female/male survival statistics."},{"metadata":{"_uuid":"b0aa0be10edd53e3ba15be9fe246f8cbe65e8c31"},"cell_type":"markdown","source":"## Distribution of  Ticket Class to Survival\n"},{"metadata":{"trusted":true,"_uuid":"5ac4f79f3b7b5a9064716ede0249a180310a8c25","_kg_hide-input":true},"cell_type":"code","source":"# Grouped by Survived and Pclass\ngp_survpclass = df_survivalinfo.groupby(['Survived','Pclass'])['Name'].count()\n\ngridsize = (1,2)\nfig1 = plt.figure(figsize=(12,8))\nax1 = plt.subplot2grid(gridsize, (0,0), colspan=2, rowspan=1)\n\n# Bar chart design\nbar_width = 0.35\ncmap = plt.get_cmap('tab20b')\n\nsurvbarcol = cmap(np.array([7]))\nnsurvbarcol = cmap(np.array([12]))\n\nbarindex = gp_survpclass[0].index   # Group by index of bar plot data\nxtickslables = [barindex[y] for y in range(len(gp_survpclass[0].index))]\n\n\n\nax1.bar(barindex\n        ,gp_survpclass[0].values\n        ,bar_width\n        ,color=nsurvbarcol\n        )\n\nax1.bar(barindex + bar_width\n        ,gp_survpclass[1].values\n        ,bar_width\n        ,color=survbarcol)\n\nax1.set_xlabel('Ticket Class')\nax1.set_ylabel('Passenger Count')\nax1.set_title('Suvivors per Ticket Class')\nax1.set_xticks(barindex + bar_width / 2)\nax1.set_xticklabels(xtickslables)\nax1.legend(labels=('not survived','survived'))\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad25094549300d933eb11777e553b250386439d0"},"cell_type":"code","source":"ptbl = pd.DataFrame.pivot_table(df_survivalinfo, values=['Fare', 'Survived'], index=['Pclass'],\n                     aggfunc={'Survived': ['sum'], 'Fare': [min,max,np.mean]})\n\nptbl\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33cf1659a02f93f92cd63abcdf594ffff7a43f79","_kg_hide-input":true},"cell_type":"code","source":"# seaborn's kdeplot, plots univariate or bivariate density estimates.\n#Size can be changed by tweeking the value used\nsns.FacetGrid(df_survivalinfo.loc[:,['Survived','Pclass']], hue=\"Survived\", height=5).map(sns.kdeplot, \"Pclass\").add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbd5732cd2aeaabde22bc888e64443bfc1f0ad6b"},"cell_type":"markdown","source":"## Survivors by Salutation"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"93db0ec5826079d3c881d4ac3b26f25d89824ba7"},"cell_type":"code","source":"# Group by salutation and survival\ndf_survival_sal = df_survivalinfo.groupby(['Salutation','Survived'], as_index=False)['Name'].count()\n\n# Rename column\ndf_survival_sal = df_survival_sal.rename(columns={\"Name\": \"Total\"})\n\n# Aggregate/Count all Salutations\ndf_survival_sal_total = df_survival_sal.groupby('Salutation', as_index=False)['Total'].agg(sum)\n\n# Not survived Salutations incl. renaming of Total column for join\ndf_survival_sal_nsuv = pd.DataFrame(df_survival_sal[df_survival_sal['Survived'] == 0])\ndf_survival_sal_nsuv = df_survival_sal_nsuv.rename(columns={'Total':'Total_notSurvived'})\ndf_survival_sal_nsuv = df_survival_sal_nsuv[['Salutation','Total_notSurvived']]\n\n\n# Survived salutations incl. renaming of Total column for join\ndf_survival_sal_suv = pd.DataFrame(df_survival_sal[df_survival_sal['Survived'] == 1])\ndf_survival_sal_suv = df_survival_sal_suv.rename(columns={'Total':'Total_Survived'})\ndf_survival_sal_suv = df_survival_sal_suv[['Salutation','Total_Survived']]\n\n# Joining all salutation survival information together\ndf_survival_sal_total = df_survival_sal_total.join(df_survival_sal_suv.set_index('Salutation'), on='Salutation', how='outer')\ndf_survival_sal_total = df_survival_sal_total.join(df_survival_sal_nsuv.set_index('Salutation'), on='Salutation', how='outer')\n\n# Fill the NaN with zeros\ndf_survival_sal_total = df_survival_sal_total.fillna(value=0)\n\ndf_survival_sal_total","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b32500116083b0942df440a0603961f35516ed01"},"cell_type":"markdown","source":"Here we see the distribution of the Salutation based on its total count and how its distribution according to the survived ones and not survived ones. This will partly be visualized in the following chart."},{"metadata":{"trusted":true,"_uuid":"d626851b138b0c1f9df0156a1917c2c5a013809a","_kg_hide-input":true},"cell_type":"code","source":"sns.set(style=\"whitegrid\")\n\n# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(6, 15))\n\n# Load the example car crash dataset\n#crashes = sns.load_dataset(\"car_crashes\").sort_values(\"total\", ascending=False)\nsalutations = df_survival_sal_total\n\n# Plot the total salutations\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"Total\", y=\"Salutation\", data=salutations,\n            label=\"Total\", color=\"b\")\n\n## Plot the survivals\nsns.set_color_codes(\"muted\")\nsns.barplot(x=\"Total_Survived\", y=\"Salutation\", data=salutations,\n            label=\"Survived\", color=\"b\")\n\n# Add a legend and informative axis label\nax.legend(ncol=2, loc=\"lower right\", frameon=True)\nax.set(xlim=(0, 350), ylabel=\"\",\n       xlabel=\"Passengers Salutation (Total, Survived)\")\nsns.despine(left=True, bottom=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4d7fc698c1db50f8a70b110544eac2ff61834d7"},"cell_type":"markdown","source":"We can see there are a lot of passengers with the salutation Mr/Mrs (of course), therefore the other ones look quite small or even barely visible through a bar. The light blue bars show the total amount of passengers for the salutation and the dark blue ones show the amount of surviving ones. The “Mr.” has got a lot of loss, the “Miss.” in comparison were luckier. If we look at the dark blue bars and compare “Mr.” and “Mrs.”, it seems there isn't much more “Mrs.” that survived than the “Mr.”. This is deceptive because not all \"Mr.\" were actually counted for the \"Mr.\" value. Think about \"Dr.\" or \"Rev.\", these are all salutations that normally refer to the title \"Mr.\"."},{"metadata":{"_uuid":"e6c56649d2147f9a0cecfea08792a4dd4217f1ba"},"cell_type":"markdown","source":"## Family Survival\n"},{"metadata":{"trusted":true,"_uuid":"119bc265b370d291952ccb29a198c40b35890e3c"},"cell_type":"code","source":"# Count of groupy\nN = 9\n\n# Group seperation by Survived Not-Survived\ng_sur_Familymembers = df_survivalinfo[df_survivalinfo.loc[:,'Survived'] == 1].groupby('Number_of_Familymembers').count().Survived\ng_nsur_Familymembers = df_survivalinfo[df_survivalinfo.loc[:,'Survived'] == 0].groupby('Number_of_Familymembers').count().Survived\n\nsur_Std = g_sur_Familymembers.std(axis=0) \nsur_Mean = g_sur_Familymembers.mean(axis=0) \nnsur_Std = g_nsur_Familymembers.std(axis=0) \nnsur_Mean = g_nsur_Familymembers.mean(axis=0) \n\n\n# the x locations for the groups\nind  = np.arange(N)\n\nwidth = 0.75       # the width of the bars: can also be len(x) sequence\n\np1 = plt.bar(ind, g_sur_Familymembers, width, yerr=sur_Mean)\np2 = plt.bar(ind, g_nsur_Familymembers, width,\n             bottom=g_sur_Familymembers, yerr=nsur_Mean)\n\n\nplt.ylabel('Passengers')\nplt.xlabel('Number Family Members')\nplt.title('Passenger Survival by Family Members')\nplt.xticks(ind, ('1', '2', '3', '4', '5', '6', '7', '8', '11'))\nplt.yticks(np.arange(-130, 670, 25))\nplt.legend((p1[0], p2[0]), ('Survived', 'Not Survived'))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"996414c957f1552ac16dfd9621ed00f4927a8568"},"cell_type":"markdown","source":"### Family Survivor by Familymember"},{"metadata":{"trusted":true,"_uuid":"eb14a055b8cd3830af436336e3a68e27f2e25802"},"cell_type":"code","source":"fammem_survived = df_survivalinfo['Number_of_Familymembers']\n#[df_survivalinfo['Survived'] == 1]\n#fare_survived = df_survivalinfo['Fare']\n\ninputfeature = df_survivalinfo[df_survivalinfo['Survived'] == 1 ]['Number_of_Familymembers']\n\n#df_survivalinfo['Number_of_Familymembers']\n\ndf_survivalinfo[df_survivalinfo['Survived'] == 1 ]\n\nmu = inputfeature.describe()['mean']  # mean of distribution\nsigma = inputfeature.describe()['std']  # standard deviation of distribution\nx = mu + sigma * inputfeature.values\n\nnum_bins = 11\n\nfig, ax = plt.subplots(figsize=(12,7))\n\n# the histogram of the data\nn, bins, patches = ax.hist(inputfeature,num_bins, density=1)\n\n# add a 'best fit' line\ny = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n     np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\nax.plot(bins, y, '--')\nax.set_xlabel('Number of Familymebers')\nax.set_ylabel('Probability density')\nax.set_title(r'Histogram of Family Member Density: $\\mu='+ str(mu) +'$, $\\sigma= $'+  str(sigma))\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a8676e57706902d0e643907ede4a3cc463e55ee"},"cell_type":"code","source":"sns.set(style=\"whitegrid\")\n\n# Draw a nested barplot to show survival for class and sex\ng = sns.catplot(x=\"Number_of_Familymembers\", y=\"Survived\", data=df_survivalinfo,\n                #x=\"Pclass\", y=\"Survived\", hue='Number_of_Familymembers', data=df_survivalinfo,\n                height=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng.set_ylabels(\"survival probability\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"882cdddbd73e4ab59553876c584f60efd91eda6e"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"9c3620a10c81100eb9d4a61dc15066cd75f076e6"},"cell_type":"markdown","source":"## Gaussian Distrbution for Fare "},{"metadata":{"trusted":true,"_uuid":"05b8e14d1a2a2caf5c57f85e9cdb5a3460e41eb2"},"cell_type":"code","source":"fare_survived = df_survivalinfo[df_survivalinfo['Survived'] == 1]['Fare']\n#fare_survived = df_survivalinfo['Fare']\n\ninputfeature = fare_survived\n\nmu = inputfeature.describe()['mean']  # mean of distribution\nsigma = inputfeature.describe()['std']  # standard deviation of distribution\nx = mu + sigma * inputfeature.values\n\nnum_bins = 50\n\nfig, ax = plt.subplots(figsize=(12,7))\n\n# the histogram of the data\nn, bins, patches = ax.hist(inputfeature,num_bins, density=1)\n\n# add a 'best fit' line\ny = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n     np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\nax.plot(bins, y, '--')\nax.set_xlabel('Fare Price')\nax.set_ylabel('Probability density')\nax.set_title(r'Histogram of Fare: $\\mu=$'+ str(mu) +', $\\sigma=$ ' + str(sigma))\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ff1a19ee1db5dca92abef23197fe9e1902f9a1a"},"cell_type":"markdown","source":"## Survivor by Fare Price\n"},{"metadata":{"trusted":true,"_uuid":"73cbec0ceb642f4a83f4faf69f334d8747a18456"},"cell_type":"code","source":"# Pivot for age and fare\nptbl_survived = pd.DataFrame.pivot_table(df_survivalinfo, values=['Fare', 'Age', 'Survived'], index=['Sex', 'Pclass'],\n                     aggfunc={'Fare': np.mean,'Age': [min, max, np.mean], 'Survived': ['sum']})\nptbl_survived","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb36454c556463bb2590d11136e0f033181fc448"},"cell_type":"markdown","source":"## Overall Bivariate Relation"},{"metadata":{"trusted":true,"_uuid":"417b567643d6958c36b2b8aceeff0604979fa20a","_kg_hide-input":true},"cell_type":"code","source":"sns.pairplot(df_survivalinfo, hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e65defcddab1798b2eb0566144e5c88b082576e"},"cell_type":"markdown","source":"# K-Nearest Neighbors based Model and Prediction\nBased on the model improvement for the KNN, the following features will be dropped and therefore not included in the prediction:\n- Name - **Cannot be used in this kind of model (KNN). **\n- Cabin - **Not filled enough, too many null values.**\n- Fare - **Do not lead to any further knowledge for the model.**\n- Ticket - **Do not lead to any further knowledge  for the model.**"},{"metadata":{"trusted":true,"_uuid":"cdbc7b7d89a0d569e09ec54b6bc429bc87d17de0"},"cell_type":"code","source":"# K-Nearest Neighbours imports\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Using here the full dataset because train and test set must have the same shape when using it with model\n\n# One hot encoding for the categorical data\ndf_all_knn_hot = df_all.copy()\ndf_all_knn_hot = df_all_knn_hot.drop(['Name','Cabin','Fare','Ticket','Lastname'], axis=1)\ndf_all_knn_hot = pd.get_dummies(df_all_knn_hot, columns=['Sex','Salutation','Embarked'])\n\n\n\n# Train test split only on test dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_all_knn_hot.loc[train_index,:], \n                                                    Survived, test_size = 0.30, random_state = 45)  ## 50  25 ## 0.25 25 , 25","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48b4d6e947150e1b0152f15dd238de024f01eeba"},"cell_type":"markdown","source":"## Visualize the Best Model Fit for KNN"},{"metadata":{"trusted":true,"_uuid":"747dd9d9e68466e278544172883366d4b73600ea"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# creating odd list of K for KNN\nmyList = list(range(1,50))\n\n# subsetting just the odd ones\nneighbors = list(myList)\n\n\n# empty list that will hold cv scores\ncv_scores = []\n\n# perform 10-fold cross validation\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n    cv_scores.append(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cdf195154d36790ea9ff47593244cc8d9a4963d"},"cell_type":"code","source":"# changing to misclassification error\nMSE = [1 - x for x in cv_scores]\n\n# determining best k\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint (\"The optimal number of neighbors is %d\" % optimal_k)\n\n# plot misclassification error vs k\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4db25112d40f5bf63b89f3af99a0c62d2f951ffb"},"cell_type":"markdown","source":"## Train the KNN\nAs shown in the previous figure, the best fit for neighbors seems to be 5. For the model training I will therefore use 5 neighbors. "},{"metadata":{"trusted":true,"_uuid":"73e387e5e03e137fd09de21dbf8d8794306247f7"},"cell_type":"code","source":"# K-Nearest Neighbours\nfrom sklearn import metrics\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n\n\nX_train = X_train\ny_train = y_train\n\n# 3 Neighbors used from the misclassification error calculation\nKNNC = KNeighborsClassifier(n_neighbors=3)\nKNNC.fit(X_train, y_train)\n\ny_pred = KNNC.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred, target_names=['0','1']))\n\n\nprint (\"Models accuracy score: \", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"125e377f896f3f304920b6850caa2ec261cd6a73"},"cell_type":"code","source":"from yellowbrick.classifier import ClassificationReport\n\nclasses = [\"will not suvive\", \"will survive\"]\n\n# Instantiate the classification model and visualizer\nvisualizer = ClassificationReport(KNNC, classes=classes, support=True)\n\nvisualizer.fit(X_train, y_train)  # Fit the visualizer and the model\nvisualizer.score(X_test, y_test)  # Evaluate the model on the test data\ng = visualizer.poof()             # Draw/show/poof the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2646407f80038c5fc46769de72649ac2b3fef91e"},"cell_type":"code","source":"# Plotting the Precision-Recall curve\ny_proba_train = KNNC.predict_proba(X_train)[:, 1]\np, r, t = metrics.precision_recall_curve(y_train, y_proba_train)\n\nplt.plot(r, p)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57d79d57463da1347c883a042a8af3ad59032fd1"},"cell_type":"markdown","source":"## Execution on Testset with KNN"},{"metadata":{"trusted":true,"_uuid":"c1a046c2725a1fbffcfe5e55d8f9439a41d13804"},"cell_type":"code","source":"titanic_submission = pd.DataFrame({'PassengerId':df_all_knn_hot.loc[test_index,:].index,\n                                   'Survived':KNNC.predict(df_all_knn_hot.loc[test_index,:])})\ntitanic_submission.PassengerId = titanic_submission.PassengerId.astype(int)\ntitanic_submission.Survived = titanic_submission.Survived.astype(int)\n\n# Overview how much suvived with k-nearest neighbor approach\ntitanic_submission.groupby('Survived').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33270dc7183261a3abebd32d66868a839121636e","scrolled":true},"cell_type":"code","source":"titanic_submission.head(15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efe0646209d7fef751aee7780974b3f65a622ccb"},"cell_type":"markdown","source":"Here you see an overview about the prediction on the test dataset."},{"metadata":{"trusted":true,"_uuid":"58e736639f87f968f3145d2981376b0c308bd6a9"},"cell_type":"code","source":"# Submission to a csv file for competition upload.\ntitanic_submission.to_csv(\"titanic_submission_knn_4.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"450721c18c33ca85c01ca4b43d4acb6fff060e47"},"cell_type":"markdown","source":"# Random Forest based Model and Prediction \n"},{"metadata":{"trusted":true,"_uuid":"20479a688e0a930f1ffcf2c12262968b7a616e16"},"cell_type":"code","source":"# Random Forest imports\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# Using here the full dataset because train and test set must have the same shape when using it with model\n\n# One hot encoding for the categorical data\ndf_all_rf_hot = df_all.copy()\ndf_all_rf_hot = df_all_rf_hot.drop(['Name','Cabin','Fare','Ticket','Lastname'], axis=1)\ndf_all_rf_hot = pd.get_dummies(df_all_rf_hot, columns=['Sex','Salutation','Embarked'])\n\n\n\n# Train test split only on test dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_all_rf_hot.loc[train_index,:], \n                                                    Survived, test_size = 0.30, random_state = 45)  ## 50  25 ## 0.25 25 , 25","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fd907312b0f6b52517fc64603222ee0e6da65b5"},"cell_type":"markdown","source":"## Visualize the Best Model Fit for RFC"},{"metadata":{"trusted":true,"_uuid":"4dbcb65f53a5ec110270740930035c5a86209196","_kg_hide-input":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# creating odd list of K for KNN \nmyList = list(range(1,30))\n\n# subsetting just the odd ones\nlevels = list(myList)\n\n\n# empty list that will hold cv scores\ncv_scores = []\n\n# perform 10-fold cross validation\nfor l in levels:\n    rfc = RandomForestClassifier( n_estimators=100)\n    scores = cross_val_score(rfc, X_train, y_train, cv=5, scoring='recall')\n    cv_scores.append(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b63967b3c321475fea6bd0db24159261d5e2e48a","_kg_hide-input":true},"cell_type":"code","source":"# changing to misclassification error \nMSE = [1 - x for x in cv_scores]\n\n# determining best k\noptimal_l = levels[MSE.index(min(MSE))]\nprint (\"The optimal level depth is %d\" % optimal_l)\n\n# plot misclassification error vs k\nplt.plot(levels, MSE)\nplt.xlabel('Level l')\nplt.ylabel('Misclassification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecd12fb44530dfec17dd45303c49d18f4e2efe44"},"cell_type":"markdown","source":"## Train the Random Forest\nAs shown in the previous figure, the best fit for neighbors seems to be 5. For the model training I will therefore use 5 neighbors. "},{"metadata":{"trusted":true,"_uuid":"bf76c6a94114e2876819c4f242cbe4ecf987dfa3","_kg_hide-input":false},"cell_type":"code","source":"# K-Nearest Neighbours\nfrom sklearn import metrics\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n#from sklearn.ensemble import ExtraTreesClassifier\n#from sklearn.tree import DecisionTreeClassifier\n\n\n\n\nX_train = X_train\ny_train = y_train\n\n# Random forest classifier  \nRFCC = RandomForestClassifier(max_depth=14,\n                              n_estimators=5000)\n#RFCC = RandomForestClassifier(n_estimators=1000)\nRFCC.fit(X_train, y_train)\n\ny_pred = RFCC.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred, target_names=['0','1']))\n\n\nprint (\"Models accuracy score: \", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c44c418e36563ab8473947ca71647305befe4fc","_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"from yellowbrick.classifier import ClassificationReport\n\nclasses = [\"will not suvive\", \"will survive\"]\n\n# Instantiate the classification model and visualizer\nvisualizer = ClassificationReport(RFCC, classes=classes, support=True)\n\nvisualizer.fit(X_train, y_train)  # Fit the visualizer and the model\nvisualizer.score(X_test, y_test)  # Evaluate the model on the test data\ng = visualizer.poof()             # Draw/show/poof the data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6f6aae5e7b863ae010c9f41b2847fda522038ba"},"cell_type":"markdown","source":"## Execution on Testset with Random Forest"},{"metadata":{"trusted":true,"_uuid":"c71a116ce15fd8add93ae9acfa7cb6e5295b8518","_kg_hide-input":true},"cell_type":"code","source":"titanic_submission_rfc = pd.DataFrame({'PassengerId':df_all_rf_hot.loc[test_index,:].index,\n                                   'Survived':RFCC.predict(df_all_rf_hot.loc[test_index,:])})\ntitanic_submission_rfc.PassengerId = titanic_submission_rfc.PassengerId.astype(int)\ntitanic_submission_rfc.Survived = titanic_submission_rfc.Survived.astype(int)\n\n# Overview how much suvived with random forest approach\ntitanic_submission_rfc.groupby('Survived').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"853b9d2fc60a8b8ecaa09d11ba85b641398c466b"},"cell_type":"code","source":"# Submission to a csv file for competition upload.\ntitanic_submission_rfc.to_csv(\"titanic_submission_rfc_5.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network based Model and Prediction\n\nNeural Network Approach to Indicate the Survivors\nIn this chapter, I will use a neural network to predict the survivors. Therefore I will start with the classic approach and the sigmoid function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing sigmoid function\nimport scipy.special\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neural Network Classdefinition"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Neural network class definition\n\nclass neuralNetwork:\n    \n    # Initialize\n    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n        # Number of nodes in each layer\n        self.inodes = inputnodes\n        self.hnodes = hiddennodes\n        self.onodes = outputnodes\n        \n        # Bulding the size of the weight matrices without normal distribution info\n        ## self.wih = (np.random.rand(self.hnodes,self.inodes) - 0.5)\n        ## self.who = (np.random.rand(self.onodes,self.hnodes) - 0.5)\n        \n        # Size of the weight matrice by random sample based on the normal distribution\n        self.wih = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n        \n        #self.whh = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n        \n        self.lrate = learningrate\n        \n        self.activationfunction = lambda x : scipy.special.expit(x);\n        \n        pass \n    \n    def train(self, inputlist, targetlist):\n        inputs = np.array(inputlist, ndmin=2).T\n        targets = np.array(targetlist, ndmin=2).T\n        \n        hidden_inputs = np.dot(self.wih, inputs)\n        hidden_outputs = self.activationfunction(hidden_inputs)\n        \n        # hidden layer 2\n        #hidden2_inputs = np.dot(self.wih, inputs)\n        #hidden2_outputs = self.activationfunction(hidden_inputs)\n            \n            \n        final_inputs = np.dot(self.who, hidden_outputs)\n        final_outputs = self.activationfunction(final_inputs)\n        \n        \n        ## BACKPROPAGATION ##\n        \n        output_errors = targets - final_outputs\n        \n        hidden_errors = np.dot(self.who.T, output_errors)\n        \n        self.who += self.lrate * np.dot((output_errors * final_outputs * (1 - final_outputs)), np.transpose(hidden_outputs))        \n        \n        self.wih += self.lrate * np.dot((hidden_errors * hidden_outputs * (1 - hidden_outputs)), np.transpose(inputs))\n        \n        pass\n    \n    def query(self, inputs_list):\n        inputs = np.array(inputs_list, ndmin = 2).T\n        \n        hidden_inputs = np.dot(self.wih, inputs)\n        hidden_outputs = self.activationfunction(hidden_inputs)\n        \n        final_inputs = np.dot(self.who, hidden_outputs)\n        final_outputs = self.activationfunction(final_inputs)\n        \n        return final_outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List to record the different configurations\n# The idea is to store: nn_accuracy,learningrate,hidden_nodes,epochs\nconf_performance_list = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Initialize Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Main configuration params for the nn\ninput_nodes = 29\nhidden_nodes = 3\noutput_nodes = 2\n\nlearningrate = 0.4\nnn_epochs = 1000     #453000   #49000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize neural network for titanic passengers\ntitanic_nn = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learningrate)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Neural Network\nIn this training chapter, I will first train the model with 75% of the training set and use the remaining 15% to test the results and adjust the configurations (learningrate, hiddenlayer nodes, epochs to train). Right after that, I will train the model again with the remaining 15% of the training data. I do this to hand over the model as much data as possible to predict on the real testset (the testset for the submission) as good as possible."},{"metadata":{},"cell_type":"markdown","source":"### Train Model 75/15"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split ( this function makes it very easy to split the data by percentage) \nX_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(df_all_knn_hot.loc[train_index,:], Survived, test_size = 0.15, random_state = 45) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare data for learning\n# To make everything easier the X_train and the y_train (targets) will be \n# combined in one df again to loop over it easier.\n\nXy_train_nn = pd.concat([X_train_nn, y_train_nn], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the nn multiple (epochs) times.\nepochs = nn_epochs\n\n\nfor e in range(epochs):\n    \n    for row in Xy_train_nn.itertuples():\n        \n        inputs = (np.asfarray(row[1:30]) / 29 * 0.99) + 0.01\n               \n        targets = np.zeros(output_nodes) + 0.01\n        targets[int(row[30])] = 0.99\n             \n        titanic_nn.train(inputs, targets)\n        \n        pass\n    pass\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test Model 75/15\nNow the remaining 15% will be used to test and adjust the model by its configurations."},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenate test set\nXy_test_nn = pd.concat([X_test_nn, y_test_nn], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testrowindex = 2\n\n#retrieving the results\nnn_result = titanic_nn.query(Xy_test_nn[Xy_test_nn.columns[0:29]].values[testrowindex])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scorecard Model 75/15\nIn the following there will be build a scorecard to check whether the neural network has predicted the results correctly or not, this will be realized by a simple true/false sign (1/0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List to store the true and false prediction results of the nn\nscorecard = []\n# List to store the matrix structure results to analyse it afterwards\nmatrixlist = []\n\nfor index,row in Xy_test_nn.iterrows():\n    \n    inputs = row[0:29].values\n    \n    correct_label = row[29]\n    \n    results = titanic_nn.query(inputs)\n    \n    label = np.argmax(results)\n    \n    print('PassengerID:', index, ' - Networks answer: ', label, ' --> Correct answer: ', correct_label)\n    \n    # Append all information to this list to get an overview about networks results and its matrix structure.\n    matrixlist.append([results, label, correct_label])\n\n    \n    if(label == correct_label):\n        scorecard.append(1)\n    else:\n        scorecard.append(0)\n    pass     \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scorecard_array = np.array(scorecard)\nnn_accuracy = scorecard_array.sum() / scorecard_array.size\n\n# Append current network settings to a list \n#   due to several runs of the network I will store here all configurations and \n#   its corresponding accuracy to adjust the model better.\nconf_performance_list.append([nn_accuracy,learningrate,hidden_nodes,epochs])\n\nprint('Accuracy score by \"75/15\"-network: ', nn_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configurations performance list\n#   This list is used to show the different configuration in comparision to the accuracy to fine tune the nn\n#   It will be commented out after I have found out the best fit\n#   Columns: nn_accuracy,learningrate,hidden_nodes,epochs\n\n# conf_performance_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train with Remaining Data (75)/15\nThis part here will train the model with the remaining 15% from our train/test split. The idea behind it is to give the model as much data as possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the nn multiple (epochs) times.\nepochs = nn_epochs\n\n\nfor e in range(epochs):\n    \n    for row in Xy_test_nn.itertuples():\n        \n        inputs = (np.asfarray(row[1:30]) / 29 * 0.99) + 0.01        \n        targets = np.zeros(output_nodes) + 0.01\n        targets[int(row[30])] = 0.99\n  \n        titanic_nn.train(inputs, targets)\n        \n        pass\n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Execution on Testset with Neural Network\nThe trained neural network will now be executed on the Titanic testset to create a submission file afterwards."},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_submission_nn = pd.DataFrame(columns=['PassengerId','Survived'])\n\nfor index,row in df_all_rf_hot.loc[test_index].iterrows():\n    \n    inputs = row[0:29].values\n    \n    results = titanic_nn.query(inputs)\n    \n    label = np.argmax(results)\n        \n    titanic_submission_nn = titanic_submission_nn.append({'PassengerId' : index , 'Survived': label} , ignore_index=True)\n        \n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing the submission file\ntitanic_submission_nn.PassengerId = titanic_submission_nn.PassengerId.astype(int)\ntitanic_submission_nn.Survived = titanic_submission_nn.Survived.astype(int)\n\ntitanic_submission_nn.groupby('Survived').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_submission_nn.to_csv(\"titanic_submission_nn_6.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network with Tensorflow"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split ( this function makes it very easy to split the data by percentage) \nX_train_tfnn, X_test_tfnn, y_train_tfnn, y_test_tfnn = train_test_split(df_all_knn_hot.loc[train_index,:], Survived, test_size = 0.15, random_state = 45) \n\n# scale the values between 0 and 1\nX_train_tfnn = tf.keras.utils.normalize(np.asfarray(X_train_tfnn),axis= -1)\nX_test_tfnn = tf.keras.utils.normalize(np.asfarray(X_test_tfnn),axis= -1)\n\ny_train_tfnn = np.asfarray(y_train_tfnn) \ny_test_tfnn = np.asfarray(y_test_tfnn) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model building\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\nmodel.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\nmodel.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))\n\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy']\n            )\nmodel.fit(X_train_tfnn, y_train_tfnn, epochs = 200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the model and show the accuracy\nval_loss, val_acc = model.evaluate(X_test_tfnn, y_test_tfnn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the model so far\nmodel.save('titanic_survivor_predictor.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the model again\nnew_model = tf.keras.models.load_model('titanic_survivor_predictor.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}