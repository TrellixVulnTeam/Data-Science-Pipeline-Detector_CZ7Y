{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"The divergence between train and test data is very important, because if there is high divergnce between them, our data engineering and model evaluations can not perform good enough. \n\nIn this notebook, I'll try to measure the divergency between train/test data of Titanic and hopefully make some insights about the dataset.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom math import log2\nfrom scipy.spatial import distance\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:58:48.268804Z","iopub.execute_input":"2022-03-22T21:58:48.269133Z","iopub.status.idle":"2022-03-22T21:58:48.274067Z","shell.execute_reply.started":"2022-03-22T21:58:48.269094Z","shell.execute_reply":"2022-03-22T21:58:48.273085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(r'../input/titanic/train.csv')\ntest =  pd.read_csv(r'../input/titanic/test.csv')\nAll = pd.concat([train, test], sort=True).reset_index(drop=True)\nAll.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:46.098365Z","iopub.execute_input":"2022-03-22T21:37:46.098855Z","iopub.status.idle":"2022-03-22T21:37:46.138054Z","shell.execute_reply.started":"2022-03-22T21:37:46.098824Z","shell.execute_reply":"2022-03-22T21:37:46.137183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"markdown","source":"Here the data will prepare to a numerical and workable format. I used the same process and code as in this notebook:\n\nhttps://www.kaggle.com/code/khashayarrahimi94/knn-xgboost-svc-ensemble-with-just-5-feature","metadata":{}},{"cell_type":"code","source":"All[\"Cabin_dumb\"]=All[\"Cabin\"]\nfor i in range(All.shape[0]):\n    if pd.isnull(All[\"Cabin\"][i])== False:\n        All[\"Cabin_dumb\"][i] = All[\"Cabin\"][i][0]\n    else:\n        All[\"Cabin_dumb\"][i] =0\nAll[\"Cabin_dumb\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:46.139244Z","iopub.execute_input":"2022-03-22T21:37:46.139474Z","iopub.status.idle":"2022-03-22T21:37:46.542407Z","shell.execute_reply.started":"2022-03-22T21:37:46.139446Z","shell.execute_reply":"2022-03-22T21:37:46.541589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All['Family'] = All['SibSp']+All['Parch']","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:46.544137Z","iopub.execute_input":"2022-03-22T21:37:46.544373Z","iopub.status.idle":"2022-03-22T21:37:46.551031Z","shell.execute_reply.started":"2022-03-22T21:37:46.544343Z","shell.execute_reply":"2022-03-22T21:37:46.550229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif as MIC\nmi_score = MIC(train.loc[: , ['Age' ,'Pclass','Parch','Fare','SibSp' ]].values.astype('int'),\n               train.loc[: , ['Age']].values.astype('int').reshape(-1, 1))\nFeature2 = ['Age' ,'Pclass','Parch','Fare','SibSp' ]\nMutual_Information_table = pd.DataFrame(columns=['Feature1', 'Feature2', 'MIC'], index=range(5))\nMutual_Information_table['Feature1'] = 'Age'\nfor feature in range(5):\n    Mutual_Information_table['Feature2'][feature] = Feature2[feature]\nfor value in range(5):\n    Mutual_Information_table['MIC'][value] = mi_score[value]\nMutual_Information_table","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:46.552506Z","iopub.execute_input":"2022-03-22T21:37:46.552978Z","iopub.status.idle":"2022-03-22T21:37:46.816195Z","shell.execute_reply.started":"2022-03-22T21:37:46.552935Z","shell.execute_reply":"2022-03-22T21:37:46.815586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"age_by_pclass_sex = round(All.groupby(['Sex', 'Pclass']).median()['Age'])\n\nfor pclass in range(1, 4):\n    for sex in ['female', 'male']:\n        print('Mean age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\nprint('Mean age of all passengers: {}'.format(round(All['Age'].mean())))\n\n# Filling the missing values in Age with the medians of Sex and Pclass groups\nAll['Age'] = All.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(round(x.median())))","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:46.81725Z","iopub.execute_input":"2022-03-22T21:37:46.817594Z","iopub.status.idle":"2022-03-22T21:37:46.839304Z","shell.execute_reply.started":"2022-03-22T21:37:46.817566Z","shell.execute_reply":"2022-03-22T21:37:46.838255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All[All['Embarked'].isnull()]\nAll['Embarked'] = All['Embarked'].fillna('S')","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:46.840448Z","iopub.execute_input":"2022-03-22T21:37:46.840708Z","iopub.status.idle":"2022-03-22T21:37:46.846699Z","shell.execute_reply.started":"2022-03-22T21:37:46.840679Z","shell.execute_reply":"2022-03-22T21:37:46.8461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All[All['Fare'].isnull()]\nmean_fare = All.groupby(['Pclass', 'Parch', 'SibSp']).Fare.mean()[3][0][0]\n# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\nAll['Fare'] = All['Fare'].fillna(mean_fare)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:46.84778Z","iopub.execute_input":"2022-03-22T21:37:46.84834Z","iopub.status.idle":"2022-03-22T21:37:46.863071Z","shell.execute_reply.started":"2022-03-22T21:37:46.848305Z","shell.execute_reply":"2022-03-22T21:37:46.862196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All['Ticket_Frequency'] = All.groupby('Ticket')['Ticket'].transform('count')\nfreq = All.head(891)['Ticket_Frequency'].value_counts().tolist()\nTicket_freq = All.head(891)['Ticket_Frequency'].unique().tolist()\n\ndeath = []\nfor n in Ticket_freq:\n    k = 0\n    for i in range(891):\n        if (All.head(891)['Ticket_Frequency'][i] == n) & (All.head(891)['Survived'][i] == 0):\n            k = k+1\n    death.append(k)    \n     \nsurvive_rate = []\nfor j,w in zip(death,freq):\n    rate = (w-j)/w\n    survive_rate.append(rate)\n\nSurvive_rate_index = {}\nfor u,r in zip(Ticket_freq,survive_rate):\n    Survive_rate_index[u] = r\nSurvive_rate_index","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:46.865654Z","iopub.execute_input":"2022-03-22T21:37:46.865898Z","iopub.status.idle":"2022-03-22T21:37:48.54593Z","shell.execute_reply.started":"2022-03-22T21:37:46.865869Z","shell.execute_reply":"2022-03-22T21:37:48.544454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_ticket_freq = []\nfor i in range(All.shape[0]):\n    new_ticket_freq.append(Survive_rate_index[All['Ticket_Frequency'][i]])\nnew_ticket_freq = pd.DataFrame(new_ticket_freq)\nAll['Ticket_Frequency'] = pd.DataFrame(new_ticket_freq)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:48.547247Z","iopub.execute_input":"2022-03-22T21:37:48.547505Z","iopub.status.idle":"2022-03-22T21:37:48.565739Z","shell.execute_reply.started":"2022-03-22T21:37:48.547475Z","shell.execute_reply":"2022-03-22T21:37:48.564818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name in All[\"Name\"]:\n    All[\"Title\"] = All[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\n\ntitle_replacements = {\"Mlle\": \"Other\", \"Major\": \"Other\", \"Col\": \"Other\", \"Sir\": \"Other\", \"Don\": \"Other\", \"Mme\": \"Other\",\n          \"Jonkheer\": \"Other\", \"Lady\": \"Other\", \"Capt\": \"Other\", \"Countess\": \"Other\", \"Dona\": \"Other\"\n                     ,\"Dr\":\"Other\",\"Rev\":\"Other\", \"Mrs\":\"Woman\",\"Ms\":\"Woman\",\"Miss\":\"Woman\"}\n\nAll.replace({\"Title\": title_replacements}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:48.567194Z","iopub.execute_input":"2022-03-22T21:37:48.567791Z","iopub.status.idle":"2022-03-22T21:37:56.097382Z","shell.execute_reply.started":"2022-03-22T21:37:48.567758Z","shell.execute_reply":"2022-03-22T21:37:56.096552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All = pd.get_dummies(All, columns=['Embarked','Pclass', 'Sex','Cabin_dumb','Title','Family'],\n                     prefix=['Embarked','Pclass', 'Sex','Cabin_dumb','Title','Family'])\nAll.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:56.098793Z","iopub.execute_input":"2022-03-22T21:37:56.099044Z","iopub.status.idle":"2022-03-22T21:37:56.132461Z","shell.execute_reply.started":"2022-03-22T21:37:56.099013Z","shell.execute_reply":"2022-03-22T21:37:56.131594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All.drop(['Ticket','Cabin','Name','PassengerId'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:56.133604Z","iopub.execute_input":"2022-03-22T21:37:56.133812Z","iopub.status.idle":"2022-03-22T21:37:56.141214Z","shell.execute_reply.started":"2022-03-22T21:37:56.133786Z","shell.execute_reply":"2022-03-22T21:37:56.140055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Test Distribution","metadata":{}},{"cell_type":"markdown","source":"Here we have clean and prepare train and test dataset and we define some functions that compute probability distributions of each columns and for train and test pat of datatest and plot them, which enable us to compare these distributions and thei divergencies. ","metadata":{}},{"cell_type":"code","source":"train_cleaned = All.head(891)\ntest_cleaned = All.tail(418)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:56.142702Z","iopub.execute_input":"2022-03-22T21:37:56.143389Z","iopub.status.idle":"2022-03-22T21:37:56.154571Z","shell.execute_reply.started":"2022-03-22T21:37:56.143339Z","shell.execute_reply":"2022-03-22T21:37:56.153616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Prob_train(feature):\n    l = []\n    for j in train_cleaned[train_cleaned.columns[feature]].unique().tolist():\n        l.append((train_cleaned[train_cleaned.columns[feature]].value_counts()[j])/train_cleaned.shape[0])\n    return l\n\ndef Prob_test(feature):\n    l = []\n    for j in test_cleaned[test_cleaned.columns[feature]].unique().tolist():\n        l.append((test_cleaned[test_cleaned.columns[feature]].value_counts()[j])/test_cleaned.shape[0])\n    return l\n    \ndef train_distribution(feature):\n    Feature_prob = {}\n    for u,r in zip(train_cleaned[train_cleaned.columns[feature]].unique().tolist(),Prob_train(feature)):\n        Feature_prob[u] = r\n    return Feature_prob\n        \ndef test_distribution(feature):\n    Feature_prob = {}\n    for u,r in zip(test_cleaned[test_cleaned.columns[feature]].unique().tolist(),Prob_test(feature) ):\n        Feature_prob[u] = r\n    return Feature_prob\n\ndef train_events(feature):\n    return train_cleaned[train_cleaned.columns[feature]].unique().tolist()\n\ndef test_events(feature):\n    return test_cleaned[test_cleaned.columns[feature]].unique().tolist()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:56.155999Z","iopub.execute_input":"2022-03-22T21:37:56.15677Z","iopub.status.idle":"2022-03-22T21:37:56.170662Z","shell.execute_reply.started":"2022-03-22T21:37:56.15673Z","shell.execute_reply":"2022-03-22T21:37:56.169636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Prob_train_normal(feature):\n    l = []\n    for j in All[All.columns[1]].unique().tolist():\n        if list(set(train_cleaned[train_cleaned.columns[feature]].tolist())).__contains__(j) == False:\n            l.append(0.000001)\n        else:\n            l.append((train_cleaned[train_cleaned.columns[feature]].value_counts()[j])/train_cleaned.shape[0])\n    return l\n\ndef Prob_test_normal(feature):\n    l = []\n    for j in All[All.columns[1]].unique().tolist():\n        if list(set(test_cleaned[test_cleaned.columns[feature]].tolist())).__contains__(j) == False:\n            l.append(0.000001)\n        else:\n            l.append((test_cleaned[test_cleaned.columns[feature]].value_counts()[j])/test_cleaned.shape[0])\n    return l","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:37:56.186293Z","iopub.execute_input":"2022-03-22T21:37:56.186612Z","iopub.status.idle":"2022-03-22T21:37:56.197289Z","shell.execute_reply.started":"2022-03-22T21:37:56.186575Z","shell.execute_reply":"2022-03-22T21:37:56.196357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distribution_plot(feature):\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 2))\n    font = {'family':'serif','color':'blue','size':15}\n    axes[0].bar(train_events(feature), Prob_train(feature))\n    axes[0].set(xlabel=train_cleaned.columns[feature], ylabel='Probability')\n    axes[0].set_title(\"Train\",fontdict = font)\n\n    axes[1].bar(test_events(feature), Prob_test(feature))\n    axes[1].set(xlabel=test_cleaned.columns[feature], ylabel='Probability')\n    axes[1].set_title(\"Test\",fontdict = font)\n    return fig.tight_layout() ","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:39:29.581511Z","iopub.execute_input":"2022-03-22T21:39:29.58233Z","iopub.status.idle":"2022-03-22T21:39:29.590173Z","shell.execute_reply.started":"2022-03-22T21:39:29.582284Z","shell.execute_reply":"2022-03-22T21:39:29.589351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfor i in range(All.shape[1]):\n    if i !=4:\n        distribution_plot(i)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:39:33.046404Z","iopub.execute_input":"2022-03-22T21:39:33.046972Z","iopub.status.idle":"2022-03-22T21:39:47.834574Z","shell.execute_reply.started":"2022-03-22T21:39:33.046921Z","shell.execute_reply":"2022-03-22T21:39:47.833578Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Kullback-Leibler Divergence","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"In mathematical statistics, the Kullback–Leibler divergence, ${\\displaystyle D_{\\text{KL}}(P\\parallel Q)}$ (also called relative entropy), is a statistical distance: a measure of how one probability distribution Q is different from a second, reference probability distribution P.\n\nFor discrete probability distributions ${\\displaystyle P}$ and ${\\displaystyle Q}$ defined on the same probability space, ${\\displaystyle {\\mathcal {X}}}$, the relative entropy from ${\\displaystyle Q}$ to ${\\displaystyle P}$ is defined to be:\n\n ${\\displaystyle D_{\\text{KL}}(P\\parallel Q)=\\sum _{x\\in {\\mathcal {X}}}P(x)\\log \\left({\\frac {P(x)}{Q(x)}}\\right)}$\n \n For distributions ${\\displaystyle P}$ and ${\\displaystyle Q}$ of a continuous random variable, relative entropy is defined to be the integral:\n\n ${\\displaystyle D_{\\text{KL}}(P\\parallel Q)=\\int _{-\\infty }^{\\infty }p(x)\\log \\left({\\frac {p(x)}{q(x)}}\\right)\\,dx}$\n \n*Wikipedia*","metadata":{}},{"cell_type":"code","source":"def kl_divergence(p, q):\n    return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KL_divergences = []\nfor i in range(All.shape[1]):\n    KL_divergences.append(kl_divergence(Prob_train_normal(i),Prob_test_normal(i)))\n    print('KL(Prob_train || Prob_test) for %s is: %.3f bits' % (All.columns[i],kl_divergence(Prob_train_normal(i),Prob_test_normal(i))))","metadata":{"execution":{"iopub.status.busy":"2022-03-22T21:53:28.974928Z","iopub.execute_input":"2022-03-22T21:53:28.975252Z","iopub.status.idle":"2022-03-22T21:53:31.80436Z","shell.execute_reply.started":"2022-03-22T21:53:28.975216Z","shell.execute_reply":"2022-03-22T21:53:31.803457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 5))\nplt.plot(All.columns.tolist(), KL_divergences)\nplt.xlabel(\"Feature\")\nplt.ylabel(\"KL\")\nplt.xticks(All.columns.tolist(),rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:14:31.541178Z","iopub.execute_input":"2022-03-22T22:14:31.541478Z","iopub.status.idle":"2022-03-22T22:14:31.883634Z","shell.execute_reply.started":"2022-03-22T22:14:31.541442Z","shell.execute_reply":"2022-03-22T22:14:31.883087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Jensen–Shannon Divergence","metadata":{}},{"cell_type":"markdown","source":"Jensen–Shannon divergence is another method of measuring the similarity between two probability distributions.\n\n\n${\\displaystyle {\\rm {JSD}}(P\\parallel Q)={\\frac {1}{2}}D(P\\parallel M)+{\\frac {1}{2}}D(Q\\parallel M)}$\n\n*Wikipedia*","metadata":{}},{"cell_type":"code","source":"js_divergences = []\nfor i in range(All.shape[1]):\n    js_divergences.append(distance.jensenshannon(Prob_train_normal(i),Prob_test_normal(i)))\n    print('JSD(Prob_train || Prob_test) for %s is: %.5f bits' % (All.columns[i],distance.jensenshannon(Prob_train_normal(i),\n                                                                        Prob_test_normal(i))))","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:15:21.592569Z","iopub.execute_input":"2022-03-22T22:15:21.593288Z","iopub.status.idle":"2022-03-22T22:15:24.477035Z","shell.execute_reply.started":"2022-03-22T22:15:21.593239Z","shell.execute_reply":"2022-03-22T22:15:24.475938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 5))\nplt.plot(All.columns.tolist(), js_divergences)\nplt.xlabel(\"Feature\")\nplt.ylabel(\"JSD\")\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:15:29.266462Z","iopub.execute_input":"2022-03-22T22:15:29.266766Z","iopub.status.idle":"2022-03-22T22:15:30.073684Z","shell.execute_reply.started":"2022-03-22T22:15:29.266732Z","shell.execute_reply":"2022-03-22T22:15:30.072698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result\n\n","metadata":{}},{"cell_type":"markdown","source":"As we can see in plots, the divergence between train and test dataset for each column is very low. Therefore, our models and EDA should work well and not harm from divergence distributions.\nA little considerations for columns \"Age\" and \"Fare\" is needed, that I will explain them soon.\n\n**High divergence in column \"Survived\" is obviousely because of its unknown values in test dataset.","metadata":{}}]}