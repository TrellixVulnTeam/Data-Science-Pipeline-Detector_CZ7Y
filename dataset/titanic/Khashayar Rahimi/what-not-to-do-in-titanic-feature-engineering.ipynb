{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n\nI have worked on the Titanic competition over time and tried several ways for days which lead me to ideas that don't work.\nIn this notebook I want to point out them so that others can read it and save their time by not doing them.\n\nThis notebook is divided into two parts:\n\n1. Ideas for data preparation that doesn't help (+Some semi-secrets in Titanic dataset).\n2. Ideas that don't improve the accuracy or even decrease it.\n\n**This notebook will be completed over time.**","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:38:57.370334Z","iopub.execute_input":"2022-03-10T07:38:57.37064Z","iopub.status.idle":"2022-03-10T07:38:57.394257Z","shell.execute_reply.started":"2022-03-10T07:38:57.370559Z","shell.execute_reply":"2022-03-10T07:38:57.393423Z"}}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom matplotlib import pyplot\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:53:34.796419Z","iopub.execute_input":"2022-03-17T21:53:34.796783Z","iopub.status.idle":"2022-03-17T21:53:34.806941Z","shell.execute_reply.started":"2022-03-17T21:53:34.796745Z","shell.execute_reply":"2022-03-17T21:53:34.805588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(r'../input/titanic/train.csv')\ntest =  pd.read_csv(r'../input/titanic/test.csv') ","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:53:44.272286Z","iopub.execute_input":"2022-03-17T21:53:44.272676Z","iopub.status.idle":"2022-03-17T21:53:44.298628Z","shell.execute_reply.started":"2022-03-17T21:53:44.272632Z","shell.execute_reply":"2022-03-17T21:53:44.297551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All = pd.concat([train, test], sort=True).reset_index(drop=True)\nAll","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:53:52.10994Z","iopub.execute_input":"2022-03-17T21:53:52.111323Z","iopub.status.idle":"2022-03-17T21:53:52.149009Z","shell.execute_reply.started":"2022-03-17T21:53:52.111267Z","shell.execute_reply":"2022-03-17T21:53:52.148259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Engineering","metadata":{}},{"cell_type":"code","source":"columns = test.columns\nfor i in range(len(columns)):\n    print(columns[i],'--->',\"train:\",train[columns[i]].isnull().sum(),\n         \"|\",\"test:\",test[columns[i]].isnull().sum()) ","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:54:03.308212Z","iopub.execute_input":"2022-03-17T21:54:03.308795Z","iopub.status.idle":"2022-03-17T21:54:03.332756Z","shell.execute_reply.started":"2022-03-17T21:54:03.308758Z","shell.execute_reply":"2022-03-17T21:54:03.331258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Filling nan values in Cabin by family relations","metadata":{}},{"cell_type":"markdown","source":"As you see above, the Cabin column has 1014 nan values and only 295 are known, But as we see in the following, the positions of the Cabins affects the survival of the passengers.\n\n\n![Cabins1](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Titanic_side_plan_annotated_English.png/1100px-Titanic_side_plan_annotated_English.png)\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Olympic_%26_Titanic_cutaway_diagram.png/400px-Olympic_%26_Titanic_cutaway_diagram.png\" style=\"height:600px;margin-top:3rem;\"> </div>\n<br>\nTo observe better the effects we can see the Titanic sinking simulation:\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://upload.wikimedia.org/wikipedia/commons/3/33/Titanic_sinking_gif.gif\" style=\"height:300px;margin-top:3rem;\"> </div>\n","metadata":{}},{"cell_type":"markdown","source":"Here we replace Cabin values by Deck.","metadata":{}},{"cell_type":"code","source":"All[\"Cabin_dumb\"]=All[\"Cabin\"]\nfor i in range(All.shape[0]):\n    if pd.isnull(All[\"Cabin\"][i])== False:\n        All[\"Cabin_dumb\"][i] = All[\"Cabin\"][i][0]\n    else:\n        All[\"Cabin_dumb\"][i] =0\n        \n\n\nAll[\"Cabin_dumb\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:54:08.629885Z","iopub.execute_input":"2022-03-17T21:54:08.63016Z","iopub.status.idle":"2022-03-17T21:54:09.08241Z","shell.execute_reply.started":"2022-03-17T21:54:08.630129Z","shell.execute_reply":"2022-03-17T21:54:09.080906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All[\"Cabin_dumb\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:54:09.01433Z","iopub.execute_input":"2022-03-10T07:54:09.014593Z","iopub.status.idle":"2022-03-10T07:54:09.021901Z","shell.execute_reply.started":"2022-03-10T07:54:09.014565Z","shell.execute_reply":"2022-03-10T07:54:09.021352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" One of the natural idea for filling the missing values of the Cabin column is that:\n <br>\n**All members of a family are in a same cabin and surely Deck.**\n\n<br>\nBut as we see in the following, If the passenger's deck is known, the deck of all his or her family will also be known.\n\n<br>\nTherefore, WE CAN NOT fill missing values in Cabin/Deck with idea that said above.","metadata":{}},{"cell_type":"code","source":"All['Name_new'] = All['Name']\nnick = [\"Mlle. \", \"Major. \", \"Col. \", \"Sir. \", \"Don. \", \"Mme. \",\"Jonkheer. \",'Mr. ',\n\"Lady. \", \"Capt. \", \"Countess. \", \"Dona. \",\"Dr. \",\"Rev. \", \"Mrs. \",\"Ms. \",\"Miss. \",'Woman. ', 'Master. ']\n                                        \nfor i in range(All.shape[0]):\n    for j in nick:\n        All['Name_new'][i]=All['Name_new'][i].replace(j,'')\nAll","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:54:17.301584Z","iopub.execute_input":"2022-03-17T21:54:17.302977Z","iopub.status.idle":"2022-03-17T21:54:25.348701Z","shell.execute_reply.started":"2022-03-17T21:54:17.302923Z","shell.execute_reply":"2022-03-17T21:54:25.34758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We check for each passenger with known deck their family members with the following conditions.\n\n## Conditions:\n\n**max(match_size)>0:**                  The number of common words in passenger's name, set to at least one word.\n<br>\n**All[\"Cabin_dumb\"][j]==0:**            The deck's value is nan.\n<br>\n**All['Pclass'][i]==All['Pclass'][j]:** Passengers be in the same Pclass.\n<br>\n**All['Family'][i]>0:**                 Passengers has family in Titanic.\n<br>\n**All['Family'][i]==All['Family'][j]:** Family member of nan and known passengers be equal.\n<br>\n**i!=j:**                               Prevent checking a passenger with him/herself. ","metadata":{}},{"cell_type":"code","source":"All['Family'] = All['SibSp']+All['Parch']\n\nclass Solution:\n    def solve(self, s0, s1):\n        s0 = s0.lower()\n        s1 = s1.lower()\n        s0List = s0.split(\" \")\n        s1List = s1.split(\" \")\n        return len(list(set(s0List)&set(s1List)))\nob = Solution()\n\nk = 0\nfor i in range(All.shape[0]):\n    if All[\"Cabin_dumb\"][i]!=0:\n        match_size =[] \n        for j in range(All.shape[0]):\n            match_size =[] \n            match = ob.solve(All['Name_new'][i],All['Name_new'][j])\n            match_size.append(match)\n            if (max(match_size)>0) & (All[\"Cabin_dumb\"][j]==0) &(All['Pclass'][i]==All['Pclass'][j])&(All['Family'][i]>0)&(All['Family'][i]==All['Family'][j])&(i!=j):\n                print(i,All['Name_new'][i],All['Family'][i],'-------' ,j,All['Name_new'][j],All['Family'][j])\n                k=k+1\n                \nprint(k)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:54:54.397395Z","iopub.execute_input":"2022-03-17T21:54:54.397681Z","iopub.status.idle":"2022-03-17T21:55:23.973601Z","shell.execute_reply.started":"2022-03-17T21:54:54.397651Z","shell.execute_reply":"2022-03-17T21:55:23.972348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At last, By checking these 39 possibilities, we understand that none of them are in the same family.\n\n# Result1:\n**Filling nan values in Cabin(Deck) column by using name and family values, is not helpful.**","metadata":{}},{"cell_type":"markdown","source":"## 2. Filling nan value in Cabin by Machine learning ","metadata":{}},{"cell_type":"markdown","source":"You may tempt to consider the Cabin column as a label, train the dataset to learn the relation between other features and the Cabin, and finally predict the nan values of the Cabin by your Machine Learning model.\n\nI tried the same and the result was terrible: 53% accuracy for sumbisssion.\n\nThis could be because of my workflow (data enginnering and modelling), but I think it will at least prevent you from wasting time doing the same thing.\n\nIn the following, we have tried to prepare data and test different models.\n","metadata":{}},{"cell_type":"markdown","source":"### Filling missing values","metadata":{}},{"cell_type":"code","source":" from sklearn.feature_selection import mutual_info_classif as MIC\nmi_score = MIC(train.loc[: , ['Age' ,'Pclass','Parch','Fare','SibSp' ]].values.astype('int'),\n               train.loc[: , ['Age']].values.astype('int').reshape(-1, 1))\nFeature2 = ['Age' ,'Pclass','Parch','Fare','SibSp' ]\nMutual_Information_table = pd.DataFrame(columns=['Feature1', 'Feature2', 'MIC'], index=range(5))\nMutual_Information_table['Feature1'] = 'Age'\nfor feature in range(5):\n    Mutual_Information_table['Feature2'][feature] = Feature2[feature]\nfor value in range(5):\n    Mutual_Information_table['MIC'][value] = mi_score[value]\nMutual_Information_table","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:55:31.957089Z","iopub.execute_input":"2022-03-17T21:55:31.95739Z","iopub.status.idle":"2022-03-17T21:55:32.251135Z","shell.execute_reply.started":"2022-03-17T21:55:31.957358Z","shell.execute_reply":"2022-03-17T21:55:32.249915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"age_by_pclass_sex = round(All.groupby(['Sex', 'Pclass']).mean()['Age'])\n\nfor pclass in range(1, 4):\n    for sex in ['female', 'male']:\n        print('Mean age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\nprint('Mean age of all passengers: {}'.format(round(All['Age'].mean())))\n\n# Filling the missing values in Age with the medians of Sex and Pclass groups\nAll['Age'] = All.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(round(x.mean())))","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:55:43.139391Z","iopub.execute_input":"2022-03-17T21:55:43.139659Z","iopub.status.idle":"2022-03-17T21:55:43.162629Z","shell.execute_reply.started":"2022-03-17T21:55:43.139631Z","shell.execute_reply":"2022-03-17T21:55:43.161978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All[All['Embarked'].isnull()]\nAll['Embarked'] = All['Embarked'].fillna('S')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:55:46.078644Z","iopub.execute_input":"2022-03-17T21:55:46.079029Z","iopub.status.idle":"2022-03-17T21:55:46.08877Z","shell.execute_reply.started":"2022-03-17T21:55:46.078996Z","shell.execute_reply":"2022-03-17T21:55:46.087786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All[All['Fare'].isnull()]\nmean_fare = All.groupby(['Pclass', 'Parch', 'SibSp']).Fare.mean()[3][0][0]\n# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\nAll['Fare'] = All['Fare'].fillna(mean_fare)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:55:47.64274Z","iopub.execute_input":"2022-03-17T21:55:47.643888Z","iopub.status.idle":"2022-03-17T21:55:47.655263Z","shell.execute_reply.started":"2022-03-17T21:55:47.643778Z","shell.execute_reply":"2022-03-17T21:55:47.654236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All['Ticket_Frequency'] = All.groupby('Ticket')['Ticket'].transform('count')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:55:49.939104Z","iopub.execute_input":"2022-03-17T21:55:49.939412Z","iopub.status.idle":"2022-03-17T21:55:49.950399Z","shell.execute_reply.started":"2022-03-17T21:55:49.939381Z","shell.execute_reply":"2022-03-17T21:55:49.949511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq = All.head(891)['Ticket_Frequency'].value_counts().tolist()\nTicket_freq = All.head(891)['Ticket_Frequency'].unique().tolist()\n\ndeath = []\nfor n in Ticket_freq:\n    k = 0\n    for i in range(891):\n        if (All.head(891)['Ticket_Frequency'][i] == n) & (All.head(891)['Survived'][i] == 0):\n            k = k+1\n    death.append(k)    \n     \nsurvive_rate = []\nfor j,w in zip(death,freq):\n    rate = (w-j)/w\n    survive_rate.append(rate)\n\nSurvive_rate_index = {}\nfor u,r in zip(Ticket_freq,survive_rate):\n    Survive_rate_index[u] = r\nSurvive_rate_index","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:55:51.625755Z","iopub.execute_input":"2022-03-17T21:55:51.626136Z","iopub.status.idle":"2022-03-17T21:55:53.484989Z","shell.execute_reply.started":"2022-03-17T21:55:51.6261Z","shell.execute_reply":"2022-03-17T21:55:53.483293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_ticket_freq = []\nfor i in range(All.shape[0]):\n    new_ticket_freq.append(Survive_rate_index[All['Ticket_Frequency'][i]])\nnew_ticket_freq = pd.DataFrame(new_ticket_freq)\nAll['Ticket_Frequency'] = pd.DataFrame(new_ticket_freq)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:55:55.472577Z","iopub.execute_input":"2022-03-17T21:55:55.472886Z","iopub.status.idle":"2022-03-17T21:55:55.493667Z","shell.execute_reply.started":"2022-03-17T21:55:55.472848Z","shell.execute_reply":"2022-03-17T21:55:55.492288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All['Ticket_class1'] = All['Ticket']\nAll['Ticket_class2'] = All['Ticket']\nAll['Ticket_class3'] = All['Ticket']\n\nfor i in range(All.shape[0]):\n    if All.Pclass[i]==1:\n        All['Ticket_class1'][i]=All['Ticket'][i]\n    else:\n        All['Ticket_class1'][i]=0\n    if All.Pclass[i]==2:\n        All['Ticket_class2']=All['Ticket'][i]\n    else:\n        All['Ticket_class2'][i]=0\n    if All.Pclass[i]==3:\n        All['Ticket_class3'][i]=All['Ticket'][i]\n    else:\n        All['Ticket_class3'][i]=0\nAll['Ticket_class1'] = pd.factorize(All['Ticket_class1'])[0]\nAll['Ticket_class2'] = pd.factorize(All['Ticket_class2'])[0]\nAll['Ticket_class3'] = pd.factorize(All['Ticket_class3'])[0]\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:55:59.464737Z","iopub.execute_input":"2022-03-17T21:55:59.465242Z","iopub.status.idle":"2022-03-17T21:56:00.804288Z","shell.execute_reply.started":"2022-03-17T21:55:59.465188Z","shell.execute_reply":"2022-03-17T21:56:00.803318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name in All[\"Name\"]:\n    All[\"Title\"] = All[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\n\ntitle_replacements = {\"Mlle\": \"Other\", \"Major\": \"Other\", \"Col\": \"Other\", \"Sir\": \"Other\", \"Don\": \"Other\", \"Mme\": \"Other\",\n          \"Jonkheer\": \"Other\", \"Lady\": \"Other\", \"Capt\": \"Other\", \"Countess\": \"Other\", \"Dona\": \"Other\"\n                     ,\"Dr\":\"Other\",\"Rev\":\"Other\", \"Mrs\":\"Woman\",\"Ms\":\"Woman\",\"Miss\":\"Woman\"}\n\nAll.replace({\"Title\": title_replacements}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:56:04.079081Z","iopub.execute_input":"2022-03-17T21:56:04.079448Z","iopub.status.idle":"2022-03-17T21:56:12.728339Z","shell.execute_reply.started":"2022-03-17T21:56:04.079414Z","shell.execute_reply":"2022-03-17T21:56:12.727043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All = pd.get_dummies(All, prefix=['Pclass', 'Sex','Embarked','Title'], columns=['Pclass', 'Sex','Embarked','Title'])","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:56:21.44032Z","iopub.execute_input":"2022-03-17T21:56:21.440642Z","iopub.status.idle":"2022-03-17T21:56:21.459304Z","shell.execute_reply.started":"2022-03-17T21:56:21.440611Z","shell.execute_reply":"2022-03-17T21:56:21.458151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All[[\"Age\", \"Fare\"]] = MinMaxScaler().fit_transform(All[[\"Age\", \"Fare\"]])\nAll['Cabin_Dumb'] = pd.factorize(All['Cabin_dumb'])[0]\nAll.drop(['Name','Cabin_dumb','Cabin','Ticket','Name','Name_new','PassengerId'], axis=1, inplace=True)\nAll_org = All.copy()\nAll.drop(['Survived'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:57:51.752823Z","iopub.execute_input":"2022-03-17T21:57:51.754215Z","iopub.status.idle":"2022-03-17T21:57:51.772214Z","shell.execute_reply.started":"2022-03-17T21:57:51.754149Z","shell.execute_reply":"2022-03-17T21:57:51.770942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_known_cabin = pd.DataFrame()\nfor i in range(All.shape[0]):\n    if All['Cabin_Dumb'][i]!=0:\n        df_known_cabin = df_known_cabin.append(pd.DataFrame(All.values[i,:]).T)\n\ndf_unknown_cabin = pd.DataFrame()\nfor i in range(All.shape[0]):\n    if All['Cabin_Dumb'][i]==0:\n        df_unknown_cabin = df_unknown_cabin.append(pd.DataFrame(All.values[i,:-1]).T)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:58:28.427139Z","iopub.execute_input":"2022-03-17T21:58:28.427457Z","iopub.status.idle":"2022-03-17T21:58:29.537516Z","shell.execute_reply.started":"2022-03-17T21:58:28.427424Z","shell.execute_reply":"2022-03-17T21:58:29.53629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Models","metadata":{}},{"cell_type":"code","source":"X = df_known_cabin.values[:, :-1]\nY = df_known_cabin.values[:,-1]\nY=Y.astype('int')\n\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('Extra', ExtraTreesClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10)\n    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:58:34.586462Z","iopub.execute_input":"2022-03-17T21:58:34.58692Z","iopub.status.idle":"2022-03-17T21:58:37.250733Z","shell.execute_reply.started":"2022-03-17T21:58:34.586874Z","shell.execute_reply":"2022-03-17T21:58:37.249895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtc=DecisionTreeClassifier()\ndtc.fit(X,Y)\npredict_cabin=dtc.predict(df_unknown_cabin.values)\npredict_cabin","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:58:50.890673Z","iopub.execute_input":"2022-03-17T21:58:50.891341Z","iopub.status.idle":"2022-03-17T21:58:50.903011Z","shell.execute_reply.started":"2022-03-17T21:58:50.891303Z","shell.execute_reply":"2022-03-17T21:58:50.901677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = Counter(predict_cabin)\ncabin_distribution = pd.DataFrame.from_dict(count, orient='index')\ncabin_distribution.plot(kind='bar')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:58:55.420376Z","iopub.execute_input":"2022-03-17T21:58:55.421075Z","iopub.status.idle":"2022-03-17T21:58:55.686946Z","shell.execute_reply.started":"2022-03-17T21:58:55.421034Z","shell.execute_reply":"2022-03-17T21:58:55.685679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_known_cabin_final = pd.DataFrame()\nfor i in range(All.shape[0]):\n    if All_org['Cabin_Dumb'][i]!=0:\n        df_known_cabin_final = df_known_cabin_final.append(pd.DataFrame(All_org.values[i,:]).T)\n\ndf_unknown_cabin_ = pd.DataFrame()\nfor i in range(All.shape[0]):\n    if All_org['Cabin_Dumb'][i]==0:\n        df_unknown_cabin_ = df_unknown_cabin_.append(pd.DataFrame(All_org.values[i,:-1]).T)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T21:59:24.160265Z","iopub.execute_input":"2022-03-17T21:59:24.160637Z","iopub.status.idle":"2022-03-17T21:59:25.18103Z","shell.execute_reply.started":"2022-03-17T21:59:24.160602Z","shell.execute_reply":"2022-03-17T21:59:25.180017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_cabin = pd.DataFrame(predict_cabin)\ndf_unknown_cabin_[22] = df_unknown_cabin_[1]\nfor i in range(df_unknown_cabin_.shape[0]):\n    df_unknown_cabin_[22].values[i] = predict_cabin[0][i]","metadata":{"execution":{"iopub.status.busy":"2022-03-17T22:00:18.570108Z","iopub.execute_input":"2022-03-17T22:00:18.570384Z","iopub.status.idle":"2022-03-17T22:00:18.59696Z","shell.execute_reply.started":"2022-03-17T22:00:18.570355Z","shell.execute_reply":"2022-03-17T22:00:18.595897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All_with_cabin = pd.concat([df_unknown_cabin_, df_known_cabin_final],ignore_index=True)\n\ntrain_cabin = pd.DataFrame()\nfor i in range(All.shape[0]):\n    if pd.isnull(All_with_cabin[4][i])== False:\n        train_cabin = train_cabin.append(pd.DataFrame(All_with_cabin.values[i,:]).T)\n\ntest_cabin = pd.DataFrame()\nfor i in range(All.shape[0]):\n    if pd.isnull(All_with_cabin[4][i])== True:\n        test_cabin = test_cabin.append(pd.DataFrame(All_with_cabin.values[i,:]).T)\n\nAll_with_cabin_final = pd.concat([train_cabin, test_cabin],ignore_index=True)\nAll_with_cabin_final.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T22:01:16.137862Z","iopub.execute_input":"2022-03-17T22:01:16.138662Z","iopub.status.idle":"2022-03-17T22:01:17.065475Z","shell.execute_reply.started":"2022-03-17T22:01:16.13862Z","shell.execute_reply":"2022-03-17T22:01:17.064563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All_with_cabin_final.insert(23, \"survived\", All_with_cabin_final[4])\nAll_with_cabin_final.drop(4, axis=1, inplace=True)\nAll_with_cabin_final","metadata":{"execution":{"iopub.status.busy":"2022-03-17T22:02:05.261339Z","iopub.execute_input":"2022-03-17T22:02:05.262037Z","iopub.status.idle":"2022-03-17T22:02:05.313869Z","shell.execute_reply.started":"2022-03-17T22:02:05.261998Z","shell.execute_reply":"2022-03-17T22:02:05.312862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now if we test some Machine Learning models on the train part of the above dataset (All_with_cabin_final.head(891)), we obtain an overfit model that result in a very poor submission.","metadata":{}},{"cell_type":"markdown","source":"# Result 2:\n\n**Filling nan values in the Cabin column by predicting them using machine learning model trained with Cabin's known values, is not helpful.**","metadata":{}},{"cell_type":"code","source":"\n","metadata":{},"execution_count":null,"outputs":[]}]}