{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis, visualization, machine learning\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas import Series\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn')\nsns.set(font_scale=2.5) \nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n%matplotlib inline\n\ndf_train = pd.read_csv('../input/titanic/train.csv')\ndf_test = pd.read_csv('../input/titanic/test.csv')\ndf_train['FamilySize'] = df_train['SibSp'] + df_train['Parch'] + 1 # 자신을 포함해야하니 1을 더합니다\ndf_test['FamilySize'] = df_test['SibSp'] + df_test['Parch'] + 1 # 자신을 포함해야하니 1을 더합니다\n\ndf_test.loc[df_test.Fare.isnull(), 'Fare'] = df_test['Fare'].mean()\n\ndf_train['Fare'] = df_train['Fare'].map(lambda i: np.log(i) if i > 0 else 0)\ndf_test['Fare'] = df_test['Fare'].map(lambda i: np.log(i) if i > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"* Let's start the feature engineering.\n* First of all, you want to populate the null data that exists in the dataset.\n* You can't fill it with any number, you can refer to statistics in the feature that contains null data, or you can squeeze other ideas to populate it.\n* This is something you should pay attention to because the performance of the model can depend on how you fill in the null data.\n* Feature engineering is intended to be used for the learning of real models, so you should apply the same test as well as train. Let's not forget."},{"metadata":{},"cell_type":"markdown","source":"# 3. 1 Fill Null"},{"metadata":{},"cell_type":"markdown","source":"### 3. 1. 1 Fill Null in Age using title\n\n* Age has 177 null data. There will be a lot of ideas to fill this up, and we'll use the title + statistics here.\n* In English, there is a title like Miss, Mr. and Mrs. Each passenger's name will have a title like this, so I'll try it.\n* In pandas series, there is a str method that changes data to string, and an extract method that allows normal expression to be applied to it. You can use this to easily extract the title. I will save the table in the initial column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Ticket'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Initial']= df_train.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n    \ndf_test['Initial']= df_test.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's take a look at the count between the Initial and Sex we extracted using the crossstab of Pandas."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the Initials with the Sex\npd.crosstab(df_train['Initial'], df_train['Sex']).T.style.background_gradient(cmap='summer_r') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's take a look at the count between the Initial and Sex we extracted using the crossstab of Pandas."},{"metadata":{"trusted":true},"cell_type":"code","source":"#It replaces a certain value in the series \"Initials\" with a different value. \n\n\ndf_train['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],\n                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)\n\ndf_test['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],\n                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby('Initial').mean()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Miss, Mr. and Mrs. related to women have a high survival rate.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby('Initial')['Survived'].mean().plot.bar(color=['black', 'red', 'green', 'blue', 'cyan'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now, we're going to fill in the null. There are so many ways to populate null data. There are ways to utilize statistics, and there are ways to create and predict and populate new machine learning algorithms based on data without null data. Here we will use how to leverage statistics.\n* where statistics means train data. We should always leave the test as unseen and fill the null data of the test based on statistics obtained from the train."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combine 'Train' and 'Test' data to populate the null values at once.\ndf_all = pd.concat([df_train, df_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We will fill the null value using the average of the Age.\n* When handling pandas dataframe, it is very convenient to index using Boolean array.\n* To interpret the first line of code below, replace the value of 'Age' of the row that is null() and meets the condition that the initial is Mr.\n* The method of replacing values using loc + Boolean + column is often used, so let's get used to it."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.groupby('Initial').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Put below code into 'loc'\ndf_train['Survived'] == 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So just return the survived one like this!! You can set it like this. It's useful just to look at it'\ndf_train.loc[df_train['Survived'] == 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here we simply filled in the null, but there are examples of more diverse methods in other kernels. I will show you a more efficient way below."},{"metadata":{"trusted":true},"cell_type":"code","source":"#우리는 이제 여기서 null 값을 채워야 하는것. \ndf_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Mr'),'Age'] = 33\ndf_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Mrs'),'Age'] = 36\ndf_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Master'),'Age'] = 5\ndf_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Miss'),'Age'] = 22\ndf_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Other'),'Age'] = 46\n\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Mr'),'Age'] = 33\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Mrs'),'Age'] = 36\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Master'),'Age'] = 5\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Miss'),'Age'] = 22\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Other'),'Age'] = 46","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Age'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['Age'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.1.2 Fill Null in Embarked"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Embarked']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Embarked'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Since Embarked has two null values and has the largest number of passengers in S, we will simply fill the null with S.\n* You can fill it easily by using the fillna method in the data frame. If you do inplace=True here, you will actually apply fillna to df_train."},{"metadata":{"trusted":true},"cell_type":"code","source":"#null값을 최대값으로 채우는 방법 \ndf_train['Embarked'].fillna('S', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Embarked'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.2 Change Age(continuous to categorical)"},{"metadata":{},"cell_type":"markdown","source":"**In two different way! Second one is much simpler **"},{"metadata":{},"cell_type":"markdown","source":"* Age is currently continuous feature. You can build a model even if you use it as it is, but you can divide the Age into several groups and category them. Changing continuous to casual may lead to information loss, but the purpose of this tutorial is to introduce various methods, so we will proceed.\n* There are many ways. You can do it yourself using loc, the indexing method of dataframe, or you can add a function using apply.\n* The first method is using loc. Loc is used frequently, so it's good to know how to use it.\n* I will divide the age into 10 years apart."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Age_cat'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[df_train['Age'] <10, 'Age_cat'] = 0\ndf_train.loc[(10 <= df_train['Age'])&(df_train['Age']<20), 'Age_cat'] =1\ndf_train.loc[(20 <= df_train['Age'])&(df_train['Age']<30), 'Age_cat'] =2\ndf_train.loc[(30 <= df_train['Age'])&(df_train['Age']<40), 'Age_cat'] =3\ndf_train.loc[(40 <= df_train['Age'])&(df_train['Age']<50), 'Age_cat'] =4\ndf_train.loc[(50 <= df_train['Age'])&(df_train['Age']<60), 'Age_cat'] =5\ndf_train.loc[(60 <= df_train['Age'])&(df_train['Age']<70), 'Age_cat'] =6\ndf_train.loc[(70 <= df_train['Age']), 'Age_cat'] = 7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.loc[df_train['Age'] <10, 'Age_cat'] = 0\ndf_test.loc[(10 <= df_train['Age'])&(df_train['Age']<20), 'Age_cat'] =1\ndf_test.loc[(20 <= df_train['Age'])&(df_train['Age']<30), 'Age_cat'] =2\ndf_test.loc[(30 <= df_train['Age'])&(df_train['Age']<40), 'Age_cat'] =3\ndf_test.loc[(40 <= df_train['Age'])&(df_train['Age']<50), 'Age_cat'] =4\ndf_test.loc[(50 <= df_train['Age'])&(df_train['Age']<60), 'Age_cat'] =5\ndf_test.loc[(60 <= df_train['Age'])&(df_train['Age']<70), 'Age_cat'] =6\ndf_test.loc[(70 <= df_train['Age']), 'Age_cat'] = 7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The second way is to create a simple function and put it into the apply method.\n* It's much easier."},{"metadata":{"trusted":true},"cell_type":"code","source":"def category_age(x):\n    if x < 10:\n        return 0\n    elif x < 20:\n        return 1\n    elif x < 30:\n        return 2\n    elif x < 40:\n        return 3\n    elif x < 50:\n        return 4\n    elif x < 60:\n        return 5\n    elif x < 70:\n        return 6\n    else:\n        return 7    \n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* If the two methods are applied well, both should produce the same results.\n* To verify this, use the all() method after comparing Boolean between Series. The all() method gives True if all values are True, False if any are False."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Age_cat_2'] = df_train['Age'].apply(category_age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#두개를 합쳐서 비교해보는 것이다 .all 함수는 이제 모든값이 true면 true값을 주는 것\n(df_train['Age_cat'] == df_train['Age_cat_2']).any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see, it's true. You can choose between the two.\n* Now we will remove the duplicate Age_cat column and the original Column Age."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(['Age', 'Age_cat_2'], axis=1, inplace=True)\ndf_test.drop(['Age'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.3 Change Initial, Embarked and Sex (string to numerical)"},{"metadata":{},"cell_type":"markdown","source":"* Currently, the initial consists of 5 pieces, Mr. Mrs, Miss, Master, and Other. When data expressed in these categories is inputted to the model, what we need to do is digitize it so that the computer can recognize it.\n* You can do it simply with a map method.\n* I'll organize it in advance and do the mapping."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Initial'] = df_train['Initial'].map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})\ndf_test['Initial'] = df_test['Initial'].map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Embarked also consists of C, Q, and S. Let's change it using map.\n* Before we do that, let's take a quick look at how to see what values are in a particular column. You can simply write the unique() method or use value_counts() to view the count."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Embarked'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Embarked'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* You can see that Embarked consists of three methods: S, C, and Q. Now, let's use the map."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Embarked'] = df_train['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\ndf_test['Embarked'] = df_test['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_train['Embarked']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's see if Null is gone. Since importing only Embarked Columns is a Series object in one pandas, you can use the innull() method to obtain Boolean values for whether or not the values in the Series are null. And using any(), if there is a single true (if there is one null), it will return true. We got False because we changed null to S."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Embarked'].isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Sex is also composed of Female and Male. Let's change it using map."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Sex'] = df_train['Sex'].map({'female': 0, 'male': 1})\ndf_test['Sex'] = df_test['Sex'].map({'female': 0, 'male': 1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ##### Now you want to look at the correlation between each feature. You can obtain a value between (-1, 1) by obtaining Pearson correction between two variables. Negative correlation by -1; positive correlation by 1, and 0 means no correlation. The formula you are looking for is as follows."},{"metadata":{},"cell_type":"markdown","source":"* We have many features, so it would be convenient to see them in a form of a maxtrix, which is called a heatmap plot, and you can draw them comfortably with the corr() method of dataframe and seaborn."},{"metadata":{"trusted":true},"cell_type":"code","source":"heatmap_data = df_train[['Survived', 'Pclass', 'Sex', 'Fare', 'Embarked', 'FamilySize', 'Initial', 'Age_cat']] \n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14, 12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(heatmap_data.astype(float).corr(), linewidths=0.1, vmax=1.0,\n           square=True, cmap=colormap, linecolor='white', annot=True, annot_kws={\"size\": 16})\n\ndel heatmap_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we have seen in the EDA, we can see that Sex and Pclass are somewhat correlated with Survived.\n* You can see that there is a correlation between fair and embarked than you think.\n* And the information we can get from this is that there are no features that are strongly correlated with each other.\n* This means that when we train a model, there is no redundant (superfluous feature. If there's a feature A or B that correlates with one or one, there's actually only one piece of information we can get.\n* Now, before we actually train the model, let's do data preprocessing. We're almost there. Let's go hip!"},{"metadata":{},"cell_type":"markdown","source":"# 3.4 One-hot encoding on Initial and Embarked\n\n* You can put the numericalized category data as it is, but you can do one-hot encoding to increase the performance of the model.\n* Numericalization simply refers to mapping to Master == 0, Miss == 1, Mr == 2, Mrs == 3, Other == 4.\n* One-hot encoding refers to the representation of the above category as a vector of five dimensions (0, 1) as shown below.\n* You can also code the above tasks directly, but you can easily solve them using get_dummy in Pandas.\n* A total of 5 categories, and after one-hot encoding, 5 new columns are created.\n* The initial is prefixed to make it easier to distinguish."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.get_dummies(df_train, columns=['Initial'], prefix='Initial')\ndf_test = pd.get_dummies(df_test, columns=['Initial'], prefix='Initial')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see, on the right you can see the one-hot encoded columns that we were trying to create.\n* I'll apply it to Embarked as well. I will express it using one-hot encoding just like the initial."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.get_dummies(df_train, columns=['Embarked'], prefix='Embarked')\ndf_test = pd.get_dummies(df_test, columns=['Embarked'], prefix='Embarked')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n* We applied one-hot encoding very easily.\n* One-hot encoding is also possible using Labelencoder + OneHotencoder with sklearn.\n* Sometimes there are more than 100 categories. If you use one-hot encoding, you can get 100 columns, which can be very hard to learn. In this case, a different method is used."},{"metadata":{},"cell_type":"markdown","source":"# 3.5 Drop columns\n* It's time to clean up the desk. Let's erase all the columns we need."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)\ndf_test.drop(['PassengerId', 'Name',  'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see, if you take out the Survived Feature (target class) of the train, you can see that both train and test have the same columns."},{"metadata":{},"cell_type":"markdown","source":"# 4 Building machine learning model and prediction using the trained model\n\n* Now that we're ready, let's use sklearn to create a machine learning model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing all the required ML packages\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \nfrom sklearn.model_selection import train_test_split ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Sklearn has machine learning from beginning to end. All tasks related to machine learning, such as feature engineering, preprocessing, supervised learning algorithms, unsupervised learning algorithms, model evaluation, and pipeline, are implemented as easy interfaces. If you want to do data analysis + machine learning, you must be familiar with this library.\n* \n* The Titanic problem is a binary classification problem because the target class (survived) is made up of 0, 1.\n* We optimize the model with the input except the survived of the train set that we have now, and we create a model to determine the survival of each sample (passenger).\n* Then, give the test set that the model did not learn as input to predict the survival of each sample (passenger) of the test set."},{"metadata":{},"cell_type":"markdown","source":"# 4.1 Preparation - Split dataset into train, valid, test set\n* First, separate the target label (Survived) from the data that will be used for the learning. You can do it simply by using drop."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train.drop('Survived', axis=1).values\ntarget_label = df_train['Survived'].values\nX_test = df_test.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Usually, only train and test are mentioned, but to make a good model, we make a separate set and evaluate the model.\n* It's not like the soccer team is going to the World Cup right after the team training, but rather to go to the World Cup after the team training, checking the team's training level (learning level) after the evaluation match (valid).\n* Train_test_split makes it easy to detach train sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tr, X_vld, y_tr, y_vld = train_test_split(X_train, target_label, test_size=0.3, random_state=2018)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Sklearn supports several machine learning algorithms.\n* We will use the Random Forest model in this tutorial.\n* Random Forest is a crystal tree-based model and an ensemble of different crystal trees.\n* Each machine learning algorithm has several parameters. The random forest classifier also has several parameters: n_estimators, max_features, max_depth, min_samples_split, min_samples_leaf, and so on. Depending on how these are set up, the performance of the model depends on the same dataset.\n* Parameter tuning requires time, experience, and understanding of algorithms. In the end, you have to use it a lot to build a good model.\n* Since this is a tutorial, let's set aside the parameter tuning for a while and proceed with the default setting.\n* Create a model object and train it with the fit method.\n* Then insert the valid set input to obtain the predicted value (whether or not the passenger is alive)."},{"metadata":{},"cell_type":"markdown","source":"# 4.2 Model generation and prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier()\nmodel.fit(X_tr, y_tr)\nprediction = model.predict(X_vld)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* With just three lines, you've built a model and even predicted it.\n* Now, let's take a look at the performance of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('{:.2f}% accuracy'.format(y_vld.shape[0], 100 * metrics.accuracy_score(prediction, y_vld)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We didn't tune any parameters, but we got 82% accuracy."},{"metadata":{},"cell_type":"markdown","source":"# 4.3 Feature importance\n* The learned model has a feature importance, which we can check to see which feature the model we have created has been affected a lot.\n* Simply put, when we think of 10 = 4x1 + 2x2 + 1*x3, we can think that x1 has a big impact on the result (10). Feature importance refers to 4, 2, and 1, and since x1 has the largest value (4), it can be said that it has the greatest impact on this model.\n* The learned model basically has feature imports, so you can easily get that figure.\n* Using the pandas series, you can easily sort and draw graphs."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import Series\n\nfeature_importance = model.feature_importances_\nSeries_feat_imp = Series(feature_importance, index=df_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 8))\nSeries_feat_imp.sort_values(ascending=True).plot.barh()\nplt.xlabel('Feature importance')\nplt.ylabel('Feature')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Fare has the greatest influence on the models we have obtained, followed by Initial_2, Age_cat, and Pclass.\n* In fact, feature importance represents the importance in the current model. If you use a different model, the feature importance may come out differently.\n* You can look at this feature importance to determine that Fare can actually be an important feature, but this is one conclusion that ultimately attributes to the model, so you should look at it statistically more.\n* With feature importance, you can perform feature selection to obtain a more accurate model, or remove feature for a faster model."},{"metadata":{},"cell_type":"markdown","source":"# 4.4 Prediction on Test set\n* Now, let's give the model a set of tests that the model didn't learn (didn't see) and predict its survival.\n* The results are actually submission, so you can find them on the leaderboard.\n* I will read the file given by the Cagle, the gender_submission.csv file and prepare for submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/titanic/gender_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now let's make a prediction for the testset and save the results as a csv file."},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(X_test)\nsubmission['Survived'] = prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('./my_first_submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}