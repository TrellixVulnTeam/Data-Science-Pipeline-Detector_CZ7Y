{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![Titanic Sinking](http://www.titanicuniverse.com/wp-content/uploads/2009/10/titanic-sinking-underwater.jpg)\n\n<h1 style = \"font-family:verdana; background-color:#C5D6FA\"><center>I. Titanic baseline models\n</center></h1>\n<p><center style=\"color:#1F4BA7; font-family:cursive;\">Understanding basic model\nperformance</center></p>","metadata":{}},{"cell_type":"markdown","source":"For this analysis we use the <b>Titanic Kaggle Competition</b> dataset found on the Kaggle Data\nRepository at the following location:\n\n<a href=https://www.kaggle.com/c/titanic/data>Titanic Kaggle Competition Dataset</a>\n\nThe objective of the analysis is to classify survivors of the Titanic disaster of 1912 according to\nsurvival.\n\nWe aim to achieve this by following the ML pipeline approach of deploying a variety of ML techniques\nto build a final predictive model. This particular analysis comprises 4 notebooks as follows:\n\n 1. <i>titanic_baseline</i> - <b>This notebook</b>, baseline predictive models (quick and dirty) to\n compare later results against\n 2. <i>titanic_eda</i> - Exploratory Descriptive Analysis (EDA)\n 3. <i>titanic_features</i> - Perform feature engineering\n 4. <i>titanic_final_model</i> - Final model\n\nWe hope to gain valuable insights by following this process. The various steps in the process can be\nelaborated on as follows (the various notebooks will focus on different parts of the process as\nindicated):\n\n- Load data (<i>all notebooks</i>)\n- Prepare data\n    - Clean data (<i>notebook 2</i>)\n        - Missing values\n        - Outliers\n        - Erroneous values\n    - Explore data (<i>notebook 2</i>)\n        - Exploratory descriptive analysis (EDA)\n        - Correlation analysis\n        - Variable cluster analysis\n    - Transform Data (<i>notebook 3</i>)\n        - Engineer features\n        - Encode data\n        - Scale & normalise data\n        - Impute data (if not done in previous steps)\n        - Feature selection/ importance analysis\n- Build model (<i>notebooks 1 & 4</i>)\n    - Model selection\n    - Data sampling (validation strategy, imbalanced classification)\n    - Hyperparameter optimisation\n- Validate model (<i>notebooks 1 & 4</i>)\n    - Accuracy testing\n- Analysis of results (<i>notebook 1 & 4</i>)\n    - Response curves\n    - Accuracy analysis\n    - Commentary\n\nThe data dictionary for this dataset is as follows:\n\n| Variable | Definition | Key |\n|----------|------------|-----|\n| survival | Survival\t| 0 = No, 1 = Yes |\n| pclass   | Ticket class |\t1 = 1st, 2 = 2nd, 3 = 3rd |\n|sex | Sex | male, female |\n|Age | Age in years | Continuous |\n|sibsp | # of siblings / spouses aboard the Titanic | 0, 1, 2, ..|\n|parch | # of parents / children aboard the Titanic | 0, 1, 2 ..|\n|ticket | Ticket number | PC 17599, STON/O2. 3101282, 330877 |\n|fare | Passenger fare | Continuous |\n|cabin | Cabin number | C123, C85, E46 |\n|embarked | Port of Embarkation\t| C = Cherbourg, Q = Queenstown, S = Southampton |\n\nLet us start the analysis for <b>notebook 1</b>!\n\nOur approach for this notebook will be to take shortcuts and rapidly build 4 models to get\nan idea of the strength of the signal in the data as well as where there might be issues\nwith the data, or obvious areas for improvement for the model. The models we will build\nare the following:\n\n 1. Logistic regression: Bread and butter classification model - Aim is to obtain an idea of how a\n classical linear model performs\n 2. Multi-layer Perceptron (MLP): Understand how a simple non-linear model performs\n 3. Decision Tree: Provide visual analysis of variable importance and strength of association\n 4. Random Forest: Use model that good at dealing with unprocessed categorical variables in order to\n ascertain potential gains of pre-processing in next steps\n\nWe will use the scikit-learn libraries to build the prototype models from first principles. In later\nnotebooks we will use Keras and PyTorch to build the final models.","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom numpy import isnan\nfrom patsy.highlevel import dmatrix\nfrom sklearn.impute import KNNImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom visualize_titanic import plot_confusion_matrix, plot_roc_curve, \\\n    plot_feature_importance, plot_feature_importance_log, plot_feature_importance_dec, plotVar, \\\n    plotAge, plotContinuous, plotCategorical, plot_learning_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, validation_curve, \\\n    GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:19.00058Z","iopub.execute_input":"2021-06-03T06:25:19.001182Z","iopub.status.idle":"2021-06-03T06:25:20.520084Z","shell.execute_reply.started":"2021-06-03T06:25:19.001031Z","shell.execute_reply":"2021-06-03T06:25:20.519109Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id = \"load\" style = \"font-family:verdana; background-color:#C5D6FA\"><center>Load Data</center></h1>","metadata":{}},{"cell_type":"code","source":"# Import data\ndf_train = pd.read_csv('../input/titanic/train.csv', header = None, names = ['passenger_id',\n                      'survived', 'p_class', 'name', 'sex', 'age', 'sib_sp', 'parch',\n                      'ticket', 'fare', 'cabin', 'embarked'],\n                       index_col=False, usecols = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n                       skiprows=1, sep=',', skipinitialspace=True)","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.522525Z","iopub.execute_input":"2021-06-03T06:25:20.522983Z","iopub.status.idle":"2021-06-03T06:25:20.545584Z","shell.execute_reply.started":"2021-06-03T06:25:20.522937Z","shell.execute_reply":"2021-06-03T06:25:20.544538Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train Data","metadata":{}},{"cell_type":"code","source":"df_train.head(20)\nprint(df_train.shape)\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.547165Z","iopub.execute_input":"2021-06-03T06:25:20.547566Z","iopub.status.idle":"2021-06-03T06:25:20.596Z","shell.execute_reply.started":"2021-06-03T06:25:20.547522Z","shell.execute_reply":"2021-06-03T06:25:20.594989Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import data\ndf_test = pd.read_csv('../input/titanic/test.csv', header = None, names = ['passenger_id',\n                     'p_class', 'name', 'sex', 'age', 'sib_sp', 'parch', 'ticket', 'fare',\n                     'cabin', 'embarked'],\n                      index_col=False, usecols = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n                      skiprows=1, sep=',', skipinitialspace=True)","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.596967Z","iopub.execute_input":"2021-06-03T06:25:20.597235Z","iopub.status.idle":"2021-06-03T06:25:20.614169Z","shell.execute_reply.started":"2021-06-03T06:25:20.597209Z","shell.execute_reply":"2021-06-03T06:25:20.613065Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Test Data","metadata":{}},{"cell_type":"code","source":"df_test.head(20)\nprint(df_test.shape)\ndf_orig = df_test.copy()","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.618285Z","iopub.execute_input":"2021-06-03T06:25:20.618596Z","iopub.status.idle":"2021-06-03T06:25:20.645907Z","shell.execute_reply.started":"2021-06-03T06:25:20.618566Z","shell.execute_reply":"2021-06-03T06:25:20.644767Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first thing to note is that the dataset read from csv file has 891 rows and 12 data\ncolumns. This is different to the Kaggle data dictionary claiming 891 rows and 10 data\ncolumns.\n\nThe two extra fields are name and ticket. These unstructured text variables will require\npre-processing. Let us do minimal exploration of the data and then start building our\nbaseline models!","metadata":{}},{"cell_type":"markdown","source":"<h1 id = \"clean\" style = \"font-family:verdana; background-color:#C5D6FA\"><center>Clean data\n</center></h1>\n<p><center style=\"color:#1F4BA7; font-family:cursive;\">Basic data cleaning, more\nto follow in next notebook...</center></p>","metadata":{}},{"cell_type":"code","source":"# Create first stab at an improvement, start with the obvious variables to create a quick model.\ndf_train = df_train.loc[:, ['survived', 'p_class', 'name', 'sex', 'age', 'sib_sp', 'parch',\n                            'ticket', 'fare', 'cabin', 'embarked']]","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.650368Z","iopub.execute_input":"2021-06-03T06:25:20.650738Z","iopub.status.idle":"2021-06-03T06:25:20.658322Z","shell.execute_reply.started":"2021-06-03T06:25:20.650702Z","shell.execute_reply":"2021-06-03T06:25:20.657304Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.loc[:, ['p_class', 'name', 'sex', 'age', 'sib_sp', 'parch', 'ticket',\n                          'fare', 'cabin', 'embarked']]","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.660051Z","iopub.execute_input":"2021-06-03T06:25:20.660468Z","iopub.status.idle":"2021-06-03T06:25:20.676035Z","shell.execute_reply.started":"2021-06-03T06:25:20.660429Z","shell.execute_reply":"2021-06-03T06:25:20.675095Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Type of data\ndf_train.info()","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.677372Z","iopub.execute_input":"2021-06-03T06:25:20.677671Z","iopub.status.idle":"2021-06-03T06:25:20.70331Z","shell.execute_reply.started":"2021-06-03T06:25:20.677642Z","shell.execute_reply":"2021-06-03T06:25:20.702102Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Type of data\ndf_test.info()\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.704734Z","iopub.execute_input":"2021-06-03T06:25:20.705377Z","iopub.status.idle":"2021-06-03T06:25:20.725582Z","shell.execute_reply.started":"2021-06-03T06:25:20.705332Z","shell.execute_reply":"2021-06-03T06:25:20.724786Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the high level checks we can see that there are missing values in the following\nfields: age, fare and embarked.","metadata":{}},{"cell_type":"code","source":"# Check for null values\ndf_train.isnull().sum()\n\n# Actual null values\ndf_train[df_train.isnull().any(axis=1)]","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.726726Z","iopub.execute_input":"2021-06-03T06:25:20.727215Z","iopub.status.idle":"2021-06-03T06:25:20.771051Z","shell.execute_reply.started":"2021-06-03T06:25:20.727172Z","shell.execute_reply":"2021-06-03T06:25:20.769945Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for null values for test data\nprint(df_test.isnull().sum())\n# Actual null values\ndf_test[df_test.isnull().any(axis=1)]","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.772408Z","iopub.execute_input":"2021-06-03T06:25:20.773035Z","iopub.status.idle":"2021-06-03T06:25:20.80718Z","shell.execute_reply.started":"2021-06-03T06:25:20.772989Z","shell.execute_reply":"2021-06-03T06:25:20.805946Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observed 177 null values for <i>age</i> and 2 for <i>embarked</i> for <i>training\ndata</i> and 86 null values for <i>age</i> and 1 for <i>fare</i> for <i>testing data</i>.","metadata":{}},{"cell_type":"markdown","source":"<h1 id = \"explore\" style = \"font-family:verdana; background-color:#C5D6FA\"><center>Explore data\n</center></h1>\n<p><center style=\"color:#1F4BA7; font-family:cursive;\">Basic exploration, more\nto follow in next notebook...</center></p>","metadata":{}},{"cell_type":"markdown","source":"We start by looking at the number of unique records per variable.","metadata":{}},{"cell_type":"code","source":"print(df_train.nunique())","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.808498Z","iopub.execute_input":"2021-06-03T06:25:20.80878Z","iopub.status.idle":"2021-06-03T06:25:20.824373Z","shell.execute_reply.started":"2021-06-03T06:25:20.808753Z","shell.execute_reply":"2021-06-03T06:25:20.822902Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no columns with only one value. We therefore retain all columns for ML purposes as there is\nenough variability to warrant using the data. There are many variables with fewer than 10 levels which\n could be considered as categorical. Based on our initial assessment of the data we will work with\n levels of measurement for the data as follows:\n\n- p_class (ordinal) - we will revisit type of encoding later\n- sex (binary) - recode (female - yes or no)\n- age (continuous)\n- sib_sp (ordinal) - check correlation - revisit encoding\n- parch (ordinal) - check correlation - revisit encoding\n- fare (continuous)\n- embarked (nominal) - recode (one hot encode) - probably categorical\n\nAt this point it seems as if we mainly have nominal and binary categorical data. We need to One Hot\nEncode one variable i.e. embarked. We will leave the ordinal data as is for the initial analysis i.e.\nlabel encode it. Next we look at the distribution of the data.\n\nWe now separate continuous and categorical variables for further analysis.","metadata":{}},{"cell_type":"code","source":"# Separate continuous and categorical variables\nnames_con = ('fare', 'age')\nnames_con_plot = ('survived', 'fare', 'age')\nnames_cat = ('survived', 'p_class', 'sex', 'sib_sp', 'parch', 'embarked')\nnames_cat_test = ('p_class', 'sex', 'sib_sp', 'parch', 'embarked')\n\ndf_train_con = df_train.loc[:, names_con]\ndf_train_con_plot = df_train.loc[:, names_con_plot]\ndf_train_cat = df_train.loc[:, names_cat]\n\ndf_test_con = df_test.loc[:, names_con]\ndf_test_cat = df_test.loc[:, names_cat_test]\n\n# Plotting label dictionary\nplot_con = [('fare', 'Fare'),\n            ('age', 'Age')]\nplot_con_plot = [('survived', 'Survived'),\n            ('fare', 'Fare'),\n            ('age', 'Age')]\nplot_cat = ['survived', ['Yes', 'No'],\n            ('p_class', ['1st', '2nd', '3rd']),\n            ('sex', ['Male', 'Female']),\n            ('sib_sp', '# siblings or spouses'),\n            ('parch', '# parents or children'),\n            ('embarked', ['Cherbourg', 'Queenstown', 'Southampton'])]","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.825949Z","iopub.execute_input":"2021-06-03T06:25:20.826265Z","iopub.status.idle":"2021-06-03T06:25:20.840999Z","shell.execute_reply.started":"2021-06-03T06:25:20.826238Z","shell.execute_reply":"2021-06-03T06:25:20.839873Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that we have two candidates for continuous variables here (age and fare). With all the\ncategorical variables present, it is likely that a tree model would be better suited to this problem\nunless significant feature engineering on categorical features is performed to ensure features are\noptimally encoded, transformed and scaled for a linear model or neural network.\n\nLet's continue with the high level analysis.\n\nThe overall survival rate was as follows (based on the training dataset):","metadata":{}},{"cell_type":"code","source":"# Plot outcome counts.\noutcome_counts = df_train_cat['survived'].value_counts(normalize = True)\n\nbase_color = sns.color_palette()[0]\nax = sns.barplot(x=outcome_counts.index, y=outcome_counts.values, alpha=0.9, color=base_color)\nax.xaxis.label.set_size(20)\n#ax.title.set_size(10)\n\nlegend_labels = ['Died', 'Survived']\n_ = plt.xticks(fontsize=14, ticks=ax.get_xticks(), labels=legend_labels)\nfor spine in plt.gca().spines.values():\n    spine.set_visible(False)\n_ = plt.yticks([])\n\n_ = plt.title('Survival Rate', fontsize=15, pad=30)\n\nfor p in ax.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    _ = ax.annotate(f'{height:.0%}', (x + width/2, y + height*1.02), ha='center', fontsize=15)\n\nplt.show();\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.843Z","iopub.execute_input":"2021-06-03T06:25:20.843553Z","iopub.status.idle":"2021-06-03T06:25:20.978911Z","shell.execute_reply.started":"2021-06-03T06:25:20.843508Z","shell.execute_reply":"2021-06-03T06:25:20.977787Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The survival statistics are as follows:\n","metadata":{}},{"cell_type":"code","source":"print(df_train_cat['survived'].value_counts())\nprint(\"\\n\")","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.980776Z","iopub.execute_input":"2021-06-03T06:25:20.981259Z","iopub.status.idle":"2021-06-03T06:25:20.989299Z","shell.execute_reply.started":"2021-06-03T06:25:20.981213Z","shell.execute_reply":"2021-06-03T06:25:20.98762Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that 38% of passengers survived and 62% died. These statistics correspond with the narrative on survival\nrate quoted in the background information on Kaggle. There it is quoted that around 32% survived and 68% died. The\nsample we are working with is thus representative of the overall population, which is important to note.\n\nWe observe that the target variable contains unbalanced classes. We need to consider revisiting the unbalanced\nclasses at a later stage - depending on the accuracy of our models. For now, we will forge ahead.\n\nNext we will consider class level counts for categorical variables.","metadata":{}},{"cell_type":"markdown","source":"#### Categorical variable overview","metadata":{}},{"cell_type":"code","source":"# Class level counts for categorical variables.\nfor variable in names_cat:\n    print(df_train_cat[variable].value_counts(normalize = True).mul(100).round(1).astype(str) + '%')\n    print(\"\\n\")","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:20.99154Z","iopub.execute_input":"2021-06-03T06:25:20.992178Z","iopub.status.idle":"2021-06-03T06:25:21.020882Z","shell.execute_reply.started":"2021-06-03T06:25:20.992125Z","shell.execute_reply":"2021-06-03T06:25:21.019636Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bar chart plot of categorical variables.\nfig, ax = plt.subplots(2, 3, figsize=(20, 15));\nbase_color = sns.color_palette()[0]\nfor variable, subplot in zip(names_cat, ax.flatten()):\n    subplot.xaxis.label.set_size(24)\n    subplot.yaxis.label.set_size(24)\n    subplot.tick_params('y', labelsize = 20);\n    subplot.tick_params('x', labelsize = 20);\n    cat_order = df_train_cat[variable].value_counts().index\n    cp = sns.countplot(data = df_train_cat, x = variable, order = cat_order, ax=subplot, color=base_color)\n    plt.tight_layout()","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:21.023866Z","iopub.execute_input":"2021-06-03T06:25:21.025082Z","iopub.status.idle":"2021-06-03T06:25:22.237837Z","shell.execute_reply.started":"2021-06-03T06:25:21.025033Z","shell.execute_reply":"2021-06-03T06:25:22.236936Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the categorical variables we observe that there were approximately twice as many passengers in class 3 than either\nclass 1 or 2. We also observe that there were nearly twice as many males as females on the Titanic. We also observe\nthat more than two thirds of passengers did not have any siblings on board. Likewise we observe that more than two\nthirds did not have a father or child on board.\n\nIt is therefore fair to say that the majority of passengers were either couples or single travellers without children\n. In the case where families did travel, the majority of families had one or two children. Very few families with\nmore children were on board the Titanic.\n\nWe also see that more than two thirds of passengers departed from Southampton.\n\nMany of these variables could contribute o correlation with survival at face value e.g. it stands to reason that\npreference would have been given in lifeboats to women and children, and that more affluent travellers would have had\naccess to better lifeboats. We will however test these hypotheses in this analysis.","metadata":{}},{"cell_type":"markdown","source":"#### Continuous variable overview","metadata":{}},{"cell_type":"code","source":"fig_age, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 6), squeeze=False)\n_ = plotAge(df=df_train_con_plot, axes=axes, single_plot=True);\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:22.23899Z","iopub.execute_input":"2021-06-03T06:25:22.2393Z","iopub.status.idle":"2021-06-03T06:25:22.478253Z","shell.execute_reply.started":"2021-06-03T06:25:22.239271Z","shell.execute_reply":"2021-06-03T06:25:22.477197Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the age distribution plot we can see that more children under the age of 15 survived than died in the incident. \nWe can see that more individuals between the ages of 20 and 40 died than survived. We can also see that more \nindividuals above the age of 80 survived compared to dying.\n\nWe can also see that the majority of individuals on the cruise were between the ages of 20 to 40. There were fewer\nteenagers compared to children under 10. There were comparatively fewer elderly people on board i.e. above 60.","metadata":{}},{"cell_type":"code","source":"# 5 number summary.\ndf_train_con.describe()\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:22.480225Z","iopub.execute_input":"2021-06-03T06:25:22.480695Z","iopub.status.idle":"2021-06-03T06:25:22.501442Z","shell.execute_reply.started":"2021-06-03T06:25:22.480636Z","shell.execute_reply":"2021-06-03T06:25:22.499683Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The fare distribution is severely skewed to the right. The kurtosis of the plot is very high with most values\nclustered closely around the median value of 14. There was a non-significant but relatively smaller number of fares\nspread between teh values of 30 and 500.\n\nThe age distribution was as previously discussed, with a minimum of 6 months and maximum of 80 years old. The\ndistribution is fairly symmetrical with a slight skew to the right. There is a young child bump to the left of the\ndistribution.","metadata":{}},{"cell_type":"code","source":"# Continuous density plot\nfig_continuous, axes = plt.subplots(nrows=len(names_con_plot)-1, ncols=2, figsize=(15, 12))\n\n# Plot frequency plot/ histogram\n_ = sns.histplot(x=plot_con[0][0], kde=True, data=df_train_con_plot, ax=axes[0][0], bins=40);\n_ = axes[0][0].set(xlabel=plot_con[0][1], ylabel='Density');\naxes[0][0].xaxis.label.set_size(24)\naxes[0][0].yaxis.label.set_size(24)\naxes[0][0].tick_params('y', labelsize = 20);\naxes[0][0].tick_params('x', labelsize = 20);\n\n# Plot violin plot\n_ = sns.violinplot(x='survived', y=plot_con[0][0], data=df_train_con_plot, ax=axes[0][1]);\n_ = axes[0][1].set(xlabel='', ylabel=plot_con[0][1]);\naxes[0][1].xaxis.label.set_size(24)\naxes[0][1].yaxis.label.set_size(24)\naxes[0][1].tick_params('y', labelsize = 20);\naxes[0][1].tick_params('x', labelsize = 20);\n_ = axes[0][1].set_xticklabels(['Died', 'Survived'])\n\n# Plot frequency plot/ histogram\n_ = sns.histplot(x=plot_con[1][0], kde=True, data=df_train_con_plot, ax=axes[1][0], bins=40);\n_ = axes[1][0].set(xlabel=plot_con[1][1], ylabel='Density');\naxes[1][0].xaxis.label.set_size(24)\naxes[1][0].yaxis.label.set_size(24)\naxes[1][0].tick_params('y', labelsize = 20);\naxes[1][0].tick_params('x', labelsize = 20);\n\n# Plot violin plot\n_ = sns.violinplot(x='survived', y=plot_con[1][0], data=df_train_con_plot, ax=axes[1][1]);\n_ = axes[1][1].set(ylabel=plot_con[1][1], xlabel='');\naxes[1][1].xaxis.label.set_size(24)\naxes[1][1].yaxis.label.set_size(24)\naxes[1][1].tick_params('y', labelsize = 20);\naxes[1][1].tick_params('x', labelsize = 20);\n_ = axes[1][1].set_xticklabels(['Died', 'Survived'])\n\nplt.tight_layout()","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:22.503362Z","iopub.execute_input":"2021-06-03T06:25:22.503802Z","iopub.status.idle":"2021-06-03T06:25:23.358702Z","shell.execute_reply.started":"2021-06-03T06:25:22.503755Z","shell.execute_reply":"2021-06-03T06:25:23.357633Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The violin plot for <i>fare</i> indicates that there is correlation between fare and survival as more people paying a\nlow fare died and chances of survival increased for higher fares, as well as lower fares close to zero (possibly for\nchildren travelling at very low cost).\n\nThe plot for <i>age</i> indicates a similar pattern with higher survival for children below 10 and higher mortality\nbetween ages of 20 and 40. The relative likelihood of survival increases again around 40 years of age as you go into\nthe older ages.\n","metadata":{}},{"cell_type":"code","source":"# Boxplot of continuous variables\nmedianprops = {'color': 'magenta', 'linewidth': 2}\nboxprops = {'color': 'black', 'linestyle': '-', 'linewidth': 2}\nwhiskerprops = {'color': 'black', 'linestyle': '-', 'linewidth': 2}\ncapprops = {'color': 'black', 'linestyle': '-', 'linewidth': 2}\nflierprops = {'color': 'black', 'marker': 'x', 'markersize': 20}\n\n_ = df_train_con.plot(kind='box', subplots=True, figsize=(20, 8), layout=(1,2), fontsize = 20, medianprops=medianprops,\n                    boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops, flierprops=flierprops);\n_ = plt.tight_layout();\n_ = plt.show();\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:23.360191Z","iopub.execute_input":"2021-06-03T06:25:23.360764Z","iopub.status.idle":"2021-06-03T06:25:23.668735Z","shell.execute_reply.started":"2021-06-03T06:25:23.360721Z","shell.execute_reply":"2021-06-03T06:25:23.667642Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distributions of the <i>fare</i> and <i>age</i> variables show that fare is skewed heavily to the right, with the\nmedian skewed to the left of the distribution as expected. The values in the final quintile are spread over wide area\nwith quite a few outliers. This distribution is heavy tailed, as can be expected of many financial distributions.\n\nThe <i>age</i> distribution is fairly symmetrical, with a few outliers to the right, but nothing out of the ordinary.\nMost of the values are bundled symmetrically around the median of 28, which is quite a young age for the average\ntraveller.","metadata":{}},{"cell_type":"markdown","source":"<h1 id = \"transform\" style = \"font-family:verdana; background-color:#C5D6FA\"><center>Transform\nvariables\n</center></h1>\n<p><center style=\"color:#1F4BA7; font-family:cursive;\">Imputation of missing\nvalues, scaling of variables and creating interaction terms</center></p>","metadata":{}},{"cell_type":"markdown","source":"It is clear from all our analyses that there are many categorical variables strongly\ncorrelated with survival. It is also clear that there are many strong variable interactions\nin the data.\n\nIt therefore makes sense to experiment with binning of the two continuous variables i.e. age\n and fare and to manually perform some interactions modelling to see if we can obtain more\n consistent results with our predictive models.\n\nWe have already looked at the age variable in detail before, let's have another look at the\nfare variable:","metadata":{}},{"cell_type":"code","source":"fig_fare, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 6), squeeze=False)\n\n_ = sns.kdeplot(data=df_train_con_plot.loc[(df_train_con_plot['survived'] == 0), 'fare'],\n                shade = True, label = 'Died')\n_ = sns.kdeplot(data=df_train_con_plot.loc[(df_train_con_plot['survived'] == 1), 'fare'],\n                shade = True, label = 'Survived')\n_ = plt.xlabel('Fare', fontsize=20)\n_ = plt.ylabel('Density', fontsize=20)\n_ = plt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n_ = plt.legend(fontsize=15)\nplt.show()\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:23.670277Z","iopub.execute_input":"2021-06-03T06:25:23.67089Z","iopub.status.idle":"2021-06-03T06:25:24.005043Z","shell.execute_reply.started":"2021-06-03T06:25:23.670843Z","shell.execute_reply":"2021-06-03T06:25:24.00407Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can bin this variable quite easily.\n\nAs age and fare have missing values, we need to impute this at time of model building to\navoid data leakage. We will therefore go ahead with the model building process and bin these\n variables after missing values have been replaced.\n\nWe will now build the following models as previously discussed:\n\n 1. Logistic regression\n 2. Multi-layer Perceptron (MLP)\n 3. Decision Tree\n 4. Random Forest\n\nOur strategy is to build our own <i>validation strategy</i> based on the training set for which\nwe have labels. We will do this by splitting this set into training and testing sets according to\na 75%/ 25% split. Any hyper-parameter optimisation will be done by using <i>cross validation</i>\non the 75% test set. The 25% testing set will be used for our final test before we apply the\nresults to the provided test set for submission.\n\nWe therefore now start by splitting the response and features for the training set as previously\ndiscussed.\n\nWe will be using this dataset for all our models from here onwards. We also perform minor\ntransformations such as encoding the <i>sex</i> variable for test and training sets. We also One\nHot Encode the <i>embarked</i> variable. We drop one of the categories for embarked to avoid\nmulti-collinearity (dummy variable trap).","metadata":{}},{"cell_type":"code","source":"df_train['age_missing'] = df_train.apply(lambda row: 1 if np.isnan(row['age']) else 0, axis=1)\ndf_test['age_missing'] = df_test.apply(lambda row: 1 if np.isnan(row['age']) else 0, axis=1)\n\n# We will now transform some variables by grouping categories together based on our EDA\n# analysis. We also encode categorical variables to numeric values in order to do the ML\n# analysis.\n# These transformations would not result in data leakage, and can hence be done before we\n# split the data into training and testing sets.\n\n# Make a copy of original dataset before imputation - we need the original for further\n# analysis.\ndf_train_trans = df_train.copy()\ndf_test_trans = df_test.copy()\n\n# Creating Deck field from the first letter of the cabin field (we create a new category for\n# missing, which is called M). As this is a categorical variable we will leave the missing\n# value field as is.\ndf_train_trans['deck'] = df_train_trans['cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ndf_test_trans['deck'] = df_test_trans['cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.006354Z","iopub.execute_input":"2021-06-03T06:25:24.006979Z","iopub.status.idle":"2021-06-03T06:25:24.045504Z","shell.execute_reply.started":"2021-06-03T06:25:24.006932Z","shell.execute_reply":"2021-06-03T06:25:24.044269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['age_missing'].value_counts()\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.046783Z","iopub.execute_input":"2021-06-03T06:25:24.047385Z","iopub.status.idle":"2021-06-03T06:25:24.05605Z","shell.execute_reply.started":"2021-06-03T06:25:24.047341Z","shell.execute_reply":"2021-06-03T06:25:24.055049Z"},"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There is only one passenger on deck T and the test set has no values for deck T.\n# The closest category is deck 'A' (checking on deck arrangements image found via Google), so\n# we change all occurrences of T to A.\nidx = df_train_trans[df_train_trans['deck'] == 'T'].index\ndf_train_trans.loc[idx, 'deck'] = 'A'\n\n## Some of the classes have very few values, we group adjacent classes together.\ndf_train_trans['deck'] = df_train_trans['deck'].replace(['B', 'D', 'E'], 'BDE')\ndf_train_trans['deck'] = df_train_trans['deck'].replace(['A', 'G'], 'AG')\ndf_train_trans['deck'] = df_train_trans['deck'].replace(['C', 'F'], 'FG')\n\ndf_test_trans['deck'] = df_test_trans['deck'].replace(['B', 'D', 'E'], 'BDE')\ndf_test_trans['deck'] = df_test_trans['deck'].replace(['A', 'G'], 'AG')\ndf_test_trans['deck'] = df_test_trans['deck'].replace(['C', 'F'], 'FG')","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.057369Z","iopub.execute_input":"2021-06-03T06:25:24.057661Z","iopub.status.idle":"2021-06-03T06:25:24.076115Z","shell.execute_reply.started":"2021-06-03T06:25:24.057632Z","shell.execute_reply":"2021-06-03T06:25:24.074986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Next we extract the title variable from the name field\ndf_train_trans['title'] = df_train_trans['name'].str.split(', ', expand=True)[1].str.split('.',\n                                                                            expand=True)[0]\n# Next we extract the title variable from the name field\ndf_test_trans['title'] = df_test_trans['name'].str.split(', ', expand=True)[1].str.split('.',\n                                                                            expand=True)[0]","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.077678Z","iopub.execute_input":"2021-06-03T06:25:24.078123Z","iopub.status.idle":"2021-06-03T06:25:24.099825Z","shell.execute_reply.started":"2021-06-03T06:25:24.078081Z","shell.execute_reply":"2021-06-03T06:25:24.098803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_trans['is_married'] = 0\ndf_train_trans.loc[df_train_trans['title'] == 'Mrs', 'is_married'] = 1\n\ndf_test_trans['is_married'] = 0\ndf_test_trans.loc[df_test_trans['title'] == 'Mrs', 'is_married'] = 1","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.101386Z","iopub.execute_input":"2021-06-03T06:25:24.101798Z","iopub.status.idle":"2021-06-03T06:25:24.118443Z","shell.execute_reply.started":"2021-06-03T06:25:24.101755Z","shell.execute_reply":"2021-06-03T06:25:24.11724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_trans['title'].replace(['Mme', 'Ms', 'Lady', 'Mlle', 'the Countess', 'Dona'], 'Miss', inplace=True)\ndf_test_trans['title'].replace(['Mme', 'Ms', 'Lady', 'Mlle', 'the Countess', 'Dona'], 'Miss', inplace=True)\n\ndf_train_trans['title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer', 'Rev',\n                                 'Dr'], 'Mr', inplace=True)\ndf_test_trans['title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer', 'Rev',\n                                'Dr'], 'Mr', inplace=True)\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.119991Z","iopub.execute_input":"2021-06-03T06:25:24.120434Z","iopub.status.idle":"2021-06-03T06:25:24.136234Z","shell.execute_reply.started":"2021-06-03T06:25:24.120388Z","shell.execute_reply":"2021-06-03T06:25:24.135129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_trans['sex'].value_counts()\n# Transform sex variable - don't need one hot encoding as variable is binary\ndf_train_trans['sex'] = df_train_trans['sex'].apply(lambda x: 1 if x == 'female' else 0)\n# Same transformation for test set - don't need one hot encoding as variable is binary\ndf_test_trans['sex'] = df_test_trans['sex'].apply(lambda x: 1 if x == 'female' else 0)\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.137742Z","iopub.execute_input":"2021-06-03T06:25:24.138064Z","iopub.status.idle":"2021-06-03T06:25:24.157866Z","shell.execute_reply.started":"2021-06-03T06:25:24.138034Z","shell.execute_reply":"2021-06-03T06:25:24.156594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace embarked with mode training set - no values missing in test set, so not required\n# to further impute. Some leakage takes place here, but only one value so not important -\n# TODO: fix this, just as a matter of principle.\ntrain_emb_mode = df_train_trans['embarked'].mode()\ndf_train_trans['embarked'].fillna(train_emb_mode.iloc[0], inplace=True)","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.159422Z","iopub.execute_input":"2021-06-03T06:25:24.159853Z","iopub.status.idle":"2021-06-03T06:25:24.166605Z","shell.execute_reply.started":"2021-06-03T06:25:24.159795Z","shell.execute_reply":"2021-06-03T06:25:24.165278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create family size feature\ndf_train_trans['fam_num'] = df_train_trans['sib_sp'] + df_train_trans['parch'] + 1\ndf_test_trans['fam_num'] = df_test_trans['sib_sp'] + df_test_trans['parch'] + 1\n\n# Create family size groupings\ndf_train_trans['fam_size'] = pd.cut(df_train_trans.fam_num, [0,1,4,7,11], labels=['single',\n                           'small', 'large', 'very_large'])\ndf_test_trans['fam_size'] = pd.cut(df_test_trans.fam_num, [0,1,4,7,11], labels=['single',\n                           'small', 'large', 'very_large'])\n\n# Now we One Hot Encode Categorical variables. We leave the dimension variables for now, as\n# we might generate some cross terms later. We don't One Hot Encode variables with missing\n# values e.g. age, as we will impute these during training, and will One Hot Encode at that\n# stage.\n\n# Transform embarked and deck variables for training set\ncategorical_cols = ['embarked', 'deck', 'title', 'p_class', 'fam_size']\ndf_train_trans['dim_embarked'] = df_train_trans['embarked']\ndf_train_trans['dim_deck'] = df_train_trans['deck']\ndf_train_trans['dim_title'] = df_train_trans['title']\ndf_train_trans['dim_p_class'] = df_train_trans['p_class']\ndf_train_trans['dim_fam_size'] = df_train_trans['fam_size']\ndf_train_trans = pd.get_dummies(df_train_trans, columns = categorical_cols, drop_first=True)\n\n# Transform embarked and deck variables for test set\ndf_test_trans['dim_embarked'] = df_test_trans['embarked']\ndf_test_trans['dim_deck'] = df_test_trans['deck']\ndf_test_trans['dim_title'] = df_test_trans['title']\ndf_test_trans['dim_p_class'] = df_test_trans['p_class']\ndf_test_trans['dim_fam_size'] = df_test_trans['fam_size']\ndf_test_trans = pd.get_dummies(df_test_trans, columns = categorical_cols, drop_first=True)\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.16852Z","iopub.execute_input":"2021-06-03T06:25:24.168951Z","iopub.status.idle":"2021-06-03T06:25:24.210761Z","shell.execute_reply.started":"2021-06-03T06:25:24.168909Z","shell.execute_reply":"2021-06-03T06:25:24.209757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we create a variable\ndf_train_trans['ticket_freq'] = df_train_trans.groupby('ticket')['ticket'].transform('count')\ndf_test_trans['ticket_freq'] = df_test_trans.groupby('ticket')['ticket'].transform('count')","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.217085Z","iopub.execute_input":"2021-06-03T06:25:24.217403Z","iopub.status.idle":"2021-06-03T06:25:24.228103Z","shell.execute_reply.started":"2021-06-03T06:25:24.217373Z","shell.execute_reply":"2021-06-03T06:25:24.227119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest performs better on un-binned data.\n\n#df_train_trans['ticket_freq'] = df_train_trans['ticket_freq'].apply(lambda x: 1 if x == 1\n#else 0)\n# Same transformation for test set - don't need one hot encoding as variable is binary\n#df_test_trans['ticket_freq'] = df_test_trans['ticket_freq'].apply(lambda x: 1 if x == 1\n#else 0)\n\n#df_train_trans['ticket_freq'].value_counts()","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.229559Z","iopub.execute_input":"2021-06-03T06:25:24.229866Z","iopub.status.idle":"2021-06-03T06:25:24.238516Z","shell.execute_reply.started":"2021-06-03T06:25:24.229837Z","shell.execute_reply":"2021-06-03T06:25:24.237409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names_all = list(df_train_trans.columns)\n#print(names_all)","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.239968Z","iopub.execute_input":"2021-06-03T06:25:24.240378Z","iopub.status.idle":"2021-06-03T06:25:24.255706Z","shell.execute_reply.started":"2021-06-03T06:25:24.240346Z","shell.execute_reply":"2021-06-03T06:25:24.254803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update dataframe fieldname values\ndrop_cols = ['name', 'sib_sp', 'parch', 'ticket',\n       'cabin', 'fam_num', 'dim_embarked', 'dim_deck',\n       'dim_title', 'dim_p_class', 'dim_fam_size']\n\n# These stay static\nnames_con = ('fare', 'age')\nnames_con_plot = ('survived', 'fare', 'age')\n\n# These change depending on prior analyses\nnames_cat = names_all.copy()\nfor x in drop_cols:\n    names_cat.remove(x)\nfor x in ['survived', 'age', 'fare']:\n    names_cat.remove(x)\n\nnames_cat_plot = names_all.copy()\nfor x in drop_cols:\n    names_cat_plot.remove(x)\nfor x in ['age', 'fare']:\n    names_cat_plot.remove(x)\n\nfor x in drop_cols:\n    names_all.remove(x)\nfor x in ['survived']:\n    names_all.remove(x)\n\nprint(\"names_all: {}\".format(names_all))","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.25718Z","iopub.execute_input":"2021-06-03T06:25:24.257556Z","iopub.status.idle":"2021-06-03T06:25:24.27232Z","shell.execute_reply.started":"2021-06-03T06:25:24.257496Z","shell.execute_reply":"2021-06-03T06:25:24.271157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group response values to form binary response\ny = df_train_trans.loc[:, 'survived']\n\n# Split data into features (X) and response (y)\nX = df_train_trans.loc[:, names_all]\n\n# Consider using another dataframe for applying testing\ndf_test_trans = df_test_trans.loc[:, names_all]\n\n# Put the response y into an array\ny = np.ravel(y)\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.273596Z","iopub.execute_input":"2021-06-03T06:25:24.273917Z","iopub.status.idle":"2021-06-03T06:25:24.288313Z","shell.execute_reply.started":"2021-06-03T06:25:24.273887Z","shell.execute_reply":"2021-06-03T06:25:24.287077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split, impute missing values and transform the data","metadata":{}},{"cell_type":"markdown","source":"We now split the data into training and test sets according to a 75/ 25% split. We next\nimpute missing values without data leakage on the training and test sets.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n#print('Percentage holdout data: {}%'.format(round(100*(len(X_test)/len(X)),0)))\nnames_train = X.columns\n\n# Replace fare in test set with median from train set - to prevent data leakage.\nmedian_fare = X_train['fare'].median()\ndf_test_trans['fare'].fillna(median_fare, inplace=True)\n\n# Replace missing values for training set\nprint(\"Number of null values in age column: {}\".format(X_train['age'].isnull().sum()))\n\n# Define imputer\nimputer = KNNImputer()\n# fit on the dataset\nimputer.fit(X_train)\n# transform the dataset\nX_train_array = imputer.transform(X_train)\n# summarize total missing\n#print('Missing: %d' % sum(isnan(X_train).flatten()))\nX_train = pd.DataFrame(X_train_array, columns=names_train)\n\nX_test_array = imputer.transform(X_test)\n# summarize total missing\n#print('Missing: %d' % sum(isnan(X_test).flatten()))\nX_test = pd.DataFrame(X_test_array, columns=names_train)\n\n# Now we fit and transform for the final model.\n# Fit and apply to the final dataset: TODO: Test whether rebuilding model on complete set\n#  performs better\n#imputer.fit(X)\n\nX_array = imputer.transform(X)\nX = pd.DataFrame(X_array, columns=names_train)\n\n# summarize total missing\n#print('Missing: %d' % sum(isnan(X).flatten()))\ndf_test_trans_array = imputer.transform(df_test_trans)\n# summarize total missing\n#print('Missing: %d' % sum(isnan(df_test_trans).flatten()))\ndf_test_trans = pd.DataFrame(df_test_trans_array, columns=names_train)","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.289628Z","iopub.execute_input":"2021-06-03T06:25:24.290002Z","iopub.status.idle":"2021-06-03T06:25:24.351183Z","shell.execute_reply.started":"2021-06-03T06:25:24.289924Z","shell.execute_reply":"2021-06-03T06:25:24.350204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Binning fare: TODO: fix problem with ranges not understood and undercounting\nfare_bins= [0, 8, 15, 30, 100, 300, 520]\nlabels = ['very_low', 'low', 'average', 'above_average', 'high', 'very_high']\ndf_train['fare_bin'] = pd.cut(df_train['fare'], bins=fare_bins, labels=labels, right=False)\ndf_train_trans['fare_bin'] = pd.cut(df_train_trans['fare'], bins=fare_bins, labels=labels,\n                                    right=False)\nX_train['dim_fare'] = X_train['fare']\nX_train['dim_age'] = X_train['age']\nX_train['dim_ticket_freq'] = X_train['ticket_freq']\nX_test['dim_fare'] = X_test['fare']\nX_test['dim_age'] = X_test['age']\nX_test['dim_ticket_freq'] = X_test['ticket_freq']\ndf_test_trans['dim_fare'] = df_test_trans['fare']\ndf_test_trans['dim_age'] = df_test_trans['age']\ndf_test_trans['dim_ticket_freq'] = df_test_trans['ticket_freq']\n\nX_train['fare_bin'] = pd.cut(X_train['fare'], bins=fare_bins, labels=labels, right=False)\nX_test['fare_bin'] = pd.cut(X_test['fare'], bins=fare_bins, labels=labels,\n                                    right=False)\ndf_train_trans['fare_bin'] = pd.cut(df_train_trans['fare'], bins=fare_bins, labels=labels,\n                                   right=False)\ndf_test_trans['fare_bin'] = pd.cut(df_test_trans['fare'], bins=fare_bins, labels=labels,\n                                   right=False)\n\n#Binning age: TODO: fix problem with ranges not understood and undercounting\nbins= [0, 4, 13, 20, 40, 60, 110]\nlabels = ['infant','child','teen','adult', 'middle_aged', 'elderly']\n#df_train['age_bin'] = pd.cut(df_train['age'], bins=bins, labels=labels, right=False)\n#df_train_trans['age_bin'] = pd.cut(df_train_trans['age'], bins=bins, labels=labels, right=False)\nX_train['age_bin'] = pd.cut(X_train['age'], bins=bins, labels=labels, right=False)\nX_test['age_bin'] = pd.cut(X_test['age'], bins=bins, labels=labels, right=False)\ndf_train_trans['age_bin'] = pd.cut(df_train_trans['age'], bins=bins, labels=labels,\n                                   right=False)\ndf_test_trans['age_bin'] = pd.cut(df_test_trans['age'], bins=bins, labels=labels, right=False)\n\n#\nX_train['dim_age_bin'] = X_train['age_bin']\nX_train['dim_fare_bin'] = X_train['fare_bin']\nX_test['dim_age_bin'] = X_test['age_bin']\nX_test['dim_fare_bin'] = X_test['fare_bin']\n\n# Transform embarked and deck variables for training set - try not binning fare\n#binning_cols = ['fare_bin', 'age_bin', 'ticket_freq']\nbinning_cols = ['age_bin', 'fare_bin']\nX_train = pd.get_dummies(X_train, columns = binning_cols, drop_first=True)\n\n# Transform embarked and deck variables for testing set\n#binning_cols = ['fare_bin', 'age_bin', 'ticket_freq']\nbinning_cols = ['age_bin', 'fare_bin']\nX_test = pd.get_dummies(X_test, columns = binning_cols, drop_first=True)\n\n# Transform embarked and deck variables for test set\ndf_train_trans['dim_age_bin'] = df_train_trans['age_bin']\ndf_train_trans['dim_fare_bin'] = df_train_trans['fare_bin']\n\ndf_test_trans['dim_age_bin'] = df_test_trans['age_bin']\ndf_test_trans['dim_fare_bin'] = df_test_trans['fare_bin']\n\ndf_train_trans = pd.get_dummies(df_train_trans, columns = binning_cols, drop_first=True)\ndf_test_trans = pd.get_dummies(df_test_trans, columns = binning_cols, drop_first=True)","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.35255Z","iopub.execute_input":"2021-06-03T06:25:24.352866Z","iopub.status.idle":"2021-06-03T06:25:24.42261Z","shell.execute_reply.started":"2021-06-03T06:25:24.352838Z","shell.execute_reply":"2021-06-03T06:25:24.420116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create interaction terms","metadata":{}},{"cell_type":"code","source":"X_train = pd.merge(X_train, df_train_trans[['dim_deck']],left_index=True, right_index=True)\n\n# create dummy variables, and their interactions: TODO: Check if deck is better cross-term\nX_train_interactions = \\\n    dmatrix('C(dim_deck) : C(sex)', X_train,\n              return_type=\"dataframe\")\nX_train_interactions.drop('Intercept', inplace=True, axis=1)\nX_train = pd.concat([X_train, X_train_interactions], axis=1)\n\nX_train.drop('sex', axis=1, inplace=True)\nX_train.drop('dim_fare', axis=1, inplace=True)\nX_train.drop('dim_age', axis=1, inplace=True)\nX_train.drop('dim_fare_bin', axis=1, inplace=True)\nX_train.drop('dim_age_bin', axis=1, inplace=True)\nX_train.drop('dim_deck', axis=1, inplace=True)\nX_train.drop('dim_ticket_freq', axis=1, inplace=True)\n\nX_test = pd.merge(X_test, df_train_trans[['dim_deck']],left_index=True, right_index=True)\nX_test_interactions = \\\n    dmatrix('C(dim_deck) : C(sex)', X_test,\n              return_type=\"dataframe\")\nX_test_interactions.drop('Intercept', inplace=True, axis=1)\nX_test = pd.concat([X_test, X_test_interactions], axis=1)\n\nX_test.drop('sex', axis=1, inplace=True)\nX_test.drop('dim_fare', axis=1, inplace=True)\nX_test.drop('dim_age', axis=1, inplace=True)\nX_test.drop('dim_fare_bin', axis=1, inplace=True)\nX_test.drop('dim_age_bin', axis=1, inplace=True)\nX_test.drop('dim_deck', axis=1, inplace=True)\nX_test.drop('dim_ticket_freq', axis=1, inplace=True)\n\ndf_test_trans = pd.merge(df_test_trans, df_train_trans[['dim_deck']],left_index=True,\n                         right_index=True)\ndf_test_trans_interactions = \\\n    dmatrix('C(dim_deck) : C(sex)', df_test_trans,\n              return_type=\"dataframe\")\ndf_test_trans_interactions.drop('Intercept', inplace=True, axis=1)\ndf_test_trans = pd.concat([df_test_trans, df_test_trans_interactions], axis=1)\n\ndf_test_trans.drop('sex', axis=1, inplace=True)\ndf_test_trans.drop('dim_fare', axis=1, inplace=True)\ndf_test_trans.drop('dim_age', axis=1, inplace=True)\ndf_test_trans.drop('dim_fare_bin', axis=1, inplace=True)\ndf_test_trans.drop('dim_age_bin', axis=1, inplace=True)\ndf_test_trans.drop('dim_deck', axis=1, inplace=True)\ndf_test_trans.drop('dim_ticket_freq', axis=1, inplace=True)\n\ninteractions_list = list(X_train_interactions.columns.values)\n\n# TODO - used interactions list to hardcode this, need to fix.\nrename_dict = {'C(dim_deck)[T.BDE]:C(sex)[0.0]': 'deck_BDE_male',\n            'C(dim_deck)[T.FG]:C(sex)[0.0]': 'deck_FG_male',\n            'C(dim_deck)[T.M]:C(sex)[0.0]': 'deck_M_male',\n            'C(dim_deck)[T.BDE]:C(sex)[1.0]': 'deck_BDE_female',\n            'C(dim_deck)[T.FG]:C(sex)[1.0]': 'deck_FG_female',\n            'C(dim_deck)[T.M]:C(sex)[1.0]': 'deck_M_female'}\nX_train.rename(columns = rename_dict, inplace=True)\nX_test.rename(columns = rename_dict, inplace=True)\ndf_test_trans.rename(columns = rename_dict, inplace=True)\n\nX_train.drop(['C(sex)[T.1.0]'], axis=1, inplace=True)\nX_test.drop(['C(sex)[T.1.0]'], axis=1, inplace=True)\ndf_test_trans.drop(['C(sex)[T.1.0]'], axis=1, inplace=True)\n\nX_train.drop('deck_BDE', axis=1, inplace=True)\nX_train.drop('deck_FG', axis=1, inplace=True)\nX_train.drop('deck_M', axis=1, inplace=True)\n\nX_test.drop('deck_BDE', axis=1, inplace=True)\nX_test.drop('deck_FG', axis=1, inplace=True)\nX_test.drop('deck_M', axis=1, inplace=True)\n\ndf_test_trans.drop('deck_BDE', axis=1, inplace=True)\ndf_test_trans.drop('deck_FG', axis=1, inplace=True)\ndf_test_trans.drop('deck_M', axis=1, inplace=True)","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.424855Z","iopub.execute_input":"2021-06-03T06:25:24.425355Z","iopub.status.idle":"2021-06-03T06:25:24.542238Z","shell.execute_reply.started":"2021-06-03T06:25:24.4253Z","shell.execute_reply":"2021-06-03T06:25:24.54116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import chi2, SelectKBest\n\n# Separate continuous variables for this step, to be added back afterwards.\nX_train_con = X_train.loc[:, ['age', 'fare', 'ticket_freq']]\nX_train.drop(['age', 'fare', 'ticket_freq', 'ticket_freq'], axis=1, inplace=True)\n# Separate continuous variables for this step, to be added back afterwards.\nX_test_con = X_test.loc[:, ['age', 'fare', 'ticket_freq']]\nX_test.drop(['age', 'fare', 'ticket_freq'], axis=1, inplace=True)\n# Separate continuous variables for this step, to be added back afterwards.\ndf_test_trans_con = df_test_trans.loc[:, ['age', 'fare', 'ticket_freq']]\ndf_test_trans.drop(['age', 'fare', 'ticket_freq'], axis=1, inplace=True)\n\n# Finally we scale our data - separately from categorical variables.\nscaler = StandardScaler()\n\n## Fit on training data set\n# Random Forest performs better on un-scaled data.\nnames_training = list(X_train_con.columns.values)\n_ = scaler.fit(X_train_con)\n#X_train_new = scaler.transform(X_train_con)\n#X_train_con = pd.DataFrame(X_train_new, columns=names_training)\n\n# Apply to test data (training)\n#X_test_new = scaler.transform(X_test_con)\n#X_test_con = pd.DataFrame(X_test_new, columns=names_training)\n\n# Scale age and fare on final dataset to final test data\n#df_test_trans_new = scaler.transform(df_test_trans_con)\n#df_test_trans_con = pd.DataFrame(df_test_trans_new, columns=names_training)\n\n# Perform categorical feature selection\nX_train = X_train.astype(float)\ny_train = y_train.astype(float)\nX_test = X_test.astype(float)\ny_test = y_test.astype(float)\ndf_test_trans =  df_test_trans.astype(float)\n\nnames_before_select = list(X_train.columns.values)\nprint(\"Categorical variables names: {}\".format(names_before_select))\nbest_feat = SelectKBest(chi2, k=15).fit(X_train, y_train)\nmask = X_train.columns.values[best_feat.get_support()]\nX_train_new = best_feat.transform(X_train)\nX_train = pd.DataFrame(X_train_new, columns=mask)\n\nX_test_new = best_feat.transform(X_test)\nX_test = pd.DataFrame(X_test_new, columns=mask)\n\ndf_test_trans_new = best_feat.transform(df_test_trans)\ndf_test_trans = pd.DataFrame(df_test_trans_new, columns=mask)\n\n# What are scores for the features\nfor i in range(len(mask)):\n\tprint('%s: \\t\\t\\t\\t%f' % (mask[i], best_feat.scores_[i]))\n\n# plot the scores\n_ = pyplot.bar([i for i in range(len(best_feat.scores_))], best_feat.scores_)\npyplot.show()\n\n# Add continuous variables back again.\nX_train = pd.merge(X_train, X_train_con[['age', 'fare', 'ticket_freq']], left_index=True,\n                   right_index=True)\nX_test = pd.merge(X_test, X_test_con[['age', 'fare', 'ticket_freq']], left_index=True,\n                  right_index=True)\ndf_test_trans = pd.merge(df_test_trans, df_test_trans_con[['age', 'fare', 'ticket_freq']],\n                  left_index=True, right_index=True)","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.544255Z","iopub.execute_input":"2021-06-03T06:25:24.544602Z","iopub.status.idle":"2021-06-03T06:25:24.787907Z","shell.execute_reply.started":"2021-06-03T06:25:24.54457Z","shell.execute_reply":"2021-06-03T06:25:24.786948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.astype(float)\nX_test = X_test.astype(float)\ndf_test_trans =  df_test_trans.astype(float)","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.789163Z","iopub.execute_input":"2021-06-03T06:25:24.789577Z","iopub.status.idle":"2021-06-03T06:25:24.794631Z","shell.execute_reply.started":"2021-06-03T06:25:24.789533Z","shell.execute_reply":"2021-06-03T06:25:24.793785Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our final dataset for model building looks as follows:","metadata":{}},{"cell_type":"code","source":"X_train.head()","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.796324Z","iopub.execute_input":"2021-06-03T06:25:24.796643Z","iopub.status.idle":"2021-06-03T06:25:24.835282Z","shell.execute_reply.started":"2021-06-03T06:25:24.796612Z","shell.execute_reply":"2021-06-03T06:25:24.834215Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id = \"build\" style = \"font-family:verdana; background-color:#C5D6FA\"><center>Build models\n</center></h1>\n<p><center style=\"color:#1F4BA7; font-family:cursive;\">Building a series of models and\ntesting model accuracy</center></p>","metadata":{}},{"cell_type":"markdown","source":"We can now start building our first model, yay! We build and test a naive logistic regression\nmodel - without any transformations or optimisations.\n\nThe objective is to ascertain the strength of association between features and responses on\nunprocessed data.","metadata":{}},{"cell_type":"markdown","source":"## Naive Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Initial model\nlog_reg = LogisticRegression(max_iter=2000000, fit_intercept = False)\n\n# Probability scores for test set\ny_score_init = log_reg.fit(X_train, y_train).decision_function(X_test)","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.836545Z","iopub.execute_input":"2021-06-03T06:25:24.836845Z","iopub.status.idle":"2021-06-03T06:25:24.92392Z","shell.execute_reply.started":"2021-06-03T06:25:24.836785Z","shell.execute_reply":"2021-06-03T06:25:24.92262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy analysis\n\nWe start by considering the single most important classification metric, the ROC curve.","metadata":{}},{"cell_type":"code","source":"# False positive Rate and true positive rate\nfpr_roc, tpr_roc, thresholds = roc_curve(y_test, y_score_init)\n\nplot_roc_curve(fpr = fpr_roc, tpr = tpr_roc)","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:24.925708Z","iopub.execute_input":"2021-06-03T06:25:24.926159Z","iopub.status.idle":"2021-06-03T06:25:25.253747Z","shell.execute_reply.started":"2021-06-03T06:25:24.926113Z","shell.execute_reply":"2021-06-03T06:25:25.252717Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Area Under the ROC curve is 0.86 which is a good score for a classifier. We next\nlook the confusion matrix and various accuracy metrics derived from the confusion matrix\nto gain more clarity.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"y_pred = log_reg.predict(X_test)\n# Accuracy before model parameter optimisation\ncnf_matrix = confusion_matrix(y_test, y_pred)\nplot_confusion_matrix(cnf_matrix, classes=['Died', 'Survived'])","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-06-03T06:25:25.255047Z","iopub.execute_input":"2021-06-03T06:25:25.255329Z","iopub.status.idle":"2021-06-03T06:25:25.686727Z","shell.execute_reply.started":"2021-06-03T06:25:25.255302Z","shell.execute_reply":"2021-06-03T06:25:25.685728Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The overall accuracy of the model is 79%, which is slightly higher than the Gender only\nmodel which achieves 77%.\n\nThe model has a Sensitivity of 73%, which means that 73% of all survivors were correctly\nidentified. If this model was used for identifying inviduals for life insurance sales\nprior to the journey, this means that 27% of survivors would have been missed for\npotential sales (without incurring costs due to death). In real terms 23 out of 84\nsurvivors were not identified as being survivors.\n\nOn the other hand the model achieved 83% for Specificity i.e. it correctly identified 115\n out of 139 individuals whom died.\n\nThe model has a precision of 72% which means that it correctly predicted 61 out of 85\nindividuals as surviving (the model predicted 85 survivors instead of 84 actual survivors).","metadata":{}},{"cell_type":"markdown","source":"## Naive MLP","metadata":{}},{"cell_type":"code","source":"# Fit and check MSE before regularisation\nmlp_reg = MLPClassifier(max_iter=50000, solver=\"adam\", activation=\"tanh\", hidden_layer_sizes=(5, 5),\n                    random_state=1)\nmlp_reg = mlp_reg.fit(X_train, y_train)\n\n# Predict\ny_pred = mlp_reg.predict(X_test)","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:25:25.688321Z","iopub.execute_input":"2021-06-03T06:25:25.688748Z","iopub.status.idle":"2021-06-03T06:25:27.840122Z","shell.execute_reply.started":"2021-06-03T06:25:25.6887Z","shell.execute_reply":"2021-06-03T06:25:27.839094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy analysis","metadata":{}},{"cell_type":"code","source":"# False positive Rate and true positive rate\nfpr_roc, tpr_roc, thresholds = roc_curve(y_test, y_pred)\n\nplot_roc_curve(fpr = fpr_roc, tpr = tpr_roc)\n\n# Accuracy before model parameter optimisation\ncnf_matrix = confusion_matrix(y_pred, y_test)\nplot_confusion_matrix(cnf_matrix, classes=['Died', 'Survived'])","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:25:27.84157Z","iopub.execute_input":"2021-06-03T06:25:27.84202Z","iopub.status.idle":"2021-06-03T06:25:28.589855Z","shell.execute_reply.started":"2021-06-03T06:25:27.841979Z","shell.execute_reply":"2021-06-03T06:25:28.588746Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The naive MLP has an AUC score of 75% which is lower than the base model score of 79%.\n\nLet us see what the Learning Curve for the naive MLP looks like.","metadata":{}},{"cell_type":"code","source":"# Default value of 5 fold CV will be used.\ntitle = r\"Learning curve (MLP - Unoptimised)\"\n_ = plot_learning_curve(mlp_reg, title, X_train, y_train, cv=None, n_jobs=-1,\n                        learn_scoring=\"accuracy\", scoring_title=\"Accuracy\")\n","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:25:28.591089Z","iopub.execute_input":"2021-06-03T06:25:28.591372Z","iopub.status.idle":"2021-06-03T06:25:46.576572Z","shell.execute_reply.started":"2021-06-03T06:25:28.591344Z","shell.execute_reply":"2021-06-03T06:25:46.575376Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The learning curve for the un-optimised MLP shows the following:\n\n- The MLP predicts 100% correctly for 1 example in training. This makes sense. With only one\n training sample the validation accuracy is around 0.65, which is actually not bad for 1\n training example.\n- The model learns consistently and fast over the first 400 observations, validation\naccuracy increases from 0.65 to 0.8.\n- The model accuracy on the training set reduces from 1 to around 0.85 during this interval,\nwhich is not a significant drop given the increase in training set size.\n- The accuracy on the training set increases to close to 0.85 towards 500 training samples\nand continues to climb (the validation accuracy at this point however decreases i.e.\ninversely proportional to the training accuracy which indicates over-fitting).\n\nBase on these observations we can draw the following conclusions:\n\n- The model performs very well on a small dataset with only a few variables.\n- There is scope for an increase in learning rate during the first 400 training samples.\nBetter feature engineering is a possible means of attaining an increased learning rate.\n- The model overfits after 400 training samples. Regularisation or other measures to avoid\noverfitting could maybe help. It would help to have validation accuracy increase along with\ntraining post 400 samples, as this is quite a small sample size to begin with.","metadata":{}},{"cell_type":"markdown","source":"## Optimised MLP\n\nWe now optimise both the number of hidden layers, nodes as well the regularisation parameter\n <i>alpha</i>.","metadata":{}},{"cell_type":"code","source":"# Optimise numbers of nodes on both layers\nvalidation_scores = {}\nprint(\"Nodes |Validation\")\nprint(\"      | score\")\n\nfor hidden_layer_size in [(i,j) for i in range(2,6) for j in range(2,6)]:\n\n    reg = MLPClassifier(max_iter=1000000, hidden_layer_sizes=hidden_layer_size, random_state=1)\n\n    score = cross_val_score(estimator=reg, X=X_train, y=y_train, cv=2)\n    validation_scores[hidden_layer_size] = score.mean()\n    print(hidden_layer_size, \": %0.5f\" % validation_scores[hidden_layer_size])","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:25:46.578542Z","iopub.execute_input":"2021-06-03T06:25:46.579317Z","iopub.status.idle":"2021-06-03T06:26:29.108654Z","shell.execute_reply.started":"2021-06-03T06:25:46.579264Z","shell.execute_reply":"2021-06-03T06:26:29.107777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check scores\nprint(\"The highest validation score is: %0.4f\" % max(validation_scores.values()))\noptimal_hidden_layer_size = [name for name, score in validation_scores.items()\n                              if score==max(validation_scores.values())][0]\nprint(\"This corresponds to nodes\", optimal_hidden_layer_size )\n","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:26:29.109841Z","iopub.execute_input":"2021-06-03T06:26:29.110127Z","iopub.status.idle":"2021-06-03T06:26:29.11577Z","shell.execute_reply.started":"2021-06-03T06:26:29.110099Z","shell.execute_reply":"2021-06-03T06:26:29.114961Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we optimise the neural network regularisation parameter.","metadata":{}},{"cell_type":"code","source":"# Select range over which to find regularisation parameter - exponential used for even\n# distribution of values\nreg_par = [np.e**n for n in np.arange(-2,4,0.5)]\n\nvalidation_scores = {}\nprint(\" alpha  |  Accuracy\")\nfor param in reg_par:\n    reg = MLPClassifier(max_iter=1000000, solver=\"adam\", activation=\"tanh\",\n                        hidden_layer_sizes=optimal_hidden_layer_size, alpha=param, random_state=1)\n    score = cross_val_score(estimator=reg, X=X_train, y=y_train, cv=2, scoring=\"accuracy\")\n    validation_scores[param] = score.mean()\n    print(\"%0.5f |  %s\" % (param, score.mean()))\n\n# Plot the accuracy function against regularisation parameter\nplt.plot([np.log(i) for i in validation_scores.keys()], list(validation_scores.values()));\nplt.xlabel(\"Ln of alpha\");\nplt.ylabel(\"Accuracy\");","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:26:29.116927Z","iopub.execute_input":"2021-06-03T06:26:29.117205Z","iopub.status.idle":"2021-06-03T06:26:48.259925Z","shell.execute_reply.started":"2021-06-03T06:26:29.117177Z","shell.execute_reply":"2021-06-03T06:26:48.258625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The highest cross-validation accuracy score and hence the value to use for the `alpha` parameter\nis as follows.","metadata":{}},{"cell_type":"code","source":"max_score = ([np.log(name) for name, score in validation_scores.items() if score==max\n(validation_scores.values())][0])\n# Find lowest value.\nprint(\"The highest accuracy score is: %s\" % (max(validation_scores.values())))\nprint(\"This corresponds to regularisation parameter e**%s\" % max_score)","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:26:48.261043Z","iopub.execute_input":"2021-06-03T06:26:48.261318Z","iopub.status.idle":"2021-06-03T06:26:48.268871Z","shell.execute_reply.started":"2021-06-03T06:26:48.261292Z","shell.execute_reply":"2021-06-03T06:26:48.267679Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MSE after regularisation","metadata":{}},{"cell_type":"code","source":"# Fit data with the best parameter\nmlp_reg_optim = MLPClassifier(max_iter=1000000, solver=\"adam\", activation=\"tanh\",\n                    hidden_layer_sizes=optimal_hidden_layer_size, alpha=np.e**(max_score),\n                              random_state=1)\n\nmlp_reg_optim.fit(X_train, y_train)\n\n# Predict\ny_pred = mlp_reg_optim.predict(X_test)","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:26:48.270186Z","iopub.execute_input":"2021-06-03T06:26:48.270483Z","iopub.status.idle":"2021-06-03T06:26:49.060995Z","shell.execute_reply.started":"2021-06-03T06:26:48.270446Z","shell.execute_reply":"2021-06-03T06:26:49.059915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy analysis","metadata":{}},{"cell_type":"code","source":"# False positive Rate and true positive rate\nfpr_roc, tpr_roc, thresholds = roc_curve(y_test, y_pred)\nplot_roc_curve(fpr = fpr_roc, tpr = tpr_roc)\n\ncnf_matrix = confusion_matrix(y_pred, y_test)\nplot_confusion_matrix(cnf_matrix, classes=['Died', 'Survived'])","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:26:49.062153Z","iopub.execute_input":"2021-06-03T06:26:49.062434Z","iopub.status.idle":"2021-06-03T06:26:49.819931Z","shell.execute_reply.started":"2021-06-03T06:26:49.062407Z","shell.execute_reply":"2021-06-03T06:26:49.818629Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The optimised MLP has an AUC score of 74%. The model accuracy is 77% which is not better\nthan the gender only model. The PPV is however lower at 64%, due to the model predicting\nmore individuals to survive (but wrongly). The model Sensitivity is however similar to\nother models in that it manages to correctly 71% of survivors. It however identifies\nfewer individuals who died correctly at 80% Specificity.","metadata":{}},{"cell_type":"code","source":"# Default value of 5 fold CV will be used.\ntitle = r\"Learning curve (MLP - Optimised)\"\n_ = plot_learning_curve(mlp_reg_optim, title, X_train, y_train, cv=None, n_jobs=-1,\n                        learn_scoring=\"accuracy\", scoring_title=\"Accuracy\")\n","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:26:49.821464Z","iopub.execute_input":"2021-06-03T06:26:49.821902Z","iopub.status.idle":"2021-06-03T06:26:57.960303Z","shell.execute_reply.started":"2021-06-03T06:26:49.821857Z","shell.execute_reply":"2021-06-03T06:26:57.9592Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The learning curve for the optimised MLP shows the following:\n\n- Regularisation decreases 100% accuracy on 1 training sample to just below 0.9, which shows\n regularisation is working well.\n- The training accuracy drops at the same rate but drops to below 0.86 which is where it\nturned before.\n- The validation set accuracy now drops at around 300 samples, which is earlier than before.\n- The validation accuracy increase post 400 samples in in unison with the training accuracy.\n\nBased on these observations we can draw the following conclusions:\n\n- Validation resulted in the accuracy of the model increasing beyond 400 samples, which\nshows that over-fitting has been addressed to some extent.\n- More data at this point would benefit the model.\n","metadata":{}},{"cell_type":"code","source":"train_scores, test_scores = validation_curve(mlp_reg_optim, X_train, y_train, \"alpha\",\n                                             reg_par, cv=None, scoring=\"accuracy\")\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\n# Reverse scores to plot increasing complexity\ntrain_scores_mean_rev = train_scores_mean[::-1]\ntrain_scores_std = train_scores_std[::-1]\ntest_scores_mean_rev = test_scores_mean[::-1]\ntest_scores_std = test_scores_std[::-1]\nreg_par_rev = reg_par[::-1]\n","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:26:57.961742Z","iopub.execute_input":"2021-06-03T06:26:57.962124Z","iopub.status.idle":"2021-06-03T06:27:45.71067Z","shell.execute_reply.started":"2021-06-03T06:26:57.962091Z","shell.execute_reply":"2021-06-03T06:27:45.709652Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(7, 5))\n\n# Plot mean accuracy scores for training and testing scores\n_ = ax.plot(reg_par_rev, train_scores_mean_rev, label = \"Training Score\", color = 'b')\n_ = ax.plot(reg_par_rev, test_scores_mean_rev, label = \"Cross Validation Score\", color = 'g')\n\n# Creating the plot\n_ = ax.set_xlim(20, 0)\n_ = ax.set_title(\"Validation Curve - MLP regularisation optimisation\")\n_ = ax.set_xlabel(\"Alpha value\")\n_ = ax.set_ylabel(\"Accuracy\")\n\n_ = plt.tight_layout()\n_ = plt.legend(loc = 'best')\n_ = plt.show()\n","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:27:45.712303Z","iopub.execute_input":"2021-06-03T06:27:45.712751Z","iopub.status.idle":"2021-06-03T06:27:45.948393Z","shell.execute_reply.started":"2021-06-03T06:27:45.712707Z","shell.execute_reply":"2021-06-03T06:27:45.947252Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The validation curve based on training and testing set validation shows that the optimal\nparameter value of 1 previously attained with Cross Validation looks accurate.\n\nThe curve is however disappointing in that the validation curve diverges from the training\ncurve at a value of around 12.5 and then doesn't really increase convincingly afterwards.\nFrom this curve it seems as if more needs to be done to reduce over-fitting of this model.\n\nOne possible option is to drop features which introduce multi-collinearity in the model.","metadata":{}},{"cell_type":"markdown","source":"## Optimised Decision Tree\n\nWe now build a Decision Tree to see it performs against the previous models. We also use\nthe Decision Tree to calculate feature importance. This will provide us with a better feeling for\n strength of association between features and the response.","metadata":{}},{"cell_type":"code","source":"# Fit a Decision Tree to data\nsamples = [sample for sample in range(1,30)]\nvalidation_scores = []\nfor sample in samples:\n    classifier1 = DecisionTreeClassifier(random_state=1, min_samples_leaf=sample)\n    score = cross_val_score(estimator=classifier1, X=X_train, y=y_train, cv=5)\n    validation_scores.append(score.mean())\n\n# Obtain the minimum leaf samples with the highest validation score\nsamples_optimum = samples[validation_scores.index(max(validation_scores))]\n\nclassifier2 = DecisionTreeClassifier(random_state=0, min_samples_leaf=samples_optimum)\nclassifier2.fit(X_train, y_train)","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:27:45.950004Z","iopub.execute_input":"2021-06-03T06:27:45.950425Z","iopub.status.idle":"2021-06-03T06:27:46.934338Z","shell.execute_reply.started":"2021-06-03T06:27:45.950381Z","shell.execute_reply":"2021-06-03T06:27:46.933132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature importances for the Decision Tree is as follows:","metadata":{}},{"cell_type":"code","source":"cols_model = X_train.columns.to_list()\n\nimportances = np.array(classifier2.feature_importances_)\nfeature_list = np.array(cols_model)\n\n# summarize feature importance\nfor i,v in enumerate(importances):\n\tprint('Feature: %10s\\tScore:\\t%.5f' % (feature_list[i],v))\n# plot feature importance\nsorted_ID=np.array(np.argsort(importances)[::-1])\nplt.figure(figsize=[10,10])\nplt.xticks(rotation='vertical')\n_ = plt.bar(feature_list[sorted_ID], importances[sorted_ID]);\nplt.show();","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:27:46.935612Z","iopub.execute_input":"2021-06-03T06:27:46.935935Z","iopub.status.idle":"2021-06-03T06:27:47.197717Z","shell.execute_reply.started":"2021-06-03T06:27:46.935903Z","shell.execute_reply":"2021-06-03T06:27:47.196599Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we have an interesting turn of events. The previously most important features was sex,\nbut has now been replaced by title_Mr. Passenger class and family size are in 2nd and 3rd\nplaces.","metadata":{}},{"cell_type":"markdown","source":"### Accuracy analysis","metadata":{}},{"cell_type":"code","source":"# Probability scores for test set\ny_pred = classifier2.predict(X_test)\n\n# False positive Rate and true positive rate\nfpr_roc, tpr_roc, thresholds = roc_curve(y_test, y_pred)\nplot_roc_curve(fpr = fpr_roc, tpr = tpr_roc)\n\ncnf_matrix = confusion_matrix(y_pred, y_test)\nplot_confusion_matrix(cnf_matrix, classes=['Died', 'Survived'])","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:27:47.19912Z","iopub.execute_input":"2021-06-03T06:27:47.199532Z","iopub.status.idle":"2021-06-03T06:27:47.984587Z","shell.execute_reply.started":"2021-06-03T06:27:47.199486Z","shell.execute_reply":"2021-06-03T06:27:47.983473Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Decision Tree obtained an AUC score of 81% which is the highest so far.\n\nSensitivity is also the highest at 84%. Specificity also does very well at 83%. This\nmodel misses 11 out of 68 individuals whom survived, which is better for insurance\npurposes than the base model.","metadata":{}},{"cell_type":"code","source":"# Default value of 5 fold CV will be used.\ntitle = r\"Learning curve (Decision Tree - Optimised)\"\n_ = plot_learning_curve(classifier2, title, X_train, y_train, cv=None, n_jobs=-1,\n                        learn_scoring=\"accuracy\", scoring_title=\"Accuracy\")","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-03T06:27:47.986364Z","iopub.execute_input":"2021-06-03T06:27:47.986788Z","iopub.status.idle":"2021-06-03T06:27:48.663614Z","shell.execute_reply.started":"2021-06-03T06:27:47.986744Z","shell.execute_reply":"2021-06-03T06:27:48.662455Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Random Forest\nNow we build a Random Forest and see what happens!","metadata":{}},{"cell_type":"code","source":"rand_forest = RandomForestClassifier(criterion= 'gini', random_state=0)\nrand_forest.fit(X_train, y_train)\n\n# Probability scores for test set\ny_pred = rand_forest.predict(X_test)","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:27:48.665214Z","iopub.execute_input":"2021-06-03T06:27:48.665654Z","iopub.status.idle":"2021-06-03T06:27:48.919935Z","shell.execute_reply.started":"2021-06-03T06:27:48.665609Z","shell.execute_reply":"2021-06-03T06:27:48.918858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy analysis","metadata":{}},{"cell_type":"code","source":"# False positive Rate and true positive rate\nfpr_roc, tpr_roc, thresholds = roc_curve(y_test, y_pred)\nplot_roc_curve(fpr = fpr_roc, tpr = tpr_roc)\n\ncnf_matrix = confusion_matrix(y_pred, y_test)\nplot_confusion_matrix(cnf_matrix, classes=['Died', 'Survived'])","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:27:48.921777Z","iopub.execute_input":"2021-06-03T06:27:48.922229Z","iopub.status.idle":"2021-06-03T06:27:49.668567Z","shell.execute_reply.started":"2021-06-03T06:27:48.92218Z","shell.execute_reply":"2021-06-03T06:27:49.667588Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Random Forest obtained an AUC score of 77% which is lower than the Decision Tree!\nThis is unexpected. The Sensitivity is also lower at 75%.","metadata":{}},{"cell_type":"code","source":"# Default value of 5 fold CV will be used.\ntitle = r\"Learning curve (Random Forest - Unoptimised)\"\n_ = plot_learning_curve(rand_forest, title, X_train, y_train, cv=None, n_jobs=-1,\n                        learn_scoring=\"accuracy\", scoring_title=\"Accuracy\")\n","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:27:49.66957Z","iopub.execute_input":"2021-06-03T06:27:49.669861Z","iopub.status.idle":"2021-06-03T06:27:52.843943Z","shell.execute_reply.started":"2021-06-03T06:27:49.669832Z","shell.execute_reply":"2021-06-03T06:27:52.843089Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The learning curve for the un-optimised Random Forest shows the following:\n\n- As for the un-optimised MLP the un-optimised Random Forest has a 100% accuracy on one\ntraining sample for the training set, which makes sense.\n- The accuracy for one training sample on the validation set is around 75% which is better\nthan the MLP.\n- The model shows large variance which is expected. Optimisation will assist with this. The\ntraining set obtains an accuracy of above 90%.\n- The maximum validation set accuracy is just above 80%.","metadata":{}},{"cell_type":"markdown","source":"## Optimised Random Forest","metadata":{}},{"cell_type":"code","source":"rand_forest = RandomForestClassifier(max_features='auto')\nparam_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10],\n               \"min_samples_split\" : [2, 4, 10, 12], \"n_estimators\": [50, 100, 400, 700]}\ngs = GridSearchCV(estimator=rand_forest, param_grid=param_grid, scoring='accuracy', cv=3,\n                  n_jobs=-1)\ngs = gs.fit(X_train, y_train)","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:27:52.845427Z","iopub.execute_input":"2021-06-03T06:27:52.845845Z","iopub.status.idle":"2021-06-03T06:29:11.410885Z","shell.execute_reply.started":"2021-06-03T06:27:52.845782Z","shell.execute_reply":"2021-06-03T06:29:11.409678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs.best_params_","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:29:11.41216Z","iopub.execute_input":"2021-06-03T06:29:11.412463Z","iopub.status.idle":"2021-06-03T06:29:11.424549Z","shell.execute_reply.started":"2021-06-03T06:29:11.412433Z","shell.execute_reply":"2021-06-03T06:29:11.422888Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final prediction - MLP\n#rand_forest = RandomForestClassifier(criterion= 'gini', min_samples_leaf=5,\n#                                     min_samples_split=2, n_estimators=100, random_state=0)\nrand_forest = RandomForestClassifier(criterion= 'entropy', min_samples_leaf=5,\n                                     min_samples_split=4, n_estimators=100, random_state=0)\nrand_forest.fit(X_train, y_train)\n#rand_forest.fit(x_train_prev, y_train)\n\n# Probability scores for test set\ny_pred = rand_forest.predict(X_test)\n#y_pred = rand_forest.predict(x_test_prev)\n","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:29:11.426273Z","iopub.execute_input":"2021-06-03T06:29:11.426677Z","iopub.status.idle":"2021-06-03T06:29:11.671356Z","shell.execute_reply.started":"2021-06-03T06:29:11.426637Z","shell.execute_reply":"2021-06-03T06:29:11.670363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = np.array(rand_forest.feature_importances_)\nfeature_list = np.array(X_train.columns)\nimportances = np.array(importances)\nsorted_ID=np.array(np.argsort(importances))\nreverse_features = feature_list[sorted_ID][::-1]\nreverse_importances = importances[sorted_ID][::-1]\n\nfor i,v in enumerate(reverse_importances):\n    print('Feature: %20s\\tScore:\\t%.5f' % (reverse_features[i],v))\n\n# Plot feature importance\n#sorted_ID=np.array(np.argsort(scores)[::-1])\n#sns.set(font_scale=1);\n_ = plt.figure(figsize=[10,10]);\n_ = plt.xticks(rotation='horizontal', fontsize=20)\n_ = plt.barh(feature_list[sorted_ID], importances[sorted_ID], align='center');\n_ = plt.yticks(fontsize=20)\n_ = plt.show();","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:29:11.672666Z","iopub.execute_input":"2021-06-03T06:29:11.673035Z","iopub.status.idle":"2021-06-03T06:29:11.961153Z","shell.execute_reply.started":"2021-06-03T06:29:11.672952Z","shell.execute_reply":"2021-06-03T06:29:11.959751Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy analysis\n","metadata":{}},{"cell_type":"code","source":"# False positive Rate and true positive rate\nfpr_roc, tpr_roc, thresholds = roc_curve(y_test, y_pred)\nplot_roc_curve(fpr = fpr_roc, tpr = tpr_roc)\n\ncnf_matrix = confusion_matrix(y_pred, y_test)\nplot_confusion_matrix(cnf_matrix, classes=['Died', 'Survived'])","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:29:11.96306Z","iopub.execute_input":"2021-06-03T06:29:11.963534Z","iopub.status.idle":"2021-06-03T06:29:12.755429Z","shell.execute_reply.started":"2021-06-03T06:29:11.963486Z","shell.execute_reply":"2021-06-03T06:29:12.754121Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The optimised Random Forest has an accuracy score of 81%. Sensitivity of 77% is however\nstill lower than the Decision Tree.\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Default value of 5 fold CV will be used.\ntitle = r\"Learning curve (Random Forest - Optimised)\"\n_ = plot_learning_curve(rand_forest, title, X_train, y_train, cv=None, n_jobs=-1,\n                        learn_scoring=\"accuracy\", scoring_title=\"Accuracy\")\n","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:29:12.757296Z","iopub.execute_input":"2021-06-03T06:29:12.757759Z","iopub.status.idle":"2021-06-03T06:29:15.660296Z","shell.execute_reply.started":"2021-06-03T06:29:12.757713Z","shell.execute_reply":"2021-06-03T06:29:15.659375Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The learning curve for the Optimised Random Forest shows the following:\n\n- The accuracy for training starts at around 0.81 for the Random Forest. Optimisation has a\nmarked effect for accuracy on one training sample.\n- Validation accuracy increases rapidly to above 0.8 before 200 samples are reached, which\nshows variance has reduced substantially.\n- Accuracy on validation remain at around 0.82 for the remainder of training samples.\n- From 200 - 500 samples accuracy on the training set also remains constant at around 0.85.","metadata":{}},{"cell_type":"markdown","source":"<h1 id = \"conclusion\" style = \"font-family:verdana; background-color:#C5D6FA\"><center>Conclusion</center></h1>\n<a class=\"anchor-link\" href=\"https://www.kaggle.com/lourenswalters/i-titanic-baseline-models-0-78#conclusion\">¶</a>\n<p><center style=\"color:#1F4BA7; font-family:cursive;\">What have we learnt?</center></p>","metadata":{}},{"cell_type":"markdown","source":"We can conclude that there is a strong signal in the data with regards to correlation\nwith survival rate. Counter-intuitively, the Logistic Regression proved to be the best\nmodel scoring approximately 86% for the C-Statistic. The Random Forest classifier is second\nwith 81% for the C-Statistic. We would have expected the Random Forest to outperform the\nLogistic Regression given all the categorical variables. The strong correlation between\nage, fare and survival probably outweighed the categorical variable influence on survival\n rate. We expect the Random Forest to improve once we add more feature to the models.\n Also, in the name of speed we used scaled values for the Random Forest and it might\n  do better on the original data. We will need to test this in the next notebook.\n\nAll in all, this was an insightful analysis. We found that the few variables we have are\nvery strong predictors for survival. In a real world setting further analysis is probably\n not necessary, as the gains from over-engineering variables for a model which does not\n have much practical value is probably not worthwhile.\n\nThis is however an educational exercise and hence we will go ahead and try to squeeze\nmore accuracy and insight out of the remaining variables. Focusing more on the insight\nthan accuracy, as at this point the model is probably accurate enough for any practical\napplication that might exist (which there obviously doesn't!).\n\nOn to the next notebook!\n","metadata":{}},{"cell_type":"code","source":"# Gender submission - Score: 0.77\n\n# Final prediction - Random Forest - Score:  0.79665\ny_pred = rand_forest.predict(df_test_trans)\ny_pred = y_pred.astype(int)\n\n#Prepare submission code\nmy_submission = pd.DataFrame({'PassengerId': df_orig.passenger_id, 'Survived': y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_rand_forest.csv', index=False)\n\n# Final prediction - MLP (optimised) - Score: ?\n#mlp_reg_optim.fit(X, y)\ny_pred = mlp_reg_optim.predict(df_test_trans)\ny_pred = y_pred.astype(int)\n\n#Prepare submission code\nmy_submission = pd.DataFrame({'PassengerId': df_orig.passenger_id, 'Survived': y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_mlp_optim.csv', index=False)\n\n# TODO: Final prediction - MLP (non-optimised) - Score: ?\ny_pred = mlp_reg.predict(df_test_trans)\ny_pred = y_pred.astype(int)\n\n#Prepare submission code\nmy_submission = pd.DataFrame({'PassengerId': df_orig.passenger_id, 'Survived': y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_mlp.csv', index=False)\n\n# Final prediction - Logistic Regression - Score: ?\ny_pred = log_reg.predict(df_test_trans)\ny_pred = y_pred.astype(int)\n\n#Prepare submission code\nmy_submission = pd.DataFrame({'PassengerId': df_orig.passenger_id, 'Survived': y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_logreg.csv', index=False)\n\n# Final prediction - Decision Tree - Score: ?\ny_pred = classifier2.predict(df_test_trans)\ny_pred = y_pred.astype(int)\n\n#Prepare submission code\nmy_submission = pd.DataFrame({'PassengerId': df_orig.passenger_id, 'Survived': y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_dec_tree.csv', index=False)","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"execution":{"iopub.status.busy":"2021-06-03T06:29:15.661783Z","iopub.execute_input":"2021-06-03T06:29:15.662137Z","iopub.status.idle":"2021-06-03T06:29:15.713249Z","shell.execute_reply.started":"2021-06-03T06:29:15.662107Z","shell.execute_reply":"2021-06-03T06:29:15.712285Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}