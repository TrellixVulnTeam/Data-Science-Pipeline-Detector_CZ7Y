{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"<h1 style=\"text-align:center\">Introduction to Hyperparameter Tuning </h1>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"<div style=\"text-align:center;\"><img src=\"https://images.unsplash.com/photo-1503792453751-9dffb431aa63?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1636&q=80\" /></div>"},{"metadata":{},"cell_type":"markdown","source":"### About the Notebook:\n\n**In this notebook, I want to address different approaches to hyperparameter tuning using the example of the titanic dataset.**"},{"metadata":{},"cell_type":"markdown","source":"### About the dataset:\n\n**Context:** \n> The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\n**About the Data:**\n\n<ul>\n    <li>survival:\tSurvival</li>\n        <ul>\n            <li>0 = No</li>\n            <li>1 = Yes </li>\n        </ul>\n    <li>pclass: A proxy for socio-economic status (SES)</li>\n        <ul>\n            <li>1 = 1st (Upper)</li>\n            <li>2 = 2nd (Middle)</li>\n            <li>3 = 3rd (Lower)</li>\n        </ul>\n    <li>sex: Sex</li>\n        <ul>\n            <li>0 = male</li>\n            <li>1 = female </li>\n        </ul>\n    <li>age: Age in years. Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5</li>\n    <li>sibsp: # of siblings / spouses aboard the Titanic. The dataset defines family relations in this way:</li>\n        <ul>\n            <li>Sibling = brother, sister, stepbrother, stepsister</li>\n            <li>Spouse = husband, wife (mistresses and fianc√©s were ignored)</li>\n        </ul>\n    <li>parch: # of parents / children aboard the Titanic. The dataset defines family relations in this way:</li>\n        <ul>\n            <li>Parent = mother, father</li>\n            <li>Child = daughter, son, stepdaughter, stepson</li>\n            <li>Some children travelled only with a nanny, therefore parch=0 for them.</li>\n        </ul>\n    <li>fare: Passenger fare</li>\n    <li>embarked: Port of Embarkation</li>\n        <ul>\n            <li>C = Cherbourg</li>\n            <li>Q = Queenstown</li>\n            <li>S = Southampton</li>\n        </ul>\n</ul> \n"},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Processing\nimport numpy as np \nimport pandas as pd \n\n\n# Data Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n# Modeling\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Hyperparameter Tuning\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing the data\n\nI would like to skip right to the point where we have prepared data without desciption.\nI am using the same approach as in this [Kernal](https://www.kaggle.com/dietzschdaniel/my-simplistic-titanic-approach-0-79665)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Loading the data\ndf_train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n\n# Handling NaN values\ndf_train['Age'] = df_train['Age'].fillna(df_train['Age'].mean())\ndf_test['Age'] = df_test['Age'].fillna(df_test['Age'].mean())\n\ndf_train['Cabin'] = df_train['Cabin'].fillna(\"Missing\")\ndf_test['Cabin'] = df_test['Cabin'].fillna(\"Missing\")\n\ndf_train = df_train.dropna()\n\ndf_test['Fare'] = df_test['Fare'].fillna(df_test['Fare'].mean())\n\n# Cleaning the data\ndf_train = df_train.drop(columns=['Name'], axis=1)\ndf_test = df_test.drop(columns=['Name'], axis=1)\n\nsex_mapping = {\n    'male': 0,\n    'female': 1\n}\ndf_train.loc[:, \"Sex\"] = df_train['Sex'].map(sex_mapping)\ndf_test.loc[:, \"Sex\"] = df_test['Sex'].map(sex_mapping)\n\ndf_train = df_train.drop(columns=['Ticket'], axis=1)\ndf_test = df_test.drop(columns=['Ticket'], axis=1)\n\ndf_train = df_train.drop(columns=['Cabin'], axis=1)\ndf_test = df_test.drop(columns=['Cabin'], axis=1)\n\ndf_train = pd.get_dummies(df_train, prefix_sep=\"__\",\n                              columns=['Embarked'])\ndf_test = pd.get_dummies(df_test, prefix_sep=\"__\",\n                              columns=['Embarked'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data now looks as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"### Preparing the data for Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Everything except the target variable\nX = df_train.drop(\"Survived\", axis=1)\n\n# Target variable\ny = df_train['Survived'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random seed for reproducibility\nnp.random.seed(42)\n\n# Splitting the data into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Later I will be using `KNeighborsClassifier()` and `RandomForestClassifier()`.\nLet's check out the scores of `KNeighborsClassifier()` and `RandomForestClassifier()` ***without*** hyperparameter tuning:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up KNeighborsClassifier()\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up RandomForestClassifier()\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\nrfc.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuning hyperparameters by hand"},{"metadata":{},"cell_type":"markdown","source":"One way is of course to tune the hyperparameters by hand.\n\nIn this example, we tune the hyperparameter `n_neighbors` of the `KNeighborsClassifier()` by looping through different `n_neighbors` values and comparing their score."},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of train scores\ntrain_scores = []\n\n# List of test scores\ntest_scores = []\n\n# List of different values for n_neighbors\nneighbors = range(1, 51) # 1 to 50\n\n# Setting up the classifier\nknn = KNeighborsClassifier()\n\n# Loop through different neighbors values\nfor i in neighbors:\n    knn.set_params(n_neighbors = i) # set neighbors value\n    \n    # Fitting the algorithm\n    knn.fit(X_train, y_train)\n    \n    # Append the training scores\n    train_scores.append(knn.score(X_train, y_train))\n    \n    # Append the test scores\n    test_scores.append(knn.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot the train and test scores for the different neighbour values:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Plotting the Train and Test scores\nplt.figure(figsize=(20,10))\nplt.plot(neighbors, train_scores, label=\"Train score\")\nplt.plot(neighbors, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 51, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")\nprint(f\"Number of Neighbours with Maximum KNN: {test_scores.index(max(test_scores)) + 1}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Maximum KNN score on the test data is reached with `n_neighbors = 29` with a score of `69.66%`."},{"metadata":{},"cell_type":"markdown","source":"## Tuning hyperparameters using ***RandomizedSearchCV()***"},{"metadata":{},"cell_type":"markdown","source":"`RandomizedSearchCV()` uses a randomized search through hyperparameters.   \n\nIn this example we use the `RandomForestClassifier()`."},{"metadata":{},"cell_type":"markdown","source":"First, we have to define a dictionary of hyperparameters we would like to check:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up dictionary with RandomForestClassifier hyperparameters\nrfc_rs_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n               \"max_depth\": [None, 3, 5, 10],\n               \"min_samples_split\": np.arange(2, 20, 2),\n               \"min_samples_leaf\": np.arange(1, 20, 2)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's set up the hyperparameter search for the `RandomForestClassifier()` using `RandomizedSearchCV()`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Setting random seed\nnp.random.seed(42)\n\n# Setting random hyperparameter search for RandomForestClassifier\nrs_rfc = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rfc_rs_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fitting random hyperparameter search model\nrs_rfc.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the best parameters\nrs_rfc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate the model\nrs_rfc.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We were able to improve the score!"},{"metadata":{},"cell_type":"markdown","source":"## Tuning hyperparameters using ***GridSearchCV()***"},{"metadata":{},"cell_type":"markdown","source":"`GridSearchCV()` uses exhaustive search over specified parameter values for an estimator.  \n\nIn this example we use the `RandomForestClassifier()`."},{"metadata":{},"cell_type":"markdown","source":"First, we again have to define the dictionary. This time we only use `n_estimators` and `max_depth` since this approach takes a lot of time to compute."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up dictionary with RandomForestClassifier hyperparameters\nrfc_gs_grid = {\"n_estimators\": np.arange(10, 1010, 100),\n               \"max_depth\": [None, 5, 10]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Setting grid hyperparameter search for RandomForestClassifier\ngs_rfc = GridSearchCV(RandomForestClassifier(),\n                          param_grid=rfc_gs_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fitting grid hyperparameter search model\ngs_rfc.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the best parameters\ngs_rfc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate the model\ngs_rfc.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case we reached the same score as without the hyperparameter tuning. We could try a different dictionary to improve the score. However, due to the long time to compute and since this is just a demonstration, I will leave it like this."},{"metadata":{},"cell_type":"markdown","source":"**If you liked this notebook or found it helpful in any way, feel free to leave an upvote - That will keep me motivated :)**\n\n**If you have any suggestions for improvement, leave a comment :)**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}