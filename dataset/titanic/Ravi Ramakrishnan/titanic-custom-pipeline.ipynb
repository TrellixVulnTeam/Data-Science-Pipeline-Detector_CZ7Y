{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing general libraries:-\n\nimport numpy as np; \nfrom scipy.stats import mode;\nimport pandas as pd;\nfrom pandasql import sqldf;\nimport regex as re;\n\nimport matplotlib.pyplot as plt; \n%matplotlib inline\nimport seaborn as sns;\nsns.set_style('darkgrid');\n\nfrom warnings import filterwarnings;\nfrom termcolor import colored;\n\nnp.random.seed(10);","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-05T16:42:52.207033Z","iopub.execute_input":"2022-04-05T16:42:52.207415Z","iopub.status.idle":"2022-04-05T16:42:53.393396Z","shell.execute_reply.started":"2022-04-05T16:42:52.207321Z","shell.execute_reply":"2022-04-05T16:42:53.392309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing model specific libraries:-\nfrom sklearn_pandas import DataFrameMapper, gen_features;\n\nfrom sklearn.compose import make_column_selector;\nfrom sklearn.base import BaseEstimator, TransformerMixin;\nfrom sklearn.pipeline import make_pipeline, Pipeline ;\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder, StandardScaler, RobustScaler, OrdinalEncoder;\nfrom sklearn.impute import SimpleImputer;\n\nfrom sklearn.model_selection import KFold, GridSearchCV;\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier;\nfrom xgboost import XGBClassifier;\nfrom lightgbm import LGBMClassifier;\nfrom sklearn.svm import SVC;\nfrom sklearn.tree import DecisionTreeClassifier;\nfrom sklearn.linear_model import LogisticRegression;\n\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score, f1_score, classification_report, confusion_matrix;","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:42:53.394947Z","iopub.execute_input":"2022-04-05T16:42:53.395158Z","iopub.status.idle":"2022-04-05T16:42:55.021669Z","shell.execute_reply.started":"2022-04-05T16:42:53.395132Z","shell.execute_reply":"2022-04-05T16:42:55.020756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Titanic- Machine Learning from disaster","metadata":{}},{"cell_type":"code","source":"# Loading relevant data-sets:-\nxytrain = pd.read_csv('../input/titanic/train.csv', encoding = 'utf8');\nxtest = pd.read_csv('../input/titanic/test.csv', encoding = 'utf8');\n\n# Splitting the training data into features and target:-\nxtrain, ytrain = xytrain.drop('Survived', axis= 1), xytrain[['Survived']];\n\nprint(colored(F\"Train-Test dataframe lengths = {len(xytrain), len(xtest)}\", color= 'blue', attrs= ['bold']));\nprint(colored(F\"\\nTrain-set information\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.info());\nprint(colored(F\"\\nTrain-set description\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.describe().style.format('{:.2f}'));","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:42:55.022754Z","iopub.execute_input":"2022-04-05T16:42:55.02306Z","iopub.status.idle":"2022-04-05T16:42:55.16911Z","shell.execute_reply.started":"2022-04-05T16:42:55.023032Z","shell.execute_reply":"2022-04-05T16:42:55.168285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data processing and visualization:-","metadata":{}},{"cell_type":"markdown","source":"### a. Target column details:-","metadata":{}},{"cell_type":"code","source":"plt.subplots(1,1,figsize= (6,6));\nax = ytrain.value_counts().plot.bar(color= 'tab:blue');\nax.set_title(\"Surviver analysis for train set\", color = 'tab:blue', fontsize= 12);\nax.set_xlabel('Survival status', color= 'tab:blue');\nax.set_yticks(range(0, len(xtrain), 50));\nax.set_ylabel('Passengers', color= 'tab:blue');\nplt.xticks(rotation = 0);\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:42:55.171382Z","iopub.execute_input":"2022-04-05T16:42:55.172389Z","iopub.status.idle":"2022-04-05T16:42:55.589035Z","shell.execute_reply.started":"2022-04-05T16:42:55.172337Z","shell.execute_reply":"2022-04-05T16:42:55.588182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### b. Passenger class and gender:-","metadata":{}},{"cell_type":"code","source":"_ = xytrain.groupby(['Sex', 'Pclass']).agg(Survivors = pd.NamedAgg('Survived', np.sum),Passengers = pd.NamedAgg('Survived', np.size)).sort_index(level=[1,0])\n_['Survival_Rate'] = _['Survivors'] / _['Passengers'];\nprint(colored(f'\\nSurvival rate by gender and pclass\\n', color = 'blue', attrs= ['bold', 'dark']));\ndisplay(_.style.format({'Survival_Rate':'{:.2%}'}))\n\n_0 = _.groupby(level= 0).agg({'Survivors':np.sum, 'Passengers':np.sum});\n_0['Survival_Rate'] = _0['Survivors'] / _0['Passengers'];\n\n_1 = _.groupby(level= 1).agg({'Survivors':np.sum, 'Passengers':np.sum});\n_1['Survival_Rate'] = _1['Survivors'] / _1['Passengers'];\n\nprint('\\n');\nfig, ax = plt.subplots(nrows= 1,ncols=2,figsize = (12,6), sharey= True);\nsns.barplot(x = _0.index, y = _0.Survival_Rate, palette = 'Blues', ax = ax[0]);\nsns.barplot(x = _1.index, y = _1.Survival_Rate, palette = 'Blues', ax= ax[1]);\nax[0].set_title(\"Survival analysis by gender\", color = 'tab:blue', fontsize = 12);\nax[1].set_title(\"Survival analysis by passenger class\", color = 'tab:blue', fontsize = 12);\nplt.yticks(np.arange(0,1,0.05),fontsize= 8, color = 'blue');\nplt.show()\n\ndel _, _0, _1;","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:42:55.590382Z","iopub.execute_input":"2022-04-05T16:42:55.591006Z","iopub.status.idle":"2022-04-05T16:42:56.003986Z","shell.execute_reply.started":"2022-04-05T16:42:55.59096Z","shell.execute_reply":"2022-04-05T16:42:56.003257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### c. Age:-","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize = (18,6));\nsns.histplot(x = xytrain['Age'], kde= True, palette = 'Blues', ax = ax[0]);\nax[0].set_title(f\"Overall age distribution analysis\", color = 'tab:blue', fontsize= 12 )\n\nsns.boxplot(x = xytrain.Pclass, y = xytrain.Age, palette = 'Blues', ax = ax[1]);\nax[1].set_title(f\"Age distribution per Pclass\", color = 'tab:blue', fontsize= 12);\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:42:56.005073Z","iopub.execute_input":"2022-04-05T16:42:56.005935Z","iopub.status.idle":"2022-04-05T16:42:56.523273Z","shell.execute_reply.started":"2022-04-05T16:42:56.00588Z","shell.execute_reply":"2022-04-05T16:42:56.522609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### d. Ticket fare:-","metadata":{}},{"cell_type":"code","source":"print(colored(f\"\\nTicket fare by Pclass and survivorship\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.groupby(['Pclass', 'Survived']).agg({'Fare': [np.amin, np.median, np.mean, np.amax]}).style.format('{:.2f}'))","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:42:56.52431Z","iopub.execute_input":"2022-04-05T16:42:56.524817Z","iopub.status.idle":"2022-04-05T16:42:56.548557Z","shell.execute_reply.started":"2022-04-05T16:42:56.524781Z","shell.execute_reply":"2022-04-05T16:42:56.547258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### e. Null valued columns:-","metadata":{}},{"cell_type":"code","source":"# Plotting null columns across the data-sets:-\ndef Plot_NullCol(df, df_type):\n    \"\"\"\n    This function plots the relevant data-set and scans for nulls across columns\n    Inputs- \n    df (dataframe):- The relevant data-frame for analysis\n    df_type (string):- Type of data (training/ test)\n    \"\"\";\n    \n    global xtrain;\n    _ = df.isna().sum(axis= 0);\n    print('\\n');\n    \n    plt.subplots(1,1, figsize= (8,8))\n    ax= _.plot.bar(color= 'tab:blue');\n    ax.set_title(f\"Columns with null values in {df_type} data\", color = 'tab:blue', fontsize= 12);\n    ax.set_yticks(range(0, len(xtrain),50));\n    ax.axhline(y= len(xtrain)/4, linewidth = 1.5, color= 'red');\n    ax.set_ylabel('Null values', color = 'tab:blue');\n    ax.set_xlabel('Features', color = 'tab:blue');\n    plt.show();\n    \n    print(colored(f\"Nulls in {df_type} data\\n\", color = 'blue'));\n    display(_);\n    del _;\n    \n\n# Plotting the train-test for nulls:-\nPlot_NullCol(xtrain, df_type= 'train');\nPlot_NullCol(xtest, df_type= 'test');","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:42:56.54998Z","iopub.execute_input":"2022-04-05T16:42:56.550804Z","iopub.status.idle":"2022-04-05T16:42:57.2535Z","shell.execute_reply.started":"2022-04-05T16:42:56.550754Z","shell.execute_reply":"2022-04-05T16:42:57.252614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### f. Cabin null inference in training set:-","metadata":{}},{"cell_type":"code","source":"print(colored(f\"\\nCabin column null inferences in training data-set\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.assign(Cabin_cat = xtrain.Cabin.str[0:1]).groupby(['Cabin_cat', 'Pclass'], dropna= False).\\\n        agg({'Fare':[np.median, np.mean, np.amin, np.amax], \n             'PassengerId':[np.size], \n             'Survived': [np.sum]})\\\n        .style.format('{:,.0f}'));","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:42:57.254889Z","iopub.execute_input":"2022-04-05T16:42:57.255124Z","iopub.status.idle":"2022-04-05T16:42:57.284492Z","shell.execute_reply.started":"2022-04-05T16:42:57.255096Z","shell.execute_reply":"2022-04-05T16:42:57.283519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating pipeline adjutant functions and classes:-","metadata":{}},{"cell_type":"code","source":"def TreatCabinNulls(df1:pd.DataFrame):\n    \"\"\"\n    This is an added function to treat the null valued cabin column in both the train and test data. \n    This is designed to impute the nulls instead of dropping the column entirely.\n    The treatment of nulls follows the below process-\n    1. Create a composite variable with the cabin category (1st letter in the cabin column) and the Pclass. \n       This is an interaction variable\n    2. Consider passengers with the same Pclass as the subject\n    3. Map the composite Cabin category with the null instances in the cabin based on the lowest fare difference. \n       Windowing SQL functions are used for the same\n    4. For cases where fare is not available, use the mode of the cabin category per Pclass. \n    \n    Input- df1 (dataframe):- Input dataframe without treatment\n    Returns- df (dataframe):- Dataframe with the cabin nulls treated \n    \"\"\";\n    \n    global xtrain, xtest;\n    \n    cabin_trmt_prf = \\\n    sqldf(f\"\"\" \n    select PassengerId, Pclass, Fare, Embarked, Cabin from xtrain \n    union all \n    select PassengerId, Pclass, Fare, Embarked, Cabin from xtest\n    \"\"\");\n\n    #  Creating the proxy variable with the fare and Pclass:-   \n    cabin_trmt_mst = \\\n    sqldf(f\"\"\"\n    select A1.PID1 as PassengerId, A1.Pclass, A1.Cabin_Ctg_Lbl\n    from \n    (\n    select a.PassengerId as PID1,a.Pclass, b.PassengerId as PID2, b.Cabin_Ctg_Lbl, a.Fare as Fare1, b.Fare as Fare2, abs(a.Fare - b.Fare) as Fare_Diff,\n    row_number() over(partition by a.PassengerId order by abs(a.Fare - b.Fare) asc) as Fare_Diff_Rank\n    from \n    (select PassengerId, Pclass, Fare, Embarked from cabin_trmt_prf WHERE Cabin is null) A \n    inner join \n    (\n    select PassengerId, Pclass, Fare, (cast(Pclass as varchar(1)) || substr(Cabin,1,1)) AS Cabin_Ctg_Lbl, Embarked \n    from cabin_trmt_prf \n    where Cabin is not null\n    ) B on (A.Pclass = B.Pclass and abs(a.Fare - b.Fare) <= 50)\n    ) A1\n    where A1.Fare_Diff_Rank == 1\n    \"\"\");\n    \n    #  Finally appending the nulls still present with the mode of the Pclass and Category label:-\n    cabin_md_sum = \\\n    sqldf(\"\"\"\n    select a.* from \n    (\n    select Pclass, Cabin_Ctg_Lbl, count(PassengerId) as cnt, row_number() over (order by count(PassengerId) desc) as rank_id\n    from cabin_trmt_mst \n    group by Pclass, Cabin_Ctg_Lbl\n    ) a\n    where a.rank_id = 1\n    \"\"\");\n \n    # Mapping the interaction variable to the relevant table:-    \n    df = df1.copy();   \n    df = sqldf(\"\"\"\n    select a.*, coalesce(coalesce(b.Cabin_Ctg_Lbl, cast(a.Pclass as varchar(1)) || substr(a.Cabin,1,1)), c.Cabin_Ctg_Lbl) as Cabin_Class_Lbl \n    from df a \n    left join cabin_trmt_mst b on a.PassengerId = B.PassengerId\n    left join cabin_md_sum c on a.Pclass = c.Pclass\n    \"\"\");  \n    \n    del cabin_trmt_mst, cabin_trmt_prf, cabin_md_sum; \n    return df;  ","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:42:57.286912Z","iopub.execute_input":"2022-04-05T16:42:57.287163Z","iopub.status.idle":"2022-04-05T16:42:57.296067Z","shell.execute_reply.started":"2022-04-05T16:42:57.287133Z","shell.execute_reply":"2022-04-05T16:42:57.295115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Xform_Data(df1:pd.DataFrame): \n    \"\"\"\n    This function does the below tasks:-\n    1. Creates a title attribute from the name column\n    2. Assigns 'others' to uncommon titles\n    3. Creates a flag for 'child' from the title. This is used to fill in the age nulls.\n    4. Creates a 'family members count' column from the Sibsp and Parch attributes\n    5. Drops superfluous columns\n    \n    Input- df (dataframe):- Analysis dataframe\n    Returns- df (dataframe):- Modified dataframe   \n    \"\"\";\n    \n    filterwarnings(action= \"ignore\");\n    df = df1.copy();\n    \n    df['Title'] = df['Name'].apply(lambda x: re.findall(r\"\\S+\\. \",x)[0].strip()[0:-1]);\n    df['Title'].loc[~df.Title.isin(['Mr', 'Mrs', 'Miss', 'Master'])] = 'Others';\n    df['Is_child'] = np.select([df['Title'].str.lower() == 'master'], ['Y'], 'N');    \n    df['Nb_Fmly_Mem'] = df['SibSp'].fillna(0) + df['Parch'].fillna(0);  \n    df = df.drop(['PassengerId', 'Ticket', 'Name', 'Cabin'], axis= 1);  \n    \n    return df;","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:43:10.305746Z","iopub.execute_input":"2022-04-05T16:43:10.306027Z","iopub.status.idle":"2022-04-05T16:43:10.315006Z","shell.execute_reply.started":"2022-04-05T16:43:10.305998Z","shell.execute_reply":"2022-04-05T16:43:10.313718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AgeImputer(BaseEstimator, TransformerMixin):\n    \"\"\"This class is designed to fill-up the age null values with the child/ adult gender based medians\"\"\";\n    def __init__(self): pass\n    def fit(self, X, y=None, **fit_params):\n        \"This function learns the training medians across the child and adult groups for filling nulls\";\n        self.median_ = X.groupby(['Is_child','Sex'])[['Age']].median().reset_index();\n        return self;\n    def transform(self, X, y=None, **transform_params):\n        \"This function imputes the null values in the relevant data-set according to the medians\";\n        X1 = X.merge(self.median_, how= 'left', on= ['Is_child', 'Sex'], suffixes= ('', '_Median'));\n        X1['Age'] = X1['Age'].fillna(X1.Age_Median);\n        X1 = X1.drop(['Age_Median'], axis= 1);\n        return X1;","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:42:57.315075Z","iopub.execute_input":"2022-04-05T16:42:57.315377Z","iopub.status.idle":"2022-04-05T16:42:57.328084Z","shell.execute_reply.started":"2022-04-05T16:42:57.31534Z","shell.execute_reply":"2022-04-05T16:42:57.327224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FareImputer(BaseEstimator, TransformerMixin):\n    \"\"\"This class is designed to fill-up the ticket fare null values with the Pclass medians\"\"\";\n    def __init__(self): pass\n    def fit(self, X, y=None, **fit_params):\n        \"This function learns the training medians across the Pclass groups for filling nulls\";\n        self.median_ = X.groupby(['Pclass'])[['Fare']].median().reset_index();\n        return self;\n    def transform(self, X, y=None, **transform_params):\n        \"This function imputes the null values in the relevant data-set according to the medians\";\n        X1 = X.merge(self.median_, how= 'left', on= ['Pclass'], suffixes= ('', '_Median'));\n        X1['Fare'] = X1['Fare'].fillna(X1.Fare_Median);\n        X1 = X1.drop(['Fare_Median'], axis= 1);\n        return X1;","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:42:57.329893Z","iopub.execute_input":"2022-04-05T16:42:57.330819Z","iopub.status.idle":"2022-04-05T16:42:57.34725Z","shell.execute_reply.started":"2022-04-05T16:42:57.330769Z","shell.execute_reply":"2022-04-05T16:42:57.34617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AgeFareBinner(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This class develops bins for the age and ticket fare, to foster stability and offer better and stable predictability.\n    Training set parameters are developed with the fit function. \n    \"\"\";\n    \n    def __init__(self, nb_age_bins: int = 5, nb_fare_bins:int = 5): \n        self.nb_age_bins = nb_age_bins\n        self.nb_fare_bins = nb_fare_bins\n    \n    def fit(self, X, y= None, **fit_params):\n        \"\"\"This function calculates the bins for the age and fare columns respectively\"\"\";     \n        self.age_bins_ = pd.qcut(X['Age'], q = self.nb_age_bins, retbins= True, labels = range(1,self.nb_age_bins + 1,1))[1];\n        self.fare_bins_ = pd.qcut(X['Fare'], q = self.nb_fare_bins, retbins= True, labels = range(1,self.nb_fare_bins + 1,1))[1];   \n        return self;\n        \n    def transform(self, X, y=None, **transform_param):\n        \"\"\"This function applies the binned results to the relevant dataframe and returns the labelled column\"\"\";        \n        X1 = X.copy();\n        \n        self.age_bins_[0] = 0.0; self.fare_bins_[0] = 0.0;\n        self.age_bins_[-1] = np.inf; self.fare_bins_[-1] = np.inf;\n        \n        X1['Age_Bin'] = pd.cut(X1['Age'], self.age_bins_, retbins= False, labels = range(1,self.nb_age_bins + 1,1), include_lowest= True);\n        X1['Fare_Bin'] = pd.cut(X1['Fare'], self.fare_bins_, retbins= False, labels = range(1,self.nb_age_bins + 1,1), include_lowest= True); \n               \n        global df_col; df_col = list(X1.columns);\n        return X1;      ","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:42:57.3486Z","iopub.execute_input":"2022-04-05T16:42:57.348895Z","iopub.status.idle":"2022-04-05T16:42:57.363766Z","shell.execute_reply.started":"2022-04-05T16:42:57.348863Z","shell.execute_reply":"2022-04-05T16:42:57.362694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Model pipeline development:-\n\nKey pipeline steps:-\n* Treat nulls in cabin- This step treats nulls in cabin using Pclass and Fare, also fills up remaining nulls after treatment with the Pclass mode\n* Transform data- This step adds the new features and drops some redundant features.\n* Impute Embarked- This step imputes the null values in the column with the mode\n* Label encoder- This is used for the selected object columns only\n* Impute age- This is used to impute age based on the grouped median of child/ adult and gender\n* Impute fare- This fills fare column nulls based on the median of Pclass fare\n* Bin Age Fare- This bins the age and fare columns (this is not used in the latest version)\n* Robust Scaler- This is used for numerical columns only as selected in the column list","metadata":{}},{"cell_type":"code","source":"nb_age_bins = 5;\nnb_fare_bins = 5;\nenc_col_lst = ['Sex', 'Embarked', 'Title', 'Is_child'];\nstd_col_lst = ['Age', 'Fare', 'SibSp', 'Parch', 'Nb_Fmly_Mem' ]\n\nData_Processor = \\\nPipeline(verbose=True, \n         steps= \\\n         [('Imp_Cabin', FunctionTransformer(func= TreatCabinNulls)),\n          ('Xform_Data', FunctionTransformer(func= Xform_Data)),\n          ('Imp_Embk', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                             features= [(['Embarked'], SimpleImputer(strategy= 'most_frequent'))])\n          ),\n          ('Lbl_Enc', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                            features = gen_features([col.split(' ') for col in enc_col_lst], [LabelEncoder]))\n          ),\n          ('Imp_Age', AgeImputer()),\n          ('Imp_Fare',FareImputer()),\n          ('Ord_Enc', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                      features= [(['Cabin_Class_Lbl'], OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value= 49))])\n          ),\n#           ('BinAgeFare', AgeFareBinner(nb_age_bins = nb_age_bins, nb_fare_bins = nb_fare_bins)),\n          ('Std', DataFrameMapper(default= None, input_df= True, df_out= True, \n                                  features = gen_features([col.split(' ') for col in std_col_lst], [RobustScaler])))\n         ]\n        );","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:44:54.535696Z","iopub.execute_input":"2022-04-05T16:44:54.536333Z","iopub.status.idle":"2022-04-05T16:44:54.548406Z","shell.execute_reply.started":"2022-04-05T16:44:54.536284Z","shell.execute_reply":"2022-04-05T16:44:54.547423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(colored(f\"\\nPipeline details\\n\", color = 'blue'));\nData_Processor.fit(xtrain, ytrain);\n\n# Train-set transformation:-\nprint(colored(f\"\\nPipeline implementation for the training set\\n\", color= 'blue', attrs= ['bold', 'dark']));\nXtrain = Data_Processor.transform(xtrain);\n\nprint(colored(f\"\\nNull check after pipeline\\n\", color = 'blue'));\ndisplay(Xtrain.isna().sum(axis=0));\nprint(colored(f'\\nData description after pipeline\\n', color = 'blue'));\ndisplay(Xtrain.describe().style.format('{:.2f}'));\n\n# Test-set transformation:-\nprint(colored(f\"\\nPipeline implementation for the test set\\n\", color= 'blue', attrs= ['bold', 'dark']));\nXtest = Data_Processor.transform(xtest);\n\nprint(colored(f\"\\nNull check after pipeline\\n\", color = 'blue'));\ndisplay(Xtest.isna().sum(axis=0));","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:44:56.879513Z","iopub.execute_input":"2022-04-05T16:44:56.879811Z","iopub.status.idle":"2022-04-05T16:44:58.544005Z","shell.execute_reply.started":"2022-04-05T16:44:56.879777Z","shell.execute_reply":"2022-04-05T16:44:58.543186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model parameter setting and implementation:-\n\nKey models and description:-\n1. Logistic Regression\n2. SVC classifier\n3. Single tree\n4. Ensemble random forest\n5. Gradient boosting machine\n6. Light GBM\n7. XgBoost Classifier\n\nModel outputs are stored in 2 tables- \n1. Model parameters profile (mdl_param_prf)- This stores the model name and relevant score metrics\n2. Model prediction profile (mdl_pred_prf) - This stores the test set predictions from each model (best estimator)","metadata":{}},{"cell_type":"code","source":"# Creating a master dictionary for the model parameters and class instances:-\nmdl_mst_dict = \\\n{\n'Logistic': [LogisticRegression(random_state = 10), {}],\n'SVC': [SVC(random_state = 10), {'C': range(3,10,1)}],\n'DTree' : [DecisionTreeClassifier(random_state= 10), {'max_depth': range(4,8,1), 'min_samples_leaf' : range(3,12,1)}],\n'RandomForest': [RandomForestClassifier(random_state =10), \n                 {'n_estimators': range(50,300,25), 'max_depth': range(4,7,1)}],\n'GBM': [GradientBoostingClassifier(random_state= 10), {'max_depth' : range(2,6,1)}],\n'LGBM': [LGBMClassifier(random_state = 10),{}],\n'XgBoost': [XGBClassifier(eval_metric= 'logloss'), {}]\n};\n\ncv = None;\n\n# Creating model output storage objects:-\nmdl_param_prf = pd.DataFrame(data= None, index= list(mdl_mst_dict.keys()), dtype= np.float32,\n                             columns= ['Precision_Score', 'Recall_Score', 'F1_Score', 'ROC_AUC_Score', 'Accuracy_Score']);\nmdl_pred_prf = pd.DataFrame(data= None, index= None, columns= None, dtype= np.int32);","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:45:10.07834Z","iopub.execute_input":"2022-04-05T16:45:10.078855Z","iopub.status.idle":"2022-04-05T16:45:10.091405Z","shell.execute_reply.started":"2022-04-05T16:45:10.078823Z","shell.execute_reply":"2022-04-05T16:45:10.090384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Implementation routine- \n1. Load the relevant model\n2. Fit the model with the grid search parameters\n3. Create relevant scores for the training set\n4. Extract test set predictions\n5. Integrate the data accordingly into the output tables","metadata":{}},{"cell_type":"code","source":"# Implementing all models:-\nfor mdl_lbl, mdl_param in mdl_mst_dict.items():\n    print(colored(f\"\\nCurrent model is {mdl_lbl}\", color = 'red', attrs= ['bold', 'dark']));\n    grid = GridSearchCV(estimator = mdl_param[0], param_grid = mdl_param[1], scoring='accuracy', refit=True,cv=cv);\n    grid.fit(Xtrain, ytrain);\n    print(colored(f\"\"\"Best estimator is \\n{grid.best_estimator_}\\n\"\"\", color = 'blue'));\n    \n    ytrain_pred = grid.predict(Xtrain);  \n    print(colored(f\"Confusion matrix\\n{confusion_matrix(ytrain['Survived'].values, ytrain_pred)}\", color = 'blue'));\n    \n    mdl_param_prf.loc[mdl_lbl] = (precision_score(ytrain['Survived'].values, ytrain_pred),\n                                  recall_score(ytrain['Survived'].values, ytrain_pred),\n                                  f1_score(ytrain['Survived'].values, ytrain_pred),\n                                  roc_auc_score(ytrain['Survived'].values, ytrain_pred),\n                                  accuracy_score(ytrain['Survived'].values, ytrain_pred)\n                                 );\n    \n    mdl_pred_prf = pd.concat((mdl_pred_prf,pd.DataFrame(grid.predict(Xtest), index = Xtest.index, columns = [mdl_lbl], dtype= np.int32)),\n                             axis= 1, join='outer');\n\nprint(colored(f\"\\n\\nTraining set scores across models:-\\n\", color = 'blue', attrs= ['bold', 'dark']))\ndisplay(mdl_param_prf.style.format('{:.2%}'));","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:45:16.839276Z","iopub.execute_input":"2022-04-05T16:45:16.839569Z","iopub.status.idle":"2022-04-05T16:46:16.08292Z","shell.execute_reply.started":"2022-04-05T16:45:16.839539Z","shell.execute_reply":"2022-04-05T16:46:16.081774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Submission file preparation:-","metadata":{}},{"cell_type":"code","source":"print(colored(f\"Sample submission file\\n\", color= 'blue', attrs= ['dark', 'bold']));\ndisplay(pd.read_csv('../input/titanic/gender_submission.csv', encoding= 'utf8').head(5));\ndisplay(pd.read_csv('../input/titanic/gender_submission.csv', encoding= 'utf8').tail(5));","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:48:17.125305Z","iopub.execute_input":"2022-04-05T16:48:17.125609Z","iopub.status.idle":"2022-04-05T16:48:17.153575Z","shell.execute_reply.started":"2022-04-05T16:48:17.125578Z","shell.execute_reply":"2022-04-05T16:48:17.153021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding new columns for specific mode values:-\nmdl_pred_prf['AllModels'] = mode(mdl_pred_prf, axis=1)[0];\nmdl_pred_prf['BoostedTree'] = mode(mdl_pred_prf[['GBM', 'LGBM', 'XgBoost']], axis=1)[0];\nmdl_pred_prf['Ensemble'] = mode(mdl_pred_prf[['GBM', 'LGBM', 'XgBoost', 'RandomForest']], axis=1)[0];","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:48:22.532482Z","iopub.execute_input":"2022-04-05T16:48:22.532976Z","iopub.status.idle":"2022-04-05T16:48:22.601509Z","shell.execute_reply.started":"2022-04-05T16:48:22.532913Z","shell.execute_reply":"2022-04-05T16:48:22.60047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sel_col_nm = 'AllModels';\npd.DataFrame(mdl_pred_prf[sel_col_nm].values, index = xtest.PassengerId, \n             columns = ['Survived'], dtype= np.int32).reset_index().to_csv(\"Submission.csv\", index= False);","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:48:27.17412Z","iopub.execute_input":"2022-04-05T16:48:27.174803Z","iopub.status.idle":"2022-04-05T16:48:27.186364Z","shell.execute_reply.started":"2022-04-05T16:48:27.174759Z","shell.execute_reply":"2022-04-05T16:48:27.185312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Rough Work:-","metadata":{}},{"cell_type":"code","source":"_ = xtest.groupby(['Pclass', 'Sex']).agg({'PassengerId': np.size});\n_.rename({'PassengerId': 'Passengers'}, axis= 1, inplace= True);\n\nfor mthd in mdl_pred_prf.columns: \n    _0 = mdl_pred_prf.groupby([xtest.Pclass, xtest.Sex]).agg({mthd: np.sum});\n    _0.columns = [mthd + '_Survivors'];\n    _0[mthd + '_SRT'] = _0[mthd + '_Survivors']/ _['Passengers'];\n    _ = pd.concat((_, _0), axis= 1);\n\nprint(colored(f\"\\nTest data details:-\\n\", color = 'blue'));\ndisplay(_.loc[:, _.columns.str.contains('_SRT', case= False)].style.format('{:.2%}'));\n\nprint(colored(f\"\\nTraining data details:-\\n\", color = 'blue'));\n","metadata":{"execution":{"iopub.status.busy":"2022-04-03T13:46:58.119352Z","iopub.execute_input":"2022-04-03T13:46:58.119841Z","iopub.status.idle":"2022-04-03T13:46:58.209177Z","shell.execute_reply.started":"2022-04-03T13:46:58.119807Z","shell.execute_reply":"2022-04-03T13:46:58.208244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_1 = xytrain.groupby(['Pclass', 'Sex']).agg({'PassengerId': np.size, 'Survived': np.sum});\n_1.columns = ['Passengers', 'Survived'];\n_1['Train_Rt'] = _1['Survived']/ _1['Passengers'];\ndisplay(_1)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T13:46:53.088684Z","iopub.execute_input":"2022-04-03T13:46:53.089118Z","iopub.status.idle":"2022-04-03T13:46:53.111137Z","shell.execute_reply.started":"2022-04-03T13:46:53.089076Z","shell.execute_reply":"2022-04-03T13:46:53.110305Z"},"trusted":true},"execution_count":null,"outputs":[]}]}