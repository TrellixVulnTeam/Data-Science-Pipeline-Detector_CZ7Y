{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic Solution Beginner Walkthrough\n\nThe purpose of this notebook is to show how I have navigated through the Kaggle Titanic project by following an effective Data Science Workflow.\n\nI will always treat this notebook as a **work in progress** because I will continuously seek to improve my work as I gain more knowledge and understanding throughout my Data Science learning journey.\n\n## Data Science Workflow\n> The foundation of this workflow was based on the author's citations in this [notebook](https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy/notebook). I figured there was still room for more C's.\n- **Comprehend.** *Exploratory Data Analysis.* Understand the nature and relationships among each features in the datasets through data analyses and visualization.\n- **Correlate.** *Feature Selection* Validate the strength of association across features with the appopriate statistical tools and metrics, and to select the features that are significantly relevant with the solution goal.\n- **Clean.** *Data Cleaning.* Identify and remedy missing/null values by imputing them with reasonable inputs.  \n- **Create.** *Feature Engineering.* Create new features out of the existing ones which can make better predictions while also reducing noise in the number of features.\n- **Classify.** *Feature Engineering.* Practice more advanced feature engineering involving effectively classifying labels/classes that are similar in terms of distribution, which also aims to reduce noise.\n- **Convert.** *Data Preprocessing.* Perform the necessary adjustments (one-hot encoding) and data transformations (i.e. sqrt, log trasformations) to make the data fit for modelling.\n- **Complete.** *Training Model.* Completion of a working and cleaned dataset in preparation for training the model and predicting solutions out of it. \n- **Configure.** *Hyperparameter Tuning.* Further optimize our learning algorithms by determining and running the optimal parameters. \n- **Combine.** *Ensemble Learning.* Combine multiple algorithms into one that can leverage the strengths and compensates the weaknesses of the tested models.\n\n## Assumptions\nTo prevent **data leakage and cheating,** analyses and data transformations will exclusively be done and derived based on training data alone without the support of the test data.\n> Data leakage happens when information outside of train set was used to assist in creating model. While there is merits to substantially achieve higher scores on our train and validation sets alone, this model will usually result in poorer performances when ran on other testing sets that are **new** and **unseen.** ","metadata":{}},{"cell_type":"code","source":"# data analysis\nimport pandas as pd\nimport numpy as np\n\n# data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:14.321491Z","iopub.execute_input":"2022-05-27T05:49:14.321752Z","iopub.status.idle":"2022-05-27T05:49:14.825819Z","shell.execute_reply.started":"2022-05-27T05:49:14.321726Z","shell.execute_reply":"2022-05-27T05:49:14.825062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Acquire Training and Testing Data\n\nThe information regarding the features are explicitly presented [HERE](https://www.kaggle.com/competitions/titanic/data?select=train.csv) in detail.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\ntest_df_copy = test_df.copy()\ndf = [train_df, test_df]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:14.827077Z","iopub.execute_input":"2022-05-27T05:49:14.827495Z","iopub.status.idle":"2022-05-27T05:49:14.85677Z","shell.execute_reply.started":"2022-05-27T05:49:14.827465Z","shell.execute_reply":"2022-05-27T05:49:14.856097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:14.857927Z","iopub.execute_input":"2022-05-27T05:49:14.858304Z","iopub.status.idle":"2022-05-27T05:49:14.882162Z","shell.execute_reply.started":"2022-05-27T05:49:14.858274Z","shell.execute_reply":"2022-05-27T05:49:14.881531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:14.884577Z","iopub.execute_input":"2022-05-27T05:49:14.88531Z","iopub.status.idle":"2022-05-27T05:49:14.900121Z","shell.execute_reply.started":"2022-05-27T05:49:14.885263Z","shell.execute_reply":"2022-05-27T05:49:14.899473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:14.979944Z","iopub.execute_input":"2022-05-27T05:49:14.980489Z","iopub.status.idle":"2022-05-27T05:49:15.004427Z","shell.execute_reply.started":"2022-05-27T05:49:14.980446Z","shell.execute_reply":"2022-05-27T05:49:15.003787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:15.159724Z","iopub.execute_input":"2022-05-27T05:49:15.16048Z","iopub.status.idle":"2022-05-27T05:49:15.173751Z","shell.execute_reply.started":"2022-05-27T05:49:15.160438Z","shell.execute_reply":"2022-05-27T05:49:15.172874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_df)/(len(train_df)+len(test_df))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:15.335532Z","iopub.execute_input":"2022-05-27T05:49:15.335996Z","iopub.status.idle":"2022-05-27T05:49:15.342822Z","shell.execute_reply.started":"2022-05-27T05:49:15.33595Z","shell.execute_reply":"2022-05-27T05:49:15.341998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-05-27T05:49:15.562234Z","iopub.execute_input":"2022-05-27T05:49:15.562489Z","iopub.status.idle":"2022-05-27T05:49:15.597336Z","shell.execute_reply.started":"2022-05-27T05:49:15.562462Z","shell.execute_reply":"2022-05-27T05:49:15.596626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations\n\n**Distribution**\n- The train-test is split around 70/30, with testing set representing 31.93% of the combined sets.\n- The survival rate in context of the training set is at 38.38%, which is representative of the original survival rate of 32.46%, 1502 out of 2224.\n- Majority (around 75%) of the people didn't aboard with siblings/spouses.\n- The distributions of SibSP and Parch are right skewed, since the means are greater than the medians.\n\n**Data Types**\n- Numerical: *Age, Sibsp, Parch, Fare*\n- Categorical: *Survival, Sex, Pclass, Embarked*\n- Mixed/Alphanumeric: *Name, Ticket, Cabin*","metadata":{}},{"cell_type":"markdown","source":"I picked a cold and chilling palette to fit the theme of the project.","metadata":{}},{"cell_type":"code","source":"# Create and set palette\ncolors = ['#79DAE8', '#2155CD']\nsns.set_palette(sns.color_palette(colors))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:15.738084Z","iopub.execute_input":"2022-05-27T05:49:15.738363Z","iopub.status.idle":"2022-05-27T05:49:15.742474Z","shell.execute_reply.started":"2022-05-27T05:49:15.738334Z","shell.execute_reply":"2022-05-27T05:49:15.74164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Numerical Features\nFor the numerical variables, the seaborn **pairplot** will be helpful in presenting the pariwise relationships across each numerical variables. The diagonal plots are the main focus here as they are treated as distribution plots of the features. The rest is just being extra and for eye candy.","metadata":{}},{"cell_type":"code","source":"# Separate the training set into groups of numerical and categorical variables.\n# Don't worry, the 'Survived' was only included in the numerical category so we can use it to classify the features when we create the pairplot\ndf_num = train_df[['Age', 'Survived', 'SibSp', 'Parch', 'Fare']]\ndf_cat = train_df[['Survived', 'Pclass', 'Sex', 'Embarked']]\n\n# Classify by 'Survived'\nsns.set_style('darkgrid')\nsns.pairplot(df_num, hue='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:15.875987Z","iopub.execute_input":"2022-05-27T05:49:15.876747Z","iopub.status.idle":"2022-05-27T05:49:20.526986Z","shell.execute_reply.started":"2022-05-27T05:49:15.876702Z","shell.execute_reply":"2022-05-27T05:49:20.526087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig= plt.figure(figsize=(15, 5))\n\nax = fig.add_subplot(1,2,1)\nsns.histplot(data=train_df, x=\"Age\", hue=\"Survived\", binwidth=2, kde=True)\nplt.title('Age',\n          fontsize = 15,\n          fontweight = 'bold',\n          fontfamily = 'serif',\n          loc = 'left')\n\nplt.subplot(1,2,2)\nsns.histplot(data=train_df, x=\"Fare\", hue=\"Survived\", binwidth=5, kde=True)\nplt.title('Fare',\n          fontsize = 15,\n          fontweight = 'bold',\n          fontfamily = 'serif',\n          loc = 'left')\n\n# Zoomed along the axes for clearer observation of Fare distribution\nplt.xlim([0, 200])\nplt.ylim([0, 100])\n\n# Define a function to create countplots with the desired style and format\ndef count_plot(df, x, y):\n    fig = plt.figure(figsize = (15, 5))\n    \n    plt.subplot(1,2,1)\n    ax = sns.countplot( x=df[x].dropna(), hue=df[y], palette=['#79DAE8', '#2155CD'])\n    plt.legend(loc='upper right')\n    plt.title(str(x),\n          fontsize = 15,\n          fontweight = 'bold',\n          fontfamily = 'serif',\n          loc = 'left')\n    ax.set(xlabel=None, ylabel=None)\n    \n    plt.subplot(1,2,2)\n    plt.ylim(-0.1, 1)\n    ax = sns.lineplot(x=df[x], y=df[y], data=df, ci=None, linewidth=2, marker=\"o\", color='#0AA1DD')\n    ax.set(xlabel=None, ylabel=None)\n    \n    plt.show()\n\ncount_plot(train_df, 'SibSp', 'Survived')\ncount_plot(train_df, 'Parch', 'Survived')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:20.528876Z","iopub.execute_input":"2022-05-27T05:49:20.529354Z","iopub.status.idle":"2022-05-27T05:49:22.191822Z","shell.execute_reply.started":"2022-05-27T05:49:20.52931Z","shell.execute_reply":"2022-05-27T05:49:22.190791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Age**\n- More normally distributed compared to the rest.\n- Majority of passengers fall in the 20-35 age bracket.\n- A large number of 20-30 yr olds didn't survive.\n- Infants (age<4) had one of the highest survival rates.\n- The oldest individual (age=80) survived.\n\n**SibSp**\n- Those with 1-2 siblings/spouses were likely to survive.\n- Large number of passengers didn't have siblings/spouses with them.\n- An outlier, with over 8 siblings/spouses, didn't survive.\n\n**Parch**\n- Those with 1-3 parents/children were likely to survive\n- Large number of passengers didn't have parents/children with them.\n- Passengers without parents and children with them were more likely to die.\n\n**Fare**\n- Skewed to the right.\n- Majority of the passengers aboarded with cheaper fares.\n- Most passengers with cheaper fares (<50) didn't survive, while those who paid higher fares (>300) tend to survive.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## Correlating Numerical Features","metadata":{}},{"cell_type":"code","source":"import matplotlib.colors\n\n# Assign palette to cmap in sns.heatmap\ncmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", ['#E8F9FD', '#79DAE8', '#0AA1DD', '#2155CD'])\n\nplt.subplots(figsize=(10,7))\nsns.heatmap(df_num.corr(), cmap=cmap, annot=True, linewidths=3, annot_kws={\"fontsize\":13}, square=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:22.193376Z","iopub.execute_input":"2022-05-27T05:49:22.193709Z","iopub.status.idle":"2022-05-27T05:49:22.512679Z","shell.execute_reply.started":"2022-05-27T05:49:22.193653Z","shell.execute_reply":"2022-05-27T05:49:22.51189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trasnforming Numerical Features\nThe **probability plot** or **quantile-quntile plot (QQplot)** allows us to plot our sample data against the quantiles of a normal distribution. This will serve as reference to see how our subsequent data transformations react to the curve, and enable us to select the best form of transformation which resulted to the best fit.\n\nWe can try use the following data transformation techniques:\n- Square Root\n- Cube Root\n- Logarithmic. **Log(x+1)** wil ensure the log transformation won't result in undefined values because our data contains 'zero' values, and log(0) returns undefined.","metadata":{}},{"cell_type":"code","source":"import scipy.stats as stats\n\n# Defining the function to generate the distribution plot alongside QQplot\ndef QQplot(df, col):\n    plt.figure(figsize = (15, 5))\n    plt.subplot(1,2,1)\n    sns.histplot(x=df[col].dropna(), color='#2155CD', kde=True)\n    plt.title(str(col),\n          fontsize = 15,\n          fontweight = 'bold',\n          fontfamily = 'serif',\n          loc = 'left')\n    \n    plt.subplot(1,2,2)\n    stats.probplot(df[col].dropna(), dist=\"norm\", plot=plt)\n    \nQQplot(train_df, 'Fare')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:22.514689Z","iopub.execute_input":"2022-05-27T05:49:22.51519Z","iopub.status.idle":"2022-05-27T05:49:23.147208Z","shell.execute_reply.started":"2022-05-27T05:49:22.515146Z","shell.execute_reply":"2022-05-27T05:49:23.146278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform Fare and compare\ntr_Fare = train_df[['Survived', 'Fare']]\n\ntr_Fare[\"Fare_sqrt\"] = tr_Fare['Fare']**(1./2)\nQQplot(tr_Fare, 'Fare_sqrt')\n\ntr_Fare[\"Fare_cbrt\"] = tr_Fare['Fare']**(1./3)\nQQplot(tr_Fare, 'Fare_cbrt')\n\ntr_Fare[\"Fare_log(x+1)\"] = np.log(tr_Fare['Fare'] + 1)\nQQplot(tr_Fare, 'Fare_log(x+1)')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:23.14838Z","iopub.execute_input":"2022-05-27T05:49:23.148634Z","iopub.status.idle":"2022-05-27T05:49:24.839387Z","shell.execute_reply.started":"2022-05-27T05:49:23.148598Z","shell.execute_reply":"2022-05-27T05:49:24.838628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(10,7))\nsns.heatmap(tr_Fare.corr(), cmap=cmap, annot=True, linewidths=3, annot_kws={\"fontsize\":13}, square=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:24.84047Z","iopub.execute_input":"2022-05-27T05:49:24.840864Z","iopub.status.idle":"2022-05-27T05:49:25.195621Z","shell.execute_reply.started":"2022-05-27T05:49:24.840835Z","shell.execute_reply":"2022-05-27T05:49:25.194098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion.** Transform *Fare* with Log(x+1) because it yielded the best fit and correlation.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"# Categorical Features\n\nLet's use **countplots** to visualize the distribution of each classification with respect to survival, and **lineplots** to determine the corresponding survival rates (in % form). ","metadata":{}},{"cell_type":"code","source":"count_plot(train_df, 'Sex', 'Survived')\ncount_plot(train_df, 'Embarked', 'Survived')\ncount_plot(train_df, 'Pclass', 'Survived')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:25.196655Z","iopub.execute_input":"2022-05-27T05:49:25.196865Z","iopub.status.idle":"2022-05-27T05:49:26.141627Z","shell.execute_reply.started":"2022-05-27T05:49:25.196839Z","shell.execute_reply":"2022-05-27T05:49:26.141066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations\n\n**Sex**\n- More males were on board.\n- Females had a higher survival rate than males did. No wonder Jack died over Rose.\n\n**Pclass**\n- Most passengers were in Pclass 3. \n- The significance of higher classes being correlated with higher survival rate is justified.\n- Pclass 1 is the only class with more survived passengers than dead passengers.\n\n**Embarked**\n- A major chunk of the passengers embarked from Southampton, and the least from Queensland.\n- Highest survival rate were found for passengers that embarked from C.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"# Alphanumeric Features\n- Create *Title* from passengers' names.\n- Create *Deck* from the first letter/unit in *Cabin.*","metadata":{}},{"cell_type":"code","source":"for dataset in df:\n    # Create Title    \n    # Split the full name into a list by comma, then return the title by indexing the 2nd position [1]\n    # Split the name into a list by period, then return the title by indexing the 1st position [0]\n    dataset['Title'] = dataset['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n    \n    # Create Deck\n    dataset['Deck'] = dataset['Cabin'].apply(lambda x: str(x)[0])\n\n# Adjust count_plot function with additional formatting (rotate x_labels)\ndef count_plot_adj(df, x, y):\n    fig = plt.figure(figsize = (15, 5))\n    \n    plt.subplot(1,2,1)\n    ax = sns.countplot(x=df[x], hue=df[y], palette=['#79DAE8', '#2155CD'])\n    plt.legend(loc='upper right')\n    plt.title(str(x),\n          fontsize = 15,\n          fontweight = 'bold',\n          fontfamily = 'serif',\n          loc = 'left')\n    plt.xticks(rotation=60)\n    ax.set(xlabel=None, ylabel=None)\n    \n    plt.subplot(1,2,2)\n    plt.ylim(-0.1, 1.1)\n    ax = sns.lineplot(x=df[x], y=df[y], data=df, ci=None, linewidth=2, marker=\"o\", color='#0AA1DD')\n    plt.xticks(rotation=60)\n    ax.set(xlabel=None, ylabel=None)\n\ncount_plot_adj(train_df, 'Title', 'Survived')\ncount_plot(train_df, 'Deck', 'Survived')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:26.142658Z","iopub.execute_input":"2022-05-27T05:49:26.142957Z","iopub.status.idle":"2022-05-27T05:49:27.017459Z","shell.execute_reply.started":"2022-05-27T05:49:26.142932Z","shell.execute_reply":"2022-05-27T05:49:27.016638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def box_plot_adj(df, x, y):\n    \n    ax = sns.boxplot(x=df[x].dropna(),\n                       y=df[y].dropna(),\n#                        palette=['#79DAE8', '#0AA1DD', '#2155CD']\n                      )\n    \n    plt.title(str(y) + ' by ' + str(x),\n          fontsize = 15,\n          fontweight = 'bold',\n          fontfamily = 'serif',\n          loc = 'left')\n    \n    ax.set(xlabel=None, ylabel=None)\n    \n\nplt.figure(figsize = (15, 7))\nbox_plot_adj(train_df, 'Title', 'Age')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:27.018808Z","iopub.execute_input":"2022-05-27T05:49:27.019039Z","iopub.status.idle":"2022-05-27T05:49:27.479107Z","shell.execute_reply.started":"2022-05-27T05:49:27.018996Z","shell.execute_reply":"2022-05-27T05:49:27.47835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Joint distribution between Pclass and Deck\nDeck_Pclass = train_df.groupby(['Pclass', 'Deck'])['Deck'].size().unstack().fillna(0)\nplt.subplots(figsize=(12,12))\nsns.heatmap(Deck_Pclass, cmap=cmap, annot=True, annot_kws={\"fontsize\":13}, linewidths=3, square=True, cbar=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:27.482818Z","iopub.execute_input":"2022-05-27T05:49:27.483057Z","iopub.status.idle":"2022-05-27T05:49:27.766142Z","shell.execute_reply.started":"2022-05-27T05:49:27.483031Z","shell.execute_reply":"2022-05-27T05:49:27.765352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prefixes Meaning\n\n- **Rev.** Ministers of most Christian denominations; ordained clergymen since 17th century.\n- **Mlle, Mme, Miss.** Unmarried female; young lady.\n- **Master.** Boys (<15 yrs old)\n- **Major, Don.** Lord, master, or owner (of a household).\n- **Col.** Colonel; army officer of high rank.\n- **the Countess.** Wife or widow of a count.\n- **Capt.** could refer to the captain of the ship.\n- **Ms.** Any women regardless of marital status.\n- **Lady.** Princesses or daughters of royal blood.\n- **Jonkheer.** Female equivalent denoting the lowest rank within the nobility.\n\n### Domain Understanding\n\nI did some research and here is what I found:\n- There are certain parts/decks in the ship that were only exclusively accessible to passengers of belonging to certain *Pclasses.*\n- The **front half** of the ship reached the bottom first, followed by the other **back half** two minutes later.\n- It appears that Amidships, located at the middle of the ship, is typically where most of the passengers stayed.\n- A detailed list of Cabins and List can be found in the link below:\n> [Titanic Facts: The Layout of the Ship | Dummies](https://www.dummies.com/article/academics-the-arts/history/20th-century/titanic-facts-the-layout-of-the-ship-180759/)\n - Cabins at Decks **A, B, C** were exclusively reserved for **Pclass 1.**\n - Cabins at Deck **D, E** were shared by all **Pclass 1, 2, 3.**\n - Cabins at Deck **F** were shared by **Pclass 2 and 3.**\n - Decks **G, T** were stated to be mostly occupied by crew members, but the presence of Pclass 1 in T makes me assume that there were only a small group of Pclass 1 there, but still dominated by Pclass 2 and 3.\n\n\n- Classify *Title* based from researched information:\n - Mlle and Ms > Miss\n - Mme > Mrs\n - Uncommon titles that had a higher survival rate will be assumed/regarded as elite status > Elite\n - Remaining uncommon titles > Others\n\n\n- Null values will be treated as a label. From meta-analysis alone, it can be inferred that majority of the cabin decks weren't obtained from the passengers who died.","metadata":{}},{"cell_type":"code","source":"for dataset in df:\n    \n    # Classify Title\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n#     dataset['Title'] = dataset['Title'].replace(['Capt', 'Col', 'Don', 'Dona', 'Dr', 'Jonkheer', 'Lady', 'Major', 'Rev', 'Sir','the Countess'], 'Others')\n    dataset['Title'] = dataset['Title'].replace(['Col', 'Lady', 'Major', 'Sir','the Countess'], 'Elite')\n    dataset['Title'] = dataset['Title'].replace(['Capt', 'Dona', 'Dr', 'Capt', 'Rev', 'Don','Jonkheer'], 'Others')\n    \n    # No Cabin\n    dataset['NoDeck'] = 0\n    dataset.loc[dataset.Deck == 'n', 'NoDeck'] = 1\n    \n    # DeckGroup\n    dataset.loc[(dataset.Deck == 'A') |\n                (dataset.Deck == 'B') |\n                (dataset.Deck == 'C'), 'DeckGroup'] = 'ABC'\n    \n    dataset.loc[(dataset.Deck == 'D') |\n                (dataset.Deck == 'E'), 'DeckGroup'] = 'DE'    \n    \n    dataset.loc[(dataset.Deck == 'F') |\n                (dataset.Deck == 'G') |\n                (dataset.Deck == 'T'), 'DeckGroup'] = 'FGT'   \n    \n    dataset.loc[dataset.Deck == 'n', 'DeckGroup'] = 'n'\n    \ncount_plot_adj(train_df, 'Title', 'Survived')\ncount_plot(train_df, 'DeckGroup', 'Survived')\ncount_plot(train_df, 'NoDeck', 'Survived')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:27.767763Z","iopub.execute_input":"2022-05-27T05:49:27.7682Z","iopub.status.idle":"2022-05-27T05:49:28.961545Z","shell.execute_reply.started":"2022-05-27T05:49:27.768161Z","shell.execute_reply":"2022-05-27T05:49:28.960709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Titles**\n\n- It justifies the correlation between 'Sex' and 'Survived' as titles with 'Mr' and 'Master' tend to have lower survival rates than 'Mrs' and 'Miss'.\n- The 'Others' category compiles a very small sample of the given dataset. Despite these titles having characterized of nobile status, it appears that they were trivial at the time of the crisis.","metadata":{}},{"cell_type":"markdown","source":"## Missing Values\n- *Age, Fare, and Embarked.* There is no need to impute *Cabin* anymore.","metadata":{"tags":[]}},{"cell_type":"code","source":"# Remove 'Embark' null values\nfor dataset in df:\n    dataset.dropna(subset=['Embarked'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:28.962797Z","iopub.execute_input":"2022-05-27T05:49:28.96338Z","iopub.status.idle":"2022-05-27T05:49:28.973769Z","shell.execute_reply.started":"2022-05-27T05:49:28.963334Z","shell.execute_reply":"2022-05-27T05:49:28.972789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def box_plot(df, x, y):\n    \n    ax = sns.boxplot(x=df[x].dropna(),\n                     y=df[y].dropna(),\n                     width = .3,\n                     palette=['#79DAE8', '#0AA1DD', '#2155CD'])\n    plt.title(str(y) + ' by ' + str(x),\n          fontsize = 15,\n          fontweight = 'bold',\n          fontfamily = 'serif',\n          loc = 'left')\n    \n    ax.set(xlabel=None, ylabel=None)\n \n\nplt.figure(figsize = (15, 5))\n\nplt.subplot(1, 2, 1)\nbox_plot(train_df, 'Pclass', 'Age')\n\nplt.subplot(1, 2, 2)\nbox_plot(train_df, 'Pclass', 'Fare')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:52.65167Z","iopub.execute_input":"2022-05-27T05:49:52.651971Z","iopub.status.idle":"2022-05-27T05:49:52.981235Z","shell.execute_reply.started":"2022-05-27T05:49:52.651935Z","shell.execute_reply":"2022-05-27T05:49:52.980265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# g = sns.FacetGrid(train_df, row='Pclass', col='Title', hue='Survived')\n# g.map(sns.boxplot, y='Age')\n\ng = sns.catplot(data=train_df,\n                x='Title',\n                y='Age',\n                hue=\"Survived\",\n                col=\"Pclass\",\n                kind=\"box\",\n               )","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:54.6239Z","iopub.execute_input":"2022-05-27T05:49:54.624526Z","iopub.status.idle":"2022-05-27T05:49:55.800895Z","shell.execute_reply.started":"2022-05-27T05:49:54.624488Z","shell.execute_reply":"2022-05-27T05:49:55.800076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's imputing *Age* based from the median of its correlated features *Pclass* and *Title.*, and *Fare* based from *Pclass.*","metadata":{}},{"cell_type":"code","source":"# Impute missing values in 'Age' and 'Fare'\nfor dataset in df:\n    dataset['Age'] = dataset.groupby(['Pclass', 'Title'])['Age'].apply(lambda x: x.fillna(x.median()))\n    dataset['Fare'] = dataset.groupby('Pclass')['Fare'].apply(lambda x: x.fillna(x.median()))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:58.654382Z","iopub.execute_input":"2022-05-27T05:49:58.654668Z","iopub.status.idle":"2022-05-27T05:49:58.684952Z","shell.execute_reply.started":"2022-05-27T05:49:58.654636Z","shell.execute_reply":"2022-05-27T05:49:58.683977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering Numerical Features\n\n- Create *FareBin* by binning *Fare*.\n- Create *AgeBin* by binning *Age*.\n- Create *FamilySize* by combining *SibSp* and *Parch*.","metadata":{}},{"cell_type":"code","source":"# Creating FamilySize\nfor dataset in df:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:49:59.946371Z","iopub.execute_input":"2022-05-27T05:49:59.946836Z","iopub.status.idle":"2022-05-27T05:49:59.954241Z","shell.execute_reply.started":"2022-05-27T05:49:59.946776Z","shell.execute_reply":"2022-05-27T05:49:59.953467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can obtain the optimal number of bins by running corr() tests over a range of bin parameters through a for loop. Notice that this strategy appears viable due to the ordinal nature of *FareGroup*.","metadata":{}},{"cell_type":"code","source":"from scipy import stats\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel = LabelEncoder()\n\n# Optimize bins for Fare via Pearson's corr test\n# Define a function to iterate through a number of different bin values\ndef corr_test(x, x_bin, min, max):\n    \n    row = 0\n    \n    # Setting up dataframe where\n        # i = no. of bins\n        # R = Pearson's R; strength of correlation\n        # p_value = statistical significance\n        \n    Corr = ['i', 'R', 'p_value']\n    Corr_tests = pd.DataFrame(columns = Corr)\n\n    # Select a range of values for bins to test with \n    for i in range(min, max):\n        \n        Corr_tests.loc[row, 'i'] = i\n\n        # Return R, p_value and store in dataframe\n        train_df[x_bin] = pd.qcut(train_df[x], i)\n        train_df[x_bin] = label.fit_transform(train_df[x_bin])\n        r, p = stats.pearsonr(train_df[x_bin], train_df['Survived'])\n\n        Corr_tests.loc[row, 'R'] = r\n        Corr_tests.loc[row, 'p_value'] = p\n\n        row += 1\n\n    # Print out dataframe\n    Corr_tests.sort_values(by=['R'], ascending=False, inplace=True)\n    return Corr_tests\n\ncorr_test('Fare', 'FareGroup', 2, 15)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:50:00.83552Z","iopub.execute_input":"2022-05-27T05:50:00.835798Z","iopub.status.idle":"2022-05-27T05:50:00.963365Z","shell.execute_reply.started":"2022-05-27T05:50:00.835768Z","shell.execute_reply":"2022-05-27T05:50:00.962456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This may not be replicated for *AgeBin* because they survival varies across different demographic groups. Let's visualize it demographically.","metadata":{}},{"cell_type":"code","source":"for dataset in df:\n    dataset.loc[(dataset['Age'] <= 6), 'AgeBin'] = 'Infant to Toddler'\n    dataset.loc[(dataset['Age'] > 6) & (dataset['Age'] <= 18), 'AgeBin'] = 'Middle Childhood'\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 26), 'AgeBin'] = 'Youth'\n    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 62), 'AgeBin'] = 'Adult'\n    dataset.loc[(dataset['Age'] > 62), 'AgeBin'] = 'Elder'\n    \ncount_plot(train_df, 'FamilySize', 'Survived')\ncount_plot_adj(train_df, 'AgeBin', 'Survived')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:50:02.82894Z","iopub.execute_input":"2022-05-27T05:50:02.829284Z","iopub.status.idle":"2022-05-27T05:50:03.610334Z","shell.execute_reply.started":"2022-05-27T05:50:02.82925Z","shell.execute_reply":"2022-05-27T05:50:03.609705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights\n- If you still remember, our log transformed *Fare* was a better candidate feature with a higher correlation value compared to the frequency encoded *FareBin*. Hence, I choose to keep the transformed *Fare*.\n- In *AgeBin*, it appears that the youth and elder groups have the lowest survival rates, followed by the adult and child group, while Infants to Toddlers had the highest.\n- *FamilySize* can be further classified into *Alone (FamilySize=1), Small (2 to 4), Large (>5).*","metadata":{}},{"cell_type":"code","source":"# Classify FamilySize\nfor dataset in df:\n    dataset.loc[dataset['FamilySize'] == 1, 'FamilyBin'] = 'Alone'\n    dataset.loc[(dataset['FamilySize'] >= 2) & (dataset['FamilySize'] <= 4), 'FamilyBin'] = 'Small'\n    dataset.loc[dataset['FamilySize'] >= 5, 'FamilyBin'] = 'Large'\n                 \ncount_plot(train_df, 'FamilyBin', 'Survived')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:50:04.868785Z","iopub.execute_input":"2022-05-27T05:50:04.869512Z","iopub.status.idle":"2022-05-27T05:50:05.176318Z","shell.execute_reply.started":"2022-05-27T05:50:04.869465Z","shell.execute_reply":"2022-05-27T05:50:05.175523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy = train_df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:50:05.787274Z","iopub.execute_input":"2022-05-27T05:50:05.787559Z","iopub.status.idle":"2022-05-27T05:50:05.793336Z","shell.execute_reply.started":"2022-05-27T05:50:05.787526Z","shell.execute_reply":"2022-05-27T05:50:05.792249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlating Categorical Features\nAssessing the strength of association (correlation) across variables is one way to conduct feature selection. This time, it wouldn't be ideal to use **Pearson's correlation matrix** like what we did for our numerical variables earlier because we are now dealing with categorical variables (both the predictors and response variables). \n> However, an exception can be made if you are comparing between **two dichotomous categorical variables** (a variable that takes on one of only two possible values). For instance, *Sex* vs *Survived*.\n\nRather than assessing the strength between two variables, it is more viable to conduct a test of independency to assess if two categorical variables are either independent or if they are in fact related to one another. Given this premise, **[Chi-square Test of Independence](https://towardsdatascience.com/chi-square-test-for-feature-selection-in-machine-learning-206b1f0b8223#:~:text=In%20feature%20selection%2C%20we%20aim,hypothesis%20of%20independence%20is%20incorrect.)** is the way to go.\n\nBefore we can conduct Chi-square tests, we must ensure that our categorical data are numerically encoded first using `LabelEncoder()`.","metadata":{"tags":[]}},{"cell_type":"code","source":"from sklearn.feature_selection import chi2\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats.contingency import association\n\nX_chi = df_copy[['Pclass', 'Sex', 'Embarked', 'AgeBin', 'FamilyBin', 'NoDeck', 'DeckGroup']]\ny_chi = df_copy['Survived']\n\ndf_encoded = pd.DataFrame()\n\nfor i in X_chi.columns:\n    X_chi[i] = label.fit_transform(X_chi[i])\n      \n\nChi2, p = chi2(X_chi, y_chi)\ndata = {'Chi2': Chi2,\n        'p_value': p\n       }\n\n_ = pd.DataFrame(index=X_chi.columns,\n                 data=data\n                )\n_","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:50:07.278399Z","iopub.execute_input":"2022-05-27T05:50:07.278678Z","iopub.status.idle":"2022-05-27T05:50:07.434206Z","shell.execute_reply.started":"2022-05-27T05:50:07.278645Z","shell.execute_reply":"2022-05-27T05:50:07.433297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion.** All of our categorical variables are significantly dependent on our target variable.","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Log transform Fare\n# Label encode Sex\nfor dataset in df:\n    dataset['Fare'] = np.log(dataset['Fare']+1)\n    dataset['Sex'] = label.fit_transform(dataset['Sex'])","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:50:11.252941Z","iopub.execute_input":"2022-05-27T05:50:11.253225Z","iopub.status.idle":"2022-05-27T05:50:11.260807Z","shell.execute_reply.started":"2022-05-27T05:50:11.253196Z","shell.execute_reply":"2022-05-27T05:50:11.260148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:50:11.749668Z","iopub.execute_input":"2022-05-27T05:50:11.750269Z","iopub.status.idle":"2022-05-27T05:50:11.77094Z","shell.execute_reply.started":"2022-05-27T05:50:11.750228Z","shell.execute_reply":"2022-05-27T05:50:11.770094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:50:13.228713Z","iopub.execute_input":"2022-05-27T05:50:13.230058Z","iopub.status.idle":"2022-05-27T05:50:13.253476Z","shell.execute_reply.started":"2022-05-27T05:50:13.229969Z","shell.execute_reply":"2022-05-27T05:50:13.252542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store relevant columns\ny_train = train_df['Survived']\n\nX_train = train_df[['Pclass', 'Sex', 'Fare', 'Embarked', 'AgeBin', 'DeckGroup', 'FamilyBin']]\nX_test = test_df[['Pclass', 'Sex', 'Fare', 'Embarked', 'AgeBin', 'DeckGroup', 'FamilyBin']]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:50:13.910056Z","iopub.execute_input":"2022-05-27T05:50:13.910355Z","iopub.status.idle":"2022-05-27T05:50:13.91935Z","shell.execute_reply.started":"2022-05-27T05:50:13.910323Z","shell.execute_reply":"2022-05-27T05:50:13.918439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\n\n# Creating dummy indicator columns for categorical variables\nX_train = pd.get_dummies(X_train, columns=['Embarked', 'AgeBin', 'FamilyBin', 'DeckGroup'])\nX_test = pd.get_dummies(X_test, columns=['Embarked', 'AgeBin', 'FamilyBin', 'DeckGroup'])\n\n# Scale num features\nX_train[['Fare', 'Pclass']] = scale.fit_transform(X_train[['Fare', 'Pclass']])\nX_test[['Fare', 'Pclass']] = scale.transform(X_test[['Fare', 'Pclass']])","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:50:38.795936Z","iopub.execute_input":"2022-05-27T05:50:38.796489Z","iopub.status.idle":"2022-05-27T05:50:38.82122Z","shell.execute_reply.started":"2022-05-27T05:50:38.796449Z","shell.execute_reply":"2022-05-27T05:50:38.82062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:50:40.842323Z","iopub.execute_input":"2022-05-27T05:50:40.843146Z","iopub.status.idle":"2022-05-27T05:50:40.866062Z","shell.execute_reply.started":"2022-05-27T05:50:40.843103Z","shell.execute_reply":"2022-05-27T05:50:40.865194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preliminary Model Training\nI begin training my model by creating a baseline assessment to determine which among the algorithms perform well based on my train and test sets, and work my way towards hyperparameter tuning and ensembling from there. Since I am working with a given dataset with a predetermined solution goal, I am running a form of machine learning algorithm known as supervised learning. Listed below are some common model algorithms to try out.\n\n> If you want to understand the math and fundamentals behind each ML algorithm, I highly recommend watching video tutorials by *StatQuest with Josh Starmer.*\n\n- [Logistic Regression](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjdkvWQ4qT3AhVEQd4KHUvNDPIQwqsBegQIFhAB&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DyIYKR4sgzI8&usg=AOvVaw3maZPWy-T2rEc4PFDM40af)\n- [Support Vector Machines](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjM0pq45KT3AhVcQfUHHVYJBysQwqsBegQIAhAB&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DefR1C6CvhmE&usg=AOvVaw1alnpuy6aMk4ogaK4NtmXy)\n- [K-Means Nearest Neighbors](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwj60qDK5KT3AhUbAYgKHcyoDbAQwqsBegQIAhAB&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DHVXime0nQeI&usg=AOvVaw1h03i8dfC0gXYPU9lFRzJ_)\n- [Decision Tree](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwi-t-rS5KT3AhWGAogKHbBfBGQQwqsBegQIAhAB&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D7VeUPuFGJHk&usg=AOvVaw2KBODG3Oh7AiSz-4h5wnMd)\n- [Random Forest](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiyptHZ5KT3AhWNEYgKHelnCqgQwqsBegQIAhAB&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DJ4Wdy0Wc_xQ&usg=AOvVaw0moI0sPTwd34hRAKxbDRVN)\n- XGBoost\n- LGBMClassifier\n- CatBoost\n\nAnother important approach which is crucial here is to conduct cross validation. It is a useful technique to address overfitting as it evaluates models through a fixed number of folds k. In my case, I decided to do 10-fold cross-validation. In other words, I do 10 different subsets of sample from training set to arrive at my solutions, then get the mean of all the accuracy scores from these tests.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n#Common Model Algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n# Defining a list of Machine Learning Algorithms I will be running\nMLA = [\n    LogisticRegression(max_iter = 2000),\n    SVC(),\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    XGBClassifier(),\n    LGBMClassifier(),\n    CatBoostClassifier(verbose=False)\n]\n\nrow_index = 0\n\n# Setting up the table to compare the performances of each model\nMLA_cols = ['Model', 'Accuracy']\nMLA_compare = pd.DataFrame(columns = MLA_cols)\n\n# Iterate and store scores in the table\nfor model in MLA:\n    MLA_compare.loc[row_index, 'Model'] = model.__class__.__name__\n    cv_results = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')\n    MLA_compare.loc[row_index, 'Accuracy'] = cv_results.mean()\n    \n    row_index+=1\n\n# Present table\nMLA_compare.sort_values(by=['Accuracy'], ascending=False, inplace=True)\nMLA_compare","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:50:45.943652Z","iopub.execute_input":"2022-05-27T05:50:45.943914Z","iopub.status.idle":"2022-05-27T05:51:02.117597Z","shell.execute_reply.started":"2022-05-27T05:50:45.943884Z","shell.execute_reply":"2022-05-27T05:51:02.117031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning\n\nI opted to use optuna, no pun intended, to run my optimizations on my Gradient Boost algorithms.\n1. CatBoost\n2. LGBM\n3. XGBoost\n\nSo, why did I consider using these as opposed to the conventional way with `GridSearchCV` or `RandomSearchCV`?\n- It offers greater versatility in selecting and customizing my grid of parameters.\n\nAnother strategy that I implemented is to tune my hyperparameters for each cross validation fold through a **for loop.** What makes this approach more effective?\n- Likewise how we want to cross validate on training model algorithms to ensure we get a more robust outcome/score by getting the most of the combinations/folds from the whole data, applying this on optimization runs also ensures that our hyperparameter candidate sets will also be tested and evaluated on the whole dataset.\n\n- This also enables me to define a very useful parameter `early_stopping_rounds` when I fit my models in each CV fold.\n> I set this to 100, this means that if the predictions don't improve after 100 trials, the training will stop.","metadata":{"tags":[]}},{"cell_type":"code","source":"import optuna\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n\n# Hypertune XGBoost\ndef objective(trial, data=X_train , target=y_train):\n\n    param = {\n        'n_estimators': 5000,\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_categorical('max_depth', [3, 4, 5, 6, 7, 8, 9, 10]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),\n        'subsample': trial.suggest_categorical('subsample', [0.5, 0.6, 0.7, 0.8, 0.9])\n    }    \n    \n    cv = StratifiedKFold( n_splits=5, shuffle=True, random_state=42)\n    \n    for idx, (train_idx, test_idx) in enumerate(cv.split(X_train, y_train)):\n        \n        x_trn, x_val = X_train.iloc[train_idx], X_train.iloc[test_idx]\n        y_trn, y_val = y_train.iloc[train_idx], y_train.iloc[test_idx]\n    \n    \n        model = XGBClassifier(**param, use_label_encoder = False)\n        \n        model.fit(x_trn,\n                  y_trn,\n                  eval_set = [(x_val, y_val)],\n                  early_stopping_rounds = 100,\n                  eval_metric = 'logloss',\n                  verbose = False\n                 )\n    \n        preds = model.predict(x_val)\n        scores = accuracy_score(y_val, preds)\n        \n    return np.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:51:33.644823Z","iopub.execute_input":"2022-05-27T05:51:33.645119Z","iopub.status.idle":"2022-05-27T05:51:34.143922Z","shell.execute_reply.started":"2022-05-27T05:51:33.645087Z","shell.execute_reply":"2022-05-27T05:51:34.142895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=30)\n\n# print(f\"\\tBest score: {study.best_value:.5f}\")\n# print(f\"\\tBest params:\", study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:51:35.240767Z","iopub.execute_input":"2022-05-27T05:51:35.241068Z","iopub.status.idle":"2022-05-27T05:51:35.244861Z","shell.execute_reply.started":"2022-05-27T05:51:35.241039Z","shell.execute_reply":"2022-05-27T05:51:35.243664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hypertune LGBM\nfrom optuna.integration import LightGBMPruningCallback\nfrom lightgbm import early_stopping\n\ndef objective(trial, data=X_train , target=y_train):\n\n    param = {\n        'n_estimators': 5000,\n        'num_leaves': trial.suggest_int('num_leaves', 20, 3000, step=20),\n#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 200, 10000, step=100),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 15),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10),\n        'subsample': trial.suggest_categorical('subsample', [0.5, 0.6, 0.7, 0.8, 0.9])\n    }    \n    \n    cv = StratifiedKFold( n_splits=5, shuffle=True, random_state=42)\n    \n    for idx, (train_idx, test_idx) in enumerate(cv.split(X_train, y_train)):\n        \n        x_trn, x_val = X_train.iloc[train_idx], X_train.iloc[test_idx]\n        y_trn, y_val = y_train.iloc[train_idx], y_train.iloc[test_idx]\n\n    \n        model = LGBMClassifier(**param)\n        \n        model.fit(x_trn,\n                  y_trn,\n                  eval_set = [(x_val, y_val)],\n                  eval_metric = 'logloss',\n                  callbacks = [early_stopping(stopping_rounds=100,\n                                              verbose = False)],\n                  verbose = False\n                 )\n    \n        preds = model.predict(x_val)\n        scores = accuracy_score(y_val, preds)\n        \n    return np.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:51:35.824568Z","iopub.execute_input":"2022-05-27T05:51:35.825006Z","iopub.status.idle":"2022-05-27T05:51:35.842822Z","shell.execute_reply.started":"2022-05-27T05:51:35.824964Z","shell.execute_reply":"2022-05-27T05:51:35.841924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=30)\n\n# print(f\"\\tBest score: {study.best_value:.5f}\")\n# print(f\"\\tBest params:\", study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:51:36.97409Z","iopub.execute_input":"2022-05-27T05:51:36.974354Z","iopub.status.idle":"2022-05-27T05:51:36.978598Z","shell.execute_reply.started":"2022-05-27T05:51:36.974327Z","shell.execute_reply":"2022-05-27T05:51:36.977515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hypertune CatBoost\ndef objective(trial, data=X_train , target=y_train):\n\n    param = {\n        'n_estimators': 5000,\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'subsample': trial.suggest_categorical('subsample', [0.5, 0.6, 0.7, 0.8, 0.9])\n    }    \n    \n    cv = StratifiedKFold( n_splits=5, shuffle=True, random_state=42)\n    \n    for idx, (train_idx, test_idx) in enumerate(cv.split(X_train, y_train)):\n        \n        x_trn, x_val = X_train.iloc[train_idx], X_train.iloc[test_idx]\n        y_trn, y_val = y_train.iloc[train_idx], y_train.iloc[test_idx]\n\n    \n        model = CatBoostClassifier(**param)\n        \n        model.fit(x_trn,\n                  y_trn,\n                  eval_set = [(x_val, y_val)],\n                  early_stopping_rounds = 100,\n                  verbose = False\n                 )\n    \n        preds = model.predict(x_val)\n        scores = accuracy_score(y_val, preds)\n        \n    return np.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:51:37.36668Z","iopub.execute_input":"2022-05-27T05:51:37.366944Z","iopub.status.idle":"2022-05-27T05:51:37.377988Z","shell.execute_reply.started":"2022-05-27T05:51:37.366915Z","shell.execute_reply":"2022-05-27T05:51:37.377097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=30)\n\n# print(f\"\\tBest score: {study.best_value:.5f}\")\n# print(f\"\\tBest params:\", study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:51:38.242237Z","iopub.execute_input":"2022-05-27T05:51:38.242523Z","iopub.status.idle":"2022-05-27T05:51:38.24748Z","shell.execute_reply.started":"2022-05-27T05:51:38.242491Z","shell.execute_reply":"2022-05-27T05:51:38.246559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optimal parameters\ncat_params = {'learning_rate': 0.08568484131642878, 'max_depth': 8, 'subsample': 0.8}\nlgbm_params = {'num_leaves': 80, 'learning_rate': 0.19028248813460252, 'max_depth': 6, 'min_child_samples': 9, 'reg_alpha': 0.022198765317398134, 'reg_lambda': 0.0054486752392737745, 'subsample': 0.5}\nxgb_params = {'learning_rate': 0.04366702374982929, 'max_depth': 10, 'min_child_weight': 3, 'subsample': 0.8}","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:51:38.969456Z","iopub.execute_input":"2022-05-27T05:51:38.969752Z","iopub.status.idle":"2022-05-27T05:51:38.97574Z","shell.execute_reply.started":"2022-05-27T05:51:38.969719Z","shell.execute_reply":"2022-05-27T05:51:38.97468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Learning\nThis method aims to improve the model by combining multiple algorithms and classifications in order to reduce the biases while also addressing the weaknesses of using standalone models. There are actually several methods of ensemble learning, listed below are some:\n- Majority Voting\n- Bagging\n- Boosting\n- Gradient Boosting\n- Random Forests\n- Stacking\n\nFor now, I will only delve into using the Majority Voting Classifier first as a working example. From the name itself, a voting ensemble involves favoring the class label (i.e. 1=Survived, 0=Died) with the majority or the most votes as the prediction. They are two types of voting:\n- **Hard voting.** summing the votes of class labels from other models and selecting the class label with the most votes as the prediction.\n- **Soft voting.** summing the predicted probabilities of classes from other models and selecting the class label with largest sum probability as the prediction.","metadata":{}},{"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.ensemble import VotingClassifier\n\nxgb_optimal = XGBClassifier(**xgb_params)\nlgbm_optimal = LGBMClassifier(**lgbm_params)\ncat_optimal = CatBoostClassifier(**cat_params)\n\n# Creating ensembles for top 3 tuned models\nEnsemble_HV = VotingClassifier(estimators =\n                               [('XGB', xgb_optimal),\n                                ('Cat', cat_optimal),\n                                ('LGBM', lgbm_optimal)],\n                               voting = 'hard')\n\nEnsemble_SV = VotingClassifier(estimators =\n                               [('XGB', xgb_optimal),\n                                ('Cat', cat_optimal),\n                                ('LGBM', lgbm_optimal)],\n                               voting = 'soft')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:51:40.754727Z","iopub.execute_input":"2022-05-27T05:51:40.755032Z","iopub.status.idle":"2022-05-27T05:51:40.762983Z","shell.execute_reply.started":"2022-05-27T05:51:40.754984Z","shell.execute_reply":"2022-05-27T05:51:40.761728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit Predictions","metadata":{}},{"cell_type":"code","source":"# Defining a function to predict solutions\ndef predict(model):\n    \n    model.fit(X_train, y_train)\n    Y_pred = model.predict(X_test)\n    pred = pd.DataFrame({\n    'PassengerId': test_df_copy['PassengerId'],\n    'Survived': Y_pred\n})\n     \n    return pred","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:51:42.023344Z","iopub.execute_input":"2022-05-27T05:51:42.023602Z","iopub.status.idle":"2022-05-27T05:51:42.028065Z","shell.execute_reply.started":"2022-05-27T05:51:42.023574Z","shell.execute_reply":"2022-05-27T05:51:42.027297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBClassifier()\nlgbm = LGBMClassifier()\ncat = CatBoostClassifier()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:51:42.629886Z","iopub.execute_input":"2022-05-27T05:51:42.630413Z","iopub.status.idle":"2022-05-27T05:51:42.634608Z","shell.execute_reply.started":"2022-05-27T05:51:42.630372Z","shell.execute_reply":"2022-05-27T05:51:42.634044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(xgb).to_csv('submission_xgb.csv', index=False)\npredict(lgbm).to_csv('submission_lgbm.csv', index=False)\npredict(cat).to_csv('submission_cat.csv', index=False)\n\npredict(xgb_optimal).to_csv('submission_xgb_optimal.csv', index=False)\npredict(lgbm_optimal).to_csv('submission_lgbm_optimal.csv', index=False)\npredict(cat_optimal).to_csv('submission_xgb_optimal.csv', index=False)\n\npredict(Ensemble_HV).to_csv('submission_HV.csv', index=False)\npredict(Ensemble_SV).to_csv('submission_SV.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:52:12.349299Z","iopub.execute_input":"2022-05-27T05:52:12.350296Z","iopub.status.idle":"2022-05-27T05:52:22.338035Z","shell.execute_reply.started":"2022-05-27T05:52:12.350248Z","shell.execute_reply":"2022-05-27T05:52:22.337185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Update Logs (Version 2)\n- Introduced new model alogirthms `XGBoostClassifier`, `LGBMClassifier`, `CatBoostClassifier` for comparison in model training\n- Integrated `optuna` in hyperparameter tuning.\n- Employed advanced feature engineering to come up with more robust and meaningful features such as *AgeBin, FareBin, and FamilyBin*.\n- Enhanced the readability and aesthetics of visualizations\n\nAny form of feedback and advise are welcome. If you have any questions and clarifications regarding my code and work, feel free to ask them in the comments section and I will be happy to engage and answer them.\n\n### My Other Works\nIf you are interested, you can go to my Kaggle profile [HERE](https://www.kaggle.com/shilongzhuang) and browse through my other works and contributions.\n\n---\n# References\nSpecial thanks and credits to these informative resources (notebooks) and guides created by talented professionals in the field.\n- [A Data Science Framework: To Achieve 99% Accuracy | Kaggle](https://www.kaggle.com/code/shilongzhuang/a-data-science-framework-to-achieve-99-accuracy/edit)\n- [Titanic Data Science Solutions | Kaggle](https://www.kaggle.com/code/startupsci/titanic-data-science-solutions)","metadata":{}}]}