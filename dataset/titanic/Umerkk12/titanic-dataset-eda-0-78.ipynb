{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/titanic/train.csv')\ntest = pd.read_csv('../input/titanic/test.csv')\ntrain = train.append(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\nmsno.bar(train, color='darkblue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Three columns need engineering: Cabin, Embarked and Age\n* We will try to fix them in the best possible way we can "},{"metadata":{},"cell_type":"markdown","source":"# Cabin Column "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Cabin'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Cabin'].fillna(0).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. It seems that there are many missing values (687) \n2. We will have to drop this column as there seems to be no rrelevance of this column to predict survivors.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('Cabin', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Embarked Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['Embarked'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Embarked'].fillna('Unknown', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Filling Embarked column with unknown as there are only two rows. \n* We will not drop the column as there are sufficient rows to keep them in our analysis. "},{"metadata":{},"cell_type":"markdown","source":"# Age Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['Age'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(['Pclass', 'Sex']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def impude_age(cols):\n    Age=cols[0]\n    Pclass = cols[1]\n    Sex = cols[2]\n    if pd.isnull(Age):\n        if Pclass == 1 and Sex =='male':\n            return 41\n        elif Pclass == 1 and Sex =='female':\n            return 34\n        elif Pclass == 2 and Sex =='male':\n            return 31\n        elif Pclass == 2 and Sex =='female':\n            return 29\n        elif Pclass == 3 and Sex =='male':\n            return 27\n        else:\n            return 22\n    else:\n        return Age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Age'] = train[['Age', 'Pclass', 'Sex']].apply(impude_age, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Age had some of the missing values.\n* These were filled in using a special formula\n* This formula took into account the PClass and Sex Column to define the missing age\n* For example a person with PClass of A and Sex 'Male' will have the average of 41\n* 41 came from the groupby that we performed earlier right before the formula above"},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.bar(train, color='green')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Checking to see if all the columns are now equal\n* Seems like everything is fine here"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['Name', 'Ticket'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Dropping these columns as they do not carry any significance."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain['Sex'] = le.fit_transform(train['Sex'])\ntrain['Embarked'] = le.fit_transform(train['Embarked'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Changing the datatypes to numeric for Embarked and Sex"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = train[891:].drop('Survived',axis=1)\ntrain = train[:891]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now that the feature engineering is done\n* We will split the two sets back again.\n* Train and Test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analyzing"},{"metadata":{"trusted":true},"cell_type":"code","source":"a= train.corr()\nplt.figure(figsize=(14,8))\nsns.heatmap(a.corr(), annot=True, cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyzing Train Set for Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2,ncols=2, figsize=(10,5))\na = sns.scatterplot(x='PassengerId', y='Fare', data=train,ax=ax[0][0], color='darkred', s=100)\nb = sns.scatterplot(x='PassengerId', y='Age', data=train,ax=ax[0][1], color='darkgreen', s=100)\nc = sns.scatterplot(x='PassengerId', y='Embarked', data=train,ax=ax[1][0], color='darkred', s=100)\nd = sns.scatterplot(x='PassengerId', y='Parch', data=train,ax=ax[1][1], color='darkred', s=100)\n\na.set_title('Fare Outliers', fontsize=20)\nb.set_title('Age Outliers', fontsize=20)\nc.set_title('Embarked Outliers', fontsize=20)\nd.set_title('Parch Outliers', fontsize=20)\n\n\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The analysis for outliers show that Fare, Embarked and Parch column have some outliers. \n* We will try to remove these outlier rows in each of the columns to make sure our analysis is accurate\n* Outliers can really influence the final model.\n* they tend to drag averages up or down and can really change the predictive capabilities of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"q_hi_f = train['Fare'].quantile(0.95)\nq_low_f = train['Fare'].quantile(0)\ntrain_1 = train[(train['Fare'] >= q_low_f) & (train['Fare']<q_hi_f)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_hi_e = train_1['Embarked'].quantile(0.9999)\nq_low_e = train_1['Embarked'].quantile(0)\ntrain_2 = train_1[(train_1['Embarked'] >= q_low_e) & (train_1['Embarked']<q_hi_e)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_hi_p = train_2['Parch'].quantile(0.97)\nq_low_p = train_2['Parch'].quantile(0)\ntrain_3 = train_2[(train_2['Parch'] >= q_low_p) & (train_2['Parch']<=q_hi_p)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2,ncols=2, figsize=(10,5))\na = sns.scatterplot(x='PassengerId', y='Fare', data=train_3,ax=ax[0][0], color='darkgreen', s=100)\nb = sns.scatterplot(x='PassengerId', y='Age', data=train_3,ax=ax[0][1], color='darkgreen', s=100)\nc = sns.scatterplot(x='PassengerId', y='Embarked', data=train_3,ax=ax[1][0], color='darkgreen', s=100)\nd = sns.scatterplot(x='PassengerId', y='Parch', data=train_3,ax=ax[1][1], color='darkgreen', s=100)\n\na.set_title('Fare Outliers', fontsize=20)\nb.set_title('Age Outliers', fontsize=20)\nc.set_title('Embarked Outliers', fontsize=20)\nd.set_title('Parch Outliers', fontsize=20)\n\n\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we can see in the above visuals now that the outliers have been very thoroughly removed \n* each column is dealt seperately to cut out the outliers which might create noise in the model\n* All this data is now around cleaned from extreme values!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_3 = train_3.drop('PassengerId',axis=1)\ntest_1 = test.drop(['PassengerId'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We will delete the passengerId column now since we do not need it anymore. \n* We required it before to do the outlier analysis"},{"metadata":{},"cell_type":"markdown","source":"# Oversampling Analysis for Train Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_3['Survived'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are imbalances in the fields\n* The problem of oversampling will occur in the model\n* This is because we have more labels of one type.\n* To fix this we will need to balance the dataset. \n* This can be done by oversampling analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_3.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nos = SMOTE()\nX_train, y_train  = os.fit_resample(train_3[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Embarked']], train_3['Survived'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The issue of oversampling is now resolved.\n* Both the labels have now equal count of values that will be inserted in the model"},{"metadata":{},"cell_type":"markdown","source":"# Scaling the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nmn = MinMaxScaler()\nX_train_scaled = mn.fit_transform(X_train)\nX_test_scaled = mn.transform(test_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The data is now scaled \n* We are now ready to apply this data to a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_scaled = pd.DataFrame(X_train_scaled, columns=[X_train.columns])\nX_train_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_scaled = pd.DataFrame(X_test_scaled, columns=test_1.columns)\nX_test_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_test_X = X_train_scaled[:300]\nval_test_y = y_train[:300]\nX_train_scaled_2 = X_train_scaled[300:]\ny_train_scaled_2 = y_train[300:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Predictive Model..Working on it right now"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(units= 7, activation='relu'))\n#model.add(Dropout(0.5))\n#model.add(Dense(units= 14, activation='relu'))\n#model.add(Dropout(0.5))\nmodel.add(Dense(units= 3, activation='relu'))\nmodel.add(Dense(units=1,kernel_initializer='uniform', activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n#early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x=X_train_scaled_2, y=y_train_scaled_2, epochs=1000, batch_size=200,\n          validation_data=(val_test_X, val_test_y)) #callbacks=early_stopping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(model.history.history).plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = model.predict_classes(X_test_scaled)\np = pd.DataFrame(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pd.concat([test, p], axis=1)\n#predictions = predictions.drop('Survived', axis=1)\npredictions = predictions.rename(columns={0:'Survived'})\npredictions = predictions[['PassengerId', 'Survived']]\npredictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictive Analysis Machine Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see that random forest classifier, decision tree and XGBoost have performed nearly the same.\n* We will chose one of these as they have showed good results."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_scaled['Fare'] = X_test_scaled['Fare'].fillna(X_test_scaled['Fare'].mean())\nX_test_scaled.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X_train_scaled[['Pclass', 'Sex', 'Fare', 'Age', 'Parch', 'Embarked', 'SibSp']]\ny = pd.DataFrame(y_train)\nrfc = RandomForestClassifier(n_estimators=500, max_depth=7)\nrfc.fit(X, y)\npredict = rfc.predict(X_test_scaled[['Pclass', 'Sex', 'Fare', 'Age', 'Parch', 'Embarked', 'SibSp']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = pd.DataFrame(test['PassengerId'])\nb = pd.DataFrame(predict, columns=['Survived'])\nprediction = pd.concat([a, b], axis=1)\nprediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction.to_csv('Predictions.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}