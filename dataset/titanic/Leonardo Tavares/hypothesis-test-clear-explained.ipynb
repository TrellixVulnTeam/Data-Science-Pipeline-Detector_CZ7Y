{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hypothesis Test - Clear Explained\n\n![](https://editor.analyticsvidhya.com/uploads/52940cover.jpg)\nsource: https://editor.analyticsvidhya.com/uploads/52940cover.jpg\n\n## Need statement\n\nConsider the following scenario: You've trained two (or three, or ten, or thousands...) **super cool predictive models** for a specific problem (like this famous titanic problem, for example). How do you actually know which model has the best of the best of the best performance? Random Forest model is always better than Decision tree? XGBoost is the best algorithm of the world? What is your opinion about KNN? and... what about the bubble sorting method? ok ok... I'm just kidding...\n\nI'll try to help you with a cool technique that **statistically** demonstrates whether a model performs better, worse or similar to another.\n\n\nPart of the work presented here is based on the following video #bam!:\n\n[![Live 2020-06-01!!! Hypothesis Testing](http://img.youtube.com/vi/hGoTUyBnbxg/0.jpg)](https://www.youtube.com/watch?v=hGoTUyBnbxg)\n\n\n\nOf course, you will find much more technical information about design of experiments (DoE) in the book below:\n\n![](https://images-na.ssl-images-amazon.com/images/I/51XN4Kgi0JL._SX396_BO1,204,203,200_.jpg)\n[Link](https://www.amazon.com.br/Design-Analysis-Experiments-Douglas-Montgomery/dp/1119722101/)\n\n\n![](https://thumbs.dreamstime.com/b/lets-go-handwritten-white-background-169989567.jpg)\n\nsource: https://thumbs.dreamstime.com/b/lets-go-handwritten-white-background-169989567.jpg\n\n\n## Please upvote me if you like, ok? (this is really  really important to me)","metadata":{}},{"cell_type":"markdown","source":"# 1. (Without) exploratory data analysis / just data engineering\n\n\nThe objective here is not to compete but to present the most basic concepts of hypothesis testing, so accuracy will not be our main concern, ok?\n\n\nWe will only use the following database features:\n* Pclass (categorical)\n* Sex (categorical, with just 2 unique values...)\n* Age (numerical)\n* Fare (numerical)\n* Embarked (categorical)\n* Survived (categorical ... of course!!!)","metadata":{}},{"cell_type":"code","source":"# general imports...\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy.stats as st\nfrom statsmodels.stats.weightstats import ztest\n\n# sklearn\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-26T14:44:52.992422Z","iopub.execute_input":"2022-05-26T14:44:52.99274Z","iopub.status.idle":"2022-05-26T14:44:53.00169Z","shell.execute_reply.started":"2022-05-26T14:44:52.992709Z","shell.execute_reply":"2022-05-26T14:44:53.000432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SOME CONSTANTS\n\nSEED = 123","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:44:53.020572Z","iopub.execute_input":"2022-05-26T14:44:53.020884Z","iopub.status.idle":"2022-05-26T14:44:53.024663Z","shell.execute_reply.started":"2022-05-26T14:44:53.020838Z","shell.execute_reply":"2022-05-26T14:44:53.023941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the database...\ndf_train = pd.read_csv('/kaggle/input/titanic/train.csv')\n\n# I will use this list further up\ncols = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']\n\n# selecting the features\ndf_train = df_train[cols + ['Survived']]\ndf_train","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-26T14:44:53.047894Z","iopub.execute_input":"2022-05-26T14:44:53.049021Z","iopub.status.idle":"2022-05-26T14:44:53.078893Z","shell.execute_reply.started":"2022-05-26T14:44:53.048973Z","shell.execute_reply":"2022-05-26T14:44:53.078148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base info\ndf_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:44:53.082107Z","iopub.execute_input":"2022-05-26T14:44:53.082632Z","iopub.status.idle":"2022-05-26T14:44:53.096428Z","shell.execute_reply.started":"2022-05-26T14:44:53.082594Z","shell.execute_reply":"2022-05-26T14:44:53.09501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in df_train.columns:\n    print(f'Column: {c}')\n    print(f'# of unique values: {len(df_train[c].unique())}')","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:44:53.119229Z","iopub.execute_input":"2022-05-26T14:44:53.119545Z","iopub.status.idle":"2022-05-26T14:44:53.128717Z","shell.execute_reply.started":"2022-05-26T14:44:53.119509Z","shell.execute_reply":"2022-05-26T14:44:53.127632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's separate the predictor variables ($X$) and the outcome ($y$)...","metadata":{}},{"cell_type":"code","source":"X = df_train[cols]\ny = df_train['Survived']","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:44:53.148316Z","iopub.execute_input":"2022-05-26T14:44:53.149298Z","iopub.status.idle":"2022-05-26T14:44:53.154918Z","shell.execute_reply.started":"2022-05-26T14:44:53.149256Z","shell.execute_reply":"2022-05-26T14:44:53.153969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create a (very very very simple) pipeline for the predictor variables, which performs the following tasks:\n\n* Numeric features:\n    * Imputer: KNNImputer\n    * Scaler: StandardScaler\n* Categorial features:\n    * Imputer: Most frequent\n    * Encoder: One Hot Encoder\n* Binary features:\n    * Imputer: Most frequent\n    * Encoder: Ordinal Encoder\n\n\nThis pipeline will be used later, at the time of the experiments, ok?","metadata":{}},{"cell_type":"code","source":"# numerical features\nnumeric_features = [\"Age\", \"Fare\"]\nnumeric_transformer = Pipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), \n           (\"scaler\", StandardScaler())]\n)\n\nPipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), \n           (\"scaler\", StandardScaler())])\n\n# categorial features\ncategorical_features = [\"Pclass\", \"Embarked\"]\ncategorical_transformer = Pipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n           (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))])\n    \n# binary features\nbinary_features = [\"Sex\"]\nbinary_transformer = Pipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n           (\"ohe\", OrdinalEncoder())])\n\n    \n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_features),\n        (\"cat\", categorical_transformer, categorical_features),\n        (\"bin\", binary_transformer, binary_features),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:44:53.167285Z","iopub.execute_input":"2022-05-26T14:44:53.167603Z","iopub.status.idle":"2022-05-26T14:44:53.176408Z","shell.execute_reply.started":"2022-05-26T14:44:53.167568Z","shell.execute_reply":"2022-05-26T14:44:53.175664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.1 The very first experiment\n\nIn this very first experiment we are going to compare a **logistic regression** with a **random forest** (both models with their default settings)\n\n\nLet's run each of the methods just once and check which one has the best accuracy, ok?\n","metadata":{}},{"cell_type":"code","source":"# Separating the sample into 70% for training and the remaining test, stratified by outcome.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED, stratify=y)\n\nprint(f'X_train shape {X_train.shape}')\nprint(f'y_train shape {y_train.shape}')\nprint('-'*10)\nprint(f'X_test shape {X_test.shape}')\nprint(f'y_test shape {y_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:44:53.191339Z","iopub.execute_input":"2022-05-26T14:44:53.192248Z","iopub.status.idle":"2022-05-26T14:44:53.20258Z","shell.execute_reply.started":"2022-05-26T14:44:53.19221Z","shell.execute_reply":"2022-05-26T14:44:53.20183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, logistic regression model....","metadata":{}},{"cell_type":"code","source":"pipe_dt = Pipeline(\n    [('preprocessor', preprocessor), \n     ('estimator', LogisticRegression(random_state=SEED))])\npipe_dt.fit(X_train, y_train)\ny_pred = pipe_dt.predict(X_test)\nacc_lr = accuracy_score(y_test, y_pred)\nprint(f'Acc (Logistic Regression): {acc_lr}')","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:44:53.215309Z","iopub.execute_input":"2022-05-26T14:44:53.215757Z","iopub.status.idle":"2022-05-26T14:44:53.259677Z","shell.execute_reply.started":"2022-05-26T14:44:53.215712Z","shell.execute_reply":"2022-05-26T14:44:53.258774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Now, random forest...","metadata":{}},{"cell_type":"code","source":"pipe_rn = Pipeline(\n    [('preprocessor', preprocessor), \n     ('estimator', RandomForestClassifier(random_state=SEED))])\npipe_rn.fit(X_train, y_train)\ny_pred = pipe_rn.predict(X_test)\nacc_rn = accuracy_score(y_test, y_pred)\nprint(f'Acc (Random Forest): {acc_rn}')","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:44:53.261277Z","iopub.execute_input":"2022-05-26T14:44:53.261655Z","iopub.status.idle":"2022-05-26T14:44:53.534014Z","shell.execute_reply.started":"2022-05-26T14:44:53.261623Z","shell.execute_reply":"2022-05-26T14:44:53.532952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check which one is the best model...","metadata":{}},{"cell_type":"code","source":"# Here we have an exceptionally complex quantum calculus...\nif acc_lr > acc_rn:\n    print('Logistic Regression is the best model!!!')\nelif acc_rn > acc_lr:\n    print('Random forest is the best model!!!')\nelse:\n    print(\"Something went wrong... (and it obviously isn't right...)\")\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-26T14:44:53.535198Z","iopub.execute_input":"2022-05-26T14:44:53.535425Z","iopub.status.idle":"2022-05-26T14:44:53.541747Z","shell.execute_reply.started":"2022-05-26T14:44:53.535394Z","shell.execute_reply":"2022-05-26T14:44:53.540983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now you can tell the whole world that the **Random Forest model is the best**, right? We can stop the discussion here....\n\n\nHmm...not yet...\n\nLet's try some more...","metadata":{}},{"cell_type":"markdown","source":"# 2.2 Second experiment (much better than the first)","metadata":{}},{"cell_type":"markdown","source":"In this second experiment, instead of running each model just once, we are going to run them 25 times (with different training and tests datasets). After that, we'll compare the average accuracies of each model, ok?","metadata":{}},{"cell_type":"code","source":"# I will store the results of the executions here\nresults = {\n    'lr':[],\n    'rn':[]\n}\n\nlocal_seed = 10\nfor i in tqdm(range(25)):\n    # Separating the sample into 70% for training and the remaining test, stratified by outcome.\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=local_seed)\n\n    pipe_dt = Pipeline(\n        [('preprocessor', preprocessor), \n         ('estimator', LogisticRegression(random_state=SEED))])\n    pipe_dt.fit(X_train, y_train)\n    y_pred = pipe_dt.predict(X_test)\n    acc_lr = accuracy_score(y_test, y_pred)\n    results['lr'].append(acc_lr)\n    \n    pipe_rn = Pipeline(\n        [('preprocessor', preprocessor), \n         ('estimator', RandomForestClassifier(random_state=SEED))])\n    pipe_rn.fit(X_train, y_train)\n    y_pred = pipe_rn.predict(X_test)\n    acc_rn = accuracy_score(y_test, y_pred)    \n    results['rn'].append(acc_rn)\n    \n    local_seed += 10\n    \n\n# Converting result to dataframe\ndf_results = pd.DataFrame(results)\ndf_results","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:44:53.543754Z","iopub.execute_input":"2022-05-26T14:44:53.54461Z","iopub.status.idle":"2022-05-26T14:45:01.020085Z","shell.execute_reply.started":"2022-05-26T14:44:53.544568Z","shell.execute_reply":"2022-05-26T14:45:01.019201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summarizing the result...\ndf_results.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:45:01.021248Z","iopub.execute_input":"2022-05-26T14:45:01.021479Z","iopub.status.idle":"2022-05-26T14:45:01.039665Z","shell.execute_reply.started":"2022-05-26T14:45:01.021451Z","shell.execute_reply":"2022-05-26T14:45:01.038651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now, who did better?","metadata":{}},{"cell_type":"code","source":"# Here we have an exceptionally complex quantum calculus...\nif df_results['lr'].mean() > df_results['rn'].mean():\n    print('Logistic Regression is the best model!!!')\nelif df_results['rn'].mean() > df_results['lr'].mean():\n    print('Random forest is the best model!!!')\nelse:\n    print(\"Something went wrong... (and it obviously isn't right...)\")\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-26T14:45:01.041Z","iopub.execute_input":"2022-05-26T14:45:01.041241Z","iopub.status.idle":"2022-05-26T14:45:01.053484Z","shell.execute_reply.started":"2022-05-26T14:45:01.041212Z","shell.execute_reply":"2022-05-26T14:45:01.052774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By now you must be really convinced that Random Forest is the **best model ever**, right?\n\n\nHmm... I don't think so...\n\n\nIn our results, was the logistic better in any round?\n\n","metadata":{}},{"cell_type":"code","source":"df_results['lr win'] = df_results['lr'] > df_results['rn']\ndf_results","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-26T14:45:01.055018Z","iopub.execute_input":"2022-05-26T14:45:01.055413Z","iopub.status.idle":"2022-05-26T14:45:01.079978Z","shell.execute_reply.started":"2022-05-26T14:45:01.055379Z","shell.execute_reply":"2022-05-26T14:45:01.078923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_win = df_results[df_results['lr win'] == True].shape[0]\nrn_win = 25 - lr_win\n\nprint(f'Logistic Regression was the best for {lr_win} times')\nprint(f'Random Forest was the best for {rn_win} times')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-26T14:45:01.08128Z","iopub.execute_input":"2022-05-26T14:45:01.08152Z","iopub.status.idle":"2022-05-26T14:45:01.08864Z","shell.execute_reply.started":"2022-05-26T14:45:01.081493Z","shell.execute_reply":"2022-05-26T14:45:01.087806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, there is no doubt that Random Forest is **the best model ever created**, right?\n\nLet's try it one more time (*I promise it will be the last experiment on this notebook*)...","metadata":{}},{"cell_type":"markdown","source":"# 2.3 Third experiment (and the most robust of all)\n\nAccording to wikipedia: \"*A statistical hypothesis test is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis.*\"\n\nAnd what does that mean in practice? It means that we will use a **robust** method to support some claim (in our case hypotheses), based on **experiments** and their **results**.\n\nLet's build our hypotheses. I have an initial thought that the random forest model will be better than the logistic regression, but I want to statistically test if that thought is correct. To test statistically, we have to write our hypotheses. \n\nThe first hypothesis is called the **null hypothesis** (or null effect hypothesis or H0). Here we place our first bet, that is, that there is no statistically significant difference between the models (hence the name null hypothesis).\n\nThe other hypothesis is called the **alternative hypothesis** (H1), and it is used when we reject the null hypothesis. But pay close attention now: We never accept the null hypothesis... or reject H0 or fail to reject H0, okay? (save this information)\n\nNote that earlier I said that the method is robust, but this does not mean that the method is infallible (there is a probability that the result of the experiment is not exactly adhering to reality). In this sense, there are 4 possibilities:\n\n* H0 is true AND we do not reject H0: this is a Right decision\n* H0 is true AND we reject H0: this is a Wrong decision (Type I Error)\n* H0 is false AND we do not reject H0: this is a Wrong decision (Type II Error)\n* H0 is false AND we reject H0: this is a Right decision\n\n\nAccording to wikipedia: \"*In statistical hypothesis testing, a type I error is the mistaken rejection of an actually true null hypothesis (also known as a \"false positive\" finding or conclusion; example: \"an innocent person is convicted\"), while a type II error is the failure to reject a null hypothesis that is actually false (also known as a \"false negative\" finding or conclusion; example: \"a guilty person is not convicted\")*\"\n\n\nThe probability of a type I error is called the **significance level** of the test (we use the Greek letter alpha $\\alpha$ to express this value). The complement of the level of statistical significance (1-$\\alpha$) is called the **confidence level**. We choose the $\\alpha$ value before the experiment starts, and we use the experiment result (p_value) to reach a conclusion. For more information see Montgomery's book ok?\n\n\nI think we already have enough to start our statistical test and actually compare the 2 models and see which one is better. Let's go!!!\n\n\nSo let's write our hypotheses and our parameters for the statistical test:\n\n**hypotheses:**\n* H0: There is no statistical difference between the LR and RN methods in terms of mean accuracies\n* H1: There is statistical difference between the LR and RN methods in terms of mean accuracies\n\n**design of experiment:**\n* $\\alpha = 0.05$\n* Each test will be run 25 times, at the end it will be verified (using the z-test) whether or not there is a statistical difference between the methods\n\n**premises**\n* I'm using the z-test because it's easier to understand, even though here some assumptions are not verified. The purpose here is to be didactic. Certainly other statistical tests (such as the T test, for example or some other non-parametric method) are more interesting, but they require a greater depth in the theory of experiment design.\n\n\nLet's implement the test then...","metadata":{}},{"cell_type":"code","source":"# I will store the results of the executions here\nresults = {\n    'lr':[],\n    'rn':[]\n}\n\nlocal_seed = 10\nfor i in tqdm(range(25)):\n    # Separating the sample into 70% for training and the remaining test, stratified by outcome.\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=local_seed)\n\n    pipe_dt = Pipeline(\n        [('preprocessor', preprocessor), \n         ('estimator', LogisticRegression(random_state=SEED))])\n    pipe_dt.fit(X_train, y_train)\n    y_pred = pipe_dt.predict(X_test)\n    acc_lr = accuracy_score(y_test, y_pred)\n    results['lr'].append(acc_lr)\n    \n    pipe_rn = Pipeline(\n        [('preprocessor', preprocessor), \n         ('estimator', RandomForestClassifier(random_state=SEED))])\n    pipe_rn.fit(X_train, y_train)\n    y_pred = pipe_rn.predict(X_test)\n    acc_rn = accuracy_score(y_test, y_pred)    \n    results['rn'].append(acc_rn)\n    \n    local_seed += 10\n    \n\n# Converting result to dataframe\ndf_results = pd.DataFrame(results)\ndf_results","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:45:01.090879Z","iopub.execute_input":"2022-05-26T14:45:01.091123Z","iopub.status.idle":"2022-05-26T14:45:08.557031Z","shell.execute_reply.started":"2022-05-26T14:45:01.091094Z","shell.execute_reply":"2022-05-26T14:45:08.556428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_results.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:45:08.557878Z","iopub.execute_input":"2022-05-26T14:45:08.558128Z","iopub.status.idle":"2022-05-26T14:45:08.579555Z","shell.execute_reply.started":"2022-05-26T14:45:08.558091Z","shell.execute_reply":"2022-05-26T14:45:08.578626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see graphically the result of the","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(12, 8))\nax = sns.distplot(df_results['lr'])\nax = sns.distplot(df_results['rn'])\nfig.legend(labels=['Logistic Regression','Random Forest'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:45:08.581067Z","iopub.execute_input":"2022-05-26T14:45:08.581312Z","iopub.status.idle":"2022-05-26T14:45:08.830705Z","shell.execute_reply.started":"2022-05-26T14:45:08.581283Z","shell.execute_reply":"2022-05-26T14:45:08.83Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well well well, you were absolutely sure that Random Forest was better than Logistic Regression... now what? What do you see in the picture above?\n\nIt looks like the histograms have a very large overlapping area, right? Is it possible to keep the same statement as before? or now it looks like the game has changed?\n\nAnd now... **the statistical test**...","metadata":{}},{"cell_type":"code","source":"alpha = 0.05\nz_calc, p_valor = ztest(x1=df_results['lr'], x2=df_results['rn'], alternative='two-sided')\nz_calc, p_valor","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:45:08.831659Z","iopub.execute_input":"2022-05-26T14:45:08.831921Z","iopub.status.idle":"2022-05-26T14:45:08.8399Z","shell.execute_reply.started":"2022-05-26T14:45:08.831885Z","shell.execute_reply":"2022-05-26T14:45:08.839053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's time to check...","metadata":{}},{"cell_type":"code","source":"if p_valor < alpha:\n    print(\"Reject H0! - There is statistical difference between the LR and RN methods in terms of mean accuracies\")\nelse:\n    print(\"Do not reject H0! - Maybe there is no statistical difference between the LR and RN methods in terms of mean accuracies\")","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:45:08.841163Z","iopub.execute_input":"2022-05-26T14:45:08.841418Z","iopub.status.idle":"2022-05-26T14:45:08.853798Z","shell.execute_reply.started":"2022-05-26T14:45:08.841388Z","shell.execute_reply":"2022-05-26T14:45:08.852878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## #Bam!!!!\n\nNow I convinced you that there is no difference between the methods right??? Read the conclusions with me...","metadata":{}},{"cell_type":"markdown","source":"# The Storytelling","metadata":{}},{"cell_type":"markdown","source":"Okay okay... and now? How to report this result?\n\nYou won't tell everything you've done, because it will be very boring! You can simply say the following:\n\n\"*According to the experiments carried out, it is not possible to state that there are statistically significant differences between the random forest and logistic regression methods.*\"\n\nAnd what could change this result? Many things!!! I will list some of them:\n\n* sample size\n* LR and RN hyperparameters\n* number of runs\n* $\\alpha$ value\n* the statistical test used\n* verification of assumptions according to the statistical test used\n* the seed value of the random number (see constant SEED)\n* if it's raining... (I'm just kidding...)\n\n\nThe message here is that the conclusions depend on several factors involved in the various stages of the experiment. So, here's some friendly advice:\n\n* Always check that the methodology applied is adequate and correct\n* Check that the parameters used for the experiment are correct\n* If a conclusion does not seem to be very reasonable, try running an independent experiment (following and verifying the methodology) and compare the results\n\n\nThank you very much for your attention, and feel free to make suggestions!!! Bye!!!\n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQeTlBWPzFucVr0vMMgbWtuF1iX_Ja16zMiuUzpx41OHktuj_PeeGQht8qiof2LWZZfv4g&usqp=CAU)","metadata":{}}]}