{"cells":[{"metadata":{},"cell_type":"markdown","source":"# I. Introduction \n\n> Hi There.. Im  kind of new in Kaggle and Python and following other people recommendation I got the titanic Data set \n> It was the First Data set that I analyzed. please feel free to leave any feedback .. and thanks in advance"},{"metadata":{},"cell_type":"markdown","source":"# II. OBTAINING Data"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip install venndata","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#basic library \nimport pandas as pd\nimport numpy as np\nimport math \nimport re \n\n#visualization \nimport matplotlib.pyplot as plt\nimport matplotlib\nimport matplotlib_venn as vplt\nfrom venndata import venn   \nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns \nimport cufflinks as cf \n%matplotlib inline\nsns.set_style(style = 'darkgrid')\ncf.go_offline()\nsns.set_style('whitegrid')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"#importing Data \ntrain= pd.read_csv('../input/titanic/train.csv')\ntest= pd.read_csv('../input/titanic/test.csv')\nID = test['PassengerId']\nVdf =pd.read_csv('../input/titanic/train.csv') #for visualization Purpose\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2 , figsize=(15,5))\n\nsns.heatmap(train.isnull(), yticklabels = False , cmap = 'plasma', ax = ax1)\nsns.heatmap(test.isnull(), yticklabels = False , cmap = 'plasma', ax = ax2)\nprint(\"massive missing value in Cabin & test data has Missing Data in Fare\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# III. Exploring Data , visualization"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"Vdf\n#defining values\n\nVdf['Pclass1'] = Vdf.Pclass.apply(lambda x: 1 if x==1 else 0)\nVdf['Pclass2'] = Vdf.Pclass.apply(lambda x: 1 if x==2 else 0)\nVdf['Pclass3'] = Vdf.Pclass.apply(lambda x: 1 if x==3 else 0)\nVdf['Male'] = Vdf.Sex.apply(lambda x: 1 if x=='male' else 0)\nVdf['Female'] = Vdf.Sex.apply(lambda x: 1 if x=='female' else 0)\n#Vdf['Kids'] = Vdf.Age.apply(lambda x: 1 if x<10 else 0)\n#Vdf['Adoles'] = Vdf.Age.apply(lambda x: 1 if 11>x<10 else 0)\n#Vdf['Adults'] = Vdf.Age.apply(lambda x: 1 if 19>x<50 else 0)\n#Vdf['Elder'] = Vdf.Age.apply(lambda x: 1 if x>50 else 0)\n##-----------------------------------------------------------------\n   \n\n\ndf2 = Vdf[['Survived', 'Pclass1', 'Pclass2', 'Pclass3', 'Male', 'Female']] #,'Elder','Kids','Elder','Adoles']]\nmatplotlib.rcParams['figure.figsize'] = [10, 10]\nfineTune=False\nlabels, radii, actualOverlaps, disjointOverlaps = venn.df2areas(df2, fineTune=fineTune)\nfig, ax = venn.venn(radii, actualOverlaps, disjointOverlaps, \n                    labels=labels, labelsize='auto', \n                    cmap='viridis', fineTune=fineTune)\n\nprint('General view of survivor')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.display import display\nfrom PIL import Image\npath=('../input/titanic-picture-ilustrive/titanic.jpg')\ndisplay(Image.open(path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# From this picture I can asume \n1. People in third class has less chances to survive\n2. I get many insight and guidelines to make sure I am doing ok \n3. Any feedback it is welcome "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Survived + Survivor segmented by sex , segmented by Pclass\nfig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,5))\nsns.countplot(x='Survived' , data=train, ax=ax1 ,palette=\"Dark2\")\nax1.set_title(\"Survived , 0 = Dead , 1 = Alive\")\nsns.countplot(x='Survived' , hue='Sex',data=train, ax=ax2, palette=\"Set1\")\nax2.set_title(\"Survived Segmented by Sex\")\nsns.countplot(x='Survived' , hue='Pclass',data=train, ax=ax3,palette=\"Paired\")\nax3.set_title(\"Survived Segmented by Sex/Pclass\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *I know there are missing values , but in order to better understand the data I create some segmentation and them I will compare them with the new graph without NAN*"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(25,10))\nsns.countplot(x='Survived' , hue = 'Sex' , data=train[train.Age < 15], ax=ax1,palette=\"Dark2\").set_title('Age between 0-15')\nsns.countplot(x='Survived' , hue = 'Sex' , data=train[(train['Age'] > 16) & (train['Age'] < 25)], ax=ax2,palette=\"Paired\").set_title('Age between 16- 25')\nsns.countplot(x='Survived' , hue = 'Sex' , data=train[(train['Age'] > 26) & (train['Age'] < 35)], ax=ax3).set_title('Age between 26- 35')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(25,10))\nsns.countplot(x='Survived' , hue = 'Sex' , data=train[(train['Age'] > 36) & (train['Age'] < 45)], ax=ax1).set_title('Age between 36-45')\nsns.countplot(x='Survived' , hue = 'Sex' , data=train[(train['Age'] > 46) & (train['Age'] < 60)], ax=ax2,palette=\"Dark2\").set_title('Age between 46- 60')\nsns.countplot(x='Survived' , hue = 'Sex' , data=train[(train['Age'] > 61) & (train['Age'] < 80)], ax=ax3).set_title('Age between 61- 80')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding Missing Values in Embarked\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Adressing missing value in Embarked in embarked \nprint(\" Missing Values in Embarked:\",train.isnull().sum()[5])\ntrain[train['Embarked'].isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Now we have to Replace them both value \"Southampton\"\ntrain['Embarked'].fillna(\"S\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Would be easier fill the issing data with S because is the place where most of people were coming from , but since they are only two I looked for the info : Miss. Amelie boarded the Titanic at Southampton as maid to Mrs George Nelson Stone. She travelled on Mrs Stone's ticket (#113572). Mrs Stone boarded the Titanic in Southampton on 10 April 1912 and was travelling in first class with her maid Amelie Icard. She occupied cabin B-28. I found that info in www.encyclopedia-titanica.org**"},{"metadata":{},"cell_type":"markdown","source":"# IV. SCRUB , Missing values - Age - Cabin , etc"},{"metadata":{},"cell_type":"markdown","source":"**We know some people was tarveling with their family, in titanic.org the information has been collected by famiy so i will do my best to filter those family that have 7 or 8 member and find their ages\n* 0    608 # traveling alone \n* 1    209 \n* 2     28\n* 4     18\n* 3     16 # 16 Family with 3 members\n* 8      7 # 7 Family with 8 members\n* 5      5**"},{"metadata":{},"cell_type":"markdown","source":"## Age"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train[(train.Age.isna())&(train.SibSp== 8)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sage_f = train[(train.Age.isna())&(train.SibSp== 8)]['Name'].to_list()\ndbirth_ = [1907,1904,1895,1892,1891,1893,1897] #their ages in www.encyclopedia-titanica.org\ndbirth= 1912 - np.array(dbirth_)  #1912 Titanic accident \ndbirth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Diccionaty \nkeys = sage_f\nvalues = dbirth\nnew_dict = dict(zip(keys, values))\nprint(new_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k, v in new_dict.items():\n    train.loc[train.Name == k, 'Age'] = v\n    \nfor k, v in new_dict.items():\n    test.loc[train.Name == k, 'Age'] = v","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Replacing Ages\nTo fill the missing values in Age , I can easily find the average age and then apply it to all of the missing value , but instead I will filter by the Mr, Mrs , Miss, Master by doing this will be more accurate**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.boxplot(x='Pclass', y='Age', data=train).set_title('Age Average per class')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Combining both Data set \ndata_titanic = [train,test]\n\nfor data in data_titanic: #Extracting Title \n    data['Title'] = data ['Name'].str.extract(' ([A-Za-z]+)\\.', expand = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.Title.value_counts().to_dict() #here I will Use Mr, Miss, Mrs, Master and the rest \"Others\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"tit_val = {\"Mr\": 2,\"Miss\": 1,\"Mrs\": 3,\"Master\": 0,\"Dr\": 2,\"Rev\": 2,\"Col\": 2,\"Major\": 2,\"Mlle\": 2,\"Capt\": 2,\"Jonkheer\": 2,\"Countess\": 2,\"Sir\": 2,\"Mme\": 2,\"Ms\": 1,\"Don\": 2,\"Lady\": 3}\nfor data in data_titanic:\n    data['Title'] = data ['Title'].map(tit_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#filling values\ntrain.groupby('Title')['Age'].mean().round()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Now We can Fill the Age \ndef impute_age (col): \n    Age=col[0]\n    Title=col[1]\n    \n    if pd.isnull(Age):\n        \n        if Title== 0 :\n            return 5\n        elif Title == 1:\n            return 22\n        elif Title == 2:\n            return 33\n        else:\n            return 36\n    else:\n        return Age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train['Age'] = train[['Age','Title']].apply(impute_age, axis = 1)\ntest['Age'] = test[['Age','Title']].apply(impute_age, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,(ax1) = plt.subplots(1,1 , figsize=(10,5))\nsns.countplot(x = 'Survived' , data= train , hue = 'Title', ax = ax1).set_title(\"Survivor-Dead By Title\")\nprint(\"Master: 0, Miss: 1, Mr: 2, Mrs: 3\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cabin"},{"metadata":{},"cell_type":"markdown","source":"**> Filling Cabin Information : Cabin could be a great parameter but it has too many missing value, however I can notice the distribution was \"first class had the top decks (A-E)\",\"second class (D-F)\", and \"third class (E-G)\" In the image (on the notebook) , I can notice that, 3rd class was in the fron/back , 2nd class was in the middle , 1st class on the top\nSo In think that Cabit and pclass are related if we change cabin to 1 2 3 we will have the same results**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def cabins (col):\n    classes = col[0]\n    cabin = col[1]\n    \n    if classes == 1:\n        return 3\n    elif classes == 2:\n        return 2\n    else:\n        return 1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2 , figsize=(15,5))\n\nsns.heatmap(train.isnull(), yticklabels = False , cmap = 'plasma', ax = ax1)\nsns.heatmap(test.isnull(), yticklabels = False , cmap = 'plasma', ax = ax2)\nprint(\"No Missing Values\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# V. Modeling "},{"metadata":{},"cell_type":"markdown","source":"### Test Part"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"p_data= train  # I will save train as p_Data because i will try so many different algorithms ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder,LabelBinarizer\n\n#Missing Values\n#print(p_data.isnull().sum())\np_data['Embarked'].fillna('S', inplace= True)\n\n#Step_1 Combine SibSp & Parch\np_data['F_A'] = p_data['SibSp']+p_data['Parch']\np_data['F_A'] =p_data.F_A.apply(lambda x :2 if x>0 else 1)\n\n#Step_2 Transform Cabin\np_data['Cabin_'] =p_data[['Pclass','Cabin']].apply(cabins, axis=1)\n\n#Step 3 Encoding Sex\nsex = pd.get_dummies(p_data['Sex'],drop_first= True)\np_data = pd.concat([p_data,sex],axis = 1)\n\n#step_4 Drop columns\np_data.drop(['PassengerId','Pclass','Name','SibSp','Parch','Ticket','Cabin','Sex'], axis = 1 , inplace = True)\np_data\n\n#Step 5 Onehot\nohc = OneHotEncoder()\nXtest= p_data.iloc[:,3].values\nXtest = Xtest.reshape(-1,1)\nEMB = ohc.fit_transform(Xtest).toarray()\nEB= pd.DataFrame(EMB,columns = ['S','C','Q'])\np_data = pd.concat([p_data ,EB], axis = 1)\np_data.drop('Embarked', axis = 1 , inplace = True)\np_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test Part"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Missing Values\n#print(p_data.isnull().sum())\ntest['Title'].fillna(1, inplace= True)\ntest['Fare'].fillna(35.6271, inplace= True)\n\n#Step_1 Combine SibSp & Parch\ntest['F_A'] = test['SibSp']+test['Parch']\ntest['F_A'] =test.F_A.apply(lambda x :2 if x>0 else 1)\n\n#Step_2 Transform Cabin\ntest['Cabin_'] =test[['Pclass','Cabin']].apply(cabins, axis=1)\n\n#Step 3 Encoding Sex\nsex = pd.get_dummies(test['Sex'],drop_first= True)\ntest = pd.concat([test,sex],axis = 1)\n\n#step_4 Drop columns\ntest.drop(['PassengerId','Pclass','Name','SibSp','Parch','Ticket','Cabin','Sex'], axis = 1 , inplace = True)\n\n#Step 5 Change into Categorical \nohc = OneHotEncoder()\nXohc= test.iloc[:,2].values\nXohc= Xohc.reshape(-1,1)\nEMB = ohc.fit_transform(Xohc).toarray()\nEB= pd.DataFrame(EMB,columns = ['S','C','Q'])\ntest = pd.concat([test ,EB], axis = 1)\ntest.drop('Embarked', axis = 1 , inplace = True)\ntest","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"test_1 = test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying MinMax scaler and Standar Scaler "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,cross_validate\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\n\n#Testing\ndata_1 = p_data  # <----- Standar Scaler\ndata_2 = p_data  # <----- MinMax\ndata_3 = p_data  # <----- \n\n\n#Difining X\nX_1=data_1.drop('Survived',axis = 1)\nX_2=data_2.drop('Survived',axis = 1)\nX_3=data_3.drop('Survived',axis = 1)\n\ny=p_data['Survived']\n\n#Splitting\nX_train, X_test, y_train, y_test = train_test_split(X_1, y, test_size=0.2, random_state=42,stratify=data_2['Cabin_'])\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y, test_size=0.2, random_state=42,stratify=data_2['Cabin_'])\n\n\n# MinMax\nMX= MinMaxScaler()\nX_1 = MX.fit_transform(X_1)\nX_train = MX.fit_transform(X_train)\nX_test = MX.fit_transform(X_test)\ntest_MX = MX.fit_transform(test_1)\n\n# Standar Scalar \nSC = StandardScaler()\nX_2 = SC.fit_transform(X_2)\nX_train_2 = SC.fit_transform(X_train_2)\nX_test_2 = SC.fit_transform(X_test_2)\ntest_SC = SC.fit_transform(test_1)\n\nprint(\"Segmenting Data ....... into Two sets ....Done...!!  \")\nprint(\"Splitting Data into X_train and X_test... Done..!!  \")\nprint(\"Applying MinMax Scaler & Standard Scaler .... Done..!!\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#General Fuction to evaluate Model \n\ndef Evaluating (model,X,y,CV, criteria=True):\n    if criteria :\n        score = cross_val_score(model,X=X ,y=y ,cv=CV, scoring='accuracy', n_jobs= 4)\n        score = np.mean(score)\n        accuracy.append(score)\n    \n    else:\n        pred = cross_val_predict(model,X=X ,y=y ,cv=CV, n_jobs= 4)\n        prediction.append(pred)\n        \nprint(\"Creating General fuction to Evaluate Algorithms.....Done\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"## Baseline"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Machine Learning\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\nLGR = LogisticRegression()\n\nXG = xgb.XGBRFClassifier(base_score=0.5, colsample_bylevel=1, colsample_bynode=0.8,\n                colsample_bytree=0.9539552926340813, gamma=0.08955017265494192,\n                learning_rate=0.07004776526310222, max_delta_step=0,\n                max_depth=24, min_child_weight=1.667528606432285, missing=None,\n                n_estimators=230, n_jobs=1, nthread=None,\n                objective='binary:logistic', random_state=0, reg_alpha=0,\n                reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n                subsample=0.9219040847026176, verbosity=1)\nSVM_1= svm.SVC(probability = True)\n\nSVM_2= svm.SVC()\n\nRF = RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=10, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=4, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=600,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)\nBG = BaggingClassifier(RF)\nNB = GaussianNB()\n\naccuracy = []\nprediction =[]\n\nfor i in [LGR, XG, SVM_1, SVM_2,  RF,BG, NB]:\n      Evaluating(i,X_1,y,CV=5)        \n\nprint(\"Processing....... Done..... \")\nprint(\"Using Cross_Validation and MinMax Scaler with 5 Folds ......Check the Accuracy Below!!\")\n\nMINMAX = pd.DataFrame(accuracy, index = ['LGR','XG','SVM_1','SVM_2','RF','BG','NB'], columns = ['MINMAX_Accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"accuracy = []\nprediction =[]\n\nfor i in [LGR,XG, SVM_1, SVM_2,  RF, BG, NB]:\n      Evaluating(i,X_2,y,CV=5)\n\n\nprint(\"Processing....... Done..... \")\nprint(\"Using Cross_Validation and Standard Scaler with 5 Folds ......Check the Accuracy Below!!\")\nSCALER = pd.DataFrame(accuracy, index = ['LGR','XG','SVM_1','SVM_2','RF','BG','NB'], columns = ['SC_Accuracy'])\n\nprint(\"It seems no to have a significant change no matter what approach I use\")\n\npd.concat([MINMAX,SCALER],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Precision Recall "},{"metadata":{},"cell_type":"markdown","source":"** In here i can find a balance between Recall and Precision , actually with a threshold of 0.7 using Randon Forest i got 0. 78 in the Leader board\"**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_precision_recall_curve\n\nfor i in [XG, SVM_1, SVM_2,  RF, BG, NB]:\n    i.fit(X_2,y)\n\nfig,axs= plt.subplots(2,2, figsize = (20,15))\nplot_precision_recall_curve(XG,X_2,y,ax=axs[0,0])\nplot_precision_recall_curve(SVM_1,X_2,y,ax=axs[0,1])\nplot_precision_recall_curve(BG,X_2,y,ax=axs[1,0])\nplot_precision_recall_curve(RF,X_2,y,ax=axs[1,1])\nplt.show(\"Precision VS ReCall\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Emsembling"},{"metadata":{},"cell_type":"markdown","source":"### Soft"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nprint(\"Emsembling models..... RandomForest, Support Vector Machine, etc, ........\\n\")\nmodelos = [('RandomForest', RF),('BG',BG), ('SVM_1',SVM_1),('XGboost',XG)]\n\nthreshold_1 = 0.8\nthreshold_2 = 0.7\n\nVC= VotingClassifier(estimators = modelos,voting='soft',n_jobs=3)\nVC.fit(X_train_2,y_train_2)\n\nfor i in [RF, BG , SVM_1, XG,VC]:\n    \n    if i == RF:\n        i.fit(X_train_2,y_train_2)\n        predictions = i.predict_proba(X_test_2)\n        y_pred = [1 if predictions[i][1]>threshold_1 else 0  for i in range(len(predictions))]\n        \n        print(i.__class__.__name__,accuracy_score(y_test_2,y_pred))\n        \n    elif i == BG:\n        i.fit(X_train_2,y_train_2)\n        predictions = i.predict_proba(X_test_2)\n        y_pred = [1 if predictions[i][1]>threshold_2 else 0  for i in range(len(predictions))]\n        \n        print(i.__class__.__name__,accuracy_score(y_test_2,y_pred))\n        \n    else:\n        i.fit(X_train_2,y_train_2)\n        predictions = i.predict(X_test_2)\n        print(i.__class__.__name__,accuracy_score(y_test_2,y_pred))\n    \n\nprint(\"\\nTesting Vagging Classifier .... Applying Cross Validation.....!!\")\nscores = cross_val_score(estimator=VC,X=X_2,y=y,cv=5,scoring='accuracy')\nprint(\"Mean Score =\", np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hard"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nprint(\"Emsembling models..... RandomForest, Support Vector Machine, etc, ........\\n\")\nmodelos = [('RandomForest', RF),('BG',BG), ('SVM_1',SVM_1),('XGboost',XG)]\n\nthreshold_1 = 0.8\nthreshold_2 = 0.8\n\nVC= VotingClassifier(estimators = modelos,voting='hard',n_jobs=3 , weights = [2,4,4,4])\nVC.fit(X_train_2,y_train_2)\n\nfor i in [RF, BG , SVM_1, XG,VC]:\n    \n    if i == RF:\n        i.fit(X_train_2,y_train_2)\n        predictions = i.predict_proba(X_test_2)\n        y_pred = [1 if predictions[i][1]>threshold_1 else 0  for i in range(len(predictions))]\n        \n        print(i.__class__.__name__,accuracy_score(y_test_2,y_pred))\n        \n    elif i == BG:\n        i.fit(X_train_2,y_train_2)\n        predictions = i.predict_proba(X_test_2)\n        y_pred = [1 if predictions[i][1]>threshold_2 else 0  for i in range(len(predictions))]\n        \n        print(i.__class__.__name__,accuracy_score(y_test_2,y_pred))\n        \n    else:\n        i.fit(X_train_2,y_train_2)\n        predictions = i.predict(X_test_2)\n        print(i.__class__.__name__,accuracy_score(y_test_2,y_pred))\n    \n\nprint(\"\\nTesting Vagging Classifier .... Applying Cross Validation.....!!\")\nscores = cross_val_score(estimator=VC,X=X_2,y=y,cv=5,scoring='accuracy')\nprint(\"Mean Score =\", np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Prediction\ny_pred = VC.predict(test_SC)\n\nsub = pd.DataFrame()\nsub['PassengerId'] = ID\nsub['Survived'] = y_pred\nsub.to_csv('VC_prediction_1.csv', index=False)\nprint(\"Predicting .......! \")\nprint(\"Submission has been saved\")\nprint(\"Accurancy up to 0.77990\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VI. Deep Learning"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n#Early Stop\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=25, \n        verbose=1, mode='auto', restore_best_weights=True)\n\nNN= Sequential()\n\nNN.add(Dense(9,activation = 'relu',input_shape=[9 ,]))\nNN.add(Dense(5,activation ='relu'))\nNN.add(Dropout(0.3))\nNN.add(Dense(2,activation ='relu'))\nNN.add(Dense(1,activation='sigmoid'))\nNN.compile(optimizer='adam', loss= 'binary_crossentropy', metrics = ['accuracy'] )\n\nNN.fit(x=X_train_2, y=y_train_2,validation_data = (X_test_2, y_test_2) , epochs=600,batch_size=700 ,verbose = 0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pd.DataFrame(NN.history.history).plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"threshold = 0.6\n\npredictions = NN.predict_proba(test_SC)\ny_pred = [1 if predictions[i]>threshold else 0  for i in range(len(predictions))]\n\nsub = pd.DataFrame()\nsub['PassengerId'] = ID\nsub['Survived'] = y_pred\nsub.to_csv('NN_predic_0.6 prediction_500.csv', index=False)\nprint(\"Predicting 0.7829 Accuracy.......! \")\nprint(\"Submission has been saved\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# -----------------Final Thoughts-----------------\n1. Thanks for passing by .. I added Voting classifier, SVM , Xgossbost , And Neural Network , however They all have the same output 0.79 Score \n2. We can see here that the way we process data it is more important that the model that we use\n3. I Will keep Working on this but please feel free to leave some comments "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}