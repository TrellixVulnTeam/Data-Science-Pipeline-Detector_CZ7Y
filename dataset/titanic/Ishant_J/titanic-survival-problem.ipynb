{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"iZ4lUNKbo4zV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading from Csv","metadata":{"id":"E39fdO7mphFd"}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/titanic/train.csv')\ndf_test = pd.read_csv('../input/titanic/test.csv')\ndf_test_results = pd.read_csv('../input/dummy-submission/Best_Submission.csv')","metadata":{"id":"H2Z8S-XHpoo6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.insert(loc=1,column='Survived',value=df_test_results['Survived'])\ndf_train = pd.concat([df_train,df_test],axis=0)\ndf_train.reset_index(inplace=True)\ndf_train.drop('index',axis=1,inplace=True)","metadata":{"id":"56in22wwkdPZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"id":"4shabubemfrA","outputId":"cb127761-1e36-47d7-9ee4-639c7b24b02a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{"id":"rydZhgGci7ZI"}},{"cell_type":"markdown","source":"## Percentage of Missing Data","metadata":{"id":"VVDyupPqo5T_"}},{"cell_type":"code","source":"def Missing_Data_Plot(df):\n  missing_data = []\n  for col_name in df.columns:\n    missing_data.append( df[col_name].isnull().values.sum()/len(df['PassengerId'])*100 )\n\n  sns.set(rc={'figure.figsize':(16,8)})\n  sns.barplot(x=df.columns, y=missing_data)\n  plt.xlabel('Columns')\n  plt.ylabel('Percentage of Missing values')\n  plt.title('Missing Values')\n  plt.yticks(np.arange(0,100,20))","metadata":{"id":"KZe7yd2bSjO3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Missing_Data_Plot(df_train)","metadata":{"id":"ELCVQWxXjjX4","outputId":"366360a7-206e-4442-b836-6e134973efee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling Missing Values","metadata":{"id":"214UWF5qh1dQ"}},{"cell_type":"markdown","source":"### 1. Age","metadata":{"id":"tB9NKoFE1sk4"}},{"cell_type":"code","source":"temp = df_train.loc[(df_train['Age'].isnull() == False)]            # Storing the dataframe with no missing values of Age into Temp","metadata":{"id":"YEtjJocVw1Zf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train['Parch'].unique())                           # Checking if the both temp and df_train have same unique value of Prach and SibSp\nprint(temp['Parch'].unique())\n\nprint(df_train['SibSp'].unique())\nprint(temp['SibSp'].unique())","metadata":{"id":"NS51cMFaw_gQ","outputId":"4ef88a5e-855f-43fe-fd5b-472897f0479b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have one value of Parch that has no Age value,\nand only two rows are there of that value\ntherefore we can reduce it to 8 as later we are going to group the Parch Field anyway.","metadata":{"id":"IJeh4ljbxh2m"}},{"cell_type":"code","source":"df_train['Parch'].loc[df_train['Parch'] == 9] = 6","metadata":{"id":"BeAIzMa_x-9V","outputId":"81a5ac17-816c-45ac-ce86-4491cdb2bd89","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = df_train.loc[(df_train['Age'].isnull() == False)] \nParch_key = list(temp['Parch'].unique())                   #Parch Feature is mapped to a Dictionary\nParch_val = [i for i in range(0,len(Parch_key))]\n\nSib_key = list(temp['SibSp'].unique())                     #SibSp Feature is mapped to a Dictionary\nSib_val = [i for i in range(0,len(Sib_key))]\n\nParch_dict = {}\nSib_dict = {}\n\nfor i in Parch_key:\n  for j in Parch_val:\n    Parch_dict[i] = j\n    Parch_val.remove(j)\n    break\n\n\nfor i in Sib_key:\n  for j in Sib_val:\n    Sib_dict[i] = j\n    Sib_val.remove(j)\n    break\n\n\nAge_mat = []                                              # Now I have created a Age Matrix with Parch and SibSp Dictionaries,\nfor i in list(temp['Parch'].unique()):                    # we can now fill age value with the coreesponding Parch & SibSp Value\n  temp_2 = []\n  for j in list(temp['SibSp'].unique()):\n    age = []\n    for ind in temp.index:\n      if temp['Parch'][ind]==i and temp['SibSp'][ind]==j:\n        age.append(temp['Age'][ind])\n    temp_2.append(np.mean(age).round())\n  Age_mat.append(temp_2)\nAge_mat = np.array(Age_mat)                               #Final Age Matrix is Saved to Age_mat","metadata":{"id":"Gq5lkiKBCOlY","outputId":"0a5154b6-2bc7-4658-b958-91966de916e8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Age_mat)                                           #How our Age Matrix Looks","metadata":{"id":"w75wj1sW0Z9N","outputId":"e96962a2-423a-4fc3-92ea-fa64239f1091","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Age_Imputer(df,Age_mat,Parch_dict,Sib_dict):                                   \n  temp = df.loc[(df['Age'].isnull() == True)]        # Here is the function which will use the parch and SibSp values \n  for ind in temp.index:\n    a = temp['Parch'][ind]  \n    b = temp['SibSp'][ind]                            # to fill the missing Age Values\n    i = Parch_dict[a]\n    j = Sib_dict[b]\n    df['Age'][ind] = Age_mat[i,j]","metadata":{"id":"Rp_7ZgV3NQAx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Age_Imputer(df_train,Age_mat,Parch_dict,Sib_dict)          # Calling the Imputer Function","metadata":{"id":"DoJUvqEqPKaf","outputId":"12b325ae-9ec6-4578-9c11-2c3d867876d3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Missing_Data_Plot(df_train)\ndf_train.info()","metadata":{"id":"I26YDnRX2zcj","outputId":"addcfb7f-a746-4920-fef3-273d099c770c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As Cabin has almost 80% values missing, there is no way we can Impute the Values without inducing somekind of bias into the data.\nTherefore we will drop this Column Later.\n\nNow we only left with one value of Fare and 2 Values of Embarked","metadata":{"id":"H7FRlwBW3Akb"}},{"cell_type":"markdown","source":"### 2. Embarked","metadata":{"id":"HID1BaStIUCr"}},{"cell_type":"code","source":"temp = df_train.loc[(df_train['Embarked'].isnull() == False)]","metadata":{"id":"kksnQiZrZqrw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = []\nfor i in range(1,4):\n  embar = []\n  P1 = temp['Embarked'].loc[(temp['Pclass'] == i)]\n  embar.append(len(P1.loc[(P1 == 'S')]))\n  embar.append(len(P1.loc[(P1 == 'Q')]))\n  embar.append(len(P1.loc[(P1 == 'C')]))\n  count.append(embar)\ncount","metadata":{"id":"10PgvutqbGrB","outputId":"03f5228e-2771-4484-8fc0-21429c80d74d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.arange(3)\nplt.bar(x+0.2, count[0]/np.sum(count[0])*100, color='r',width=0.2, label = 'Class-1')\nplt.bar(x, count[1]/np.sum(count[1])*100, color='orange',width=0.2, label = 'Class-2')\nplt.bar(x-0.2, count[2]/np.sum(count[2])*100, color='g',width=0.2, label = 'Class-3')\nplt.xticks(x, ['S', 'Q', 'C'])\nplt.legend()\nplt.title('Relation between Class and Embarked')\nplt.xlabel('Embarked')\nplt.ylabel('%age. of Passengers')\nplt.show()","metadata":{"id":"ZaO_OXC2D_8R","outputId":"7ff8702d-5049-463f-8220-145b086cdfed","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from the plots :\n1. That most of the passenger have Embarked from 'S'\n2. The passengers that Embarked from 'S' are mostly of Class-3 & Class-2\n3. The passengers that Embarked from 'C' are mostly of Class-1\n\nSince there are only 2 missing values we can :\n1. Either Fill the most frequent value i.e. 'S'.\n2. Or We can Fill the values as per the Class they are in.\n\nI have used the Second Method Since this Notebook is common for Playground Competition, which have the same dataset but with many missing values.","metadata":{"id":"ROUIDd8y4b1z"}},{"cell_type":"code","source":"def Embarked_Imputer(df):                                                       # Function to impute values of Emabrked features using PClass Feature\n  temp = df.loc[df['Embarked'].isnull() == True]\n  for idk in temp.index:\n    if temp['Pclass'][idk] == 3 or temp['Pclass'][idk] == 2:\n      df['Embarked'][idk] = 'S'\n    else:\n      df['Embarked'][idk] = 'C'","metadata":{"id":"GS_pRf9LcmMz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Embarked_Imputer(df_train)","metadata":{"id":"5pnLVfMddZHy","outputId":"ab42618f-bb3b-4087-9076-5cd4e637d501","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Fare","metadata":{"id":"c0MN_5uK6G3n"}},{"cell_type":"markdown","source":"For Fare Imputation, according to my hypothesis should be dependent of 3 factors only :\n1. Age of Passenger (\"Age\")\n2. From where he is Boarding the Ship (\"Embarked\")\n3. Which Class he is travelling (\"Pclass\")\n\nTherefore i have made a 2D matrix of fare with x ad y index being the \"Age_Bin\" and \"Emabrked & Class Combined\"","metadata":{"id":"0mul-H7E6TfB"}},{"cell_type":"code","source":"temp = df_train.dropna(axis=0)","metadata":{"id":"FBIfddfkmghe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_1 = (temp['Embarked'] == 'S' )\nmask_2 = (temp['Embarked'] == 'Q')\nmask_3 = (temp['Embarked'] == 'C')\ntemp_1 = temp[{'Age','Fare','Pclass'}].loc[mask_1]\ntemp_2 = temp[{'Age','Fare','Pclass'}].loc[mask_2]\ntemp_3 = temp[{'Age','Fare','Pclass'}].loc[mask_3]","metadata":{"id":"TpWViC-xobyF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embarked_filter = [temp_1, temp_2, temp_3]","metadata":{"id":"NPcYIH8v7KeE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Class_Filter(temp):\n  group = temp.groupby(by='Pclass')\n  temp_4 = np.array(group.get_group(1))\n  temp_5 = np.array(group.get_group(2))\n  temp_6 = np.array(group.get_group(3))\n  class_filter = [temp_4,temp_5,temp_6]\n  return class_filter ","metadata":{"id":"Nfm9Cs9E8SMF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_filtered = []\nfor filt in embarked_filter:\n  class_filtered.append(Class_Filter(filt))  ","metadata":{"id":"w1IeBi_0rpEf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = np.where(class_filtered[0][0][0] == 35)\nf = np.where(class_filtered[0][0][0] == 53.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def age_bin(temp):\n  age_cl_1 = []\n  age_cl_2 = []\n  age_cl_3 = []\n  age_cl_4 = []\n  new_fare = []\n  for i in range(0,len(temp)):\n    age = temp[i,a[0][0]]\n    fare = temp[i,f[0][0]].round(2)\n    if age > 0 and age <= 20:\n      age_cl_1.append(fare)\n    if age > 20 and age <= 40:\n      age_cl_2.append(fare)\n    if age > 40 and age <= 60:\n      age_cl_3.append(fare)\n    if age > 60 and age <= 90:\n      age_cl_4.append(fare)\n\n  new_fare = [np.mean(age_cl_1).round(2), np.mean(age_cl_2).round(2), np.mean(age_cl_3).round(2), np.mean(age_cl_4).round(2)]\n  return new_fare","metadata":{"id":"-tMVAVm4w6ta","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fare = []\nfor i in range(0,3):\n  for j in range(0,3):\n    slice = class_filtered[i][j]\n    fare.append(age_bin(slice))","metadata":{"id":"ozzVeLNG-2Uv","outputId":"daa7c7db-3b14-41c1-87f3-f3037acecc4b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Fare_mat = np.array(fare)","metadata":{"id":"PwNbYWXC_uR8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Fare_mat)                            #How our Fare matrix Looks","metadata":{"id":"WHH5KbTf11yC","outputId":"3f592340-a906-445d-d204-3dc27e4164b7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualising Distribution of 'Fare' with 'Class','Emarked' and 'Age group'","metadata":{"id":"opPbzpl-P96g"}},{"cell_type":"code","source":"Age_bin = np.arange(4)\nplt.figure(figsize=[16,12])\nplt.bar(Age_bin-0.2,Fare_mat[0],color = 'orange', width = 0.2, label = 'Class-1')\nplt.bar(Age_bin,Fare_mat[1],color = 'red', width = 0.2, label = 'Class-2')\nplt.bar(Age_bin+0.2,Fare_mat[2],color = 'blue', width = 0.2, label = 'Class-3')\nplt.xticks(Age_bin,['0-20','20-40','40-60','>60'])\nplt.xlabel('Age Groups')\nplt.ylabel('Fare')\nplt.title(\"Fare Distribution for Emabrked = 'S'\")\nplt.legend(loc='upper right')\nplt.show()","metadata":{"id":"bsRR8f_apln9","outputId":"2ba86a3c-1ce2-4f8c-fafc-24e6e2495bc6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Age_bin = np.arange(4)\nplt.figure(figsize=[16,12])\nplt.bar(Age_bin-0.2,Fare_mat[3],color = 'orange', width = 0.2, label = 'Class-1')\nplt.bar(Age_bin,Fare_mat[4],color = 'red', width = 0.2, label = 'Class-2')\nplt.bar(Age_bin+0.2,Fare_mat[5],color = 'blue', width = 0.2, label = 'Class-3')\nplt.xticks(Age_bin,['0-20','20-40','40-60','>60'])\nplt.xlabel('Age Groups')\nplt.ylabel('Fare')\nplt.title(\"Fare Distribution for Emabrked = 'Q'\")\nplt.legend(loc='upper right')\nplt.show()","metadata":{"id":"T_Fes6R-MDRv","outputId":"ed5ad18d-9b95-4034-8c47-b89652bcce20","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Age_bin = np.arange(4)\nplt.figure(figsize=[16,12])\nplt.bar(Age_bin-0.2,Fare_mat[6],color = 'orange', width = 0.2, label = 'Class-1')\nplt.bar(Age_bin,Fare_mat[7],color = 'red', width = 0.2, label = 'Class-2')\nplt.bar(Age_bin+0.2,Fare_mat[8],color = 'blue', width = 0.2, label = 'Class-3')\nplt.xticks(Age_bin,['0-20','20-40','40-60','>60'])\nplt.xlabel('Age Groups')\nplt.ylabel('Fare')\nplt.title(\"Fare Distribution for Emabrked = 'C'\")\nplt.legend(loc='upper right')\nplt.show()","metadata":{"id":"AJV9J7xHMIQX","outputId":"113f522f-f1f6-4da9-8a24-7f43d1902820","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Fare_Imputer(train,new):\n  df_array = np.array(train)\n  age_dict = { '0': 0,'1': 1,'2': 2,'3': 3}\n  comb_dict = { 'S1': 0, 'S2': 1, 'S3': 2, 'Q1': 3, 'Q2': 4, 'Q3': 5, 'C1': 6, 'C2': 7, 'C3': 8 }\n  for i in range(0,len(train['PassengerId'])):\n    if np.isnan(df_array[i,9]):\n      age = df_array[i,5]\n      clas = df_array[i,2]\n      embar = df_array[i,11]\n      comb = str(embar) + str(clas)\n      x = comb_dict[comb]\n      y = age_dict[str(math.floor(age/20))]\n      if np.isnan(new[x,y]):\n        df_array[i,9] = train['Fare'].median()\n      else: \n        df_array[i,9] = new[x,y].round(2)\n  col = train.columns\n  return pd.DataFrame(df_array,columns=col)","metadata":{"id":"UnEC08cwTpFC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = Fare_Imputer(df_train,Fare_mat)\n","metadata":{"id":"1b_1pg7-XCGg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"id":"Q8myPGP6_wO9","outputId":"aa787885-0a7a-40e4-9c90-58f8288f7da5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df_train = df_train.copy()","metadata":{"id":"xCApPt6TDqf5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering : Adding extra features\n","metadata":{"id":"9AKln9p9uBgG"}},{"cell_type":"markdown","source":"## 1. Deriving a new feature \"Alone\" if a traveler is alone or not","metadata":{"id":"bcQea2O_kfuo"}},{"cell_type":"code","source":"# Alone_Function creates a new feature \"Alone\"\n\ndef Alone(df):\n  df.insert(12,'Alone',\"\")\n  df.insert(13,'Small Family',\"\")\n  df.insert(14,'Big Family',\"\")\n  alone = []\n  sf = []\n  bf = []\n\n  for ind in df.index:\n    if df['SibSp'][ind] == 0 and df['Parch'][ind] == 0:\n      alone.append(1)\n      sf.append(0)\n      bf.append(0)\n    elif df['SibSp'][ind] > 2 or df['Parch'][ind] > 2 :\n      alone.append(0)\n      sf.append(0)\n      bf.append(1)\n    else:\n      alone.append(0)\n      sf.append(1)\n      bf.append(0)\n  df.iloc[:,12] = alone\n  df.iloc[:,13] = sf\n  df.iloc[:,14] = bf","metadata":{"id":"NMyYHyaNZYfV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Alone(new_df_train) #Applying Function on Train Set","metadata":{"id":"ww0lR2cHoaC-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualising  feature Alone with Survival","metadata":{"id":"JTYNmQ-FQkRe"}},{"cell_type":"code","source":"col_list = ['Alone','Small Family','Big Family']\nSurvived = [ (new_df_train[col].loc[(new_df_train['Survived'] == 1)].sum()/new_df_train[col].sum()*100).round(2) for col in col_list ]\nNot_Survived = [ (new_df_train[col].loc[(new_df_train['Survived'] == 0)].sum()/new_df_train[col].sum()*100).round(2) for col in col_list ]","metadata":{"id":"l8qCFsiFKEJc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = [16,8])\nplt.bar(0, Survived[0], color='orange',width=0.2, label = 'Survived')\nplt.bar(1,Survived[1], color='orange',width=0.2)\nplt.bar(2, Survived[2], color='orange',width=0.2)\n\nplt.bar(0.2, Not_Survived[0], color='g',width=0.2, label = 'Not-Survived')\nplt.bar(1.2, Not_Survived[1], color='g',width=0.2,)\nplt.bar(2.2, Not_Survived[2], color='g',width=0.2,)\n\nplt.bar(0.1, (Survived[0]/Not_Survived[0])*100, color='black',width=0.01, label = 'Chances of Survival')\nplt.bar(1.1, (Survived[1]/Not_Survived[0])*100, color='black',width=0.01,)\nplt.bar(2.1, (Survived[2]/Not_Survived[0])*100, color='black',width=0.01,)\n\nplt.xlabel('Percentage of Passengers')\nplt.ylabel('Type of Group of Passenger')\nplt.title('Group of Passengers vs Survival')\nplt.xticks([0.1,1.1,2.1],['Alone','Small Family','Big Family'])\nplt.legend()","metadata":{"id":"K4N_AlW3_eqi","outputId":"e4812b2b-4630-48fa-97a9-fe6fd806603e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from the Graph it is clear that \n1. most of the passengers that were traveling alone have died, and chances of survival of alone person are least.\n2. Most of the small families survived, may be its because of the fact that small family members are less in no. hence gathered easily at the rescue point.","metadata":{"id":"wC-RdcC1mHfb"}},{"cell_type":"markdown","source":"## 1. Deriving a new feature \"Salutation\" i.e. Title of the person travelling then we will group them by 'Regular' ,'Officer', 'Royalty'","metadata":{"id":"wTzRfQ5mOdgL"}},{"cell_type":"code","source":"#Salutation Function for Extracting Salutation and Length of Name from Name Column\ndef Salutation(df):\n  temp = []\n  length = []\n  for name in df['Name']: \n    temp_name = name.split(\", \")[1]\n    length.append(len(name))\n    temp.append(temp_name.split(\" \")[0])\n  df['Salutation'] = temp\n  df['Len_Name'] = length","metadata":{"id":"odagfvnJR4Tn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Salutation(new_df_train)","metadata":{"id":"IN1Zi2_cSGqN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sal_list = new_df_train['Salutation'].unique()\nsal_count = []\nfor i in sal_list:\n  temp=0\n  for j in new_df_train['Salutation']:\n    if j == i:\n      temp+=1\n  sal_count.append(temp)    \nsal_count\n\nplt.figure(figsize=[16,8])\nsns.barplot(x=sal_list, y=sal_count)\nplt.xlabel('Salutation Type')\nplt.ylabel('Count of Salutation')","metadata":{"id":"Lcf5LAVQSpPd","outputId":"85f3b89a-fadc-4cd2-eb97-380ff5fac54e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = []\nfor sal in new_df_train['Salutation']:\n  if sal in ['Mr.','Mrs.']:\n    temp.append('Adults')\n  elif sal in ['Miss','Master.']:\n    temp.append('Teen/Kid')\n  elif sal in ['Dr.','Major.','Col.','Capt.']:\n    temp.append('Officer')\n  else:\n    temp.append('Royalty')\nnew_df_train['New_Sal'] = temp","metadata":{"id":"HDSJS4O7LL9G","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sal_list = new_df_train['New_Sal'].unique()\nSurvived_Sal = []\nNot_Survived_Sal = []\nfor i in sal_list:\n  tot = new_df_train['Survived'].loc[(new_df_train['New_Sal'] == i)]\n  Survived_Sal.append( round( ( tot.sum() / len(tot) ),4) * 100 )\n  Not_Survived_Sal.append( round( ( ( len(tot) - tot.sum() ) / len(tot) ),4)*100 )\nprint(Survived_Sal)\nprint(Not_Survived_Sal)","metadata":{"id":"aKTUkcNUMepF","outputId":"9e4377bd-f0b7-4f4d-ccb5-362d52b58294","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Sal_type = np.arange(4)\nplt.bar(Sal_type-0.2,Survived_Sal,color = 'orange', width = 0.2, label = 'Survived')\nplt.bar(Sal_type,Not_Survived_Sal,color = 'g', width = 0.2, label = 'Not-Survived')\n\nplt.xticks(Age_bin,['Adults','Teens/Kids','Officers','Royalty'])\nplt.xlabel('Sal Types')\nplt.ylabel('No. of Passengers')\nplt.title(\"Survival on basis of Salutation\")\nplt.legend()\nplt.show()","metadata":{"id":"DW1nTCgNRT9E","outputId":"115423b1-55dc-4eed-a516-466634364fb5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\ndef Encoder(df,col_name):\n  Encoder = OneHotEncoder(sparse=False,handle_unknown='ignore')\n  temp = pd.DataFrame(Encoder.fit_transform(df[[col_name]]))\n  temp.columns = Encoder.get_feature_names([col_name])\n  return temp","metadata":{"id":"5hsrHZvDy2DU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_columns = {'Pclass','Embarked','Sex','Parch','SibSp','New_Sal'}\nfor col in encoded_columns:\n  temp = Encoder(new_df_train,col)\n  new_df_train = pd.concat([new_df_train,temp],axis=1)","metadata":{"id":"pPc1OQxsy5fz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping Extra Columns like **'Name' , 'Sex' , 'Embarked' , 'Cabin' , 'Ticket' , 'New_Sal' , 'Salutation'**","metadata":{"id":"OkJIDZ7D6UYg"}},{"cell_type":"code","source":"drop_col_list = {'Name', 'Sex','Embarked','Cabin','Ticket','New_Sal','Salutation'}\nnew_df_train.drop(drop_col_list,axis=1,inplace=True)\nnew_df_train = new_df_train.reset_index(drop=True)","metadata":{"id":"B7Cve6kL6yJl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df_train.info()","metadata":{"id":"sc_c-7VrXLsC","outputId":"aa098954-cb76-4cd6-be6e-552f629ddd86","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df_train = new_df_train.astype(int)\nnew_df_train['Fare'] = new_df_train['Fare'].astype(float)","metadata":{"id":"FMXYXH4kk0aP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df_train.info()","metadata":{"id":"LjUR2Bx0VJdd","outputId":"f7c795f7-f192-46da-e638-bebbeb1e0be4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection / Feature Importance","metadata":{"id":"N91j45J7_ooE"}},{"cell_type":"markdown","source":"### Importing Neccessary Models","metadata":{"id":"Qhgnm3o9BYzZ"}},{"cell_type":"code","source":"# Importing Models for Feature Selection\nfrom sklearn.feature_selection import SelectFromModel\n\n# Importing TrainTest Split and MinMaxScaler for Scaling the Data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Importing Grid Search CV for HyperParameterTuning and Stratifiedkfold for CV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\n# Importing Classifier Models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Importing Metrics From Sklearn for Evaluation\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report","metadata":{"id":"bxV_S6DuAFxa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = new_df_train['Survived']\nX = new_df_train.drop( 'Survived' ,axis=1 )","metadata":{"id":"LsE0J104ZF2y","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=50, max_features='sqrt')\nclf = clf.fit(X,Y)\nfeatures = pd.DataFrame()\nfeatures['feature'] = X.columns\nfeatures['Importance'] = clf.feature_importances_\nfeatures.sort_values(by=['Importance'],ascending = True,inplace=True)\nfeatures.set_index('feature',inplace=True)\n\n# Ploting the Feature Importance\nfeatures.plot(kind = 'barh', figsize = (25,25))","metadata":{"id":"9S0kJ1c6Y5ny","outputId":"68788f97-3248-4d31-b63d-71b317192d9f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SelectFromModel(clf,prefit=True)\nreduced_df_train = model.transform(X)","metadata":{"id":"whSQhpQfcZLy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = reduced_df_train[:891,:]\ntest_set = reduced_df_train[891:,:]\nY = np.array(Y)\nY = Y.reshape(1309,1)\nX = train_set","metadata":{"id":"fdOM5RnxdNn3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting Data","metadata":{"id":"Jf3tgvvi7PsU"}},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,Y[:891,-1], random_state = 42,test_size=0.3)","metadata":{"id":"-L4sFpzR74tX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"id":"7DBFXGuj8RBc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Grid Search CV for Hyperparameter tuning Of Models","metadata":{"id":"MN57moa8g3mm"}},{"cell_type":"markdown","source":"# Using Models without HyperParameter Tuning","metadata":{"id":"hIKgEZIMgqCb"}},{"cell_type":"code","source":"classifier = RandomForestClassifier()\nclassifier.fit(X_train,y_train)\n\nclassifier_2 = LogisticRegression()\nclassifier_2.fit(X_train,y_train)\n\nclassifier_3 = GradientBoostingClassifier()\nclassifier_3.fit(X_train,y_train)\n\nclassifier_4 = XGBClassifier()\nclassifier_4.fit(X_train,y_train)\n\nclassifier_5 = VotingClassifier(estimators=[('rf',classifier),('gb',classifier_3),('xgb',classifier_4)], voting='hard',n_jobs=-1,)\nclassifier_5.fit(X_train,y_train)","metadata":{"id":"DIS45k6CkJQ2","outputId":"75b9b5c5-7387-4b61-9bc3-289c313c6376","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(X_test)\nprint(\"\\033[1m\" + \"For Random Forest :\" + \"\\033[0m\")\nprint(classification_report(y_test,y_pred))\n\ny_pred_2 = classifier_2.predict(X_test)\nprint(\"\\033[1m\" + \"For Logistic Regression :\" + \"\\033[0m\")\nprint(classification_report(y_test,y_pred_2))\n\ny_pred_3 = classifier_3.predict(X_test)\nprint(\"\\033[1m\" + \"For Gardient Boosting Classifier :\" + \"\\033[0m\")\nprint(classification_report(y_test,y_pred_3))\n\ny_pred_4 = classifier_4.predict(X_test)\nprint(\"\\033[1m\" + \"For XGBoost Classifier :\" + \"\\033[0m\")\nprint(classification_report(y_test,y_pred_4))\n\ny_pred_5 = classifier_5.predict(X_test)\nprint(\"\\033[1m\" + \"For Voting Classifier :\" + \"\\033[0m\")\nprint(classification_report(y_test,y_pred_5))","metadata":{"id":"6NSYathvovsA","outputId":"d5e31bff-b885-4d25-c0a0-65007c989b9a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary of Predictions :\npredictions = [y_pred, y_pred_2, y_pred_3,y_pred_4,y_pred_5]\nj = 1\nfor i in predictions:\n  print(\"\\033[1m\" +'For Classifier :' + str(j) + \"\\033[0m\" )\n  print(accuracy_score(y_test,i))\n  print(confusion_matrix(y_test,i))\n  j+=1","metadata":{"id":"9Osn_ens4024","outputId":"e75f1b64-1857-44e5-df3a-f1b83ff49006","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculating Best Parameters for RandomForestClassifier","metadata":{"id":"j9ASIt47Gqas"}},{"cell_type":"code","source":"grid   =   [ {\n               'max_depth': [4,6,8],                                             #Random_forest_grid\n               'min_samples_split': [2, 3, 10],\n               'min_samples_leaf': [1, 3, 10],\n               'criterion' : ['gini', 'entropy' ],\n               'max_features' : ['sqrt','auto','log2'],\n               'n_estimators': [50,10] } ]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_val = StratifiedKFold(n_splits = 3)  \nGSCV = GridSearchCV( RandomForestClassifier(), param_grid = grid[0], cv = cross_val, verbose=0 )\nGSCV.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\" Best Parameters:\" + str(GSCV.best_params_))\nprint(\" Best Score:\" + str(GSCV.best_score_))\nRF_param = GSCV.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculating Best Parameters for Logistic Regression","metadata":{}},{"cell_type":"code","source":"grid   =   [  {\n               'solver' : ['newton-cg', 'lbfgs', 'liblinear'],            #Logistic_Regression_Grid\n               'penalty' : ['l1', 'l2'],\n               'C': [0.001, 0.01, 0.1, 1, 10]  }  ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GSCV = GridSearchCV( LogisticRegression(), param_grid = grid[0], cv = cross_val, verbose=0 )\nGSCV.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\" Best Parameters:\" + str(GSCV.best_params_))\nprint(\" Best Score:\" + str(GSCV.best_score_))\nLR_param = GSCV.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# After Hyperparameter Tuning of Models","metadata":{}},{"cell_type":"code","source":"classifier = RandomForestClassifier(**RF_param)\nclassifier.fit(X_train,y_train)\n\nclassifier_2 = LogisticRegression(**LR_param)\nclassifier_2.fit(X_train,y_train)\n\nclassifier_3 = GradientBoostingClassifier(n_estimators = 180, min_samples_split = 2, min_samples_leaf = 4, max_depth = 50, loss = 'deviance', learning_rate = 0.01, criterion = 'mse')\nclassifier_3.fit(X_train,y_train)\n\nclassifier_4 = XGBClassifier()\nclassifier_4.fit(X_train,y_train)\n\nclassifier_5 = VotingClassifier(estimators=[('rf',classifier), ('gb',classifier_3),('xgb',classifier_4)], voting='hard',n_jobs=-1,)\nclassifier_5.fit(X_train,y_train)","metadata":{"id":"hKm96DY5GkHx","outputId":"126e4044-28bb-4596-dec2-774caf07ff7d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(X_test)\nprint(\"For Random Forest :\")\nprint(classification_report(y_test,y_pred))\n\ny_pred_2 = classifier_2.predict(X_test)\nprint(\"For Logistic Regression :\")\nprint(classification_report(y_test,y_pred_2))\n\ny_pred_3 = classifier_3.predict(X_test)\nprint(\"For Gradient Boosting :\")\nprint(classification_report(y_test,y_pred_3))\n\ny_pred_4 = classifier_4.predict(X_test)\nprint(\"For Xtreme Gradient Boosting :\")\nprint(classification_report(y_test,y_pred_4))\n\ny_pred_5 = classifier_5.predict(X_test)\nprint(\"For Voting Classifier :\")\nprint(classification_report(y_test,y_pred_5))","metadata":{"id":"n60SdqBsHKp6","outputId":"f34a3363-7917-4821-dfca-5351984027b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary of Predictions after hyper Parameter Tuning:\npredictions_2 = [y_pred, y_pred_2, y_pred_3,y_pred_4,y_pred_5]\nj = 1\nfor i in predictions:\n  print('For Classifier :' + str(j))\n  print(accuracy_score(y_test,i))\n  print(confusion_matrix(y_test,i))\n  j+=1","metadata":{"id":"shq2Ec-vHWeg","outputId":"7bd5ff99-11a1-4d35-cf93-74b7c1066e76","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparasion of Predictions :\nj = 1\nfor i in range(0,len(predictions)):\n  print('For Classifier :' + str(j))\n  print('Without HyperParameter Tuning' + str(accuracy_score(y_test,predictions[i])) )\n  print('After HyperParameter Tuning' + str(accuracy_score(y_test,predictions_2[i])) )\n  j+=1","metadata":{"id":"oNYj9nu-HNrZ","outputId":"016917b8-b2ff-472d-cca9-4d5c684bc8a2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Keras Sequential Model","metadata":{"id":"ynatiq07PNSu"}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\n\ndim = X_train.shape[1]\nmodel = Sequential()\nmodel.add(Dense(8, input_dim=dim, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the keras model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit the keras model on the dataset\nmodel.fit(X_train,y_train, epochs=100, batch_size=10)\n# evaluate the keras model\n_, accuracy = model.evaluate(X_train, y_train)\nprint('Accuracy: %.2f' % (accuracy*100))","metadata":{"id":"iEl6F2YfSg6T","outputId":"7eaa396e-39f8-4042-c324-3081aa995ddc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_6 = model.predict(X_test,batch_size=10).round(0)","metadata":{"id":"njLqqhVvTWfH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_6 = y_pred_6.astype(int)","metadata":{"id":"cR19aclwT7ir","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_6))\nprint(accuracy_score(y_test,y_pred_6))\nprint(confusion_matrix(y_test,y_pred_6))","metadata":{"id":"lxa1_Ei9TjNS","outputId":"f872788f-96ea-4bd5-fac2-09fd8c499475","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Submission File","metadata":{"id":"1RDu01tChg_W"}},{"cell_type":"code","source":"result = pd.DataFrame(classifier.predict(scaler.transform(test_set)),columns={'Survived'})","metadata":{"id":"YgxZn1I0vUtJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/titanic/test.csv')\nsubmission =pd.concat([submission['PassengerId'],result],axis=1)\nsubmission.to_csv('./Submission_5.csv',index=False)","metadata":{"id":"pJRr53jtSf-i","trusted":true},"execution_count":null,"outputs":[]}]}