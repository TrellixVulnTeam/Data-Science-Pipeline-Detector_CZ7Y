{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"------------------------------------------\n------------------------------------------\n# <p style=\"background-color:gray; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 10px 100px; color:black; hight:max\"> Upvote my work if you found it useful.🎯 </p>\n------------------------------------------\n------------------------------------------\n\n# <p style=\"background-color:#C0392B; font-family:newtimeroman; font-size:175%; text-align:center; border-radius: 15px 50px;\">Predicting whether passengers on the Titanic would survive or not 🚢</p>\n<img src=\"https://miro.medium.com/max/2400/1*ePbfZdw6sz397xLWlFZLCQ.jpeg\" alt=\"Titanic\" hight=50 width=800></img>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Introduction</b></p>\n<b>The objective of this project is to build a model to predict whether passengers on the Titanic would survive or not based on pattern extracted from analysing 14 descriptive features like their age, Sex, class of travel, port Embarked etc.<br></b>\n<b>This project consists of two phases:\n<ul>\n    <li>Phase I: Focuses on data preprocessing and exploration, as covered in this report.\n    <li>Phase II : The model building, validation and prediction.\n</ul>\n<p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\">We have 12 descriptive features:</p>\n<ul>\n    <li>PassengerId : Passenger's Id\n    <li>Age : Age of the Passenger\n    <li>Sex : Sex of the Passenger\n    <li>Name : Name of the Passenger\n    <li>Embarked : \n        <ul>\n            <li>Southampton\n            <li>Cherbourg\n            <li>Queenstown\n        </ul>\n    <li>Parch : Number of Parents/Children Aboard\n    <li>SibSp : Number of Siblings/Spouses Aboard\n    <li>Fare : Passenger Fare\n    <li>Ticket : Ticket Number\n    <li>Cabin : Cabin\n    <li>Pclass : \n        <ul>\n            <li>1 = 1st\n            <li>2 = 2nd\n            <li>3 = 3rd\n        </ul>\n    <li>Survived :\n        <ul>\n            <li>1 for Survived \n            <li>0 for Not-Survived\n        </ul>\n    </ul>\n<h2><span>&#8226;</span> Outline:</h2>\n<ul>\n    <li><a href=\"#Phase I\"><b>Phase I</b><a/>\n        <ul>\n            <li><a href=\"#head-1\">Data Pre-processing</a>\n            <li><a href=\"#head-2\">Setup and Basic EDA</a>  \n                <ul>\n                    <li><a href=\"#head-2-1\">Univariate Visualisation</a>\n                        <ul>\n                            <li><a href=\"#head-2-1-1\">Categorical Features</a>\n                                <ul>\n                                    <li><a href=\"#sex\">sex column</a>\n                                    <li><a href=\"#pclass\">Pclass column</a>\n                                    <li><a href=\"#embarked\">Embarked column</a>\n                                    <li><a href=\"#parch\">parch column</a>\n                                    <li><a href=\"#sibsp\">SibSp column</a>\n                                    <li><a href=\"#ticket\">Ticket column</a>\n                                    <li><a href=\"#cabin\">Cabin column</a>\n                                </ul>\n                            <li><a href=\"#head-2-1-2\">Numerical Features</a>\n                                <ul>\n                                    <li><a href=\"#age\">Age column</a>\n                                    <li><a href=\"#fare\">Fare column</a>\n                                </ul>\n                        </ul>\n                    <li><a href=\"#head-2-2\">Multivariate Visualisation</a>\n                        <ul>\n                            <li><a href=\"#sct_mtx\">Scatter Matrix for the data</a>\n                            <li><a href=\"#corr_mtx\">Correlation Matrix for the data</a>\n                            <li><a href=\"#multi\">Multi-Violin and Multi-Box plots for each column</a>\n                        </ul>\n                </ul>\n        </ul>\n        <li><a href=\"#Phase II\"><b>Phase II:</b></a>\n        <ul>\n            <li><a href=\"#prep_ml\">Prepare the data for the machine learning model.</a>\n            <li><a href=\"#ml_models\">Comparing different Machine learning models.</a>\n            </ul>","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px;\">Importing necessary modules and libraries📚</p>","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://media1.tenor.com/images/047e6fd4e7169886e992a8899e62b90b/tenor.gif?itemid=12547153\" height=\"200\" style=\"margin: 0 ; max-width: 950px;\" frameborder=\"0\" scrolling=\"auto\" title=\"House price prediction\"></img>","metadata":{}},{"cell_type":"code","source":"#main libraries\nimport os\nimport re\nimport pickle\nimport numpy as np\nimport pandas as pd\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly \nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks as cf\n\n#machine learning libraries:\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_score, train_test_split\nfrom sklearn.preprocessing  import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import KNNImputer, IterativeImputer\nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# You can go offline on demand by using\ncf.go_offline() \n\n# To connect java script to your notebook\ninit_notebook_mode(connected=False)\n\n# set some display options:\npd.set_option(\"display.float\", \"{:.4f}\".format)\nplt.rcParams['figure.dpi'] = 100\ncolors = px.colors.qualitative.Prism\npio.templates.default = \"plotly_white\"\n\n# see our files:\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-29T04:53:44.045457Z","iopub.execute_input":"2021-07-29T04:53:44.046178Z","iopub.status.idle":"2021-07-29T04:53:49.176085Z","shell.execute_reply.started":"2021-07-29T04:53:44.046085Z","shell.execute_reply":"2021-07-29T04:53:49.175116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"Phase I\"></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 100px;\">Phase I</p>","metadata":{}},{"cell_type":"markdown","source":"## Data Pre-processing <a class=\"anchor\" id=\"head-1\"></a>","metadata":{}},{"cell_type":"markdown","source":"### Getting the Data","metadata":{}},{"cell_type":"code","source":"#import the data\ntrain_df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n#join all the data together\nfull_df = pd.concat([train_df,test_df])\n\n#make a copy of the original data\ntrain_df_orig = train_df.copy()\ntest_df_orig = test_df.copy()\n\n#show the head of the data\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:53:49.178695Z","iopub.execute_input":"2021-07-29T04:53:49.179133Z","iopub.status.idle":"2021-07-29T04:53:49.248607Z","shell.execute_reply.started":"2021-07-29T04:53:49.179086Z","shell.execute_reply":"2021-07-29T04:53:49.247566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Exploration/Analysis","metadata":{}},{"cell_type":"code","source":"#the shape of the data\nprint('This data contains {} rows and {} columns splited into train/test datasets with ratio {}'.\\\n      format(full_df.shape[0],full_df.shape[1],round((test_df.shape[0]/train_df.shape[0])*100,2)))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:53:49.250527Z","iopub.execute_input":"2021-07-29T04:53:49.250956Z","iopub.status.idle":"2021-07-29T04:53:49.257633Z","shell.execute_reply.started":"2021-07-29T04:53:49.250914Z","shell.execute_reply":"2021-07-29T04:53:49.256533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = train_df.columns\nprint(f'We have {len(cols)} columns : \\n{cols}')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:53:49.259642Z","iopub.execute_input":"2021-07-29T04:53:49.260163Z","iopub.status.idle":"2021-07-29T04:53:49.270309Z","shell.execute_reply.started":"2021-07-29T04:53:49.260119Z","shell.execute_reply":"2021-07-29T04:53:49.269387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#see information about the data\n\n#get the sum of all missing values in the dataset\nmissing_values = train_df.isnull().sum()\n\n#sorting the missing values in a pandas Series\nmissing_values = missing_values.sort_values(ascending=False)\n\nfeature_dtypes = train_df.dtypes\nfeature_names = missing_values.index.values\nmissing_values = missing_values.values\nrows, columns = train_df.shape\n\nprint(\"=\" * 50)\nprint('====> This data contains {} rows and {} columns'.format(rows,columns))\nprint(\"=\" * 50)\nprint()\n\nprint(\"{:15} {:15} {:35} {:15}\".format('Feature Name'.upper(),\n                                     'Data Format'.upper(),\n                                     'Missing values (Num - Perc)'.upper(),\n                                     'Three Samples'.upper()))\n\nfor feature_name, dtype, missing_value in zip(feature_names,feature_dtypes[feature_names],missing_values):\n    print(\"{:17} {:17} {:25}\".format(feature_name,\n                                 str(dtype), \n                                 str(missing_value) + ' - ' + \n                                 str(round(100*missing_value/sum(missing_values),3))+' %'), end=\"\")\n\n    for i in np.random.randint(0,len(train_df),2):\n        print(train_df[feature_name].iloc[i], end=\",\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:12.341117Z","iopub.execute_input":"2021-07-29T05:02:12.341491Z","iopub.status.idle":"2021-07-29T05:02:12.359947Z","shell.execute_reply.started":"2021-07-29T05:02:12.341461Z","shell.execute_reply":"2021-07-29T05:02:12.358558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#show the types of columns\ntrain_df.dtypes.to_frame().rename(columns={0:'Column type'})","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:22.396746Z","iopub.execute_input":"2021-07-29T05:02:22.397116Z","iopub.status.idle":"2021-07-29T05:02:22.410971Z","shell.execute_reply.started":"2021-07-29T05:02:22.397086Z","shell.execute_reply":"2021-07-29T05:02:22.409831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#finding the unique values in each column\nfor col in train_df.columns:\n    print('We have {} unique values in {} column'.format(len(train_df[col].unique()),col))\n    print('__'*30)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:22.799057Z","iopub.execute_input":"2021-07-29T05:02:22.79941Z","iopub.status.idle":"2021-07-29T05:02:22.812936Z","shell.execute_reply.started":"2021-07-29T05:02:22.799378Z","shell.execute_reply":"2021-07-29T05:02:22.81163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['SibSp'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:23.596019Z","iopub.execute_input":"2021-07-29T05:02:23.596367Z","iopub.status.idle":"2021-07-29T05:02:23.603562Z","shell.execute_reply.started":"2021-07-29T05:02:23.596338Z","shell.execute_reply":"2021-07-29T05:02:23.602223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Parch'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:23.848186Z","iopub.execute_input":"2021-07-29T05:02:23.848542Z","iopub.status.idle":"2021-07-29T05:02:23.855473Z","shell.execute_reply.started":"2021-07-29T05:02:23.848512Z","shell.execute_reply":"2021-07-29T05:02:23.854406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Embarked'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:24.208961Z","iopub.execute_input":"2021-07-29T05:02:24.209301Z","iopub.status.idle":"2021-07-29T05:02:24.216105Z","shell.execute_reply.started":"2021-07-29T05:02:24.209273Z","shell.execute_reply":"2021-07-29T05:02:24.215021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Pclass'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:24.65858Z","iopub.execute_input":"2021-07-29T05:02:24.65899Z","iopub.status.idle":"2021-07-29T05:02:24.665383Z","shell.execute_reply.started":"2021-07-29T05:02:24.658956Z","shell.execute_reply":"2021-07-29T05:02:24.664471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Sex'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:25.057136Z","iopub.execute_input":"2021-07-29T05:02:25.057532Z","iopub.status.idle":"2021-07-29T05:02:25.064817Z","shell.execute_reply.started":"2021-07-29T05:02:25.057497Z","shell.execute_reply":"2021-07-29T05:02:25.063499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Age columns vary from {} to {}'.format(train_df['Age'].min(),train_df['Age'].max()))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:25.384084Z","iopub.execute_input":"2021-07-29T05:02:25.384442Z","iopub.status.idle":"2021-07-29T05:02:25.39127Z","shell.execute_reply.started":"2021-07-29T05:02:25.384412Z","shell.execute_reply":"2021-07-29T05:02:25.389995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#describe our data\ntrain_df[train_df.select_dtypes(exclude='object').columns].drop('PassengerId',axis=1).describe().\\\nstyle.background_gradient(axis=1,cmap=sns.light_palette('skyblue', as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:25.751236Z","iopub.execute_input":"2021-07-29T05:02:25.751625Z","iopub.status.idle":"2021-07-29T05:02:25.809346Z","shell.execute_reply.started":"2021-07-29T05:02:25.751569Z","shell.execute_reply":"2021-07-29T05:02:25.808069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#find the null values in each column\ntrain_df.isnull().sum().to_frame().rename(columns={0:'Null values'})","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:26.340161Z","iopub.execute_input":"2021-07-29T05:02:26.340512Z","iopub.status.idle":"2021-07-29T05:02:26.35365Z","shell.execute_reply.started":"2021-07-29T05:02:26.340483Z","shell.execute_reply":"2021-07-29T05:02:26.352465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visuaize the null values in each column\nplt.figure(figsize=(20,6));\nsns.heatmap(train_df.isnull(), cmap='viridis');","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:27.326189Z","iopub.execute_input":"2021-07-29T05:02:27.326554Z","iopub.status.idle":"2021-07-29T05:02:28.045333Z","shell.execute_reply.started":"2021-07-29T05:02:27.32652Z","shell.execute_reply":"2021-07-29T05:02:28.044485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets see the correlation between columns and target column\ncorr = train_df.corr()\ncorr['Survived'].sort_values(ascending=False)[1:].to_frame()\\\n.style.background_gradient(axis=1,cmap=sns.light_palette('green', as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:28.047106Z","iopub.execute_input":"2021-07-29T05:02:28.047413Z","iopub.status.idle":"2021-07-29T05:02:28.068328Z","shell.execute_reply.started":"2021-07-29T05:02:28.047384Z","shell.execute_reply":"2021-07-29T05:02:28.067557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets take a look to the shape of columns\ntrain_df.skew().to_frame().rename(columns={0:'Skewness'}).sort_values('Skewness')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:28.160067Z","iopub.execute_input":"2021-07-29T05:02:28.160634Z","iopub.status.idle":"2021-07-29T05:02:28.178674Z","shell.execute_reply.started":"2021-07-29T05:02:28.160558Z","shell.execute_reply":"2021-07-29T05:02:28.177341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize columns have highest Skewness\nfig, axes = plt.subplots(1,3, figsize=(20, 8));\nfig.suptitle('Highest Skewness', fontsize=25);\n\nfor i,col in zip(range(3),['Fare','SibSp','Parch']):\n    sns.kdeplot(train_df[col], ax=axes[i],hue=train_df['Survived'])\n    axes[i].set_title(col+' Distribution')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:02:29.034124Z","iopub.execute_input":"2021-07-29T05:02:29.034668Z","iopub.status.idle":"2021-07-29T05:02:29.769505Z","shell.execute_reply.started":"2021-07-29T05:02:29.034635Z","shell.execute_reply":"2021-07-29T05:02:29.768429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>We have alot of null values in cabin and age columns\n<li>Survived column have a higher correlation with:\n    <ul>\n        <li>Pclass <b> -0.338481</b>\n        <li>Fare <b> 0.257307</b>\n        <li>Parch <b> 0.081629</b> \n    </ul>\n<li>We have some Columns with a high Skewness:\n    <ul>\n        <li>Fare <b> 4.7873</b>\n        <li>SibSp <b> 3.6954</b>\n    </ul>","metadata":{}},{"cell_type":"markdown","source":"# Setup and Basic EDA\n<a  id=\"head-2\"></a>","metadata":{}},{"cell_type":"markdown","source":"### Basic plotting functions","metadata":{}},{"cell_type":"code","source":"#use all data in visualization\ndf = pd.concat([train_df,test_df], axis=0)\n\n#create a new column for the total number of family (Passenger )\ndf['family count']=df['Parch']+df['SibSp']+1 \n\n#cpitalize sex column\ndf['Sex'] = df['Sex'].apply(lambda x:x.title())\n\n#create a new column based on survived column (replace 1 with survived and 0 survived non-survived)\ndf['target'] = df['Survived'].map({1:'Survived',0:'Not Survived'})\n\n#use columns with lowercases\ndf = df.rename(columns=lambda x:x.lower())","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:04:30.038116Z","iopub.execute_input":"2021-07-29T05:04:30.038552Z","iopub.status.idle":"2021-07-29T05:04:30.058274Z","shell.execute_reply.started":"2021-07-29T05:04:30.038515Z","shell.execute_reply":"2021-07-29T05:04:30.057073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets define a function to plot a bar plot easily\n\ndef bar_plot(df,x,x_title,y,title,colors=None,text=None):\n    fig = px.bar(x=x,\n                 y=y,\n                 text=text,\n                 labels={x: x_title.title()},          # replaces default labels by column name\n                 data_frame=df,\n                 color=colors,\n                 barmode='group',\n                 template=\"simple_white\",\n                 color_discrete_sequence=px.colors.qualitative.Prism)\n    \n    texts = [temp[col].values for col in y]\n    for i, t in enumerate(texts):\n        fig.data[i].text = t\n        fig.data[i].textposition = 'inside'\n        \n    fig['layout'].title=title\n\n    for trace in fig.data:\n        trace.name = trace.name.replace('_',' ').title()\n\n    fig.update_yaxes(tickprefix=\"\", showgrid=True)\n\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:04:30.586231Z","iopub.execute_input":"2021-07-29T05:04:30.586622Z","iopub.status.idle":"2021-07-29T05:04:30.59637Z","shell.execute_reply.started":"2021-07-29T05:04:30.586573Z","shell.execute_reply":"2021-07-29T05:04:30.595073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">Data Exploration and Analysis 🔍</p>","metadata":{}},{"cell_type":"markdown","source":"<a id='head-2-1'></a>\n<h1>Univariate Visualisation</h1>","metadata":{}},{"cell_type":"markdown","source":"<a id='head-2-1-1'></a>\n<h2>Categorical Features</h2>","metadata":{}},{"cell_type":"markdown","source":"<a id='sex'></a>\n### Sex column ","metadata":{}},{"cell_type":"code","source":"temp = pd.DataFrame()\n\nfor sex in pd.unique(df['sex']).tolist():\n    temp[sex] = df[df['sex']==sex]['target'].value_counts()\n    \ntemp = temp.rename(columns={0:'Female',1:'Male'}).T\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp.reset_index(),\n         'index',\n         'age',\n         ['Total sum','Survived','Not Survived'],\n         title='Survived and Not-survived grouped by sex')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:04:31.546606Z","iopub.execute_input":"2021-07-29T05:04:31.547189Z","iopub.status.idle":"2021-07-29T05:04:31.876342Z","shell.execute_reply.started":"2021-07-29T05:04:31.547153Z","shell.execute_reply":"2021-07-29T05:04:31.875435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>Most of passengers are males.\n<li>Females have a high probability of survival.\n<li>The male death rate is much higher than the female passenger’s death rate.","metadata":{}},{"cell_type":"markdown","source":"<a id='pclass'></a>\n### Pclass column ","metadata":{}},{"cell_type":"code","source":"temp = pd.DataFrame()\n\nfor p in set(pd.unique(df['pclass'])):\n    temp[p] = df[df['pclass']==p]['target'].value_counts()\n    \ntemp = temp.rename(columns={1:'Class 1',2:'Class 2', 3:'Class 3'}).T\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp.reset_index(),\n         'index',\n         'Pclass',\n         ['Total sum','Survived','Not Survived'],\n         title='Survived and Not-survived grouped by Pclass')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:04:31.97158Z","iopub.execute_input":"2021-07-29T05:04:31.973869Z","iopub.status.idle":"2021-07-29T05:04:32.081364Z","shell.execute_reply.started":"2021-07-29T05:04:31.97381Z","shell.execute_reply":"2021-07-29T05:04:32.080486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>Most of passengers were in Class 3.\n<li>The survive rate in Class-3 is the worst, and followed by class-2 and lastly, class-1.(make sense because Class-3 ticket is cheaper) and class 1 is more expenssive.\n<li>passengers in Class 1 and Class 2 have a high probability of survival.<br>\n<p>So Pclass has a strong relation with the probability of survival","metadata":{}},{"cell_type":"markdown","source":"<a id='family_count'></a>\n### Family Count column ","metadata":{}},{"cell_type":"code","source":"#before applying particular test we have to look for Contingency table\nfamily_count = pd.crosstab(index=df['family count'],columns=df['target'])\nfamily_count ","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:04:32.725919Z","iopub.execute_input":"2021-07-29T05:04:32.726508Z","iopub.status.idle":"2021-07-29T05:04:32.762388Z","shell.execute_reply.started":"2021-07-29T05:04:32.726474Z","shell.execute_reply":"2021-07-29T05:04:32.761575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = pd.crosstab(index=df['family count'],columns=df['target']).reset_index()\n\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp,\n         'family count',\n         'Family number',\n         ['Total sum','Survived','Not Survived'],\n         title='Survived and Not-survived grouped by Family Number')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:04:34.246778Z","iopub.execute_input":"2021-07-29T05:04:34.247441Z","iopub.status.idle":"2021-07-29T05:04:34.355183Z","shell.execute_reply.started":"2021-07-29T05:04:34.247399Z","shell.execute_reply":"2021-07-29T05:04:34.354002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>Solo travellers have low probability of survival.\n<li>Groupes (from 2 to 4) have a high survive rate.\n<li>As group size increases, the probability of survival decreases.","metadata":{}},{"cell_type":"markdown","source":"<a id='embarked'></a>\n### Embarked Count column ","metadata":{}},{"cell_type":"code","source":"df['embarked'].value_counts().to_frame().rename(columns={'embarked':'Total Count'})","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:05:34.911046Z","iopub.execute_input":"2021-07-29T05:05:34.911568Z","iopub.status.idle":"2021-07-29T05:05:34.922196Z","shell.execute_reply.started":"2021-07-29T05:05:34.911536Z","shell.execute_reply":"2021-07-29T05:05:34.92133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we are still using the whole data for visualizion \n#but only train_df is counted because test_df doesn't have Survived column\ntemp = pd.DataFrame()\n\nfor e in df['embarked'].unique().tolist():\n    temp[e] = df[df['embarked']==e]['target'].value_counts()\n    \ntemp = temp.T.rename(index={'S':'Southampton','C':'Cherbourg','Q':'Queenstown'})\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp.reset_index(),\n         'index',\n         'Embarked',\n         ['Total sum','Survived','Not Survived'],\n         title='Survived and Not-survived grouped by Embarked column')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:05:36.271991Z","iopub.execute_input":"2021-07-29T05:05:36.272523Z","iopub.status.idle":"2021-07-29T05:05:36.374783Z","shell.execute_reply.started":"2021-07-29T05:05:36.27249Z","shell.execute_reply":"2021-07-29T05:05:36.373799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>The graph shows that about 69.9% of the people boarded from Southampton (914/1309 = 0.698). \n<li>Just over 20.6% boarded from Cherbourg (270/1309 = 0.206) and the rest boarded from Queenstown, which is about 9.39% (123/1309 = 0.206). ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"head-2-1-2\"></a>\n<h2>Numerical Features</h2>","metadata":{}},{"cell_type":"markdown","source":"<a id='age'></a>\n### Age column ","metadata":{}},{"cell_type":"code","source":"df['age_category'] = pd.cut(df['age'].fillna(df['age'].mean()).astype(int), bins=[-1,11,18,22,27,33,40,66,100],\n                            labels=[\"<=11\",\"11-18\",\"19-22\",\"23-27\",\"28-33\",\"34-40\",\"41-66\",\">=67\"])\n\ntemp = pd.DataFrame()\nfor age in df['age_category'].unique().tolist():\n    temp[age] = df[df['age_category']==age]['target'].value_counts()\n\ntemp = temp.T.reset_index()\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp.reset_index(),\n         'index',\n         'Age Category',\n         ['Total sum','Survived','Not Survived'],\n         title='Survived and Not-survived grouped by Age column')\n\n\nfig = make_subplots(rows=2, cols=2,\n                    specs=[[{\"colspan\": 2}, None],\n                           [{}, {}]],\n                    subplot_titles=('Age distribution',\n                                    'Survived',\n                                    'Not Survived'))\n\nfig.add_trace(go.Histogram(x=df['age']),\n              row=1, col=1)\n\nfig.add_trace(go.Histogram(x=df[df['target']=='Survived']['age']),\n              row=2, col=1)\n\nfig.add_trace(go.Histogram(x=df[df['target']=='Not Survived']['age']),\n              row=2, col=2)\n\nfig.update_layout(showlegend=False, title_text='Distribution for Age')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:05:57.076Z","iopub.execute_input":"2021-07-29T05:05:57.076391Z","iopub.status.idle":"2021-07-29T05:05:57.283477Z","shell.execute_reply.started":"2021-07-29T05:05:57.076354Z","shell.execute_reply":"2021-07-29T05:05:57.282199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>Most of Passengers were between 28 and 33.\n<li>Age column is positive skewed, meaning that few Passengers were higher than 50.\n<li>The graph shows the relationship between Age and survival rate. It becomes apparent that age group between 15 and 25 has the worst survival rate.\nWith this, we could conclude that. The attribute Age has a serious quality problem: some age values are negative and large number 177 values are missing. If it is to be used as a predictor in a prediction model, it needs a lot of work in the stage of preprocess.","metadata":{}},{"cell_type":"markdown","source":"<a id='fare'></a>\n### Fare column ","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=2,\n                    specs=[[{\"colspan\": 2}, None],\n                           [{}, {}]],\n                    subplot_titles=('Fare distribution',\n                                    'Survived',\n                                    'Not Survived'))\n\nfig.add_trace(go.Histogram(x=df['fare'][:len(train_df)]),\n              row=1, col=1)\n\nfig.add_trace(go.Histogram(x=df[df['target']=='Survived']['fare'][:len(train_df)]),\n              row=2, col=1)\n\nfig.add_trace(go.Histogram(x=df[df['target']=='Not Survived']['fare'][:len(train_df)]),\n              row=2, col=2)\n\nfig.update_layout(showlegend=False, title_text='Distribution for Fare')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:06:17.246053Z","iopub.execute_input":"2021-07-29T05:06:17.246411Z","iopub.status.idle":"2021-07-29T05:06:17.318331Z","shell.execute_reply.started":"2021-07-29T05:06:17.246382Z","shell.execute_reply":"2021-07-29T05:06:17.317329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='head-2-2'></a>\n<h1>Multivariate Visualisation</h1>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sct_mtx\"></a>\n### Scatter Matrix","metadata":{}},{"cell_type":"code","source":"#create a scatter plot for the columns that have a hih correlation with target column\ntrain_df['target'] = df['target'][:len(train_df)]\nplt.figure();\nsns.set(style='whitegrid', context='talk', palette='viridis');\nsns.pairplot(data=train_df,hue='target');","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:06:19.221073Z","iopub.execute_input":"2021-07-29T05:06:19.221453Z","iopub.status.idle":"2021-07-29T05:06:35.35283Z","shell.execute_reply.started":"2021-07-29T05:06:19.221418Z","shell.execute_reply":"2021-07-29T05:06:35.351343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"corr_mtx\"></a>\n### Correlation Matrix ","metadata":{}},{"cell_type":"code","source":"#Correlation Map\ncorr = df.corr()\n\ncorr.iplot(kind='heatmap',\n           colorscale='Blues',\n           hoverinfo='all',\n           layout = go.Layout(title='Correlation Heatmap for the correlation between our columns',\n                              titlefont=dict(size=20)))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:06:35.35528Z","iopub.execute_input":"2021-07-29T05:06:35.355755Z","iopub.status.idle":"2021-07-29T05:06:35.680862Z","shell.execute_reply.started":"2021-07-29T05:06:35.355702Z","shell.execute_reply":"2021-07-29T05:06:35.679991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"multi\"></a>\n<h2>Multi-Violin and Multi-Box plots for each column</h2>","metadata":{}},{"cell_type":"code","source":"#lets create a function to plot a multi-violin easily\n\ndef multi_violin(df,iter_col,dist_col,color_col='survived'):\n    if len(df[color_col].unique())!= 2:\n        return 'Maximun number of unique values in the color columns is 2'\n    i = 0\n    data = []\n    for ite in df[iter_col]:\n        data.append(go.Violin(x=df[df[iter_col]==ite][iter_col],\n                              y=df[df[color_col]==df[color_col].unique().tolist()[0]][dist_col],\n                              name=str(df[color_col].unique().tolist()[0]),\n                              jitter=0,\n                              meanline={'visible':True},\n                              line={\"color\": '#F78181'},\n                              side='negative',\n                              marker=dict(color= '#81F781'),\n                              showlegend=(i==0)))\n\n        data.append(go.Violin(x=df[df[iter_col]==ite][iter_col],\n                              y=df[df[color_col]==df[color_col].unique().tolist()[1]][dist_col],\n                              name=str(df[color_col].unique().tolist()[1]),\n                              jitter=0,\n                              meanline={'visible':True},\n                              line={\"color\": '#00FF40'},\n                              side='positive',\n                              marker=dict(color= '#81F781'),\n                              showlegend=(i==0)))\n        i+=1\n\n\n    layout = dict(title='Distribution of {} column for each {} colored by {}'.format(dist_col.replace('_',' ').title(),\n                                                                                     iter_col.replace('_',' ').title(),\n                                                                                     color_col.replace('_',' ').title()),\n                  width=1000,height=600,\n                  yaxis=dict(title='Distribution',titlefont=dict(size=20)),\n                  xaxis=dict(title=iter_col))\n\n    iplot(dict(data=data,layout=layout))    \n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:06:35.68207Z","iopub.execute_input":"2021-07-29T05:06:35.682567Z","iopub.status.idle":"2021-07-29T05:06:35.696107Z","shell.execute_reply.started":"2021-07-29T05:06:35.68252Z","shell.execute_reply":"2021-07-29T05:06:35.694949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a function to plot multi-box plots easily\n\ndef multi_box(df,cat_col,dist_col,color_col):\n    \n    y = []\n    x = []\n    \n    if len(df[color_col].unique())!= 2:\n        return 'Maximun number of unique values in the color columns is 2'\n    \n    for c in set(df[cat_col].unique().tolist()):\n        for t in set(df[color_col].unique()):\n            y.append(df[(df[cat_col]==c) & (df[color_col]==t)][dist_col].values)\n            x.append(cat_col+'('+str(c)+')'+' ('+str(t)+')')        \n\n    colors = ['rgba(251, 43, 43, 0.5)', 'rgba(125, 251, 137, 0.5)', \n              'rgba(251, 43, 43, 0.5)', 'rgba(125, 251, 137, 0.5)', \n              'rgba(251, 43, 43, 0.5)', 'rgba(125, 251, 137, 0.5)',\n              'rgba(251, 43, 43, 0.5)', 'rgba(125, 251, 137, 0.5)', \n              'rgba(251, 43, 43, 0.5)', 'rgba(125, 251, 137, 0.5)', \n              'rgba(251, 43, 43, 0.5)', 'rgba(125, 251, 137, 0.5)']\n\n    traces = []\n\n    for xd, yd, cls in zip(x, y, colors[:2*len(df[cat_col].unique())]):\n            traces.append(go.Box(y=yd,\n                                 name=xd,\n                                 boxpoints='all',\n                                 jitter=0.5,\n                                 whiskerwidth=0.2,\n                                 fillcolor=cls,\n                                 marker=dict(size=2),\n                                 line=dict(width=1)))\n\n    layout = go.Layout(title='{} distribution colord by {} grouped by {}'.format(dist_col.title(),\n                                                                                 color_col.title(),\n                                                                                 cat_col.title()),\n        xaxis=dict(title=cat_col,\n                   titlefont=dict(size=16)),\n        \n        yaxis=dict(title='Distribution',\n                   autorange=True,\n                   showgrid=True,\n                   zeroline=True,\n                   dtick=5,\n                   gridcolor='rgb(255, 255, 255)',\n                   gridwidth=1,\n                   zerolinecolor='rgb(255, 255, 255)',\n                   zerolinewidth=2,\n                   titlefont=dict(\n                   size=16)),\n        \n        margin=dict(l=40,\n                    r=30,\n                    b=80,\n                    t=100),\n        \n        paper_bgcolor='rgb(255, 255, 255)',\n        plot_bgcolor='rgb(255, 243, 192)',\n        showlegend=False)\n\n    fig = go.Figure(data=traces, layout=layout)\n    iplot(fig)    ","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:08:32.750928Z","iopub.execute_input":"2021-07-29T05:08:32.751523Z","iopub.status.idle":"2021-07-29T05:08:32.765402Z","shell.execute_reply.started":"2021-07-29T05:08:32.751474Z","shell.execute_reply":"2021-07-29T05:08:32.764437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Age distribution for Pclass column","metadata":{}},{"cell_type":"code","source":"multi_box(df.dropna(),'pclass','age','target')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:08:34.463629Z","iopub.execute_input":"2021-07-29T05:08:34.464151Z","iopub.status.idle":"2021-07-29T05:08:34.525992Z","shell.execute_reply.started":"2021-07-29T05:08:34.464119Z","shell.execute_reply":"2021-07-29T05:08:34.525267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_violin(df=df.dropna(),iter_col='pclass',dist_col='age',color_col='target')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:08:43.551843Z","iopub.execute_input":"2021-07-29T05:08:43.552355Z","iopub.status.idle":"2021-07-29T05:08:45.091956Z","shell.execute_reply.started":"2021-07-29T05:08:43.552324Z","shell.execute_reply":"2021-07-29T05:08:45.090894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>In any class the age distribution of survived passebgers is right skewed, meaning that most of survived passengers in each class were younger.","metadata":{}},{"cell_type":"code","source":"multi_box(df.dropna(),'sex','age','target')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:10:50.781172Z","iopub.execute_input":"2021-07-29T05:10:50.781567Z","iopub.status.idle":"2021-07-29T05:10:50.839082Z","shell.execute_reply.started":"2021-07-29T05:10:50.781535Z","shell.execute_reply":"2021-07-29T05:10:50.83821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_violin(df=df.dropna(),iter_col='sex',dist_col='age',color_col='target')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T05:11:01.920971Z","iopub.execute_input":"2021-07-29T05:11:01.921488Z","iopub.status.idle":"2021-07-29T05:11:03.864395Z","shell.execute_reply.started":"2021-07-29T05:11:01.921456Z","shell.execute_reply":"2021-07-29T05:11:03.8633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Phase II\"></a>\n<a class=\"anchor\" id=\"head-1\"></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 100px;\">Phase II</p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"prep_ml\"></a>\n<h2>Prepare the data for the machine learning model</h2>","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">Data Cleaning🔧</p>","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\ntest_PassengerId = test['PassengerId'].values #for submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#extracting the unique titles\ntitle_list = pd.concat([train,test])['Name'].apply(lambda x: re.findall(r'[, ]\\w+[.]',x)[0][:-1]).unique()\n                                           \n# Using this iteratively I was able to get a full list of titles.\ntitle_list = ['Mr', 'Mrs', 'Miss', 'Master', 'Don', 'Rev', 'Dr', 'Mme', 'Ms','Major', \n             'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'Countess','Jonkheer', 'Dona']\n\n \n# replacing all titles with mr, mrs, miss, master, and boy \ndef replace_titles(x):\n    \n    title=x['Title'].strip()\n    if (x['Age']<13):\n        return 'Boy'\n    if title in ['Don', 'Rev', 'Col','Capt','Sir','Major','Jonkheer']:\n        return 'Mr'\n    elif title in ['Countess', 'Mme']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms','Lady','Dona']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title\n    \n#create a new columns containing the title for each name\ntrain['Title'] = train['Name'].apply(lambda x: re.findall(r'[, ]\\w+[.]',x)[0][:-1])\ntest['Title'] = test['Name'].apply(lambda x: re.findall(r'[, ]\\w+[.]',x)[0][:-1])\n\n# apply replacing title function to all titles\ntrain['Title'] = train.apply(replace_titles, axis=1)\ntest['Title'] = test.apply(replace_titles, axis=1)\n\n#delete name column,PassengerId,Ticket\ndel train['Name']\ndel test['Name']\n\n# Since the Ticket attribute has 681 unique tickets, it will be a bit tricky to convert them into useful categories. \n# So we will drop it from the dataset.\ndel train['Ticket']\ndel test['Ticket']\n\n#drop PassengerId column\ndel train['PassengerId']\ndel test['PassengerId']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Train data has : {train[\"Title\"].unique()}'),\nprint(f'Test data has : {test[\"Title\"].unique()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Null values\n<p>Lets impute the missing values using <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html\">KNNImputer </a>from sklearn<br>\n<p>To impute Cabin and Embarked (Categorical) columns you have to encode the strings to numerical values. In the below code snippet I am using ordinal encoding method to encode the categorical variables in my training data and then imputing using KNN.\n<p>You can see <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\">Basic Feature Engineering with the Titanic Data</a> ","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputed_data = []                    #initialize a list for the imputed datasets\n\nfor df in [train,test]:\n    \n    imputed_df = df.copy()           #initialize the imputed Dataframe\n    categorical = ['Pclass','Sex','SibSp','Parch','Cabin','Embarked','Title'] #define the categorical data\n    numerical = ['Age','Fare']                                                #define the numerical data\n    \n    #encoding Cabin and Embarked columns to numeric values\n    imputed_df[['Cabin','Embarked','Sex','Title']] = imputed_df[['Cabin','Embarked','Sex','Title']].\\\n    apply(lambda series: pd.Series(\n        LabelEncoder().fit_transform(\n            series[series.notnull()]),\n        index=series[series.notnull()].index))\n\n    #define an imputer for numerical columns\n    imp_num = IterativeImputer(estimator=RandomForestRegressor(),\n                               initial_strategy='median',\n                               max_iter=10,\n                               random_state=0)\n\n    #define an imputer for categorical columns\n    imp_cat = IterativeImputer(estimator=XGBClassifier(verbosity = 0),\n                               max_iter=10,\n                               initial_strategy='most_frequent',\n                               random_state=0)\n    \n    #impute the numerical column(Age)\n    imputed_df[numerical] = imp_num.fit_transform(imputed_df[numerical])\n    \n    #impute the categorical columns(Embarked,Cabin)\n    imputed_df[categorical] = imp_cat.fit_transform(imputed_df[categorical])\n\n    #return the imputed value to its string values (Decoding) \n    for col in ['Cabin','Embarked','Sex','Title']:\n        imputed_df[col] = LabelEncoder().fit(df[col]).inverse_transform(imputed_df[col].astype(int))\n    \n    imputed_df['Age'] = imputed_df['Age'].apply(lambda x:int(round(x,0)))\n    \n    imputed_data.append(imputed_df)\n    \ntrain_df = imputed_data[0]  \ntest_df = imputed_data[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adding new features","metadata":{}},{"cell_type":"code","source":"for dataset in [train_df,test_df]:\n    \n    dataset.columns = [x.lower() for x in dataset.columns]\n    \n    \n    #lets create age category\n    dataset['age_category'] = pd.cut(dataset['age'].astype(int), bins=[-1,11,18,22,27,33,40,66,100],\n                                labels=[1,2,3,4,5,6,7,8]).to_frame()\n    \n    #create a new column for the total number of family (Passenger )\n    dataset['family count'] = dataset['parch']+dataset['sibsp']+1 \n    dataset['family count'] = dataset['family count'].astype(int)\n    \n    # Age times Class\n    dataset['age_class'] = dataset['age_category']* dataset['pclass']\n    dataset['age_class'] = dataset['age_class'].astype(int) \n    \n    # Fare per Person\n    dataset['fare_per_person'] = dataset['fare']/(dataset['family count'])\n    dataset['fare_per_person'] = round(dataset['fare_per_person'].astype(float), 0).astype(int)\n    \n    # Is alone\n    dataset['is_alone'] = 0\n    dataset.loc[dataset['family count'] == 1, 'is_alone'] = 1\n    \n    #convert pclass, sibsp and parch columns to int\n    dataset[['pclass','sibsp','parch']] = dataset[['pclass','sibsp','parch']].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Preprocessing","metadata":{}},{"cell_type":"code","source":"train_set = train_df\nX_set = train_set.drop('survived', axis=1)\ny_set = train_set['survived']\ntest_set  = test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first choose columns to train the model with\nscaled_data = []\nnum_features = ['age','fare']\ncat_features = ['age_category','title','sex','embarked','is_alone','age_class','cabin','pclass','sibsp']\n\ndf = pd.concat([X_set,test_set])\n\n# convert our categorical columns to dummies instead of LabelEncoding\nfor col in cat_features:\n    dumm = pd.get_dummies(df[col], prefix = col,dtype=int)\n    del df[col]\n    df = pd.concat([df,dumm], axis=1)\n\nX_set = df[:len(X_set)]\ntest_set = df[len(X_set):]\n\nfor data in [X_set,test_set]:\n    # scaling  our numeric columns\n    std = StandardScaler()\n    for col in num_features:\n        data[col] = pd.Series(std.fit_transform(data[col].values.reshape(-1,1)).reshape(-1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ml_models\"></a>\n<h2>Comparing different Machine learning models</h2>","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">Initializing our models📝</p>","metadata":{}},{"cell_type":"code","source":"#split the data\nX_train, X_test, y_train, y_test = train_test_split(X_set, y_set, \n                                                    test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define models to test:\nbase_models = [(\"SVM\",      SVC()),                                                               #Support Vector Machines\n               (\"kNN\",      KNeighborsClassifier(n_neighbors = 3)),                               #KNeighborsClassifier\n               (\"LR_model\", LogisticRegression(random_state=42,n_jobs=-1)),                       #Logistic Regression model\n               (\"DT_model\", DecisionTreeClassifier(random_state=42)),                             #Decision tree model\n               (\"RF_model\", RandomForestClassifier(random_state=42, n_jobs=-1)),                  #Random Forest model\n               (\"XGBoost\", XGBClassifier()),                                                      #XGBoost model\n               (\"Bagging_model\",BaggingClassifier(base_estimator=DecisionTreeClassifier(),        #Bagging model\n                                                 max_samples=30,\n                                                 n_estimators=500,\n                                                 n_jobs=-1,\n                                                 bootstrap=True,\n                                                 oob_score=True)),\n               (\"Random_subspaces_model\",BaggingClassifier(base_estimator=DecisionTreeClassifier(),#Random subspaces model\n                                                           n_estimators=100,\n                                                           bootstrap=False,\n                                                           max_samples=1.0,\n                                                           max_features=True,\n                                                           bootstrap_features=True,\n                                                           n_jobs=-1)),\n                (\"Random_Patches_model\", BaggingClassifier(base_estimator=DecisionTreeClassifier(),#Random Patches model\n                                                            n_estimators=100,\n                                                            bootstrap=True,\n                                                            max_samples=1.0,\n                                                            max_features=True,\n                                                            bootstrap_features=True,\n                                                            n_jobs=-1)),\n                (\"AdaBoost_model\",AdaBoostClassifier(DecisionTreeClassifier(),                      #AdaBoost model\n                                                    n_estimators=100,\n                                                    learning_rate=0.01)),\n                (\"GradientBoosting\",GradientBoostingClassifier(max_depth=2,                        #GradientBoosting model\n                                                              n_estimators=100))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>To evaluate the performance of any machine learning model we need to test it on some unseen data, based on the models \nperformance on unseen data we can say weather our model is :\n    <ul>\n        <li>Under-fitting.\n        <li>Over-fitting.\n        <li>Well generalized.\n    </ul>\n<b>Cross validation (CV)</b> is one of the technique used to test the effectiveness of a machine learning models, it is also a re-sampling procedure used to evaluate a model if we have a limited data.<br>\nTo perform CV we need to keep aside a sample/portion of the data on which is not used to train the model, later use this sample for testing/validating.<br>\nSo, k-fold cross validation is used for two main purposes:\n<ul>\n    <li>To tune hyper parameters.\n    <li>To better evaluate the performance of a model.","metadata":{}},{"cell_type":"markdown","source":"<h3><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\">BaggingClassifier</a>:</h3><br>\n<li>Bagging and Pasting have the same algorithm, each model is trained using subsets but the way you chose the subsets changes:\n<ul>\n    <li>In Bagging you do sampling with replacement.\n    <li>In Pasting you do sampling without replacement.\n</ul>\nSo two algorithms allow training instances to be sampled several times accroce multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor.\n<li>In Bagging we control sampling by <b>(max_sample,Bootstrap).</b>\n    Random subspaces and Random Patches are extension to Bagging algorithm, but they do feature sampling instead of instances sampling, they are useful when we have a high dimensions inputs.<br>\n<ul>\n    <li>In Random subspaces you do sampling to both training and features.\n    <li>In Random Patches you keep all training instances <b>(Bootstrap=False, max_sample=1)</b>, but you do sampling feature by <b>(Bootstrap_feature=True)</b>.\n</ul>\n<h3><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\">AdaBoostClassifier</a>:</h3><br>\n<li>AdaBoost Combines a lot of week learners to make Classification <b>(week learners like stump in DecisionTree)</b>","metadata":{}},{"cell_type":"code","source":"kfolds = 5   # it is better to be 1/(size of testing test)\nsplit = StratifiedKFold(n_splits=kfolds,\n                        shuffle=True, \n                        random_state=42)  # use shuffle to ensure random distribution of data\n\n# Preprocessing, fitting, making predictions and scoring for every model:\nmodels_data = {'min_score':{},'max_score':{},'mean_score':{},'std_dev':{}}\nfor name, model in base_models:\n    # get cross validation score for each model:\n    cv_results = cross_val_score(model, \n                                 X_set, y_set, \n                                 cv=split,\n                                 scoring=\"accuracy\",\n                                 n_jobs=-1)\n    \n    # output:\n    #To find the average of all the accuracies.\n    min_score = round(min(cv_results)*100, 4)\n    models_data['min_score'][name] = min_score\n     \n    #To find the max accuracy of all the accuracies.\n    max_score = round(max(cv_results)*100, 4)\n    models_data['max_score'][name] = max_score\n    \n    #To find the min accuracy of all the accuracies.\n    mean_score = round(np.mean(cv_results)*100, 4)\n    models_data['mean_score'][name] = mean_score\n    \n    # let's find the standard deviation of the data to see degree of variance in the results obtained by our model.\n    std_dev = round(np.std(cv_results), 4)\n    models_data['std_dev'][name] = std_dev\n    \n    print(f\"{name} cross validation accuarcy score: {mean_score} +/- {std_dev} (std) ---> min: {min_score}, max: {max_score}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_df = pd.DataFrame(models_data).sort_values(by='mean_score',ascending=False)\nmodels_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'Mininam, Maximam and Mean score for each model'\nmodels_df.iplot(kind='bar',\n               title=title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets train our models with all test sets \naccuracies = {}\nmodels = {}\nmodel = base_models\nfor name,model in base_models:\n    model.fit(X_train, y_train)\n    models[name]=model\n    acc = model.score(X_test, y_test)*100\n    accuracies[name] = acc\n    print(\"{} Accuracy Score : {:.3f}%\".format(name,acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_res = pd.DataFrame(data=accuracies.items())\nmodels_res.columns = ['Model','Test score']\nmodels_res.sort_values('Test score',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model_df = models_df.join(models_res.set_index('Model'))\nnew_model_df['(Test Score - Cross_Validation Score)%'] = new_model_df['Test score'] - new_model_df['mean_score']\nnew_model_df.sort_values('(Test Score - Cross_Validation Score)%',ascending=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>We found that <b>GradientBoosting</b> with K-Fold Cross Validation reached a good mean accracy and a low value of standard deviation. This values of std is extremely low, which means that our model has a very low variance, which is actually very good since that means that the prediction that we obtained on one test set is not by chance. Also, the model performed good on all test sets. which mean that our model has no overfitting.","metadata":{}},{"cell_type":"code","source":"Y_pred = models['GradientBoosting'].fit(X_set, y_set).predict(test_set)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_PassengerId,\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission_GB.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <h1 align=\"center\">Thanks for reading</h1>\n<h2 align=\"center\" style='color:red' > If you like the notebook or learned something please upvote! </h2>\n<b><li>You can also see:</li></b>\n<ul>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/house-price-prediction-top-8'>House price prediction (Top 8%)</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/prediction-of-heart-disease-machine-learning'>Prediction of Heart Disease (Machine Learning)</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/data-exploration-and-visualization-uber-data'>Data exploration and visualization(Uber Data)</a><br>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/hotel-booking-eda-cufflinks-and-plotly'>Hotel booking EDA (Cufflinks and plotly)\n</a><br>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/suicide-rates-visualization-and-geographic-maps/edit/run/53135916'>Suicide Rates visualization and Geographic maps</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/superstore-data-analysis-with-plotly-clustering'>Superstore Data Analysis With Plotly(Clustering)</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/superstore-analysis-with-cufflinks-and-pandas'>Superstore Analysis With Cufflinks and pandas</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/learn-data-analysis-using-sql-and-pandas'>Learn Data Analysis using SQL and Pandas</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/european-soccer-database-with-sqlite3'>European soccer database with sqlite3</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/chinook-questions-with-sqlite'>Chinook data questions with sqlite3</a>\n\n</ul>","metadata":{}}]}