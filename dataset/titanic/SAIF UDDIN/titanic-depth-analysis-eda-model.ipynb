{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this notebook...\n\nThe Objective of this notebook is to give an idea how is the workflow in any predictive modeling problem. How do we check features, how do we add new features and some Machine Learning Concepts. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it.\n\nYou can check out my previous work related to it in some other aproach.[click here](https://www.kaggle.com/saife245/titanic-deep-learning-model-with-80-accuracy)\n\n\n## <font color='red'>If You Like the notebook and think that it helped you PLEASE UPVOTE.</font>;)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Introduction...\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. That's why the name DieTanic. This is a very unforgetable disaster that no one in the world can forget.\n\nIt took about $7.5 million to build the Titanic and it sunk under the ocean due to collision. The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle.\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Topic will be convered in build the Machine Learning Model..\n\nA) Introduction\n\nB) Load the data\n\nC) Analyze the dataset\n\nD) Feature engineering \n\nE) Filling missing Values\n\nF) Modeling\n\nG) Prediction\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## First understand the Problem statment...\n\nIn this challenge, we have to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing library\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# About the dataset..¶\nThe data has been split into two groups:\n\ntraining set (train.csv) test set (test.csv) The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Data Defination**\n\n*Variable Definition Key*\n\n* survival Survival 0 = No, 1 = Yes\n\n* pclass Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n\n* sex Male or Female\n\n* Age Age in years\n\n* sibsp # of siblings / spouses aboard the Titanic\n\n* parch # of parents / children aboard the Titanic\n\n* ticket Ticket number\n\n* fare Passenger fare\n\n* cabin Cabin number\n\n* embarked Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\n* Variable Notes pclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\n\n* age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n* sibsp: The dataset defines family relations in this way...\n\n* Sibling = brother, sister, stepbrother, stepsister\n\n* Spouse = husband, wife (mistresses and fiancés were ignored)\n\n* parch: The dataset defines family relations in this way...\n\n* Parent = mother, father\n\n* Child = daughter, son, stepdaughter, stepson\n\n* Some children travelled only with a nanny, therefore parch=0 for them","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyze the dataset and see the feature effect the Survival","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#First check out the survival rate\ntrain['Survived'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['Survived'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pclass\n\nClass played a critical role in survival, as the survival rate decreased drastically for the lowest class. This variable is both useful and clean, and I will be treating it as a categorical variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Survived'].groupby(train['Pclass']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['Pclass'], hue=train['Survived'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SEX**\n\nAnother Important Feature is ** SEX**.Sex matter lot for deciding the survival rate. Women and childern has greater chance of survial than men.let compare the survival rate with sex.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Sex'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Survival rate w.r.t. sex\ntrain['Survived'].groupby(train['Sex']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['Sex'], hue=train['Survived'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Age**\n\nAs sex affect the survival rate. Age also matter a lot for deciding the survival rate.Before comparing the survival with Age, we have to handle the missing value of age columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Survived'].groupby(pd.qcut(train['Age'],5)).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.qcut(train['Age'],5).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['Age'], hue=train['Survived'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sibsp** ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Survived'].groupby(train['SibSp']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['SibSp'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['SibSp'], hue=train['Survived'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parch...\n\npassengers with zero parents or children had a lower likelihood of survival than otherwise, but that survival rate was only slightly less than the overall population survival rate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Survived'].groupby(train['Parch']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Parch'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['Parch'], hue=train['Survived'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ticket..\n\nNot all factor directly contribute in deciding factor of survival.Just like Ticket column,but it may help in deciding other factor which may effect the survival like ticket column help in decid type of ticket each survivar has which help to decide the location of people in it. It can be done with the help of number of characters in the Ticket column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Ticket'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Ticket_Length'] = train['Ticket'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Ticket_Length'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Another important feature is of using ticket first letter as room number\ntrain['Ticket_Letter'] = train['Ticket'].apply(lambda x: str(x)[0])\ntrain['Ticket_Letter'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(['Ticket_Letter'])['Survived'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['Ticket_Letter'], hue=train['Survived'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fare...\n\nNow looking closely you can figure out that Fare is closely related to Ticket, room, class and survival.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.qcut(train['Fare'], 5).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Survived'].groupby(pd.qcut(train['Fare'], 5)).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(pd.qcut(train['Fare'],10), columns=train['Pclass'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['Fare'], hue=train['Survived'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Embarked**\n\nLooks like the Cherbourg people had a 20% higher survival rate than the other embarking locations. This is very likely due to the high presence of upper-class passengers from that location.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Embarked'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Embarked'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Embarked'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['Embarked'], hue=train['Survived'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['Embarked'], hue=train['Pclass'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Name**\n\nI am not very clear about the contribution of name in survival rate. let's try to extract some information from Name.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Name'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Name_Title'] = train['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\ntrain['Name_Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Survived'].groupby(train['Name_Title']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's see any correlation between name lenth and survival\ntrain['Name_Length'] = train['Name'].apply(lambda x: len(x))\ntrain['Survived'].groupby(pd.qcut(train['Name_Length'],5)).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.qcut(train['Name_Length'],5).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['Name_Length'], hue=train['Survived'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['Name_Title'], hue=train['Survived'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.pairplot(data=train, hue='Survived', palette = 'seismic',\n                 size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\n\n*Now wwe have clear idea of dependencies of different fetarue on survival of a person. Let's apply **Feature Engineering** to extract some more features.*\n\nFeature engineering is the art of converting raw data into useful features. There are several feature engineering techniques that you can apply to be an artist.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#first handel the Age \ndef age_impute(train, test):\n    for i in [train, test]:\n        i['Age_Null_Flag'] = i['Age'].apply(lambda x: 1 if pd.isnull(x) else 0)\n        data = train.groupby(['Name_Title', 'Pclass'])['Age']\n        i['Age'] = data.transform(lambda x: x.fillna(x.mean()))\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see above **sibsp** and **Parch** are weak feature to decide the survival but with the help of this we can generate the new feature which can effect the survival more.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def family_size(train, test):\n    for i in [train, test]:\n        i['Family_Size'] = np.where((i['SibSp']+i['Parch']) == 0 , 'Solo',\n                           np.where((i['SibSp']+i['Parch']) <= 3,'Nuclear', 'Big'))\n        del i['SibSp']\n        del i['Parch']\n    return train, test\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Ticket column is used to create two new columns: Ticket_Letter, which indicates the first letter of each ticket (with the smaller-n values being grouped based on survival rate); and Ticket_Length, which indicates the length of the Ticket field.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def ticket_grouped(train, test):\n    for i in [train, test]:\n        i['Ticket_Letter'] = i['Ticket'].apply(lambda x: str(x)[0])\n        i['Ticket_Letter'] = i['Ticket_Letter'].apply(lambda x: str(x))\n        i['Ticket_Letter'] = np.where((i['Ticket_Letter']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']), i['Ticket_Letter'],\n                                   np.where((i['Ticket_Letter']).isin(['W', '4', '7', '6', 'L', '5', '8']),\n                                            'Low_ticket', 'Other_ticket'))\n        i['Ticket_Length'] = i['Ticket'].apply(lambda x: len(x))\n        del i['Ticket']\n    return train, test\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This first function creates two separate columns: a numeric column indicating the length of a passenger's Name field, and a categorical column that extracts the passenger's title.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def names(train, test):\n    for i in [train, test]:\n        i['Name_Length'] = i['Name'].apply(lambda x: len(x))\n        i['Name_Title'] = i['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\n        del i['Name']\n    return train, test\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Handling Missing Value**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We fill the null values in the Embarked column with the most commonly occuring value, which is 'S.'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def embarked_impute(train, test):\n    for i in [train, test]:\n        i['Embarked'] = i['Embarked'].fillna('S')\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Handling the missing value of fare columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Fare'].fillna(train['Fare'].mean(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**One-Hot-Encoding**\n\nCreating the dummies value for the character value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def dummies(train, test, columns = ['Pclass', 'Sex', 'Embarked', 'Ticket_Letter', 'Name_Title', 'Family_Size']):\n    for column in columns:\n        train[column] = train[column].apply(lambda x: str(x))\n        test[column] = test[column].apply(lambda x: str(x))\n        good_cols = [column+'_'+i for i in train[column].unique() if i in test[column].unique()]\n        train = pd.concat((train, pd.get_dummies(train[column], prefix = column)[good_cols]), axis = 1)\n        test = pd.concat((test, pd.get_dummies(test[column], prefix = column)[good_cols]), axis = 1)\n        del train[column]\n        del test[column]\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Droping few column which doesn't useful for prediction**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop(train, test, bye = ['PassengerId']):\n    for i in [train, test]:\n        for z in bye:\n            del i[z]\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = names(train, test)\ntrain, test = age_impute(train, test)\ntrain, test = embarked_impute(train, test)\ntrain, test = family_size(train, test)\ntrain, test = ticket_grouped(train, test)\ntrain, test = dummies(train, test, columns = ['Pclass', 'Sex', 'Embarked', 'Ticket_Letter','Name_Title', 'Family_Size'])\ntrain, test = drop(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the correlation between features and survival\nsns.heatmap(train.corr(),annot=True,linewidths=0.2) \nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpreting The Heatmap The first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\nPOSITIVE CORRELATION: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\nNEGATIVE CORRELATION: If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as MultiColinearity as both of them contains almost the same information.\n\nSo do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can put the cabin column in dataset but there is lot of missing value in it which may reduce the model performance.It give accuracy of 82%.so we drop the Cabin column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_column = ['Cabin']\ntrain.drop(drop_column, axis=1, inplace = True)\ntest.drop(drop_column, axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Final Check the train dataset\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Final Check for Test dataset\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying the ML Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nclassifier_svc = SVC(kernel = 'rbf',random_state = 0)\nclassifier_svc.fit(train.iloc[:, 1:], train.iloc[:, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classifier_svc.score(train.iloc[:, 1:], train.iloc[:, 0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(criterion='gini', \n                             n_estimators=700,\n                             min_samples_split=10,\n                             min_samples_leaf=1,\n                             max_features='auto',\n                             oob_score=True,\n                             random_state=1,\n                             n_jobs=-1)\n\nclassifier.fit(train.iloc[:, 1:], train.iloc[:, 0])\n\nprint(\"%.4f\" % classifier.oob_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classifier.score(train.iloc[:, 1:], train.iloc[:, 0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nscores = model_selection.cross_val_score(classifier, train.iloc[:, 1:], train.iloc[:, 0], scoring = 'accuracy', cv = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_forest = classifier.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pd.DataFrame(prediction_forest, columns=['Survived'])\ntest = pd.read_csv(os.path.join('../input', 'test.csv'))\npredictions = pd.concat((test.iloc[:, 0], predictions), axis = 1)\npredictions.to_csv('output.csv', sep=\",\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions['Survived'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# End notes...\n\n## <font color='blue'>I hope you find this kernel useful and enjoyable.Please upvote it.</font>\n\n### Your comments and feedback are most welcome.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}