{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Titanic Survival Prediction\n\n1. [Import Libraries](#heading1)\n2. [Read Data](#heading2)\n3. [Data Cleaning & Feature Engineering](#heading3)\n4. [Exploratory Data Analysis](#heading4)\n5. [Model Building & Evaluation](#heading5)\n  * [Logistic Regression](#subheading1)\n  * [Gaussian Naive Bayes](#subheading2)\n  * [Linear Discriminant Analysis (LDA)](#subheading3)\n  * [k Nearest Neighbors (kNN)](#subheading4)\n  * [Support Vector Machine (SVM)](#subheading5)\n  * [Decision Tree](#subheading6)\n  * [Random Forest](#subheading7)\n  * [XGBoost](#subheading8)\n  * [Model Stacking](#subheading9)\n  * [Result Comparison](#subheading10)\n6. [Conclusion](#heading6)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Import Libraries <a id=\"heading1\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set seed value for reproducing the same results\nseed = 101","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Read Data <a id=\"heading2\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train data preview\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test data preview\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the 'Survived' column is missing in the test set. We have to predict that label for each passenger in the test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of train data\ntrain_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of test data\ntest_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Cleaning & Feature Engineering <a id=\"heading3\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train data descriptive statistics\ntrain_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test data descriptive statistics\ntest_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For both train and test datasets, the statistics for 'Fare' column seem a bit strange. The minimum fare is 0 and the maximum is around 512, with 75% of values less than 31.5 and the mean being 35.6. We need to analyze this further to see if there are any outliers.\n\nFor this purpose, we can make use of a boxplot. It will help us understand the variation in the 'Fare' values by visually displaying the distribution of the data points.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(7, 5))\nplt.boxplot(train_data['Fare'])\nplt.title('Boxplot of Fare')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like there are a few extreme data points. Let's explore this further.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieve rows with Fare greater than 500\ntrain_data[train_data['Fare']>500]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since all of the passengers have the same ticket number, we can conclude that the fare was calculated for the entire group and not each individual. Hence, we will not discard these rows.\n\nTo standardize the fare calculation across all passengers in the dataset, the obvious step would be to divide fare by the number of people on the same ticket and get the individual fare. But factors such as reduced fares for children, missing values, etc., will further complicate things. Therefore, we will leave it as it is. For an in-depth understanding of the titanic dataset (particularly fare calculation), you can explore [Encyclopedia Titanica](https://www.encyclopedia-titanica.org/).\n\nBefore we proceed further, we also need to analyze passengers who had 0 fare.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieve rows with Fare equal to 0\ntrain_data[train_data['Fare']==0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, it looks like there are no data errors; just some passengers who got a free ride for whatever reason (visit Encyclopedia Titanica if you're interested to find out why).\n\nNext, we will check for missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing values in each column in train data\ntrain_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing values in each column in test data\ntest_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, let's deal with the missing 'Age' values. For that purpose, we will first extract title of each passenger from their name.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to extract title from passenger's name\ndef extract_title(df):\n    title = df['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())\n    return title","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count of each title in train data\ntrain_data['Title'] = extract_title(train_data)\ntrain_data['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count of each title in test data\ntest_data['Title'] = extract_title(test_data)\ntest_data['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are many titles with very few counts, we will map them to main categories (titles that are more frequently occurring).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to map titles to main categories\ndef map_title(df):\n    title_category = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Sir\": \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\": \"Royalty\",\n    \"Dona\": \"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\": \"Mr\",\n    \"Mrs\": \"Mrs\",\n    \"Miss\": \"Miss\",\n    \"Master\": \"Master\",\n    \"Lady\": \"Royalty\"\n    }\n    new_title = df['Title'].map(title_category)\n    return new_title","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count of each title in train data after mapping\ntrain_data['Title'] = map_title(train_data)\ntrain_data['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count of each title in test data after mapping\ntest_data['Title'] = map_title(test_data)\ntest_data['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have extracted titles from names, we can group data by title and impute missing age values using the median age of each category. We will also group by 'Pclass' as it will help in accurately calculating the median age within each class.\n\nNote: We are using median value instead of mean because extreme values (or outliers) have a lot more impact on mean than median.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group train data by 'Pclass', 'Title' and calculate the median age\ntrain_data.groupby(['Pclass', 'Title']).median()['Age']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One thing to note here is that unlike the 'Master' title, there is no separate category for young female passengers. If we go back and look at the original dataset, we will realize that the 'Miss' title includes both young and adult females. We can somewhat solve this by identifying passengers with 'Miss' title having 1 or 2 value in the 'Parch' column. This way we can retrieve passengers who are most likely, young females (there's also a small chance that the retrieved passenger is a female adult because the 'Parch' column not only reveals the number of parents but also the number of children).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to identify passengers who have the title 'Miss' and, 1 or 2 value in the 'Parch' column\ndef is_young(df):\n    young = []\n    for index, value in df['Parch'].items():\n        if ((df.loc[index, 'Title'] == 'Miss') and (value == 1 or value == 2)):\n            young.append(1)\n        else:\n            young.append(0)\n    return young","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group train data by 'Pclass', 'Title', 'Is_Young(Miss)' and calculate the median age\ntrain_data['Is_Young(Miss)'] = is_young(train_data)\ngrouped_age = train_data.groupby(['Pclass', 'Title', 'Is_Young(Miss)']).median()['Age']\ngrouped_age","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks better as we can now guess the missing age values more accurately than before. We will apply this function to the test data as well.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['Is_Young(Miss)'] = is_young(test_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will impute the missing age values according to the grouped data shown above.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill missing age values in train and test data\ntrain_data.set_index(['Pclass', 'Title', 'Is_Young(Miss)'], drop=False, inplace=True)\ntrain_data['Age'].fillna(grouped_age, inplace=True)\ntrain_data.reset_index(drop=True, inplace=True)\ntest_data.set_index(['Pclass', 'Title', 'Is_Young(Miss)'], drop=False, inplace=True)\ntest_data['Age'].fillna(grouped_age, inplace=True)\ntest_data.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A very important thing that needs to be addressed is that I've only used the train data to calculate the median ages for replacing missing values in both train and test datasets. Many people, especially those participating in data science competitions, use test data as well for preprocessing purposes. This may help people improve their model's test accuracy and rank higher in competitions, but it is considered a major mistake in real world applications (known as **data leakage**). Models built using this approach do not generalize too well to the new/unseen data and give results that are a lot poorer than expected. Hence, test data should never be used for data preprocessing and should only be used for testing purposes.\n\nFor replacing the missing 'Fare' value in test data, we will simply group the train data by 'Pclass' and repeat the same steps as above.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group train data by 'Pclass' and calculate the median fare\ngrouped_fare = train_data.groupby('Pclass').median()['Fare']\ngrouped_fare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill the missing fare value in test data\ntest_data.set_index('Pclass', drop=False, inplace=True)\ntest_data['Fare'].fillna(grouped_fare, inplace=True)\ntest_data.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will drop all of the unnecessary rows and columns:\n* Name: We've extracted the information that we needed (i.e. Title) and don't need this column anymore\n* Cabin: Majority of the values are missing so we will drop the entire column\n* Embarked: Only 2 values are missing in train data so we can just remove those 2 entire rows\n* Ticket: Doesn't seem to provide any useful information so we will drop the entire column\n* Is_Young(Miss): Purpose of creating this column has been fulfilled and we don't need it anymore","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop unnecessary rows and columns\ntrain_data.drop(columns=['Name', 'Cabin', 'Ticket', 'Is_Young(Miss)'], inplace=True)\ntest_data.drop(columns=['Name', 'Cabin', 'Ticket', 'Is_Young(Miss)'], inplace=True)\ntrain_data.dropna(subset=['Embarked'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is always good to verify that there are no remaining missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values in train data after data cleaning\ntrain_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values in test data after data cleaning\ntest_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Exploratory Data Analysis <a id=\"heading4\"></a>\n\nIn this section, we will try to find some interesting insights using visual methods.\n\nFirst, we will look at the class distribtuion.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(7, 5))\nsns.countplot(x='Survived', data=train_data)\nplt.title('Class Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that the classes are slightly imbalanced since majority of the passengers did not survive. In scenarios like this, the same ratio is expected in test data so we don't need to worry about the imbalanced classes.\n\nNext, let's find out the ratio of survivors with respect to other variables (i.e. 'Sex', 'Pclass', 'Embarked', 'Title').","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(7, 5))\nsns.barplot(x='Sex', y='Survived', data=train_data, ci=None)\nplt.title('Ratio of survivors based on sex')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(7, 5))\nsns.barplot(x='Pclass', y='Survived', data=train_data, ci=None)\nplt.title('Ratio of survivors based on ticket class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(7, 5))\nsns.barplot(x='Embarked', y='Survived', data=train_data, ci=None)\nplt.title('Ratio of survivors based on port of embarkation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(7, 5))\nsns.barplot(x='Title', y='Survived', data=train_data, ci=None)\nplt.title('Ratio of survivors based on title')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on these visualizations, we can conclude the following:\n* Females had a way higher survival rate than males\n* Lower ticket class (with 3 being the lowest) means less chance of survival\n* Passengers who embarked from port 'C' had slightly more chances of survival\n* Passengers with the title 'Mr' and 'Officer' had really low chances of survival as compared to other passengers\n\nNote: The accuracy of these findings also depends on other factors such as the frequency distribution within each categorical variable. For example, if there is only 1 female in the entire dataset and she survived, then the survival rate of females will be 100% which cannot be considered a concrete finding. Hence, depending on the type of problem being solved, further data analysis should be done if required.\n\nNext, we will compute the pairwise correlation of different variables, focusing mainly on how different features correlate with the target variable 'Survived'. But first, we need to convert all of the categorical variables into numeric data type.\n\nTo convert 'Sex' variable into numeric format, we will simply encode male with 1 and female with 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode 'Sex' variable values\nle = LabelEncoder()\ntrain_data['Sex'] = le.fit_transform(train_data['Sex'])\ntest_data['Sex'] = le.transform(test_data['Sex'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For 'Embarked' and 'Title' variables, we will use dummy variables to represent different values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert 'Embarked' and 'Title' into dummy variables\ntrain_data = pd.get_dummies(train_data, columns=['Embarked', 'Title'])\ntest_data = pd.get_dummies(test_data, columns=['Embarked', 'Title'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is how the dataset looks like after conversion:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we can calculate the correlation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairwise correlation of columns\ncorr = train_data.corr()\ncorr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's convert this into a visualization for better comprehension. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 8))\n\n# Draw the heatmap with the mask\nsns.heatmap(corr, mask=mask, cmap='RdBu_r', linewidths=.5, cbar_kws={'shrink': .7})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we just focus on the 'Survived' variable, we will notice that:\n* It has a comparatively strong negative correlation with 'Pclass', 'Sex' and 'Title_Mr'\n* It has a comparatively strong positive correlation with 'Fare', 'Embarked_C', 'Title_Miss' and 'Title_Mrs'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5. Model Building & Evaluation <a id=\"heading5\"></a>\n\nBefore we can start building the machine learning models, we need to apply feature scaling to standardize the independent variables within a particular range. This is required because some machine learning algorithms (such as kNN) tend to give more weightage to features with high magnitudes than features with low magnitudes, regardless of the unit of the values. To bring all features to the same level of magnitudes, we need to apply feature scaling.\n\nIn this case, we will use the MinMaxScaler to scale each feature to a (0, 1) range.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply feature scaling using MinMaxScaler\nscaler = MinMaxScaler()\ntrain_data.iloc[:, 2:] = scaler.fit_transform(train_data.iloc[:, 2:])\ntest_data.iloc[:, 1:] = scaler.transform(test_data.iloc[:, 1:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is how the dataset looks like after feature scaling (remember, we only need to scale predictor variables):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will split our train and test datasets with respect to predictor (X) and response (y) variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train = train_data.iloc[:, 2:], test_data.iloc[:, 1:], train_data['Survived']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 'y_test' is not provided in this dataset. For getting the test scores, we will have to submit our predictions online. To make the entire process a bit smoother, we will write a function that takes in model predictions and generates a file in the required format to submit online.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to generate submission file to get test score\ndef submission(preds):\n    test_data['Survived'] = preds\n    predictions = test_data[['PassengerId', 'Survived']]\n    predictions.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can finally start building machine learning models to predict which of the passengers survived.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Logistic Regression <a id=\"subheading1\"></a>\n\nImportant parameters that we will tune:\n* penalty: Used to specify the norm used in the penalization\n* C: Inverse of regularization strength\n\nFor hyperparameter tuning, we will use grid search cross validation over the specified parameter values. We will repeat 5-fold cross validation 10 times so that we can further improve the model performance and reduce overfitting. This will lead to better results for test/unseen data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification model\nlogreg = LogisticRegression()\n\n# Parameters to tune\nparams = [{'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n           'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}]\n\n# Hyperparameter tuning using GridSearchCV\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=seed)\nlr_clf = GridSearchCV(logreg, params, cv=cv, n_jobs=-1)\nlr_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters\nlr_clf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score\nlr_clf.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The train accuracy is 82.7%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score\ny_preds = lr_clf.predict(X_test)\nsubmission(y_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After submission, the test accuracy is found to be 76.8%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Gaussian Naive Bayes <a id=\"subheading2\"></a>\n\nUsing default parameters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification model\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score\ny_preds = gnb.predict(X_test)\nsubmission(y_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After submission, the test accuracy is found to be 75.1%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.3 Linear Discriminant Analysis (LDA) <a id=\"subheading3\"></a>\n\nUsing default parameters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification model\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score\ny_preds = lda.predict(X_test)\nsubmission(y_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After submission, the test accuracy is found to be 77.5%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.4 k Nearest Neighbors (kNN) <a id=\"subheading4\"></a>\n\nImportant parameters that we will tune:\n* n_neighbors: Number of neighbors to use\n* p: For choosing between manhattan distance and euclidean distance metrics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification model\nknn = KNeighborsClassifier()\n\n# Parameters to tune\nparams = [{'n_neighbors': range(1, 21),\n           'p': [1, 2]}]\n\n# Hyperparameter tuning using GridSearchCV\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=seed)\nknn_clf = GridSearchCV(knn, params, cv=cv, n_jobs=-1)\nknn_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters\nknn_clf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score\nknn_clf.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The train accuracy is 82.1%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score\ny_preds = knn_clf.predict(X_test)\nsubmission(y_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After submission, the test accuracy is found to be 77.3%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.5 Support Vector Machine (SVM) <a id=\"subheading5\"></a>\n\nImportant parameters that we will tune:\n* C: Penalty parameter for determining the trade-off between setting a larger margin and lowering misclassification\n* kernel: Specifies the kernel type to be used in the algorithm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification model\nsvm = SVC(max_iter=10000)\n\n# Parameters to tune\nparams = [{'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n           'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}]\n\n# Hyperparameter tuning using GridSearchCV\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=seed)\nsvm_clf = GridSearchCV(svm, params, cv=cv, n_jobs=-1)\nsvm_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters\nsvm_clf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score\nsvm_clf.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The train accuracy is 82.8%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score\ny_preds = svm_clf.predict(X_test)\nsubmission(y_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After submission, the test accuracy is found to be 77.8%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.6 Decision Tree <a id=\"subheading6\"></a>\n\nImportant parameters that we will tune:\n* max_depth: Maximum depth of the tree\n* min_samples_split: Minimum number of samples required to split an internal node\n* max_features: Number of features to consider when looking for the best split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification model\ndt = DecisionTreeClassifier(random_state=seed)\n\n# Parameters to tune\nparams = [{'max_depth': [5, 7, 10, None],\n           'min_samples_split': [2, 5, 10],\n           'max_features': ['sqrt', 5, 7, 10]}]\n\n# Hyperparameter tuning using GridSearchCV\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=seed)\ndt_clf = GridSearchCV(dt, params, cv=cv, n_jobs=-1)\ndt_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters\ndt_clf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score\ndt_clf.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The train accuracy is 81.6%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score\ny_preds = dt_clf.predict(X_test)\nsubmission(y_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After submission, the test accuracy is found to be 78%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.7 Random Forest <a id=\"subheading7\"></a>\n\nImportant parameters that we will tune:\n* n_estimators: Number of trees in the forest\n* max_depth: Maximum depth of the tree\n* min_samples_split: Minimum number of samples required to split an internal node\n* max_features: Number of features to consider when looking for the best split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note: This cell will take a while to run depending on the available processing power\n\n# Classification model\nrf = RandomForestClassifier(random_state=seed)\n\n# Parameters to tune\nparams = [{'n_estimators': range(50, 550, 50),\n           'max_depth': [5, 7, 10, None],\n           'min_samples_split': [2, 5, 10],\n           'max_features': ['sqrt', 5, 7, 10]}]\n\n# Hyperparameter tuning using GridSearchCV\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=seed)\nrf_clf = GridSearchCV(rf, params, cv=cv, n_jobs=-1)\nrf_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters\nrf_clf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score\nrf_clf.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The train accuracy is 83.7%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score\ny_preds = rf_clf.predict(X_test)\nsubmission(y_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After submission, the test accuracy is found to be 77%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.8 XGBoost <a id=\"subheading8\"></a>\n\nImportant parameters that we will tune:\n* max_depth: Maximum depth of the tree\n* learning_rate: Controls the contribution of each tree\n* n_estimators: Number of trees","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note: This cell will take a while to run depending on the available processing power\n\n# Classification model\nxgboost = xgb.XGBClassifier(random_state=seed)\n\n# Parameters to tune\nparams = [{'max_depth': [3, 5, 10],\n           'learning_rate': [0.01, 0.02, 0.03, 0.05, 0.07, 0.1],\n           'n_estimators': range(100, 1100, 100)}]\n\n# Hyperparameter tuning using GridSearchCV\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=seed)\nxgb_clf = GridSearchCV(xgboost, params, cv=cv, n_jobs=-1)\nxgb_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters\nxgb_clf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score\nxgb_clf.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The train accuracy is 82.9%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score\ny_preds = xgb_clf.predict(X_test)\nsubmission(y_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After submission, the test accuracy is found to be 76.8%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.9 Model Stacking <a id=\"subheading9\"></a>\n\nIn this part, we will stack all of our best performing models using the stacking classifier. Predictions generated by various models will be optimally combined to form a new set of predictions. (Note: The new predictions may not always give better result than the individual models).\n\nUsing default parameters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Models that we will input to stacking classifier\nbase_estimators = list()\nbase_estimators.append(('lda', lda))\nbase_estimators.append(('knn', knn_clf.best_estimator_))\nbase_estimators.append(('svm', svm_clf.best_estimator_))\nbase_estimators.append(('dt', dt_clf.best_estimator_))\nbase_estimators.append(('rf', rf_clf.best_estimator_))\n\n# Stacking classifier\nstacking_clf = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression(), cv=5, n_jobs=-1)\nstacking_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score\ny_preds = stacking_clf.predict(X_test)\nsubmission(y_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After submission, the test accuracy is found to be 78%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.10 Result Comparison <a id=\"subheading10\"></a>\n\n\n| Model | Train Accuracy (%) | Test Accuracy (%) |\n| ----- | ------------------ | ----------------- |\n| Logistic Regression | 82.7 | 76.8 |\n| Gaussian Naive Bayes | N/A | 75.1 |\n| Linear Discriminant Analysis | N/A | 77.5 |\n| k Nearest Neighbors | 82.1 | 77.3 |\n| Support Vector Machine | 82.8| 77.8 |\n| Decision Tree | 81.6 | 78 |\n| Random Forest | 83.7 | 77 |\n| XGBoost | 82.9 | 76.8 |\n| Model Stacking | N/A | 78 |","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Looking at the above table, we can observe the following:\n* Random Forest gave the highest train accuracy of 83.7%\n* Decision Tree and Stacking Classifier performed best for test/unseen data with an accuracy of 78%\n* Most of the models performed really similar in terms of test accuracy\n* Due to the small dataset size, all models have (slightly) overfitted the train data, giving lower test scores than expected","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 6. Conclusion <a id=\"heading6\"></a>\n\nThis notebook gave a brief overview of how different steps are performed in a data science project life cycle. We started by reading in the dataset, preprocessing it, exploring it to find useful insights, and finally built various machine learning models and evaluated them. The main objective of this project was to analyze the titanic dataset and predict whether a passenger will survive or not, based on various input features. To further build and improve upon this project, a lot of techniques could be tried.\n\nInnovative ways of feature engineering like combining the 'SibSp' and 'Parch' features, or applying different data preprocessing methods such as binning the 'Age' column could be tried to help improve the overall performance. One technique that will surely improve the scores is to further hypertune the models. Due to limited time and processing power available, we only performed grid search over a few combinations of paramters' values (we also skipped many parameters and used their default value). The extra time spent on tuning the parameters usually leads to better results.\n\nAdditionally, there are other options for trying and improving the prediction accuracy such as applying feature selection techniques or building deep learning models (e.g. neural networks). Part of a job of data scientists is to be creative, keep experimenting and try figuring out new ways of improving upon their work. The 'titanic survival prediction' task is no exception.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}