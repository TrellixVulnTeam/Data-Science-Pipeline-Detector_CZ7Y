{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploratory Data Analysis :","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('/kaggle/input/titanic/train.csv')\nplt.style.use('ggplot')\nfig = plt.figure(figsize = (8,5))\ndf.Survived.value_counts(normalize = True).plot(kind='bar', color= ['red','blue'], alpha = 0.5, rot = 1)\nplt.title('Survived and Deceased')\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-14T16:19:37.396956Z","iopub.execute_input":"2022-04-14T16:19:37.397538Z","iopub.status.idle":"2022-04-14T16:19:37.708212Z","shell.execute_reply.started":"2022-04-14T16:19:37.397504Z","shell.execute_reply":"2022-04-14T16:19:37.707142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(df.Survived, df.Age, color='maroon', alpha = 0.09)\nplt.title('Relation between Survival and Age')\nplt.show()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-04-14T16:19:47.937057Z","iopub.execute_input":"2022-04-14T16:19:47.937471Z","iopub.status.idle":"2022-04-14T16:19:48.257014Z","shell.execute_reply.started":"2022-04-14T16:19:47.937433Z","shell.execute_reply":"2022-04-14T16:19:48.256079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plot shows people with older age died somewhat more than people with younger age. More specifically, survival rate is denser in age range 10–30 and mortality rate is denser in range 30–50.","metadata":{}},{"cell_type":"code","source":"df.Pclass.value_counts(normalize = True).plot(kind='bar', color=['orange','red','green'], alpha = 0.6, rot = 1)\nplt.title('Class Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:19:56.098266Z","iopub.execute_input":"2022-04-14T16:19:56.098647Z","iopub.status.idle":"2022-04-14T16:19:56.228446Z","shell.execute_reply.started":"2022-04-14T16:19:56.098615Z","shell.execute_reply":"2022-04-14T16:19:56.22731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plot shows almost 55% of the passengers belong to 3rd class, 25% belong to 1st class and rest 20% belong to 2nd class respectively.","metadata":{}},{"cell_type":"code","source":"for i in [1,2,3]:\n    df.Age[df.Pclass == i].plot(kind = 'kde', alpha = 1.0)\nplt.title(\"Class vs Age\")\nplt.legend((\"1st\", \"2nd\", \"3rd\"))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:20:03.997731Z","iopub.execute_input":"2022-04-14T16:20:03.998098Z","iopub.status.idle":"2022-04-14T16:20:04.961856Z","shell.execute_reply.started":"2022-04-14T16:20:03.998064Z","shell.execute_reply":"2022-04-14T16:20:04.960843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plot corresponds to kernel density estimate (simply put it is just a way to represent the probability density function) for the three types of classes of the passengers w.r.t. age. The plot shows the younger passengers are more among the 3rd class passengers and older passengers are among the 1st class ones. The middle aged ones are in 2nd class mostly.","metadata":{}},{"cell_type":"code","source":"df.Embarked.value_counts(normalize = True).plot(kind='bar', color=['orange','red','green'], alpha = 0.6, rot = 1)\nplt.title('Places where embarked')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:20:14.078999Z","iopub.execute_input":"2022-04-14T16:20:14.079853Z","iopub.status.idle":"2022-04-14T16:20:14.217351Z","shell.execute_reply.started":"2022-04-14T16:20:14.079809Z","shell.execute_reply":"2022-04-14T16:20:14.216213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above code is for knowing the information how many of the passengers embarked from Southampton (from where Titanic actually started its journey), how many embarked from Cherbourg (where Titanic stopped first in France) and how many embarked from Queenstown (where it stopped last in Ireland). The above graph shows the figs are around 70%, 20% and 10% respectively.","metadata":{}},{"cell_type":"code","source":"m_color = '#F8BA00'\ndf.Survived[df.Sex == 'male'].value_counts(normalize = True).plot(kind='bar', alpha = 0.6, color = m_color, rot = 1)\nplt.title('Male Survived')\nplt.show()\nf_color = '#FA0000'\ndf.Survived[df.Sex == 'female'].value_counts(normalize = True).plot(kind='bar', alpha = 0.6, color = f_color, rot = 1)\nplt.title('Female Survived')\nplt.show()\ndf.Sex[df.Survived == 1].value_counts(normalize = True).plot(kind='bar', alpha = 0.6, color = [f_color, m_color], rot = 1)\nplt.title('Gender of Survived')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:20:23.638356Z","iopub.execute_input":"2022-04-14T16:20:23.638746Z","iopub.status.idle":"2022-04-14T16:20:24.127475Z","shell.execute_reply.started":"2022-04-14T16:20:23.638712Z","shell.execute_reply":"2022-04-14T16:20:24.126543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plots are to visualize the correlation of survival with gender. Among male passengers, 80% died (20% survived); among female passengers, only 30% died (70% survived). Among all survivors, 70% were female and 30% were male. Perhaps the reason being female and children were rescued first.","metadata":{}},{"cell_type":"code","source":"for i in [1,2,3]:\n    df.Survived[df.Pclass == i].plot(kind = 'kde', alpha = 0.9, rot = 1)\nplt.title(\"Class vs Survived\")\nplt.legend((\"1st\", \"2nd\", \"3rd\"))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:20:31.917535Z","iopub.execute_input":"2022-04-14T16:20:31.917904Z","iopub.status.idle":"2022-04-14T16:20:32.180345Z","shell.execute_reply.started":"2022-04-14T16:20:31.917872Z","shell.execute_reply":"2022-04-14T16:20:32.179558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graph shows that 3rd class passengers have very high mortality rate compared to 1st class passengers (see *violet hump* has a *great height* but *red hump* has *lower height* above ***x-axis’ point 0.00***). And 1st class passengers have higher survival rate compared to 3rd class passengers (see *red hump* is much *higher* compared to *violet hump* above ***x-axis’ point 1.00***).","metadata":{}},{"cell_type":"code","source":"plt.subplot2grid((4,4),(0,0), rowspan = 2, colspan = 2)\ndf.Survived[(df.Sex == 'male') & (df.Pclass == 1)].value_counts(normalize = True).plot(kind='bar', alpha = 0.5, color = m_color, rot = 1)\nplt.title('Rich Male Survived')\nplt.show()\nplt.subplot2grid((4,4),(0,1), rowspan = 2, colspan = 2)\ndf.Survived[(df.Sex == 'male') & (df.Pclass == 3)].value_counts(normalize = True).plot(kind='bar', alpha = 0.5, color = m_color, rot = 1)\nplt.title('Poor Male Survived')\nplt.show()\nplt.subplot2grid((4,4),(1,0), rowspan = 2, colspan = 2)\ndf.Survived[(df.Sex == 'female') & (df.Pclass == 1)].value_counts(normalize = True).plot(kind='bar', alpha = 0.5, color = f_color, rot = 1)\nplt.title('Rich Female Survived')\nplt.show()\nplt.subplot2grid((4,4),(1,1), rowspan = 2, colspan = 2)\ndf.Survived[(df.Sex == 'female') & (df.Pclass == 3)].value_counts(normalize = True).plot(kind='bar', alpha = 0.5, color = f_color, rot = 1)\nplt.title('Poor Female Survived')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:20:39.217771Z","iopub.execute_input":"2022-04-14T16:20:39.218145Z","iopub.status.idle":"2022-04-14T16:20:39.678864Z","shell.execute_reply.started":"2022-04-14T16:20:39.218115Z","shell.execute_reply":"2022-04-14T16:20:39.677787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"40% of rich men (1st class) survived, 60% of them died; 10% of poor men survived, 90% of them died; almost 100% of rich women survived, only <1% of them died; 50% of poor women survived, rest 50% of them died.","metadata":{}},{"cell_type":"markdown","source":"# Data Pre-Processing :\n\nWe will use below function for data pre-processing. It finds the cells in the data set where “Fare” and “Age” have no value (blank cells) and fills them with median values of those columns respectively. For convenience, it marks “male” passengers with “0” and female passengers with “1”. Also, it finds the blank cells in “Embarked” column and fills them with “S” (Southampton by default). For convenience, it marks “S” (Sothampton embarked) with “0”, “C” with 1 and “Q” with 2 respectively.","metadata":{}},{"cell_type":"code","source":"def clean_data(data):\n    data[\"Fare\"] = data[\"Fare\"].fillna(data[\"Fare\"].dropna().median())\n    data[\"Age\"] = data[\"Age\"].fillna(data[\"Age\"].dropna().median())\n    \n    data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 0\n    data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 1\n    \n    data[\"Embarked\"] = data[\"Embarked\"].fillna(\"S\")\n    data.loc[data[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n    data.loc[data[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n    data.loc[data[\"Embarked\"] == \"Q\", \"Embarked\"] = 2","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:20:47.557609Z","iopub.execute_input":"2022-04-14T16:20:47.557965Z","iopub.status.idle":"2022-04-14T16:20:47.567725Z","shell.execute_reply.started":"2022-04-14T16:20:47.557936Z","shell.execute_reply":"2022-04-14T16:20:47.566806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will use the above function for finding the score of prediction for survival. Below code takes “Survived” as dependent variable and rest of the features as independent variables to fit the entire thing into regression model and finds the accuracy of the model.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:20:53.492312Z","iopub.execute_input":"2022-04-14T16:20:53.492711Z","iopub.status.idle":"2022-04-14T16:20:53.497005Z","shell.execute_reply.started":"2022-04-14T16:20:53.492674Z","shell.execute_reply":"2022-04-14T16:20:53.495969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling :\n\nI kept the modelling part pretty simple. I used only Logistic Regression, Decision Tree and Random Forest models.\n\n**(1A) Logistic Regression :**","metadata":{}},{"cell_type":"code","source":"# Check score with simple Logistic Regression Model\nimport pandas as pd\nfrom sklearn import linear_model\ntrain = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nclean_data(train)\ntarget = train['Survived']\nfeatures = train[['Pclass', 'Age', 'Fare', 'Embarked', 'Sex', 'SibSp', 'Parch']]\nclassifier = linear_model.LogisticRegression(solver='liblinear')\nclassifier_ = classifier.fit(features, target)\nprint(classifier_.score(features, target))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:21:00.559594Z","iopub.execute_input":"2022-04-14T16:21:00.56034Z","iopub.status.idle":"2022-04-14T16:21:00.885009Z","shell.execute_reply.started":"2022-04-14T16:21:00.560295Z","shell.execute_reply":"2022-04-14T16:21:00.883811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**(1B) Logistic Regression with Polynomial Degree 2 :**\n\nNow we will check whether fitting a regression model with polynomial degree = 2 gives better accuracy in prediction or not. The below code does not produce better accuracy. So, to avoid overfitting, we will not try fitting it with polynomial of degree = 2.","metadata":{}},{"cell_type":"code","source":"# Check score with Logistic Regression Model with Polynomial Degree = 2\nfrom sklearn import linear_model, preprocessing\npoly = preprocessing.PolynomialFeatures(degree=2)\npoly_features = poly.fit_transform(features)\nclassifier_ = classifier.fit(poly_features, target)\nprint(classifier_.score(poly_features, target))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:21:07.297036Z","iopub.execute_input":"2022-04-14T16:21:07.297463Z","iopub.status.idle":"2022-04-14T16:21:07.350996Z","shell.execute_reply.started":"2022-04-14T16:21:07.297423Z","shell.execute_reply":"2022-04-14T16:21:07.349834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe a very little improvement using polynomial degree 2. \n\n**(2A) Decision Tree Model :**\n\nNow, we will try fitting the problem with a decision tree model.","metadata":{}},{"cell_type":"code","source":"# Check score with Decision Tree Model\nimport pandas as pd\nfrom sklearn import tree\ntrain = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nclean_data(train)\ntarget = train[\"Survived\"]\nfeatures = train[[\"Pclass\", \"Age\", \"Fare\", \"Embarked\", \"Sex\", \"SibSp\", \"Parch\"]]\ndecision_tree = tree.DecisionTreeClassifier(random_state = 42)\ndecision_tree_ = decision_tree.fit(features, target)\nprint(decision_tree_.score(features, target)) ","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:21:14.957893Z","iopub.execute_input":"2022-04-14T16:21:14.958259Z","iopub.status.idle":"2022-04-14T16:21:15.082795Z","shell.execute_reply.started":"2022-04-14T16:21:14.958227Z","shell.execute_reply":"2022-04-14T16:21:15.081743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, the decision tree produces 97.98% accuracy which is a better-fit model compared to previous logistic regression models (simple and polynomial deg =2). We can make this decision tree model more generalized by using a generalized decision tree and using cross validation (50 fold). The model improvement also indicates at reducing overfitting, apart from just enhancing accuracy.\n\n**(2B) Decision Tree Model with 50-Fold Cross Validation :**","metadata":{}},{"cell_type":"code","source":"# Making the Decision Tree more generalized to reduce overfitting\nfrom sklearn import model_selection\ngeneralized_tree = tree.DecisionTreeClassifier(\n                    random_state = 1,\n                    max_depth = 7,\n                    min_samples_split = 2)\ngeneralized_tree_ = generalized_tree.fit(features, target)\nscores = model_selection.cross_val_score(generalized_tree, features, target, scoring = 'accuracy', cv = 50)\nprint(scores)\nprint(scores.mean())","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:21:22.598519Z","iopub.execute_input":"2022-04-14T16:21:22.598911Z","iopub.status.idle":"2022-04-14T16:21:22.774639Z","shell.execute_reply.started":"2022-04-14T16:21:22.598881Z","shell.execute_reply":"2022-04-14T16:21:22.773686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that the overall score has decreased but this generalized tree avoids overfitting. We can visualize the tree picture using **graphviz**.","metadata":{}},{"cell_type":"code","source":"import graphviz\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.preprocessing import StandardScaler as scaler\n\ndata = export_graphviz(DecisionTreeClassifier(max_depth=3).fit(features, target), out_file=None, \n                       feature_names = ['Pclass', 'Age', 'Fare', 'Embarked', 'Sex', 'SibSp', 'Parch'],\n                       class_names = ['Survived (0)', 'Survived (1)'], \n                       filled = True, rounded = True, special_characters = True)\n# we have intentionally kept max_depth short here to accommodate the entire visual-tree\ngraph = graphviz.Source(data)\ngraph","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:21:30.03847Z","iopub.execute_input":"2022-04-14T16:21:30.038903Z","iopub.status.idle":"2022-04-14T16:21:30.580672Z","shell.execute_reply.started":"2022-04-14T16:21:30.03887Z","shell.execute_reply":"2022-04-14T16:21:30.579403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**(3) Random Forest :**\n\nNext, we will try fitting the problem with a Random Forest model to improve the accuracy of the result. Since Random Forest is a bagging ensemble model consisting of multiple decision trees, it will improve the performance of the model. We will first tune the hyperparameters and then will use the model with best hyperparameters.","metadata":{}},{"cell_type":"code","source":"#Perform Grid Search to tune hyperparameters of the Random Forest model\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(random_state = 1)\nn_estimators = [1740, 1742, 1745, 1750]\nmax_depth = [6, 7, 8]\nmin_samples_split = [4, 5, 6]\nmin_samples_leaf = [4, 5, 6] \noob_score = ['True']\n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, oob_score = oob_score)\n\ngridF = GridSearchCV(forest, hyperF, verbose = 1, n_jobs = 4)\nbestF = gridF.fit(features, target)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:21:39.037699Z","iopub.execute_input":"2022-04-14T16:21:39.038325Z","iopub.status.idle":"2022-04-14T16:38:46.26311Z","shell.execute_reply.started":"2022-04-14T16:21:39.038269Z","shell.execute_reply":"2022-04-14T16:38:46.261894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(bestF)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:42:03.271234Z","iopub.execute_input":"2022-04-14T16:42:03.271876Z","iopub.status.idle":"2022-04-14T16:42:03.277025Z","shell.execute_reply.started":"2022-04-14T16:42:03.271827Z","shell.execute_reply":"2022-04-14T16:42:03.276147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check score with Random Forest Model having the best hyperparameters\nfrom sklearn.ensemble import RandomForestClassifier\ntrain = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nclean_data(train)\ntarget = train[\"Survived\"]\nfeatures = train[[\"Pclass\", \"Age\", \"Fare\", \"Embarked\", \"Sex\", \"SibSp\", \"Parch\"]]\nr_forest = RandomForestClassifier(criterion='gini',bootstrap=True,\n                                    n_estimators=1745,\n                                    max_depth=7,\n                                    min_samples_split=6,\n                                    min_samples_leaf=6,\n                                    max_features='auto',\n                                    oob_score=True,\n                                    random_state=123,\n                                    n_jobs=-1,\n                                    verbose=0)\nrf_clf = r_forest.fit(features, target)\nprint(rf_clf.score(features, target)) ","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:42:14.539076Z","iopub.execute_input":"2022-04-14T16:42:14.539473Z","iopub.status.idle":"2022-04-14T16:42:21.447253Z","shell.execute_reply.started":"2022-04-14T16:42:14.53944Z","shell.execute_reply":"2022-04-14T16:42:21.446119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_clf.oob_score_","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:42:29.997921Z","iopub.execute_input":"2022-04-14T16:42:29.998336Z","iopub.status.idle":"2022-04-14T16:42:30.004453Z","shell.execute_reply.started":"2022-04-14T16:42:29.998303Z","shell.execute_reply":"2022-04-14T16:42:30.00361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that Random Forest model is also giving a high accuracy. Since Random Forest is an ensemble model, it works better compared to usual Decision Tree model. We can also make several trials with different parameter-ranges for tuning the hyperparameters of the Random Forest model in order to get the maximum accuracy. However, since we have already got good quite accuracy with our Random Forest model, we will use it for final submission. \n\n# Plotting Decision Regions using Mlxtend :\n\nBefore final submission, we will just check plotting decision regions applying different models on the training data set. I have used [**mlxtend library**](http://rasbt.github.io/mlxtend/) written by ***Sebastian Raschka***. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.plotting import plot_decision_regions\n\nvalue = 1.50\nwidth = 0.75\n\nclf1 = LogisticRegression(solver='liblinear', random_state=0)\nclf2 = RandomForestClassifier(random_state=0)\nclf3 = DecisionTreeClassifier(random_state=0) \neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1, 1, 1], voting='soft')\n\nX_list = train[[\"Pclass\", \"Age\", \"Fare\", \"Embarked\", \"Sex\", \"SibSp\", \"Parch\"]]\nX = np.asarray(X_list, dtype=np.float32)\ny_list = train[\"Survived\"]\ny = np.asarray(y_list, dtype=np.int32)\n\n# Plotting Decision Regions\ngs = gridspec.GridSpec(2, 2)\nfig = plt.figure(figsize=(10, 8))\n\nlabels = ['Logistic Regression',\n          'Random Forest',\n          'Decision Tree',\n          'Ensemble']\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n                         labels,\n                         itertools.product([0, 1],\n                         repeat=2)):\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, \n                                filler_feature_values={2: value, 3: value, 4: value, 5: value, 6: value}, \n                                filler_feature_ranges={2: width, 3: width, 4: width, 5: width, 6: width},\n                                legend=2)\n    plt.title(lab)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:42:40.67793Z","iopub.execute_input":"2022-04-14T16:42:40.678383Z","iopub.status.idle":"2022-04-14T16:42:46.899239Z","shell.execute_reply.started":"2022-04-14T16:42:40.678344Z","shell.execute_reply":"2022-04-14T16:42:46.898265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the decision boundary made with Decision Tree model and Ensemble Model ***(Ensemble of three models with equal weights)*** are almost similar. They are combination of squares and rectangles formed by split partitions. Decision regions plotted with Logistic Regression are very distinct for two classes. For Random Forest, the regions are quite distinctly divided as well. Moreover, Random Forest has divided both the classes into almost equal proportion (balanced). This is a reconfirmation for choosing Random Forest as the best model for submission.","metadata":{}},{"cell_type":"markdown","source":"# Submission using Random Forest :","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn import tree\ntest = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nclean_data(test)\nprediction = rf_clf.predict(test[[\"Pclass\", \"Age\", \"Fare\", \"Embarked\", \"Sex\", \"SibSp\", \"Parch\"]])\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': prediction})\noutput.to_csv('titanic_submission.csv', index=False)\nprint(\"Submission successful\")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T16:42:52.481061Z","iopub.execute_input":"2022-04-14T16:42:52.481673Z","iopub.status.idle":"2022-04-14T16:42:53.336287Z","shell.execute_reply.started":"2022-04-14T16:42:52.481623Z","shell.execute_reply":"2022-04-14T16:42:53.335327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}