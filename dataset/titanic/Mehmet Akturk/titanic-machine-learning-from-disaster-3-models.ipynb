{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overview"},{"metadata":{},"cell_type":"markdown","source":"**Titanic Survival Prediction:**\n\nUse machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\n**Variables and Their Types:**\n\nSurvival: Survival -> 0 = No, 1 = Yes\n\nPclass: Ticket class -> 1 = 1st, 2 = 2nd, 3 = 3rd\n\nSex: Sex\n\nAge: Age in years\n\nSibSp: # of siblings / spouses aboard the Titanic\n\nParch: # of parents / children aboard the Titanic\n\nTicket: Ticket number\n\nFare: Passenger fare\n\nCabin: Cabin number\n\nEmbarked: Port of Embarkation -> C = Cherbourg, Q = Queenstown, S = Southampton\n\n\n\n**Variable Notes:**\n\nPclass: A proxy for socio-economic status (SES)\n- 1st = Upper\n- 2nd = Middle\n- 3rd = Lower\n\nAge: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nSibSp: The dataset defines family relations in this way...\n- Sibling = brother, sister, stepbrother, stepsister\n- Spouse = husband, wife (mistresses and fianc√©s were ignored)\n\nParch: The dataset defines family relations in this way...\n- Parent = mother, father\n- Child = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them."},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Libraries Importing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport time\nfrom contextlib import contextmanager\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler,minmax_scale\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Help Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_preprocessing():\n\n    train = pd.read_csv(\"../input/titanic/train.csv\")\n    test = pd.read_csv(\"../input/titanic/test.csv\")\n\n    train = train.drop(['Ticket'], axis = 1)\n    test = test.drop(['Ticket'], axis = 1)\n\n    train['Fare'] = train['Fare'].replace(512.3292, 300)\n    test['Fare'] = test['Fare'].replace(512.3292, 300)\n\n    train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].mean())\n    test[\"Age\"] = test[\"Age\"].fillna(test[\"Age\"].mean())\n\n    # Fill NA with the most frequent value:\n    train[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n    test[\"Embarked\"] = test[\"Embarked\"].fillna(\"S\")\n\n    test[\"Fare\"] = test[\"Fare\"].fillna(12)\n\n    train[\"CabinBool\"] = (train[\"Cabin\"].notnull().astype('int'))\n    test[\"CabinBool\"] = (test[\"Cabin\"].notnull().astype('int'))\n\n    train = train.drop(['Cabin'], axis = 1)\n    test = test.drop(['Cabin'], axis = 1)\n\n    # Map each Embarked value to a numerical value:\n    embarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n\n    train['Embarked'] = train['Embarked'].map(embarked_mapping)\n    test['Embarked'] = test['Embarked'].map(embarked_mapping)\n\n    \n    lbe = preprocessing.LabelEncoder()\n    \n    train[\"Sex\"] = lbe.fit_transform(train[\"Sex\"])\n    test[\"Sex\"] = lbe.fit_transform(test[\"Sex\"])\n\n    train[\"Title\"] = train[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)\n    test[\"Title\"] = test[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n    train['Title'] = train['Title'].replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    train['Title'] = train['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    train['Title'] = train['Title'].replace('Mlle', 'Miss')\n    train['Title'] = train['Title'].replace('Ms', 'Miss')\n    train['Title'] = train['Title'].replace('Mme', 'Mrs')\n\n    test['Title'] = test['Title'].replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    test['Title'] = test['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    test['Title'] = test['Title'].replace('Mlle', 'Miss')\n    test['Title'] = test['Title'].replace('Ms', 'Miss')\n    test['Title'] = test['Title'].replace('Mme', 'Mrs')\n\n    train[[\"Title\",\"PassengerId\"]].groupby(\"Title\").count()\n    train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n\n    # Map each of the title groups to a numerical value\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 5}\n\n    train['Title'] = train['Title'].map(title_mapping)\n    test['Title'] = test['Title'].map(title_mapping)\n\n    train = train.drop(['Name'], axis = 1)\n    test = test.drop(['Name'], axis = 1)\n\n    bins = [0, 5, 12, 18, 24, 35, 60, np.inf]\n    \n    mylabels = ['Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n    \n    train['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = mylabels)\n    test['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = mylabels)\n\n    # Map each Age value to a numerical value:\n    age_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\n    \n    train['AgeGroup'] = train['AgeGroup'].map(age_mapping)\n    test['AgeGroup'] = test['AgeGroup'].map(age_mapping)\n    \n    \n    # after mapping AgeGroup values change from category to numeric\n    train['AgeGroup'] = pd.to_numeric(train['AgeGroup'])\n    test['AgeGroup'] = pd.to_numeric(test['AgeGroup'])\n    \n    \n\n    #dropping the Age feature for now, might change:\n    train = train.drop(['Age'], axis = 1)\n    test = test.drop(['Age'], axis = 1)\n\n    # Map Fare values into groups of numerical values:\n    train['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\n    test['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\n    \n    # after mapping FareBand values change from category to numeric\n    train['FareBand'] = pd.to_numeric(train['FareBand'])\n    test['FareBand'] = pd.to_numeric(test['FareBand'])\n    \n    \n    # Drop Fare values:\n    train = train.drop(['Fare'], axis = 1)\n    test = test.drop(['Fare'], axis = 1)\n\n    print(\"Data Preprocessing Process Has Been Finished\\n\")\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(train, test):\n    \n    train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\n    test[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"] + 1\n\n    # Create new feature of family size:\n    train['Single'] = train['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n    train['SmallFam'] = train['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\n    train['MedFam'] = train['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n    train['LargeFam'] = train['FamilySize'].map(lambda s: 1 if s >= 5 else 0)\n\n     # Create new feature of family size:\n    test['Single'] = test['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n    test['SmallFam'] = test['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\n    test['MedFam'] = test['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n    test['LargeFam'] = test['FamilySize'].map(lambda s: 1 if s >= 5 else 0)\n\n     # Convert Title and Embarked into dummy variables:\n    train = pd.get_dummies(train, columns = [\"Title\"])\n    train = pd.get_dummies(train, columns = [\"Embarked\"], prefix=\"Em\")\n\n    test = pd.get_dummies(test, columns = [\"Title\"])\n    test = pd.get_dummies(test, columns = [\"Embarked\"], prefix=\"Em\")\n\n    # Create categorical values for Pclass:\n    train[\"Pclass\"] = train[\"Pclass\"].astype(\"category\")\n    train = pd.get_dummies(train, columns = [\"Pclass\"],prefix=\"Pc\")\n\n    test[\"Pclass\"] = test[\"Pclass\"].astype(\"category\")\n    test = pd.get_dummies(test, columns = [\"Pclass\"],prefix=\"Pc\")\n    \n    \n    \n    print(\"Feature Engineering Process Has Been Finished\\n\")\n    \n    return train, test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def gbm_model(train):\n\n    # Dependent and independent variables assign\n    \n    X = train.drop(['Survived', 'PassengerId'], axis=1)\n    Y = train[\"Survived\"]\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)\n    \n    \n    # Standartization of variables\n    \n#     x_train =StandardScaler().fit_transform(x_train)\n#     y_train = train['Survived']\n#     x_test = StandardScaler().fit_transform(x_test)\n\n\n    gbm = GradientBoostingClassifier()\n\n    gbm_params = {\n            'n_estimators': [200, 500],\n            'subsample': [1.0],\n            'max_depth': [8],\n            'learning_rate': [0.01,0.02],\n            \"min_samples_split\": [10]}\n\n    gbm_cv_model = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 5)\n\n    gbm_cv_model.fit(x_train, y_train)\n\n    print(gbm_cv_model.best_params_ , \"\\n\")\n\n    gbm_tuned = GradientBoostingClassifier(learning_rate = gbm_cv_model.best_params_[\"learning_rate\"], \n                        max_depth = gbm_cv_model.best_params_[\"max_depth\"],\n                        min_samples_split = gbm_cv_model.best_params_[\"min_samples_split\"],\n                        n_estimators = gbm_cv_model.best_params_[\"n_estimators\"],\n                        subsample = gbm_cv_model.best_params_[\"subsample\"])\n\n    gbm_tuned.fit(x_train, y_train)\n\n    y_pred = gbm_tuned.predict(x_test)\n    print(\"Accuracy Score of Your Model:\")\n    print(round(accuracy_score(y_pred, y_test) * 100, 2))\n    \n    return gbm_tuned","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"def submission(gbm_tuned, test):\n    \n    #set ids as PassengerId and predict survival \n    ids = test['PassengerId']\n\n    predictions = gbm_tuned.predict(test.drop('PassengerId', axis=1))\n\n    #set the output as a dataframe and convert to csv file named submission.csv\n    output = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\n    \n    output.to_csv('submission.csv', index=False)\n    print(\"Submission file has been created\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Main"},{"metadata":{"trusted":true},"cell_type":"code","source":"def main():\n    \n    with timer(\"Pre processing Time\"):\n        train, test = data_preprocessing()\n    \n    with timer(\"Feature Engineering\"):\n        train, test = feature_engineering(train, test)\n        \n    with timer(\"Create Model\"):\n        gbm_tuned = gbm_model(train)\n        \n    with timer(\"Submission\"):\n        submission(gbm_tuned, test)   \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    with timer(\"Full model run\"):\n        main()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Conclusion\n\n1. If this tutorial is not enough you can check also https://www.kaggle.com/berenkeskin/data-science-starter-notebook-with-3-ml-algorithms\n\n2. I used here GBM Model but i tried also LightGBM and XGBoost Models. Results of both models:\n    * LightGBM Accuracy Score of Your Model: 82.12\n    * LightGBM Create Model time - done in 590s\n\n    * XGBoost Accuracy Score of Your Model: 82.68\n    * XGBoost Create Model time - done in 1354s\n\n    \n3. After this tutorial, my aim is to prepare 'kernel' which is connected to Deep Learning 'not clear' data set.\n4. If you have any suggestions, please could you write for me? I wil be happy for comment and critics!\n5. Thank you for your suggestion and votes ;)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}