{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-25T11:59:53.905816Z","iopub.execute_input":"2022-03-25T11:59:53.906547Z","iopub.status.idle":"2022-03-25T11:59:53.917193Z","shell.execute_reply.started":"2022-03-25T11:59:53.906506Z","shell.execute_reply":"2022-03-25T11:59:53.916114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting up data","metadata":{}},{"cell_type":"code","source":"# Set up data sets\nfrom sklearn.model_selection import train_test_split\n\nX_test_full = pd.read_csv('../input/titanic/test.csv', index_col='PassengerId')\nX_full = pd.read_csv('../input/titanic/train.csv', index_col='PassengerId')\nX_full.dropna(axis=0, subset=['Survived'])\n\ny = X_full.Survived\nX_full.drop(['Survived'], axis=1, inplace=True)\n\nfeature_columns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch']\nX = X_full[feature_columns]\nX_test = X_test_full[feature_columns]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\nprint('Amount of rows for training set: %d' %(X.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T13:20:32.576103Z","iopub.execute_input":"2022-03-25T13:20:32.576358Z","iopub.status.idle":"2022-03-25T13:20:32.602183Z","shell.execute_reply.started":"2022-03-25T13:20:32.57633Z","shell.execute_reply":"2022-03-25T13:20:32.601472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T12:27:54.165454Z","iopub.execute_input":"2022-03-25T12:27:54.166121Z","iopub.status.idle":"2022-03-25T12:27:54.177578Z","shell.execute_reply.started":"2022-03-25T12:27:54.166079Z","shell.execute_reply":"2022-03-25T12:27:54.176753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"print('Amount of missing values in each column: ')\nX.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T12:27:57.4735Z","iopub.execute_input":"2022-03-25T12:27:57.473765Z","iopub.status.idle":"2022-03-25T12:27:57.482551Z","shell.execute_reply.started":"2022-03-25T12:27:57.473735Z","shell.execute_reply":"2022-03-25T12:27:57.481733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=3>There are missing values for the Age feature. A bit of data analysis is necessary in order to fill in those empty values in a meaningful way. **Reminder: we should use the training set when deriving features from data to avoid contamination.**","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nsns.set_theme(style='darkgrid')\n\nfig, ax = plt.subplots(2, 2, figsize=(18, 8))\nax[0,0].scatter(x=X_train.Age, y=X_train.SibSp, alpha=0.75, c=X_train.SibSp,  cmap=cm.get_cmap('summer'))\nax[0,0].set_title('Siblings / spouses per age')\nax[0,1].scatter(x=X_train.Age, y=X_train.Parch, c=X_train.Parch, cmap=cm.get_cmap('viridis'))\nax[0,1].set_title('Parents / children per age')\n\nx1 = X_train[X_train.Age.isnull()]\nax[1,0].scatter(range(len(x1)), x1.SibSp, alpha=0.75, c=x1.SibSp,  cmap=cm.get_cmap('summer'))\nax[1,0].set_title('Siblings / spouses for each missing age person')\nax[1,1].scatter(range(len(x1)), x1.Parch, c=x1.Parch, cmap=cm.get_cmap('viridis'))\nax[1,1].set_title('Parents / children for each missing age person')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T12:27:59.788523Z","iopub.execute_input":"2022-03-25T12:27:59.789066Z","iopub.status.idle":"2022-03-25T12:28:00.484307Z","shell.execute_reply.started":"2022-03-25T12:27:59.789017Z","shell.execute_reply":"2022-03-25T12:28:00.483611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=3>The vast majority of people with missing input for the Age column have no siblings/spouses or parents/children, therefore it makes sense to use the average age within that group (people with no siblings/spouses or parents/children) as an estimation for the age values.","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\navg_Age = X_train[(X_train.SibSp + X_train.Parch) == 0].Age.mean()\n\nnumerical_transformer = SimpleImputer(strategy='constant', fill_value=avg_Age)\n\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder()),\n    ('imputer', SimpleImputer(strategy='most_frequent'))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, make_column_selector(dtype_include=[np.integer, np.floating])),\n    ('cat', categorical_transformer,  make_column_selector(dtype_include=object))\n])\n\nprint('Amount of missing values in each column for all data after preprocessing: ')\npd.DataFrame(preprocessor.fit_transform(X_train)).isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T12:28:03.012628Z","iopub.execute_input":"2022-03-25T12:28:03.01289Z","iopub.status.idle":"2022-03-25T12:28:03.0388Z","shell.execute_reply.started":"2022-03-25T12:28:03.012862Z","shell.execute_reply":"2022-03-25T12:28:03.037843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hypertuning","metadata":{}},{"cell_type":"markdown","source":"<font size=3>Now that we have our transformers which can handle both missing and categorical data, we can use them during **cross validation** and try to find the best set of parameters for our model.\n    \n<font size=3>Since we are dealing with a classification problem, the **XGBoost Classifier** (eXtreme Gradient Boosting) will be chosen as the model. It works by assigning scores to each leaf as it adds trees (CARTs) to the ensemble model. Then, the trees are run based on the input parameters and scores are calculated, so they can be summed to obtain the prediction result.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%; text-align:center\">\n<img align=middle src=https://www.researchgate.net/profile/Li-Mingtao-2/publication/335483097/figure/fig3/AS:934217085100032@1599746118459/A-general-architecture-of-XGBoost.ppm width=500px>","metadata":{}},{"cell_type":"markdown","source":"<font size=3>We also have to make a custom function to implement cross validation if we want to make use of XGBoost's **early_stopping_rounds**, since we need access to the validation splits during the cross validation process.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold, ParameterGrid\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\ndef XGB_CVGridSearch(estimator, X, y, estimator_params, n_splits=5, verbose=0, vectorize_output=False):\n    # Searches for the best combination of given parameters for estimator\n    \n    skf = StratifiedKFold(n_splits=n_splits)\n    best_params = None\n    best_score = 0\n    for params_permut in ParameterGrid(estimator_params):\n        val_scores = np.empty(skf.n_splits)\n        train_scores = val_scores.copy()\n        for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n            X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n            X_val, y_val = X.iloc[val_index], y.iloc[val_index]\n            estimator.set_params(**params_permut)\n            estimator.fit(X_train, y_train,\n                        model__eval_set=[(preprocessor.transform(X_val), y_val)],\n                        model__early_stopping_rounds=10,\n                        model__verbose=0)\n            val_scores[i] = estimator.score(X_val, y_val)\n            train_scores[i] = estimator.score(X_train, y_train)\n        val_score, train_score = val_scores.mean(), train_scores.mean()\n        if verbose >= 1:\n            print(\"Train score: %f | Validation score: %f\" %(train_score, val_score))\n            if verbose >= 2:\n                print('Best iteration: ', estimator.get_params()['model'].get_booster().best_ntree_limit)\n                print('Params: ', params_permut)\n        if val_score > best_score:     \n            best_params = params_permut\n            best_params['model__n_estimators'] = estimator.get_params()['model'].get_booster().best_ntree_limit\n            best_score = val_score\n            \n    preds = estimator.predict(X_val)\n    probas = estimator.predict_proba(X_val)[:, 1]\n    accuracy = accuracy_score(y_val, preds)\n    roc = roc_auc_score(y_val, probas)\n    print(\"Accuracy: \", accuracy, \"ROC AUC: \", roc, \"\\nBest score: \", best_score, \", with parameters: \", best_params)\n    \n    if vectorize_output == True:\n        return {key:[value] for key, value in best_params.items()}\n    else:\n        return best_params","metadata":{"execution":{"iopub.status.busy":"2022-03-25T13:11:15.776574Z","iopub.execute_input":"2022-03-25T13:11:15.776851Z","iopub.status.idle":"2022-03-25T13:11:15.790367Z","shell.execute_reply.started":"2022-03-25T13:11:15.77682Z","shell.execute_reply":"2022-03-25T13:11:15.789677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=3>Here comes the **grid search** process, where we try all possible combinations of parameters across our grid. We will be using our custom cross validation function in order to tune the model's **hyperparameters**.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nmy_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', XGBClassifier(use_label_encoder=False,\n                            objective='binary:logistic',\n                            eval_metric='logloss'))\n])\n\ngrid_params0 = {\n    'model__n_estimators': [1000],\n    'model__learning_rate': [3, 1, 0.1, 0.01],\n}\n\ngrid_params0 = XGB_CVGridSearch(my_pipeline, X_train, y_train, grid_params0, verbose=1, vectorize_output=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T13:14:37.783078Z","iopub.execute_input":"2022-03-25T13:14:37.783329Z","iopub.status.idle":"2022-03-25T13:14:39.682436Z","shell.execute_reply.started":"2022-03-25T13:14:37.7833Z","shell.execute_reply":"2022-03-25T13:14:39.681841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_params1 = grid_params0.copy()\ngrid_params1.update({\n    'model__gamma': [0, 0.01, 0.05, 0.1],\n    'model__max_depth': [8, 10, 12],\n    'model__min_child_weight': [0.5, 1, 1.5, 3]\n})\n\ngrid_params1 = XGB_CVGridSearch(my_pipeline, X_train, y_train, grid_params1, verbose=1, vectorize_output=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T13:15:11.004532Z","iopub.execute_input":"2022-03-25T13:15:11.004829Z","iopub.status.idle":"2022-03-25T13:15:24.736459Z","shell.execute_reply.started":"2022-03-25T13:15:11.004778Z","shell.execute_reply":"2022-03-25T13:15:24.735703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_pipeline.set_params(**grid_params1)\nmy_pipeline.fit(X_train, y_train, model__verbose=False)\nprint(\"Accuracy: \", accuracy_score(y_val, my_pipeline.predict(X_val)))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T14:00:11.885643Z","iopub.execute_input":"2022-03-25T14:00:11.886438Z","iopub.status.idle":"2022-03-25T14:00:11.941942Z","shell.execute_reply.started":"2022-03-25T14:00:11.88639Z","shell.execute_reply":"2022-03-25T14:00:11.941252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=3>Once the model has been trained, we can use a **ROC curve** to select the best general threshold for it based on F1 scores.\n    \n<font size=3>The following function iterates through a list of thresholds and returns the one with the highest F1 score.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n\ndef GetROCCurve(estimator, X, y_true, thresholds):\n    tpr, fpr = [], []\n    best_f1 = 0\n    final_t = 0\n    \n    for t in thresholds:\n        preds = (estimator.predict_proba(X)[:, 1] >= t) * 1\n        cm = confusion_matrix(y_true, preds)\n        tn, fp, fn, tp = cm.flatten()\n\n        t_tpr = tp / (tp + fn)\n        t_fpr = fp / (fp + tn)\n\n        tpr.append(t_tpr)\n        fpr.append(t_fpr)\n        \n        f1 = f1_score(y_true, preds)\n        print('Accuracy: ', accuracy_score(y_true, preds), 'F1: ', f1, 'for threshold:', t)\n        \n        if f1 > best_f1:\n            best_f1 = f1\n            final_t = t\n\n    print('Best threshold: ', final_t)\n    plt.figure(figsize=(5,5))\n    plt.suptitle('Roc Curve')\n    plt.title('Best F1: ' + '{:.6f}'.format(best_f1))\n    plt.ylabel('TPR')\n    plt.xlabel('FPR')\n    plt.plot(fpr, tpr, alpha=0.75)\n    plt.scatter(fpr, tpr)\n    \n    \n    return best_f1","metadata":{"execution":{"iopub.status.busy":"2022-03-25T13:40:32.90661Z","iopub.execute_input":"2022-03-25T13:40:32.907175Z","iopub.status.idle":"2022-03-25T13:40:32.917128Z","shell.execute_reply.started":"2022-03-25T13:40:32.907133Z","shell.execute_reply":"2022-03-25T13:40:32.916229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshs = [0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1]\nthreshold = GetROCCurve(my_pipeline, X_train, y_train, threshs)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T13:41:03.802224Z","iopub.execute_input":"2022-03-25T13:41:03.802489Z","iopub.status.idle":"2022-03-25T13:41:04.136504Z","shell.execute_reply.started":"2022-03-25T13:41:03.80246Z","shell.execute_reply":"2022-03-25T13:41:04.135725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating Predictions","metadata":{}},{"cell_type":"markdown","source":"<font size=3>Now that we have all of our parameters set up, we can use the whole dataset for training and finally **generate our predictions** to be submitted.","metadata":{}},{"cell_type":"code","source":"my_pipeline.fit(X, y, model__verbose=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T14:00:23.476698Z","iopub.execute_input":"2022-03-25T14:00:23.477397Z","iopub.status.idle":"2022-03-25T14:00:23.5523Z","shell.execute_reply.started":"2022-03-25T14:00:23.47736Z","shell.execute_reply":"2022-03-25T14:00:23.551627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = (my_pipeline.predict_proba(X_test)[:, 1] >= threshold) * 1\npreds[:20]","metadata":{"execution":{"iopub.status.busy":"2022-03-25T13:46:01.470742Z","iopub.execute_input":"2022-03-25T13:46:01.471293Z","iopub.status.idle":"2022-03-25T13:46:01.486361Z","shell.execute_reply.started":"2022-03-25T13:46:01.471254Z","shell.execute_reply":"2022-03-25T13:46:01.485834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission = pd.DataFrame({'PassengerId': X_test.index, 'Survived': preds})\nmy_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T13:36:47.836755Z","iopub.execute_input":"2022-03-25T13:36:47.837604Z","iopub.status.idle":"2022-03-25T13:36:47.844967Z","shell.execute_reply.started":"2022-03-25T13:36:47.837563Z","shell.execute_reply":"2022-03-25T13:36:47.844232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission.value_counts(subset='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-03-25T13:56:21.150617Z","iopub.execute_input":"2022-03-25T13:56:21.151223Z","iopub.status.idle":"2022-03-25T13:56:21.158213Z","shell.execute_reply.started":"2022-03-25T13:56:21.151181Z","shell.execute_reply":"2022-03-25T13:56:21.157523Z"},"trusted":true},"execution_count":null,"outputs":[]}]}