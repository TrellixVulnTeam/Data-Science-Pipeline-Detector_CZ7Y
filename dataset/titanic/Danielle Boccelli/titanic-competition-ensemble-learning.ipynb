{"cells":[{"metadata":{},"cell_type":"markdown","source":"Visualizations created during the development of the model herein can be found here:\nhttps://www.kaggle.com/db102291/titanic-competition-visualization-w-seaborn","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nimport category_encoders as ce\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd \nimport random as rand","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Import training and testing data\ntrain_data = pd.read_csv(\"../input/titanic/train.csv\")\ntest_data = pd.read_csv(\"../input/titanic/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing values?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Which columns have missing values?\ndisplay(train_data.isnull().sum().sort_values(ascending=False))\ndisplay(test_data.isnull().sum().sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cabin letter\ntrain_data['Cabin_new'] = train_data['Cabin'].str[0]\ntest_data['Cabin_new'] = test_data['Cabin'].str[0]\n\n#Family size\ntrain_data['Fam_size'] = train_data['SibSp'] + train_data['Parch']\ntest_data['Fam_size'] = test_data['SibSp'] + test_data['Parch']\n\n#Title\ntrain_data['Title']=train_data.Name.str.extract('([A-Za-z]+)\\.')\ntest_data['Title']=test_data.Name.str.extract('([A-Za-z]+)\\.')\n\n#Sex-Class\ntrain_data['Sex_class'] = train_data['Sex'] + \"_\" + str(train_data['Pclass'])\ntest_data['Sex_class'] = test_data['Sex'] + \"_\" + str(test_data['Pclass'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imputation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fare\ntrain_data['Fare'].fillna(train_data['Fare'].median(), inplace=True)\ntest_data['Fare'].fillna(train_data['Fare'].mean(), inplace=True)\n\n#Age\ntrain_data['Age'].fillna(train_data['Age'].mean(), inplace=True)\ntest_data['Age'].fillna(train_data['Age'].mean(), inplace=True)\n\n#Cabin_new\ntrain_data['Cabin_new'].fillna('X', inplace=True)\ntest_data['Cabin_new'].fillna('X', inplace=True)\n\n#Embarked\ntrain_data['Embarked'].fillna('S', inplace=True) #most common value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setup for model\ny = train_data[\"Survived\"]\nfeatures = [\"Pclass\", \"Sex\", \"Parch\", \"SibSp\", \"Fam_size\", \"Embarked\", \"Fare\", \"Age\", \"Cabin_new\", \"Title\", \"Sex_class\"]\ncat_features = ['Sex', 'Embarked', 'Pclass', \"Cabin_new\", \"Title\", \"Sex_class\"]\n\nX = train_data[features]\nX_test = test_data[features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the count encoder\ncount_enc = ce.CountEncoder(cols=cat_features)\n\n# Learn encoding from the training set\ncount_enc.fit(X[cat_features])\n\n# Apply encoding to the train and validation sets as new columns\n# Make sure to add `_count` as a suffix to the new columns\ntrain_encoded = X.join(count_enc.transform(X[cat_features]).add_suffix('_count'))\ntest_encoded = X_test.join(count_enc.transform(X_test[cat_features]).add_suffix('_count'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the CatBoost encoder\ncb_enc = ce.CatBoostEncoder(cols=cat_features, random_state=126)\n\n# Learn encoding from the training set\ncb_enc.fit(X[cat_features], y)\n\n# Apply encoding to the train and validation sets as new columns\n# Make sure to add `_cb` as a suffix to the new columns\ntrain_encoded = X.join(cb_enc.transform(X[cat_features]).add_suffix('_cb'))\ntest_encoded = X_test.join(cb_enc.transform(X_test[cat_features]).add_suffix('_cb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Update X for Count Encoding\n#features_encoded = [\"Pclass_count\", \"Sex_count\", \"Fam_size\", \"Embarked_count\", \"Fare\", \"Age\", \"Sex_class_count\"]\n#X = train_encoded[features_encoded]\n#X_test = test_encoded[features_encoded]\n\n#Update X for CatBoost Encoding\nfeatures_encoded = [\"Pclass_cb\", \"Sex_cb\", \"Embarked_cb\", \"Fam_size\", \"Fare\", \"Age\", \"Sex_class_cb\"]\nX = train_encoded[features_encoded]\nX_test = test_encoded[features_encoded]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler() #to normalize data for neural net\n\nmodel_rf = RandomForestClassifier(random_state=126)\nmodel_xgb = XGBClassifier(random_state=126)\nmodel_mlp = MLPClassifier(random_state=126)\n\nparam_grid_rf = {\n    'model__n_estimators': [90, 110, 130],\n    'model__max_depth': [7, 10, 13],\n    'model__criterion': ['gini', 'entropy']}\n\nparam_grid_xgb = {\n    'model__n_estimators': [150, 180, 210],\n    'model__max_depth': [5, 8],\n    'model__learning_rate': [0.08, 0.1, 0.12]}\n\nparam_grid_mlp = {'model__hidden_layer_sizes': [(25, 25, 25), (50,50,50)],\n                  'model__activation': ['tanh', 'relu'],\n                  'model__solver': ['sgd', 'adam'],\n                  'model__alpha': [0.0001, 0.00001],\n                  'model__learning_rate': ['constant','adaptive']}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_pipeline = Pipeline(steps=[('model', model_rf)])\n\nsearch = GridSearchCV(my_pipeline, param_grid_rf, n_jobs=-1, verbose=10, cv=5)\nsearch.fit(X, y)\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_pipeline = Pipeline(steps=[('model', model_xgb)])\n\nsearch = GridSearchCV(my_pipeline, param_grid_xgb, n_jobs=-1, verbose=10, cv=5)\nsearch.fit(X, y)\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multi-layer Perceptron","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#MLP \nmy_pipeline = Pipeline(steps=[('preprocess', scaler), \n                              ('model', model_mlp)])\n\nsearch = GridSearchCV(my_pipeline, param_grid_mlp, n_jobs=-1, verbose=10, cv=5)\nsearch.fit(X, y)\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Voting Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Building a voting classifier \nmodel_rf = RandomForestClassifier(random_state=126, criterion='entropy', max_depth=10, n_estimators=110)\nmodel_xgb = XGBClassifier(random_state=126, learning_rate=0.1, max_depth=5, n_estimators=180)\nmodel_mlp = MLPClassifier(random_state=126, activation='tanh', alpha=1e-04, hidden_layer_sizes=(25, 25, 25), learning_rate='constant', solver='adam')\n\nmodel_vote = VotingClassifier(estimators=[('RF', model_rf), ('XGB', model_xgb), ('MLP', model_mlp)], voting='hard')\nmodel_vote.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model_vote.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}