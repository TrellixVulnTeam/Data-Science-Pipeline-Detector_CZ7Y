{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"font-family: 'Poppins', sans-serif; color: #ff7f50; font-size: 32px; text-align: center;\">Titanic view more detailed</h1>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 18px; font-weight: 500; text-align: center;\">I would like to touch on what I gained and how I see things through the \"titanic\".</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 18px; font-weight: 500; text-align: center;\">I've tried to make the graphs and other information easier to read by devised colors and percentage notation.  Please refer to this!</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 18px; font-weight: 500; text-align: center;\">I made a number of innovations in feature creation that other Notebooks have not done. In addition, we'll be digging deeper into the technologies that appears at the end than anywhere else!</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 18px; font-weight: 500; text-align: center;\">I will describe English and Japanese both in order to make it more accessible to a wider audience(for my own English study😊😊).</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 18px; font-weight: 500; text-align: center;\">Here are some of the many things I've learned. I will also update this notebook from time to time. If you find this notebook, please <b>DO UPVOTE</b> and <b>Comments!</b>!! It's my motivation!!</p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section1\"></a>\n<h3 style=\"font-family: 'Poppins', sans-serif; font-size: 29px; color: #ffa500\">Introduction</h3>\n<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">Titanic(RMS Titanic, Yoyal Mail Ship Titanic) was a British-flagged ocean liner built in the early 20th century. <br> She was the second of the Olympic-class passenger ships owned by the White Star Line, but on her maiden voyage, she struck an iceberg at midnight on April 14, 1912, and sank in the early hours of the following day, April 15. by wiki<br>\n    -----<br>\n    タイタニック（英語: RMS Titanic、ロイヤルメールシップ・タイタニック）は、20世紀初頭に建造されたイギリス船籍のオーシャン・ライナー。\nホワイト・スター・ライン社が保有するオリンピック級客船の2番船であったが、処女航海中の1912年4月14日深夜に氷山に衝突し、その際の損傷による浸水が原因となって翌15日未明に沈没した。 wikiより </p>","metadata":{}},{"cell_type":"markdown","source":"<h3 style = \"font-family: 'Poppins', sans-serif; font-size: 29px; color:#ffa500\">Loading Data</h3>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T11:02:29.654156Z","iopub.execute_input":"2021-10-25T11:02:29.654717Z","iopub.status.idle":"2021-10-25T11:02:29.66306Z","shell.execute_reply.started":"2021-10-25T11:02:29.654631Z","shell.execute_reply":"2021-10-25T11:02:29.662314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')\nsubmission_data = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\ndata = pd.concat([train_data, test_data], ignore_index = True, sort = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:35:57.05575Z","iopub.execute_input":"2021-08-28T16:35:57.056199Z","iopub.status.idle":"2021-08-28T16:35:57.10524Z","shell.execute_reply.started":"2021-08-28T16:35:57.056168Z","shell.execute_reply":"2021-08-28T16:35:57.104321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.tail(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 18px;\">※Since test_data is for predicting <b>Survived</b>, so <b>Survived</b> column does not exist.<br>-----<br>※テストデータは、<b>Survived</b>を予測するものなので、<b>Survived</b>カラムは存在しません。","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section2\"></a>\n<h3 style=\"font-family: 'Poppins', sans-serif; font-size: 29px; color: #ffa500\">brief description of each column</h3>\n\n<ol style = \"font-family: 'Poppins', sans-serif; font-size:16px;\">\n    <li>PassengerId -- passenger identifiation unique ID</li>\n    <li>Survived -- Survivor flag (0 = dead, 1 = survived)</li>\n    <li>Pclass -- ticket class:\n        <ul>\n            <li>1 = Upper Class</li>\n            <li>2 = Middle Class</li>\n            <li>3 = Lower Class</li>\n        </ul>\n    </li>\n    <li>Name -- as is</li>\n    <li>Sex -- as is</li>\n    <li>Age -- as is</li>\n    <li>SibSp -- number of siblings/spouses on Titanic</li>\n    <li>Parch -- number of parents/children on Titanic</li>\n    <li>Ticket -- ticket number</li>\n    <li>Fare -- price</li>\n    <li>Cabin -- Cabin number</li>\n    <li>Embarked -- port of departure\n        <ul>\n            <li>S = Southampton</li>\n            <li>C = Cherbourg</li>\n            <li>Q = Queenstown</li>\n        </ul>\n    </li>\n</ol>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section3\"></a>\n<h3 style=\"font-family: 'Poppins', sans-serif; font-size: 29px; color: #ffa500\">deep description of each column</h3>\n\n<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">For a deeper look, use pandas_Pprofiling.<br>-----<br>より深く見るために、pandas_profilingを使います。</p>","metadata":{}},{"cell_type":"code","source":"import pandas_profiling as pp\npp.ProfileReport(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the easy way\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T11:29:35.297233Z","iopub.execute_input":"2021-08-25T11:29:35.29762Z","iopub.status.idle":"2021-08-25T11:29:35.31395Z","shell.execute_reply.started":"2021-08-25T11:29:35.297582Z","shell.execute_reply":"2021-08-25T11:29:35.313176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section2\"></a>\n> <span>\n<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">\n   You can find more details above about <a href=\"#section2\">data status</a>.<br>-----\n<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">\n    上に、<a href=\"#section2\">データの状態</a>の詳細が記載されています。</p>\n    \n<ol style = \"font-family: 'Poppins', sans-serif; font-size:16px;\">\n    <strong>Categorial Features(質的変数):</strong>\n        <ul>\n            <li>Sex, Embarked</li>\n        </ul>\n    <strong>Ordinal Features(順序変数):</strong>\n        <ul>\n            <li>Pclass</li>\n        </ul>\n    <strong>Continuous Features(連続変数):</strong>\n        <ul>\n            <li>Age, Fare</li>\n        </ul>\n    <strong>Discrete Features(離散変数):</strong>\n        <ul>\n            <li>SibSp, Parch</li>\n        </ul><br>\n    ※※PassengerID is not used for this analysis because it is just an ID.<br>-----<br>\n    ※※PassengerIDは、ただのIDなので分析には使いません。\n</ol>\n</span>","metadata":{}},{"cell_type":"markdown","source":"<h4><b>What is Categorial Feature? -- 質的変数とは何ぞ？</b></h4>\n<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">a categorical variables are those that cannot generally be measured in terms of number or quantity, such as gender, occupation, marital status, etc.<br>-----<br>\n    質的変数とは、性別、職業、配偶者の有無など、一般に数や量で測れない変数を指します。 </p><br>\n<h4><b>What is Ordinal Feature? -- 順序変数とは何ぞ？</b></h4>\n<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">an ordinal variables are variable whose value interval has no meaning.<br>-----<br>\n    順序変数は、値の間隔に意味がない変数を指します。 </p><br>\n<h4><b>What is Continuous Feature? -- 連続変数とは何ぞ？</b></h4>\n<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">a continuous variable is a variable that takes a connected value and has an infinite number between the values.<br>-----<br>\n    連続変数とは、繋がった値をとる変数で、値と値の間に無限に取りうる値がある変数を指します。\n</p><br>\n<h4><b>What is Discrete Feature? -- 離散変数とは何ぞ？</b></h4>\n<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">a discrete variable is a variable that has no values between its values.<br>-----<br>\n    離散変数とは、値の間に値が存在しない変数を指します。 </p><br>","metadata":{}},{"cell_type":"markdown","source":"<h3 style = \"font-family: 'Poppins', sans-serif; font-size: 29px; color:#ffa500\">Exploratory Data Analysis and Data Cleaning where needed</h3>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section4\"></a>","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T08:29:07.210548Z","iopub.execute_input":"2021-08-25T08:29:07.210903Z","iopub.status.idle":"2021-08-25T08:29:07.226479Z","shell.execute_reply.started":"2021-08-25T08:29:07.210873Z","shell.execute_reply":"2021-08-25T08:29:07.225515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 17px;\">　From the above, <b>Age</b> and <b>Embarked</b> are missing.I will try to fix them.<br>-----<br>　上図より、<b>Age</b>と<b>Embarked</b>に、欠損が見られます。後で直します。</p>","metadata":{}},{"cell_type":"markdown","source":"<h2>How many Survived?</h2>","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport matplotlib.pylab as pylab\nimport matplotlib.cm as cm","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:15:09.885959Z","iopub.execute_input":"2021-08-29T05:15:09.886441Z","iopub.status.idle":"2021-08-29T05:15:10.002887Z","shell.execute_reply.started":"2021-08-29T05:15:09.886406Z","shell.execute_reply":"2021-08-29T05:15:10.00184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize = (18, 8))\ntrain_data['Survived'].value_counts().plot.pie(colors = ['#ff7f50', '#ff4500'], explode = [0, 0.1], autopct = '%1.1f%%', ax = ax[0],shadow = True)\nax[0].set_title('Survived %')\nax[0].set_ylabel('')\nsns.countplot('Survived', data = train_data, ax = ax[1], palette='Reds_r')\nax[1].set_title('Survived graph')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:36:07.321408Z","iopub.execute_input":"2021-08-28T16:36:07.32179Z","iopub.status.idle":"2021-08-28T16:36:07.635829Z","shell.execute_reply.started":"2021-08-28T16:36:07.321761Z","shell.execute_reply":"2021-08-28T16:36:07.635139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">It shows that more than half of the passengers died. In the data, only 38.4% of the 891 passengers survived…　To look deeper, we will explore the relationship with other features.<br>-----<br>\n    半分以上の乗客が亡くなられたことが分かります。データ上では、891人の内、38.4%しか生存できていません。もっと深く見るため、他の変数との関連を探ります。</p>","metadata":{}},{"cell_type":"markdown","source":"<h2><b>Sex</b> vs Survived</h2>","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(1, 3, figsize = (18, 8))\ndata[['Sex', 'Survived']].groupby(['Sex']).mean().plot.bar(ax = ax[0])\nax[0].set_title('Sex vs Survived')\nsns.countplot('Sex', hue = 'Survived', data = data, ax = ax[1])\nax[1].set_title('Sex : Survived count')\ndata.groupby('Sex')['Survived'].value_counts(normalize = True).mul(100).rename('percent').reset_index().pipe((sns.barplot, 'data'), x = 'Sex', y = 'percent', hue = 'Survived', ax = ax[2])\nax[2].set_title('Sex : Survived percent')\n# h1, l1 = ax[1].get_legend_handles_labels()\n# h2, l2 = ax[2].get_legend_handles_labels()\n# ao = ax[1].legend(h1 + h2, l1 + l1, loc = 'lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:36:10.642168Z","iopub.execute_input":"2021-08-28T16:36:10.642761Z","iopub.status.idle":"2021-08-28T16:36:11.134113Z","shell.execute_reply.started":"2021-08-28T16:36:10.642724Z","shell.execute_reply":"2021-08-28T16:36:11.133008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">From the graph, we found that females survived more than males(even though the number of males was higher…). This fact is going to be very useful for our analysis!! <br>Make a note of this.<br>-----<br>\n    グラフより、生き残った割合は、女性の方が男性よりも生き延びていることが分かりました（人数は男性の方が多いのに…）。この事実は、分析にとても使えそうです！！<br>メモメモφ(｡_｡*) </p>","metadata":{}},{"cell_type":"markdown","source":"<h2><b>Pclass</b> vs Survived</h2>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">Please recall. <b>Pclass</b> is an ordinal variable. When making comparisons based on ordinal variables, I recommended <b>cross tabulation</b>.<br>-----<br>\n    思い出してください。<b>Pclass</b>は順序変数でした。順序変数を基に比較する場合、自分は<b>クロス集計</b>をおすすめします。 </p>","metadata":{}},{"cell_type":"code","source":"pd.crosstab(train_data.Pclass, train_data.Survived, margins = True).style.background_gradient(cmap = 'Oranges')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:36:15.908244Z","iopub.execute_input":"2021-08-28T16:36:15.908575Z","iopub.status.idle":"2021-08-28T16:36:15.992952Z","shell.execute_reply.started":"2021-08-28T16:36:15.908548Z","shell.execute_reply":"2021-08-28T16:36:15.99206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataf, ax = plt.subplots(2, 2, figsize = (18, 12))\ndata['Pclass'].value_counts().plot.bar(color = ['#ff4500', '#ff6347', '#ff7f50'], ax = ax[0, 0])\nax[0, 0].set_title('count per Pclass')\nax[0, 0].set_ylabel('count')\nsns.countplot('Pclass', hue = 'Survived', data = data, ax = ax[0, 1], palette = 'Reds_r', order = data['Pclass'].value_counts().index)\n\nplass_percent_df = data.groupby('Pclass')['Survived'].value_counts(normalize = True).mul(100).rename('percent').reset_index()\ng = sns.barplot('Pclass', 'percent', hue = 'Survived', data = plass_percent_df, ax = ax[1, 1], palette = 'Reds_r')\ng._axes.set_ylim(0, 100)\nfor p in g._axes.patches:\n    txt = str(p.get_height().round(2)) + '%'\n    txt_x = p.get_x()\n    txt_y = p.get_height()\n    g._axes.text(txt_x + 0.03, txt_y + 1, txt)\n\nax[0, 1].set_title('Pclass count div Survived')\ndata.groupby('Pclass')['Survived'].mean().plot.pie(colors = [cm.spring(float(co) / 3) for co in range(3)], autopct = '%1.1f%%', ax = ax[1, 0], shadow = True, explode=(0.03, 0.03, 0.03), textprops={'size': 'xx-large'})\nax[1, 0].set_title('Survived rate per Pclass')\n# ax[1, 0].set_ylabel('Percent')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:36:16.677493Z","iopub.execute_input":"2021-08-28T16:36:16.677875Z","iopub.status.idle":"2021-08-28T16:36:17.371676Z","shell.execute_reply.started":"2021-08-28T16:36:16.677843Z","shell.execute_reply":"2021-08-28T16:36:17.370805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">From the upper left figure, we can see that most of the passengers are Pclass3. The upper right figure also shows that most of the dead are in Pclass3.<br>But since most of the passengers were originally Pclass3, let's look at the percentages.The bottom left figure shows that about <b>half</b> of the survivors are Pclass1, more than <b>35%</b> are in Pclass2. Less than <b>20%</b> are in Pclass3.<br>The lower right figure shows the breakdown by Pclass. Pclass3 is still noteworthy, as more than <b>75%</b> of the passengers in Pclass3 dead.(As I recall, cabin of Pclass3 had a Pclass1 cabin directly above it, and the door was locked.)\n    <br>-----<br>\n    左上の図より、ほとんどの乗客がPclass3であることが分かります。右上の図より、死者のほとんどがPclass3であることも確認できます。ですが、もともと乗客のほとんどがPclass3なので、パーセンテージを見てみましょう。左下の図より、生存者の約<b>半分</b>がPclass1の人で、<b>35%</b>がPclass2です。Pclass3の人は、<b>2割</b>にも達していません。また、右下の図より、Pclassごとの内訳が覗けます。やはり、注目すべきはPclass3です。Pclass3では<b>75%</b>以上の人が亡くなられています。位が上の人からレスキューされたということでしょう。。（確か、三等船室の場所は真上に一等船室があり、ドアがロックされていたはず。。）</p>","metadata":{}},{"cell_type":"code","source":"train_data.groupby('Pclass').count()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:36:26.210001Z","iopub.execute_input":"2021-08-28T16:36:26.210339Z","iopub.status.idle":"2021-08-28T16:36:26.229323Z","shell.execute_reply.started":"2021-08-28T16:36:26.210311Z","shell.execute_reply":"2021-08-28T16:36:26.228618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">Sex and Pclass respectively, we got some interesting analysis! Let's look a little deeper. Now, let's combine both.\n    <br>-----<br>\n    SexとPclassそれぞれで、面白い分析ができました！もう少し深く見ていきましょう。今度は、両方を組み合わせてみます。</p>","metadata":{}},{"cell_type":"markdown","source":"<h2><b>Pclass</b> - <b>Sex</b> vs Survived</h2>","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.Sex, data.Survived], data.Pclass, margins = True).style.background_gradient(cmap = 'Oranges')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:36:30.646908Z","iopub.execute_input":"2021-08-28T16:36:30.647398Z","iopub.status.idle":"2021-08-28T16:36:30.705674Z","shell.execute_reply.started":"2021-08-28T16:36:30.647367Z","shell.execute_reply":"2021-08-28T16:36:30.704934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">When you plot categorical features, <b>factorplot</b> is recommended. As shown in the figure below, it produces a very clear division.<br>-----<br>\n    質的変数をプロットする際は、<b>factorplot</b>がおすすめです。下図のように、とても分かりやすい分割が成されます。</p>","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', hue = 'Sex', data = data, palette = 'Reds')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:36:32.828119Z","iopub.execute_input":"2021-08-28T16:36:32.828477Z","iopub.status.idle":"2021-08-28T16:36:33.381757Z","shell.execute_reply.started":"2021-08-28T16:36:32.828432Z","shell.execute_reply":"2021-08-28T16:36:33.380703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">As we knew from out previous analysis, the survival rate for female passengers with Pclass1 is very high. Conversely, the survival rate for males in Pclass1 is less than 40%.<br>The amazing thing is that the survival rate of Pclass3 women is higher than that of Pclass1 man. Half of them survived. <br>In this case, we can see that the rescue prioritized women regardless of their location. The situation must have been such that they had to bet on the patience of the men.<br>-----<br>\n    これまでの分析でわかっていたことですが、女性でかつPclass1の乗客は、生存率がすごく高いです。逆に、Pclass1でも、男性の生存率は4割にも満たないことも分かります。<br>すごいのは、Pclass3の女性は、Pclass1の男性よりも生存率が高いことです。半分が生き残っています。<br>これに関しては、場所関係なく女性を優先してレスキューしたことが分かりますね。男性の忍耐に賭けないといけないほどの状況だったのでしょう。。</p>","metadata":{}},{"cell_type":"markdown","source":"<h2><b>Embarked</b> vs Survived</h2>","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Embarked', 'Survived', data = train_data, color = '#E2421F')\nfig = plt.gcf()\nfig.set_size_inches(5, 3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:36:39.992025Z","iopub.execute_input":"2021-08-28T16:36:39.992357Z","iopub.status.idle":"2021-08-28T16:36:40.353206Z","shell.execute_reply.started":"2021-08-28T16:36:39.99233Z","shell.execute_reply":"2021-08-28T16:36:40.352183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">We can see that the survival rate of S-port is remarkable low. Also, there is more than half of the survived passenger in C-port.<br>　Let's take a closer look.\n    <br>-----<br>\n    Sポートの生存率が顕著に低いのが分かりますね。また、Cポートでは半分以上の人が生き残っています。もう少し見てみましょう。</p>","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(2, 2, figsize = (20,15))\nsns.countplot('Embarked', data = data, ax = ax[0,0], palette = 'Reds_r')\nax[0,0].set_title('count per Embarked')\nsns.countplot('Embarked', hue = 'Sex', data = data, ax = ax[0,1], palette = 'Oranges_r')\nax[0,1].set_title('count per Embarked div Sex')\nplass_percent_df = data.groupby('Embarked')['Survived'].value_counts(normalize = True).mul(100).rename('percent').reset_index()\ng = sns.barplot('Embarked', 'percent', hue = 'Survived', data = plass_percent_df, ax = ax[1, 0], palette = 'spring')\ng._axes.set_ylim(0, 100)\n\nfor p in g._axes.patches:\n    txt = str(p.get_height().round(2)) + '%'\n    txt_x = p.get_x()\n    txt_y = p.get_height()\n    g._axes.text(txt_x + 0.105, txt_y + 1, txt)\nsns.countplot('Embarked', hue = 'Pclass', data = data, ax = ax[1,1], palette = 'Reds')\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace = 0.2, hspace = 0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:36:45.231988Z","iopub.execute_input":"2021-08-28T16:36:45.232348Z","iopub.status.idle":"2021-08-28T16:36:45.83437Z","shell.execute_reply.started":"2021-08-28T16:36:45.232319Z","shell.execute_reply":"2021-08-28T16:36:45.833621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">We can see that most of the passengers had board from S-port. And, most of them are Pclass 3 people. Previous analysis showed that most of the dead were from Pclass 3, so it make sense that most of the dead were from S-port, where most of them departed.\n<br>From the lower left figure, we can see that there are more survivors among those who boarded from C-port. <br>This also seems to be influenced by the large number of Pclass 1 people from the lower right figure. \n    <br>-----<br>　ほとんどの乗客が、Sポートから乗客していることが分かりますね。そして、そのほとんどがPclasse3の方たちです。前の分析で、死者のほとんどがPclass3の人であることが判明し、その人たちが多く出発したSポートからの死者が多いのは納得がいきます。<br>　左下の図から、Cから乗船した人たちは、生存者の方が多いことが分かりますね。これも、右下の図から、Pclass1の人が多いことが影響してそうです。<br>　ポートQはほとんどがPclass3の人です。ですが、ポートSとあまり生存率が変わっていないのは運が良かったのでしょうか。</p>","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', hue = 'Sex', col = 'Embarked', data = data, palette = 'Oranges')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:36:55.404963Z","iopub.execute_input":"2021-08-28T16:36:55.405338Z","iopub.status.idle":"2021-08-28T16:36:56.6632Z","shell.execute_reply.started":"2021-08-28T16:36:55.405303Z","shell.execute_reply":"2021-08-28T16:36:56.66228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">Here are some interesting findings.<br>\n1. First of all, regardless of Embarked, the survival rate for Pclass1 and 2 women is almost 1.<br>\n2. The few Pclass 1 and 2 people in Embark-Q, only the women survived and none of the men did.  We also found out that most of the men in Embark-Q died regardless of Pclass...<br>I wonder if it changes so much between males and females.\n    <br>-----<br>　面白いことが分かりますね。<br>\n    　１．先ず、Embarkedに関係なく、Pclass1と2の女性は生存率がほぼ1です。<br>\n    　２．ポートQにいる、数少ないPclass1,2の人たちは、女性だけ生き残り、男性は誰も生存できなかったことも分かりました。男性はほとんどが死んでしまったことも判明しました。<br>　ここまで男性と女性でかわってくると、少し疑問を抱いてしまうほどです。。</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">It gets too complicated, so it becomes a little difficult to see, let's dare to make it 'crosstab'.\n    <br>-----<br>　ここまで入り組むと、少し見づらくなってしまうので、あえてcrosstabにしてみましょう。</p>","metadata":{}},{"cell_type":"code","source":"pd.crosstab([train_data.Embarked, train_data.Pclass], [train_data.Sex, train_data.Survived], margins = True).style.background_gradient(cmap = 'Oranges')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:36:59.477332Z","iopub.execute_input":"2021-08-28T16:36:59.477662Z","iopub.status.idle":"2021-08-28T16:36:59.561722Z","shell.execute_reply.started":"2021-08-28T16:36:59.477634Z","shell.execute_reply":"2021-08-28T16:36:59.560906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">This one is easier to understand. The Embarked-S is by far the darker color, and the numbers also show that the men were more unlucky.\n    <br>-----<br>　こっちのほうが分かりやすいですね。圧倒的にEmbarked-Sが色が濃く、男性の方が不運だったことも数字で分かります。。</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">By the way, Embrked has a missing value.（See <a href = \"#section4\">here</a>）.<br>\nLet's take a look.\n    <br>-----<br>　そういえば、Embarkedには欠損値がありましたね。（<a href = \"#section4\">ここ</a>を参照）<br>　見てみましょう。</p>","metadata":{}},{"cell_type":"code","source":"data[data['Embarked'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:37:02.142451Z","iopub.execute_input":"2021-08-28T16:37:02.142784Z","iopub.status.idle":"2021-08-28T16:37:02.160435Z","shell.execute_reply.started":"2021-08-28T16:37:02.142757Z","shell.execute_reply":"2021-08-28T16:37:02.159319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">Both have a Pclass 1 and a Fare 80.0.　So, under the condition of Pclass = 1, we compare the median of Fare for each Embarked.\n    <br>-----<br>　共に、Pclassが1で、Fareが80.0です。なので、Pclassが1で、EmbarkedごとのFareの中央値を比較します。</p>","metadata":{}},{"cell_type":"code","source":"C = data[(data['Embarked'] == 'C') & (data['Pclass'] == 1)]['Fare'].median()\nS = data[(data['Embarked'] == 'S') & (data['Pclass'] == 1)]['Fare'].median()\nQ = data[(data['Embarked'] == 'Q') & (data['Pclass'] == 1)]['Fare'].median()\nprint(C, S, Q)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:37:04.148361Z","iopub.execute_input":"2021-08-28T16:37:04.148723Z","iopub.status.idle":"2021-08-28T16:37:04.162437Z","shell.execute_reply.started":"2021-08-28T16:37:04.148695Z","shell.execute_reply":"2021-08-28T16:37:04.16133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">Looking at this, C is the closest to 80.0.　So it seems best to fill the missing of Embarked with C.\n    <br>-----<br>　これをみると、80.0に一番近いのはCですね。なので、Embarkedの欠損はＣで埋めるのが一番よさそうです。</p>","metadata":{}},{"cell_type":"code","source":"data['Embarked'] = data['Embarked'].fillna('C')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:37:06.392469Z","iopub.execute_input":"2021-08-28T16:37:06.392797Z","iopub.status.idle":"2021-08-28T16:37:06.397172Z","shell.execute_reply.started":"2021-08-28T16:37:06.392769Z","shell.execute_reply":"2021-08-28T16:37:06.396464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Embarked'].isnull().any()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:37:08.706944Z","iopub.execute_input":"2021-08-28T16:37:08.707336Z","iopub.status.idle":"2021-08-28T16:37:08.713762Z","shell.execute_reply.started":"2021-08-28T16:37:08.707301Z","shell.execute_reply":"2021-08-28T16:37:08.712821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><b>Age</b> - <b>Pclass</b> - <b>Age</b> vs Survived</h2>","metadata":{}},{"cell_type":"code","source":"print('Age range: min =', train_data['Age'].min(), ', max =', train_data['Age'].max())","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:37:10.831282Z","iopub.execute_input":"2021-08-28T16:37:10.831633Z","iopub.status.idle":"2021-08-28T16:37:10.83812Z","shell.execute_reply.started":"2021-08-28T16:37:10.831603Z","shell.execute_reply":"2021-08-28T16:37:10.837089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 'section12'></a>\n<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">This time, we will use <b>violinplot</b>.　This is a graph that allows you to see the density of the distribution of data, and is ideal for drawing numerical data like \"Age\".<br>-----<br>\n今回は、<b>violinplot</b>を使います。これは、データの分布の密度を確認できるグラフで、数値データを描画するのにもってこいです。</p>","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize = (18, 8))\nsns.violinplot('Pclass', 'Age', hue = 'Survived', data = data, split = True, ax = ax[0], palette = 'spring')\nax[0].set_title('Pclass - Age vs Survived')\nax[0].set_yticks(range(0, 100, 10))\nsns.violinplot('Sex', 'Age', hue = 'Survived', data = data, split = True, ax = ax[1], palette = 'spring')\nax[1].set_title('Sex - Age vs Survived')\nax[1].set_yticks(range(0, 100, 10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:41:30.544659Z","iopub.execute_input":"2021-08-28T16:41:30.545065Z","iopub.status.idle":"2021-08-28T16:41:30.991397Z","shell.execute_reply.started":"2021-08-28T16:41:30.545021Z","shell.execute_reply":"2021-08-28T16:41:30.990457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section5\"></a>\n<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">What we can read from this graph is that:<br>\n<ul>\n <li>The survival rate of children under 10 years old is high regardless of their Pclass.\n <li>From the left figure, the survival rate of middle-age passengers in Pclass1 is high.(The right figure shows that women contribute this survival rate. ← This is what we found in the previous analysis.)\n <li>As Pclass get closer to 1, the age group goes up and the survival rate goes down.\n <li>Males have a higher survival rate for children. But women have a higher survival rate for the elderly.\n</ul><br>-----<br>\nこのグラフから読み取れるのは、<br>\n<ul>\n<li>10才以下の子供の生存率は、Pclassに関係なく高い。</li>\n<li>左図から、Pclass1にいる中年の乗客の生存率が高い。(右図を見ると、女性が生存率に大きく貢献している。←これは、前の分析でもわかったこと。)年齢が中年の人ほど、生存率が高いことが分かった。</li>\n<li>Pclassが1になるにつれ年齢層は高くなり、生存率は低くなる。</li>\n<li>男性の方が、子供の生存率が高い。だが、女性の方が高齢者の生存率は高い。</li>\n</ul>\n</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">\n　Don't forget. 　Please take a look <a href=\"#section4\">here</a>.　There were 263 missing values in this training data.　Normally, we would use the mean or median of the data to deal with this. <br>\n　But is it really safe to use the mean or median when there are 263 missing values?　In the avobe analysis, we know that <b>age</b> can be used as a major feature. 　We should processed with caution here.<br>　\nSo, let's take a look at the <b>Name</b> column.　There we find acronyms like 'Mr' and 'Mrs'!.　Let's use them.<br>\n　To do so, let's first look at the age of each acronym.<br>-----<br>　忘れてはいけません。<a href=\"#section4\">ここ</a>を見てください。今回のトレーニングデータには、<b>177</b>の欠損値がありました。普通は、データの平均や中央値を入れて対応します。<br>　ですが、本当に177も欠損値があるのに、平均値や中央値を入れてしまって大丈夫なのでしょうか。上の分析で、年齢も大きな特徴量として使えることが分かっています。ここは慎重に事を進めるべきです。<br>　そこで、Nameカラムを見てみましょう。そこには、MrやMrsのような頭語があるではありませんか！！これを使いましょう。<br>　そのために、まずは頭語それぞれの年齢を見てみましょう。</p>","metadata":{}},{"cell_type":"markdown","source":"<h2>The battle to fill in missing values in <b>Age</b></h2>","metadata":{}},{"cell_type":"code","source":"data['Initial'] = 0\nfor _ in data:\n    data['Initial'] = data.Name.str.extract(r'([A-Za-z]+)\\.')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:41:36.343407Z","iopub.execute_input":"2021-08-28T16:41:36.34377Z","iopub.status.idle":"2021-08-28T16:41:36.422839Z","shell.execute_reply.started":"2021-08-28T16:41:36.343719Z","shell.execute_reply":"2021-08-28T16:41:36.421648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2021-08-25T11:10:15.221171Z","iopub.execute_input":"2021-08-25T11:10:15.221538Z","iopub.status.idle":"2021-08-25T11:10:15.252068Z","shell.execute_reply.started":"2021-08-25T11:10:15.221508Z","shell.execute_reply":"2021-08-25T11:10:15.251071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size = 3>It's done!　Some acronyms exist for men and some for women, so let's look at them by gender.<br>-----<br>\n出来ています！頭語には、男性用と女性用のものも存在するので、男女別で見てみましょう。\n</font>","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Initial, data.Sex).T.style.background_gradient(cmap = 'Reds')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:41:38.65874Z","iopub.execute_input":"2021-08-28T16:41:38.659105Z","iopub.status.idle":"2021-08-28T16:41:38.715758Z","shell.execute_reply.started":"2021-08-28T16:41:38.659075Z","shell.execute_reply":"2021-08-28T16:41:38.715093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size = 3>We can see that Mr. and Master are used by men, while Mrs. and Miss are used by women.　However there are some initials that are misspelled or imply the same thing, so we will summarize them into five.<br>-----<br>MrやMasterは男性で、MrsやMissは女性で使われていることが分かりますね。ですが、いくつか誤記されたイニシャルだったり、同じものを含意するものも見受けられるので、今回は5つにまとめます。</font>","metadata":{}},{"cell_type":"code","source":"data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess', 'the Countess', 'Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],['Miss', 'Mrs', 'Mrs', 'Officer', 'Officer', 'Royalty', 'Royalty', 'Royalty', 'Master', 'Officer', 'Officer', 'Officer', 'Royalty', 'Royalty', 'Royalty'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:41:40.568614Z","iopub.execute_input":"2021-08-28T16:41:40.568975Z","iopub.status.idle":"2021-08-28T16:41:40.577845Z","shell.execute_reply.started":"2021-08-28T16:41:40.568944Z","shell.execute_reply":"2021-08-28T16:41:40.576952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 'section11'></a>\n<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">\n　Let's look at this by honorific title.　Mr. has the lowest survival rate by far.　And the survival rate of Mrs. is the highest.　There is that much difference, so we can use them as a feature.<br>-----<br>敬称別にみてみましょう。Mrの生存率がダントツで低いですね。また、Mrsが最も高いことも分かります。ここまで差があると、特徴量として使えそうです。\n    </p>","metadata":{}},{"cell_type":"code","source":"sns.barplot('Initial', 'Survived', data = data, palette = 'spring')\ndata.groupby(['Initial'])['Age'].mean()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:41:42.128293Z","iopub.execute_input":"2021-08-28T16:41:42.128622Z","iopub.status.idle":"2021-08-28T16:41:42.468113Z","shell.execute_reply.started":"2021-08-28T16:41:42.128594Z","shell.execute_reply":"2021-08-28T16:41:42.467108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-success\" role=\"alert\">\n  A Common Approach\n</div>","metadata":{}},{"cell_type":"markdown","source":"<font size = 3>Let's start with the general method first.　In each, we will look at the average.<br>-----<br>先ずは一般的な手法からです。それぞれで、平均を見ていきます。</font>","metadata":{}},{"cell_type":"code","source":"data.groupby('Initial')['Age'].mean()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:42:04.679966Z","iopub.execute_input":"2021-08-28T16:42:04.680542Z","iopub.status.idle":"2021-08-28T16:42:04.689829Z","shell.execute_reply.started":"2021-08-28T16:42:04.680487Z","shell.execute_reply":"2021-08-28T16:42:04.688975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size = 3>It seems to be filled in the right values better than the mean and median！(Let's hope it isn't a fake name..)<br>-----<br>平均値や中央値より、正しい値を埋められそうです！！（偽名でないことを祈りましょう( ˇωˇ人 )）</font>","metadata":{}},{"cell_type":"code","source":"data.loc[(data.Age.isnull()) & (data.Initial == 'Master'), 'Age'] = 5\ndata.loc[(data.Age.isnull()) & (data.Initial == 'Miss'), 'Age'] = 22\ndata.loc[(data.Age.isnull()) & (data.Initial == 'Mr'), 'Age'] = 33\ndata.loc[(data.Age.isnull()) & (data.Initial == 'Mrs'), 'Age'] = 36\ndata.loc[(data.Age.isnull()) & (data.Initial == 'Other'), 'Age'] = 46","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:42:25.383979Z","iopub.execute_input":"2021-08-28T16:42:25.384526Z","iopub.status.idle":"2021-08-28T16:42:25.398412Z","shell.execute_reply.started":"2021-08-28T16:42:25.384497Z","shell.execute_reply":"2021-08-28T16:42:25.397406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.Age.isnull().any()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:42:27.958276Z","iopub.execute_input":"2021-08-28T16:42:27.958615Z","iopub.status.idle":"2021-08-28T16:42:27.966211Z","shell.execute_reply.started":"2021-08-28T16:42:27.958587Z","shell.execute_reply":"2021-08-28T16:42:27.965139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize = (18, 8))\ndata[data['Survived'] == 0].Age.plot.hist(ax = ax[0], bins = 20, edgecolor = 'black', color = 'orange')\nax[0].set_title('Not Survived')\nax[0].set_xticks(range(0, 85, 5))\ndata[data['Survived'] == 1].Age.plot.hist(ax = ax[1], bins = 20, edgecolor = 'black', color = 'pink')\nax[1].set_title('Survived')\nax[1].set_xticks(range(0, 85, 5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:42:28.823926Z","iopub.execute_input":"2021-08-28T16:42:28.824304Z","iopub.status.idle":"2021-08-28T16:42:29.424507Z","shell.execute_reply.started":"2021-08-28T16:42:28.824272Z","shell.execute_reply":"2021-08-28T16:42:29.423477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size = 3>　When we fill in the missing values, we can see that there are still many middle-age passengers and a high survival rate for children.<br>\n　Oh, and if we look at the diagram on the right, the oldest passenger is still alive!　Let's check it out(；◔ิд◔ิ)!<br>-----<br>欠損値を埋めても、やはり中年の乗客が多く、子供の生存率が高いことが分かりますね。<br>おや、右図を見てみると、最高齢の乗客が生きている！？確認してみよう(；◔ิд◔ิ)。。</font>","metadata":{}},{"cell_type":"code","source":"data[data['Age'] == data['Age'].max()].Survived","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:42:39.878208Z","iopub.execute_input":"2021-08-28T16:42:39.878551Z","iopub.status.idle":"2021-08-28T16:42:39.887561Z","shell.execute_reply.started":"2021-08-28T16:42:39.878522Z","shell.execute_reply":"2021-08-28T16:42:39.886391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size = 3>She's alive!  What a relief( ；∀；)<br>-----<br>　御存命だ―――！！！良かった( ；∀；)</font>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-success\" role=\"alert\">\n  A Little More Technical\n</div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">\n　Let me get straight to the point.　When I used the Age variable supplemented by the average age by honorific title that we did above, the accuracy <b>dropped</b> for some reason.　I guess that the age of each honorific title varies.　(So simple tasks such as filling in missing values with the average value will be less accurate.)<br>-----<br>　結果から言います。自分は、上でやった敬称別平均年齢で補完したAge変数を使うと、なぜか精度が<b>落ちました</b>。敬称を一つとっても、年齢はバラバラなのでしょう。（逆に言うと、Age変数を平均値で埋めるなどの単純な作業は、より精度が落ちると思います。）</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">\n　So I didn't adopt it as a feature, but when I checked it, I found that there is a concept of <b>estimating the missing value</b> of Age using complete data without missing values(sounds amazing).　Let's give it a try!<br>-----<br>　なので特徴量には採用しなかったのですが、調べてみると、欠損値がない完全なデータを使って、Ageの<b>欠損値を推定する</b>という考え方があるそうです（すごそう）。やってみましょう！<b></b>\n    </p>","metadata":{}},{"cell_type":"code","source":"#First, initialize the data\n#先ずはデータの初期化\ntrain_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')\nsubmission_data = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\ndata = pd.concat([train_data, test_data], ignore_index = True, sort = False)\n\ndata['Embarked'] = data['Embarked'].fillna('C')\n\ndata['Initial'] = 0\nfor _ in data:\n    data['Initial'] = data.Name.str.extract(r'([A-Za-z]+)\\.')\n    \ndata['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess', 'the Countess', 'Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],['Miss', 'Mrs', 'Mrs', 'Officer', 'Officer', 'Royalty', 'Royalty', 'Royalty', 'Master', 'Officer', 'Officer', 'Officer', 'Royalty', 'Royalty', 'Royalty'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:07:47.406982Z","iopub.execute_input":"2021-08-29T05:07:47.407384Z","iopub.status.idle":"2021-08-29T05:07:47.549488Z","shell.execute_reply.started":"2021-08-29T05:07:47.40735Z","shell.execute_reply":"2021-08-29T05:07:47.548562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:43:17.791922Z","iopub.execute_input":"2021-08-28T16:43:17.792405Z","iopub.status.idle":"2021-08-28T16:43:17.822273Z","shell.execute_reply.started":"2021-08-28T16:43:17.792374Z","shell.execute_reply":"2021-08-28T16:43:17.82126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nage_df = data[['Age', 'Pclass', 'Sex', 'Parch', 'SibSp']]\n\n#one-hot encoding \nage_df = pd.get_dummies(age_df)\n\n#Since we are guessing the missing data in Age, we will use it as test data\nnot_null_age = age_df[age_df.Age.notnull()].values\nis_null_age = age_df[age_df.Age.isnull()].values\n#split train_data into explanation variable and target variable.\nX = not_null_age[:, 1:]\ny = not_null_age[:, 0]\n\n#create the inference model by RandomForest\nrfr = RandomForestRegressor(random_state = 0, n_estimators = 100, n_jobs = -1)\nrfr.fit(X, y)\n\n#using above model, we complement the Age feature in test data\npre_Ages = rfr.predict(is_null_age[:, 1:])\ndata.loc[(data.Age.isnull()), 'Age'] = pre_Ages","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:14:58.094326Z","iopub.execute_input":"2021-08-29T05:14:58.094853Z","iopub.status.idle":"2021-08-29T05:15:00.00872Z","shell.execute_reply.started":"2021-08-29T05:14:58.094801Z","shell.execute_reply":"2021-08-29T05:15:00.007362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.Age.isnull().any()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:15:00.791136Z","iopub.execute_input":"2021-08-29T05:15:00.79161Z","iopub.status.idle":"2021-08-29T05:15:00.801826Z","shell.execute_reply.started":"2021-08-29T05:15:00.791558Z","shell.execute_reply":"2021-08-29T05:15:00.800836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"facet = sns.FacetGrid(data[:len(train_data)], hue = 'Survived', aspect = 2, palette = 'Oranges')\nfacet.map(sns.kdeplot, 'Age', shade = True)\nfacet.set(xlim = (0, data.loc[:len(train_data), 'Age'].max()))\nfacet.add_legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:15:18.181087Z","iopub.execute_input":"2021-08-29T05:15:18.181553Z","iopub.status.idle":"2021-08-29T05:15:18.591598Z","shell.execute_reply.started":"2021-08-29T05:15:18.181519Z","shell.execute_reply":"2021-08-29T05:15:18.590698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Problem with <b>Age</b> feature in Machine Learning Models…</h2>","metadata":{}},{"cell_type":"markdown","source":"<font size = 3>　As mentioned above, Age is a continuous variable.　If we try to analyze a dataset with ages ranging from 0 to 80 like this time, we will end up with as many as 80 Age variable. This isn't computationally efficient, nor is it clean.<br>　\nAs you can see, there is no problem with quantitive variables.　For example, 'Sex' is fine because it is only either male or female.　The problem is that it is a continuous variable that can take many values.<br>　\nSo this time, we will prepare a new column called <b>Age_range</b> and apply it to each of the equally divided age groups.　This time, we will use 5 divisions.　The oldest was 80, so we will divide every 16 years.<br>-----<br>　先ほど触れたように、Ageは連続変数です。今回のように、0歳から80歳までいるデータセットで分析しようとすると、多くて80ものAge変数が存在してしまうことになります。これは、計算量的にも良くないですし、美しくありません。<br>　お分かりだと思いますが、質的変数では問題ありません。Sexだと、maleかfemaleのどちらかですから、大丈夫です。問題なのは、多くの値を取り得る連続変数であることです。<br>　なので、今回は<b>Age_range</b>という新しいカラムを用意して、等分された年齢層にそれぞれの乗客を当てはめていきます。今回は5分割でいきます。最高齢が80でしたので、16歳ごとの分割です。\n</font>","metadata":{}},{"cell_type":"code","source":"data['Age_range'] = 0\ndata.loc[(data['Age'] <= 16), 'Age_range'] = 0\ndata.loc[(data['Age'] > 16) & (data['Age'] <= 32), 'Age_range'] = 1\ndata.loc[(data['Age'] > 32) & (data['Age'] <= 48), 'Age_range'] = 2\ndata.loc[(data['Age'] > 48) & (data['Age'] <= 64), 'Age_range'] = 3\ndata.loc[(data['Age'] > 64), 'Age_range'] = 4","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:16:18.846228Z","iopub.execute_input":"2021-08-29T05:16:18.846672Z","iopub.status.idle":"2021-08-29T05:16:18.862416Z","shell.execute_reply.started":"2021-08-29T05:16:18.846638Z","shell.execute_reply":"2021-08-29T05:16:18.861241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Age_range'].value_counts().to_frame().style.background_gradient(cmap='Oranges')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:16:20.40967Z","iopub.execute_input":"2021-08-29T05:16:20.410078Z","iopub.status.idle":"2021-08-29T05:16:20.465502Z","shell.execute_reply.started":"2021-08-29T05:16:20.410043Z","shell.execute_reply":"2021-08-29T05:16:20.463974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 'section10'></a>\n<font size = 3>　Now, Let's check Age_range again, based on Pclass and Sex.<br>-----<br>　ここで、もう一度PclassやSexを踏まえ、Age_rangeを確認してみましょう。\n</font>","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Age_range', 'Survived', data = data, hue = 'Sex', col = 'Pclass', palette = 'Reds')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:16:23.748429Z","iopub.execute_input":"2021-08-29T05:16:23.748863Z","iopub.status.idle":"2021-08-29T05:16:25.632241Z","shell.execute_reply.started":"2021-08-29T05:16:23.748829Z","shell.execute_reply":"2021-08-29T05:16:25.630679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500; text-align: left;\">　As we expected, the overall survival rate for women and children is high, while Pclass 3 remains low.　It is sad to see that the survival rate for men becomes lower as they move up in age group.<br>-----<br>　やはり、全体的に見ても女性や子供の生存率は高いですね。Pclass3は依然と低いのが見て取れます。男性が年齢層を上げるにつれて生存率が低くなるのか悲しいです。</p>","metadata":{}},{"cell_type":"markdown","source":"<h2>Children are not to be taken on board <b>alone</b></h2>","metadata":{}},{"cell_type":"code","source":"len(data[data['Age'] <= 10])","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:45:39.233955Z","iopub.execute_input":"2021-08-28T16:45:39.234454Z","iopub.status.idle":"2021-08-28T16:45:39.240529Z","shell.execute_reply.started":"2021-08-28T16:45:39.234422Z","shell.execute_reply":"2021-08-28T16:45:39.239788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　This shows that there are 89 children under the age of 10.　It is impossible for a child  of this age to board the Titanic alone.　They must have boarded with their parents. If I were a parent, I would definitely give priority to my child.　I'm sure others do too.　This may have a significantly impact on the survival rate. Let's take a deeper look.<br>　If we look at the <a href = '#section2'>columns</a>, we can see <b>SibSp</b> and <b>Parch</b>.　These represent the number of siblings, spouses, and parents.　This seems to be useful.　We will use this to create a new variable <b>Family_Size</b>.<br>-----<br>　これをみると、10歳以下の子供は89人もいます。この年齢で、一人でTitanicに乗車するのはありえません。親と一緒に乗車したと思われます。もし自分が親ならば、絶対に子供を優先して助けます。他の人もそうだと思います。これは、生存率に大きな影響をあたえるのではないでしょうか。もう少し深く見てみましょう。<br>　<a href = '#section2'>カラム</a>をみると、<b>SibSp</b>と<b>Parch</b>というものがあります。兄弟や配偶者、親の数を表します。これは使えそうです。これをつかって、新しい変数<b>Family_Size</b>を作ります。\n</p>","metadata":{}},{"cell_type":"code","source":"data['Family_Size'] = 0\ndata['Family_Size'] = data['SibSp'] + data['Parch'] + 1#this 1 is the person himself.","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:16:30.2262Z","iopub.execute_input":"2021-08-29T05:16:30.226632Z","iopub.status.idle":"2021-08-29T05:16:30.235663Z","shell.execute_reply.started":"2021-08-29T05:16:30.22659Z","shell.execute_reply":"2021-08-29T05:16:30.23349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　On the other hand, if they are alone, what happens to their survival rate?　To analyze this as well, we will create an <b>Alone</b> variable.<br>-----<br>　逆に一人だと、生存率はどうなるんでしょう？？<br>　これも分析するために、<b>Alone</b>変数を作ってみます。\n</p>","metadata":{}},{"cell_type":"code","source":"data['Alone'] = 0\ndata.loc[data.Family_Size == 1, 'Alone'] = 1","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:16:32.446884Z","iopub.execute_input":"2021-08-29T05:16:32.447301Z","iopub.status.idle":"2021-08-29T05:16:32.45447Z","shell.execute_reply.started":"2021-08-29T05:16:32.447266Z","shell.execute_reply":"2021-08-29T05:16:32.453394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:45:54.369726Z","iopub.execute_input":"2021-08-28T16:45:54.370238Z","iopub.status.idle":"2021-08-28T16:45:54.402756Z","shell.execute_reply.started":"2021-08-28T16:45:54.370206Z","shell.execute_reply":"2021-08-28T16:45:54.402119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize = (18, 8))\nsns.barplot('Family_Size', 'Survived', data = data, ax = ax[0], palette = 'spring')\nax[0].set_title('Family_Size vs Survived')\nsns.barplot('Alone', 'Survived', data = data, palette = 'Oranges_r')\nax[1].set_title('Alone vs Survived')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:16:35.594757Z","iopub.execute_input":"2021-08-29T05:16:35.595147Z","iopub.status.idle":"2021-08-29T05:16:36.30325Z","shell.execute_reply.started":"2021-08-29T05:16:35.595112Z","shell.execute_reply":"2021-08-29T05:16:36.301634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　Family_Size, Alone =１ represents one person. It turns out that the survival rate decreases when they are alone.<br>　On the other hand, it turns out that the survival rate is higher in a general group such as a family of four.　I guess it is the family bond. However when the number of family members exceeds four, the survival rate decreases as the number of family increases. <br>　So, let's divide them into groups based on the survival rate according to the number of family members.<br>-----<br>　Family_Size, Alone =１が一人です。一人だと、生存率は下がることが分かりました。<br>　逆に、4人家族などの一般的なまとまりで、生存率が高くなることが分かりました。家族の絆でしょうか。ですが、4を超えると、人数が増えるにつれて生存率が下がっています。なので、家族人数による生存率でグループに分けましょう。\n</p>","metadata":{}},{"cell_type":"code","source":"def Family_label(s):\n    if (s >= 2) & (s <= 4):\n        return 2\n    elif ((s >= 5) & (s <= 7)) | (s == 1):\n        return 1\n    elif (s > 7):\n        return 0\n    \ndata['Family_label'] = data['Family_Size'].apply(Family_label)\nsns.barplot('Family_label', 'Survived', data = data, palette = 'Oranges')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:16:40.268492Z","iopub.execute_input":"2021-08-29T05:16:40.268865Z","iopub.status.idle":"2021-08-29T05:16:40.553356Z","shell.execute_reply.started":"2021-08-29T05:16:40.268835Z","shell.execute_reply":"2021-08-29T05:16:40.551542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　It is clearly devided.　Let's take a look at 'Alone'.<br>-----<br>　綺麗に分けられてますね。<br>　Aloneも見てみましょう。\n</p>","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Alone', 'Survived', data = data, hue = 'Sex', col = 'Pclass', palette = 'Reds')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:16:44.376899Z","iopub.execute_input":"2021-08-29T05:16:44.377343Z","iopub.status.idle":"2021-08-29T05:16:45.555356Z","shell.execute_reply.started":"2021-08-29T05:16:44.3773Z","shell.execute_reply":"2021-08-29T05:16:45.55397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　We found out that regardless of Pclass or Sex, it is dangerous to be alone.　To be honest, I had expected that it would be easier to escape when they are alone, but it turns out that just having someone who lead can make all the difference.<br>-----<br>　PclassやSexに関係なく、一人は危ないということが分かりました。正直、自分は一人の方が逃げやすいと予想していたのですが、やはり先導してくれる人がいるだけで、ここまで結果が変わってくるのですね。。\n</p>","metadata":{}},{"cell_type":"markdown","source":"<h3>By the way, what about <b>SibSp</b> and <b>Parch</b>?</h3>","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize = (18, 8))\nsns.barplot('SibSp', 'Survived', data = train_data, ax = ax[0], palette = 'Oranges_r')\nax[0].set_title('SibSp vs Survived')\nsns.barplot('Parch', 'Survived', data = train_data, ax = ax[1], palette = 'spring')\nax[1].set_title('Parch vs Survived')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:16:49.281861Z","iopub.execute_input":"2021-08-29T05:16:49.28227Z","iopub.status.idle":"2021-08-29T05:16:50.079096Z","shell.execute_reply.started":"2021-08-29T05:16:49.282211Z","shell.execute_reply":"2021-08-29T05:16:50.077805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　\n　After all, looking at the both SibSp and Parch, the survival rate for alone is low.　Always should be with someone.<br>-----<br>\n　やはり、<b>SibSp</b>と<b>Parch</b>両方を見ても、ひとりの生存率は低いですね。<br>　常にだれかと一緒にいましょう。´-_ゝ-)´-_ゝ-)\n</p>","metadata":{}},{"cell_type":"markdown","source":"<h2>Let's dig a little deeper into <b>Name</b> feature.</h2>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">\n　Name contains other interesting information besides the honorific title.　It's the last name.<br>　The conventional feature like SibSp and Parch cannot reveal family relationship beyond the third degree.　Therefore, we try to analyze them by using surnames.<br>-----<br>　Nameには、敬称以外にも面白い情報が入っています。苗字です。<br>　今までの特徴量SibSpやParchでは、3親等以降の家族関係がわかりません。そこで、苗字を使うことで分析を試みます。\n    </p>","metadata":{}},{"cell_type":"code","source":"data['Surname'] = data['Name'].map(lambda name:name.split(',')[0].strip())","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:16:54.677235Z","iopub.execute_input":"2021-08-29T05:16:54.677671Z","iopub.status.idle":"2021-08-29T05:16:54.687043Z","shell.execute_reply.started":"2021-08-29T05:16:54.677633Z","shell.execute_reply":"2021-08-29T05:16:54.685334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\nf, ax = plt.subplots(1, 1, figsize = (12, 8))\nd__ = {k: v for k, v in dict(data['Surname'].value_counts()).items() if v >= 2}\ny__ = d__.values()\nc__ = collections.Counter(y__)\nlabels_, values_ = zip(*c__.items())\nindexes = np.arange(len(labels_))\nplt.bar(indexes, values_, 1, color = ['#E2421F'])\nax.set_title('num of people who have the same ticket')\nplt.xticks(indexes, labels_)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:16:58.29184Z","iopub.execute_input":"2021-08-29T05:16:58.292292Z","iopub.status.idle":"2021-08-29T05:16:58.490713Z","shell.execute_reply.started":"2021-08-29T05:16:58.292254Z","shell.execute_reply":"2021-08-29T05:16:58.489479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">\n　As we can see above, there are many people who have the same last name.　As with the features we've seen so far, let's take a look at many of them.<br>-----<br>　上の様に、同じ苗字を持っている人がたくさんいます。今までの特徴量の様に、色々と見てみましょう。\n    </p>","metadata":{}},{"cell_type":"code","source":"#同じ苗字の出現頻度をカウント\ndata['family_sur_num'] = data['Surname'].map(data['Surname'].value_counts())\ndata","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:17:01.907861Z","iopub.execute_input":"2021-08-29T05:17:01.908334Z","iopub.status.idle":"2021-08-29T05:17:01.962139Z","shell.execute_reply.started":"2021-08-29T05:17:01.908294Z","shell.execute_reply":"2021-08-29T05:17:01.960416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">\n　Look at <a href = #section10>here</a>, we found the following.<br><ul>\n<li>There are tendencies that women or younger age have higher survival rate.\n<li>Men's survival rate tend to decrease as the age increases.</ul><br>\n　So we will do a group analysis of these two using their surnames.<br>-----<br>\n<a href = #section10>ここ</a>をみると、\n<ul>\n<li>女性でまたは年齢が低いと生存率が高い傾向にある</li>\n<li>男性で年齢が重なると生存率が低くなる傾向にある</li>\n    <br>ことがわかっていましたよね。そこで、この二つを苗字を使ったグループ分析を行います。\n</ul>    \n</p>","metadata":{}},{"cell_type":"code","source":"#data['family_sur_num'] >= 2 represents the duplications of surnames.\nFemale_Child_Group = data.loc[(data['family_sur_num'] >= 2) & ((data['Age'] <= 16) | (data['Sex'] == 'female'))]\nMale_Adult_group = data.loc[(data['family_sur_num'] >= 2) & (data['Age'] > 16) & (data['Sex'] == 'male')]\nFemale_Child_mean = Female_Child_Group.groupby('Surname')['Survived'].mean()\nMale_Adult_mean = Male_Adult_group.groupby('Surname')['Survived'].mean()\nprint(Female_Child_mean.value_counts())\nprint('-' * 28)\nprint(Male_Adult_mean.value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:17:08.106083Z","iopub.execute_input":"2021-08-29T05:17:08.106518Z","iopub.status.idle":"2021-08-29T05:17:08.131854Z","shell.execute_reply.started":"2021-08-29T05:17:08.106483Z","shell.execute_reply":"2021-08-29T05:17:08.130122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">\n　We found something interesting.　As well as second-degree kinship represented by SibSp and Parch, there are new findings in relation about beyond third degree kinship represented by surname.<br>　\nFirst, there are 113 groups that are 16 years old or younger or female with a 100% survival rate, while only 32 groups have a 0% survival rate.<br>　\n　Also, there are 115 groups that are 16 years old or older and male with 0% survival rate, while 21 groups have a 100% survival rate.\n<br>\n<br>　This means that there are certain surnames that have the opposite fate from the majority of groups.<br>-----<br>　面白いことが分かりました。SibSpやParchで表される2親等と同じく、Surnameで表される3親等以降の関連でも、新しい発見が出ました。<br>　先ず、16才以下または女性のグループは、113グループと多くが生存率100%な一方で、32グループに限っては生存率0%です。<br>　また、16才を超えかつ男性のグループは、115グループと多くが生存率0%な一方で、21グループに限っては生存率100%です。<br>　つまり、とある名字を持つと、大多数のグループとは逆の運命をたどってしまう苗字があるということです。\n</p>","metadata":{}},{"cell_type":"code","source":"Dead_List = set(Female_Child_mean[Female_Child_mean.apply(lambda x:x == 0)].index)\nSurvive_List = set(Male_Adult_mean[Male_Adult_mean.apply(lambda x:x == 1)].index)\nprint(Dead_List)\nprint(Survive_List)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:17:11.348995Z","iopub.execute_input":"2021-08-29T05:17:11.349359Z","iopub.status.idle":"2021-08-29T05:17:11.359502Z","shell.execute_reply.started":"2021-08-29T05:17:11.349326Z","shell.execute_reply":"2021-08-29T05:17:11.358401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 16px;\">\n　Above are the groups with surnames that had opposite fates.　This means that when we make predictions with a test data, it will be more accurate if we always have passengers with the same surname guessed as <b>alive or dead</b>.　So, we will rewrite the test data to make it so.<br>　\nBring in the appropriate gender and age from <a href = '#section10'>here</a> and <a href = '#section12'>here</a>, and the appropriate honorific title from <a href = '#section11'>here</a>.<br>-----<br>　上が、それぞれ逆の運命をたどった苗字のグループです。ということは、テストデータで予測するときも、同じ苗字を持つ乗客を、かならず<b>生存・死亡</b>と推測してもらったほうが、精度が上がりそうです。なので、そうなるためにテストデータの書き換えを行います。<br>　<a href = '#section10'>ここ</a>と<a href = '#section12'>ここ</a>より適切な性別と年齢を、<a href = '#section11'>ここ</a>より適切な敬称を持ってきます。</p>","metadata":{}},{"cell_type":"code","source":"#それぞれのリストを反映させます。\ndata.loc[(data['Survived'].isnull()) & (data['Surname'].apply(lambda x:x in Dead_List)), ['Sex', 'Age', 'Initial']] = ['male', 28.0, 'Mr']\ndata.loc[(data['Survived'].isnull()) & (data['Surname'].apply(lambda x:x in Survive_List)), ['Sex', 'Age', 'Initial']] = ['female', 5.0, 'Mrs']","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:17:15.347673Z","iopub.execute_input":"2021-08-29T05:17:15.348053Z","iopub.status.idle":"2021-08-29T05:17:15.364016Z","shell.execute_reply.started":"2021-08-29T05:17:15.348024Z","shell.execute_reply":"2021-08-29T05:17:15.362883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In comment, I had a question about above section. If you had trouble understanding the data rewriting above, please take a look at the Comment!<br>-----<br>\nCommentの方に、質問をしてくださった方がいました。もし上のデータ書き換えが分かりにくかった場合は、Commentの方を見てみてください！","metadata":{}},{"cell_type":"markdown","source":"<h2><b>Fare</b> is also continuous!</h2>","metadata":{}},{"cell_type":"code","source":"print('Fare range: min =', data['Fare'].min(), ', max =', data['Fare'].max())","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:46:50.91504Z","iopub.execute_input":"2021-08-28T16:46:50.915381Z","iopub.status.idle":"2021-08-28T16:46:50.921477Z","shell.execute_reply.started":"2021-08-28T16:46:50.915353Z","shell.execute_reply":"2021-08-28T16:46:50.920665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　What does zero mean?　I think it's like a free trial. I wonder if there's a tradition like that for maiden voyages..<br>　\nWhen it comes to money-related matters, Pclass is the place to be, so let's see what's related.<br>-----<br>　0ってなんだろう？？無料で体験みたいなことデスカネ。。処女航海にはそういうしきたりがあるんですかね…<br>　お金関連といえば<b>Pclass</b>ですから、関連を見てみましょう。\n</p>","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(1, 3, figsize = (18, 8))\nsns.distplot(data[data['Pclass'] == 1].Fare, ax = ax[0])\nax[0].set_title('Fares in Pclass1')\nsns.distplot(data[data['Pclass'] == 2].Fare, ax = ax[1])\nax[1].set_title('Fares in Pclass2')\nsns.distplot(data[data['Pclass'] == 3].Fare, ax = ax[2])\nax[2].set_title('Fare in Pclass3')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:17:21.692967Z","iopub.execute_input":"2021-08-29T05:17:21.693328Z","iopub.status.idle":"2021-08-29T05:17:22.357281Z","shell.execute_reply.started":"2021-08-29T05:17:21.693289Z","shell.execute_reply":"2021-08-29T05:17:22.356136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　Pclass has a large distribution especially.　It is concentrated in the 0~100 range, but some people have more than 500.<br>　\nNow, Fare also had a missing value.<br>-----<br>　Pclass1は特に、分布が大きいですね。0~100のところに集中してはいますが、500を超えている人もいます。<br>　さて、Fareにも欠損値がありました。\n</p>","metadata":{}},{"cell_type":"code","source":"data.loc[data['Fare'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:17:25.993671Z","iopub.execute_input":"2021-08-29T05:17:25.994369Z","iopub.status.idle":"2021-08-29T05:17:26.022414Z","shell.execute_reply.started":"2021-08-29T05:17:25.994328Z","shell.execute_reply":"2021-08-29T05:17:26.020995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　Since the missing value of Fare are male with Pclass3 and Embarked-S, so let's fill in the medians of Fare that match this condition.（In this case, the number of missing values is small, so the process is simple.）<br>-----<br>　Fareの欠損値は、Pclassが3で男性、EmbarkedがSなので、それにマッチするFareのメジアンを埋めましょう。(今回は欠損の数が少ないので、処理は簡単目にしました。)\n</p>","metadata":{}},{"cell_type":"code","source":"fare = data.loc[(data['Embarked'] == 'S') & (data['Pclass'] == 3) & (data['Sex'] == 'male'), 'Fare'].median()\ndata['Fare'] = data['Fare'].fillna(fare)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:17:36.765093Z","iopub.execute_input":"2021-08-29T05:17:36.765535Z","iopub.status.idle":"2021-08-29T05:17:36.778508Z","shell.execute_reply.started":"2021-08-29T05:17:36.765496Z","shell.execute_reply":"2021-08-29T05:17:36.776455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　For the same reason as Age, Fare is also continuous variable.　In the case of Age, we devided it by the value we decided because we can predict the approximate physical strength of a person.　However, in the case of Fare, it must be difficult to divide without prior knowledge.<br>　\nSo, we use <b>qcut</b> in pandas.　This will do the binning process so that the number of elements in each bin is equal.　In this case, let's split it into four parts.<br>　It may sound persistent, but <b>deal with the missing values and then do the binning process.</b><br>-----<br>　Ageと同じ理由で、Fareも連続変数です。Ageの場合、値ごとでおおよその人の体力などが予想できるので、自分で決めた値で分割しました。ですが、Fareの場合、それだけで何かを予測するのは難しいと思います。そこで、pandasにあるqcutを使います。<br>　これは、各ビンに含まれる要素数が等しくなるようにビニング処理をしてくれます。今回は、4分割にしてみましょう。<br>　しつこいようですが、<b>欠損値を処理してから、ビニング処理を行いましょう。</b>\n</p>","metadata":{}},{"cell_type":"code","source":"data['Fare'].isnull().any()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:17:41.431083Z","iopub.execute_input":"2021-08-29T05:17:41.431488Z","iopub.status.idle":"2021-08-29T05:17:41.437721Z","shell.execute_reply.started":"2021-08-29T05:17:41.431452Z","shell.execute_reply":"2021-08-29T05:17:41.437022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Fare_range'] = pd.qcut(data['Fare'], 4)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:17:43.341939Z","iopub.execute_input":"2021-08-29T05:17:43.342479Z","iopub.status.idle":"2021-08-29T05:17:43.359139Z","shell.execute_reply.started":"2021-08-29T05:17:43.342447Z","shell.execute_reply":"2021-08-29T05:17:43.357894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot('Fare_range', data = data, palette='Reds_r')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:17:44.032248Z","iopub.execute_input":"2021-08-29T05:17:44.032859Z","iopub.status.idle":"2021-08-29T05:17:44.190345Z","shell.execute_reply.started":"2021-08-29T05:17:44.03282Z","shell.execute_reply":"2021-08-29T05:17:44.189435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.groupby(['Fare_range'])['Survived'].mean().to_frame().style.background_gradient(cmap = 'Oranges')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:17:46.283141Z","iopub.execute_input":"2021-08-29T05:17:46.283799Z","iopub.status.idle":"2021-08-29T05:17:46.308091Z","shell.execute_reply.started":"2021-08-29T05:17:46.283753Z","shell.execute_reply":"2021-08-29T05:17:46.306421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　From the above figure, we can see that as Fare_range increases, the survival rate does.  As with Pclass, we can see that the money has a lot to do with survival. This could also be used for analysis.<br>\n　Now that we have found a boundary that can be devided into the same number of elements by qcut, let's transform it in the same way as <b>Age_range</b>.<br>-----<br>　上図を見る限り、Fare_rangeが増えるにつれて、生存率も高くなっていることが分かります。Pclassと同じで、お金は生存に大きく関係していることが分かります。これも分析に使えそうです。qcutによって同じ要素数に分けられる境界を見つけれたので、Age_rangeと同じように変換しましょう。\n</p>","metadata":{}},{"cell_type":"code","source":"data['Fare_cat'] = 0\ndata.loc[data['Fare'] <= 7.896, 'Fare_cat'] = 0\ndata.loc[(data['Fare'] > 7.896) & (data['Fare'] <= 14.454), 'Fare_cat'] = 1\ndata.loc[(data['Fare'] > 14.454) & (data['Fare'] <= 31.275), 'Fare_cat'] = 2\ndata.loc[(data['Fare'] > 31.275) & (data['Fare'] <= 513), 'Fare_cat'] = 3","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:17:49.33653Z","iopub.execute_input":"2021-08-29T05:17:49.336969Z","iopub.status.idle":"2021-08-29T05:17:49.352603Z","shell.execute_reply.started":"2021-08-29T05:17:49.336931Z","shell.execute_reply":"2021-08-29T05:17:49.350882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Fare_cat', 'Survived', data = data, hue = 'Sex', palette = 'Reds')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:17:49.474442Z","iopub.execute_input":"2021-08-29T05:17:49.47497Z","iopub.status.idle":"2021-08-29T05:17:50.10403Z","shell.execute_reply.started":"2021-08-29T05:17:49.47493Z","shell.execute_reply":"2021-08-29T05:17:50.102449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　For both males and females, the survival rate is highest for those in the highest Fare class.  Interestingly, the survival rate is steadly increasing for males this time.<br>　With the current processing, Fare can now be incorporated into the model in the same way as Age, so that's OK!!  I'm sure it will be a useful feature for us!<br>-----<br>　男性も女性も、一番高いFareクラスの人たちの生存率が一番高いです。面白いことに今回は、男性の方が順調に生存率が上がっていますね。<br>\n　今の処理で、FareもAgeと同じようにモデルに組み込むことができるようになったので、解決です。きっと、役に立つ特徴となってくれるに違いありません。 \n</p>","metadata":{}},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:48:04.621061Z","iopub.execute_input":"2021-08-28T16:48:04.621401Z","iopub.status.idle":"2021-08-28T16:48:04.670589Z","shell.execute_reply.started":"2021-08-28T16:48:04.621372Z","shell.execute_reply":"2021-08-28T16:48:04.669945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　Now that we have most of the variables, let's take a look at the correlation between them.<br>-----<br>　大体の変数がそろったところで、一度変数間の相関を見てみましょう。\n</p>","metadata":{}},{"cell_type":"code","source":"sns.heatmap(data.corr(), annot = True, cmap = 'Oranges', linewidths = 0.2, annot_kws = {'size': 20})\nfig = plt.gcf()\nfig.set_size_inches(18, 15)\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:17:56.414305Z","iopub.execute_input":"2021-08-29T05:17:56.414836Z","iopub.status.idle":"2021-08-29T05:17:57.832475Z","shell.execute_reply.started":"2021-08-29T05:17:56.414801Z","shell.execute_reply":"2021-08-29T05:17:57.830973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　From the above figure, we can see the correlation between each feature.<br>　\nTo closer the value is to 1, the more positive correlation is, and the closer it is to -1, the more negative correlation is.<br>　\nSince we create Family_size from SibSp and Parch, Fare_cat from Fare, etc., there is a strong correlation between them.<br>-----<br>　上図より、それぞれの特徴量の相関が見れます。<br>　1に近ければ近いほど、正の相関が、-1に近ければ近いほど負の相関があります。<br>　SibSpやParchからFamily_Sizeを、FareからFare_cat等を作っているので、それらに相関が強く表れています。\n</p>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-success\" role=\"alert\">\n  <h2>Are you sure you want to do this?…</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　(<b>Causion!</b>)\n　Below is the list of things that I thought might be useful when conducting the analysis.            It is not necessarily useful for improving accuracy, but please let me share them.<br>-----<br>　(<b>注意！！</b>)<br>　分析を行うにあたり、こういう考え方もあるのでは？と思ったことを記載しています。必ずしも精度向上に役に立つという訳ではありませんが、共有させてください。</p>","metadata":{}},{"cell_type":"markdown","source":"<h2><b>Cabin</b></h2>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　Until now, Cabin was removed because of the large number of missing values, and Initial was only used for Age completion for integration to the model. Normally, this is where it ends.<br>　But let's take a look at the Titanic image. <br>-----<br>　今までは、Cabinは欠損値が多かったため削除し、IntialはAgeの補完とモデルへの組み込みでしか使われていませんでした。普通は、ここで終わります。ですが、少しTitanicの画像を見てみます。\n<img src= \"https://qiita-image-store.s3.amazonaws.com/0/120341/7f4b870d-1335-5b9a-7f67-08d2ad7602e0.png\" alt =\"Titanic\" style=\"height: 750px; width:auto; align: center;\"></p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　The image shows a cross-sectional view of the Titanic. The alphabet on the side is Cabin. According to the wiki, the lifeboats should have been on the deck, so the closer Cabin is to A, the higher the survival rate.<br>　\nHowever, there are too many missing values, so we usually don't use it. But, in this c ase, we will use it because we want to have many features for <a href = \"#section9\">feature selection</a> later on.<br>-----<br>　画像は、Titanic号の断面図になります。横に振られているアルファベットが、Cabinです。Wikiによると、救命ボートはデッキにあったはずですから、CabinがAに近いほど生存率が高いと思われます。<br>　ただ、欠損値が多すぎるので普通は使いません。ですが、今回は後に行う<a href = \"#section9\">特徴量選択</a>のために、なるべく特徴量を多くしたいので、使うことにします。</p>","metadata":{}},{"cell_type":"code","source":"data['Cabin'] = data['Cabin'].fillna('Unknown')\ndata['Cabin_label'] = data['Cabin'].str.get(0)\nsns.barplot(x = 'Cabin_label', y = 'Survived', data = data, palette = 'spring')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:18:02.660115Z","iopub.execute_input":"2021-08-29T05:18:02.660492Z","iopub.status.idle":"2021-08-29T05:18:03.1428Z","shell.execute_reply.started":"2021-08-29T05:18:02.660453Z","shell.execute_reply":"2021-08-29T05:18:03.141597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><b>Ticket</b></h2>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　Ticket is also too random to be used normally. But, there are passengers who have the same ticket number.<br>-----<br>　Ticketも、ランダムすぎるので普通は使いません。ですが、同じチケット番号を持っている乗客もいます。</p>","metadata":{}},{"cell_type":"code","source":"import collections, sys\nf, ax = plt.subplots(1, 1, figsize = (10, 8))\nd__ = {k: v for k, v in dict(data['Ticket'].value_counts()).items() if v >= 2}\ny__ = d__.values()\nc__ = collections.Counter(y__)\nlabels_, values_ = zip(*c__.items())\nindexes = np.arange(len(labels_))\nplt.bar(indexes, values_, 1, color = ['#E2421F'])\nax.set_title('num of people who have the same ticket')\nplt.xticks(indexes, labels_)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:18:06.773788Z","iopub.execute_input":"2021-08-29T05:18:06.774203Z","iopub.status.idle":"2021-08-29T05:18:06.969252Z","shell.execute_reply.started":"2021-08-29T05:18:06.774171Z","shell.execute_reply":"2021-08-29T05:18:06.968162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　If the number on the Ticket is the same, wouldn't they be in the same place? If so, it seems like they would naturally share the same way.<br>　\nFor the same reason as Cabin, we want to add more variables, so we will create a variable that says how many people have the same ticket number.<br>-----<br>　Ticketのナンバーが同じなら、同じ場所に居そうじゃないですか？そうすると、自然と同じ運命をたどる気がするのです。<br>　Cabinと同じ理由で、変数を増やしたいので、Ticketナンバーが同じ人が何人いるのかという変数を作ります。</p>","metadata":{}},{"cell_type":"code","source":"Ticket_count = dict(data['Ticket'].value_counts())\ndata['TicketGroup'] = data['Ticket'].map(Ticket_count)\nsns.barplot('TicketGroup', 'Survived', data = data, palette = 'spring_r')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:18:10.459911Z","iopub.execute_input":"2021-08-29T05:18:10.460423Z","iopub.status.idle":"2021-08-29T05:18:10.996703Z","shell.execute_reply.started":"2021-08-29T05:18:10.460391Z","shell.execute_reply":"2021-08-29T05:18:10.995955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　We can see that there is a high survival rate for people whose number of people with the same ticket number is distributed betweeen 2 and 4. Also, 11 people is zero, and the others are medium.<br>　So we devide them into there groups.<br>-----<br>　同じチケット番号を持っている人数が、2~4の生存率が高いですね。また、11人はゼロで、他が中くらいです。なので3つのグループに分けます。</p>","metadata":{}},{"cell_type":"code","source":"data.loc[(data['TicketGroup'] >= 2) & (data['TicketGroup'] <= 4), 'Ticket_label'] = 2\ndata.loc[(data['TicketGroup'] >= 5) & (data['TicketGroup'] <= 8) | (data['TicketGroup'] == 1), 'Ticket_label'] = 1\ndata.loc[data['TicketGroup'] == 11, 'Ticket_label'] = 0\n\nsns.barplot('Ticket_label', 'Survived', data = data, palette = 'spring')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:18:13.684912Z","iopub.execute_input":"2021-08-29T05:18:13.685267Z","iopub.status.idle":"2021-08-29T05:18:13.943627Z","shell.execute_reply.started":"2021-08-29T05:18:13.685229Z","shell.execute_reply":"2021-08-29T05:18:13.942351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:48:39.478647Z","iopub.execute_input":"2021-08-28T16:48:39.479005Z","iopub.status.idle":"2021-08-28T16:48:39.531871Z","shell.execute_reply.started":"2021-08-28T16:48:39.478977Z","shell.execute_reply":"2021-08-28T16:48:39.5308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style = \"font-family: 'Poppins', sans-serif; color: #ff00ff\">Color Map</h3>\n\nOne of the comments asked me to explain more about <b>color map</b>! Here are the two main ones.<ul><li>Sequential Colormap<li>Qualitative Colormap</ul>","metadata":{}},{"cell_type":"markdown","source":"<h4><b>Sequential Colormap</b></h4>\nThis palette is appropriate for variables with <b>numbers or sorted values</b>.\nI will show the list of the color below.\n\n> `_r` means *'reverse'*.","metadata":{}},{"cell_type":"code","source":"def cmap_plot(cmap_list, ctype):\n    cmaps = cmap_list\n    \n    n = len(cmaps)\n    \n    fig = plt.figure(figsize = (8.25, n * .20))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequential_cmap = ('Greys', 'Reds', 'Oranges', \n         'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n         'Purples', 'YlGnBu', 'Blues', 'PuBu', 'GnBu', 'PuBuGn', 'BuGn',\n         'Greens', 'YlGn','bone', 'gray', 'pink', 'afmhot', 'hot', 'gist_heat', 'copper', \n         'Wistia', 'autumn_r', 'summer_r', 'spring_r', 'cool', 'winter_r')            \n\ncmap_plot(sequential_cmap, 'Sequential')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: 'Poppins', sans-serif; font-size: 29px; color: #ffa500\">Predictive modeling in more depth than anywhere else</h3>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　Now, here's where we get a little in-depth with the tech!<br>\n　If you read on, you should be able to submit a score over <b>0.8(top 1%!!)</b>, so stay with me!<br>-----<br>\n　ここから、少し踏み入った技術の紹介になります！<br>\n　読んでいただければ、スコアが<b>0.8(トップ1%)</b>を超えるサブミットができるはずですので、お付き合いください！\n</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_selection import SelectKBest","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:46:38.50023Z","iopub.execute_input":"2021-08-29T05:46:38.501152Z","iopub.status.idle":"2021-08-29T05:46:38.508086Z","shell.execute_reply.started":"2021-08-29T05:46:38.501103Z","shell.execute_reply":"2021-08-29T05:46:38.506717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　First, let's prepare the data to be used.<br>-----<br>　まずは、使用するデータを整えましょう。\n</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　Putting all variables we have now into the model is not benefical, as it will only make more calculations and take up more time. It may learn something useless. <br>　So, we remote variables that are not needed.<br>　\nIn this case, it will be like the following.<br>-----<br>　今ある、すべての変数をモデルに入れても、計算が多くなり、時間を割いてしまうだけで、メリットはありません。無駄なものを学習しかねません。そこで、必要のない変数を取り除きます。<br>　今回の場合は、以下の様になります。\n<ul>\n        <li><b><b>Name</b></b></li>Not needed, since we got an <b>Initial</b> that is more likely to describe the person.<br>-----<br>その人をより表すであろう<b>Initial</b>を取得したので、必要ありません。<br>\n        <li><b><b>Age</b></b></li>Not needed, since we got the <b>Age_range</b>.<br>-----<br><b>Age_range</b>を取得したので、必要ありません。<br>\n        <li><b>Ticket</b></li>Not needed, since we got the <b>Ticket_label</b>.<br>-----<br><b>Ticket_label</b>を取得したので、必要ありません。<br>\n        <li><b><b>Fare</b></b></li>Not needed, since we got the <b>Fare_cat</b>.<br>-----<br><b>Fare_cat</b>を取得したので、必要ありません。<br>\n        <li><b><b>Cabin</b></b></li>Not needed, since we got the <b>Cabin_label</b>.<br>-----<br><b>Cabin_label</b>を取得したので、必要ありません。<br>\n        <li><b><b>Fare_range</b></b></li>Not needed, since we got the <b>Fare_cat</b>.<br>-----<br><b>Fare_cat</b>のための変数なので、必要ありません。<br>\n        <li><b><b>PassengerId</b></b></li>Just a meaningless identification value, so we don't need it.<br>-----<br>意味のない、ただの識別値です。必要ありません。<br>\n        <li><b><b>Surname</b></b></li>A variable to see the relationship by honorific title. Not required.<br>-----<br>敬称別の関係を見るための変数です。必要ありません。<br>\n        <li><b><b>Family_Size</b></b></li>Not needed, since we got the <b>Family_label</b>.<br>-----<br><b>Family_label</b>のための変数なので、必要ありません。<br>\n        <li><b><b>family_sur_num</b></b></li>For rewriting the test data. Not necessary.<br>-----<br>テストデータの書き換えのためでした。必要ありません。<br>\n</ul>\n \n</p>","metadata":{}},{"cell_type":"code","source":"data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_range','PassengerId', 'Surname', 'Family_Size', 'family_sur_num'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:18:24.639674Z","iopub.execute_input":"2021-08-29T05:18:24.640179Z","iopub.status.idle":"2021-08-29T05:18:24.649446Z","shell.execute_reply.started":"2021-08-29T05:18:24.640134Z","shell.execute_reply":"2021-08-29T05:18:24.648467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:18:26.621403Z","iopub.execute_input":"2021-08-29T05:18:26.622267Z","iopub.status.idle":"2021-08-29T05:18:26.654422Z","shell.execute_reply.started":"2021-08-29T05:18:26.622185Z","shell.execute_reply":"2021-08-29T05:18:26.653148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　　Since Machine Learning Model cannot understand Strings(In this case, categorical variable like Sex, Embarked and so on), so convert them into Numeric.<br>-----<br>　機械学習モデルは文字列（ここではSex, Embarkedなどのカテゴリー変数）を理解できないため、数値に変換します。\n</p>","metadata":{}},{"cell_type":"code","source":"#Machine Learning Models cannot understand strings!\ndata_ = pd.get_dummies(data)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:18:30.733479Z","iopub.execute_input":"2021-08-29T05:18:30.734222Z","iopub.status.idle":"2021-08-29T05:18:30.750592Z","shell.execute_reply.started":"2021-08-29T05:18:30.734167Z","shell.execute_reply":"2021-08-29T05:18:30.748802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data_.columns)\nprint(len(data_.columns))\nprint(data_.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:18:32.975404Z","iopub.execute_input":"2021-08-29T05:18:32.975831Z","iopub.status.idle":"2021-08-29T05:18:32.983552Z","shell.execute_reply.started":"2021-08-29T05:18:32.975792Z","shell.execute_reply":"2021-08-29T05:18:32.98191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = data_[data_['Survived'].notnull()]\ntest_data = data_[data_['Survived'].isnull()].drop('Survived', axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:18:42.211232Z","iopub.execute_input":"2021-08-29T05:18:42.211588Z","iopub.status.idle":"2021-08-29T05:18:42.221224Z","shell.execute_reply.started":"2021-08-29T05:18:42.211556Z","shell.execute_reply":"2021-08-29T05:18:42.219993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:18:43.458394Z","iopub.execute_input":"2021-08-29T05:18:43.458998Z","iopub.status.idle":"2021-08-29T05:18:43.466045Z","shell.execute_reply.started":"2021-08-29T05:18:43.458955Z","shell.execute_reply":"2021-08-29T05:18:43.46447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"section9\"></a>\n<h2><b>Are you trying to manually narrow down the fetures?</b></h2>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n　Manually selecting explanatory variables is where we can use our cultivated experience, but let's use the algorithms that out predecessors have created for us. In this article, I will introduce a few of them. <br>-----<br>　手動で説明変数を選択するのは、培ってきた経験を活かす場ではありますが、ここは先人たちが作ってくれたアルゴリズムを使いましょう。今回は、その中でもいくつかを紹介します。\n</p>","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#menu\" style=\"font-size:20pt; background-color:coral; color:white\"  class=\"list-group-item list-group-item-action\">Univariate statistics（単変量統計）</a>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　Univariate statistics is a method that calculates the statistical relationship between individual features and target, and tries to select the features that are related with the highest degree of confidence.<br>　\nThe key of this method is that it is <b>univariate</b>. In other words, features that are meaningful when combined with other features are discarded. This makes the calculation faster.<br>-----<br>　単変量統計とは、個々の特徴量とターゲットとの間にある統計的関係を計算し、最も高い確信度で関連している特徴量を選択しようとする手法です。<br>　この手法の肝は、<b>単変量であること</b>です。すなわち、他の特徴量と組み合わさって意味を持つような特徴量は捨てられます。その分計算は高速です。\n</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　<h3 style = \"color:orange\">１．SelectKBest</h3>\n</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    Select top k features in the explanatory variables.<br>-----<br>　説明変数の内、上位k個を選択します。\n</p>","metadata":{}},{"cell_type":"code","source":"train__X = train_data.values[:, 1:]\ntrain__y = train_data.values[:, 0]\ntest__X = test_data.values","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:20:25.709404Z","iopub.execute_input":"2021-08-29T05:20:25.709792Z","iopub.status.idle":"2021-08-29T05:20:25.716699Z","shell.execute_reply.started":"2021-08-29T05:20:25.709761Z","shell.execute_reply":"2021-08-29T05:20:25.71546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_regression\n\nselector = SelectKBest(score_func = f_regression, k = 20)#obtain 20 features from the top. \nselector.fit(train__X, train__y)\nmask = selector.get_support()\nX_shape = selector.transform(train__X)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:54:21.719303Z","iopub.execute_input":"2021-08-28T16:54:21.719713Z","iopub.status.idle":"2021-08-28T16:54:21.737248Z","shell.execute_reply.started":"2021-08-28T16:54:21.719663Z","shell.execute_reply":"2021-08-28T16:54:21.736164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = data_.columns.drop('Survived')[mask]\na","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:54:57.251783Z","iopub.execute_input":"2021-08-28T16:54:57.252189Z","iopub.status.idle":"2021-08-28T16:54:57.259638Z","shell.execute_reply.started":"2021-08-28T16:54:57.252154Z","shell.execute_reply":"2021-08-28T16:54:57.258722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　Above is the fetures selected by SelectKBest.<br>　\nKeep this up, and we'll use the others!<br>-----<br>　上が、SelectKBestで選択された特徴量になります。<br>　この調子で、他のものも使ってみましょう！\n</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins'; sans-serif; font-size: 15px; font-weight: 500;\">\n    　<h3 style = \"color:orange\">２．SelectPercentile</h3>\n</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　Here, it select the top k% explanatoly variables. <br>-----<br>　こちらは、説明変数の内、上位k%を選択します。\n</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectPercentile, f_regression\n\nselector = SelectPercentile(score_func = f_regression, percentile = 70)\nselector.fit(train__X, train__y)\nmask = selector.get_support()\nprint(data_.columns.drop('Survived')[mask])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:55:02.660305Z","iopub.execute_input":"2021-08-28T16:55:02.660838Z","iopub.status.idle":"2021-08-28T16:55:02.673307Z","shell.execute_reply.started":"2021-08-28T16:55:02.660787Z","shell.execute_reply":"2021-08-28T16:55:02.672219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins'; sans-serif; font-size: 15px; font-weight: 500;\">\n    　<h3 style = \"color:orange\">３．GenericUnivariateSelect</h3>\n\n　It can be used in the following ways.<br>-----<br>　以下のような使い方をします。\n   </p>","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import GenericUnivariateSelect, f_regression\n\nselector = GenericUnivariateSelect(mode = 'fwe', score_func = f_regression, param = 70)\nselector.fit(train__X, train__y)\nmask = selector.get_support()\nprint(data_.columns.drop('Survived')[mask])","metadata":{"execution":{"iopub.status.busy":"2021-08-28T16:55:27.790303Z","iopub.execute_input":"2021-08-28T16:55:27.790665Z","iopub.status.idle":"2021-08-28T16:55:27.801673Z","shell.execute_reply.started":"2021-08-28T16:55:27.790638Z","shell.execute_reply":"2021-08-28T16:55:27.80071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Time for a detour...</h2>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　We will take a short detour from here. What will be described is a description of other variables that can be set as arguments of GenericUnivariateSelect. \nIf you want to continue solving Titanic, please skip to here.<br>-----<br>　ここから少し寄り道します。記載することは、<b>GenericUnivariateSelect</b>の引数に設定できるほかの変数の説明です。Titanicを解く上での解法を続けたい方は、<a href = \"#section8\">ここ</a>まで飛んでください。</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　In addition to the fwe we saw above, there are k_best, fpr and fdr in mode.<br>　To understand this, remember <a href = \"https://towardsdatascience.com/visual-guide-to-the-confusion-matrix-bb63730c8eba\">confusion matrix</a>.<br>-----<br>\n    <br>　modeには、上で見たfwe以外に、<b>k_best</b>, <b>fpr</b>, <b>fdr</b>が<a href = https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.GenericUnivariateSelect.html>存在します</a>。<br>　これを知るには、<a href = \"https://qiita.com/TsutomuNakamura/items/a1a6a02cb9bb0dcbb37f\">混同行列</a>を思い出しましょう。<br>\n    　<img src = \"https://miro.medium.com/max/1050/1*85t6zbUiQA0fotnhDaJLaA.png\"  alt =\"Titanic\" style=\"height: 300px; width:auto; align: left;\"><img src= \"https://cz-cdn.shoeisha.jp/static/images/article/9995/9995_001.png\" alt =\"Titanic\" style=\"height: 150px; width:auto; align: left;\">\n    <br>\n</p>","metadata":{}},{"cell_type":"markdown","source":"<b>fpr</b> is a false positive, which is the percentage of negative data that is incorrectly predicted as positive. The feature values are determined by decreasing this fpr value.<br>-----<br><b>fpr</b> は、偽陽性といわれるもので、陰性データを間違って陽性と予測した割合です。これを小さくしていく形で特徴量を決定していきます。<br>\n<table class=\"bunsuu\" summary=\"テーブルによる表示例1\">\n   <tbody>\n      <tr>\n         <td rowspan = \"2\" class = \"kigou\"><b>FPR</b></td>\n         <td rowspan=\"2\" class=\"kigou\"><b>＝</b></td>\n          <td class=\"kasen\"><b>　FP</b></td>\n      </tr>\n      <tr>\n         <td><b>TN + FP</b></td>\n      </tr>\n   </tbody>\n</table>\n<b>fdr</b> is a false discovery rate, which is the percentage of data predicted to be positive that are actually negative.<br>-----<br><b>fdr</b> は、偽発見率といわれるもので、陽性と予測したデータの内、本当は陰性データである割合を指します。<br>\n<table class=\"bunsuu\" summary=\"テーブルによる表示例1\">\n   <tbody>\n      <tr>\n         <td rowspan = \"2\" class = \"kigou\"><b>FDR</b></td>\n         <td rowspan=\"2\" class=\"kigou\"><b>＝</b></td>\n          <td class=\"kasen\"><b>　FP</b></td>\n      </tr>\n       <tr>\n         <td><b>TP + FP</b></td>\n      </tr>\n   </tbody>\n</table><br>　<b>fwe</b> refers to family-wise error, which is the probability that at least one correct null hypothesis set among all hypothesis sets will be incorrectly rejected. (If anything, the R language should have a more detailed implementation of this. I'll skip it this time because it gets too mathematical.)<br>-----<br><b>fwe</b>は、ファミリーワイズエラーを指し、すべての仮説集合の中で少なくとも一つの正しい帰無仮説集合が誤って棄却されてしまう確率の事を指します。<br>（どちらかというとR言語の方が詳しく実装されているはずです。数学チックになってしまうので今回は飛ばします。）<br>　","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">Now, let's try to narrow down the features using these.<br>-----<br>では、これらを使って特徴量を絞ってみましょう。</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import GenericUnivariateSelect, f_regression\n\nselector = GenericUnivariateSelect(mode = 'fpr', score_func = f_regression, param = 70)\nselector.fit(train__X, train__y)\nmask = selector.get_support()\nprint(data_.columns.drop('Survived')[mask])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:22:24.019796Z","iopub.execute_input":"2021-08-29T05:22:24.020219Z","iopub.status.idle":"2021-08-29T05:22:24.040795Z","shell.execute_reply.started":"2021-08-29T05:22:24.020185Z","shell.execute_reply":"2021-08-29T05:22:24.038973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selector = GenericUnivariateSelect(mode = 'fdr', score_func = f_regression, param = 70)\nselector.fit(train__X, train__y)\nmask = selector.get_support()\nprint(data_.columns.drop('Survived')[mask])","metadata":{"execution":{"iopub.status.busy":"2021-08-28T17:03:59.092274Z","iopub.execute_input":"2021-08-28T17:03:59.092622Z","iopub.status.idle":"2021-08-28T17:03:59.102923Z","shell.execute_reply.started":"2021-08-28T17:03:59.092593Z","shell.execute_reply":"2021-08-28T17:03:59.101544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selector = GenericUnivariateSelect(mode = 'fwe', score_func = f_regression, param = 70)\nselector.fit(train__X, train__y)\nmask = selector.get_support()\nprint(data_.columns.drop('Survived')[mask])","metadata":{"execution":{"iopub.status.busy":"2021-08-28T17:04:00.746537Z","iopub.execute_input":"2021-08-28T17:04:00.746909Z","iopub.status.idle":"2021-08-28T17:04:00.760657Z","shell.execute_reply.started":"2021-08-28T17:04:00.746864Z","shell.execute_reply":"2021-08-28T17:04:00.759323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　It is not that difference (haha).  By the way, the default is <b>percentile</b>.<br>\n　Now, let's move on to the next one.<br>-----<br>　そこまで変わらないですね（笑）まだあります！因みにデフォルトは<b>percentile</b>です。<br>　では、次の紹介です。\n</p>","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#menu\" style=\"font-size:20pt; background-color:coral; color:white\"  class=\"list-group-item list-group-item-action\">Model Base Feature Selection(モデルベース特徴量選択)</a>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n　Model-base feature selection is a method that determine the importance of individual features and keeps only the important ones using a supervised learning model.<br>\n　Unlike the case of univariate selection, this is considering all features at the same time, so capturing the interaction between variables. (This makes the calculation a little slower.)<br>-----<br>　モデルベース特徴量選択は、教師あり学習モデルを用いて個々の特徴量の重要性を判断し、重要なものだけを残す手法です。<br>単変量選択の場合とは違い、すべての特徴量を同時に考慮するので、変数間の交互作用を捉えられます。(その分計算が少し遅くなります。)\n</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins'; sans-serif; font-size: 15px; font-weight: 500;\">\n    　<h3 style = \"color:orange\">SelectFromModel</h3>\n</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\n#estimator として RandomForestRegressor を仕様。重要度が median以上のものを選択。\nselector = SelectFromModel(RandomForestRegressor(n_estimators = 100, random_state = 42), threshold = 'median')\nselector.fit(train__X, train__y)\nsm_mask = selector.get_support()\nprint(data_.columns.drop('Survived')[sm_mask])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:22:19.917252Z","iopub.execute_input":"2021-08-29T05:22:19.9176Z","iopub.status.idle":"2021-08-29T05:22:20.216028Z","shell.execute_reply.started":"2021-08-29T05:22:19.917571Z","shell.execute_reply":"2021-08-29T05:22:20.215033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#menu\" style=\"font-size:20pt; background-color:coral; color:white\"  class=\"list-group-item list-group-item-action\">Iterative Feature Selection(反復特徴量選択)</a>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n　Unlike univariate statistics which uses no model, and model-base feature selection which uses only one model, this time we will create a series of model using different features. <br>\n　<b>RFE(recursive feature elimination)</b> creates model by starting with all features and then removes the least important feature in the model. This then creates the model again and removes the least important feature.<br>-----<br>　モデルを使わない単変量統計、モデルを一つだけ使うモデルベース特徴量選択とは違い、今回は、異なる特徴量を用いた一連のモデルを作成します。<br>　<b>RFE(recursive feature elimination)</b>:再帰的特徴量削減は、すべての特徴量から開始してモデルを作り、そのモデルで最も重要度が低い特徴量を削除します。そしてまたモデルをつくり、最も重要度が低い特徴量を削除します。\n</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\n\nselector = RFE(RandomForestRegressor(n_estimators = 100, random_state = 42), n_features_to_select = 15)\nselector.fit(train__X, train__y)\nrfe_mask = selector.get_support()\nprint(data_.columns.drop('Survived')[rfe_mask])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:20:55.888763Z","iopub.execute_input":"2021-08-29T05:20:55.88916Z","iopub.status.idle":"2021-08-29T05:21:00.037245Z","shell.execute_reply.started":"2021-08-29T05:20:55.889127Z","shell.execute_reply":"2021-08-29T05:21:00.035835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n　The disadvantage is that it takes a considerable amount of time because it is learning random forest iteratively for n_features_to_select.<br>　\nThis time, we will analyze with this 15 features selected by RFE.<br>-----<br>　n_features_to_selectの分、ランダムフォレストを繰り返し学習しているので、相当な時間がかかってしまうのが欠点。<br>　今回は、このRFEによって選択された、選ばれし15の特徴量を使って分析をしていきます！\n</p>","metadata":{}},{"cell_type":"code","source":"print(len(train_data.columns))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:21:20.462258Z","iopub.execute_input":"2021-08-29T05:21:20.462695Z","iopub.status.idle":"2021-08-29T05:21:20.468986Z","shell.execute_reply.started":"2021-08-29T05:21:20.462654Z","shell.execute_reply":"2021-08-29T05:21:20.467773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"li = ['Survived']\nli.extend(data_.columns.drop('Survived')[rfe_mask].tolist())\ntrain_data_ = train_data[li]\nprint(len(train_data_.columns)) #'Survived' + 15 selected features","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:21:23.864953Z","iopub.execute_input":"2021-08-29T05:21:23.865371Z","iopub.status.idle":"2021-08-29T05:21:23.875376Z","shell.execute_reply.started":"2021-08-29T05:21:23.865338Z","shell.execute_reply":"2021-08-29T05:21:23.873909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_ = test_data[data_.columns.drop('Survived')[rfe_mask].tolist()]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:04:26.207066Z","iopub.execute_input":"2021-08-29T08:04:26.207419Z","iopub.status.idle":"2021-08-29T08:04:26.213448Z","shell.execute_reply.started":"2021-08-29T08:04:26.207391Z","shell.execute_reply":"2021-08-29T08:04:26.212568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:04:40.079653Z","iopub.execute_input":"2021-08-29T08:04:40.080175Z","iopub.status.idle":"2021-08-29T08:04:40.105004Z","shell.execute_reply.started":"2021-08-29T08:04:40.080143Z","shell.execute_reply":"2021-08-29T08:04:40.104079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:24:59.922424Z","iopub.execute_input":"2021-08-28T18:24:59.922774Z","iopub.status.idle":"2021-08-28T18:24:59.946108Z","shell.execute_reply.started":"2021-08-28T18:24:59.922737Z","shell.execute_reply":"2021-08-28T18:24:59.945429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(train_data_, test_size = 0.3, random_state = 0, stratify = train_data['Survived'])#don't forget 'stratify'\ntrain_X = train[train.columns[1:]]\ntrain_Y = train[train.columns[:1]]\ntest_X = test[test.columns[1:]]\ntest_Y = test[test.columns[:1]]\nX = train_data_[train_data_.columns[1:]]\nY = train_data_['Survived']\nX_combined = pd.concat((train_X, test_X))\ny_combined = pd.concat((train_Y, test_Y))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:21:29.053803Z","iopub.execute_input":"2021-08-29T05:21:29.054181Z","iopub.status.idle":"2021-08-29T05:21:29.070774Z","shell.execute_reply.started":"2021-08-29T05:21:29.05415Z","shell.execute_reply":"2021-08-29T05:21:29.069539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_X.shape)\nprint(test_X.shape)\nprint(train_Y.shape)\nprint(test_Y.shape)\nprint(y_combined.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:40:07.899582Z","iopub.execute_input":"2021-08-28T18:40:07.899939Z","iopub.status.idle":"2021-08-28T18:40:07.906212Z","shell.execute_reply.started":"2021-08-28T18:40:07.899903Z","shell.execute_reply":"2021-08-28T18:40:07.905284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape)\nprint(Y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:25:29.314303Z","iopub.execute_input":"2021-08-28T18:25:29.314622Z","iopub.status.idle":"2021-08-28T18:25:29.32256Z","shell.execute_reply.started":"2021-08-28T18:25:29.314594Z","shell.execute_reply":"2021-08-28T18:25:29.321574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: 'Poppins', sans-serif; font-size: 29px; color: #ffa500\">Deep Deep introduction of the algorithm </h3>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n　The originator of machine learning is the perceptron. It is a type of neural network that uses multiple formal neurons connected in a network-like fashion. <br>　However, there is a problem that perceptron does not converge on data that cannot be completely linearly separated. Therefore, various methods have been devised to deal with non-linear data.<br>　In the following sections, we will explore this and try to use it in practice.<br>-----<br>　機械学習の元祖といえば、<b>パーセプトロン</b>ですね。ニューラルネットワークの一種で、形式ニューロンを複数用いてネットワーク状に接続したモノでしたね。<br>　ですが、パーセプトロンは完全に線形分離できないようなデータでは収束しないことが問題です。そこでいろいろな工夫がなされ、非線形にも対応できるようになってきました。<br>　以下ではそれを掘り下げながら、実際に使ってみます。","metadata":{}},{"cell_type":"markdown","source":"# <b style = \"font-size:20px\">LogisticRegression</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n　Logistic regression is an algorithm for linear and binary classification problems. It is a regression method by its name, but what a model for classification. If you know where the name comes from, please let me know! ＜(_ _)＞<br>　\nBe careful. This is binary classification. It is multiple regression analysis that predicts  continuous values. <br>-----<br>　ロジスティック回帰は、線形分類問題と二値分類問題に対するアルゴリズムです。名前からして回帰なのですが、なんと分類のためのモデルなんです。名前の由来を知っている方は是非教えてください！＜(_ _)＞<br>　気を付けてください。二値分類です。連続値を予測するのは重回帰分析です。</p>","metadata":{}},{"cell_type":"markdown","source":"<img src = \"http://media5.datahacker.rs/2021/01/83-1024x579.jpg\" alt =\"Titanic\" style=\"height: 300px; width:auto; align: left;\">","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n　Rather than thinking of it as classification, it is easier to think of it as caputuring the class label y=1. The function that predicts the probability that a data point belongs to class 1 given a feature x is called the logistic sigmoid function, sometimes simply called the sigmoid function.<br>-----<br>　分類という考え方より、クラスラベルy = 1を捉えると考えたほうが分かりやすいです。特徴量xが与えられた場合にデータ点がクラス1に所属している確率を予測する関数をロジスティックシグモイド関数と呼び、単にシグモイド関数と呼ばれたりします。</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n<math>\n\\[ z = \\log \\frac{p(y = 1|\\mathbf{x})}{1 - p(y = 1|\\mathbf{x})} \\Longleftrightarrow p(y = 1|\\mathbf{x}) = \\frac{1}{1+e^{-z}} \\]\n    </math>\n    </p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n　Now, let's take a look at the sigmoid. If you look at this, you can see why this can only be used for binary classification.<br>-----<br>　では、シグモイドを見てみましょう。これを見れば、二値分類でしか使えないことが分かるはずです。</p>","metadata":{}},{"cell_type":"code","source":"def sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\nz = np.arange(-7, 7, 0.1)\nsig_z = sigmoid(z)\nplt.plot(z, sig_z, color = '#E2421F')\n\nplt.axvline(0.0, color = 'k')\nplt.ylim(-0.1, 1.1)\nplt.xlabel('z')\nplt.ylabel('sigmoid(z)')\n\nplt.yticks(np.arange(0.0, 1.1, 0.5))\nax = plt.gca()\nax.yaxis.grid(True, color = 'orange')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T17:12:01.42558Z","iopub.execute_input":"2021-08-28T17:12:01.425939Z","iopub.status.idle":"2021-08-28T17:12:01.636594Z","shell.execute_reply.started":"2021-08-28T17:12:01.425904Z","shell.execute_reply":"2021-08-28T17:12:01.635735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n<math>\n    <mfrac numalign=\"left\">\n    　The inner product of the feature vector x and the weight w to be put into the activation function can be expressed by z = <span style=\"font-weight:bold\">w</span><sup>T</sup><span style=\"font-weight:bold\">x</span>, so<br>-----<br>　活性化関数に入れる特徴量ベクトルxとウェイトwの内積は、\n    z = <span style=\"font-weight:bold\">w</span><sup>T</sup><span style=\"font-weight:bold\">x</span>で表せるので、<br><br>\n\\[ f(z) = \\frac{1}{1+e^{-z}} = \\frac{1}{1+e^{-\\mathbf{w}^{T}\\mathbf{x}}} \\]<br>\n   　and we use the threshold to convert the predicted probability into the binary outcome measure.<br>-----<br>となり、閾値を使って、予測された確率を二値の成果指標に変換する。<br><br>\n    \\[\n\\begin{eqnarray}\n\\hat{y} =\n  \\begin{cases}\n    1 & ( f(z) \\ge 0.5 ) \\\\\n    0 & ( f(z) \\lt 0.5 )\n  \\end{cases}\n\\end{eqnarray}\n\\]<br>\n    　So, logistic regression is a binary classification. <br>-----<br>なので、ロジスティック回帰は2値分類なのです。\n        </mfrac>\n</math>\n    </p>","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(train_X,train_Y)\npre1 = model.predict(test_X)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(pre1,test_Y))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:21:36.514894Z","iopub.execute_input":"2021-08-29T05:21:36.515294Z","iopub.status.idle":"2021-08-29T05:21:36.580521Z","shell.execute_reply.started":"2021-08-29T05:21:36.51526Z","shell.execute_reply":"2021-08-29T05:21:36.579458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b style = \"font-size:20px\">Linear Support Vector Machine</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n　Suppert Vector Machine is an extension of the Perceptron. The goal is to maximize what is called the <b>margin</b>, red dotted line in the below figure. In other words, it is classifying.<br>-----<br>　サポートベクトルマシンは、パーセプトロンの拡張です。下図の赤い点線 <b>マージン</b> と呼ばれるものを最大化する事を目的としています。つまり、分類しているのです。\n</p><img src= \"https://miro.medium.com/max/724/0*hUAVXd1XaQSsrK-9.png\" alt =\"Titanic\" style=\"height: 450px; width:auto; align: center;\">","metadata":{}},{"cell_type":"code","source":"model = svm.SVC(kernel = 'linear', C = 0.1, gamma = 0.1)\nmodel.fit(train_X, train_Y)\npre2 = model.predict(test_X)\nprint('accuracy: ', metrics.accuracy_score(pre2, test_Y))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:21:41.334546Z","iopub.execute_input":"2021-08-29T05:21:41.334916Z","iopub.status.idle":"2021-08-29T05:21:41.366292Z","shell.execute_reply.started":"2021-08-29T05:21:41.334848Z","shell.execute_reply":"2021-08-29T05:21:41.364973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b style = \"font-size:20px\">Radial Support Vector Machine</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n　In the above, we have set kernel to linear. This is linear separation. Actually, there is one more thing. It is <b>rbf</b>.<br>　This caputures non-linear data as a high dimensional space using kernel trick.<br>-----<br>　上では、kernelをlinearにしました。線形分離です。実はもう一つあります。rbfです。これは、非線形のデータを、カーネルトリックと使って分離超平面を高次元空間にして捉えます。<br><br>\n    <img src= \"https://miro.medium.com/max/1050/0*ngkO1BblQXnOTcmr.png\" alt =\"Titanic\" style=\"height: 300px; width:auto; align: center;\">\n    <br><br>　And, if we put a line between red and blue using projection function, we have classified them. <br>-----<br>　あとは、射影関数を使って赤と青の間に線を入れれば、分類できたことになりますね。</p>","metadata":{}},{"cell_type":"code","source":"model = svm.SVC(kernel='rbf',C=1,gamma=0.1)\nmodel.fit(train_X,train_Y)\npre3 = model.predict(test_X)\nprint('Accuracy for rbf SVM is ',metrics.accuracy_score(pre3,test_Y))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:39:14.728106Z","iopub.execute_input":"2021-08-29T05:39:14.728504Z","iopub.status.idle":"2021-08-29T05:39:14.762271Z","shell.execute_reply.started":"2021-08-29T05:39:14.728469Z","shell.execute_reply":"2021-08-29T05:39:14.761399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_combined.iloc[:, 0].min()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T09:26:32.233488Z","iopub.execute_input":"2021-06-29T09:26:32.23395Z","iopub.status.idle":"2021-06-29T09:26:32.240274Z","shell.execute_reply.started":"2021-06-29T09:26:32.233921Z","shell.execute_reply":"2021-06-29T09:26:32.239168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b style = \"font-size:20px\">Decision Tree</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　Decision tree classifiers are powerful when to consider semantic interpretability. As the name suggests, it classifies data by the way it makes decisions on a series of questions. So let's take a look!<br>-----<br>　　決定木分類器は、意味解釈可能性に配慮する場合に力を発揮します。名前の通り、一連の質問に対して決定を下すという方法により、データを分類する。<br><br><img src= \"https://www.cs.cmu.edu/~bhiksha/courses/10-601/decisiontrees/DT.png\" alt =\"Titanic\" style=\"height: 300px; width:auto; align: left;\"></p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n  　In this case, it ask questions such as \"Is the gender male?\", \"Is the Fare more over 60.0?\" and so on, and divide the data by the feture with the greatest information gain. This is repeated until a leaf node appears.\nThe information gain is the smallness of variation in the elements of the segmented set, and the data is segmented by the feature that maximizes this value.\nIn the next section, we will see about the information gain.<br>-----<br>　今回の場合は、「性別は男性か。」「Fareは60.0以上か」等と質問していき、情報利得が最大となる特徴量でデータを分割する。葉ノードが表れるまでこれを繰り返します。<br>　情報利得とは、分割された集合の要素についてのばらつきの少なさの事であり、これが最大になる特徴量でデータを分割します。<br>　次では、情報利得について説明します。</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n  　In order to devide the nodes by the features that is the highest information gain, we need to define the objective function that we want to optimize in the decision tree learning algorithm. Again, I represent it in a general form.<br>-----<br>　最も情報利得の高い特徴量でノードを分割するには、決定木学習アルゴリズムにおいて最適化の対象としたい目的関数を定義する必要があります。今回も、一般的な形で表します。</p><br> 情報利得の式 <br>　<ul><li>f：feature to be segmented\n<li>Dp：parent dataset\n<li>Dj：dataset of jth child node\n<li>I：impurity<br><br>\n\n　In short, information gain is just the sum of the \"impurity of parent node\" and the \"impurity of child node\".<br>-----<br>　\n    <li>f：分割を行う特徴量</li>\n    <li>Dp：親のデータセット</li>\n    <li>Dj：j番目の子ノードのデータセット</li>\n    <li>I：不純度</li><br>である。要は、情報利得は「親ノードの不純度」と「子ノードの不純度」の合計に過ぎない。","metadata":{}},{"cell_type":"markdown","source":"<math>\n    <script src=\n\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">\n</script>\n<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n  　\n     There are two indicators of impurity: <b>Gini impurity</b> and <b>entropy</b>. Gini impurity is denoted by IG, and entropy by IH.<br>-----<br>　不純度の指標には、「ジニ不純度」と「エントロピー」があります。ジニ不純度は$I_{G}$、エントロピーは$I_{H}$で表記します。<br>　\n$$I_{H}(t) = -\\sum_{i=1}^cp(i|t)\\log_2p(i|t)$$\n    <br>　　p(i|t) is the percentage of data points belonging to class i at a particular node t. Thus, if all the data points of a node belong to the same class, the entropy is zero. In  binary classification, it is the case when p(i = 1|t) = 1 or p(i = 0|t) = 0.<br>　\nOn the other hand, the entropy is maximum (1 in the case of binary classification) when p(i|t) are equal to each other, i.e., when each class is uniformly distributed. In other words, p(i = 1|t) = 0.5 or p(i = 0|t) = 0.5. <br>　Therefore, we can say that entropy is a condition that tries to maximize the interdependence of two probabilities.<br>-----<br>　p(i|t)は、特定のノードtにおいてクラスiに所属しているデータ点の割合です。よって、ノードのデータ点がすべて同じクラスの所属している場合、エントロピーは0です。二値分類では、p(i = 1|t) = 1または、p(i = 0|t) = 0の場合です。<br>　逆に、エントロピーが最大（二値分類の場合、1）になるのは、p(i|t)同士が同じ値であること、すなわち各クラスが一様に分布している場合です。つまりp(i = 1|t) = 0.5または、p(i = 0|t) = 0.5の場合です。よって、エントロピーは二つの確率の相互依存度が最大になるよう試みる条件の事と言えます。</p></math>","metadata":{}},{"cell_type":"markdown","source":"<math>\n    <script src=\n\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">\n</script>\n<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n# In the case of Gini impurity, this is as follows.<br>-----<br>　ジニ不純度の場合だと、以下のようになります。　$$I_{G}(t) = \\sum_{i=1}^cp(i|t)(1 - p(i|t)) = 1 - \\sum_{i=1}^cp(i|t)^2$$<br><br>　This time, if we put the N as sample num of training data and ni as the num of training data that belongs to class i, then we can represent it p(i|t)=niN, so it will looks like this.<br>-----<br>　今回、Nをトレーニングデータのサンプル数、niをクラスiに属するトレーニングデータ数とすると、$p(i|t) = \\frac{ni}{N}$　と置けるので、$$I_{G}(t) = 1 - \\frac{1}{N}$$<br>　Therefore, we can see that when Gini impurity is maximized, it will approximate to 1.<br>-----<br>　　となります。よって、ジニ不純度が最大となる場合、1に近似していくことが分かります。</p></math>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　　But in fact, entropy and Gini impurity often have similar results. The conditions for maximum and minimum are similar, so it is better to spend time pruning the decision tree than to try to decide which one to use.<br>-----<br>　ですが実際、エントロピーとジニ不純度は似た結果となることが多いです。最大最小になる条件が似てますので、どっちを使うかに時間を使うよりかは決定木の剪定に時間をかけたほうが良いです。</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　Just in case, let's take a look at it visually.<br>-----<br>　一応、視覚的にも見てみましょう。</p>","metadata":{}},{"cell_type":"code","source":"def gini(p):\n    return (p) * (1 - (p)) + (1 - p) * (1 - (1 - p))\n\ndef entropy(p):\n    return - p * np.log2(p) - (1 - p) * np.log2((1 - p))\n\nx = np.arange(0.0, 1.0, 0.01)\nent = [entropy(p) if p != 0 else None for p in x]\n#entropy scaling ver\nsc_ent = [e * 0.5 if e else None for e in ent]\n\nfig = plt.figure()\nax = plt.subplot(111)\n\nfor va, la, ls, co in zip([ent, sc_ent, gini(x)], ['Entropy', 'Scaled_Entropy', 'Gini inpurity'], ['-', '--', '-.'], ['blue', 'red', 'green']):\n    line = ax.plot(x, va,label = la, linestyle = ls, color = co, lw = 2)\n\nax.legend(loc = 'upper center', bbox_to_anchor = (0.5, 1.15), ncol = 5, fancybox = True, shadow = False)\nax.axhline(y = 0.5, linewidth = 1, color = 'k', linestyle = '--')\nax.axhline(y = 1.0, linewidth = 1, color = 'k', linestyle = '--')\n\nplt.ylim([0, 1.1])\nplt.xlabel('p(i = 1)')\nplt.ylabel('impurity index')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T18:43:37.208816Z","iopub.execute_input":"2021-08-28T18:43:37.209218Z","iopub.status.idle":"2021-08-28T18:43:37.399024Z","shell.execute_reply.started":"2021-08-28T18:43:37.209171Z","shell.execute_reply":"2021-08-28T18:43:37.397953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DecisionTreeClassifier(\n    criterion = 'gini',# or criterion = 'entropy'\n    max_depth = 11, random_state = 1)\nmodel.fit(train_X, train_Y)\npre4 = model.predict(test_X)\nprint(metrics.accuracy_score(pre4, test_Y))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:21:57.68201Z","iopub.execute_input":"2021-08-29T05:21:57.682445Z","iopub.status.idle":"2021-08-29T05:21:57.701092Z","shell.execute_reply.started":"2021-08-29T05:21:57.682408Z","shell.execute_reply":"2021-08-29T05:21:57.699194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　We've done it. Also, scikit-learn has a function to visualize the trained decision tree model. This time we narrowing down the number of features to four for easier viewing to see the trained result.<br>-----<br>　出来ましたね。また、scikit-learnには、訓練された決定木モデルを可視化する機能があります。今回は見やすいように特徴量を4つに絞り、もう一度訓練した結果を示したいと思います。</p>","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\nimport sys\n\nselector = SelectKBest(score_func = f_regression, k = 5)\nselector.fit(train__X, train__y)\nmask = selector.get_support()\nva = data_.columns.drop('Survived')[mask]\ntrain_data__ = train_data[va]\n\ntrain__, _ = train_test_split(train_data__, test_size = 0.1, random_state = 0, stratify = train_data['Survived'])\ntrain_X_ = train__[train__.columns[1:]]\ntrain_Y_ = train__[train__.columns[:1]]\n\nmodel = DecisionTreeClassifier()\nmodel.fit(train_X_, train_Y_)\n\ntree.plot_tree(model)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:22:32.007502Z","iopub.execute_input":"2021-08-29T05:22:32.007924Z","iopub.status.idle":"2021-08-29T05:22:32.542542Z","shell.execute_reply.started":"2021-08-29T05:22:32.007885Z","shell.execute_reply":"2021-08-29T05:22:32.541032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　It is easy to see!<br>　\nIt's hard to see when there is a lot of features like this time, but when there's not much, it's very useful!<br>-----<br>　見やすいですよね！<br>　今回みたいに特徴量が多いと見えづらいですが、少ない時にはとても役に立ちそうです！！</p>","metadata":{}},{"cell_type":"markdown","source":"<a id = 'section16'></a>\n# <b style = \"font-size:20px\">Random Forest</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\nIn explaining RandomForest, I will explain some words that come up. <br>　\n<li><b>Ensemble learning</b>: A learning method that achieve high generalization performance by combining multiple machine learning models. In simple terms, it is the same ideas as incorporating the opinions of ten people is more robust than the opinion of one person.<br>　\n<li><b>Bootstrap sampling</b>: This is a method of sampling from a single sample set allowing duplicates, and creating a new sample set. Since duplicates are allowed, some data may not be selected, and this is called OOB (Out Of Bag).<br>-----<br>RandomForestを説明するにあたり、でてくる単語を説明します。\n<ul>\n    <li><b>アンサンブル学習</b>：複数の機械学習モデルを組み合わせることで、高い汎化性能を実現する学習法。簡単に考えれば、一人の意見より十人の意見を取り入れたほうが頑健なものになるのと同じ考えです。</li>\n    <li><b>ブートストラップ</b>：元データから一のデータを復元抽出というやり方でサンプリングする。</li>\n</ul>　<img src= \"https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png\" alt =\"Titanic\" style=\"height: 450px; width:auto; align: left;\"><br>　RandomForest can be thought of as ensemble of decision trees. By averaging multiple decision trees that each of which has a high variance, we can build a model with higher generalization performance and robustness against overlearning.<br>　The procedure is as follows. <ul>\n    <li>１．Sampling random bootstrap sample of size n with replacement. (Select n data points randomly from train dataset.)</li>\n    <li>２．Grow a decision tree from bootstrap sample. And do the following tasks at each of its nodes.</li>\n        <ul>\n            <li>2.1　Extract d features randomly non-replacement.</li>\n            <li>2.2　By maximize the information gain, the nodes are devided using features that result in an optimal division according to the objective function.</li>\n        </ul>\n    <li>３．Repeat steps 1-2 k times.</li>\n    <li>４．Summarize the predictions for each decision tree and assign class labels based on the majority vote.</li>\n<br>　Let's use it in practice.<br>-----<br>　\n\n</ul><br>\n　RandomForestは、決定木のアンサンブルとみなせます。それぞれバリアンスが高い複数の決定木を平均化することで、より汎化性能が高く過学習に対して堅牢なモデルを構築します。<br>　手順は以下の4つです。<ul>\n    <li>１．サイズnのランダムなブーストラップ標本を復元抽出します。（訓練データセットからn個のデータ点をランダムに選択する。）</li>\n    <li>２．ブーストラップ標本から決定木を成長させます。その各ノードで以下の作業をします。</li>\n        <ul>\n            <li>2.1　d個の特徴量をランダムに非復元抽出します。</li>\n            <li>2.2　情報利得を最大化することで、目的関数に従って最適な分割となる特徴量を使ってノードを分割する。</li>\n        </ul>\n    <li>３．手順1~2をk回繰り返す。</li>\n    <li>４．決定機ごとの予測をまとめ、多数決に基づいてクラスラベルを割り当てる。</li>\n\n</ul><br>\n　実際に使ってみましょう。</p>","metadata":{}},{"cell_type":"code","source":"#ジニ不純度を指標とするインスタンスを生成\nmodel = RandomForestClassifier(criterion = 'gini', n_estimators = 22)\nmodel.fit(train_X, train_Y)\npre4_ = model.predict(test_X)\nprint(metrics.accuracy_score(pre4_, test_Y))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:38:08.082886Z","iopub.execute_input":"2021-08-29T07:38:08.083242Z","iopub.status.idle":"2021-08-29T07:38:08.151451Z","shell.execute_reply.started":"2021-08-29T07:38:08.083213Z","shell.execute_reply":"2021-08-29T07:38:08.150037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　As for entropy, try it for yourself! However, gini impurity is often used as the default, and I had the impression that gini scores better in my hands.<br>\nWe'll see about ensemble including Randomforest later.<br>-----<br>　entropyは、自分で試してみてください！ですが、ジニ不純度が多くデフォルトとして採用されている分、自分の手元でもginiの方がスコアが高い印象でした。<br>　RandomForestをはじめ、アンサンブルに関しては、後に詳しく説明します。\n</p>","metadata":{}},{"cell_type":"markdown","source":"# <b style = \"font-size:20px\">K-Nearest neighbor classifier (KNN)</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　KNN is a prime example of lazy learning.  Lazy is, not because of its apparent simplicity, but because it memorize the training dataset without learning discriminant function from training data. A discriminant function is a function that maps an input to a class label.<br>　\nPerceptron, logistic regression, and linear SVM are called parametric models, which estimate parameters from train dataset. On the contrary, KNN belongs to the subcategory of non-parametric models an d is called instance-based learning. This has features that memorize train dataset and its train process cost is zero. It can also be used for both classification and regression. <br>　\nThe procedure for the KNN algorithm is as simple as follows.\n<ul>\n<li>1. Select a value of k and a distance index.</li>\n<li>2. Find k nearest neighbor data point from the data point to be classified. </li>\n<li>3. Assign class labels by majority vote.</li>\n</ul><br>-----<br>　KNNは<b>怠惰学習</b>の代表的な例です。怠惰というのは、その見かけの単純さというより、訓練データから判別関数を学習せずに訓練データセットを暗記するためです。判別関数は、入力を直接クラスラベルに対応させる関数のことです。<br>　パーセプトロン、ロジスティック回帰、線形SVMはパラメトリックモデルと呼ばれ、訓練データセットからパラメータを推定するものです。逆にKNNはノンパラメトリックモデルのサブカテゴリに属しており、インスタンスに基づく学習と呼ばれます。訓練データセットを記憶し、学習過程のコストが0という特徴がある。また、分類と回帰のどちらにも利用できます。<br>　KNNアルゴリズムの手順は、以下のような単純なものです。\n<ul>\n    <li>１．kの値と距離指標を選択する。</li>\n    <li>２．分類したいデータ点からk個の最新傍のデータ点を見つけ出す。</li>\n    <li>３．多数決によりクラスラベルを割り当てる。</li>\n</ul>\n\n  <br><img src= \"https://helloacm.com/wp-content/uploads/2016/03/2012-10-26-knn-concept.png\" alt =\"Titanic\" style=\"height: 300px; width:auto; align: left;\"></p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　In this case, the data represented by the star is classified into class B. Let's use it in practice.<br>-----<br>　この場合、星で表されているデータはクラスBに分類されます。実際に使ってみましょう。\n</p>","metadata":{}},{"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors = 10)\nmodel.fit(train_X, train_Y)\npre5 = model.predict(test_X)\nprint(metrics.accuracy_score(pre5, test_Y))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:22:45.266388Z","iopub.execute_input":"2021-08-29T05:22:45.267045Z","iopub.status.idle":"2021-08-29T05:22:45.298458Z","shell.execute_reply.started":"2021-08-29T05:22:45.266995Z","shell.execute_reply":"2021-08-29T05:22:45.297041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b style = \"font-size:20px\">Gaussian Naive Bayes</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　<img src= \"https://iq.opengenus.org/content/images/2020/02/Illustration-of-how-a-Gaussian-Naive-Bayes-GNB-classifier-works-For-each-data-point.png\" alt =\"Titanic\" style=\"height: 300px; width:auto; align: left;\"><br><br>　When dealing with continuous data, it is often assumed that the continuous values of each class are distributed according to a normal (or gaussian) distribution. The likelihood is as follows.<br>-----<br>　連続データを扱う場合、各クラスの連続値が正規分布（またはガウス分布）に従って分布していると仮定することが多いです。尤度は以下のようになります。<math>\n    <script src=\n\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">\n</script>\n<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n$$P(x_i|y)\\frac{1}{\\sqrt{2\\pi\\sigma_y^2}}\\exp(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y})$$</math><br>　Gaissian Naive Bayes supports continuous value features and models each one to fit a Gaussian distribution.<br>　\nThe figure above shows how the Gaussian Naive Bayes Classifier works. <br>　\nFor every data points, the point and respective class mean z-score distance is calculated. In other words, it is the distance from the class average divided by the standard deviation of the class. <br>-----<br>　Gaussian Naive Bayesは連続値の特徴をサポートし、それぞれをガウス分布に適合するようにモデル化します。<br>　上図は、Gaussian Naive Bayes分類器がどのように動くかを示しています。全てのデータ点で、そのポイントとそれぞれのクラス平均のz-score距離が計算されます。すなわち、クラス平均からの距離をそのクラスの標準偏差で割ったものになります。</p>","metadata":{}},{"cell_type":"code","source":"model = GaussianNB()\nmodel.fit(train_X, train_Y)\npre6 = model.predict(test_X)\nprint(metrics.accuracy_score(pre6, test_Y))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:22:48.764982Z","iopub.execute_input":"2021-08-29T05:22:48.765372Z","iopub.status.idle":"2021-08-29T05:22:48.778189Z","shell.execute_reply.started":"2021-08-29T05:22:48.765339Z","shell.execute_reply":"2021-08-29T05:22:48.777013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b style = \"font-size:20px\">Are you going to determine the hyperparameters manually?</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　SVM's C and gamma, and Random Forest's n_estimators are what set the behavior of the classification model. We have to tell it what to do. However, it takes time to test each one for accuracy. <br>　\nSo, we use <b>Optuna</b>. This will search for the best hyperparameters for you.<br>　\nLet's try it with <b>rbf</b>. <br>-----<br>　SVMのCやgamma、RandomForestのn_estimatorsは、分類モデルの挙動を設定するものです。それは、こちらから指示してあげないといけません。なのですが、精度の良いものを一つ一つ試すのは時間がかかります。<br>　そこで、Optunaを使います。これは、最適なハイパーパラメータを探索してくれます。<br>　試しにrbfでやってみましょう。</p>","metadata":{}},{"cell_type":"code","source":"import optuna\n\ndef objective(trial):\n    params = {\n        'kernel': 'rbf',\n        'C': trial.suggest_loguniform('C', 1e-10, 1e10),\n        'gamma': trial.suggest_loguniform('gamma', 1e-10, 3.0)\n    }\n    model_ = svm.SVC(**params)\n    model_.fit(train_X, train_Y)\n    pre = model_.predict(test_X)\n    acc = metrics.accuracy_score(pre, test_Y)\n    score = 1 - acc\n    return score","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:46:53.892276Z","iopub.execute_input":"2021-08-29T05:46:53.89285Z","iopub.status.idle":"2021-08-29T05:46:53.899431Z","shell.execute_reply.started":"2021-08-29T05:46:53.892812Z","shell.execute_reply":"2021-08-29T05:46:53.898262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The execution result is long, so please skip to <a href = '#section029'> here</a>.","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(sampler = optuna.samplers.RandomSampler(seed = 0))\nstudy.optimize(objective, n_trials = 100)\nprint(study.best_params)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:46:55.069165Z","iopub.execute_input":"2021-08-29T05:46:55.069561Z","iopub.status.idle":"2021-08-29T05:48:44.516157Z","shell.execute_reply.started":"2021-08-29T05:46:55.069526Z","shell.execute_reply":"2021-08-29T05:48:44.514604Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\"><a id = \"section029\">　</a>Using this, let's look at the classification results again.<br>-----<br>　これを使って、もう一度分類結果をみてみましょう。</p>","metadata":{}},{"cell_type":"code","source":"model = svm.SVC(kernel='rbf', C = 2164.383970701054, gamma = 0.001062561994257736)\nmodel.fit(train_X, train_Y)\npre_3 = model.predict(test_X)\nprint('Accuracy for rbf SVM is ', metrics.accuracy_score(pre_3, test_Y))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:49:14.86052Z","iopub.execute_input":"2021-08-29T05:49:14.860969Z","iopub.status.idle":"2021-08-29T05:49:14.960656Z","shell.execute_reply.started":"2021-08-29T05:49:14.860933Z","shell.execute_reply":"2021-08-29T05:49:14.959615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　It's getting high!! Let's look at something else.<br>-----<br>　高くなりました！！時間がかかるので、他にも見てみましょう。</p>","metadata":{}},{"cell_type":"code","source":"import optuna\n\ndef objective2(trial):\n    params = {\n        'n_neighbors': trial.suggest_int('n_neighbors', 1.0, 100.0)\n    }\n    model = KNeighborsClassifier(**params)\n    model.fit(train_X, train_Y)\n    pre = model.predict(test_X)\n    acc = metrics.accuracy_score(pre, test_Y)\n    score = 1 - acc\n    return score","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:27:19.316037Z","iopub.execute_input":"2021-08-29T05:27:19.316542Z","iopub.status.idle":"2021-08-29T05:27:19.322615Z","shell.execute_reply.started":"2021-08-29T05:27:19.31651Z","shell.execute_reply":"2021-08-29T05:27:19.321515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The execution result is also long, so please skip to <a href = '#section030'> here</a>.","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(sampler = optuna.samplers.RandomSampler(seed = 0))\nstudy.optimize(objective2, n_trials = 100)\nprint(study.best_params)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:27:20.614604Z","iopub.execute_input":"2021-08-29T05:27:20.615028Z","iopub.status.idle":"2021-08-29T05:27:23.407882Z","shell.execute_reply.started":"2021-08-29T05:27:20.614995Z","shell.execute_reply":"2021-08-29T05:27:23.406472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"section030\"></a>","metadata":{}},{"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors = 22)\nmodel.fit(train_X, train_Y)\npre_4 = model.predict(test_X)\nprint(metrics.accuracy_score(pre_4, test_Y))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:28:00.196159Z","iopub.execute_input":"2021-08-29T05:28:00.19702Z","iopub.status.idle":"2021-08-29T05:28:00.230163Z","shell.execute_reply.started":"2021-08-29T05:28:00.196976Z","shell.execute_reply":"2021-08-29T05:28:00.229071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective3(trial):\n    params = {\n        'C': trial.suggest_loguniform('C', 1e-10, 1e10)\n    }\n    model = LogisticRegression(**params)\n    model.fit(train_X, train_Y)\n    pre = model.predict(test_X)\n    acc = metrics.accuracy_score(pre, test_Y)\n    score = 1 - acc\n    return score","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:47:46.021976Z","iopub.execute_input":"2021-08-29T07:47:46.022413Z","iopub.status.idle":"2021-08-29T07:47:46.029507Z","shell.execute_reply.started":"2021-08-29T07:47:46.022379Z","shell.execute_reply":"2021-08-29T07:47:46.027991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The execution result is also long, so please skip to <a href = '#section031'> here</a>.","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(sampler = optuna.samplers.RandomSampler(seed = 0))\nstudy.optimize(objective3, n_trials = 100)\nprint(study.best_params)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:47:47.057263Z","iopub.execute_input":"2021-08-29T07:47:47.057656Z","iopub.status.idle":"2021-08-29T07:47:50.815743Z","shell.execute_reply.started":"2021-08-29T07:47:47.057623Z","shell.execute_reply":"2021-08-29T07:47:50.813945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"section031\"></a>","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression(C = 9.468257892380656)\nmodel.fit(train_X, train_Y)\npre_5 = model.predict(test_X)\nprint(metrics.accuracy_score(pre_5, test_Y))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:50:49.808182Z","iopub.execute_input":"2021-08-29T07:50:49.808545Z","iopub.status.idle":"2021-08-29T07:50:49.884136Z","shell.execute_reply.started":"2021-08-29T07:50:49.808516Z","shell.execute_reply":"2021-08-29T07:50:49.883061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">　This is getting high too! But, there is also a problem with this.<br>-----<br>　これも高くなりました！！ですが、これにも問題があるのです。</p>","metadata":{}},{"cell_type":"markdown","source":"# <b style = \"font-size:20px\">Problems on Data Partitioning</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　When evaluating different settings of estimators, for example when determining the argument: n_estimators of RandomForest, there is a possibility of overtraining on the testset because the parameters can be adjusted until the estimator is optimal. <br>　So, the solution is to prepare another part of the dataset as the validation data, train with the training data, then evaluate with the validation data, and when the experienment is successful, do the final evaluation with the test set.<br>　\nThis is where Cross Validation is used. We'll see this using a diagram. There are three steps. <ul>\n<li>1. Train the model using k-1 pieces of training data.\n<li>2. The model obtained in step 1 is evaluated on the rest part of the data(it plays an role of a test set in train_test_split.)\n<li>3. Using the parameters that got good results in step 1 and 2, we make a final evaluation on the test set.</ul><br>-----<br>　推定量の異なる設定を評価する際、例えばRandomForestの引数n_estimatorsを決める際、推定量が最適になるまでパラメータを調整できるため、テストセットで過学習する可能性があります。そこで、データセットの別の部分を検証データとして用意し、訓練データで訓練をした後検証データで評価を行い、実験がうまくいったときに、テストセットで最終的な評価を行うことで、解決します。<br>　そこで、Cross Validationを使います。図を用いて説明します。手順は3つです。\n<ul>\n    <li>１．訓練データの k-1 個分を使ってモデルを訓練する。</li>\n    <li>２．1で得られたモデルは、データの残りの部分にて評価される。（train_test_splitでいう、テストセットの役割を果たします。）</li>\n    <li>３．1，2で良い結果が得られたパラメータを使って、テストセットにて最終評価をする。</li>\n\n</ul>\n\n  <br><img src= \"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" alt =\"Titanic\" style=\"height: 450px; width:auto; align: left;\"></p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　In this case, we will use the most famous k-fold validation.<br>-----<br>　今回は、一番有名なk-fold cross validationを使います。</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:50:05.952306Z","iopub.execute_input":"2021-08-29T05:50:05.952731Z","iopub.status.idle":"2021-08-29T05:50:05.958227Z","shell.execute_reply.started":"2021-08-29T05:50:05.952697Z","shell.execute_reply":"2021-08-29T05:50:05.956667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits = 10, random_state = 22, shuffle=True)#the 'fold' in the above figure becomes 10.\ncv_mean = []\naccuracy = []\nstd = []\nclassifiers = ['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels = [svm.SVC(kernel = 'linear'), svm.SVC(kernel = 'rbf'), LogisticRegression(), KNeighborsClassifier(), DecisionTreeClassifier(), GaussianNB(), RandomForestClassifier()]\nfor m in models:\n    model = m\n    cv_result = cross_val_score(model, X, Y, cv = kfold, scoring = \"accuracy\")\n    cv_mean.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\n    \nnew_models_dataframe2 = pd.DataFrame({'CV Mean': cv_mean, 'Std': std}, index = classifiers)\nnew_models_dataframe2","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:50:42.932151Z","iopub.execute_input":"2021-08-29T05:50:42.932498Z","iopub.status.idle":"2021-08-29T05:50:46.809615Z","shell.execute_reply.started":"2021-08-29T05:50:42.932469Z","shell.execute_reply":"2021-08-29T05:50:46.80856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　Let's look at the case without feature selection.<br>-----<br>　特徴量選択しなかった場合でも見てみましょう。</p>","metadata":{}},{"cell_type":"code","source":"X_ = train_data[train_data.columns[1:]]\nY_ = train_data['Survived']\nkfold_ = KFold(n_splits = 10, random_state = 22, shuffle=True)#the 'fold' in the above figure becomes 10.\ncv_mean_ = []\naccuracy_ = []\nstd_ = []\nclassifiers_ = ['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels_ = [svm.SVC(kernel = 'linear'), svm.SVC(kernel = 'rbf'), LogisticRegression(), KNeighborsClassifier(), DecisionTreeClassifier(), GaussianNB(), RandomForestClassifier()]\nfor m in models_:\n    model = m\n    cv_result = cross_val_score(model, X_, Y_, cv = kfold, scoring = \"accuracy\")\n    cv_mean_.append(cv_result.mean())\n    std_.append(cv_result.std())\n    accuracy_.append(cv_result)\n    \nnew_models_dataframe2_ = pd.DataFrame({'CV Mean': cv_mean_, 'Std': std_}, index = classifiers_)\nnew_models_dataframe2_","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:51:01.27223Z","iopub.execute_input":"2021-08-29T05:51:01.272592Z","iopub.status.idle":"2021-08-29T05:51:05.445414Z","shell.execute_reply.started":"2021-08-29T05:51:01.27256Z","shell.execute_reply":"2021-08-29T05:51:05.444106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　In my environment, the result are below 0.8 for decision tree and naive Bayes. By Selecting features, we can get stable results with any classifier.<br>-----<br>　僕の環境では決定木とナイーブベイズで0.8を下回りました。特徴量選択によって、どの分類器でも安定した結果が得られるようになっています。</p>","metadata":{}},{"cell_type":"code","source":"plt.subplots(figsize = (12, 6))\nbox = pd.DataFrame(accuracy, index = [classifiers])\nbox.T.boxplot()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:51:31.682446Z","iopub.execute_input":"2021-08-29T05:51:31.682883Z","iopub.status.idle":"2021-08-29T05:51:31.942143Z","shell.execute_reply.started":"2021-08-29T05:51:31.682834Z","shell.execute_reply":"2021-08-29T05:51:31.940848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_models_dataframe2['CV Mean'].plot.barh(width = 0.8)\nplt.title('Ave CV Mean Accuracy')\nfig = plt.gcf()\nfig.set_size_inches(8, 5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:51:54.329507Z","iopub.execute_input":"2021-08-29T05:51:54.329886Z","iopub.status.idle":"2021-08-29T05:51:54.711316Z","shell.execute_reply.started":"2021-08-29T05:51:54.329834Z","shell.execute_reply":"2021-08-29T05:51:54.710068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　Using confusion matrix, we can get summary results of where the model went wrong and which classes are predicted incorrectly.<br>-----<br>　confusion matrix を使って、モデルがどこで間違えたのか、どのクラスを間違って予測したかの要約結果を得ることができます。</p>","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(3, 3, figsize = (12, 10))\ny_pred = cross_val_predict(LogisticRegression(), X, Y, cv = 10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax = ax[0, 0], annot = True, fmt = '2.0f')\nax[0, 0].set_title('LogisticRegression')\ny_pred = cross_val_predict(svm.SVC(kernel = 'rbf'), X, Y, cv = 10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax = ax[0, 1], annot = True, fmt = '2.0f')\nax[0, 1].set_title('rbf-SVM')\ny_pred = cross_val_predict(svm.SVC(kernel = 'linear'), X, Y, cv = 10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax = ax[0, 2], annot = True, fmt = '2.0f')\nax[0, 2].set_title('linear')\ny_pred = cross_val_predict(RandomForestClassifier(), X, Y, cv = 10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax = ax[1, 0], annot = True, fmt = '2.0f')\nax[1, 0].set_title('RandomForest')\ny_pred = cross_val_predict(DecisionTreeClassifier(), X, Y, cv = 10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax = ax[1, 1], annot = True, fmt = '2.0f')\nax[1, 1].set_title('DecisionTree')\ny_pred = cross_val_predict(KNeighborsClassifier(), X, Y, cv = 10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax = ax[1, 2], annot = True, fmt = '2.0f')\nax[1, 2].set_title('KNN')\ny_pred = cross_val_predict(GaussianNB(), X, Y, cv = 10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax = ax[2, 0], annot = True, fmt = '2.0f')\nax[2, 0].set_title('Naive Bayes')\nplt.subplots_adjust(hspace = 0.2, wspace = 0.2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:52:05.2528Z","iopub.execute_input":"2021-08-29T05:52:05.253175Z","iopub.status.idle":"2021-08-29T05:52:11.328672Z","shell.execute_reply.started":"2021-08-29T05:52:05.253146Z","shell.execute_reply":"2021-08-29T05:52:11.327453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　Here is how to look at the figure above. The diagonal line on the left shows the number of correct predictions for each class, and the diagonal line on the right shows the number of incorrect predictions. Let's take LogisticRegression as an example. <br>　\nThe correct prediction is 473 (dead) + 261 (survivors) / 891 = 0.824%. On the other hand, 76 dead people are mispredicted as survivors, and 81 survivors are mispredicted as dead people.<br>　\nLooking at this, rbf predicts the dead people most correctly, and Naive Bayes predicts the survivors most correctly.<br>-----<br>　上の図の見方を説明します。左の対角線は各クラスの正しい予測の数を、右の対角線は誤った予測の数を示しています。左上のLogisticRegressionを例にします。<br>　正しい予測は473(死者) + 261(生存者) / 891 = 0.824%です。逆に、76の死者が生存者と誤予測され、81の生存者が死者とご予測されています。<br>　これを見ると、rbfが一番正しい死者の予測し、Naive Bayesが一番正しい生存者の予測をしています。</p>","metadata":{}},{"cell_type":"markdown","source":"# <b style = \"font-size:30px\">Ensembling</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n　The RandomForest we did <a href = '#section16'>here</a> is also based on ensemble learning. We will explore the ensemble learning in depth here. <br>-----<br>　<a href = '#section16'>ここ</a>でやったRandomForestもアンサンブル学習に基づいたものです。そのアンサンブル学習を、ここで深掘りしていきます。</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n      This time, we will see the four main methods.<br>-----<br>　今回では、主に4つを紹介します。<br>　１．Voting<br>　２．Bagging<br>　３．Boosting<br>　４．Stacking<br><br>　Before we get into that, let's look at how amazing the ensemble using a little math. First, in a binary classification task, majority prediction can be described as follows.<br>-----<br>　その前に、少し数学を使って、アンサンブルのすごさを見てみましょう。先ず、二値分類タスクにおいて、多数決予測は次のように記述できます。<br>　\n$$C(x) = sign\\Biggr[\\sum_{j}^{m}C_j(x)\\Biggr] = \n\\begin{cases}\n1 \\quad (\\sum_jC_j(x) \\ge 0) \\\\\n-1 \\quad (\\sum_jC_j(x) < 0) \\\\\n\\end{cases}\n$$<br><br>　Assume that the misclassification rates of this base classifiers in binary classification task are all equal ε. Furthermore, we assume that each classifier is independant and there is no correlation in misclassification rate.<br>　\nUnder this assumption, we can express the misclassification rate of an ensemble of base classifiers as probability mass function of binomial distribution. <br>　In simple terms, this is probability that the number of classifiers that classify incorrectly, y, is more than or equal to k. <br>-----<br>　二値分類タスクのこのベース分類器の誤分類率がすべて等しくεであるとする。さらに、それぞれの分類器が独立し、誤分類率に相関がないものとします。この仮定の下、ベース分類器のアンサンブルの誤分類率を二項分布の確率質量関数として表せます。簡単に言うと、誤分類する分類器の個数yがk以上となる確率です。<br><br>$$P(y \\ge k) = \\sum_{k}^{n} (_nC_k) \\epsilon^k(1 - \\epsilon)^{n - k}$$<br><br>　This is probability of misclassification. Let's put in some numbers and see the probability that 11 classifiers get more than half wrong actually. However, we assume that the misclassification rate of each classifier ε is 0.25.<br>-----<br>　これは誤分類する確率です。実際に数値を入れてみましょう。11個の分類器が半分以上間違える確率を見ます。ただし、格分類器の誤分類率ε = 0.25とします。<br><br>$$P(y \\ge k) = \\sum_{k = 6}^{11} (_{11}C_k) 0.25^k (1 - 0.25)^{11-k} = 0.034$$<br>　If there conditions are met, we can see that the ensemble misclassification rate is 0.034, which is much lower than the classifier 0.25 here. This is amazing!!!<br>-----<br>　これらの条件が満たされていれば、アンサンブルの誤分類率は、ここの分類器0.25と比べて0.034と、各段に低いことが分かります。スゴイ！！！！！</p>","metadata":{}},{"cell_type":"code","source":"from scipy.special import comb\nimport math\nn_classifier = 11\nerror = 0.25\nstart_ = int(math.ceil(n_classifier / 2.))\nprobs = [(comb(n_classifier, k) * (error ** k) * (1 - error) ** (n_classifier - k)) for k in range(start_, n_classifier + 1)]\nsum(probs)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T05:59:33.289618Z","iopub.execute_input":"2021-08-29T05:59:33.290104Z","iopub.status.idle":"2021-08-29T05:59:33.299079Z","shell.execute_reply.started":"2021-08-29T05:59:33.290068Z","shell.execute_reply":"2021-08-29T05:59:33.298202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b style = \"font-size:20px; color: red\">Voting</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n      Voting is the most simple in ensemble. It takes a majority vote from a multi simple machine learning model combination. <br>-----<br>　Votingは、アンサンブルの中でも一番シンプルです。複数の単純な機械学習モデルの組み合わせから多数決を取ります。<br><br><img src= \"https://www.researchgate.net/publication/324014302/figure/fig2/AS:644424015040514@1530654066950/Majority-voting-algorithm.png\" alt =\"Titanic\" style=\"height: 400px; width:auto; align: left;\"><br><br>　To see the power of ensemble, we will try it without any parameter tuning.<br>-----<br>　アンサンブルの力を見るために、一切のパラメータチューニングをせずにやってみます。</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nensemble_voting = VotingClassifier(estimators = [\n    ('KNN', KNeighborsClassifier()),\n    ('rbf', svm.SVC(probability = True, kernel = 'rbf')),\n    ('RFC', RandomForestClassifier()),\n    ('LR', LogisticRegression()),\n    ('DT', DecisionTreeClassifier(random_state = 0)),\n    ('NB', GaussianNB()),\n    ('linear', svm.SVC(kernel = 'linear', probability = True))\n], voting = 'soft').fit(train_X, train_Y)\n\nprint(ensemble_voting.score(test_X, test_Y))\nprint(cross_val_score(ensemble_voting, X, Y, cv = 20, scoring = 'accuracy').mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-29T06:00:31.789589Z","iopub.execute_input":"2021-08-29T06:00:31.790038Z","iopub.status.idle":"2021-08-29T06:00:45.267806Z","shell.execute_reply.started":"2021-08-29T06:00:31.789998Z","shell.execute_reply":"2021-08-29T06:00:45.266176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n     Just by combining the two, we are able to get the stable high accuray. There are many more great methods, so let's look at them as we use them together.<br>-----<br>　組み合わせているだけで、安定した高精度が得られています。もっとすごいものがたくさんあるので、一緒に使いながら見ていきましょう。</p>","metadata":{}},{"cell_type":"markdown","source":"# <b style = \"font-size:20px; color: red\">Bagging</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n     　It is similar to <b>Voting</b>. However, while Voting uses the same training set for each classifier, Bagging extracts bootstrap samples from the first training set.<br>-----<br>　Votingと似ています。ですが、Votingはそれぞれの分類器に同じ訓練セットを使うのに対し、Baggingは最初の訓練セットからブーストラップ標本を抽出します。<br><br>　<img src= \"https://static.packt-cdn.com/products/9781787125933/graphics/B07030_07_06.jpg\" alt =\"Titanic\" style=\"height: 400px; width:auto; align: left;\"></p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n      Here, by using unpruned decision trees as the base classifier, we create an ensemble made by 500 decision trees and train it on bootstrap samples of different training datasets.<br>-----<br>　ここでは、剪定されていない決定木をベース分類器として利用することで、500個の決定木から成るアンサンブルを作成し、訓練データセットの異なるブートストラップ標本で学習させる。</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\ntree = DecisionTreeClassifier(max_depth = None)\nmodel = BaggingClassifier(base_estimator = tree, n_estimators = 800)\nmodel.fit(train_X, train_Y)\nbag_pre = model.predict(test_X)\nprint(metrics.accuracy_score(bag_pre, test_Y))\nprint(cross_val_score(model, X, Y, cv = 10, scoring = 'accuracy').mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-29T06:00:45.269451Z","iopub.execute_input":"2021-08-29T06:00:45.269938Z","iopub.status.idle":"2021-08-29T06:01:12.382433Z","shell.execute_reply.started":"2021-08-29T06:00:45.269855Z","shell.execute_reply":"2021-08-29T06:01:12.381071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n     　That is a much higher score than with a simple decision tree! <br>　\nThere is something what to check here. If the classification task is complex, or if the number of dataset's dimention is large, a single decision tree is likely to be overtrained. This is because the bias becomes low. <br>　So we have to bag an ensemble of classifiers with as low a bias as possible, such as an unpruned decision tree like this one.<br>-----<br>　単純な決定木のときよりスコアが上がっていますね！<br>　ここで確認したい点があります。分類タスクが複雑であったり、データセットの次元数が多かったりすると、単一な決定木では過学習に陥りやすいです。バイアスが低くなるからです。なので、今回のように剪定されていない決定木など、できるだけバイアスの低い分類器から成るアンサンブルでバギングしたいです。</p>","metadata":{}},{"cell_type":"markdown","source":"# <b style = \"font-size:20px; color: red\">Boosting</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n     　In boosting, the ensemble consists of a very simple (weak) base classifier. The performance of this learner, also known as a weak learner, is only slightly better than a guesser. We try to improve the performance of ensemble by training the misclassified training data to the weak learner later.<br>　\nThe procedure is as follows.<br>\n<ul>\n<li>1. Extract a random subset d1 of the training data from training dataset D in a non-recoverable manner and train weak learner C1. \n<li>2. Unrecoverably extract a second random training subset d2 from training data and add 50% of previously misclassified data points to train weak learner C2. \n<li>3. Identify the training data d3 that the result of C1 and C2 is different from training dataset D and train the third weak learner C3.\n<li>4. Combine the weak learner C1, C2 and C3 by majority vote.</ul><br>-----<br>　ブースティングでは、アンサンブルは非常に単純な（弱い）ベース分類器で構成されます。この弱学習器とも呼ばれる学習器の性能は、当て推量をわずかに上回る程度です。誤分類された訓練データを後から弱学習器に学習させることで、アンサンブルの性能向上を試みます。<br>　手順は以下の4つです。\n<ul>\n    <li>１．訓練データセットDから訓練データのランダムなサブセットd1を非復元抽出し、弱学習器C1を訓練する。</li>\n    <li>２．2つ目のランダムな訓練サブセットd2を訓練データから非復元抽出し、以前に誤分類されたデータ点の50%を追加して、弱学習器C2を訓練する。</li>\n    <li>３．訓練データセットDからC1とC2の結果が異なる訓練データd3を洗い出し、3つ目の弱学習器C3を学習する。</li>\n    <li>４．弱学習器C1, C2, C3を多数決により組み合わせる。</li>\n</ul></p>","metadata":{}},{"cell_type":"markdown","source":"<b style = \"font-size:20px\">Adaboost</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins'; sans-serif; font-size: 15px; font-weight: 500;\">\n     　Adaboost is the famous boosting algorithm.<br>-----<br>　アダブーストは、ブースティングのアルゴリズムで一番有名です。<br><br><img src= \"https://miro.medium.com/max/1400/0*KYszvMnr3nCtjaGy.png\" alt =\"Titanic\" style=\"height: 400px; width:auto; align: left;\"><br>　On the far left, the bounds are determined by minimizing the cost function (or the impurity that described previously). In the middle, the data points that is midclassified (circled) in the left are given a large weight, and the data point that is correctly classified are given small weights. Then, repeat the same way on the right, and the final output is determined by majority voting on each boundary.<br>-----<br>　一番左で、コスト関数（または前に説明した不純度）を最小化することによって境界を決定します。真ん中では、左で誤分類された（丸がついている）データ点の重みが大きくなり、正しく分類されたデータ点の重みが小さくなっています。そして、右で同じことを繰り返し、それぞれの境界の多数決で、最終的な出力が決定されます。</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(n_estimators = 200, random_state = 0, learning_rate = 0.1)\nresult = cross_val_score(ada, X, Y, scoring = 'accuracy')\nprint(result.mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-29T06:04:41.437252Z","iopub.execute_input":"2021-08-29T06:04:41.437649Z","iopub.status.idle":"2021-08-29T06:04:43.628722Z","shell.execute_reply.started":"2021-08-29T06:04:41.437617Z","shell.execute_reply":"2021-08-29T06:04:43.627699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b style = \"font-size:20px\">Gradient Boosting</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins'; sans-serif; font-size: 15px; font-weight: 500;\">\n     　The overall idea is the same as Adaboost. The different is that Adaboost used weights to identify the correct and incorrect answers of the previous model, while GradientBoosting uses gradients. <br>　To put it a bit more, AdaBoost tweaks the weights of instances in every interaction, while GradientBoosting tries to fit the new predictor to the residual error created by the previous predictor. <br>　In summary, there is a difference in the way it judge the shortcomings of weak learners.<br>-----<br>　全体的な考え方は、Adaboostと同じです。違いとしては、Adaboostでは、前のモデルの正解不正解を重みによって識別していましたが、GradientBoostingでは勾配を利用します。もう少し言うと、AdaBoostはすべての相互作用でインスタンスの重みを微調整するのですが、勾配ブースティングは、新しい予測子を前の予測子によって作成された残差エラーに適合させようとします。まとめると、弱学習器の欠点を判断する方法に違いがあるということです。</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngrad = GradientBoostingClassifier(n_estimators = 500, random_state = 0, learning_rate = 0.1)\nresult = cross_val_score(grad, X, Y, cv = 10, scoring = 'accuracy')\nprint(result.mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-29T06:08:49.321122Z","iopub.execute_input":"2021-08-29T06:08:49.321492Z","iopub.status.idle":"2021-08-29T06:08:55.004042Z","shell.execute_reply.started":"2021-08-29T06:08:49.321461Z","shell.execute_reply":"2021-08-29T06:08:55.002635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b style = \"font-size:20px\">XGBoost</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins'; sans-serif; font-size: 15px; font-weight: 500;\">\n     　You can think of this as a more computationally efficient implementation of GradientBoosting. The flow is the same.<br>-----<br>　GradientBoostingの計算効率の良くなった実装と思ってもらってよいと思います。流れは同じです。</p>","metadata":{}},{"cell_type":"code","source":"import xgboost as xg\nxgboost = xg.XGBClassifier()\nresult = cross_val_score(xgboost, X, Y, cv = 10, scoring  = 'accuracy')\nprint(result.mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-29T06:27:25.125751Z","iopub.execute_input":"2021-08-29T06:27:25.126491Z","iopub.status.idle":"2021-08-29T06:27:26.37413Z","shell.execute_reply.started":"2021-08-29T06:27:25.12645Z","shell.execute_reply":"2021-08-29T06:27:26.372144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b style = \"font-size:20px\">HistGradientBoostingClassifier</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins'; sans-serif; font-size: 15px; font-weight: 500;\">\n     　This is faster than XGBoost when the data is large. It uses OpenMP for parallelization and can learn even if there are missing values.<br>-----<br>　データが大きいと、XGBoostよりも高速です。並列化にOpenMPを利用しており、欠損値があっても学習できます。</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nhgb = HistGradientBoostingClassifier()\nres = cross_val_score(hgb, X, Y, cv = 10, scoring = 'accuracy')\nprint(res.mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-29T06:15:27.376788Z","iopub.execute_input":"2021-08-29T06:15:27.377205Z","iopub.status.idle":"2021-08-29T06:15:33.199024Z","shell.execute_reply.started":"2021-08-29T06:15:27.377172Z","shell.execute_reply":"2021-08-29T06:15:33.198005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins'; sans-serif; font-size: 15px; font-weight: 500;\">\n     　Let's search the parameter using optuna in XGBoost that is the most famous above.<br>-----<br>　上の中でも一番有名なXGBoostで、optunaを使ってパラメータを探索しましょう。</p>","metadata":{}},{"cell_type":"code","source":"def objective_xg(trial):\n    params = {\n        'eta': trial.suggest_loguniform('eta', 1e-8, 1.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n        'max_depth': trial.suggest_int('max_depth', 1, 20),\n        #'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-8, 1.0),\n        #'max_delta_step': trial.suggest_loguniform('max_delta_step', 1e-8, 1.0),\n        #'subsample': trial.suggest_uniform('subsample', 0.0, 1.0),\n        #'reg_lambda': trial.suggest_uniform('reg_lambda', 0.0, 1000.0),\n        #'reg_alpha': trial.suggest_uniform('reg_alpha', 0.0, 1000.0)\n    }\n    \n    model = xg.XGBClassifier(**params)\n    score = cross_val_score(model, X, Y, cv = 10, scoring = 'accuracy')\n    score = score.mean()\n\n    return 1 - score","metadata":{"execution":{"iopub.status.busy":"2021-08-29T06:23:19.60062Z","iopub.execute_input":"2021-08-29T06:23:19.601039Z","iopub.status.idle":"2021-08-29T06:23:19.608344Z","shell.execute_reply.started":"2021-08-29T06:23:19.601006Z","shell.execute_reply":"2021-08-29T06:23:19.60731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The execution result is very long, so please skip to <a href = '#section033'>here</a>.","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(sampler = optuna.samplers.RandomSampler(seed = 0))\nstudy.optimize(objective_xg, n_trials = 100)\nprint(study.best_params)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T06:23:21.962462Z","iopub.execute_input":"2021-08-29T06:23:21.962863Z","iopub.status.idle":"2021-08-29T06:25:50.084912Z","shell.execute_reply.started":"2021-08-29T06:23:21.962829Z","shell.execute_reply":"2021-08-29T06:25:50.083908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"section033\">　</a>","metadata":{}},{"cell_type":"code","source":"print(study.best_params['eta'],study.best_params['gamma'],\n                                study.best_params['max_depth']#,study.best_params['min_child_weight'],\n                                #study.best_params['max_delta_step'],study.best_params['subsample'],\n                                #study.best_params['reg_lambda'],study.best_params['reg_alpha']\n     )","metadata":{"execution":{"iopub.status.busy":"2021-08-29T06:26:35.365852Z","iopub.execute_input":"2021-08-29T06:26:35.366292Z","iopub.status.idle":"2021-08-29T06:26:35.373764Z","shell.execute_reply.started":"2021-08-29T06:26:35.366255Z","shell.execute_reply":"2021-08-29T06:26:35.372585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xg\nxgboost = xg.XGBClassifier(eta = study.best_params['eta'], gamma = study.best_params['gamma'], max_depth = study.best_params['max_depth']#, min_child_weight = study.best_params['min_child_weight'],\n                           #max_delta_step = study.best_params['max_delta_step'], subsample = study.best_params['subsample'], reg_lambda = study.best_params['reg_lambda'], reg_alpha = study.best_params['reg_alpha']\n                          )\nresult = cross_val_score(xgboost, X, Y, cv = 10, scoring  = 'accuracy')\nprint(result.mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-29T06:26:52.763514Z","iopub.execute_input":"2021-08-29T06:26:52.763957Z","iopub.status.idle":"2021-08-29T06:26:53.297945Z","shell.execute_reply.started":"2021-08-29T06:26:52.763919Z","shell.execute_reply":"2021-08-29T06:26:53.296986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b style = \"font-size:20px; color: red\">Stacking</b>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　Stacking is easy to understand if you think of it as an ensemble of two. The first layer consists of individual classifiers, which feed their predictions to the second layers. Then, in the second layer, another classifier is fitted to the predictions of the first layer to produce the final predictions. This time, we use <b>StackingClassifier</b><br>　\nAlso, <a href=http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/>StackingCVClassifier</a> is an upgraded version of the regular StackingClassifier using cross-validation. This one is not implemented in scikit-learn at this time, so please go to the URL and have a look!<br>-----<br>　スタッキングは、2個のアンサンブルと考えるとわかりやすいです。一つ目の層では、ここの分類器で構成されており、それらの分類器の予測値を二つ目の層に供給します。そして、二つ目の層では、別の分類器が一つ目の層の予測値に適合され、最終的な予測値を生成します。今回は、<b>StackingClassifier</b>を使用します。<br>　また、通常のStackingClassifierを、クロスバリデーションを使ってバージョンアップさせたものが<a href=http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/>StackingCVClassifier</a>になります。こちらは現時点ではscikit-learnに実装されていないので、URLに飛んで是非見てみてください！\n</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:20:23.485499Z","iopub.execute_input":"2021-08-29T07:20:23.485953Z","iopub.status.idle":"2021-08-29T07:20:23.492017Z","shell.execute_reply.started":"2021-08-29T07:20:23.485917Z","shell.execute_reply":"2021-08-29T07:20:23.490459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf1 = svm.SVC(kernel = 'rbf', C = 2164.383970701054, gamma = 0.001062561994257736)\nclf2 = LogisticRegression(C = 9.468257892380656)\nclf3 = KNeighborsClassifier(n_neighbors = 22)\nclf4 = GaussianNB()\nclf5 = RandomForestClassifier(criterion = 'gini', n_estimators = 22, random_state = 42)\n\nstacking_classifier_estimators = [\n    ('rbf', clf1), ('knc', clf3), ('gnv', clf4), ('rfc', clf5)\n]\nclf = StackingClassifier(\n    estimators = stacking_classifier_estimators, final_estimator = clf2\n)\n\nresult = cross_val_score(clf, X, Y, cv = 10, scoring = 'accuracy')\nprint(result.mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:54:22.017349Z","iopub.execute_input":"2021-08-29T07:54:22.017761Z","iopub.status.idle":"2021-08-29T07:54:31.212648Z","shell.execute_reply.started":"2021-08-29T07:54:22.017727Z","shell.execute_reply":"2021-08-29T07:54:31.211148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b style = \"font-size:20px; color: black\">Feature Importance</b>","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(2,2,figsize=(15,12))\nmodel=RandomForestClassifier(random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\nax[0,0].set_title('Feature Importance in Random Forests')\nmodel=AdaBoostClassifier(random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\nax[0,1].set_title('Feature Importance in AdaBoost')\nmodel=GradientBoostingClassifier(random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\nax[1,0].set_title('Feature Importance in Gradient Boosting')\nmodel=xg.XGBClassifier(learning_rate=0.1)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\nax[1,1].set_title('Feature Importance in XgBoost')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:51:23.766627Z","iopub.execute_input":"2021-08-29T07:51:23.767021Z","iopub.status.idle":"2021-08-29T07:51:25.518394Z","shell.execute_reply.started":"2021-08-29T07:51:23.766987Z","shell.execute_reply":"2021-08-29T07:51:25.517248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Poppins', sans-serif; font-size: 15px; font-weight: 500;\">\n    　With this, we can visually see which features were used. As expected, it seems that the initial (Mr), which had a high mortality rate, the gender (Male or Female), which shows a large difference, and Pclass which shows the rank passengers are recognized as the most important ones.<br>　\nNow let's submit actually. This time, we'll submit the result by StackingClassifier.<br>-----<br>　これを使うと、どの特徴量を使ったのかが視覚的にわかります。やはり、死亡率が高かったイニシャル(Mr)や、大きな差が見られた性別(Male or Female)、乗客の位を表すPclass辺りが重要度の高いものとして認識されているようです。<br>　では、実際に提出してみましょう。今回は<b>StackingClassifier</b>で分析した結果にします。\n</p>","metadata":{}},{"cell_type":"code","source":"last_model = StackingClassifier(estimators = stacking_classifier_estimators, final_estimator = clf2)\nlast_model.fit(X, Y)\nlast_prediction = last_model.predict(test_data_)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:07:47.244998Z","iopub.execute_input":"2021-08-29T08:07:47.245407Z","iopub.status.idle":"2021-08-29T08:07:48.336326Z","shell.execute_reply.started":"2021-08-29T08:07:47.245373Z","shell.execute_reply":"2021-08-29T08:07:48.335236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"last_prediction","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:08:05.146463Z","iopub.execute_input":"2021-08-29T08:08:05.147138Z","iopub.status.idle":"2021-08-29T08:08:05.158261Z","shell.execute_reply.started":"2021-08-29T08:08:05.147096Z","shell.execute_reply":"2021-08-29T08:08:05.156996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:57:14.285614Z","iopub.execute_input":"2021-08-29T07:57:14.286088Z","iopub.status.idle":"2021-08-29T07:57:14.297642Z","shell.execute_reply.started":"2021-08-29T07:57:14.28605Z","shell.execute_reply":"2021-08-29T07:57:14.296797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data['Survived'] = last_prediction.astype(int)\nsubmission_data","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:35:52.641254Z","iopub.execute_input":"2021-08-29T08:35:52.641729Z","iopub.status.idle":"2021-08-29T08:35:52.724336Z","shell.execute_reply.started":"2021-08-29T08:35:52.641621Z","shell.execute_reply":"2021-08-29T08:35:52.72271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data.to_csv('last_submission.csv', index = False)\nprint('Congratulations!! We did it!!')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:09:47.252503Z","iopub.execute_input":"2021-08-29T08:09:47.252931Z","iopub.status.idle":"2021-08-29T08:09:47.263699Z","shell.execute_reply.started":"2021-08-29T08:09:47.252897Z","shell.execute_reply":"2021-08-29T08:09:47.262906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Other Work( please **upvote**＜(_ _)＞ )\n\n* [The power of normality and visualization\n](https://www.kaggle.com/fightingmuscle/the-power-of-normality-and-visualization)\n* [EDA more technical🔥](https://www.kaggle.com/fightingmuscle/eda-more-technical)\n\n> These notebooks were put together by me as a beginner, so I believe anyone can learn from them!\n\n* [How did I get the silver medal?(0.717)【Infer】](https://www.kaggle.com/fightingmuscle/how-did-i-get-the-silver-medal-0-717-infer/comments)\n\n> I got silver medal for the first time! I published my inference code and trained models. My training code will be available soon, please wait. ","metadata":{}},{"cell_type":"markdown","source":"\n<h4 style = \n  \"color: #505050;\n  padding: 1.5em;\n  display: inline-block;\n  line-height: 2;\n  width: 600px;\n  background: #dbebf8;\n  vertical-align: middle;\n  border-radius: 30px 0px 0px 25px;\n  font-family:'Corben', cursive;\n  font-size:24px;\">\n    If this notebook is useful for you, <br>please <b>UPVOTE</b> and <b>COMMENTS</b>٩(๑❛ᴗ❛๑)۶ </h4>","metadata":{}}]}