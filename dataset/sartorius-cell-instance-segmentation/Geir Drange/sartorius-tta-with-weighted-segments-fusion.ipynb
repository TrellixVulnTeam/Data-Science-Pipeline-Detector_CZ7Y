{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro\nMost Kagglers are familiar with Weighted Boxes Fusion in object detection. What about Weighted Segments Fusion (WSF) for use in instance segmentation? There are papers on the topic, but not much code, so I decided to give it a try. There are many possible approaches to this problem; my take on it is a two-stage process: Start with WBF on the bounding boxes, and then fuse the segments based on the WBF output. WSF can of course used for ensembling models too, not only TTA.   \n\nReferences:\n  * [Create COCO annotations for Sartorius dataset](https://www.kaggle.com/mistag/sartorius-create-coco-annotations)\n  * [Cell shape analysis](https://www.kaggle.com/mistag/sartorius-cell-shape-analysis)\n  * [Trained CenterMask2 model](https://www.kaggle.com/mistag/train-sartorius-detectron2-centermask2)\n  * [Competition Metric : mAP IoU](https://www.kaggle.com/theoviel/competition-metric-map-iou)\n  * [Weighted boxes fusion](https://github.com/ZFTurbo/Weighted-Boxes-Fusion) by [ZFTurbo](https://kaggle.com/zfturbo)","metadata":{}},{"cell_type":"markdown","source":"# Libraries installation\nInstall Detectron2, WBF, Centermask2 et.al.","metadata":{}},{"cell_type":"code","source":"!pip install --no-index \\\n../input/detectron2-download-code-for-offline-install-ii/detectron2/detectron2-0.6-cp37-cp37m-linux_x86_64.whl \\\n--find-links=../input/detectron2-download-code-for-offline-install-ii/detectron2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-06T15:59:33.452066Z","iopub.execute_input":"2021-12-06T15:59:33.452744Z","iopub.status.idle":"2021-12-06T15:59:53.179969Z","shell.execute_reply.started":"2021-12-06T15:59:33.452672Z","shell.execute_reply":"2021-12-06T15:59:53.179116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --no-index \\\n/kaggle/input/wbf-download-code-for-offline-installation/Weighted-Boxes-Fusion/ensemble_boxes-1.0.7-py3-none-any.whl \\\n--find-links=/kaggle/input/wbf-download-code-for-offline-installation/Weighted-Boxes-Fusion","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-06T15:59:53.1829Z","iopub.execute_input":"2021-12-06T15:59:53.183616Z","iopub.status.idle":"2021-12-06T16:00:00.238216Z","shell.execute_reply.started":"2021-12-06T15:59:53.183574Z","shell.execute_reply":"2021-12-06T16:00:00.237347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -R ../input/train-sartorius-detectron2-centermask2/centermask2 centermask2","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-06T16:00:00.240813Z","iopub.execute_input":"2021-12-06T16:00:00.241535Z","iopub.status.idle":"2021-12-06T16:00:14.067446Z","shell.execute_reply.started":"2021-12-06T16:00:00.241491Z","shell.execute_reply":"2021-12-06T16:00:14.066542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, json, cv2, random\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport glob, gc\nfrom ensemble_boxes import nms\nfrom skimage import measure\nimport albumentations as A\nfrom skimage import measure\nfrom scipy import stats\nimport torch\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-06T16:00:14.070804Z","iopub.execute_input":"2021-12-06T16:00:14.071075Z","iopub.status.idle":"2021-12-06T16:00:18.691365Z","shell.execute_reply.started":"2021-12-06T16:00:14.071037Z","shell.execute_reply":"2021-12-06T16:00:18.690663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create tracking WBF\nThe WBF algorithm returns fused boxes, and we need to keep track of which original boxes were used for each fused box to process the correct masks afterwards. A tracking WBF function is created below that returns a list of original boxes are created below. This will be stage one in the WSF algorithm.","metadata":{}},{"cell_type":"code","source":"%%writefile wbf_tracking.py\n# coding: utf-8\n__author__ = 'ZFTurbo: https://kaggle.com/zfturbo'\n# Modified by Mista G: https://www.kaggle.com/mistag\n\nimport warnings\nimport numpy as np\nfrom numba import jit\n\n@jit(nopython=True)\ndef bb_intersection_over_union(A, B) -> float:\n    xA = max(A[0], B[0])\n    yA = max(A[1], B[1])\n    xB = min(A[2], B[2])\n    yB = min(A[3], B[3])\n\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA) * max(0, yB - yA)\n\n    if interArea == 0:\n        return 0.0\n\n    # compute the area of both the prediction and ground-truth rectangles\n    boxAArea = (A[2] - A[0]) * (A[3] - A[1])\n    boxBArea = (B[2] - B[0]) * (B[3] - B[1])\n\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    return iou\n\n\ndef prefilter_boxes(boxes, scores, labels, weights, thr):\n    # Create dict with boxes stored by its label\n    new_boxes = dict()\n\n    for t in range(len(boxes)):\n\n        if len(boxes[t]) != len(scores[t]):\n            print('Error. Length of boxes arrays not equal to length of scores array: {} != {}'.format(len(boxes[t]), len(scores[t])))\n            sys.exit()\n\n        if len(boxes[t]) != len(labels[t]):\n            print('Error. Length of boxes arrays not equal to length of labels array: {} != {}'.format(len(boxes[t]), len(labels[t])))\n            sys.exit()\n\n        for j in range(len(boxes[t])):\n            score = scores[t][j]\n            if score < thr:\n                continue\n            label = int(labels[t][j])\n            box_part = boxes[t][j]\n            x1 = max(float(box_part[0]), 0.)\n            y1 = max(float(box_part[1]), 0.)\n            x2 = max(float(box_part[2]), 0.)\n            y2 = max(float(box_part[3]), 0.)\n\n            # Box data checks\n            if x2 < x1:\n                warnings.warn('X2 < X1 value in box. Swap them.')\n                x1, x2 = x2, x1\n            if y2 < y1:\n                warnings.warn('Y2 < Y1 value in box. Swap them.')\n                y1, y2 = y2, y1\n            if x1 > 1:\n                warnings.warn('X1 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                x1 = 1\n            if x2 > 1:\n                warnings.warn('X2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                x2 = 1\n            if y1 > 1:\n                warnings.warn('Y1 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                y1 = 1\n            if y2 > 1:\n                warnings.warn('Y2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                y2 = 1\n            if (x2 - x1) * (y2 - y1) == 0.0:\n                warnings.warn(\"Zero area box skipped: {}.\".format(box_part))\n                continue\n\n            # [label, score, weight, model index, x1, y1, x2, y2]\n            b = [int(label), float(score) * weights[t], weights[t], t, x1, y1, x2, y2]\n            if label not in new_boxes:\n                new_boxes[label] = []\n            new_boxes[label].append(b)\n\n    # Sort each list in dict by score and transform it to numpy array\n    for k in new_boxes:\n        current_boxes = np.array(new_boxes[k])\n        new_boxes[k] = current_boxes[current_boxes[:, 1].argsort()[::-1]]\n\n    return new_boxes\n\n\ndef get_weighted_box(boxes, conf_type='avg'):\n    \"\"\"\n    Create weighted box for set of boxes\n    :param boxes: set of boxes to fuse\n    :param conf_type: type of confidence one of 'avg' or 'max'\n    :return: weighted box (label, score, weight, x1, y1, x2, y2)\n    \"\"\"\n\n    box = np.zeros(8, dtype=np.float32)\n    conf = 0\n    conf_list = []\n    w = 0\n    for b in boxes:\n        box[4:] += (b[1] * b[4:])\n        conf += b[1]\n        conf_list.append(b[1])\n        w += b[2]\n    box[0] = boxes[0][0]\n    if conf_type == 'avg':\n        box[1] = conf / len(boxes)\n    elif conf_type == 'max':\n        box[1] = np.array(conf_list).max()\n    elif conf_type in ['box_and_model_avg', 'absent_model_aware_avg']:\n        box[1] = conf / len(boxes)\n    box[2] = w\n    box[3] = -1 # model index field is retained for consistensy but is not used.\n    box[4:] /= conf\n    return box\n\n\ndef find_matching_box(boxes_list, new_box, match_iou):\n    best_iou = match_iou\n    best_index = -1\n    for i in range(len(boxes_list)):\n        box = boxes_list[i]\n        if box[0] != new_box[0]:\n            continue\n        iou = bb_intersection_over_union(box[4:], new_box[4:])\n        if iou > best_iou:\n            best_index = i\n            best_iou = iou\n\n    return best_index, best_iou\n\n\ndef weighted_boxes_fusion_tracking(boxes_list, scores_list, labels_list, weights=None, iou_thr=0.55, skip_box_thr=0.0, conf_type='avg', allows_overflow=False):\n    '''\n    :param boxes_list: list of boxes predictions from each model, each box is 4 numbers.\n    It has 3 dimensions (models_number, model_preds, 4)\n    Order of boxes: x1, y1, x2, y2. We expect float normalized coordinates [0; 1]\n    :param scores_list: list of scores for each model\n    :param labels_list: list of labels for each model\n    :param weights: list of weights for each model. Default: None, which means weight == 1 for each model\n    :param iou_thr: IoU value for boxes to be a match\n    :param skip_box_thr: exclude boxes with score lower than this variable\n    :param conf_type: how to calculate confidence in weighted boxes. 'avg': average value, 'max': maximum value, 'box_and_model_avg': box and model wise hybrid weighted average, 'absent_model_aware_avg': weighted average that takes into account the absent model.\n    :param allows_overflow: false if we want confidence score not exceed 1.0\n\n    :return: boxes: boxes coordinates (Order of boxes: x1, y1, x2, y2).\n    :return: scores: confidence scores\n    :return: labels: boxes labels\n    :return: wbfo: original boxes coordinates for each fused box\n    '''\n\n    if weights is None:\n        weights = np.ones(len(boxes_list))\n    if len(weights) != len(boxes_list):\n        print('Warning: incorrect number of weights {}. Must be: {}. Set weights equal to 1.'.format(len(weights), len(boxes_list)))\n        weights = np.ones(len(boxes_list))\n    weights = np.array(weights)\n\n    if conf_type not in ['avg', 'max', 'box_and_model_avg', 'absent_model_aware_avg']:\n        print('Unknown conf_type: {}. Must be \"avg\", \"max\" or \"box_and_model_avg\", or \"absent_model_aware_avg\"'.format(conf_type))\n        sys.exit()\n\n    filtered_boxes = prefilter_boxes(boxes_list, scores_list, labels_list, weights, skip_box_thr)\n    if len(filtered_boxes) == 0:\n        return np.zeros((0, 4)), np.zeros((0,)), np.zeros((0,)), np.zeros((0, 4))\n    \n    overall_boxes = []\n    original_boxes = []\n    for label in filtered_boxes:\n        boxes = filtered_boxes[label]\n        new_boxes = []\n        weighted_boxes = []\n        # Clusterize boxes\n        for j in range(0, len(boxes)):\n            index, best_iou = find_matching_box(weighted_boxes, boxes[j], iou_thr)\n            if index != -1:\n                new_boxes[index].append(boxes[j])\n                weighted_boxes[index] = get_weighted_box(new_boxes[index], conf_type)\n            else:\n                new_boxes.append([boxes[j].copy()])\n                weighted_boxes.append(boxes[j].copy())\n        # Rescale confidence based on number of models and boxes\n        original_boxes.append(new_boxes)\n        for i in range(len(new_boxes)):\n            clustered_boxes = np.array(new_boxes[i])\n            if conf_type == 'box_and_model_avg':\n                # weighted average for boxes\n                weighted_boxes[i][1] = weighted_boxes[i][1] * len(clustered_boxes) / weighted_boxes[i][2]\n                # identify unique model index by model index column\n                _, idx = np.unique(clustered_boxes[:, 3], return_index=True)\n                # rescale by unique model weights\n                weighted_boxes[i][1] = weighted_boxes[i][1] *  clustered_boxes[idx, 2].sum() / weights.sum()\n            elif conf_type == 'absent_model_aware_avg':\n                # get unique model index in the cluster\n                models = np.unique(clustered_boxes[:, 3]).astype(int)\n                # create a mask to get unused model weights\n                mask = np.ones(len(weights), dtype=bool)\n                mask[models] = False\n                # absent model aware weighted average\n                weighted_boxes[i][1] = weighted_boxes[i][1] * len(clustered_boxes) / (weighted_boxes[i][2] + weights[mask].sum())\n            elif conf_type == 'max':\n                weighted_boxes[i][1] = weighted_boxes[i][1] / weights.max()\n            elif not allows_overflow:\n                weighted_boxes[i][1] = weighted_boxes[i][1] * min(len(weights), len(clustered_boxes)) / weights.sum()\n            else:\n                weighted_boxes[i][1] = weighted_boxes[i][1] * len(clustered_boxes) / weights.sum()\n        overall_boxes.append(np.array(weighted_boxes))\n    overall_boxes = np.concatenate(overall_boxes, axis=0)\n    sidx = overall_boxes[:, 1].argsort()\n    overall_boxes = overall_boxes[sidx[::-1]]\n    boxes = overall_boxes[:, 4:]\n    scores = overall_boxes[:, 1]\n    labels = overall_boxes[:, 0]\n    # sort originals accoring to wbf\n    original_boxes = original_boxes[0]\n    wbfo = [original_boxes[i] for i in sidx[::-1]]\n    return boxes, scores, labels, wbfo","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T16:00:18.694737Z","iopub.execute_input":"2021-12-06T16:00:18.694941Z","iopub.status.idle":"2021-12-06T16:00:18.705264Z","shell.execute_reply.started":"2021-12-06T16:00:18.694912Z","shell.execute_reply":"2021-12-06T16:00:18.704488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wbf_tracking import weighted_boxes_fusion_tracking","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:00:18.707717Z","iopub.execute_input":"2021-12-06T16:00:18.707968Z","iopub.status.idle":"2021-12-06T16:00:18.723192Z","shell.execute_reply.started":"2021-12-06T16:00:18.707935Z","shell.execute_reply":"2021-12-06T16:00:18.722519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import trained model\nThen we need a trained model to test on. We'll use [this one based on Detectron2](https://www.kaggle.com/mistag/train-sartorius-detectron2-centermask2), which is pretty poor performing, but plenty good enough for this exercise.  \n\nIn the first part, we configure the model to max 20 detections, to have some easy data to work on.","metadata":{}},{"cell_type":"code","source":"#Configure model for inference:\n%cd /kaggle/working/centermask2\n\nfrom centermask.config import get_cfg # important! Use get_cfg from the centermask repo and not Detectron2\n\ncfg = get_cfg()\ncfg.merge_from_file(\"./configs/centermask/test.yaml\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\ncfg.MODEL.WEIGHTS = './output/model_final.pth'\ncfg.MODEL.FCOS.POST_NMS_TOPK_TEST = 20 # Max number of detections per image\npredictor20 = DefaultPredictor(cfg)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-06T16:00:18.724264Z","iopub.execute_input":"2021-12-06T16:00:18.724531Z","iopub.status.idle":"2021-12-06T16:00:22.775055Z","shell.execute_reply.started":"2021-12-06T16:00:18.724491Z","shell.execute_reply":"2021-12-06T16:00:22.774234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we read train.csv from the data set and the test data set that was used in training. The latter will be used to evaluate the performance of the TTA function.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sartorius-cell-instance-segmentation/train.csv')\ndf = df.groupby('id').agg(list).reset_index()\n\nfor col in df.columns[2:]:\n    df[col] = df[col].apply(\n        lambda x: np.unique(x)[0] if len(np.unique(x)) == 1 else np.unique(x)\n    )\n\nwith open ('/kaggle/input/sartorius-create-coco-annotations/test_fold_0.json', 'r') as f:\n    tset = json.load(f)\n\ntest_fids = []\nfor i in range(len(tset['images'])):\n    test_fids.append(tset['images'][i]['file_name'].split('/')[-1].split('.')[0])\n\nshsy5y_fids, astro_fids, cort_fids = [],[],[]\nfor fid in test_fids:\n    ctype = df[df.id == fid].cell_type.iloc[0]\n    if ctype == 'shsy5y':\n        shsy5y_fids.append(fid)\n    elif ctype == 'astro':\n        astro_fids.append(fid)\n    else:\n        cort_fids.append(fid)\n\nprint('Test files per class: shsy5y={}, astro={}, cort={}'.format(len(shsy5y_fids), len(astro_fids), len(cort_fids)))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T16:00:22.776588Z","iopub.execute_input":"2021-12-06T16:00:22.776979Z","iopub.status.idle":"2021-12-06T16:00:24.37627Z","shell.execute_reply.started":"2021-12-06T16:00:22.776937Z","shell.execute_reply":"2021-12-06T16:00:24.375481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TTA\nThe TTA function is defined below. The image is transformed according to the TTA list, predictions are made after each transform, and the predicted boxes and masks are transformed back to the original orientation, before everything is returned as lists. We create a dict with box coordinates as keys to the mask list, so we can fetch the correct mask in stage two of WSF.\n\nThe TT augmentations will be:\n  * Horizontal flip\n  * Vertical flip\n  * Rotate 180 degrees  \n  \nAlong with the original, this will result in 4 predictions per image.","metadata":{}},{"cell_type":"code","source":"# create dict key from bbox\ndef bbox_to_key(bbox):\n    return str(np.round(bbox, 6))\n\n# TTA inputs:\n# file: image to process\n# predictor_list: list of predictors to use, single or multiple for ensembling\n# aug_list: list of augmentations to perform. Augmentations must be \"bidirectional\" - applying twice will get back to original.\n#           Also augmentations must support image, bboxes and masks\ndef TTA(file, predictor_list, aug_list=[None]):\n    boxes = []\n    box_scores = []\n    masks = []\n    masks_lkup =[]\n    pclass = []\n    im = cv2.imread(file)\n    for predict in predictor_list:\n        for aug in aug_list:\n            # perform augmentations\n            if aug is not None:\n                transform = aug\n                ima = transform(image=im)['image']\n            else:\n                ima = im\n            # make prediction\n            pred = predict(ima)\n            h, w = pred['instances'].image_size[0], pred['instances'].image_size[1]\n            classes = pred['instances'].pred_classes.cpu().numpy()-1\n            if len(pclass) == 0:\n                pclass = classes\n            else:\n                pclass = np.concatenate((pclass, classes))\n            # get box predictions, and nomrmalize to 0-1 range\n            pred_boxes = [A.normalize_bbox(box, h, w) for box in pred['instances'].pred_boxes.tensor.cpu().numpy()]\n            # transform back to original\n            if aug is not None:\n                pred_boxes = transform(image=ima, bboxes=pred_boxes)['bboxes']\n            # get mask prediction\n            pred_masks = pred['instances'].pred_masks.cpu().numpy()*1\n            # transform back to original\n            if aug is not None:\n                pred_masks = transform(image=ima, masks=pred_masks)['masks']\n            # lookup table for bbox to mask index reference\n            pred_dict = {}\n            for i in range(len(pred_boxes)):\n                pred_dict[bbox_to_key(pred_boxes[i])] = i\n            # append results to list\n            boxes.append(np.array(pred_boxes))\n            box_scores.append(np.array(pred['instances'].scores.detach().cpu().numpy()))\n            masks.append(np.array(pred_masks, dtype=np.uint8))\n            masks_lkup.append(pred_dict)\n    \n            del pred, pred_boxes, pred_masks, ima, pred_dict\n    \n    del im\n    gc.collect()\n    predicted_class = stats.mode(pclass)[0][0]\n    return boxes, box_scores, masks, masks_lkup, predicted_class","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:04:56.932917Z","iopub.execute_input":"2021-12-06T16:04:56.933197Z","iopub.status.idle":"2021-12-06T16:04:56.948515Z","shell.execute_reply.started":"2021-12-06T16:04:56.933166Z","shell.execute_reply":"2021-12-06T16:04:56.946521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test TTA\nWe will run TTA on an image from the validation data set, created in [this notebook](https://www.kaggle.com/mistag/sartorius-create-coco-annotations). The predictor is configured to return only 20 detections. The predictions are visualized on the augmented images.","metadata":{}},{"cell_type":"code","source":"TITLES = ['Original', 'Horizontal flip', 'Vertical flip', 'Rotation 180']\n\ndef plt_pred(file):\n    fig = plt.figure(figsize=(20,15))\n    im = cv2.imread(file)\n    # org\n    fig.add_subplot(2, 2, 1)\n    plt.tight_layout()\n    outputs = predictor20(im)\n    v = Visualizer(im[:, :, ::-1])\n    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    plt.title(TITLES[0])\n    plt.imshow(out.get_image()[:, :, ::-1]);\n    # flip horizontal/vertical/both\n    for i in range(1, -2, -1):\n        imh = cv2.flip(im, i)\n        fig.add_subplot(2, 2, 3-i)\n        plt.tight_layout()\n        outputs = predictor20(imh)\n        v = Visualizer(imh[:, :, ::-1])\n        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n        plt.title(TITLES[2-i])\n        plt.imshow(out.get_image()[:, :, ::-1]);\n\nFILE = '/kaggle/input/sartorius-cell-instance-segmentation/train/'+cort_fids[0]+'.png'\nplt_pred(FILE)","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T16:05:15.542756Z","iopub.execute_input":"2021-12-06T16:05:15.543489Z","iopub.status.idle":"2021-12-06T16:05:24.220903Z","shell.execute_reply.started":"2021-12-06T16:05:15.543428Z","shell.execute_reply":"2021-12-06T16:05:24.219782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, call the TTA function and plot output predictions that shall be mapped back to original image:","metadata":{}},{"cell_type":"code","source":"AUGMENTATIONS = [None, A.HorizontalFlip(p=1.0), A.VerticalFlip(p=1.0), A.Rotate(limit=(180,180), p=1.0)]\n\nimg_org = cv2.imread(FILE)\nboxes, box_scores, masks, masks_lkup, pred_class = TTA(FILE, [predictor20], AUGMENTATIONS)\n\ndef show_boxes(im, boxes_list, h, w, color=(31, 119, 180), orig=False):\n    thickness = 2\n    idx = 0\n    if orig:\n        idx = 4\n    for i in range(len(boxes_list)):\n        x1 = int(h * boxes_list[i][idx])\n        y1 = int(w * boxes_list[i][idx+1])\n        x2 = int(h * boxes_list[i][idx+2])\n        y2 = int(w * boxes_list[i][idx+3])\n        cv2.rectangle(im, (x1, y1), (x2, y2), color, thickness)\n    return im\n\nfig = plt.figure(figsize=(20, 15))\ncolumns = 2\nrows = (len(AUGMENTATIONS)//columns) + (len(AUGMENTATIONS) % 2)\nfor i in range(1,len(AUGMENTATIONS)+1):\n    fig.add_subplot(rows, columns, i)\n    plt.tight_layout()\n    img = img_org\n    img = show_boxes(img, boxes[i-1], img.shape[1], img.shape[0])\n    plt.title(TITLES[i-1])\n    plt.imshow(img)\n    \nplt.show();","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T16:06:00.97391Z","iopub.execute_input":"2021-12-06T16:06:00.97416Z","iopub.status.idle":"2021-12-06T16:06:03.377149Z","shell.execute_reply.started":"2021-12-06T16:06:00.974132Z","shell.execute_reply":"2021-12-06T16:06:03.376412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Boxes are all ready for the WSF algorithm. Next, do the same check with the masks:","metadata":{}},{"cell_type":"code","source":"def show_masks(im, masks_list, h, w):\n    m = np.zeros((w,h), dtype=np.uint8)\n    for i in range(len(masks_list)):\n        m = np.logical_or(m, masks_list[i])\n    return im * np.dstack([m]*3)\n\nfig = plt.figure(figsize=(20, 15))\ncolumns = 2\nrows = (len(AUGMENTATIONS)//columns) + (len(AUGMENTATIONS) % 2)\nfor i in range(1,len(AUGMENTATIONS)+1):\n    fig.add_subplot(rows, columns, i)\n    plt.tight_layout()\n    img = img_org\n    plt.imshow(show_masks(img, masks[i-1], img.shape[1], img.shape[0]))\n    plt.title(TITLES[i-1])\nplt.show();","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T16:06:22.014699Z","iopub.execute_input":"2021-12-06T16:06:22.015163Z","iopub.status.idle":"2021-12-06T16:06:22.942525Z","shell.execute_reply.started":"2021-12-06T16:06:22.015126Z","shell.execute_reply":"2021-12-06T16:06:22.941776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All OK - the TTA function is working!","metadata":{}},{"cell_type":"markdown","source":"# Weighted Segments Fusion\nThe Weighted Segments Fusion algorithm is:\n  * Make predictions from augmented images (transform boxes and masks back to original) or ensemble models\n  * Run WBF on boxes (while keepig track of original boxes and masks)\n  * Fuse masks and apply threshold\n  * Crop fused mask to WBF box  \n  \nThe first bullet is handled by the `TTA()` function, and the second bullet is handled by the `weighted_boxes_fusion_tracking()` function. The function `get_wsf_mask()` defined below that takes care of the  last two bullets. ","metadata":{}},{"cell_type":"code","source":"# Fuse masks that belong to fused boxes\ndef get_wsf_mask(wbf_box, wbf_org, pmasks, pmasks_lkup, thres=0.5):\n    w, h = 520, 704\n    mask = np.zeros((w, h), dtype=np.uint8)\n    for i in range(len(wbf_org)):\n        key = bbox_to_key(wbf_org[i][4:])\n        model = int(wbf_org[i][3])\n        try:\n            ind = pmasks_lkup[model][key]\n            mask = mask + pmasks[model][ind]\n        except:\n            pass\n    # convert thres to integer based on number of boxes\n    threshold = max(1, int(thres*len(wbf_org)))\n    # remove pixels outside WBF box\n    m2 = np.zeros((w, h), dtype=np.uint8)\n    x1 = max(0, int(h * wbf_box[0]))\n    y1 = max(0, int(w * wbf_box[1]))\n    x2 = min(h, int(h * wbf_box[2]))\n    y2 = min(w, int(w * wbf_box[3]))\n    m2[y1:y2, x1:x2] = 1\n    mask = (mask >= threshold) * m2\n    return mask.astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:07:16.167884Z","iopub.execute_input":"2021-12-06T16:07:16.168518Z","iopub.status.idle":"2021-12-06T16:07:16.179549Z","shell.execute_reply.started":"2021-12-06T16:07:16.168471Z","shell.execute_reply":"2021-12-06T16:07:16.17797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WSF Implementation\nBelow we define a function `get_masks_tta()` that implements the WSF algorithm for TTA. For completeness we also define `get_masks()` that only makes prediction on a single image.","metadata":{}},{"cell_type":"code","source":"DEBUG = False\n\nTHRESHOLDS = [.53, .45, .55]\nCLASS_LABELS = ['background', 'shsy5y', 'astro', 'cort']\n\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n# Implement WSF for TTA\ndef get_masks_tta(fn, pred_list, aug_list, thres=.5):\n    im = cv2.imread(str(fn))\n    h, w = im.shape[0], im.shape[1]\n    # do TTA\n    boxes, box_scores, masks, masks_lkup, pred_class = TTA(str(fn), pred_list, aug_list)\n    # create dummy labels\n    labels = []\n    for i in range(len(boxes)):\n        labels.append(np.ones(len(boxes[i]), dtype=np.uint8))\n    # weighted boxes fusion\n    wbf_boxes, wbf_scores, _, wbf_originals = weighted_boxes_fusion_tracking(boxes, \n                                                                             box_scores, \n                                                                             labels_list=labels, \n                                                                             iou_thr=0.55, \n                                                                             skip_box_thr=THRESHOLDS[pred_class])\n    # Finally, process masks, making sure there is no overlap\n    res = []\n    used = np.zeros(im.shape[:2], dtype=int)\n    # extract limits from DataFrame\n    pred_label = CLASS_LABELS[pred_class+1]\n    min_key = pred_label+' min'\n    major_axis_len_min = cell_df[cell_df.feature == 'major_axis_len'][min_key].iloc[0]\n    # process\n    for i in range(len(wbf_boxes)):\n        mask = get_wsf_mask(wbf_boxes[i], wbf_originals[i], masks, masks_lkup, thres=thres)\n        # get shape properties\n        try:\n            props = measure.regionprops(mask)\n        except:\n            continue\n        # if there are multiple separated masks, pick the larger one\n        areas = []\n        for a in range(len(props)):\n            areas.append(props[a].area)\n        try:\n            target = np.argmax(areas)\n        except:\n            continue\n        # extract properties of interest \n        major_axis_len = props[target].major_axis_length\n        # check against limits\n        if major_axis_len >= major_axis_len_min:\n            mask = mask * (1-used)\n            # check if mask is chopped up by previous detections\n            if len(measure.find_contours(mask, 0.5, positive_orientation='low')) == 1:\n                used += mask\n                res.append(rle_encode(mask))\n            else:\n                if DEBUG:\n                    print('{}: Chopped\\'n\\'dropped #{}'.format(fn.split('/')[-1], i))\n        else:\n            if DEBUG:\n                print('{}: Failed limits #{}'.format(fn.split('/')[-1], i))\n                \n    if DEBUG:\n        print('{}: {}, {} boxes of {} left after processing'.format(fn.split('/')[-1], pred_label, len(res), len(wbf_boxes)))\n                \n    del boxes, box_scores, masks, masks_lkup\n    del wbf_boxes, wbf_scores, labels, wbf_originals\n    gc.collect()\n          \n    return res\n\n# Prediction on single image\ndef get_masks(fn, predictor, thres=-1):\n    im = cv2.imread(str(fn))\n    pred = predictor(im)\n    h, w = pred['instances'].image_size[0], pred['instances'].image_size[1]\n    # we are going to apply non-max suppression, so scale boxes to 0-1 range\n    pred_boxes = [A.normalize_bbox(box, h, w) for box in pred['instances'].pred_boxes.tensor.cpu().numpy()]\n    # create dict with scores and masks with bbox as key\n    pred_dict = {}\n    for i in range(len(pred_boxes)):\n        pred_dict[str(np.round(pred_boxes[i], 6))] = {'box_score': float(pred['instances'].scores[i].cpu().numpy()),\n                                                      'mask_score': float(pred['instances'].mask_scores[i].cpu().numpy()),\n                                                      'mask': pred['instances'][i].pred_masks.cpu().numpy()[0]*1}\n    # non-max suppression (cannot use WBF here or other methods that change the boxes)\n    boxes, scores, _ = nms([pred_boxes], \n                           [pred['instances'].scores.detach().cpu().numpy()], \n                           [np.ones(len(pred['instances'].scores))],\n                           weights=None, \n                           iou_thr=0.5)\n    # class is category with most predictions\n    pred_class = torch.mode(pred['instances'].pred_classes)[0] - 1\n    # drop predictions with score below threshold\n    if thres == -1:\n        take = scores >= THRESHOLDS[pred_class]\n    else:\n        take = scores >= thres\n    boxes = boxes[take]\n    # Finally, process masks, making sure there is no overlap\n    res = []\n    used = np.zeros(im.shape[:2], dtype=int)\n    # extract limits from DataFrame\n    pred_label = CLASS_LABELS[pred_class+1]\n    min_key = pred_label+' min'\n    major_axis_len_min = cell_df[cell_df.feature == 'major_axis_len'][min_key].iloc[0]\n    # process\n    for i in range(len(boxes)):\n        key = str(np.round(boxes[i], 6))\n        if key in pred_dict:\n            mask = pred_dict[key]['mask']\n            # get shape properties\n            try:\n                props = measure.regionprops(mask)\n            except:\n                continue\n            # if there are multiple separated masks, pick the larger one\n            areas = []\n            for a in range(len(props)):\n                areas.append(props[a].area)\n            try:\n                target = np.argmax(areas)\n            except:\n                continue\n            # use filled image as new mask\n            mask2 = np.zeros(im.shape[:2], dtype=int)\n            try:\n                mask2[props[target].bbox[0]:props[target].bbox[2], props[target].bbox[1]:props[target].bbox[3]] = props[target].filled_image\n            except:\n                continue\n            # extract properties of interest \n            major_axis_len = props[target].major_axis_length\n            # check against limits\n            if major_axis_len >= major_axis_len_min:\n                mask2 = mask2 * (1-used)\n                # check if mask is chopped up by previous detections\n                if len(measure.find_contours(mask2, 0.5, positive_orientation='low')) == 1:\n                    used += mask2\n                    res.append(rle_encode(mask2))\n                else:\n                    if DEBUG:\n                        print('{}: Chopped\\'n\\'dropped #{}'.format(fn.split('/')[-1], i))\n            else:\n                if DEBUG:\n                    print('{}: Failed limits #{}'.format(fn.split('/')[-1], i))\n        else:\n            if DEBUG:\n                print('{}: Missing key #{}'.format(fn.split('/')[-1], key))\n    if DEBUG:\n        print('{}: {}, {} boxes of {} left after processing'.format(fn.split('/')[-1], pred_label, len(res), len(boxes)))\n        \n    del boxes, scores, pred_dict\n    gc.collect()\n    return res","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T16:12:23.04941Z","iopub.execute_input":"2021-12-06T16:12:23.049785Z","iopub.status.idle":"2021-12-06T16:12:23.08227Z","shell.execute_reply.started":"2021-12-06T16:12:23.04975Z","shell.execute_reply":"2021-12-06T16:12:23.081482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import [cell shape data](https://www.kaggle.com/mistag/sartorius-cell-shape-analysis):","metadata":{}},{"cell_type":"code","source":"cell_df = pd.read_pickle('/kaggle/input/sartorius-cell-shape-analysis/shape_data.pkl')\ncell_df","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:08:01.277142Z","iopub.execute_input":"2021-12-06T16:08:01.277695Z","iopub.status.idle":"2021-12-06T16:08:01.307631Z","shell.execute_reply.started":"2021-12-06T16:08:01.27765Z","shell.execute_reply":"2021-12-06T16:08:01.306946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Configure model for competition grade detections:","metadata":{}},{"cell_type":"code","source":"from centermask.config import get_cfg # important! Use get_cfg from the centermask repo and not Detectron2\n\ncfg = get_cfg()\ncfg.merge_from_file(\"./configs/centermask/test.yaml\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\ncfg.MODEL.WEIGHTS = './output/model_final.pth'\ncfg.MODEL.FCOS.POST_NMS_TOPK_TEST = 800 # Max number of detections per image\npredictor = DefaultPredictor(cfg)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:08:04.988486Z","iopub.execute_input":"2021-12-06T16:08:04.989036Z","iopub.status.idle":"2021-12-06T16:08:05.939441Z","shell.execute_reply.started":"2021-12-06T16:08:04.988997Z","shell.execute_reply":"2021-12-06T16:08:05.938694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## IOU metric\nWe use IOU calulations from [this notebook](https://www.kaggle.com/theoviel/competition-metric-map-iou). ","metadata":{}},{"cell_type":"code","source":"def rles_to_mask(encs, shape):\n    \"\"\"\n    Decodes a rle.\n\n    Args:\n        encs (list of str): Rles for each class.\n        shape (tuple [2]): Mask size.\n\n    Returns:\n        np array [shape]: Mask.\n    \"\"\"\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint)\n    for m, enc in enumerate(encs):\n        if isinstance(enc, np.float) and np.isnan(enc):\n            continue\n        enc_split = enc.split()\n        for i in range(len(enc_split) // 2):\n            start = int(enc_split[2 * i]) - 1\n            length = int(enc_split[2 * i + 1])\n            img[start: start + length] = 1 + m\n    return img.reshape(shape)\n\ndef compute_iou(labels, y_pred):\n    \"\"\"\n    Computes the IoU for instance labels and predictions.\n\n    Args:\n        labels (np array): Labels.\n        y_pred (np array): predictions\n\n    Returns:\n        np array: IoU matrix, of size true_objects x pred_objects.\n    \"\"\"\n\n    true_objects = len(np.unique(labels))\n    pred_objects = len(np.unique(y_pred))\n\n    # Compute intersection between all objects\n    intersection = np.histogram2d(\n        labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects)\n    )[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins=true_objects)[0]\n    area_pred = np.histogram(y_pred, bins=pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n    iou = intersection / union\n    \n    return iou[1:, 1:]  # exclude background\n\ndef precision_at(threshold, iou):\n    \"\"\"\n    Computes the precision at a given threshold.\n\n    Args:\n        threshold (float): Threshold.\n        iou (np array): IoU matrix.\n\n    Returns:\n        int: Number of true positives,\n        int: Number of false positives,\n        int: Number of false negatives.\n    \"\"\"\n    matches = iou > threshold\n    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n    tp, fp, fn = (\n        np.sum(true_positives),\n        np.sum(false_positives),\n        np.sum(false_negatives),\n    )\n    return tp, fp, fn\n\ndef iou_map(truths, preds, verbose=0):\n    \"\"\"\n    Computes the metric for the competition.\n    Masks contain the segmented pixels where each object has one value associated,\n    and 0 is the background.\n\n    Args:\n        truths (list of masks): Ground truths.\n        preds (list of masks): Predictions.\n        verbose (int, optional): Whether to print infos. Defaults to 0.\n\n    Returns:\n        float: mAP.\n    \"\"\"\n    ious = [compute_iou(truth, pred) for truth, pred in zip(truths, preds)]\n\n    if verbose:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        tps, fps, fns = 0, 0, 0\n        for iou in ious:\n            tp, fp, fn = precision_at(t, iou)\n            tps += tp\n            fps += fp\n            fns += fn\n\n        p = tps / (tps + fps + fns)\n        prec.append(p)\n\n        if verbose:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tps, fps, fns, p))\n\n    if verbose:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n\n    return np.mean(prec)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T16:08:08.986436Z","iopub.execute_input":"2021-12-06T16:08:08.986725Z","iopub.status.idle":"2021-12-06T16:08:09.003188Z","shell.execute_reply.started":"2021-12-06T16:08:08.986694Z","shell.execute_reply":"2021-12-06T16:08:09.002326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test WSF implementation\nBelow we will test prediction on an image from the test set of the trained model, and compare the score of TTA+WSF algorithm with plain vanilla prediction:","metadata":{}},{"cell_type":"code","source":"# helper function to vizualize prediction\ndef viz_pred(fid, use_tta=False):\n    idx = df.index[df.id == fid][0]\n    shape = df[['height', 'width']].values[idx]\n    rles = df[df.id == fid].annotation.iloc[0]\n    masks = rles_to_mask(rles, shape).astype(np.uint16)\n    fig, ax = plt.subplots(1, 2, figsize=(24,12))\n    fig.tight_layout()\n    plt.subplot(1,2,1)\n    plt.imshow(masks)\n    plt.axis(False)\n    plt.title('Ground truth {}'.format(fid))\n    plt.subplot(1,2,2)\n    f = '/kaggle/input/sartorius-cell-instance-segmentation/train/'+fid+'.png'\n    if not use_tta:\n        y_masks = get_masks(f, predictor)\n    else:\n        y_masks = get_masks_tta(f, [predictor], AUGMENTATIONS)\n    y_mask = rles_to_mask(y_masks, shape).astype(np.uint16)\n    plt.imshow(y_mask)\n    plt.axis(False)\n    ap = iou_map([masks] , [y_mask], verbose=0)\n    plt.title('Prediction, mAP={}'.format(round(ap,3)))\n    plt.show();","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T16:12:29.983024Z","iopub.execute_input":"2021-12-06T16:12:29.983602Z","iopub.status.idle":"2021-12-06T16:12:29.992273Z","shell.execute_reply.started":"2021-12-06T16:12:29.98356Z","shell.execute_reply":"2021-12-06T16:12:29.991534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"viz_pred(test_fids[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:12:30.192033Z","iopub.execute_input":"2021-12-06T16:12:30.192587Z","iopub.status.idle":"2021-12-06T16:12:36.663006Z","shell.execute_reply.started":"2021-12-06T16:12:30.192544Z","shell.execute_reply":"2021-12-06T16:12:36.662387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"viz_pred(test_fids[0], use_tta=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:12:36.664469Z","iopub.execute_input":"2021-12-06T16:12:36.665091Z","iopub.status.idle":"2021-12-06T16:12:55.261009Z","shell.execute_reply.started":"2021-12-06T16:12:36.665054Z","shell.execute_reply":"2021-12-06T16:12:55.260383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mAP is increased from 0.091 to 0.157 - pretty good! Another one:","metadata":{}},{"cell_type":"code","source":"viz_pred(test_fids[10])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:14:20.761272Z","iopub.execute_input":"2021-12-06T16:14:20.761554Z","iopub.status.idle":"2021-12-06T16:14:22.760882Z","shell.execute_reply.started":"2021-12-06T16:14:20.761524Z","shell.execute_reply":"2021-12-06T16:14:22.76018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"viz_pred(test_fids[10], use_tta=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:14:24.46909Z","iopub.execute_input":"2021-12-06T16:14:24.469648Z","iopub.status.idle":"2021-12-06T16:14:30.633467Z","shell.execute_reply.started":"2021-12-06T16:14:24.469608Z","shell.execute_reply":"2021-12-06T16:14:30.632629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"mAP is improved from .304 to .456! Of course, the TTA+WSF does not always result in an improvement on every image. However, by running the scoring function on the entire test set (122 images), the improvement from single prediction to TTA+WSF is:  \n\n| Class | mAP | TTA+WSF mAP |\n|-------|-----|-------------|\n| astro |0.048|        0.054|\n|  cort |0.269|        0.295|\n|shsy5y |0.140|        0.146|  \n\nEven though the trained model used here is pretty poor performing, the WSF+TTA definately gives an improvement!","metadata":{}},{"cell_type":"markdown","source":"# Summary\nIn this notebook a Weighted Segments Fusion algorithm has been created that gives a noticeable improvement in prediction score. If someone tries this on an already good performing model, please give a comment if an improvement was still observed.  \n\nNew test: I applied WSF to the model [in this notebook](https://www.kaggle.com/mistag/pred-sartorius-detectron2-centermask2) and the PB score went fro 2.14 to 2.40 - almost 10% improvement!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}