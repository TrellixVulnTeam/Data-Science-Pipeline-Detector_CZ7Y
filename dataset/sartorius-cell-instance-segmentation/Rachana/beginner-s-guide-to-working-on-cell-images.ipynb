{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:40:35.202855Z","iopub.execute_input":"2021-12-29T13:40:35.203309Z","iopub.status.idle":"2021-12-29T13:40:35.234958Z","shell.execute_reply.started":"2021-12-29T13:40:35.203201Z","shell.execute_reply":"2021-12-29T13:40:35.233832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('/kaggle/input/sartorius-cell-instance-segmentation/')","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:40:35.237023Z","iopub.execute_input":"2021-12-29T13:40:35.237279Z","iopub.status.idle":"2021-12-29T13:40:35.249363Z","shell.execute_reply.started":"2021-12-29T13:40:35.237251Z","shell.execute_reply":"2021-12-29T13:40:35.248663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction:\n1. Neurological disorders, including neurodegenerative diseases such as Alzheimer's and brain tumors, are a leading cause of death and disability across the globe. However, it is hard to quantify how well these deadly disorders respond to treatment. \n\n2. One accepted method is to review neuronal cells via light microscopy, which is both accessible and non-invasive. Unfortunately, segmenting individual neuronal cells in microscopic images can be challenging and time-intensive. Accurate instance segmentation of these cells—with the help of computer vision—could lead to new and effective drug discoveries to treat the millions of people with these disorders.\n\n3. Current solutions have limited accuracy for neuronal cells in particular. In internal studies to develop cell instance segmentation models, the neuroblastoma cell line SH-SY5Y consistently exhibits the lowest precision scores out of eight different cancer cell types tested. This could be because neuronal cells have a very unique, irregular and concave morphology associated with them, making them challenging to segment with commonly used mask heads.\n\n4. In this competition, you’ll detect and delineate distinct objects of interest in biological images depicting neuronal cell types commonly used in the study of neurological disorders. More specifically, you'll use phase contrast microscopy images to train and test your model for instance segmentation of neuronal cells. Successful models will do this with a high level of accuracy.\n","metadata":{}},{"cell_type":"markdown","source":"# data\nIn this competition we are segmenting neuronal cells in images. The training annotations are provided as run length encoded masks, and the images are in PNG format. The number of images is small, but the number of annotated objects is quite high. The hidden test set is roughly 240 images.\n\nFiles: \nA. train.csv - IDs and masks for all training objects. None of this metadata is provided for the test set.\n1. id - unique identifier for object\n2. annotation - run length encoded pixels for the identified neuronal cell\n3. width - source image width\n4. height - source image height\n5. cell_type - the cell line\n6. plate_time - time plate was created\n\nB. sample_submission.csv - a sample submission file in the correct format\n\nC. train - train images in PNG format\n\nD. test - test images in PNG format. Only a few test set images are available for download; the remainder can only be accessed by your notebooks when you submit.\n\nE. train_semi_supervised - unlabeled images offered in case you want to use additional data for a semi-supervised approach.\n\nLIVECell_dataset_2021 - A mirror of the data from the LIVECell dataset. LIVECell is the predecessor dataset to this competition. You will find extra data for the SH-SHY5Y cell line, plus several other cell lines not covered in the competition dataset that may be of interest for transfer learning.\n\n","metadata":{}},{"cell_type":"code","source":"# import libraries:\nimport pandas as pd\nimport numpy as np\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib.colors import ListedColormap\n\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.losses import BinaryCrossentropy\n\n\nfrom statistics import mean","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:38:06.430738Z","iopub.execute_input":"2021-12-29T14:38:06.43134Z","iopub.status.idle":"2021-12-29T14:38:06.444109Z","shell.execute_reply.started":"2021-12-29T14:38:06.431267Z","shell.execute_reply":"2021-12-29T14:38:06.442956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getImagePaths(path):\n    \"\"\"\n    Function to Combine Directory Path with individual Image Paths\n    \n    parameters: path(string) - Path of directory\n    returns: image_names(string) - Full Image Path\n    \"\"\"\n    image_names = []\n    for dirname, _, filenames in os.walk(path):\n        for filename in tqdm(filenames):\n            fullpath = os.path.join(dirname, filename)\n            image_names.append(fullpath)\n    return image_names","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:40:43.907531Z","iopub.execute_input":"2021-12-29T13:40:43.907889Z","iopub.status.idle":"2021-12-29T13:40:43.914886Z","shell.execute_reply.started":"2021-12-29T13:40:43.907848Z","shell.execute_reply":"2021-12-29T13:40:43.913851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get complete image paths for train and test datasets\nDIRECTORY_PATH = \"../input/sartorius-cell-instance-segmentation\"\nTRAIN_CSV = DIRECTORY_PATH + \"/train.csv\"\nTRAIN_PATH = DIRECTORY_PATH + \"/train\"\nTEST_PATH = DIRECTORY_PATH + \"/test\"\nTRAIN_SEMI_SUPERVISED_PATH = DIRECTORY_PATH + \"/train_semi_supervised\"\nSAMPLE_SUBMISSION_PATH = DIRECTORY_PATH + \"/sample_submission.csv\"\n\ntrain_images_path = getImagePaths(TRAIN_PATH)\ntest_images_path = getImagePaths(TEST_PATH)\ntrain_semi_supervised_path = getImagePaths(TRAIN_SEMI_SUPERVISED_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:40:43.916574Z","iopub.execute_input":"2021-12-29T13:40:43.916939Z","iopub.status.idle":"2021-12-29T13:40:44.669323Z","shell.execute_reply.started":"2021-12-29T13:40:43.916892Z","shell.execute_reply":"2021-12-29T13:40:44.668773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Meta Data\nThe meta data is given only for the train data, meaning that they cannot be used to predict the test data but they should be used to construct a solid cross validation strategy. So let's start understanding the statistical properties of the meta data.\n\nThe meta data, which is given in the train.csv file, contains 7 categorical and 2 numerical features (see table below). Each row points to an image with the id column and its associated mask with the annotation column. The annotations are given in the \"run length encoded pixels\" format. Furthermore each row contains the cell type information.","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(TRAIN_CSV)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:40:57.680946Z","iopub.execute_input":"2021-12-29T13:40:57.68159Z","iopub.status.idle":"2021-12-29T13:40:58.359813Z","shell.execute_reply.started":"2021-12-29T13:40:57.681552Z","shell.execute_reply":"2021-12-29T13:40:58.359033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_train.head())\ndisplay(df_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:40:58.386579Z","iopub.execute_input":"2021-12-29T13:40:58.387165Z","iopub.status.idle":"2021-12-29T13:40:58.413912Z","shell.execute_reply.started":"2021-12-29T13:40:58.387128Z","shell.execute_reply":"2021-12-29T13:40:58.41255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:40:59.062338Z","iopub.execute_input":"2021-12-29T13:40:59.062701Z","iopub.status.idle":"2021-12-29T13:40:59.161509Z","shell.execute_reply.started":"2021-12-29T13:40:59.062661Z","shell.execute_reply":"2021-12-29T13:40:59.159586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# column wise unique values:\ndf_train.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:40:59.738769Z","iopub.execute_input":"2021-12-29T13:40:59.739573Z","iopub.status.idle":"2021-12-29T13:40:59.879658Z","shell.execute_reply.started":"2021-12-29T13:40:59.739529Z","shell.execute_reply":"2021-12-29T13:40:59.878895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#number of images in each directory:\nprint(f\"Number of train images: {len(train_images_path)}\")\nprint(f\"Number of test images:  {len(test_images_path)}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:00.306125Z","iopub.execute_input":"2021-12-29T13:41:00.306715Z","iopub.status.idle":"2021-12-29T13:41:00.312027Z","shell.execute_reply.started":"2021-12-29T13:41:00.306676Z","shell.execute_reply":"2021-12-29T13:41:00.311223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are only 606 images in the train set, which is small for training neural network models and can easily lead to an overfitting problem. However, it's well known that this problem can be easily mitigated with the use of appropriate augmentation techniques (e.g. Ronneberger et al. 2015).\n\nAll the images have the same shape: (704 x 520) px. This is nice to have in a dataset because there won't be any complications due to a variable image resolution.","metadata":{}},{"cell_type":"code","source":"print(f'Number of unique images: {df_train.id.nunique()}')\nprint(f'Do all the images have a width of 704: {(df_train[\"width\"]==704).all()}')\nprint(f'Do all the images have a height of 520: {(df_train[\"height\"]==520).all()}')","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:01.234711Z","iopub.execute_input":"2021-12-29T13:41:01.235362Z","iopub.status.idle":"2021-12-29T13:41:01.249337Z","shell.execute_reply.started":"2021-12-29T13:41:01.235324Z","shell.execute_reply":"2021-12-29T13:41:01.248548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\n\ninstances_per_image = df_train.groupby('id').size().sort_values()\n\n#instances_per_image.index = range(606)\n#instances_per_image.median()\ninstances_per_image.plot.bar(ax=ax)\n\nax.set_xticklabels([])\nax.set_xlabel('Images')\nax.set_ylabel('Number of Instances')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:01.746527Z","iopub.execute_input":"2021-12-29T13:41:01.746958Z","iopub.status.idle":"2021-12-29T13:41:05.066451Z","shell.execute_reply.started":"2021-12-29T13:41:01.746907Z","shell.execute_reply":"2021-12-29T13:41:05.065757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of instances in each image is remarkably variable (see figure below). Some statistical measures are as follows:\n\n1. Most of the images have more than 47 instances annotated.\n2. The minimum number of instances is 4.\n3. The maximum number of instances is 790.\n\nThese numbers are extremely critical to train an instance segmentation model. For instance, the famous Mask RCNN model requires the information of \"maximum number of detections\".","metadata":{}},{"cell_type":"markdown","source":"Cell Types Distribution:\n\nEach image is annotated with one of the three cell types: shsy5y, asto, and cort. \n\nThe distribution of the cell types is shown in the figure below. \n\nWhile the most represented cell type is shsy5y (70%) in the train set, cell types cort and astro are annotated only ~10% each. This means that the data is biased towards cell type shsy5y.","metadata":{}},{"cell_type":"code","source":"#distribution of cell types in %:\ncell_types = df_train.cell_type.value_counts()\ncell_types = cell_types/df_train.shape[0]*100\ncell_types.plot.bar()\nplt.title('Cell Type Distribution')\nplt.ylabel(' Percentage of instances')\nplt.xlabel('Cell Type')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:05.068Z","iopub.execute_input":"2021-12-29T13:41:05.068691Z","iopub.status.idle":"2021-12-29T13:41:05.268762Z","shell.execute_reply.started":"2021-12-29T13:41:05.068655Z","shell.execute_reply":"2021-12-29T13:41:05.268079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. The number of unique id and cell_type combinations is equal to the number of unique images. This means that each image is associated with a unique cell type! See the result below. This also explains the distribution of the image numbers in the train.csv file. The images observed abundantly are associated with the most observed cell type shsy5y.\n\n2. Since each image is associated with only 1 cell type, we can count the number of images associated with each cell type. The figure below shows that most of the images are associated with the cell type cort, which agrees with our previous findings.","metadata":{}},{"cell_type":"code","source":"# no of imges associated with each cell type:\nfig, ax = plt.subplots(1, 1)\ndf_train.groupby(['id','cell_type'])['cell_type'].first().value_counts().plot.bar(ax=ax)\nax.set_ylabel('Number of Images')\nax.set_xlabel('Cell Types')\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:05.270194Z","iopub.execute_input":"2021-12-29T13:41:05.270698Z","iopub.status.idle":"2021-12-29T13:41:05.457903Z","shell.execute_reply.started":"2021-12-29T13:41:05.270654Z","shell.execute_reply":"2021-12-29T13:41:05.457083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#distribution of plate time:\nplate_time = df_train.plate_time.value_counts()\nplate_time = plate_time/df_train.shape[0]*100\nplate_time.plot.bar()\nplt.title('Plate Time Distribution')\nplt.ylabel(' Percentage of instances')\nplt.xlabel('Plate Time')","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:05.460047Z","iopub.execute_input":"2021-12-29T13:41:05.46038Z","iopub.status.idle":"2021-12-29T13:41:05.681834Z","shell.execute_reply.started":"2021-12-29T13:41:05.460336Z","shell.execute_reply":"2021-12-29T13:41:05.681097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#distribution of elasped time:\nelasped_time = df_train.elapsed_timedelta.value_counts()\nelasped_time = elasped_time/df_train.shape[0]*100\nelasped_time.plot.bar()\nplt.title('Elasped Time Distribution')\nplt.ylabel(' Percentage of instances')\nplt.xlabel('Elasped Time')","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:05.683111Z","iopub.execute_input":"2021-12-29T13:41:05.683599Z","iopub.status.idle":"2021-12-29T13:41:05.925342Z","shell.execute_reply.started":"2021-12-29T13:41:05.683553Z","shell.execute_reply":"2021-12-29T13:41:05.924105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Images EDA and visualisation:\n1. All images defined in train_df are of the same size - 520 * 704\n2. Number of annotations per image are very varied with the minimum being 4 and maximum being 790","metadata":{}},{"cell_type":"code","source":"# image size: All images defined in train_df are of the same size - 520 * 704\ndf_train[[\"height\", \"width\"]].describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:05.927379Z","iopub.execute_input":"2021-12-29T13:41:05.927916Z","iopub.status.idle":"2021-12-29T13:41:05.957475Z","shell.execute_reply.started":"2021-12-29T13:41:05.927871Z","shell.execute_reply":"2021-12-29T13:41:05.956671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# annotations count:\nannot_counts = df_train.groupby('id')[['annotation']].count().sort_values('annotation')\nannot_counts","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:06.310031Z","iopub.execute_input":"2021-12-29T13:41:06.310322Z","iopub.status.idle":"2021-12-29T13:41:06.348404Z","shell.execute_reply.started":"2021-12-29T13:41:06.310291Z","shell.execute_reply":"2021-12-29T13:41:06.347748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annot_counts.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:06.726041Z","iopub.execute_input":"2021-12-29T13:41:06.727026Z","iopub.status.idle":"2021-12-29T13:41:06.742519Z","shell.execute_reply.started":"2021-12-29T13:41:06.726981Z","shell.execute_reply":"2021-12-29T13:41:06.741559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('ggplot')\nplt.figure(figsize = (10, 6))\nplt.hist(annot_counts, bins = 50, alpha = 0.8)\nplt.xlabel(\"Number of annotations\")\nplt.ylabel(\"Count\")\nplt.title(\"Number of annotations per image\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:07.183357Z","iopub.execute_input":"2021-12-29T13:41:07.184241Z","iopub.status.idle":"2021-12-29T13:41:07.462486Z","shell.execute_reply.started":"2021-12-29T13:41:07.184197Z","shell.execute_reply":"2021-12-29T13:41:07.46141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_multiple_img(images_paths, rows, cols):\n    \"\"\"\n    Function to Display Images from Dataset.\n    \n    parameters: images_path(string) - Paths of Images to be displayed\n                rows(int) - No. of Rows in Output\n                cols(int) - No. of Columns in Output\n    \"\"\"\n    figure, ax = plt.subplots(nrows=rows,ncols=cols,figsize=(16,8) )\n    for ind,image_path in enumerate(images_paths):\n        image=cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n        try:\n            ax.ravel()[ind].imshow(image)\n            ax.ravel()[ind].set_axis_off()\n        except:\n            continue;\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:07.643777Z","iopub.execute_input":"2021-12-29T13:41:07.644131Z","iopub.status.idle":"2021-12-29T13:41:07.652595Z","shell.execute_reply.started":"2021-12-29T13:41:07.644098Z","shell.execute_reply":"2021-12-29T13:41:07.651545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display train images:\ndisplay_multiple_img(train_images_path[100:150], 2,2)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:08.122481Z","iopub.execute_input":"2021-12-29T13:41:08.122855Z","iopub.status.idle":"2021-12-29T13:41:09.638734Z","shell.execute_reply.started":"2021-12-29T13:41:08.122817Z","shell.execute_reply":"2021-12-29T13:41:09.637596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display train semisupervised images:\ndisplay_multiple_img(train_semi_supervised_path[100:150], 2, 2)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:09.641377Z","iopub.execute_input":"2021-12-29T13:41:09.641698Z","iopub.status.idle":"2021-12-29T13:41:11.030108Z","shell.execute_reply.started":"2021-12-29T13:41:09.641663Z","shell.execute_reply":"2021-12-29T13:41:11.028693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#display test images:\ndisplay_multiple_img(test_images_path, 1, 3)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:11.031373Z","iopub.execute_input":"2021-12-29T13:41:11.032166Z","iopub.status.idle":"2021-12-29T13:41:11.445421Z","shell.execute_reply.started":"2021-12-29T13:41:11.032125Z","shell.execute_reply":"2021-12-29T13:41:11.444356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's time to look at the images and the masks now. The figures below shows randomly selected images corresponding to each of the three distinct cell types. Each cell type has its own unique morphological properties.\n\nastro instances are the biggest in shape. They cover a lot of space in the masks.\ncort instances are smaller than the other cell types in general and they are in circle-like shapes. They don't cover much space in the masks.\nshsy5y instances are slightly bigger, elongated and more abundant than the cort instances. They cover more space than the cort cells.","metadata":{}},{"cell_type":"code","source":"def make_mask(mask_files, image_shape=(520, 704), color=False):\n    mask = np.zeros(image_shape).ravel()\n    for i, mask_file in enumerate(mask_files):\n        couples = np.array(mask_file.split()).reshape(-1, 2).astype(int)\n        couples[:, 1] = couples[:, 0] + couples[:, 1]\n        for couple in couples:\n            if color:\n                mask[couple[0]: couple[1]] = i\n            else:\n                mask[couple[0]: couple[1]] = 1\n    mask = mask.reshape(520, 704)\n    return mask\n\ndef plot_image(image_id='0030fd0e6378'):\n    fig, ax = plt.subplots(1, 2, figsize=(14,5))\n    cell_type = df_train.loc[df_train['id'] == image_id, 'cell_type'][0:1].values\n    \n    file_name = os.path.join(\n        '../input/sartorius-cell-instance-segmentation',\n        'train', image_id + '.png')\n    image = plt.imread(file_name)\n    mask_files = df_train.loc[df_train['id'] == image_id, 'annotation']\n    mask = make_mask(mask_files)\n\n    ax[0].imshow(\n        image,\n        cmap = plt.get_cmap('winter'), \n        origin = 'upper',\n        vmax = np.quantile(image, 0.99),\n        vmin = np.quantile(image, 0.05)\n    )\n    ax[0].set_title(f'Source [{image_id}]')\n    ax[0].axis('off')\n    \n    ax[1].imshow(\n        image,\n        cmap = plt.get_cmap('winter'), \n        origin = 'upper',\n        vmax = 255,\n        vmin = 0)\n    ax[1].imshow(mask, alpha=1, cmap=plt.get_cmap('seismic'))\n    ax[1].set_title(f'Source [{image_id}] + Mask {cell_type}')\n    ax[1].axis('off')\n    plt.show()\n\nselect_image_ids = []\nselect_image_ids.append(df_train.loc[df_train['cell_type'] == 'astro', 'id'].sample(1).to_list()[0])\nselect_image_ids.append(df_train.loc[df_train['cell_type'] == 'cort', 'id'].sample(1).to_list()[0])\nselect_image_ids.append(df_train.loc[df_train['cell_type'] == 'shsy5y', 'id'].sample(1).to_list()[0])\n\nfor image_id in select_image_ids:\n    plot_image(image_id)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:11.646295Z","iopub.execute_input":"2021-12-29T13:41:11.647035Z","iopub.status.idle":"2021-12-29T13:41:12.895646Z","shell.execute_reply.started":"2021-12-29T13:41:11.64699Z","shell.execute_reply":"2021-12-29T13:41:12.894736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation:\n\nLoading and transforming the input images and their corresponding grayscale masks¶\n","metadata":{}},{"cell_type":"code","source":"DIRECTORY_PATH = \"../input/sartorius-cell-instance-segmentation\"\nTRAIN_CSV = DIRECTORY_PATH + \"/train.csv\"\nTRAIN_PATH = DIRECTORY_PATH + \"/train\"\nTEST_PATH = DIRECTORY_PATH + \"/test\"\nTRAIN_SEMI_SUPERVISED_PATH = DIRECTORY_PATH + \"/train_semi_supervised\"\nSAMPLE_SUBMISSION_PATH = DIRECTORY_PATH + \"/sample_submission.csv\"\n\ntrain_images_path = getImagePaths(TRAIN_PATH)\ntest_images_path = getImagePaths(TEST_PATH)\ntrain_semi_supervised_path = getImagePaths(TRAIN_SEMI_SUPERVISED_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:41:12.89723Z","iopub.execute_input":"2021-12-29T13:41:12.898289Z","iopub.status.idle":"2021-12-29T13:41:13.369229Z","shell.execute_reply.started":"2021-12-29T13:41:12.898242Z","shell.execute_reply":"2021-12-29T13:41:13.367932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image(image_id):\n    image = cv2.imread(f\"../input/sartorius-cell-instance-segmentation/train/{image_id}.png\", cv2.IMREAD_GRAYSCALE)\n    return image.reshape(*INPUT_IMG_SHAPE, 1)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:45:55.253785Z","iopub.execute_input":"2021-12-29T13:45:55.254177Z","iopub.status.idle":"2021-12-29T13:45:55.259691Z","shell.execute_reply.started":"2021-12-29T13:45:55.254139Z","shell.execute_reply":"2021-12-29T13:45:55.258899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/c/sartorius-cell-instance-segmentation/discussion/291627\ndef rle_decode(mask_rle, shape=(520, 704, 1)):\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)  # Needed to align to RLE direction\n\ndef rle_encode(img):\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:45:57.170176Z","iopub.execute_input":"2021-12-29T13:45:57.170489Z","iopub.status.idle":"2021-12-29T13:45:57.18089Z","shell.execute_reply.started":"2021-12-29T13:45:57.170458Z","shell.execute_reply":"2021-12-29T13:45:57.179898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# each mask annotation has one area\nmask = df_train[df_train[\"id\"] == \"0030fd0e6378\"][\"annotation\"].tolist()[0]\nimg = rle_decode(mask)\nplt.imshow(img, cmap=\"gray\");\n","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:45:58.084389Z","iopub.execute_input":"2021-12-29T13:45:58.08488Z","iopub.status.idle":"2021-12-29T13:45:58.33833Z","shell.execute_reply.started":"2021-12-29T13:45:58.084844Z","shell.execute_reply":"2021-12-29T13:45:58.337408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model building:\n","metadata":{}},{"cell_type":"code","source":"sample_submission=pd.read_csv(SAMPLE_SUBMISSION_PATH)\nIMG_HEIGHT = 520\nIMG_WIDTH = 704\nIMG_CHANNELS = 1\nTRAIN_PATH = '../input/sartorius-cell-instance-segmentation/train/'\n\ntrain_ids = df_train['id'].unique().tolist()\ntest_ids = sample_submission['id'].unique().tolist()\n\n# Get and resize train images and masks\nX_train = np.zeros((df_train['id'].nunique(), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nY_train = np.zeros((df_train['id'].nunique(), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:46:01.277187Z","iopub.execute_input":"2021-12-29T13:46:01.277715Z","iopub.status.idle":"2021-12-29T13:46:01.443766Z","shell.execute_reply.started":"2021-12-29T13:46:01.277665Z","shell.execute_reply":"2021-12-29T13:46:01.442721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n    path = TRAIN_PATH + id_\n    img = cv2.imread(path + '.png')[:,:]\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32) -125\n    img = np.expand_dims(img, axis = 2)\n    X_train[n] = img\n    \n    labels = df_train[df_train[\"id\"]\n                        == id_][\"annotation\"].tolist()\n    mask = np.zeros((520, 704, 1))\n    for label in labels:\n        mask += rle_decode(label, shape=(520, 704, 1))\n    mask = mask.clip(0, 1)\n\n    Y_train[n] = mask\nprint(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:46:02.03311Z","iopub.execute_input":"2021-12-29T13:46:02.033448Z","iopub.status.idle":"2021-12-29T13:46:52.336895Z","shell.execute_reply.started":"2021-12-29T13:46:02.033416Z","shell.execute_reply":"2021-12-29T13:46:52.335931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get and resize test images\ntest_images_id = []\nX_test = np.zeros((sample_submission['id'].nunique(), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nfor n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n    path = TRAIN_PATH.replace('train', 'test') + id_\n    img = cv2.imread(path + '.png')[:,:]\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32) -125\n    img = np.expand_dims(img, axis = 2)\n    X_test[n] = img\n    test_images_id.append(id_)\nprint(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:51:22.298391Z","iopub.execute_input":"2021-12-29T13:51:22.299364Z","iopub.status.idle":"2021-12-29T13:51:22.350863Z","shell.execute_reply.started":"2021-12-29T13:51:22.299324Z","shell.execute_reply":"2021-12-29T13:51:22.349819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape,Y_train.shape,X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:51:22.987468Z","iopub.execute_input":"2021-12-29T13:51:22.988188Z","iopub.status.idle":"2021-12-29T13:51:22.994542Z","shell.execute_reply.started":"2021-12-29T13:51:22.988142Z","shell.execute_reply":"2021-12-29T13:51:22.993702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_id_num = 40\nplt.imshow(X_train[sample_id_num][:,:,0], cmap = 'gray')\nplt.show()\nplt.imshow(Y_train[sample_id_num][:,:,0])\nplt.show()\n\nprint('Input image:','Min:', X_train[sample_id_num][:,:,0].min(), '; Max:', X_train[sample_id_num][:,:,0].max(), '; Mean:', X_train[sample_id_num][:,:,0].mean())\nprint('Mask:','Min:', Y_train[sample_id_num][:,:,0].min(), '; Max:', Y_train[sample_id_num][:,:,0].max(), '; Mean:', Y_train[sample_id_num][:,:,0].mean())\n","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:51:24.239679Z","iopub.execute_input":"2021-12-29T13:51:24.240488Z","iopub.status.idle":"2021-12-29T13:51:24.670679Z","shell.execute_reply.started":"2021-12-29T13:51:24.24044Z","shell.execute_reply":"2021-12-29T13:51:24.669879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dice_coefficient\ndef dice_coefficient(y_true, y_pred):\n    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n    denominator = tf.reduce_sum(y_true + y_pred)\n    return numerator / (denominator + tf.keras.backend.epsilon())","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:51:25.619588Z","iopub.execute_input":"2021-12-29T13:51:25.619903Z","iopub.status.idle":"2021-12-29T13:51:25.625813Z","shell.execute_reply.started":"2021-12-29T13:51:25.619872Z","shell.execute_reply":"2021-12-29T13:51:25.624688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model\nmodel = keras.Sequential([\n    # Convolutional layer 1\n    keras.layers.Conv2D(filters=20, kernel_size=5, strides=1,\n                  padding='same',input_shape=[IMG_WIDTH,IMG_HEIGHT,IMG_CHANNELS],\n                  activation='relu'),\n    keras.layers.BatchNormalization(),\n    \n    # Convolutional layer 2\n    keras.layers.Conv2D(filters=10, kernel_size=1),\n\n    # Convolutional layer 3\n    keras.layers.Conv2D(filters=10, kernel_size=5, strides=1,\n                  padding='same', activation='relu'),\n    keras.layers.BatchNormalization(),\n\n    # Convolutional layer 4\n    keras.layers.Conv2D(filters=1, kernel_size=1),\n])","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:51:26.90049Z","iopub.execute_input":"2021-12-29T13:51:26.901248Z","iopub.status.idle":"2021-12-29T13:51:27.093774Z","shell.execute_reply.started":"2021-12-29T13:51:26.901202Z","shell.execute_reply":"2021-12-29T13:51:27.092867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = BinaryCrossentropy(from_logits=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:51:28.232074Z","iopub.execute_input":"2021-12-29T13:51:28.232374Z","iopub.status.idle":"2021-12-29T13:51:28.236868Z","shell.execute_reply.started":"2021-12-29T13:51:28.232343Z","shell.execute_reply":"2021-12-29T13:51:28.235886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss=loss)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:51:30.127796Z","iopub.execute_input":"2021-12-29T13:51:30.128612Z","iopub.status.idle":"2021-12-29T13:51:30.149438Z","shell.execute_reply.started":"2021-12-29T13:51:30.128569Z","shell.execute_reply":"2021-12-29T13:51:30.148653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit model\nn_epochs = 10\nbatch_size = 32\nfrom keras.callbacks import EarlyStopping\nearlystopper = EarlyStopping(patience=20, verbose=1)\n\nresults = model.fit(X_train, Y_train, validation_split=0.15, batch_size=batch_size, epochs=n_epochs, \n                    callbacks=[earlystopper])\nprint(\"Done!\")","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:07:50.053086Z","iopub.execute_input":"2021-12-29T14:07:50.053412Z","iopub.status.idle":"2021-12-29T14:30:39.500288Z","shell.execute_reply.started":"2021-12-29T14:07:50.053379Z","shell.execute_reply":"2021-12-29T14:30:39.49961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,4))\nplt.plot(results.history['loss'])\nplt.plot(results.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.legend(['loss', 'val_loss'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:30:45.719949Z","iopub.execute_input":"2021-12-29T14:30:45.7206Z","iopub.status.idle":"2021-12-29T14:30:45.951448Z","shell.execute_reply.started":"2021-12-29T14:30:45.72056Z","shell.execute_reply":"2021-12-29T14:30:45.950457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(Y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:31:19.770599Z","iopub.execute_input":"2021-12-29T14:31:19.770949Z","iopub.status.idle":"2021-12-29T14:31:19.777068Z","shell.execute_reply.started":"2021-12-29T14:31:19.770915Z","shell.execute_reply":"2021-12-29T14:31:19.775658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_train = model.predict(X_train, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:31:20.646001Z","iopub.execute_input":"2021-12-29T14:31:20.646847Z","iopub.status.idle":"2021-12-29T14:32:01.764401Z","shell.execute_reply.started":"2021-12-29T14:31:20.646794Z","shell.execute_reply":"2021-12-29T14:32:01.76339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:32:01.776191Z","iopub.execute_input":"2021-12-29T14:32:01.776904Z","iopub.status.idle":"2021-12-29T14:32:01.788972Z","shell.execute_reply.started":"2021-12-29T14:32:01.776869Z","shell.execute_reply":"2021-12-29T14:32:01.788097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Threshold predictions\npreds_train_t = (preds_train > 0.5).astype(np.uint8)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:32:59.567891Z","iopub.execute_input":"2021-12-29T14:32:59.568248Z","iopub.status.idle":"2021-12-29T14:32:59.964393Z","shell.execute_reply.started":"2021-12-29T14:32:59.568211Z","shell.execute_reply":"2021-12-29T14:32:59.96345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(preds_train_t[0], cmap=\"gray\");\n","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:33:00.22166Z","iopub.execute_input":"2021-12-29T14:33:00.221998Z","iopub.status.idle":"2021-12-29T14:33:00.531572Z","shell.execute_reply.started":"2021-12-29T14:33:00.221961Z","shell.execute_reply":"2021-12-29T14:33:00.530319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unoptimized and slow; any way to speed up?\n\ndef get_threshold(Y, pred):\n    scores = list(pred.ravel())\n    mask = list(Y.ravel())\n    \n    idxs=np.argsort(scores)[::-1]\n    mask_sorted=np.array(mask)[idxs]\n    sum_mask_one=np.cumsum(mask_sorted)\n    IoU=sum_mask_one/(np.arange(1,len(mask_sorted)+1)+np.sum(mask_sorted)-sum_mask_one)\n    best_IoU_idx=IoU.argmax()\n    best_threshold=scores[idxs[best_IoU_idx]]\n    best_IoU=IoU[best_IoU_idx]\n\n    return best_threshold, best_IoU","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:33:00.889314Z","iopub.execute_input":"2021-12-29T14:33:00.88964Z","iopub.status.idle":"2021-12-29T14:33:00.900473Z","shell.execute_reply.started":"2021-12-29T14:33:00.889587Z","shell.execute_reply":"2021-12-29T14:33:00.899451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(preds_train.shape)\nprint(Y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:33:01.701333Z","iopub.execute_input":"2021-12-29T14:33:01.702091Z","iopub.status.idle":"2021-12-29T14:33:01.709889Z","shell.execute_reply.started":"2021-12-29T14:33:01.701886Z","shell.execute_reply":"2021-12-29T14:33:01.709003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_threshold(Y_train[0], preds_train[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:33:02.689313Z","iopub.execute_input":"2021-12-29T14:33:02.689658Z","iopub.status.idle":"2021-12-29T14:33:02.96068Z","shell.execute_reply.started":"2021-12-29T14:33:02.689607Z","shell.execute_reply":"2021-12-29T14:33:02.959736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_thresholds = []         # one for each image\nimg_IoUs = []\nfor Y, P in tqdm(zip(Y_train, preds_train), total=Y_train.shape[0]):\n\n    best_img_threshold, best_img_IoU = get_threshold(Y, P)\n    img_thresholds.append(best_img_threshold)\n    img_IoUs.append(best_img_IoU)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:33:03.469502Z","iopub.execute_input":"2021-12-29T14:33:03.47038Z","iopub.status.idle":"2021-12-29T14:35:41.916541Z","shell.execute_reply.started":"2021-12-29T14:33:03.470332Z","shell.execute_reply":"2021-12-29T14:35:41.915694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_threshold = np.mean(img_thresholds)\nbest_threshold_spread = np.std(img_thresholds)\navg_IoU = mean(img_IoUs)\n\nprint(f\"Best threshold: {best_threshold:.3g} (+-{best_threshold_spread:.3g}), Avg. Train IoU: {avg_IoU:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:37:26.426873Z","iopub.execute_input":"2021-12-29T14:37:26.42717Z","iopub.status.idle":"2021-12-29T14:37:26.435974Z","shell.execute_reply.started":"2021-12-29T14:37:26.427139Z","shell.execute_reply":"2021-12-29T14:37:26.435095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dice_coefficient(Y_train, preds_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:37:27.62576Z","iopub.execute_input":"2021-12-29T14:37:27.626163Z","iopub.status.idle":"2021-12-29T14:37:29.448926Z","shell.execute_reply.started":"2021-12-29T14:37:27.626129Z","shell.execute_reply":"2021-12-29T14:37:29.447758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_Y = (preds_train >= best_threshold)\n    \ndef plot(img_Y, img_pred):\n    output = np.zeros_like(img_Y)\n    output = np.where((img_Y == 0) & (img_pred == 1), 1, output)\n    output = np.where((img_Y == 1) & (img_pred == 0), 2, output)\n    output = np.where((img_Y == 1) & (img_pred == 1), 3, output)\n\n    plt.figure(figsize=(10,10))\n    plt.imshow(output, cmap=ListedColormap(['black', 'gray', 'orange', 'green']))\n    plt.xticks([])\n    plt.yticks([]);\n","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:37:29.450877Z","iopub.execute_input":"2021-12-29T14:37:29.45148Z","iopub.status.idle":"2021-12-29T14:37:29.619234Z","shell.execute_reply.started":"2021-12-29T14:37:29.451434Z","shell.execute_reply":"2021-12-29T14:37:29.618072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = 5\nfor i in range(N):\n    img_Y = Y_train[i]\n    img_pred = pred_Y[i]\n    \n    plot(img_Y, img_pred)\n    plt.show()\n\n# green: correct prediction\n# gray: false positive (too much)\n# orange: false negative (missed)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:38:25.159594Z","iopub.execute_input":"2021-12-29T14:38:25.160181Z","iopub.status.idle":"2021-12-29T14:38:25.851472Z","shell.execute_reply.started":"2021-12-29T14:38:25.160126Z","shell.execute_reply":"2021-12-29T14:38:25.850525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_test = model.predict(X_test, verbose=1)\npreds_test_t = (preds_test >= best_threshold).astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:38:26.554761Z","iopub.execute_input":"2021-12-29T14:38:26.555591Z","iopub.status.idle":"2021-12-29T14:38:26.804277Z","shell.execute_reply.started":"2021-12-29T14:38:26.55554Z","shell.execute_reply":"2021-12-29T14:38:26.803353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_test_t[1].shape","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:38:28.106243Z","iopub.execute_input":"2021-12-29T14:38:28.106569Z","iopub.status.idle":"2021-12-29T14:38:28.113167Z","shell.execute_reply.started":"2021-12-29T14:38:28.106532Z","shell.execute_reply":"2021-12-29T14:38:28.112575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test samples\nfrom random import randint\nix = randint(0, len(preds_test_t)-1)\nprint(ix)\nplt.imshow(X_test[ix])\nplt.show()\nplt.imshow(np.squeeze(preds_test_t[ix]))\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:38:29.113598Z","iopub.execute_input":"2021-12-29T14:38:29.114212Z","iopub.status.idle":"2021-12-29T14:38:29.621385Z","shell.execute_reply.started":"2021-12-29T14:38:29.114176Z","shell.execute_reply":"2021-12-29T14:38:29.620434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(preds_test_t[0].shape)\nprint(preds_test_t[1].shape)\nprint(preds_test_t[2].shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:38:31.302677Z","iopub.execute_input":"2021-12-29T14:38:31.30302Z","iopub.status.idle":"2021-12-29T14:38:31.309963Z","shell.execute_reply.started":"2021-12-29T14:38:31.302982Z","shell.execute_reply":"2021-12-29T14:38:31.308935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_overlap(msk):\n    msk = msk.astype(np.bool).astype(np.uint8)\n    return np.any(np.sum(msk, axis=-1)>1)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:38:32.17445Z","iopub.execute_input":"2021-12-29T14:38:32.175143Z","iopub.status.idle":"2021-12-29T14:38:32.180829Z","shell.execute_reply.started":"2021-12-29T14:38:32.175103Z","shell.execute_reply":"2021-12-29T14:38:32.179956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for test_mask in preds_test_t:\n    print(check_overlap(test_mask))","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:38:33.726023Z","iopub.execute_input":"2021-12-29T14:38:33.72655Z","iopub.status.idle":"2021-12-29T14:38:33.735457Z","shell.execute_reply.started":"2021-12-29T14:38:33.726516Z","shell.execute_reply":"2021-12-29T14:38:33.734675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the mask into each cluster nucleus for the submision\n# seen on https://www.kaggle.com/c/sartorius-cell-instance-segmentation/discussion/288376\ndef post_process(mask, min_size=80):\n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = []\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            a_prediction = np.zeros((520, 704), np.float32)\n            a_prediction[p] = 1\n            predictions.append(a_prediction)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:38:34.705559Z","iopub.execute_input":"2021-12-29T14:38:34.705907Z","iopub.status.idle":"2021-12-29T14:38:34.714111Z","shell.execute_reply.started":"2021-12-29T14:38:34.705865Z","shell.execute_reply":"2021-12-29T14:38:34.713188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the nucleus thing\nplt.imshow(Y_train[4], cmap=\"gray\");","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:38:35.762333Z","iopub.execute_input":"2021-12-29T14:38:35.762982Z","iopub.status.idle":"2021-12-29T14:38:36.051734Z","shell.execute_reply.started":"2021-12-29T14:38:35.762934Z","shell.execute_reply":"2021-12-29T14:38:36.05087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_component, component = cv2.connectedComponents(Y_train[4].astype(np.uint8))\nnum_component","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:38:36.773728Z","iopub.execute_input":"2021-12-29T14:38:36.774019Z","iopub.status.idle":"2021-12-29T14:38:36.787028Z","shell.execute_reply.started":"2021-12-29T14:38:36.77399Z","shell.execute_reply":"2021-12-29T14:38:36.785798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(component, cmap=\"gray\");","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:38:53.671005Z","iopub.execute_input":"2021-12-29T14:38:53.671954Z","iopub.status.idle":"2021-12-29T14:38:53.970234Z","shell.execute_reply.started":"2021-12-29T14:38:53.671876Z","shell.execute_reply":"2021-12-29T14:38:53.968969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compenent_1 = (component == 1)\nplt.imshow(compenent_1, cmap=\"gray\");","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:39:05.529546Z","iopub.execute_input":"2021-12-29T14:39:05.530342Z","iopub.status.idle":"2021-12-29T14:39:05.81183Z","shell.execute_reply.started":"2021-12-29T14:39:05.530302Z","shell.execute_reply":"2021-12-29T14:39:05.81114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final = post_process(Y_train[4])\nfinal[0].shape","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:39:22.059339Z","iopub.execute_input":"2021-12-29T14:39:22.05992Z","iopub.status.idle":"2021-12-29T14:39:22.227341Z","shell.execute_reply.started":"2021-12-29T14:39:22.059867Z","shell.execute_reply":"2021-12-29T14:39:22.226398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(final[0], cmap=\"gray\");","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:39:31.40244Z","iopub.execute_input":"2021-12-29T14:39:31.402794Z","iopub.status.idle":"2021-12-29T14:39:31.688094Z","shell.execute_reply.started":"2021-12-29T14:39:31.402762Z","shell.execute_reply":"2021-12-29T14:39:31.687108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# old submision\npredicted2 = [rle_encode(test_mask2) for test_mask2 in preds_test_t]\nlen(predicted2[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:42:15.623191Z","iopub.execute_input":"2021-12-29T14:42:15.624097Z","iopub.status.idle":"2021-12-29T14:42:15.665385Z","shell.execute_reply.started":"2021-12-29T14:42:15.624048Z","shell.execute_reply":"2021-12-29T14:42:15.664513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_isolated_points_from_rle(strin):\n    t2 = strin.split(\" \")\n    a = []\n    for i in range(0, len(t2), 2):\n        if t2[i+1]!=\"1\":\n            a.append(t2[i])\n            a.append(t2[i+1])\n    return ' '.join(a)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:42:27.414331Z","iopub.execute_input":"2021-12-29T14:42:27.414917Z","iopub.status.idle":"2021-12-29T14:42:27.421183Z","shell.execute_reply.started":"2021-12-29T14:42:27.414873Z","shell.execute_reply":"2021-12-29T14:42:27.420272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_filt = [remove_isolated_points_from_rle(s) for s in predicted2]","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:42:37.838826Z","iopub.execute_input":"2021-12-29T14:42:37.839227Z","iopub.status.idle":"2021-12-29T14:42:37.849018Z","shell.execute_reply.started":"2021-12-29T14:42:37.839183Z","shell.execute_reply":"2021-12-29T14:42:37.848054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new version with the mask nucleus split\npredicted_nucleus = []\ntest_nucleus_image_id = []\n\nfor index, s in enumerate(preds_test_t):\n    nucleus = post_process(s)\n    for nucl in nucleus:\n        predicted_nucleus.append(nucl)\n        test_nucleus_image_id.append(test_images_id[index])","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:42:47.222711Z","iopub.execute_input":"2021-12-29T14:42:47.223781Z","iopub.status.idle":"2021-12-29T14:42:48.282472Z","shell.execute_reply.started":"2021-12-29T14:42:47.223733Z","shell.execute_reply":"2021-12-29T14:42:48.281227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(predicted_nucleus[0], cmap=\"gray\");","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:43:00.055376Z","iopub.execute_input":"2021-12-29T14:43:00.055724Z","iopub.status.idle":"2021-12-29T14:43:00.346001Z","shell.execute_reply.started":"2021-12-29T14:43:00.055686Z","shell.execute_reply":"2021-12-29T14:43:00.34504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted2 = [rle_encode(test_mask2) for test_mask2 in predicted_nucleus]\nprint(predicted2[0])\npredicted_filt = [remove_isolated_points_from_rle(s) for s in predicted2]\nprint(predicted_filt[0])\n","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:43:09.414015Z","iopub.execute_input":"2021-12-29T14:43:09.414349Z","iopub.status.idle":"2021-12-29T14:43:09.653363Z","shell.execute_reply.started":"2021-12-29T14:43:09.414313Z","shell.execute_reply":"2021-12-29T14:43:09.6525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = sample_submission.copy()\n#submit['predicted'] = predicted2\nsubmit = pd.DataFrame({'id':test_nucleus_image_id, 'predicted':predicted_filt})","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:43:18.430345Z","iopub.execute_input":"2021-12-29T14:43:18.430663Z","iopub.status.idle":"2021-12-29T14:43:18.437386Z","shell.execute_reply.started":"2021-12-29T14:43:18.430616Z","shell.execute_reply":"2021-12-29T14:43:18.436713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(submit.shape)\nsubmit.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:43:40.202Z","iopub.execute_input":"2021-12-29T14:43:40.202832Z","iopub.status.idle":"2021-12-29T14:43:40.21509Z","shell.execute_reply.started":"2021-12-29T14:43:40.202787Z","shell.execute_reply":"2021-12-29T14:43:40.214452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:43:56.922092Z","iopub.execute_input":"2021-12-29T14:43:56.922974Z","iopub.status.idle":"2021-12-29T14:43:56.937778Z","shell.execute_reply.started":"2021-12-29T14:43:56.922927Z","shell.execute_reply":"2021-12-29T14:43:56.936992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References:\n1. https://www.kaggle.com/ishandutta/sartorius-indepth-eda-explanation-model\n2. https://www.kaggle.com/tolgadincer/sartorius-eda-general-overview-and-outliers\n3. https://www.kaggle.com/carlosgut/sartorius-simple-cnn-keras\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}