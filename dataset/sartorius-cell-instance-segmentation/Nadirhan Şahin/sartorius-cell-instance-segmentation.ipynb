{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow==1.13.1\n!pip install keras==2.2.5\n!pip install mrcnn\n!pip install scikit-image\n!pip install imgaug\n!pip install IPython\n!pip install 'h5py==2.10.0' --force-reinstall","metadata":{"execution":{"iopub.status.busy":"2022-02-13T23:17:08.975316Z","iopub.execute_input":"2022-02-13T23:17:08.975643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage.io import imread\nimport matplotlib.pyplot as plt\nimport mrcnn.utils as utils\nimport mrcnn.model as modellib\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport os\nimport sys\nimport json\nimport time\nimport skimage\nimport imageio\nimport glob\nimport imgaug\nimport multiprocessing\n\nfrom PIL import Image, ImageDraw\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import KFold\nfrom PIL import Image, ImageEnhance\nfrom mrcnn.config import Config\nfrom mrcnn import visualize","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Dataset","metadata":{}},{"cell_type":"code","source":"HEIGHT = 520\nWIDTH = 704\nSHAPE = (HEIGHT, WIDTH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindf = pd.read_csv('../input/sartorius-cell-instance-segmentation'+'/train.csv')\n\nid_unique = traindf['id'].unique()\n\ndef get_file_path(image_id):\n    return f'/kaggle/input/sartorius-cell-instance-segmentation/train/{image_id}.png'\ntraindf['file_path'] = traindf['id'].apply(get_file_path)\n\nCELL_NAMES = np.sort(traindf['cell_type'].unique())\nCELL_NAMES_DICT = dict([(v, k) for k, v in enumerate(CELL_NAMES)])\n\ntraindf['cell_type_label'] = traindf['cell_type'].apply(CELL_NAMES_DICT.get) + 1\nID2CELL_LABEL = dict(\n    [(k, v) for k, v in traindf[['id', 'cell_type_label']].itertuples(name=None, index=False)]\n)\n\ntraindf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_decode(annotations, shape=(520, 704)):\n    '''\n    returns binary masked map \n    '''\n    s = annotations.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id = \"0030fd0e6378\"\nimg = imread(\"../input/sartorius-cell-instance-segmentation/train/0030fd0e6378.png\")\nplt.imshow(img, cmap='gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imageMaks=traindf[traindf['id']==id]\nallMask = np.zeros(img.shape)\nfor i in range(len(imageMaks['annotation'])):\n    allMask = allMask + rle_decode(imageMaks['annotation'].iloc[i])\nplt.imshow(allMask, cmap=\"Greens\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_decode_by_image_id(image_id):\n    rows = traindf.loc[traindf['id'] == image_id]\n    \n    # Image Shape\n    mask = np.full(shape=[len(rows), np.prod(SHAPE)], fill_value=0, dtype=np.uint8)\n    \n    for idx, (_, row) in enumerate(rows.iterrows()):\n        s = row['annotation'].split()\n        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n        starts -= 1\n        ends = starts + lengths\n        for lo, hi in zip(starts, ends):\n            mask[idx, lo:hi] = True\n    \n    mask = mask.reshape([len(rows), *SHAPE])\n    mask = np.moveaxis(mask, 0, 2)\n    \n    return mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf ./train ./test\n!mkdir ./train ./test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CellDataset(utils.Dataset):\n\n    def load_data(self, image_ids, form, image_group):\n   \n        for i, name in enumerate(CELL_NAMES):\n            self.add_class('cell', 1 + i, name)\n       \n        # Add the image using the base method from utils.Dataset\n        for vertical_flip in [True, False]:\n            for horizontal_flip in [True, False]:\n                for image in tqdm(image_ids):\n                    self.add_image('cell', \n                           image_id=image, \n                           path=(f'../input/sartorius-cell-instance-segmentation/train/{image}.png'),\n                           label = ID2CELL_LABEL[image],\n                           height=512, width=512,\n                          vertical_flip=vertical_flip, horizontal_flip=horizontal_flip,\n                      )\n            \n            \n    def load_mask(self, image_id):\n        \"\"\" Load instance masks for the given image.\n        MaskRCNN expects masks in the form of a bitmap [height, width, instances].\n        Args:\n            image_id: The id of the image to load masks for\n        Returns:\n            masks: A bool array of shape [height, width, instance count] with\n                one mask per instance.\n            class_ids: a 1D array of class IDs of the instance masks.\n        \"\"\"\n    \n        info = self.image_info[image_id]\n        image_id = info['id']\n    \n        # Get masks by image_id\n        masks = rle_decode_by_image_id(image_id)\n        #masks = pad_image(masks, 0)\n\n        # Get label\n        _, _, size = masks.shape\n        label = info['label']\n        class_ids = np.full(size, label, dtype=np.int32)\n        \n        return masks, class_ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.shuffle(id_unique)\nid_unique_test, id_unique_val = id_unique[:600], id_unique[600:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = CellDataset()\ndataset_train.load_data(id_unique_test, 'png', 'train')\ndataset_train.prepare()\ndataset_val = CellDataset()\ndataset_val.load_data(id_unique_val, 'png', 'validation')\ndataset_val.prepare()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 1\nN_SAMPLES = traindf['id'].nunique()\nclass CellConfig(Config):\n    \"\"\"Configuration for training on the cigarette butts dataset.\n    Derives from the base Config class and overrides values specific\n    to the cigarette butts dataset.\n    \"\"\"\n    \n    NAME = \"cell\"\n\n    # Set batch size to 1.\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = BATCH_SIZE\n    STEPS_PER_EPOCH = int(N_SAMPLES / BATCH_SIZE)\n    \n    # Number of Classes\n    NUM_CLASSES = 1 + len(CELL_NAMES)\n\n    # Input image resizing\n    # Random crops of size 512x512\n    IMAGE_RESIZE_MODE = \"crop\"\n    IMAGE_MIN_DIM = 512\n    IMAGE_MAX_DIM = 512\n    IMAGE_MIN_SCALE = 2.0\n\n    \n    BACKBONE = \"resnet50\"\n\n    # Training Structure\n    FPN_CLASSIF_FC_LAYERS_SIZE = 1024\n    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n    # Regions of Interest\n    PRE_NMS_LIMIT = 6000\n    # Non Max Supression\n    POST_NMS_ROIS_TRAINING = 2000\n    POST_NMS_ROIS_INFERENCE = 2000\n    # Instances\n    MAX_GT_INSTANCES = 790\n    TRAIN_ROIS_PER_IMAGE = 200\n    DETECTION_MAX_INSTANCES = 200\n    \n    # Thresholds\n    RPN_NMS_THRESHOLD = 0.70        # IoU Threshold for RPN proposals and GT\n    DETECTION_MIN_CONFIDENCE = 0.50 # Non-Background Confidence Threshold\n    DETECTION_NMS_THRESHOLD = 0.30  # IoU Threshold for ROI and GT\n    ROI_POSITIVE_RATIO = 0.33\n    \n    # Prediction Mask Shape\n    USE_MINI_MASK = True\n    MINI_MASK_SHAPE = (56, 56)\n    \n    # DO NOT train Batch Normalization because of small batch size\n    # There are too few samples to correctly train the normalization\n    TRAIN_BN = False\n    \n    # Learning Rate\n    LEARNING_RATE = 0.01\n    WEIGHT_DECAY = 0.0\n    N_WARMUP_STEPS = 2\n    LR_SCHEDULE = True\n    \n    # Dataloader Queue Size (was set to 100 but resulted in OOM error)\n    MAX_QUEUE_SIZE = 10\n    \n    # Cache Items\n    CACHE = True\n    \n    # Debug mode will disable model checkpoints\n    DEBUG = False\n    \n    # Do not use multithreading as this slows down the dataloader!\n    WORKERS = 0\n    \n    # Losses\n    LOSS_WEIGHTS = {\n        'rpn_class_loss': 1.0,    # is the class of the bbox correct? / RPN anchor classifier loss (Forground/Background)\n        'rpn_bbox_loss': 1.0,     # is the size of the bbox correct? / RPN bounding box loss graph (bbox of generic object)\n        'mrcnn_class_loss': 1.0,  # loss for the classifier head of Mask R-CNN (Background / specific class)\n        'mrcnn_bbox_loss': 1.0,   # is the size of the bounding box correct or not? / loss for Mask R-CNN bounding box refinement\n        'mrcnn_mask_loss': 1.0,   # is the class correct? is the pixel correctly assign to the class? / mask binary cross-entropy loss for the masks head\n    }\n    \nconfig = CellConfig()\nconfig.display()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir='model_checkpoints')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path to COCO-dataset weights\nCOCO_MODEL_PATH = './mask_rcnn_coco.h5'\n\n# Download COCO trained weights from Releases if needed\nif not os.path.exists(COCO_MODEL_PATH):\n    utils.download_trained_weights(COCO_MODEL_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exclude = [\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"]\n    \n# Excluce FC layer if it is not the original size\nif config.FPN_CLASSIF_FC_LAYERS_SIZE != 1024:\n    print(f'Excluding FC layer')\n    exclude += [\n        \"mrcnn_class_conv1\", \"mrcnn_class_bn1\", \"mrcnn_class_conv2\", \"mrcnn_class_bn2\",\n    ]\n    \n# using coco weights\nmodel.load_weights(\n    COCO_MODEL_PATH,\n    by_name=True,\n    exclude=exclude,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.train(\n    dataset_train, dataset_val, \n    learning_rate=config.LEARNING_RATE,\n    epochs=3, \n    layers=\"all\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.keras_model.save(\"./maskrcnn.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}