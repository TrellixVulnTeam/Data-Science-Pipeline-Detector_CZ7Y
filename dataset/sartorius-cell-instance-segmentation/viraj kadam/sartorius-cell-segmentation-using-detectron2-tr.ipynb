{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# References and Resources\n* Pytorch RCNN : [ https://www.kaggle.com/julian3833/sartorius-starter-torch-mask-r-cnn-lb-0-273 ]\n* TF Efficientdet : [ https://www.kaggle.com/dschettler8845/train-sartorius-segmentation-eda-effdet-tf ]\n* PyTorch Detectron : P1 [ https://www.kaggle.com/slawekbiel/positive-score-with-detectron-1-3-input-data/ ], P2 [ https://www.kaggle.com/slawekbiel/positive-score-with-detectron-2-3-training/notebook ]\n* Mask RCNN Paper : [  https://arxiv.org/pdf/1703.06870v3.pdf ]\n* Run Length Encoding : [ https://en.wikipedia.org/wiki/Run-length_encoding#Example ]\n* Open CV [ https://learnopencv.com/getting-started-with-opencv/ ]\n* Pytorch UNET : [ https://www.kaggle.com/ebinan92/unet-with-deep-watershed-transform-dwt-train ]\n* Keras UNET   :  [ https://www.kaggle.com/ammarnassanalhajali/sartorius-segmentation-keras-u-net-training ]\n* Pytorch Detectron [  ]https://www.kaggle.com/ammarnassanalhajali/sartorius-segmentation-detectron2-training ]\n* Guide to using detectron2 : [ https://www.analyticsvidhya.com/blog/2021/08/your-guide-to-object-detection-with-detectron2-in-pytorch/ ]\n* captum ai segmentation tutorial : [ https://captum.ai/tutorials/Segmentation_Interpret ]\n* [ https://www.tensorflow.org/tutorials/images/segmentation ] \n* TF UNET [ www.kaggle.com/arunamenon/cell-instance-segmentation-unet-eda ]\n* [ https://medium.com/@ngocson2vn/a-gentle-explanation-of-backpropagation-in-convolutional-neural-network-cnn-1a70abff508b ]\n* PDF - NEURONAL CELL TYPES : [https://www.cell.com/current-biology/pdf/S0960-9822(04)00440-3.pdf] \n* Youtube :UNET FOR SEGMENTATION - Digital Sreeni: [https://www.youtube.com/watch?v=lOZDTDOlqfk]\n* Youtube : CELL IMAGE SEGMENTATION - MIT           : [https://www.youtube.com/watch?v=lOZDTDOlqfk]\n* [https://spark-in.me/post/playing-with-dwt-and-ds-bowl-2018] ","metadata":{}},{"cell_type":"markdown","source":"# **Installing detectron**\n\n* Detectron Starter [ https://www.analyticsvidhya.com/blog/2021/08/your-guide-to-object-detection-with-detectron2-in-pytorch/ ]\n\n","metadata":{}},{"cell_type":"code","source":"!pip install 'git+https://github.com/facebookresearch/detectron2.git' -q\n!pip install pycocotools -q","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:27:09.253107Z","iopub.execute_input":"2021-12-18T10:27:09.253727Z","iopub.status.idle":"2021-12-18T10:30:13.157225Z","shell.execute_reply.started":"2021-12-18T10:27:09.253619Z","shell.execute_reply":"2021-12-18T10:30:13.156376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# requirements file\n!pip freeze > requirements.txt","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:30:13.160698Z","iopub.execute_input":"2021-12-18T10:30:13.160934Z","iopub.status.idle":"2021-12-18T10:30:16.19775Z","shell.execute_reply.started":"2021-12-18T10:30:13.160897Z","shell.execute_reply":"2021-12-18T10:30:16.196859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# pytorch\nimport torch, torchvision\nprint(torch.__version__)\nif  torch.cuda.is_available():\n    print('GPU \\n')\n    !nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:30:16.200861Z","iopub.execute_input":"2021-12-18T10:30:16.201104Z","iopub.status.idle":"2021-12-18T10:30:17.691862Z","shell.execute_reply.started":"2021-12-18T10:30:16.201077Z","shell.execute_reply":"2021-12-18T10:30:17.690962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# general libraries\nimport pandas as pd\nimport numpy as np\nimport pandas as pd \nimport os, json, cv2, random\nfrom tqdm import tqdm_notebook as tqdm \nimport matplotlib.pyplot as plt\n# import skimage.io as io\n# from pathlib import Path\nimport seaborn as sns\n\n\n# for data preprocessing and augmentation\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n\n#coco \nfrom pycocotools.coco import COCO\nfrom pycocotools import mask as maskUtils\nfrom joblib import Parallel, delayed\nfrom fastcore.all import *\n# detectron2\nimport pycocotools.mask as mask_util\nfrom detectron2.structures import BoxMode\nfrom detectron2 import model_zoo                 # pretrained models are available in model zoo\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultPredictor, DefaultTrainer, launch\nfrom detectron2.structures import BoxMode\nfrom detectron2.utils.visualizer import ColorMode\nfrom detectron2.utils.logger import setup_logger\nfrom detectron2.utils.visualizer import Visualizer\n\n\n# data loading and utils\nfrom detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.data import detection_utils as utils\nimport detectron2.data.transforms as T\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.evaluation.evaluator import DatasetEvaluator\n\n\n# filter warinings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Initialize the detectron2 logger and set its verbosity level to \"DEBUG\".\nsetup_logger()","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:33:34.553996Z","iopub.execute_input":"2021-12-18T10:33:34.554284Z","iopub.status.idle":"2021-12-18T10:33:34.597496Z","shell.execute_reply.started":"2021-12-18T10:33:34.554254Z","shell.execute_reply":"2021-12-18T10:33:34.596823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sartorius Cell Instance Segmentation : Data Summary .**\n\n* We have 606 Images in the Train set, and 3 Images in the public test set. Images are in PNG format.\n* The annotations and paths for training images are given in the train.csv.\n* The submission format is as given in sample_submission.csv.\n* There are about 1900+ Images present without annotations, which can be used for semi-supervised learning at path '../input/sartorius-cell-instance-segmentation/train_semi_supervised'.\n* There are also Images from LIVECell dataset with annotations, which could be included in training at path '../input/sartorius-cell-instance-segmentation/LIVECell_dataset_2021'.\n","metadata":{}},{"cell_type":"code","source":"#directory paths \n\nlivecell_ds = '../input/sartorius-cell-instance-segmentation/LIVECell_dataset_2021'\ntrain_dir = '../input/sartorius-cell-instance-segmentation/train'\ntest_dir  =  '../input/sartorius-cell-instance-segmentation/test'\n\n\n#csv files \nsample_sub = pd.read_csv('../input/sartorius-cell-instance-segmentation/sample_submission.csv')\ntrain  = pd.read_csv('../input/sartorius-cell-instance-segmentation/train.csv')\n\n\n#checking the csv \ntrain.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:30:20.037799Z","iopub.execute_input":"2021-12-18T10:30:20.038244Z","iopub.status.idle":"2021-12-18T10:30:20.606418Z","shell.execute_reply.started":"2021-12-18T10:30:20.038192Z","shell.execute_reply":"2021-12-18T10:30:20.605726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple EDA","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\n\nsns.countplot(data=train,x='cell_type')\nplt.title('Instance of each Class in data')\nplt.xlabel('Cell Type')\nplt.ylabel('Cell Instance Count')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:30:20.607856Z","iopub.execute_input":"2021-12-18T10:30:20.608317Z","iopub.status.idle":"2021-12-18T10:30:20.883276Z","shell.execute_reply.started":"2021-12-18T10:30:20.608281Z","shell.execute_reply":"2021-12-18T10:30:20.882574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_an=train.groupby('id').size().sort_values(ascending=False).reset_index()\nprint('Maximum annotation in a image are with ID-{}: NUM(Annot) {}'.format(max_an.iloc[0]['id'],max_an[0][0]))","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:30:20.884384Z","iopub.execute_input":"2021-12-18T10:30:20.885097Z","iopub.status.idle":"2021-12-18T10:30:20.900466Z","shell.execute_reply.started":"2021-12-18T10:30:20.88506Z","shell.execute_reply":"2021-12-18T10:30:20.899581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Average Annotations per Image',len(train)/train['id'].nunique())\n\nmax_an=train.groupby('id').size().sort_values(ascending=False).reset_index()\nprint('Maximum annotation in a image are with ID-{}: NUM(Annot) {}'.format(max_an.iloc[0]['id'],max_an.iloc[0][0]))\n\nprint('Min annotation in a image are with ID-{}: NUM(Annot) {}'.format(max_an.iloc[-1]['id'],max_an.iloc[-1][0]))","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:30:20.90177Z","iopub.execute_input":"2021-12-18T10:30:20.902089Z","iopub.status.idle":"2021-12-18T10:30:20.924058Z","shell.execute_reply.started":"2021-12-18T10:30:20.902053Z","shell.execute_reply":"2021-12-18T10:30:20.923254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_classes = train['cell_type'].unique()\n\nfor clas in unique_classes:\n    clas_df= train[train['cell_type']==clas]\n    \n    print('Average Annotations for class {} are {}'.format(clas,len(clas_df)/clas_df['id'].nunique()))\n    \n    max_an=clas_df.groupby('id').size().sort_values(ascending=False).reset_index()\n    print('Max num of annot for {} ID-{}: NUM(Annot) {}'.format(clas,max_an.iloc[0]['id'],max_an.iloc[0][0]))\n    print('Min num of annotation for {} with ID-{}: NUM(Annot) {}'.format(clas,max_an.iloc[-1]['id'],max_an.iloc[-1][0]))\n    print('-'*20,'.'*10,'-'*20)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:30:20.925267Z","iopub.execute_input":"2021-12-18T10:30:20.925567Z","iopub.status.idle":"2021-12-18T10:30:20.995262Z","shell.execute_reply.started":"2021-12-18T10:30:20.925532Z","shell.execute_reply":"2021-12-18T10:30:20.994585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the data in coco format.","metadata":{}},{"cell_type":"markdown","source":"**Getting the data ready in COCO format (Just for demonstration purpose,using a pre-prepared dataset from [ https://www.kaggle.com/slawekbiel/positive-score-with-detectron-2-3-training/notebook ].)**","metadata":{}},{"cell_type":"code","source":"#from : https://www.kaggle.com/coldfir3/efficient-coco-dataset-generator?scriptVersionId=79100851\n\ndef rle2mask(rle, img_w, img_h):\n    '''convert run length encoding to a image mask'''\n    ## transforming the string into an array of shape (2, N)\n    array = np.fromiter(rle.split(), dtype = np.uint)\n    array = array.reshape((-1,2)).T\n    array[0] = array[0] - 1\n    \n    ## decompressing the rle encoding (ie, turning [3, 1, 10, 2] into [3, 4, 10, 11, 12])\n    # for faster mask construction\n    starts, lenghts = array\n    mask_decompressed = np.concatenate([np.arange(s, s + l, dtype = np.uint) for s, l in zip(starts, lenghts)])\n\n    ## Building the binary mask\n    msk_img = np.zeros(img_w * img_h, dtype = np.uint8)\n    msk_img[mask_decompressed] = 1\n    msk_img = msk_img.reshape((img_h, img_w))\n    msk_img = np.asfortranarray(msk_img) ## This is important so pycocotools can handle this object\n    \n    return msk_img\n\n\ndef annotate(idx, row, cat_ids):\n    \n    mask = rle2mask(row['annotation'], row['width'], row['height']) # Binary mask\n    c_rle = maskUtils.encode(mask) # Encoding it back to rle (coco format)\n    c_rle['counts'] = c_rle['counts'].decode('utf-8') # converting from binary to utf-8\n    area = maskUtils.area(c_rle).item() # calculating the area\n    bbox = maskUtils.toBbox(c_rle).astype(int).tolist() # calculating the bboxes\n    annotation = {\n        'segmentation': c_rle,\n        'bbox': bbox,\n        'area': area,\n        'image_id':row['id'], \n        'category_id':cat_ids[row['cell_type']], \n        'iscrowd':0, \n        'id':idx\n    }\n    return annotation\n\ndef coco_structure(df, workers = 6):\n    \n    ## Building the header\n    cat_ids = {name:idx+1 for idx, name in enumerate(df.cell_type.unique())}    \n    \n    cats =[{'name':name, 'id':idx} for name,idx in cat_ids.items()]\n    \n    images = [{'id':idx, 'width':row.width, 'height':row.height, 'file_name':f'train/{idx}.png'} for idx,row in df.groupby('id').agg('first').iterrows()]\n    \n    # Building the annotations\n    annotations = Parallel(n_jobs=workers)(delayed(annotate)(idx, row, cat_ids) for idx, row in tqdm(df.iterrows(), total = len(df)))\n        \n    return {'categories':cats, 'images':images, 'annotations':annotations}\n\n\n\n\n#checking the code on first 1000 rows in train csv \ncoco_annotations = coco_structure(train.iloc[:1000])\n\nwith open('annotations_train_sample.json', 'w', encoding='utf-8') as f:\n    json.dump(coco_annotations, f, ensure_ascii=True, indent=4)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-18T10:30:20.998746Z","iopub.execute_input":"2021-12-18T10:30:20.99906Z","iopub.status.idle":"2021-12-18T10:30:25.522312Z","shell.execute_reply.started":"2021-12-18T10:30:20.999032Z","shell.execute_reply":"2021-12-18T10:30:25.521538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at the generated format\n\nprint('Categories in the data',coco_annotations['categories'])\nprint('_'*50,'---','_'*50)\nprint('Images dict example', coco_annotations['images'][0])\nprint('_'*50,'---','_'*50)\nprint('Annotations dict example',coco_annotations['annotations'][0])","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:30:25.523653Z","iopub.execute_input":"2021-12-18T10:30:25.523931Z","iopub.status.idle":"2021-12-18T10:30:25.534559Z","shell.execute_reply.started":"2021-12-18T10:30:25.523882Z","shell.execute_reply":"2021-12-18T10:30:25.531676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data ","metadata":{}},{"cell_type":"code","source":"# laoding data\n\n\n# path to the data preprocessed by : https://www.kaggle.com/slawekbiel\ndataDir=Path('../input/sartorius-cell-instance-segmentation/')\n\n\n#get configuration \ncfg = get_cfg()\n\n# type \ncfg.INPUT.MASK_FORMAT='bitmask'\n\n#resgister coco instance\n#train\nregister_coco_instances('sartorius_train',{}, '../input/sartorius-cell-instance-segmentation-coco/annotations_train.json', dataDir)\n#validation \nregister_coco_instances('sartorius_val',{},'../input/sartorius-cell-instance-segmentation-coco/annotations_val.json', dataDir)\n#all the train data \nregister_coco_instances('sartorius_train_full',{},'../input/sartorius-cell-instance-segmentation-coco/annotations_all.json', dataDir)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:33:42.031621Z","iopub.execute_input":"2021-12-18T10:33:42.031873Z","iopub.status.idle":"2021-12-18T10:33:42.040201Z","shell.execute_reply.started":"2021-12-18T10:33:42.031846Z","shell.execute_reply":"2021-12-18T10:33:42.038222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get all train data (all 606 Images)\nall_metadata = MetadataCatalog.get('sartorius_train_full')\nall_dat = DatasetCatalog.get('sartorius_train_full')","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:33:49.532755Z","iopub.execute_input":"2021-12-18T10:33:49.533258Z","iopub.status.idle":"2021-12-18T10:33:54.10391Z","shell.execute_reply.started":"2021-12-18T10:33:49.533222Z","shell.execute_reply":"2021-12-18T10:33:54.103159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualizing some samples**","metadata":{}},{"cell_type":"code","source":"#select image \nch = all_dat[5]\nimg = cv2.imread(ch[\"file_name\"])\n\n#get visual\nvisualizer = Visualizer(img[:, :, ::-1], metadata=all_metadata)\nout = visualizer.draw_dataset_dict(ch)\n\n#plot \nplt.figure(figsize = (20,15))\nplt.imshow(out.get_image()[:, :, ::-1])\nplt.axis('off')\nplt.title('sample image')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:34:12.27096Z","iopub.execute_input":"2021-12-18T10:34:12.271663Z","iopub.status.idle":"2021-12-18T10:34:13.236307Z","shell.execute_reply.started":"2021-12-18T10:34:12.271627Z","shell.execute_reply":"2021-12-18T10:34:13.235457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ch = all_dat[10]\nimg = cv2.imread(ch[\"file_name\"])\n\n\nvisualizer = Visualizer(img[:, :, ::-1], metadata=all_metadata)\nout = visualizer.draw_dataset_dict(ch)\n\nplt.figure(figsize = (20,15))\nplt.imshow(out.get_image()[:, :, ::-1])\nplt.axis('off')\nplt.title('sample image')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:34:22.20694Z","iopub.execute_input":"2021-12-18T10:34:22.207462Z","iopub.status.idle":"2021-12-18T10:34:23.02637Z","shell.execute_reply.started":"2021-12-18T10:34:22.207425Z","shell.execute_reply":"2021-12-18T10:34:23.023975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ch = all_dat[100]\nimg = cv2.imread(ch[\"file_name\"])\n\n\nvisualizer = Visualizer(img[:, :, ::-1], metadata=all_metadata)\nout = visualizer.draw_dataset_dict(ch)\n\nplt.figure(figsize = (20,15))\nplt.imshow(out.get_image()[:, :, ::-1])\nplt.axis('off')\nplt.title('sample image')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:34:34.291472Z","iopub.execute_input":"2021-12-18T10:34:34.291743Z","iopub.status.idle":"2021-12-18T10:34:35.156244Z","shell.execute_reply.started":"2021-12-18T10:34:34.291715Z","shell.execute_reply":"2021-12-18T10:34:35.155503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics and Training Helper functions","metadata":{}},{"cell_type":"code","source":"# Taken from https://www.kaggle.com/theoviel/competition-metric-map-iou\n\n\ndef precision_at(threshold, iou):\n    matches = iou > threshold\n    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n    return np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n\ndef score(pred, targ):\n    pred_masks = pred['instances'].pred_masks.cpu().numpy()\n    enc_preds = [mask_util.encode(np.asarray(p, order='F')) for p in pred_masks]\n    enc_targs = list(map(lambda x:x['segmentation'], targ))\n    ious = mask_util.iou(enc_preds, enc_targs, [0]*len(enc_targs))\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, ious)\n        p = tp / (tp + fp + fn)\n        prec.append(p)\n    return np.mean(prec)\n\nclass MAPIOUEvaluator(DatasetEvaluator):\n    def __init__(self, dataset_name):\n        dataset_dicts = DatasetCatalog.get(dataset_name)\n        self.annotations_cache = {item['image_id']:item['annotations'] for item in dataset_dicts}\n            \n    def reset(self):\n        self.scores = []\n\n    def process(self, inputs, outputs):\n        for inp, out in zip(inputs, outputs):\n            if len(out['instances']) == 0:\n                self.scores.append(0)    \n            else:\n                targ = self.annotations_cache[inp['image_id']]\n                self.scores.append(score(out, targ))\n\n    def evaluate(self):\n        return {\"MaP IoU\": np.mean(self.scores)}\n\nclass Trainer(DefaultTrainer):\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        return MAPIOUEvaluator(dataset_name)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:34:35.157532Z","iopub.execute_input":"2021-12-18T10:34:35.157883Z","iopub.status.idle":"2021-12-18T10:34:35.175187Z","shell.execute_reply.started":"2021-12-18T10:34:35.157852Z","shell.execute_reply":"2021-12-18T10:34:35.174584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting Config for the model","metadata":{}},{"cell_type":"code","source":"#getting Mask RCNN architecture -get config file for RCCN_resnet50\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n\n#set training and test data dir\n\n#using all the data for training\ncfg.DATASETS.TRAIN = (\"sartorius_train_full\",)\n\n#validation data(inc in training)\ncfg.DATASETS.TEST = (\"sartorius_val\",)\n\n\n#num workers\ncfg.DATALOADER.NUM_WORKERS = 2\n\n# loading pretrained weights\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n\n#default\ncfg.SOLVER.IMS_PER_BATCH = 2\n\n#learning rate for base model\ncfg.SOLVER.BASE_LR = 0.001\n\n# number of iterations\ncfg.SOLVER.MAX_ITER = 7500\n\n#Region of Interest batch size\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n\n#number of output classes\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n\n#threshold\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n\ncfg.SOLVER.WARMUP_ITERS = 10 #How many iterations to go from 0 to reach base LR\ncfg.SOLVER.STEPS = (2000,4000,6000) #change the LR 0.25,0.5\ncfg.TEST.EVAL_PERIOD = 250\n#save model\ncfg.SOLVER.CHECKPOINT_PERIOD=2000\n\ncfg.TEST.EVAL_PERIOD = len(DatasetCatalog.get('sartorius_train_full')) // cfg.SOLVER.IMS_PER_BATCH  # Once per epoch","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:34:52.764045Z","iopub.execute_input":"2021-12-18T10:34:52.764724Z","iopub.status.idle":"2021-12-18T10:34:56.60082Z","shell.execute_reply.started":"2021-12-18T10:34:52.76468Z","shell.execute_reply":"2021-12-18T10:34:56.600075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training\n\n#make a directory to store model outputs\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n\n#intanciate trainer with configs from pervious cells\ntrainer = Trainer(cfg) \ntrainer.resume_or_load(resume=True)\n\n#start train\ntrainer.train()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-18T10:35:32.350597Z","iopub.execute_input":"2021-12-18T10:35:32.350867Z","iopub.status.idle":"2021-12-18T10:36:17.455085Z","shell.execute_reply.started":"2021-12-18T10:35:32.350838Z","shell.execute_reply":"2021-12-18T10:36:17.454157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting History**","metadata":{}},{"cell_type":"code","source":"metrics_df = pd.read_json(\"./output/metrics.json\",\n                          orient=\"records\",\n                          lines=True).sort_values(\"iteration\")\n\n#get values \nx1 = metrics_df.iteration\nl_cls=metrics_df.loss_cls\nl_msk=metrics_df.loss_mask\nl_bb=metrics_df.loss_box_reg\n\n\nplt.subplots(figsize=(16,8))\nplt.plot(x1,l_cls,color='b',label='Class_loss')\nplt.plot(x1,l_msk,color='k',label='Mask_loss')\nplt.plot(x1,l_bb,color='g',label='BBox_loss')\n\nplt.title('Train Summary')\nplt.legend()\nplt.xlabel('Iterations')\nplt.ylabel('Loss')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:36:20.902205Z","iopub.execute_input":"2021-12-18T10:36:20.903074Z","iopub.status.idle":"2021-12-18T10:36:21.182539Z","shell.execute_reply.started":"2021-12-18T10:36:20.903031Z","shell.execute_reply":"2021-12-18T10:36:21.181732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PLotting predicted Images against Original Images","metadata":{}},{"cell_type":"code","source":"#set dpi \nplt.rcParams['figure.dpi'] = 300","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:36:22.621705Z","iopub.execute_input":"2021-12-18T10:36:22.622266Z","iopub.status.idle":"2021-12-18T10:36:22.626055Z","shell.execute_reply.started":"2021-12-18T10:36:22.622228Z","shell.execute_reply":"2021-12-18T10:36:22.625282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # set a custom testing threshold\n\n\n#predict (with the model we trained above)\npredictor = DefaultPredictor(cfg)\n\n#get tvalidataion data for eval\ndataset_dicts = DatasetCatalog.get('sartorius_val')\n\n\n\noutp,outt = [],[]  # predicted, original\n\n#selecting 6 sampeles to evaluate on \nfor d in random.sample(dataset_dicts, 6):    \n    img = cv2.imread(d[\"file_name\"])\n    outputs = predictor(img)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n    \n    #visualize\n    v = Visualizer(img[:, :, ::-1],\n                   metadata = MetadataCatalog.get('sartorius_train'), \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n    )\n    \n    \n    out_pred = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    visualizer = Visualizer(img[:, :, ::-1])\n    out_target = visualizer.draw_dataset_dict(d)\n    outp.append(out_pred)\n    outt.append(out_target)\n\n    plt.subplots(6,2,figsize=(30,60))\n\n    \nn=1\nfor i in range(len(outp)):\n    plt.subplot(6,2,n)\n    plt.imshow(outp[i].get_image())\n    plt.axis('off')\n    plt.title('Predicted(Thresh=0.5)')\n    n+=1\n    \n    plt.subplot(6,2,n)\n    plt.imshow(outt[i].get_image())\n    plt.axis('off')\n    plt.title('Original')\n    if n==12:\n        break\n    n+=1\n    \n    \nplt.tight_layout()\nplt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:36:24.725721Z","iopub.execute_input":"2021-12-18T10:36:24.726331Z","iopub.status.idle":"2021-12-18T10:37:20.008852Z","shell.execute_reply.started":"2021-12-18T10:36:24.72629Z","shell.execute_reply":"2021-12-18T10:37:20.007879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualizing with a different prediction threshold**","metadata":{}},{"cell_type":"markdown","source":"**Thresh = 0.35**","metadata":{}},{"cell_type":"code","source":"# changing pred thresh\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.35\n\n\n#predictor\npredictor = DefaultPredictor(cfg)\n\n\noutp,outt = [],[]  # predicted, original\n\n#selecting 5 sampeles to evaluate on \nfor d in random.sample(dataset_dicts, 10):    \n    img = cv2.imread(d[\"file_name\"])\n    outputs = predictor(img)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n    \n    #visualize\n    v = Visualizer(img[:, :, ::-1],\n                   metadata = MetadataCatalog.get('sartorius_train'), \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n    )\n    \n    \n    out_pred = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    visualizer = Visualizer(img[:, :, ::-1])\n    out_target = visualizer.draw_dataset_dict(d)\n    outp.append(out_pred)\n    outt.append(out_target)\n\n\n    \n    \nplt.subplots(6,2,figsize=(30,60))\n#plot\nn=1\nfor i in range(len(outp)):\n    plt.subplot(6,2,n)\n    plt.imshow(outp[i].get_image())\n    plt.axis('off')\n    plt.title('Predicted(Thresh=0.5)')\n    n+=1\n    \n    plt.subplot(6,2,n)\n    plt.imshow(outt[i].get_image())\n    plt.axis('off')\n    plt.title('Original')\n    if n==12:\n        break\n    n+=1\n    \n    \nplt.tight_layout()\nplt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:37:20.01107Z","iopub.execute_input":"2021-12-18T10:37:20.011619Z","iopub.status.idle":"2021-12-18T10:37:46.265153Z","shell.execute_reply.started":"2021-12-18T10:37:20.011574Z","shell.execute_reply":"2021-12-18T10:37:46.263865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Thresh = 0.25**","metadata":{}},{"cell_type":"code","source":"# changing pred thresh\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.25\n\n\n#predictor\npredictor = DefaultPredictor(cfg)\n\n\noutp,outt = [],[]  # predicted, original\n\n#selecting 5 sampeles to evaluate on \nfor d in random.sample(dataset_dicts, 10):    \n    img = cv2.imread(d[\"file_name\"])\n    outputs = predictor(img)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n    \n    #visualize\n    v = Visualizer(img[:, :, ::-1],\n                   metadata = MetadataCatalog.get('sartorius_train'), \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n    )\n    \n    \n    out_pred = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    visualizer = Visualizer(img[:, :, ::-1])\n    out_target = visualizer.draw_dataset_dict(d)\n    outp.append(out_pred)\n    outt.append(out_target)\n    \n\n    #plot\nplt.subplots(6,2,figsize=(30,60))\n\nn=1\nfor i in range(len(outp)):\n    plt.subplot(6,2,n)\n    plt.imshow(outp[i].get_image())\n    plt.axis('off')\n    plt.title('Predicted(Thresh=0.25)')\n    n+=1\n    \n    plt.subplot(6,2,n)\n    plt.imshow(outt[i].get_image())\n    plt.axis('off')\n    plt.title('Original')\n    if n==12:\n        break\n    n+=1\n    \n    \nplt.tight_layout()\nplt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:37:46.266805Z","iopub.execute_input":"2021-12-18T10:37:46.267356Z","iopub.status.idle":"2021-12-18T10:38:11.400027Z","shell.execute_reply.started":"2021-12-18T10:37:46.267292Z","shell.execute_reply":"2021-12-18T10:38:11.39899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Thresh = 0.15**","metadata":{}},{"cell_type":"code","source":"# changing pred thresh\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.15\n\n\n#predictor\npredictor = DefaultPredictor(cfg)\n\n\noutp,outt = [],[]  # predicted, original\n\n#selecting 5 sampeles to evaluate on \nfor d in random.sample(dataset_dicts, 10):    \n    img = cv2.imread(d[\"file_name\"])\n    outputs = predictor(img)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n    \n    #visualize\n    v = Visualizer(img[:, :, ::-1],\n                   metadata = MetadataCatalog.get('sartorius_train'), \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n    )\n    \n    \n    out_pred = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    visualizer = Visualizer(img[:, :, ::-1])\n    out_target = visualizer.draw_dataset_dict(d)\n    outp.append(out_pred)\n    outt.append(out_target)\n    \n    \n#plot\nplt.subplots(6,2,figsize=(30,60))\nn=1\nfor i in range(len(outp)):\n    plt.subplot(6,2,n)\n    plt.imshow(outp[i].get_image())\n    plt.axis('off')\n    plt.title('Predicted(Thresh=0.15)')\n    n+=1\n    \n    plt.subplot(6,2,n)\n    plt.imshow(outt[i].get_image())\n    plt.axis('off')\n    plt.title('Original')\n    if n==12:\n        break\n    n+=1\n    \n    \nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:38:11.402197Z","iopub.execute_input":"2021-12-18T10:38:11.403061Z","iopub.status.idle":"2021-12-18T10:38:33.516015Z","shell.execute_reply.started":"2021-12-18T10:38:11.403017Z","shell.execute_reply":"2021-12-18T10:38:33.514881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sample Inference","metadata":{}},{"cell_type":"code","source":"\n# path to competiton directory\ncomp_dir = Path('../input/sartorius-cell-instance-segmentation')\n\n# get test names \ntest_paths = (comp_dir/'test').ls()\n\nprint('Number of Images in Test', len(test_paths))\nprint('Test Images Paths  \\n',test_paths)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:39:58.766095Z","iopub.execute_input":"2021-12-18T10:39:58.766356Z","iopub.status.idle":"2021-12-18T10:39:58.773926Z","shell.execute_reply.started":"2021-12-18T10:39:58.766327Z","shell.execute_reply":"2021-12-18T10:39:58.773131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Helper functions**","metadata":{}},{"cell_type":"code","source":"# From https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_decode(mask_rle, shape=(520, 704)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)  # Needed to align to RLE direction\n\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-18T10:40:02.502497Z","iopub.execute_input":"2021-12-18T10:40:02.502762Z","iopub.status.idle":"2021-12-18T10:40:02.511446Z","shell.execute_reply.started":"2021-12-18T10:40:02.502734Z","shell.execute_reply":"2021-12-18T10:40:02.510679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_img(path):\n    im = cv2.imread(str(path))\n    return im\n    \n\n\ndef predict_masks(path,\n                  Predictor_Model, # predicted model\n                  class_thresh, # threshold for each class\n                  pixel_thresh): # pixel threshold for each class\n    \n    im = load_img(path)\n    pred = Predictor_Model(im)#get prediction\n    pred_class = torch.mode(pred['instances'].pred_classes)[0] # get classes\n    \n    take = pred['instances'].scores >= class_thresh[pred_class] # segment pixels based on what class it is from\n    pred_masks = pred['instances'].pred_masks[take]\n    pred_masks = pred_masks.cpu().numpy()\n    used = np.zeros(im.shape[:2], dtype=int) \n    \n    res = []\n    \n    for mask in pred_masks:\n        mask = mask * (1-used)\n        if mask.sum() >= pixel_thresh[pred_class]: # skip predictions with small area\n            used += mask\n            res.append(rle_encode(mask))\n            \n    return res","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-18T10:40:03.357354Z","iopub.execute_input":"2021-12-18T10:40:03.357609Z","iopub.status.idle":"2021-12-18T10:40:03.366831Z","shell.execute_reply.started":"2021-12-18T10:40:03.35758Z","shell.execute_reply":"2021-12-18T10:40:03.364594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Set thresholds and configure predictor**","metadata":{}},{"cell_type":"code","source":"# set params \n\ndetect_per_img = 600\n\n\n# using the thresholds as author..will try to tweak them later \nclss=['shsy5y','astro','cort']\n\nclass_thresh= [.15, .35, .55]\nthresholds_dict = dict(zip(clss,class_thresh))\n#pixel_thresh\npixel_thresh= [75, 150, 75]\n\n# for predictions\nids, masks=[],[]  # img ids, mask annotations\n\nimg_masks = []\n\n# path to best predictor model\npredictor_model_path = './output/model_final.pth'","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:40:05.566777Z","iopub.execute_input":"2021-12-18T10:40:05.56705Z","iopub.status.idle":"2021-12-18T10:40:05.57258Z","shell.execute_reply.started":"2021-12-18T10:40:05.567022Z","shell.execute_reply":"2021-12-18T10:40:05.571762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#config dict\ncfg = get_cfg()\n#get model arch\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n#mask format\ncfg.INPUT.MASK_FORMAT='bitmask'\n#num of classes\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 3 \n# weights from training notebook model\ncfg.MODEL.WEIGHTS = predictor_model_path\n#num of detections per image\ncfg.TEST.DETECTIONS_PER_IMAGE = detect_per_img\n","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:42:39.132274Z","iopub.execute_input":"2021-12-18T10:42:39.132549Z","iopub.status.idle":"2021-12-18T10:42:39.154232Z","shell.execute_reply.started":"2021-12-18T10:42:39.132517Z","shell.execute_reply":"2021-12-18T10:42:39.153588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference**","metadata":{}},{"cell_type":"code","source":"predictor = DefaultPredictor(cfg)\n\nfor path in test_paths:\n    encoded_masks = predict_masks(path=path,\n                                  Predictor_Model=predictor, # predicted model\n                                  class_thresh = class_thresh, # threshold for each class\n                                  pixel_thresh = pixel_thresh)      # pixel threshold(area) for each class\n\n    img_masks.append(encoded_masks) # append masks for further visualization\n    for encoding in encoded_masks:\n        #save img ids\n        ids.append(path.stem)\n        # save individual encodings\n        masks.append(encoding)\n\n        \n        \nprint('Number of Annotations predicted for 3 Test Images are ',len(masks))","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:42:40.951602Z","iopub.execute_input":"2021-12-18T10:42:40.951881Z","iopub.status.idle":"2021-12-18T10:42:42.221744Z","shell.execute_reply.started":"2021-12-18T10:42:40.95185Z","shell.execute_reply":"2021-12-18T10:42:42.221013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_predcted_mask(img_path,\n                       encoded_mask,\n                       thresholds_dict=thresholds_dict):\n    '''plot mask with imag'''\n    print('Thresholds used are ',thresholds_dict)\n    \n    \n    fig,axs=plt.subplots(1,2,figsize=(16,8))\n    \n    img = load_img(img_path)\n    \n    axs[0].imshow(img)\n    plt.axis('off')\n    axs[0].set_title('Test Image')\n    \n    for enc in encoded_masks:\n        decoded = rle_decode(enc)\n        axs[1].imshow(np.ma.masked_where(decoded==0, decoded))\n        plt.axis('off')\n        axs[1].set_title('Predicted Mask')\n    \n    plt.tight_layout()\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:44:26.590322Z","iopub.execute_input":"2021-12-18T10:44:26.590584Z","iopub.status.idle":"2021-12-18T10:44:26.597399Z","shell.execute_reply.started":"2021-12-18T10:44:26.590555Z","shell.execute_reply":"2021-12-18T10:44:26.59672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting predicted Images\n","metadata":{}},{"cell_type":"code","source":"print('Test Image 1')\nplot_predcted_mask(img_path =test_paths[0],encoded_mask=img_masks[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:44:28.751779Z","iopub.execute_input":"2021-12-18T10:44:28.752296Z","iopub.status.idle":"2021-12-18T10:44:29.838414Z","shell.execute_reply.started":"2021-12-18T10:44:28.752257Z","shell.execute_reply":"2021-12-18T10:44:29.837759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Test Image 2')\nplot_predcted_mask(img_path =test_paths[1],img_masks[1])","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:30:25.76636Z","iopub.status.idle":"2021-12-18T10:30:25.766829Z","shell.execute_reply.started":"2021-12-18T10:30:25.766583Z","shell.execute_reply":"2021-12-18T10:30:25.766607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Test Image 3')\nplot_predcted_mask(img_path =test_paths[2],img_masks[2])","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:30:25.768011Z","iopub.status.idle":"2021-12-18T10:30:25.768845Z","shell.execute_reply.started":"2021-12-18T10:30:25.768597Z","shell.execute_reply":"2021-12-18T10:30:25.768622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**save some models(and delete the rest)**","metadata":{}},{"cell_type":"code","source":"#saving these output files\nsave_list = ['./output/events.out.tfevents.1639574743.943df9329b01.34.0',\n             './output/metrics.json',\n             './output/model_0002999.pth',\n            './output/model_final.pth']\n\n#path where outputs of detectron2 are saved\nsource = './output'\n#path to move \ndestination='./detectron_outputs'\nos.mkdir(destination)\n\n\nfor file_path in save_list:\n    shutil.copy(file_path,destination)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:30:25.770284Z","iopub.status.idle":"2021-12-18T10:30:25.770689Z","shell.execute_reply.started":"2021-12-18T10:30:25.77047Z","shell.execute_reply":"2021-12-18T10:30:25.770491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#delete other saved files \nshutils.rmtree(source)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:30:25.771928Z","iopub.status.idle":"2021-12-18T10:30:25.772647Z","shell.execute_reply.started":"2021-12-18T10:30:25.772394Z","shell.execute_reply":"2021-12-18T10:30:25.772419Z"},"trusted":true},"execution_count":null,"outputs":[]}]}