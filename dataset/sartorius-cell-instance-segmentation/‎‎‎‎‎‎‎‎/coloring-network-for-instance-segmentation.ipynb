{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Coloring network for instance segmentation\n\nThe objective of this notebook is to show the architecture of a coloring network for instance segmentation.\nIt is based on Unet and make use of [1]:\n\n- A seed net to predict the seed of cell. This is instance detection and finds the centroids of target objects. \n\n- A color net to propagate the segmentation of seed to the whole cell. This step replaces watershed transform which can be unstable and produce over-segmentation for thin, long target object, especially when there is multiple seeds (i.e. multiple basins) detected for one object\n\n\n[1] https://www.kaggle.com/hengck23/split-adjoining-cell-into-subsets-of-non-touching","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nsys.path.append('../input/coloring-network')\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.cuda.amp as amp\nis_amp = True   \n\nfrom timm.models.resnet import *\n \nimport numpy as np\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport skimage.measure\nfrom color_graph import *\n\nimage_width  = 704\nimage_height = 520\n\n#helper\ndef image_show(image, mode='gray'):\n    image = (image-image.min())/(image.max()-image.min()+0.0001)\n    if mode=='gray':\n        plt.imshow(image,'gray')\n        \n    if mode=='rgb':\n        plt.imshow(image[...,::-1])\n        \n        \ndef rle_decode(rle, width=image_width, height=image_height, fill=1, dtype=np.float32):\n    s = rle.split()\n    start  = np.asarray(s[0::2], dtype=int)-1\n    length = np.asarray(s[1::2], dtype=int)\n    end = start + length\n    image = np.zeros(height * width, dtype=dtype)\n    for s, e in zip(start, end):\n        image[s:e] = fill\n    image = image.reshape(height, width) #.T\n    return image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-04T01:06:21.877759Z","iopub.execute_input":"2021-12-04T01:06:21.878169Z","iopub.status.idle":"2021-12-04T01:06:21.891623Z","shell.execute_reply.started":"2021-12-04T01:06:21.878133Z","shell.execute_reply":"2021-12-04T01:06:21.890957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass Backbone(nn.Module):\n    def __init__(self, in_channel=3, arch='34d'):\n        super().__init__()\n        if arch=='18d':\n            e = resnet18d(pretrained=True)\n        if arch=='34d':\n            e = resnet34d(pretrained=True)\n        if arch=='50d':\n            e = resnet50d(pretrained=True)\n\n        if in_channel!=3:\n            e.conv1[0] = nn.Conv2d(in_channel, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n        self.block0 = nn.Sequential(\n            e.conv1,\n            e.bn1,\n            e.act1,\n        )\n        self.block1 = nn.Sequential(\n            e.maxpool,\n            e.layer1,\n        )\n        self.block2 = e.layer2\n        self.block3 = e.layer3\n        self.block4 = e.layer4\n        del e    #dropped\n\n    def forward(self,x ):\n        x0 = self.block0(x)\n        x1 = self.block1(x0)\n        x2 = self.block2(x1)\n        x3 = self.block3(x2)\n        x4 = self.block4(x3)\n        return [x0,x1,x2,x3,x4]\n\n\nclass FeaturePyramidNet(nn.Module):\n    def __init__( self, in_channel, out_channel ):\n        super(FeaturePyramidNet, self).__init__()\n        self.inner_block = nn.ModuleList()\n        self.layer_block = nn.ModuleList()\n\n        for in_c in in_channel:\n            self.inner_block.append(\n                nn.Conv2d(in_c, out_channel, 1)\n            )\n            self.layer_block.append(\n                nn.Conv2d(out_channel, out_channel, 3, padding=1)\n            )\n\n        #extra\n        self.extra_max_pool = nn.MaxPool2d(kernel_size=2,stride=2)\n\n        # initialize parameters now to avoid modifying the initialization of top_blocks\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_uniform_(m.weight, a=1)\n                nn.init.constant_(m.bias, 0)\n\n\n    def forward(self, x):\n        num_layer = len(x)\n        out = []\n\n        last_inner = self.inner_block[-1](x[-1])\n        last_layer = self.layer_block[-1](last_inner)\n        out.append(last_layer)\n\n        for i in range(num_layer-2, -1, -1):\n            lateral    = self.inner_block[i](x[i])\n            top_down   = F.interpolate(last_inner, size=lateral.shape[-2:], mode='nearest')\n            last_inner = lateral + top_down\n            last_layer = self.layer_block[i](last_inner)\n            out.insert(0,last_layer)\n\n        out.append(\n            self.extra_max_pool(out[-1])\n        )\n        return out\n\n#----------------------------------------------------------------------------------------\nclass SeedNet(nn.Module):\n    def __init__(self, ):\n        super().__init__()\n\n        self.backbone = Backbone(arch='50d')\n        #self.fpn = FeaturePyramidNet([64, 64, 128, 256, 512], 32)\n        self.fpn = FeaturePyramidNet([64, 256, 512, 1024, 2048], 64)\n        self.fuse = nn.Sequential(\n            nn.Conv2d(64 * 6, 256, kernel_size=3, padding=1, bias=None),\n            nn.BatchNorm2d(256),\n            nn.SiLU(),\n            nn.Conv2d(256, 128, kernel_size=3, padding=1, bias=None),\n            nn.BatchNorm2d(128),\n            nn.SiLU(),\n\n            nn.UpsamplingBilinear2d(scale_factor=2),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1, bias=None),\n            nn.BatchNorm2d(64),\n            nn.SiLU(),\n        )\n        self.seed = nn.Sequential(\n            nn.Conv2d(64, 4, kernel_size=1,padding=0),\n        )#3 cell_type + background\n\n\n    def forward(self, input):\n        x = input['x']\n\n        feature = self.backbone(x)\n        feature = self.fpn(feature)\n\n        z = []\n        for i, f in enumerate(feature):\n            if i != 0:\n                f = F.interpolate(f, size=feature[0].shape[-2:], mode='nearest')\n            z.append(f)\n        z = torch.cat(z, 1)  # torch.Size([1, 192, 256, 256])\n        z = self.fuse(z)\n\n        # ---\n        seed = self.seed(z)\n        output = {\n            'seed' : seed,\n        }\n        return output\n\n\nclass ColorNet(nn.Module):\n    def __init__(self, ):\n        super().__init__()\n\n        self.backbone = Backbone(in_channel=3+1,arch='34d' ) \n        self.fpn = FeaturePyramidNet([64,64,128,256,512],32)\n        self.fuse= nn.Sequential(\n            nn.Conv2d(32*6, 256, kernel_size=3,padding=1, bias=None),\n            nn.BatchNorm2d(256),\n            nn.SiLU(),\n            nn.Conv2d(256, 128, kernel_size=3,padding=1, bias=None),\n            nn.BatchNorm2d(128),\n            nn.SiLU(),\n\n            nn.UpsamplingBilinear2d(scale_factor=2),\n            nn.Conv2d(128, 64, kernel_size=3,padding=1, bias=None),\n            nn.BatchNorm2d(64),\n            nn.SiLU(),\n        )\n        self.color = nn.Sequential(\n            nn.Conv2d(64, 1, kernel_size=1,padding=0),\n        )\n\n    def forward(self, input):\n        x        = input['x']\n        seed     = input['sampled_seed']\n        batch_size, num_color, H, W = seed.shape\n\n        #duplicate for num of colored seeds\n        x = x.reshape(batch_size, 1, 3, H, W).expand(-1,num_color,-1,-1,-1)\n        seed = seed.reshape(batch_size, num_color, 1, H, W)\n        x = torch.cat([x, seed,], 2).reshape(batch_size*num_color, -1, H, W)\n\n        feature = self.backbone(x)\n        feature = self.fpn(feature)\n\n        z=[] #resize fpn feature maps to same size\n        for i, f in enumerate(feature):\n            if i!=0:\n                f = F.interpolate(f,size=feature[0].shape[-2:],mode='nearest')\n            z.append(f)\n        z = torch.cat(z,1)\n        z = self.fuse(z)\n\n        #---\n        color = self.color(z)\n        color = color.reshape(batch_size, num_color, H, W)\n        output ={\n            'color' : color,\n        }\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:56:13.641469Z","iopub.execute_input":"2021-12-04T00:56:13.641932Z","iopub.status.idle":"2021-12-04T00:56:13.680806Z","shell.execute_reply.started":"2021-12-04T00:56:13.641881Z","shell.execute_reply":"2021-12-04T00:56:13.68013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's applied a learned model on the training images (**not validation**) to inspect the results. Can the solution learned complex details of the instance segmentation?\n\nIf the results looks good, then we can proceed to the next stage of generalisatiing the newtork to unseen validation image.","metadata":{}},{"cell_type":"code","source":"# load the learned model\nseed_net  = SeedNet() \ncolor_net = ColorNet() \n\nmodel_file = '../input/coloring-network/00024500.model.pth'\nf = torch.load(model_file, map_location=lambda storage, loc: storage)\nseed_net.load_state_dict(f['seed_state_dict'],strict=True)   \ncolor_net.load_state_dict(f['color_state_dict'],strict=True)   \nprint('load model ok !')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:56:13.682376Z","iopub.execute_input":"2021-12-04T00:56:13.683024Z","iopub.status.idle":"2021-12-04T00:56:20.883528Z","shell.execute_reply.started":"2021-12-04T00:56:13.68298Z","shell.execute_reply":"2021-12-04T00:56:20.882388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load some images\n\nimage_id =[\n    '11c2e4fcac6d', #astro example\n    '7ad870da5a63', #cort example\n    '1c10ee85de67', #shsy5y example\n]\n        \nimage = []\nfor id in image_id:\n    image_file = '../input/sartorius-cell-instance-segmentation/train/%s.png'%(id)\n    m = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n    image.append(m)\n    \n#change to torch tensor\nx = torch.from_numpy(np.stack(image)).unsqueeze(1)\nx = x.float()/255\n\nmx = torch.median(x)\nx  = x-mx\nx  = x.repeat(1,3,1,1)\nx[:,0] *= 2\nx[:,1] *= 4\nx[:,2] *= 16\nx = torch.tanh(x)\n\ninput ={\n    'x': x\n}\n","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:56:20.886148Z","iopub.execute_input":"2021-12-04T00:56:20.886466Z","iopub.status.idle":"2021-12-04T00:56:21.050835Z","shell.execute_reply.started":"2021-12-04T00:56:20.88642Z","shell.execute_reply":"2021-12-04T00:56:21.050049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#apply seed net and visualise results\n\nseed_net.eval()\nwith amp.autocast(enabled = is_amp):\n    with torch.no_grad():\n        output = seed_net(input)\n\n    \ndef show_predicted_seed(output, input) : \n    \n    x = input['x'].data.cpu().numpy()*0.5 +0.5\n    x = np.ascontiguousarray(x.transpose(0,2,3,1))\n    \n    seed = torch.softmax(output['seed'],1).float().data.cpu().numpy()\n    seed_argmax = np.argmax(seed,1)\n                \n    batch_size = len(x)\n    for b in range(batch_size):\n        overlay = x[b].copy()  \n        overlay[seed_argmax[b]!=0] = (1,0,0)\n        plt.figure(figsize=(12, 9)), image_show(overlay, 'rgb')\n        \nshow_predicted_seed(output, input) ","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:56:21.054818Z","iopub.execute_input":"2021-12-04T00:56:21.055895Z","iopub.status.idle":"2021-12-04T00:56:36.165796Z","shell.execute_reply.started":"2021-12-04T00:56:21.05584Z","shell.execute_reply":"2021-12-04T00:56:36.165089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split seeds\nnum_color=8 #exclude background (which is zero)\ndef split_seed_and_add_to_input(output, input): \n    s = torch.argmax(output['seed'], 1, keepdim=True) != 0\n    s = s.data.cpu().numpy()\n    \n    #Label connected regions (CCL) \n    s_l = skimage.measure.label(s, background=0) #optional : filter invalid case (e.g. too small, etc)\n    expand = expand_labels(s_l, distance=20)\n    \n    sampled_seed = []\n    batch_size = len(s)\n    for b in range(batch_size):\n        e_l = do_color_label(expand[b,0], num_color=num_color)\n        print('assigned color', np.unique(e_l))\n        e_l = e_l*(s_l[b,0]>0)\n        sampled_seed.append(e_l)\n        \n    sampled_seed = np.stack(sampled_seed)\n    sampled_seed = torch.from_numpy(sampled_seed)\n    \n    sampled_seed = F.one_hot(sampled_seed.long(),num_color+1).float()\n    sampled_seed = sampled_seed.permute(0,3,1,2).contiguous()\n    sampled_seed[:,0]=1-sampled_seed[:,1:].sum(1,keepdim=False) \n    print('sampled_seed.shape :', sampled_seed.shape)\n    \n    input['sampled_seed'] = sampled_seed.detach()\n    return input\n        \ndef show_sampled_seed(input) : \n    \n    s = input['sampled_seed']\n    s = F.interpolate(s,size=(128,128),mode='bilinear',align_corners=False)\n    s = s.data.cpu().numpy() \n    batch_size, num_color,h, w = s.shape\n    \n   \n    for b in range(batch_size):\n        overlay=[]\n        for i in range(num_color):\n            si = s[b,i]\n            si[0]=si[-1]=si[:,0]=si[:,-1]=0.5\n            overlay.append(si)\n        overlay = np.hstack(overlay)\n        plt.figure(figsize=(24, 18)), image_show(overlay, 'gray')\n        \n         \nwith torch.no_grad():\n    input = split_seed_and_add_to_input(output, input)\n\nshow_sampled_seed(input)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:56:36.167323Z","iopub.execute_input":"2021-12-04T00:56:36.167563Z","iopub.status.idle":"2021-12-04T00:56:38.647528Z","shell.execute_reply.started":"2021-12-04T00:56:36.167536Z","shell.execute_reply":"2021-12-04T00:56:38.646703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#apply color net and visualise results\ncolor_net.eval()\nwith amp.autocast(enabled = is_amp):\n    with torch.no_grad():\n        output1 = color_net(input)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_predicted_color(output, input) : \n    colormap = np.array([\n            [  0,  0,  0],\n            [ 77,159,255],\n            [  0,255,  0],\n            [255,  0,  0],\n            [  0,255,255],\n            [255,255,  0],\n            [255,150,255],\n            #[234,178,200],\n            [  0,  0,255],\n            [255,255,255],\n        ])\n    \n    color = torch.softmax(output['color'], 1)\n    color_small = F.interpolate(color,size=(128,128),mode='bilinear', align_corners=False)\n    color_argmax = color.argmax(1).data.cpu().numpy()\n    color_small  = color_small.float().data.cpu().numpy()       \n           \n    batch_size, num_color,h, w = color.shape\n    for b in range(batch_size):\n        \n        overlay=[] \n        for i in range(num_color):\n            m = color_small[b,i].copy()\n            m[:,0]=m[:,-1]=m[0]=m[-1]=0.5\n            overlay.append(m)\n\n        overlay = np.hstack(overlay)\n        plt.figure(figsize=(24, 18)), image_show(overlay, 'gray')\n          \n        overlay = draw_label_to_overlay(color_argmax[b],colormap)\n        plt.figure(figsize=(12, 9)), image_show(overlay, 'rgb')\n        \nshow_predicted_color(output1, input) ","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:58:05.716283Z","iopub.execute_input":"2021-12-04T00:58:05.716623Z","iopub.status.idle":"2021-12-04T00:58:08.38828Z","shell.execute_reply.started":"2021-12-04T00:58:05.716578Z","shell.execute_reply":"2021-12-04T00:58:08.38735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I note that the seeds of yellow (color layer 4) stick together after region growing. Hence the seed splitting algorithm still needs to be improved. \n\nDifferent splitting can be used as test-time augmentation (TTA) for inference.","metadata":{}},{"cell_type":"code","source":"# show region growing on for a few color channel\n\ndef show_region_growing(output, input) : \n \n    \n    color = torch.softmax(output['color'], 1)\n    color = color.float().data.cpu().numpy()\n    \n    x = input['x'].data.cpu().numpy()*0.5 +0.5\n    x = np.ascontiguousarray(x.transpose(0,2,3,1))\n    \n    seed = torch.softmax(input['sampled_seed'],1).float().data.cpu().numpy()\n    seed_argmax = np.argmax(seed,1)\n           \n    batch_size, num_color,h, w = color.shape\n    for b in range(batch_size):\n        overlay =[]\n        for i in [2,3,4]:\n            o = x[b].copy() \n            o[...,1] = 1-(1-o[...,1])*(1-color[b,i]) \n            o[seed_argmax[b]!=0] = (1,0,0)\n            o[:,0]=o[:,-1]=o[0]=o[-1]=1\n            overlay.append(o) \n        overlay = np.hstack(overlay)\n        plt.figure(figsize=(24, 18)), image_show(overlay, 'rgb')\n        \nshow_region_growing(output1, input) ","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:58:08.389602Z","iopub.execute_input":"2021-12-04T00:58:08.38982Z","iopub.status.idle":"2021-12-04T00:58:11.245559Z","shell.execute_reply.started":"2021-12-04T00:58:08.389792Z","shell.execute_reply":"2021-12-04T00:58:11.244764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try with the \"ground-truth splitting\"","metadata":{}},{"cell_type":"code","source":"#ground truth\ntrain_df = pd.read_csv('../input/sartorius-cell-instance-segmentation/train.csv')\n\nlabel = [] \nfor id in image_id: \n    df = train_df[train_df['id']==id].reset_index(drop=True)\n    l = np.zeros((image_height,image_width), dtype=np.int32)\n    for i,d in df.iterrows():\n        m = rle_decode(d.annotation, fill=True, dtype=np.bool)\n        l[m]=i+1\n    label.append(l)\n    \nlabel = torch.from_numpy(np.stack(label)).long().unsqueeze(1)\ntarget = {\n    'label': label,\n}\n#print(label.shape)\n    \n\n#split seeds based on ground truth (e.g. use for training)\nnum_color=8 #exclude background (which is zero)\ndef split_seed_and_add_to_input_and_target(output, input, target): \n    l = target['label'] \n    s = torch.argmax(output['seed'], 1, keepdim=True) != 0\n    s = s * l \n \n    if 1: #remove miss:\n        batch_size = len(s)\n        for b in range(batch_size):\n            #https://discuss.pytorch.org/t/intersection-between-to-vectors-tensors/50364/9\n            # a.symmetric_difference(b)\n\n            u1 = torch.unique(s[b]).data.cpu().numpy()\n            u2 = torch.unique(l[b]).data.cpu().numpy()\n            miss = list(set(u1) ^ set(u2))\n            #print(miss)\n            for m in miss:\n                l[b][l[b]==m]=0\n\n        # graph-color sampling -----------------------------\n        s = s.data.cpu().numpy()\n        l = l.data.cpu().numpy()\n\n        sampled_seed  = []\n        sampled_color = []\n        for b in range(batch_size):\n            l0 = l[b,0]\n            unique, index, inverse, area = \\\n                np.unique(l0, return_index=True, return_inverse=True, return_counts=True)\n\n            l1 = inverse.reshape(l0.shape)\n            expand = expand_labels(l1, distance=30)\n            l2 = do_color_label(expand, num_color=num_color)\n            l2[l2>num_color]=0\n            l2 = l2 * (l0 > 0)\n            s2 = l2*(s[b,0]>0)\n\n            sampled_seed.append(s2)\n            sampled_color.append(l2)\n#             if 0: #debug\n#                 overlay = []\n#                 for i in range(num_color + 1):\n#                     overlay.append(np.vstack([s2==i,l2==i]))  \n#                 overlay = np.hstack(overlay)\n#                 image_show('overlay', overlay, min=0, max=1, resize=0.7)\n#                 cv2.waitKey(0)\n\n\n        sampled_seed  = torch.from_numpy(np.stack(sampled_seed)).long().to(input['x'].device)\n        sampled_color = torch.from_numpy(np.stack(sampled_color)).long().to(input['x'].device)\n        sampled_seed  = F.one_hot(sampled_seed, num_color + 1).float()\n        sampled_color = F.one_hot(sampled_color, num_color + 1).float()\n\n        input['sampled_seed' ] = sampled_seed.permute(0,3,1,2).contiguous()\n        target['sampled_color'] = sampled_color.permute(0,3,1,2).contiguous()\n        return input,target\n\n\ninput,target = split_seed_and_add_to_input_and_target(output, input, target)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T01:29:57.00052Z","iopub.execute_input":"2021-12-04T01:29:57.000805Z","iopub.status.idle":"2021-12-04T01:29:58.321437Z","shell.execute_reply.started":"2021-12-04T01:29:57.000776Z","shell.execute_reply":"2021-12-04T01:29:58.320428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#apply color net and visualise results\ncolor_net.eval()\nwith amp.autocast(enabled = is_amp):\n    with torch.no_grad():\n        output1 = color_net(input)\n        \n'''\nfor training, apply loss like:\ncolor_loss(predict=output1['color'], truth=target['sampled_color'])\n\n'''        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_predicted_color(output, input, target) : \n    colormap = np.array([\n            [  0,  0,  0],\n            [ 77,159,255],\n            [  0,255,  0],\n            [255,  0,  0],\n            [  0,255,255],\n            [255,255,  0],\n            [255,150,255],\n            #[234,178,200],\n            [  0,  0,255],\n            [255,255,255],\n        ])\n    \n    color = torch.softmax(output['color'], 1)\n    color_small = F.interpolate(color,size=(128,128),mode='bilinear', align_corners=False)\n    color_argmax = color.argmax(1).data.cpu().numpy()\n    color_small  = color_small.float().data.cpu().numpy()       \n           \n        \n    color_hat = target['sampled_color']\n    color_small_hat = F.interpolate(color_hat,size=(128,128),mode='bilinear', align_corners=False)\n    color_argmax_hat = color_hat.argmax(1).data.cpu().numpy()\n    color_small_hat  = color_small_hat.float().data.cpu().numpy()       \n    \n    \n    batch_size, num_color,h, w = color.shape\n    for b in range(batch_size):\n        \n        overlay = [] \n        for i in range(num_color):\n            m_hat = color_small_hat[b,i].copy()\n            m_hat[:,0]=m_hat[:,-1]=m_hat[0]=m_hat[-1]=0.5 \n            m = color_small[b,i].copy()\n            m[:,0]=m[:,-1]=m[0]=m[-1]=0.5 \n            overlay.append(np.vstack([m_hat,m]))\n\n        overlay = np.hstack(overlay)\n        plt.figure(figsize=(24, 18)), image_show(overlay, 'gray')\n        \n        \n        m1 = draw_label_to_overlay(color_argmax_hat[b],colormap)\n        m2 = draw_label_to_overlay(color_argmax[b],colormap)\n        m1[:,0]=m1[:,-1]=m1[0,:]=m1[-1,:]=(255,255,255) \n        m2[:,0]=m2[:,-1]=m2[0,:]=m2[-1,:]=(255,255,255)  \n        overlay = np.hstack([m1, m2])\n        plt.figure(figsize=(24, 18)), image_show(overlay, 'rgb')\n         \n        \nshow_predicted_color(output1, input, target)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T01:38:31.92222Z","iopub.execute_input":"2021-12-04T01:38:31.922524Z","iopub.status.idle":"2021-12-04T01:38:35.839603Z","shell.execute_reply.started":"2021-12-04T01:38:31.92249Z","shell.execute_reply":"2021-12-04T01:38:35.838668Z"},"trusted":true},"execution_count":null,"outputs":[]}]}