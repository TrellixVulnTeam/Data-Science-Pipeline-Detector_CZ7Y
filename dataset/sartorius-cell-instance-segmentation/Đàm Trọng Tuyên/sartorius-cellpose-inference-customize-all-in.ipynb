{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Inference example with a Cellpose model: www.cellpose.org\nThe model is based on U-Net, however rather than training it directly on bitmask targets they first convert them to \"spatial flows\" representations and train on that. This makes segmentation of dense and touching cells more reliable. For details and additional tricks they use see the paper \"Cellpose: a generalist algorithm for cellular segmentation\".\n\nTo train it I used the script provided in the cellpose repo ie: `python -m cellpose --train ...` after I converted the dataset to the input format it expects.\n\nIn inference I just submit the masks as they were returned from the model - no postprocessing","metadata":{}},{"cell_type":"code","source":"!pip install --no-index ../input/cellpose-install/cellpose-0.7.2-py3-none-any.whl --find-links=../input/cellpose-install","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-28T16:47:33.125098Z","iopub.execute_input":"2021-12-28T16:47:33.12541Z","iopub.status.idle":"2021-12-28T16:47:48.865203Z","shell.execute_reply.started":"2021-12-28T16:47:33.12538Z","shell.execute_reply":"2021-12-28T16:47:48.864174Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### I'm running it in a separate process rather than as a regular notebook because I've faced issues with numpy version not updating after the `pip install` above. If you know how to fix this please let me know.","metadata":{}},{"cell_type":"code","source":"# !cp -r ../input/cellpose-code ./\n# %cd ./cellpose-code","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /content\n!cp '/kaggle/input/sartorius-cellpose-model/pseudo_3k_26_50_cellpose_residual_on_style_on_concatenation_off_train_2021_12_18_14_33_07.383445' /content","metadata":{"execution":{"iopub.status.busy":"2021-12-28T16:55:38.678984Z","iopub.execute_input":"2021-12-28T16:55:38.679348Z","iopub.status.idle":"2021-12-28T16:55:40.202656Z","shell.execute_reply.started":"2021-12-28T16:55:38.679315Z","shell.execute_reply":"2021-12-28T16:55:40.201406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile run.py\nimport torch\nimport numpy as np\nfrom cellpose import models, io, plot\nfrom pathlib import Path\nimport pandas as pd\n\ndef rle_encode(img):\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ntest_dir = Path('/kaggle/input/sartorius-cell-instance-segmentation/test')\ntest_files = [fname for fname in test_dir.iterdir()]\nmodel =  models.CellposeModel(gpu=True,\\\n                            pretrained_model=['/kaggle/input/sartorius-cellpose-final/fold0_cellpose_2021_12_29_04_58_56.251435_epoch_19',\n                                              '/kaggle/input/sartorius-cellpose-final/fold1_cellpose_2021_12_29_09_09_40.859558_epoch_19',\n                                              '/kaggle/input/sartorius-cellpose-final/fold2_cellpose_2021_12_29_09_22_20.324645_epoch_19',\n                                              '/kaggle/input/sartorius-cellpose-final/fold3_cellpose_2021_12_29_09_35_14.250530_epoch_19',\n                                              '/kaggle/input/sartorius-cellpose-final/fold4_cellpose_2021_12_29_09_48_19.213322_epoch_19',\n                                             ])\nsize_model = torch.load('/kaggle/input/sartorius-cellpose-model/pseudo_size_model.pth')\nids, masks = [],[]\nfor fn in test_files:\n    test_image = io.imread(str(fn))\n    predicted_diams, diams_style = size_model.eval(test_image, channels=[0,0])\n    preds, flows, _ = model.eval(test_image, diameter=predicted_diams, channels=[0,0],\\\n                                 augment=True, resample=True,)\n#                                  diam_threshold=12, min_size=14,\\\n#                                  flow_threshold=0.4, mask_threshold=0.1)\n    for i in range (1, preds.max() + 1):\n        ids.append(fn.stem)\n        masks.append(rle_encode(preds == i))\n        \npd.DataFrame({'id':ids, 'predicted':masks}).to_csv('/kaggle/working/submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-28T16:55:49.669934Z","iopub.execute_input":"2021-12-28T16:55:49.670675Z","iopub.status.idle":"2021-12-28T16:55:49.679339Z","shell.execute_reply.started":"2021-12-28T16:55:49.670637Z","shell.execute_reply":"2021-12-28T16:55:49.678264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python run.py","metadata":{"execution":{"iopub.status.busy":"2021-12-28T16:55:50.317723Z","iopub.execute_input":"2021-12-28T16:55:50.318373Z","iopub.status.idle":"2021-12-28T16:56:30.534234Z","shell.execute_reply.started":"2021-12-28T16:55:50.318337Z","shell.execute_reply":"2021-12-28T16:56:30.533008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %cd ..\n# !rm -r cellpose-code","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.read_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-28T16:56:42.331584Z","iopub.execute_input":"2021-12-28T16:56:42.332105Z","iopub.status.idle":"2021-12-28T16:56:42.393662Z","shell.execute_reply.started":"2021-12-28T16:56:42.332053Z","shell.execute_reply":"2021-12-28T16:56:42.39255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}