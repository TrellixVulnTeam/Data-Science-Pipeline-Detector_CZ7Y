{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Sartorius - Cell Type Image Classification - Tensorflow Approch**\n\n# Summary\n\n- This notebook should adress the task to classify the cell type astro, cort or shyshy in order to be able\n  to train a own segmentation network for every singly cell type - which could lead to better results, not has to.\n  \n- The architecture is a resnet (50) with a small number of additional standard layers. It is all done via TF/Keras.\n\n- The images used for this are the images from \\train and \\train_semi_supervised. These roughly 2500 images with a shape of (520, 704) \n  are then split up to tiles with a shape of (128,128) (the small remaining parts of the oiginal images were discarded),\n  the images were then resized and also extended to rgb (via generator) to (224,224)\n  in order to get the best results for the resnet (which was aligned with this image size). \n  Using no tiles with a resizing to also (224,224) brought me also to a high validation accuracy but first in the end of the training - after 30 epochs\n  beginnig to constantly rising to 0.95, but before that there was a plateau at about 0.3 (random guessing with 3 classes).\n  With he tiles approch I got a fast and more stable training -> see k-fold cross validation below with a 0.99 validation accracy score.\n  The k fold validation in this case uses stratifiied sampling in order to avoid overfitting by \n  spreading the tiles of one image to the train or validation set not both.\n  \n- Shoutout to Yoshi_K and his notebook (https://www.kaggle.com/yoshikuwano/classified-by-cell-types-before-segmentation-1-2),\n  who achieved similar results with his Pytorch approach, and thanks for the communication for this problem.\n  I had to change my setting so I could not reproduce his results with the same topology, so I had to use a much much smaller learning rate,\n  with higher ones I got a very useless fluctuating accuracies from 0.3 to 0.95.\n  \n- In this notebook the functions needed are provided first, then in the Training section, there are the function called in order to\n  get the trainig loop done. In the ouput of the notebook the final weights are stored.\n  \n- I would be very pleased if the notebook is at least a bit useful for some people and even its only the k-fold plot\n  and i would be very very thankful for any discussion, questions, tips, mistakes other approaches and so on to get better at deep learning.\n  \n  Much Love from Germany, Andreas!\n\n\n","metadata":{"id":"N-5T_k0zX3RO"}},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"markdown","source":"## *Global Variables*","metadata":{"id":"Jii5LlfR4HN3"}},{"cell_type":"code","source":"batch_size    = 32\nimg_len_x     = 224\nimg_len_y     = 224 \ninput_shape = (img_len_x,img_len_y,1)\ntarget_size = input_shape[0:2]\nnum_classes = 3","metadata":{"id":"sJYiPAVF4N_i","execution":{"iopub.status.busy":"2021-11-28T13:24:48.225429Z","iopub.execute_input":"2021-11-28T13:24:48.22575Z","iopub.status.idle":"2021-11-28T13:24:48.243419Z","shell.execute_reply.started":"2021-11-28T13:24:48.225668Z","shell.execute_reply":"2021-11-28T13:24:48.24266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *Imports*","metadata":{"id":"IzJLrwuXucKZ"}},{"cell_type":"code","source":"### Import packages ###\n\n#  math operations\nimport numpy as np\nimport pandas as pd\nimport random\n\n# image processing\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport cv2\nfrom PIL import Image\n\n# deep learning frameworks\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.densenet import DenseNet121\nfrom tensorflow.keras.layers import Dense, AveragePooling2D, MaxPooling2D, BatchNormalization, Flatten, Input, Dense, Conv2D, Dropout, ZeroPadding2D, Convolution2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import SGD\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nimport gc\n\n# splitting data frame\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\n\n# path management\nimport os\nimport shutil\n\n# display otitions\npd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', -1)\n\n# measuring time\nimport time\nfrom tqdm import tqdm","metadata":{"id":"qqykMstiuhK2","outputId":"ae1edcef-dc36-41cc-b8aa-eed2a88a0b30","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T13:24:48.246224Z","iopub.execute_input":"2021-11-28T13:24:48.246799Z","iopub.status.idle":"2021-11-28T13:24:53.621327Z","shell.execute_reply.started":"2021-11-28T13:24:48.246769Z","shell.execute_reply":"2021-11-28T13:24:53.620549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *Helper Functions*","metadata":{"id":"XkxX0BGtX3RV"}},{"cell_type":"code","source":"### print(color.BOLD + color.RED + \"\" + color.END)\n\nclass color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'\n\n\n# use color shemes for k fold plots extend if k > 5 should be used\nlist_colors_red  = ['darkviolet','mediumpurple','violet','fuchsia','red']\nlist_colors_blue = ['lightgreen','palegreen','mediumspringgreen','seagreen','darkolivegreen']","metadata":{"id":"4cGgPYrAX3RX","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T13:24:53.622593Z","iopub.execute_input":"2021-11-28T13:24:53.622996Z","iopub.status.idle":"2021-11-28T13:24:53.628739Z","shell.execute_reply.started":"2021-11-28T13:24:53.622961Z","shell.execute_reply":"2021-11-28T13:24:53.628177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *Data Integration and Preprocessing*","metadata":{"id":"sp1ZwtoAyOkO"}},{"cell_type":"markdown","source":"- load train df","metadata":{}},{"cell_type":"code","source":"def load_train_csv(path = \"../input/sartorius-cell-instance-segmentation/train.csv\", print_df = True):\n \n  train_df = pd.read_csv(path)\n  \n  if print_df: \n    print(\"Csv: \", path ,\"\\n\")\n    print(f'Training Set Shape: {train_df.shape} - {train_df[\"id\"].nunique()} Images \\n')\n    print(\"Head of df:\")\n    print(train_df.head(10))\n    print(\"\")\n    print(\"different cell types:\")\n    print(train_df[\"cell_type\"].drop_duplicates().reset_index(drop=True),\"\\n\")\n\n  else:\n    return train_df  \n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T13:24:53.630574Z","iopub.execute_input":"2021-11-28T13:24:53.630947Z","iopub.status.idle":"2021-11-28T13:24:53.653031Z","shell.execute_reply.started":"2021-11-28T13:24:53.630917Z","shell.execute_reply":"2021-11-28T13:24:53.652095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- extract .csv from train semi supervised images ","metadata":{}},{"cell_type":"code","source":"\ndef get_train_semi_supervised_csv(path = \"../input/sartorius-cell-instance-segmentation/train_semi_supervised\", print_df = True, store_on_drive = False):\n\n\n  # source of images\n  train_semi_supervised_images = os.listdir(\"../input/sartorius-cell-instance-segmentation/train_semi_supervised\")\n \n  # get emtpy dataset from train.csv and emtpy it\n  train_df = load_train_csv(print_df = False)\n  train_df = train_df[0:0]\n\n  # function to get information for every file\n  def get_cols_from_filename(filename):\n      \n      image_id = filename.split('.')[0]\n      cell_type = filename.split('[')[0]\n      filename_split = filename.split('_')\n      plate_time = filename_split[-3]\n      sample_date = filename_split[-4]\n      sample_id = '_'.join(filename_split[:3]) + '_' + '_'.join(filename_split[-2:]).split('.')[0]\n      \n      return image_id, cell_type, plate_time, sample_date, sample_id\n\n  # getting information of images\n  for filename in train_semi_supervised_images:\n\n      image_id, cell_type, plate_time, sample_date, sample_id = get_cols_from_filename(filename)\n      sample = {\n          'id': image_id,\n          'annotation': np.nan,\n          'width': 704,\n          'height': 520,\n          'cell_type': cell_type,\n          'plate_time': plate_time,\n          'sample_date': sample_date,\n          'sample_id': sample_id\n      }\n      train_df = train_df.append(sample, ignore_index=True)\n    \n  train_df['cell_type'] = train_df['cell_type'].str.rstrip('s')\n\n  train_df.to_csv('./train_semi_supervised.csv', index= False)\n\n  if store_on_drive == True:\n    train_df.to_csv('./train_semi_supervised.csv', index= False)\n\n  if print_df:\n    print(f'Training Set Shape: {train_df.shape} - {train_df[\"id\"].nunique()} Images \\n')\n    print(\"Head of df:\")\n    print(train_df.head(10))\n    print(\"\")\n    print(\"different cell types:\")\n    print(train_df[\"cell_type\"].drop_duplicates().reset_index(drop=True))\n\n  else:\n    return train_df\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T13:24:53.65455Z","iopub.execute_input":"2021-11-28T13:24:53.65482Z","iopub.status.idle":"2021-11-28T13:24:53.668702Z","shell.execute_reply.started":"2021-11-28T13:24:53.654784Z","shell.execute_reply":"2021-11-28T13:24:53.668026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- load train.csv and the just created rain_semi_supervised.csv to get full dataframe","metadata":{}},{"cell_type":"code","source":"def load_df(return_df = True): \n  \n  # image path:\n  path_train = \"../input/sartorius-cell-instance-segmentation/train/\"\n  path_train_semi_supervised = \"../input/sartorius-cell-instance-segmentation/train_semi_supervised\"\n\n\n  # train.csv -> get only one row per images, shorten because of annotations\n  df_1 = pd.read_csv(\"../input/sartorius-cell-instance-segmentation/train.csv\")\n  df_1 = df_1.drop(columns=['annotation'])\n  df_1 = df_1.drop_duplicates().reset_index(drop=True)\n  df_1[\"csv\"] = \"train.csv\"\n  df_1[\"image_path\"] = df_1[\"id\"].apply(lambda x: path_train + \"/\" + x + \".png\")\n  print(\"\\nlength of train:\", len(df_1))\n\n   \n  # train<-semi_supervied.csv -> already only one row per images\n  df_2 = pd.read_csv(\"./train_semi_supervised.csv\")\n  df_2 = df_2.drop(columns=['annotation'])\n  df_2[\"csv\"] = \"train_semi_supervised.csv\"\n  df_2[\"image_path\"] = df_2[\"id\"].apply(lambda x: path_train_semi_supervised + \"/\" + x + \".png\")\n  print(\"\\nlength of train semi supervised:\", len(df_2))\n\n  # combine both\n  df_train = pd.concat([df_1, df_2])\n  df_train = df_train.reset_index(drop=True)\n\n  # drop unneccesary cols\n  df_train = df_train.drop(columns=[\"sample_date\", \"height\", \"width\", \"sample_id\", \"elapsed_timedelta\", \"plate_time\"])\n\n  # checks\n  print(\"\\nlength of train df:\", len(df_train),\"\\n\")\n  print(color.BOLD  + \"glimpse at df:\" + color.END)\n  print(df_train.head(3))\n  print(df_train.tail(3))\n  print(\"\\nDistribution of cell types:\")\n  print(df_train['cell_type'].value_counts())\n\n  if return_df == True:\n    return df_train\n  ","metadata":{"id":"iUCVY7PgX3Ra","execution":{"iopub.status.busy":"2021-11-28T13:24:53.670726Z","iopub.execute_input":"2021-11-28T13:24:53.670949Z","iopub.status.idle":"2021-11-28T13:24:53.682328Z","shell.execute_reply.started":"2021-11-28T13:24:53.670918Z","shell.execute_reply":"2021-11-28T13:24:53.680806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Collection of Preprocess Data Funtions","metadata":{"id":"enWYN0l5yjTn"}},{"cell_type":"code","source":"###  several functions for image preprocessing\n\n\ndef image_to_array(image_path, rgb = 0):\n  'rgb = 0: reads as grayscale'\n  'rgb = 1: reads as rgb'\n  image = cv2.imread(image_path, rgb)\n  return image\n\n\ndef store_as_png(image, save_path):\n    img_save_arr = Image.fromarray(image) \n    img_save_arr.save(save_path)\n\n\ndef cropp_image(image, target_size=target_size):\n  image = image[0:target_size[0], 0:target_size[1]]\n  return image\n\n\ndef contrast_enhancing_right(image, quantil = 0.95):\n    'shifting all to the left quantil value'\n\n    quantil = np.quantile(image, quantil)\n    image_c = (image / quantil * 1.0).astype(np.float32)\n    image_c[image_c > 1.0] = 1.0\n    #image_c = image_c[..., np.newaxis]\n    \n    return image_c\n\n \ndef contrast_enhancing_left_and_right(image, quantil = 0.975):\n    'moving to the left and then'\n    'shifting all to the left quantil value'\n\n    quantil_left = np.quantile(image[image>0.], 1 - quantil)\n    image_c = image - quantil_left\n    image_c[image_c < 0.] = 0.\n\n    quantil_right = np.quantile(image_c, quantil)\n    image_c = (image_c / quantil_right * 1.0).astype(np.float32)\n    image_c[image_c > 1.0] = 1.0\n\n    return image_c\n        \n    \ndef cv2_hist_eq(image):\n    'using pre biuld cv2 preset'\n    image = (image * 255).astype(np.uint8)\n\n    image_c = cv2.equalizeHist(image)\n    image_c = (image_c / 255.).astype(np.float32)\n    image_c[image_c > 1.0] = 1.0\n    \n    image_c = image_c[..., np.newaxis]\n\n    return image_c","metadata":{"id":"PYek33vuzfoE","execution":{"iopub.status.busy":"2021-11-28T13:24:53.68337Z","iopub.execute_input":"2021-11-28T13:24:53.683922Z","iopub.status.idle":"2021-11-28T13:24:53.696107Z","shell.execute_reply.started":"2021-11-28T13:24:53.683739Z","shell.execute_reply":"2021-11-28T13:24:53.69543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- making tiles and visualize check ( if tiling is True, tiles are created by the given \"len_tile\", no further resizing (generator does it by itsself),\n  if tiling is false, the images are just loaded and resized to the given target size, see \"Global Variables\"-section above.\n  All files are then stored in the output folder.","metadata":{}},{"cell_type":"code","source":"# preprocess data and store it as image png files in extra folder /content/colab/processed_data\n# get new train df\n\ndef making_tiles(df, path = './', len_tile = 64, tiling = True):\n    \n    \n  # getting paths for the images in train folder\n  image_paths = list(df['image_path'])\n  ids = list(df['id'])\n  #print(image_paths[0:3])\n      \n  # list of images loaded as array\n  images_array = [image_to_array(image_path) for image_path in image_paths]\n  print(\"image shape: \" , images_array[0].shape)\n \n  save_paths = []\n  ids_processed = []\n  shape_x = 520 # x is axis=0 (vertical, down to bottom)\n  shape_y = 704 # y is axis=1 (horizontal, left to right)\n\n  n_tiles = (shape_x // len_tile) * ((shape_y // len_tile))\n  n_tiles_x = (shape_x // len_tile)\n  n_tiles_y = ((shape_y // len_tile))\n\n  if tiling == True:\n    print(\"Number of tiles: \", n_tiles, \" - tile shape: \", \"(\",len_tile,\",\",len_tile,\")\", \"- sum of pixels: \", (len_tile * len_tile* n_tiles) ,  \" - rest pix : \", shape_x*shape_y - len_tile * len_tile * n_tiles)\n    print(\"loss in %: \", round((shape_x*shape_y - len_tile * len_tile * n_tiles)/(shape_x*shape_y),2)*100,\"%\")\n          \n    # store tiles of images\n    print(\"\\n\")\n    for i in range(len(image_paths)): #images\n      \n      img = images_array[i]\n      img = img[0:shape_x, 0:shape_y]\n      \n      id = ids[i]\n      count_tiles_per_id = 0\n      \n      for j in range(0, n_tiles_x): \n        for k in range(0, n_tiles_y):\n\n          count_tiles_per_id += 1\n          save_path = path + id + \"_\" + str(count_tiles_per_id).zfill(2) + str(\".png\")\n          save_paths.append(save_path)\n          ids_processed.append(id)\n\n          img_tile =  img[ (j * len_tile) : ((j+1) * len_tile), (k * len_tile) : ((k+1) * len_tile)]\n          store_as_png(img_tile, save_path)\n\n\n  if tiling == False:\n    print(\"No tiling, keep images and resize it to target size\")\n    for i in range(len(image_paths)): #images\n      \n      img = images_array[i]\n      img = cv2.resize(img, target_size)\n      \n      id = ids[i]\n    \n      save_path = path + id + str(\".png\")\n      save_paths.append(save_path)\n      ids_processed.append(id)\n      store_as_png(img, save_path)\n\n\n  # visualize example\n  if tiling == False:\n    id = 0\n    img_original = cv2.imread(save_paths[id], 0)\n    f, ax = plt.subplots(nrows = 1, ncols = 1, figsize=(12, 12))\n    ax.imshow(img_original, cmap=\"gray\")\n    ax.axis('off')\n    plt.title(f'img original - with resized shape: {img_original.shape}' , fontsize = 16)\n    plt.show()\n\n  if tiling == True:\n    id = 0\n    img_original = cv2.imread(image_paths[id], 0)\n    f, ax = plt.subplots(nrows = 1, ncols = 1, figsize=(12, 12))\n    ax.imshow(img_original, cmap=\"gray\")\n    ax.axis('off')\n    plt.title(f'img original - Shape: {img_original.shape} - and tiles: {img_tile .shape}' , fontsize = 16)\n    plt.show()\n\n    c = 0\n    f, ax = plt.subplots(nrows = n_tiles_x, ncols = n_tiles_y, figsize=(12, 8))\n    f.subplots_adjust(hspace=0.01)\n    for j in range(0, n_tiles_x): \n      for k in range(0, n_tiles_y):\n          c += 1\n          save_path = path + ids[id] + \"_\" + str(c).zfill(2) + str(\".png\")\n          tile = cv2.imread(save_path, 0)\n          ax[j][k].imshow(tile, cmap=\"gray\", vmin=0, vmax=255)\n          ax[j][k].axis('off')\n    plt.show()\n\n  # make df processed\n  frame = {'id':ids_processed, 'image_path': save_paths}\n  df_processed = pd.DataFrame(frame)\n  df_processed = df_processed.merge(df[[\"id\",\"cell_type\"]], how = 'left', left_on = \"id\", right_on = \"id\")\n\n  print(\"\\nlength of train df:\", len(df_processed),\"\\n\")\n  print(color.BOLD  + \"glimpse at df processed:\" + color.END)\n  print(df_processed.head(5))\n\n  # free ram\n  del images_array\n  del save_paths\n  del image_paths\n  gc.collect()\n\n  \n  return df_processed\n","metadata":{"id":"l8QQGLbOyxSQ","execution":{"iopub.status.busy":"2021-11-28T13:24:53.697272Z","iopub.execute_input":"2021-11-28T13:24:53.697825Z","iopub.status.idle":"2021-11-28T13:24:53.720665Z","shell.execute_reply.started":"2021-11-28T13:24:53.697791Z","shell.execute_reply":"2021-11-28T13:24:53.720049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Resampling train df to adress imbalanced data (\"DOWN\", \"UP\" or None)","metadata":{"id":"gNCpFrXmwPWd"}},{"cell_type":"code","source":"# RESAMPLING\n\ndef resampling(df_train, resampling_on = \"DOWN\", label = \"cell_type\"):\n\n\n  distribution_df = df_train[label].value_counts().rename_axis(label).reset_index(name='counts')\n  print('Distribution target without Resampling:\\n', distribution_df)\n\n  # new dataframe for resampling\n  df_res = df_train.copy()\n  counts = distribution_df['counts']\n  y      = df_res[label] \n\n\n  if resampling_on == 'DOWN':\n\n    min_counts = min(counts)\n\n    for ind in range(0,num_classes):\n\n      if (((counts.iloc[ind]) - min_counts)/(counts.iloc[ind]))>0.01:\n        class_i = distribution_df[label].iloc[ind]\n        list_indices_i   = list(df_res[y == class_i].index)\n        list_indices_i_sample = random.sample(list_indices_i,counts[ind]-int(min_counts*1.001))\n        list_indices_i_sample = list(list_indices_i_sample)  \n        list_indices_i_sample = sorted(list_indices_i_sample)\n        df_res = df_res.drop(list_indices_i_sample, axis = 0)\n\n    df_res = df_res.reset_index(drop=True)\n        \n\n  if resampling_on == 'UP':\n\n    max_counts = max(counts)\n\n    for ind in range(0,num_classes):\n\n      if ((max_counts - counts.iloc[ind])/(max_counts))>0.05:\n        class_i = distribution_df[label].iloc[ind]          \n        list_indices_i   = list(df_res[y == class_i].index)\n        list_indices_i_sample = random.choices(list_indices_i, weights = None, k = (max_counts-counts[ind]))\n        list_indices_i_sample = list(list_indices_i_sample)\n        df_res_up = df_res.iloc[list_indices_i_sample]\n        df_res = df_res.append(df_res_up)\n                \n    df_res = df_res.sample(frac=1).reset_index(drop=True)       \n\n\n  print('\\n\\nResults of Resampling: --', resampling_on, '---\\n')\n  distribution_df_res = df_res[label].value_counts().rename_axis(label).reset_index(name='counts')     \n  print('Distribution target ressampled:\\n',distribution_df_res , '\\n')\n  print('original data:  ',df_train.shape[0],'\\nresampled data: ',  df_res.shape[0])  \n                        \n       \n  return df_res            \n                ","metadata":{"id":"ETOyEj3vwTvy","execution":{"iopub.status.busy":"2021-11-28T13:24:53.721746Z","iopub.execute_input":"2021-11-28T13:24:53.722335Z","iopub.status.idle":"2021-11-28T13:24:53.736627Z","shell.execute_reply.started":"2021-11-28T13:24:53.722295Z","shell.execute_reply":"2021-11-28T13:24:53.735985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *Training*","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"- get single fold (for quick overview over training performance and for final training on whole data set\")","metadata":{}},{"cell_type":"code","source":"### train and validation set split with train df\n\ndef single_fold(train_df, label = None, test_size = 0.3):\n\n  print(\"Glimpse on train df before shuffleing\")\n  print(train_df.head())\n  print(\"\")\n  print(\"Glimpse on train df after shuffleing\")\n  train_df = train_df.sample(frac=1)\n  print(train_df.head())\n  print(\"\\n\")\n    \n  # parameters\n  random_state = 1\n  test_size    = test_size\n\n  X = train_df.drop(label, axis = 1)\n  y = train_df[label]       \n        \n  X_train, X_val, y_train, y_val = train_test_split(      X, \n                                                          y, \n                                                          shuffle = True,\n                                                          test_size = test_size, \n                                                          random_state = random_state)\n        \n  X_train = X_train.reset_index(drop=True)\n  X_val   = X_val.reset_index(drop=True)\n  y_train = y_train.reset_index(drop=True)\n  y_val   = y_val.reset_index(drop=True)\n        \n        \n    \n   \n  # glimpse at train and validation sets\n  print(\"\\n\")\n  print(color.BOLD + \"Glimpse on splitted data:\" + color.END)\n  print(color.BOLD + \"X_train  head and tail:\"  + color.END)\n  print(X_train.head())\n  print(X_train.tail())\n  print(\"\")\n  print(color.BOLD + \"y_train head and tail: \" + color.END)\n  print(y_train.head())\n  print(y_train.tail())\n   \n  # check some specs\n  print(\"\\n\")\n  print(\"\\n\")\n  print(\"length of train df: \", len(train_df))\n  print(\"\")\n  print(\"length of X train:  \", len(X_train))\n  print(\"length of X val  :  \", len(X_val))\n  print(\"\")\n  print(\"length of X train and val  :  \", len(X_train)+len(X_val))\n  print(\"length of y train:  \", len(y_train))\n  print(\"length of y val:    \", len(X_val))\n  print(\"length of y train and val  :  \", len(y_train)+len(y_val))\n        \n    \n    # concat for data generator\n  X_y_train = pd.concat([X_train, y_train], axis = 1)\n  X_y_val   = pd.concat([X_val,   y_val],  axis = 1) \n               \n  print(\"\")\n  print(color.BOLD + \"X_y_train head and tail: \"  + color.END)\n  print(X_y_train.head())\n  print(X_y_train.tail())\n        \n \n  return X_y_train, X_y_val    \n","metadata":{"id":"bbQZwG7s4uRY","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T13:24:53.739648Z","iopub.execute_input":"2021-11-28T13:24:53.740066Z","iopub.status.idle":"2021-11-28T13:24:53.753425Z","shell.execute_reply.started":"2021-11-28T13:24:53.740032Z","shell.execute_reply":"2021-11-28T13:24:53.75262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- k fold stratified","metadata":{"id":"UwApa8AoX3Rd"}},{"cell_type":"code","source":"### train and validation set split with train df and k fold split\n\ndef get_k_fold_stratified(df,  k = 5, strat = None):\n\n    'get k stratified folds of df along strat stored in dictionary X_y_dict: X per k_fold_i'\n    \n    print(\"\")\n    print(\"Stratified after column: \", strat)\n    \n    # reducing df to strat\n    df_strat = df[[strat]]\n    df_strat = df_strat.drop_duplicates()\n    df_strat = df_strat.reset_index(drop=True)\n    \n    #shuffeling df\n    print(\"Glimpse on df_strat before shuffleing\")\n    print(df_strat.head())\n    print(\"\")\n    print(\"Glimpse on train df_strat after shuffleing\")\n    df_strat = df_strat.sample(frac=1)\n    print(df_strat.head())\n    print(\"\\n\")\n    print(\"Glimpse on train df_strat after shuffleing and resetting index\")\n    df_strat =  df_strat.reset_index(drop=True)\n    print(df_strat.head())\n    print(\"\\n\")\n    \n    # parameters\n    len_df = len(df)\n    len_df_strat = len(df_strat)\n    len_k_fold = int(len_df_strat // k) \n    \n    if len_k_fold == 0:\n        raise Exception(\"k higher than len df_strat!\")\n    \n    # creating folds\n    X_y_dict = {}\n    \n    for i in range(k):\n        key_dict = \"k_fold_\" + str(i+1)\n        \n        # list of indices last one gets all data not to fotget the last line in case if uneven len df\n        range_low_i = i * len_k_fold\n        range_up_i  = ((i + 1) * len_k_fold) if i < (k-1) else len_df_strat\n        list_indices_i = list(range(range_low_i, range_up_i))\n        \n        X_strat = df_strat.iloc[list_indices_i]\n        \n        # merge other colums to X_strat\n        X_merged = X_strat.merge(df, how = 'left', left_on = strat, right_on = strat)\n        \n        X_y_dict[key_dict] = X_merged\n            \n        print(key_dict, \"length of fold_strat: \",  len(X_strat),\"/\", round(len(X_strat)/(len_df_strat)*100,0),\"%\")\n        print(key_dict, \"length of fold_merged: \", len(X_merged),\"/\", round(len(X_merged)/(len_df)*100,0),\"%\")\n        print(\"\")\n        \n    # check if length of folds are the same \n    len_check = 0\n    for i in range(k):\n        len_check = len(X_y_dict[\"k_fold_\" + str(i+1)]) + len_check\n    if len_check != len_df:\n        raise Exception(\"Error in splitting!\")\n        \n    print(\"========================================\")\n    print(\"Length of df_merged: \", len_df)\n    print(\"Length of folds: \",     len_check)\n    \n    return X_y_dict\n","metadata":{"id":"KMG3-gdOX3Rd","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T13:24:53.754852Z","iopub.execute_input":"2021-11-28T13:24:53.755566Z","iopub.status.idle":"2021-11-28T13:24:53.770006Z","shell.execute_reply.started":"2021-11-28T13:24:53.75553Z","shell.execute_reply":"2021-11-28T13:24:53.769368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- unstratified fold","metadata":{"id":"OEw-6_lS2zxZ"}},{"cell_type":"code","source":"### train and validation set split with train df and k fold split\n\ndef get_k_fold_unstratified(df,  k = 5):\n\n    'get k unstratified folds of df stored in dictionary X_y_dict: X per k_fold_i'\n    \n    print(\"\")\n    print(color.BOLD +  \"Get k Folds\" + color.END + \"\\n\")\n    \n    #shuffeling df\n    #print(\"Glimpse on df before shuffleing\")\n    #print(df.head())\n    #print(\"\")\n    #print(\"Glimpse on f after shuffleing\")\n    df = df.sample(frac=1)\n    #print(df.head())\n    #print(\"\\n\")\n    print(\"Glimpse on df after shuffleing and resetting index\")\n    df =  df.reset_index(drop=True)\n    print(df.head())\n    print(\"\\n\")\n    \n    # parameters\n    len_df = len(df)\n    len_k_fold = int(len_df // k) \n    \n    if len_k_fold == 0:\n        raise Exception(\"k higher than len df!\")\n    \n    # creating folds\n    X_y_dict = {}\n    \n    print(\"Results k folding:\")\n    \n    for i in range(k):\n        key_dict = \"k_fold_\" + str(i+1)\n        \n        # list of indices last one gets all data not to fotget the last line in case if uneven len df\n        range_low_i = i * len_k_fold\n        range_up_i  = ((i + 1) * len_k_fold) if i < (k-1) else len_df\n        list_indices_i = list(range(range_low_i, range_up_i))\n        \n        X = df.iloc[list_indices_i]\n        X_y_dict[key_dict] = X \n            \n        print(key_dict, \": length of fold: \", len(X),\"/\", round(len(X)/(len_df)*100,0),\"%\")\n        print(\"\")\n \n    # check if length of folds are the same \n    len_check = 0\n    for i in range(k):\n        len_check = len(X_y_dict[\"k_fold_\" + str(i+1)]) + len_check\n    if len_check != len_df:\n        raise Exception(\"Error in splitting!\")\n        \n    print(\"========================================\")\n    print(\"Length of df: \", len_df)\n    print(\"Length of folds: \", len_check)\n    \n    return X_y_dict    \n","metadata":{"id":"h9L1CJf5X3Re","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T13:24:53.772961Z","iopub.execute_input":"2021-11-28T13:24:53.773215Z","iopub.status.idle":"2021-11-28T13:24:53.784473Z","shell.execute_reply.started":"2021-11-28T13:24:53.773129Z","shell.execute_reply":"2021-11-28T13:24:53.783652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Image Data Generator for train and validation (for single fold training)","metadata":{"id":"xMQHdF_XX3Rg"}},{"cell_type":"code","source":"### get image data geneator\n\ndef get_data_generator_train(X_y_train, X_y_val, rescale = 1./255., label = None,  color_mode = None,\n                             preprocessing_function = None, shuffle = True,  batch_size = batch_size,\n                             samplewise_center = False, samplewise_std_normalization = False,  horizontal_flip = False, vertical_flip = False, target_size = target_size):\n        \n    ###### ImageDataGenerator\n   \n    train_datagen = ImageDataGenerator(rescale = rescale,\n                                       rotation_range = 0,\n                                       width_shift_range = 0.,\n                                       height_shift_range = 0.,\n                                       shear_range = 0.,\n                                       zoom_range = 0., \n                                       fill_mode = 'nearest',\n                                       samplewise_center = samplewise_center,\n                                       samplewise_std_normalization = samplewise_std_normalization ,\n                                       horizontal_flip = horizontal_flip,\n                                       vertical_flip = vertical_flip,\n                                       preprocessing_function = preprocessing_function,\n                                       dtype= 'float32')\n\n    valid_datagen = ImageDataGenerator(rescale = rescale,\n                                       samplewise_center = False,\n                                       samplewise_std_normalization = False,\n                                       preprocessing_function = preprocessing_function,\n                                       dtype= 'float32')\n    \n    \n    ##### flow from Dataframe\n    \n    train_generator = train_datagen.flow_from_dataframe(\n                                        X_y_train,  \n                                        x_col = 'image_path',\n                                        y_col =  label,\n                                        target_size = target_size, # would be resized to\n                                        color_mode =  color_mode,\n                                        batch_size  = batch_size,\n                                        shuffle = shuffle,\n                                        class_mode = \"categorical\")\n\n\n    validation_generator = valid_datagen.flow_from_dataframe(\n                                        X_y_val,  \n                                        x_col = 'image_path',\n                                        y_col = label,\n                                        target_size = target_size, # would be resized to\n                                        color_mode =  color_mode,\n                                        batch_size = batch_size,\n                                        shuffle = shuffle,\n                                        class_mode = \"categorical\")\n    \n    return  train_generator, validation_generator\n","metadata":{"id":"j697wK2zX3Rg","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T13:24:53.785729Z","iopub.execute_input":"2021-11-28T13:24:53.785991Z","iopub.status.idle":"2021-11-28T13:24:53.798797Z","shell.execute_reply.started":"2021-11-28T13:24:53.785956Z","shell.execute_reply":"2021-11-28T13:24:53.79807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Class weights for imbalanced data (another option instead of resampling metho see above)","metadata":{"id":"iwSTPyjHL1vt"}},{"cell_type":"code","source":"# get class weights\ndef get_class_weights(train_datagen, X_y_train, label=\"cell_type\"):\n\n  class_weights={}\n\n  dict_generator = train_datagen.class_indices\n\n  print(\"class labels: \", dict_generator)\n\n  samples = list(X_y_train[label])\n  len_ = len(samples)\n \n  \n  for class_ in list(set(samples)):\n  \n    count = samples.count(class_) \n    weight = round(len_/count, 4)\n    class_num = dict_generator[class_]\n    class_weights[class_num] = weight \n  \n  print(\"class weights: \", class_weights)\n\n  return class_weights\n","metadata":{"id":"ouwHSFH6LyqF","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T13:24:53.801053Z","iopub.execute_input":"2021-11-28T13:24:53.80167Z","iopub.status.idle":"2021-11-28T13:24:53.808856Z","shell.execute_reply.started":"2021-11-28T13:24:53.801601Z","shell.execute_reply":"2021-11-28T13:24:53.808187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- get cnn model (own implementation)","metadata":{"id":"zZLsp7RdX3Rg"}},{"cell_type":"code","source":"### CNN model architecture\n\ndef define_model(input_shape, num_classes = num_classes, print_summary = True):\n      \n\n    # architecture\n    model = Sequential([\n                         Conv2D(8, (11, 11), padding = \"SAME\", input_shape=input_shape),\n                         BatchNormalization(),\n                         Activation('relu'),\n                         AveragePooling2D(pool_size=(2, 2), strides = (2,2)),\n                                          \n                         Conv2D(16, (5, 5), padding = \"SAME\"),\n                         BatchNormalization(),\n                         Activation('relu'),\n                         AveragePooling2D(pool_size=(2, 2), strides = (2,2)),\n \n                         Conv2D(32, (3, 3), padding = \"SAME\"),\n                         BatchNormalization(),\n                         Activation('relu'),\n                         AveragePooling2D(pool_size=(2, 2), strides = (2,2)),\n                        \n                         Conv2D(32, (3, 3), padding = \"SAME\"),\n                         BatchNormalization(),\n                         Activation('relu'),\n                         AveragePooling2D(pool_size=(2, 2), strides = (2,2)),\n                                                                   \n                         Conv2D(32, (3, 3), padding = \"SAME\"),\n                         BatchNormalization(),\n                         Activation('relu'),\n                         AveragePooling2D(pool_size=(3, 3), strides = (2,2)),\n        \n                         Conv2D(32, (3, 3), padding = \"SAME\"),\n                         BatchNormalization(),\n                         Activation('relu'),\n                         AveragePooling2D(pool_size=(3, 3), strides = (2,2)),\n           \n                                          \n                         Flatten(),\n                         Dense(16),\n                         Activation('relu'),\n                         Dropout(0.2),\n                         Dense(num_classes, activation = \"softmax\")\n                        ])\n\n    \n   \n    # summary\n    if print_summary == True:\n      model.summary()\n        \n    \n    return model\n","metadata":{"id":"9GgHoq0yX3Rg","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T13:24:53.810122Z","iopub.execute_input":"2021-11-28T13:24:53.810828Z","iopub.status.idle":"2021-11-28T13:24:53.82307Z","shell.execute_reply.started":"2021-11-28T13:24:53.81077Z","shell.execute_reply":"2021-11-28T13:24:53.822354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- use a pretrained model","metadata":{}},{"cell_type":"code","source":"### load keras pretrained model\n\ndef get_pretrained_model(target_size, num_classes, print_summary = True, trainable_last_layers = 0):\n\n\n  # load model\n  inputs = tf.keras.Input(shape=(target_size[0], target_size[1], 3))\n  resnet = tf.keras.applications.ResNet50(weights=\"imagenet\",include_top=False,input_tensor=inputs)\n\n  # freeze layers\n  for i,layer in enumerate(resnet.layers):\n\n    if (len(resnet.layers) - i) <= trainable_last_layers:\n      layer.trainable = True\n      \n    else:\n      layer.trainable = False   \n    \n  #for  i,layer in enumerate(resnet.layers):\n    #print(i, layer.name,\"-\", layer.trainable)\n      \n  # add specific new layers\n  model = tf.keras.models.Sequential()\n  model.add(resnet)\n  model.add(MaxPooling2D(pool_size=(3, 3)))\n  model.add(Flatten())\n  model.add(Dense(256, activation='relu'))\n  model.add(Dropout(0.25))\n  model.add(BatchNormalization())\n  model.add(Dense(64, activation='relu'))\n  model.add(Dropout(0.1))\n  model.add(BatchNormalization())\n  model.add(Dense(num_classes, activation='softmax'))\n\n  # summary\n  if print_summary == True:\n    model.summary()\n    \n  return model","metadata":{"id":"GG3qLQ23PXXj","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T13:24:53.824604Z","iopub.execute_input":"2021-11-28T13:24:53.825052Z","iopub.status.idle":"2021-11-28T13:24:53.835703Z","shell.execute_reply.started":"2021-11-28T13:24:53.825017Z","shell.execute_reply":"2021-11-28T13:24:53.834994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- compile model","metadata":{}},{"cell_type":"code","source":"def compile_model(model, optimizer = \"RMSprop\", lr = 0.001, print_opt = True):\n\n    # get optimizer\n    if optimizer == \"SGD\":\n      opt = tf.keras.optimizers.SGD(learning_rate = lr)\n    if optimizer == \"RMSprop\":\n      opt = tf.keras.optimizers.RMSprop(learning_rate  = lr)\n    if optimizer == \"Adam\":\n      opt = tf.keras.optimizers.Adam(learning_rate = lr)\n    \n        \n    model.compile(loss='categorical_crossentropy', optimizer = opt, metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"AUC\")])\n    \n    # summary\n    if print_opt == True:\n      print(\"Used optimizer : \", opt)\n      print(\"Learning rate from optimizer:\", K.eval(model.optimizer.lr), \"\\n\")     \n        \n    return model\n","metadata":{"id":"izDoFTk0weip","execution":{"iopub.status.busy":"2021-11-28T13:24:53.837169Z","iopub.execute_input":"2021-11-28T13:24:53.837511Z","iopub.status.idle":"2021-11-28T13:24:53.846986Z","shell.execute_reply.started":"2021-11-28T13:24:53.837477Z","shell.execute_reply":"2021-11-28T13:24:53.846288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Fit model with generator for single fold trainig","metadata":{"id":"OcgfKmpiX3Rh"}},{"cell_type":"code","source":"### full model: fit with generator\n\ndef fit_model_cnn(model, train_generator, validation_generator, n_epochs = 10, class_weights = None):\n\n            \n    #checkpoint \n    checkpoint_filepath = './sartorius_cell_type_classification.h5'\n    \n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n                                                     filepath = checkpoint_filepath,\n                                                     save_weights_only = True,\n                                                     monitor = 'val_accuracy',\n                                                     verbose = True,\n                                                     mode = 'max',\n                                                     save_best_only = True,\n                                                     save_freq = 'epoch')\n\n    # early stop\n    early_stop = EarlyStopping(monitor = 'val_loss', patience = 5, verbose = True)\n\n\n    CNN_history     =     model.fit(\n                                    train_generator, \n                                    steps_per_epoch = int(len(X_y_train)/batch_size) ,\n                                    validation_data =  validation_generator,\n                                    epochs = n_epochs,\n                                    class_weight = class_weights,\n                                    callbacks = [checkpoint]\n                                    )\n    \n    return CNN_history\n","metadata":{"id":"gVoHLtQVX3Rh","execution":{"iopub.status.busy":"2021-11-28T13:24:53.847927Z","iopub.execute_input":"2021-11-28T13:24:53.848512Z","iopub.status.idle":"2021-11-28T13:24:53.857478Z","shell.execute_reply.started":"2021-11-28T13:24:53.84848Z","shell.execute_reply":"2021-11-28T13:24:53.856873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- k fold training","metadata":{"id":"JqFMGp4PX3Rh"}},{"cell_type":"code","source":"### find best ratio range: for every single lower ratio lim analyse training behavior\n\ndef k_fold_training(df, n_epochs = 3, k = 5, batch_size = 32, stratify = \"Yes\", strat = \"id\"):\n\n    \n    # inits of lists for storing results of metrics and loss \n    k_fold_history = {}\n    \n    \n    # get X_y_dict with folds \n    if stratify == \"No\":\n        print(\"Unstratified Sampling\")\n        X_y_dict = get_k_fold_unstratified(df,  k)\n    else:\n        print(\"Stratified Sampling\")\n        X_y_dict = get_k_fold_stratified(df,  k, strat = strat)\n    \n    # traning loop\n    for j in range(k):\n        \n        print(\"\\n\")\n        print(color.BOLD + color.RED +  \"k fold = \" + str(j+1) + color.END  + \"\\n\")\n        print(\"============================================================\")\n        print(\"\\n\")\n        print(color.BOLD +  \"Get k Fold Concat train and validation sets for fold k = \" + str(j+1) + color.END + \"\\n\")\n        \n        k_fold_j = \"k_fold_\" + str(j+1) \n        \n        X_train  = pd.DataFrame()\n        X_val    = pd.DataFrame()\n \n        # get training set and validation set\n        print(\"Concatening single folds to train and validation\")\n        for k_fold_i, X_i in X_y_dict.items():\n            \n            if k_fold_i == k_fold_j:\n                X_val = X_i\n            else:\n                X_train = pd.concat([X_train, X_i])\n                        \n        print(\"\")\n        print(\"Data preparation for next fold for training finished!\")\n        print(k_fold_j,\"- training : len X_train:\", len(X_train), \"/\", round(len(X_train)/(len(X_train)+len(X_val))*100,0),\"%\" , \" ||  len X_val:\", len(X_val),\n                                    \"/\",round(len(X_val)/(len(X_train)+len(X_val))*100,0),\"%\", \" 1/k = \", round(1/k*100,0), \"%\")\n        print(\"\")\n        \n        # construct data generator\n        print(\"Generator Output:\")\n        train_generator, validation_generator = get_data_generator_train(\n                                                                          X_train,\n                                                                          X_val,\n                                                                          label = \"cell_type\",\n                                                                          color_mode = \"rgb\",\n                                                                          preprocessing_function = None,\n                                                                          shuffle = True,\n                                                                          batch_size = batch_size,\n                                                                          samplewise_center = False,\n                                                                          samplewise_std_normalization = False, \n                                                                          horizontal_flip = False,\n                                                                          vertical_flip = False,\n                                                                          target_size = target_size\n                                                                          )\n      \n\n        # model and training\n        print(\"\")\n        print(color.BOLD +  \"Training for fold k = \" + str(j+1)  + color.END + \"\\n\")\n        \n        \n        # reset and recompile model to avoid independencies on k - fold training\n        model = get_pretrained_model(target_size, num_classes, print_summary = True, trainable_last_layers = 1000)\n        model = compile_model(model = model, optimizer = \"Adam\" , lr = 1e-5)\n\n        history     =     model.fit(\n                                      train_generator, \n                                      steps_per_epoch = int(len(X_train)/batch_size) ,\n                                      validation_data =  validation_generator,\n                                      epochs = n_epochs,\n                                      class_weight = None\n                                      )\n        \n       \n        # storing results\n        k_fold_history[k_fold_j] = history.history \n        print(\"\\n\")\n    # end training loop        \n    \n    return k_fold_history \n","metadata":{"id":"7_HtZuAWX3Ri","execution":{"iopub.status.busy":"2021-11-28T13:24:53.858917Z","iopub.execute_input":"2021-11-28T13:24:53.859409Z","iopub.status.idle":"2021-11-28T13:24:53.876054Z","shell.execute_reply.started":"2021-11-28T13:24:53.859376Z","shell.execute_reply":"2021-11-28T13:24:53.875308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation / Plotting Functions","metadata":{"id":"2jeNUDr5FX0t"}},{"cell_type":"markdown","source":"- plot metrics for single fold","metadata":{"id":"j4A3GYYrX3Rl"}},{"cell_type":"code","source":"### loss, accuracy and auc per epoch and training and validation set \n\ndef show_metrics_single_fold(CNN_history):\n    \n    #accuracy\n    plt.plot(CNN_history.history['accuracy'], color = 'blue')\n    plt.plot(CNN_history.history['val_accuracy'], color = 'red')\n    plt.title('CNN: Accuracy vs. epochs')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Training', 'Validation'], loc='upper left')\n    plt.show()\n        \n    #loss\n    plt.plot(CNN_history.history['loss'], color='blue')\n    plt.plot(CNN_history.history['val_loss'], color='red')\n    plt.title('CNN: Loss vs. epochs')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Training', 'Validation'], loc='upper left')\n    plt.show()\n    \n    try:\n        #AUC\n        plt.plot(CNN_history.history['AUC'], color='blue')\n        plt.plot(CNN_history.history['val_AUC'], color='red')\n        plt.title('CNN: AUC vs. epochs')\n        plt.ylabel('AUC')\n        plt.xlabel('Epoch')\n        plt.legend(['Training', 'Validation'], loc='upper left')\n        plt.show()\n    except:\n        print(\"AUC not setted!\")\n    ","metadata":{"id":"6SNzoL-TX3Rm","execution":{"iopub.status.busy":"2021-11-28T13:24:53.878221Z","iopub.execute_input":"2021-11-28T13:24:53.878418Z","iopub.status.idle":"2021-11-28T13:24:53.890006Z","shell.execute_reply.started":"2021-11-28T13:24:53.878394Z","shell.execute_reply":"2021-11-28T13:24:53.889334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- plot metrics for k fold","metadata":{}},{"cell_type":"code","source":"### loss, accuracy and auc per epoch and training and validation set per ratio limit analysis\n\ndef show_metrics_k_fold(k_fold_history):\n\n    # inits\n    fold = \"k_fold_1\" # dummy\n    k    = len(k_fold_history.keys()) # number of folds\n    max_ = 1e6\n    n_epochs    = len(k_fold_history[\"k_fold_1\"][\"loss\"]) # dummy\n    \n    \n    # loop over metrics\n    for metric in list(k_fold_history[fold].keys()):\n        \n        train_min  = list(np.ones(n_epochs) * max_)\n        val_min    = list(np.ones(n_epochs) * max_)\n        train_max  = list(np.zeros(n_epochs))\n        val_max    = list(np.zeros(n_epochs))\n        \n        train_mean_array = np.zeros((k,n_epochs))\n        val_mean_array  = np.zeros((k,n_epochs))\n        \n        if metric.find(\"val\") < 0:\n \n            plt.figure(figsize=(12,8))\n            \n            # loop over folds\n            for j in range(k):\n                \n                k_fold = \"k_fold_\" + str(j+1)\n                \n                # find min max and mean for train and val along all folds\n                train_min = np.minimum(k_fold_history[k_fold][metric], train_min)\n                train_max = np.maximum(k_fold_history[k_fold][metric], train_max)\n                \n                val_min   = np.minimum(k_fold_history[k_fold][\"val_\" + metric], val_min)\n                val_max   = np.maximum(k_fold_history[k_fold][\"val_\" + metric], val_max)\n                \n                train_mean_array[j,:]    = k_fold_history[k_fold][metric]\n                val_mean_array[j,:]      = k_fold_history[k_fold][\"val_\" + metric]\n                \n                \n                # plot every metric per fold\n                plt.plot(k_fold_history[k_fold][metric],          color = 'red',  linestyle = ':') \n                plt.plot(k_fold_history[k_fold][\"val_\" + metric], color = 'deepskyblue', linestyle = ':')\n                \n            \n            # gettin mean value per matric for every epoch\n            train_mean = np.mean(train_mean_array, axis = 0)\n            val_mean   = np.mean(val_mean_array, axis = 0)\n                  \n            plt.plot(train_max,  color='crimson', label=\"train\") \n            plt.plot(train_mean, color='darkred', linewidth=5)  \n            plt.plot(train_min,  color='crimson')\n            \n            plt.plot(val_max,   color='dodgerblue', label = \"val\")\n            plt.plot(val_mean,  color='darkblue',  linewidth=5)\n            plt.plot(val_min,   color='dodgerblue')\n            \n            plt.fill_between(range(n_epochs), val_max,  val_min,    alpha=0.5, color='skyblue')\n            plt.fill_between(range(n_epochs), train_max, train_min, alpha=0.5, color='pink')\n            \n            plt.title(f'{metric}  vs. epochs')\n            plt.ylabel(f'{metric}')\n            plt.xlabel('epoch')\n            if metric == \"loss\":\n                plt.legend(loc='upper right')\n            else:\n                plt.legend(loc='upper left')\n            plt.show()\n            \n            # print additional results for last epoch:\n            print(\"\")\n            print(color.BOLD +  \"results for last epoch for\", metric, \":\" + color.END  )\n            print(\"train mean: \" , round(train_mean[-1],2), \" - train max: \",  round(train_max[-1],2),  \" - train min: \",  round(train_min[-1], 2))\n            print(\"val mean:   \" , round(val_mean[-1],2), \" - train max: \",  round(val_max[-1],2),  \" - train min: \",  round(val_min[-1], 2))\n            print(\"=====================================================================================================\")\n            print(\"\\n\")\n","metadata":{"id":"agkyh-9RX3Rm","execution":{"iopub.status.busy":"2021-11-28T13:24:53.891274Z","iopub.execute_input":"2021-11-28T13:24:53.891703Z","iopub.status.idle":"2021-11-28T13:24:53.911009Z","shell.execute_reply.started":"2021-11-28T13:24:53.891655Z","shell.execute_reply":"2021-11-28T13:24:53.910308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Process / Evaluation","metadata":{"id":"PMiK8FR-X3Rn"}},{"cell_type":"markdown","source":"- load data","metadata":{}},{"cell_type":"code","source":"get_train_semi_supervised_csv(path = \"../input/sartorius-cell-instance-segmentation/train_semi_supervised\", print_df = True, store_on_drive = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T13:24:53.912307Z","iopub.execute_input":"2021-11-28T13:24:53.913107Z","iopub.status.idle":"2021-11-28T13:25:08.067646Z","shell.execute_reply.started":"2021-11-28T13:24:53.913016Z","shell.execute_reply":"2021-11-28T13:25:08.066179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = load_df(return_df = True) ","metadata":{"id":"iWwklZ8SX3Rn","outputId":"b4e4ab69-d991-497f-bf91-db754a7ed5c2","execution":{"iopub.status.busy":"2021-11-28T13:25:08.068895Z","iopub.execute_input":"2021-11-28T13:25:08.070308Z","iopub.status.idle":"2021-11-28T13:25:08.413781Z","shell.execute_reply.started":"2021-11-28T13:25:08.070269Z","shell.execute_reply":"2021-11-28T13:25:08.413073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- get tiles","metadata":{}},{"cell_type":"code","source":"train_df = making_tiles(train_df, path = './', len_tile = 128, tiling = True)","metadata":{"id":"UV31Tojr9sAo","outputId":"73da649d-62f6-43fc-89c5-a2c55d8a5a41","execution":{"iopub.status.busy":"2021-11-28T13:25:08.415231Z","iopub.execute_input":"2021-11-28T13:25:08.415484Z","iopub.status.idle":"2021-11-28T13:25:22.462339Z","shell.execute_reply.started":"2021-11-28T13:25:08.415451Z","shell.execute_reply":"2021-11-28T13:25:22.460931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Downsampling","metadata":{}},{"cell_type":"code","source":"train_df_res = resampling(train_df, resampling_on = \"DOWN\" , label = \"cell_type\")","metadata":{"id":"EK9DZC8PNMT2","outputId":"5e03b608-e759-407d-b7f8-dc6184319a61","execution":{"iopub.status.busy":"2021-11-28T13:25:22.463564Z","iopub.status.idle":"2021-11-28T13:25:22.464205Z","shell.execute_reply.started":"2021-11-28T13:25:22.46395Z","shell.execute_reply":"2021-11-28T13:25:22.463973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- get model and train k fold with stratified sampling (along image id)","metadata":{}},{"cell_type":"code","source":"k_fold_history = k_fold_training(df = train_df_res, n_epochs = 7, k = 5, batch_size = batch_size, stratify = \"Yes\", strat = \"id\")","metadata":{"id":"4DrzhEz9X3Rq","execution":{"iopub.status.busy":"2021-11-28T13:25:22.465679Z","iopub.status.idle":"2021-11-28T13:25:22.466313Z","shell.execute_reply.started":"2021-11-28T13:25:22.466061Z","shell.execute_reply":"2021-11-28T13:25:22.466084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_metrics_k_fold(k_fold_history)","metadata":{"id":"NeubBC67WAq4","execution":{"iopub.status.busy":"2021-11-28T13:25:22.467521Z","iopub.status.idle":"2021-11-28T13:25:22.468173Z","shell.execute_reply.started":"2021-11-28T13:25:22.467922Z","shell.execute_reply":"2021-11-28T13:25:22.467945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- final trainig on whole data ( approx. by a very small test size)","metadata":{}},{"cell_type":"code","source":"X_y_train, X_y_val = single_fold(train_df_res, label = \"cell_type\", test_size = 0.01)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T13:25:22.469386Z","iopub.status.idle":"2021-11-28T13:25:22.47Z","shell.execute_reply.started":"2021-11-28T13:25:22.469773Z","shell.execute_reply":"2021-11-28T13:25:22.469796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_datagen, valid_datagen = get_data_generator_train(\n                                                        X_y_train, \n                                                        X_y_val,\n                                                        label = \"cell_type\",\n                                                        color_mode = \"rgb\",\n                                                        preprocessing_function = None,\n                                                        shuffle = True,\n                                                        batch_size = batch_size,\n                                                        samplewise_center = False,\n                                                        samplewise_std_normalization = False,\n                                                        horizontal_flip = True,\n                                                        vertical_flip = True,\n                                                        target_size = target_size\n                                                        )\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T13:25:22.471358Z","iopub.status.idle":"2021-11-28T13:25:22.472226Z","shell.execute_reply.started":"2021-11-28T13:25:22.471903Z","shell.execute_reply":"2021-11-28T13:25:22.47193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CNN = get_pretrained_model(target_size, num_classes, print_summary = True, trainable_last_layers = 1000)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T13:25:22.473571Z","iopub.status.idle":"2021-11-28T13:25:22.474288Z","shell.execute_reply.started":"2021-11-28T13:25:22.473994Z","shell.execute_reply":"2021-11-28T13:25:22.474021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CNN = compile_model(model= CNN, optimizer = \"Adam\" , lr = 1e-5) ","metadata":{"execution":{"iopub.status.busy":"2021-11-28T13:25:22.475585Z","iopub.status.idle":"2021-11-28T13:25:22.476328Z","shell.execute_reply.started":"2021-11-28T13:25:22.476041Z","shell.execute_reply":"2021-11-28T13:25:22.476068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fit_model_cnn(model = CNN, train_generator = train_datagen, validation_generator = valid_datagen, n_epochs = 5, class_weights = None)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T13:25:22.477649Z","iopub.status.idle":"2021-11-28T13:25:22.478396Z","shell.execute_reply.started":"2021-11-28T13:25:22.478083Z","shell.execute_reply":"2021-11-28T13:25:22.478108Z"},"trusted":true},"execution_count":null,"outputs":[]}]}