{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ¦  Sartorius - Starter Torch Mask R-CNN\n### A self-contained, simple, pure Torch Mask R-CNN implementation, with `LB=0.273`\n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/30201/logos/header.png)\n\nFollowing [this discussion thread](https://www.kaggle.com/c/sartorius-cell-instance-segmentation/discussion/279790), in this notebook we build a base starter Mask R-CNN with pytorch.\n\nThe code is an adapted version from [this notebook](https://www.kaggle.com/abhishek/mask-rcnn-using-torchvision-0-17/) by the first quadruple kaggle grandmaster [Abishek](https://www.kaggle.com/abhishek).\n\nThe [previous U-net model](https://www.kaggle.com/julian3833/sartorius-starter-baseline-torch-u-net), which I was expecting to enter a steep improvement regime with quick-wins, hit a ceiling at `0.03`, no matter what changes I performed ðŸ¥².\nData augmentation, changes in the architecture, and other changes didn't work. The suggestion that semantic segmentation doesn't work seems reasonable, since the individuals cannot be split by connected components, as they overlap heavily.\n\nThis is a follow up notebook with a Mask R-CNN, which was proposed by one of the top competitors ([Inoichan](https://www.kaggle.com/inoueu1)) as a more suitable architecture for this task.\n\nI'm not very familiar with the architecture, but it seems that it is the state-of-the art for \"instance segmentation\".\nIt classifies individuals, gets bounding boxes around them and, most importantly, provides a separated mask for each of them.\n\nYou can read more about it [here](https://viso.ai/deep-learning/mask-r-cnn/).\n\n\nThis model predicts different masks for different individual, rather that an unique mask for the whole picture and thus is better to address the problem at hand.\n\nAt the end, any overlapping pixel is removed, to ensure the non-overlapping policy. That wasn't required with the U-net, since the output was only one unique mask and therefore no overlap could have happened.\n\n\n## Please _DO_ upvote!\n\n\n<h3 style=\"text-align:center; background-color:#C8FF33;padding:40px;border-radius: 30px;\">\nSee also this notebook: <b><a href=\"https://www.kaggle.com/julian3833/sartorius-classifier-mask-r-cnn-lb-0-28\">ðŸ¦  Sartorius - Classifier + Mask R-CNN [LB=0.28]</a></b> using this model along with a simple Resnet Classifier to achieve 0.28\n</h3>\n\n\n\n\n### Changelog\n\n|| Version | Comments | LB |\n|---|  --- | --- | --- |\n|**Best**|33| Roll back to `V31`. Best conf from [here](https://www.kaggle.com/julian3833/sartorius-classifier-mask-r-cnn-lb-0-28). | `0.273` |\n||32| A lot of epochs | `0.273` |\n|**Best**|31| `MIN_SCORE=0.59`. `BOX_DETECTIONS_PER_IMG = 539`. Best conf from [here](https://www.kaggle.com/julian3833/sartorius-classifier-mask-r-cnn-lb-0-28). |`0.273` |\n||30| Version 18 with `MIN_SCORE=0.5`. Remove validation. | `0.27` |\n||28| V27 but pick best epoch using mask-only validation loss. 18 epochs. | `0.205` |\n||27| V18 + 7.5% validation (`PCT_IMAGES_VALIDATION`) w/best epoch for pred. Added `BOX_DETECTIONS_PER_IMG` and `MIN_SCORE` but not used yet. | `0.178` |\n||24| 8 epochs. With Scheduler. | `0.195` |\n||23| 8 epochs. Mask loss only. | `0.036` |\n||22| V18 + Normalize. (7 epochs = `0.189`) | `0.202`|\n||19| 3 epochs size 25%. 3 epochs size 50%. 6 epochs full sized| `0.178` |\n| |__18__| __8 epochs. Added vertical flip. Full sized.__ Tidied-up code.|  `0.202` |\n||15| 12 -> 15 epochs. Setup classification head with classes. Bugfix in `analyze_train_sample`|  `0.172` |\n|| 14 | 12 epochs. Full sized |`0.173` |\n|| 8 | 12 epochs. Resize to (256, 256) |`0.057` |\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-23T04:12:16.649005Z","iopub.execute_input":"2021-10-23T04:12:16.6494Z","iopub.status.idle":"2021-10-23T04:12:16.726081Z","shell.execute_reply.started":"2021-10-23T04:12:16.649296Z","shell.execute_reply":"2021-10-23T04:12:16.723107Z"}}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"#%pip install -Uqqq pycocotools","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:19.652281Z","iopub.execute_input":"2022-06-15T03:41:19.652587Z","iopub.status.idle":"2022-06-15T03:41:19.658218Z","shell.execute_reply.started":"2022-06-15T03:41:19.652556Z","shell.execute_reply":"2022-06-15T03:41:19.657028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The notebooks is self-contained\n# It has very few imports\n# No external dependencies (only the model weights)\n# No train - inference notebooks\n# We only rely on Pytorch\nimport os\nimport time\nimport random\nimport collections\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n# import pycocotools._mask as maskUtils\n\nimport torch\nimport torchvision\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-15T03:41:19.6663Z","iopub.execute_input":"2022-06-15T03:41:19.666691Z","iopub.status.idle":"2022-06-15T03:41:19.674658Z","shell.execute_reply.started":"2022-06-15T03:41:19.666663Z","shell.execute_reply":"2022-06-15T03:41:19.673975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fix randomness\n\ndef fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \nfix_all_seeds(2021)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:19.679086Z","iopub.execute_input":"2022-06-15T03:41:19.679646Z","iopub.status.idle":"2022-06-15T03:41:19.690765Z","shell.execute_reply.started":"2022-06-15T03:41:19.679613Z","shell.execute_reply":"2022-06-15T03:41:19.68959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dict = {}\nfor i in range(8):\n    label_dict[str(i)] = i\nlabel_dict","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:19.692713Z","iopub.execute_input":"2022-06-15T03:41:19.692961Z","iopub.status.idle":"2022-06-15T03:41:19.705862Z","shell.execute_reply.started":"2022-06-15T03:41:19.692933Z","shell.execute_reply":"2022-06-15T03:41:19.70535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"TRAIN_CSV = \"../input/resize-tooth-panoramic/tooth_train_ver4.csv\"\nTRAIN_PATH = \"../input/resize-tooth-panoramic/radiograph/train\"\nTEST_PATH = \"../input/resize-tooth-panoramic/radiograph/val\"\n\nWIDTH = 567\nHEIGHT = 300\n\n# Reduced the train dataset to 5000 rows\nTEST = False\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nRESNET_MEAN = (0.485, 0.456, 0.406)\nRESNET_STD = (0.229, 0.224, 0.225)\n\nBATCH_SIZE = 1\n\n# No changes tried with the optimizer yet.\nMOMENTUM = 0.9\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 0.0005\n\n# Changes the confidence required for a pixel to be kept for a mask. \n# Only used 0.5 till now.\nMASK_THRESHOLD = 0.6\n\n# Normalize to resnet mean and std if True.\nNORMALIZE = False \n\n\n# Use a StepLR scheduler if True. Not tried yet.\nUSE_SCHEDULER = False\n\n# Number of epochs\nNUM_EPOCHS = 30\n\n\nBOX_DETECTIONS_PER_IMG = 10\n\n\nMIN_SCORE = 0.59","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:19.707256Z","iopub.execute_input":"2022-06-15T03:41:19.708255Z","iopub.status.idle":"2022-06-15T03:41:19.730341Z","shell.execute_reply.started":"2022-06-15T03:41:19.708138Z","shell.execute_reply":"2022-06-15T03:41:19.729338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Traning Dataset","metadata":{}},{"cell_type":"markdown","source":"## Utilities\n\n\n### Transformations\n\nJust Horizontal and Vertical Flip for now.\n\nNormalization to Resnet's mean and std can be performed using the parameter `NORMALIZE` in the top cell. Haven't tested it yet.\n\nThe first 3 transformations come from [this](https://www.kaggle.com/abhishek/maskrcnn-utils) utils package by Abishek, `VerticalFlip` is my adaption of HorizontalFlip, and `Normalize` is of my own.","metadata":{}},{"cell_type":"code","source":"# These are slight redefinitions of torch.transformation classes\n# The difference is that they handle the target and the mask\n# Copied from Abishek, added new ones\nclass Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\nclass VerticalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-2)\n            bbox = target[\"boxes\"]\n            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n            target[\"boxes\"] = bbox\n            target[\"masks\"] = target[\"masks\"].flip(-2)\n        return image, target\n\nclass HorizontalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n            target[\"masks\"] = target[\"masks\"].flip(-1)\n        return image, target\n\nclass Normalize:\n    def __call__(self, image, target):\n        image = F.normalize(image, RESNET_MEAN, RESNET_STD)\n        return image, target\n\nclass ToTensor:\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target\n    \n\ndef get_transform(train):\n    transforms = [ToTensor()]\n    if NORMALIZE:\n        transforms.append(Normalize())\n    \n    # Data augmentation for train\n    if train: \n        transforms.append(HorizontalFlip(0.5))\n        transforms.append(VerticalFlip(0.5))\n\n    return Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:19.732205Z","iopub.execute_input":"2022-06-15T03:41:19.732493Z","iopub.status.idle":"2022-06-15T03:41:19.752753Z","shell.execute_reply.started":"2022-06-15T03:41:19.732456Z","shell.execute_reply":"2022-06-15T03:41:19.751585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n    return img.reshape(shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:19.754821Z","iopub.execute_input":"2022-06-15T03:41:19.755347Z","iopub.status.idle":"2022-06-15T03:41:19.774745Z","shell.execute_reply.started":"2022-06-15T03:41:19.755146Z","shell.execute_reply":"2022-06-15T03:41:19.774099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import cv2\n# mask = cv2.imread('../input/resize-tooth-panoramic/mask/train/01-0.jpg')\n# _,mask = cv2.threshold(mask,0,255,cv2.THRESH_BINARY)\n\n# rle = ' '.join(str(x) for x in rle_encode(mask))\n# rle","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:19.83409Z","iopub.execute_input":"2022-06-15T03:41:19.834411Z","iopub.status.idle":"2022-06-15T03:41:19.838419Z","shell.execute_reply.started":"2022-06-15T03:41:19.834388Z","shell.execute_reply":"2022-06-15T03:41:19.837776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_info = collections.defaultdict(dict)\n# temp_df = df.groupby('id')[['annotation','cell_type']].agg(lambda x: list(x)).reset_index()\n# for index, row in temp_df.iterrows():\n#     if int(row['id'] < 10):\n#         image_info[index] = {\n#                 'image_id': row['id'],\n#                 'label': label_dict[str(row['cell_type'][0])],\n#                 'image_path': os.path.join(self.image_dir,\"0\" + str(row['id']) + '.jpg'),\n#                 'annotations': row[\"annotation\"]\n#                 }","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:19.839416Z","iopub.execute_input":"2022-06-15T03:41:19.839593Z","iopub.status.idle":"2022-06-15T03:41:19.856788Z","shell.execute_reply.started":"2022-06-15T03:41:19.839569Z","shell.execute_reply":"2022-06-15T03:41:19.855799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"class CellDataset(Dataset):\n    def __init__(self, image_dir, df, transforms=None, resize=False):\n        self.transforms = transforms\n        self.image_dir = image_dir\n        self.df = df\n        \n        self.should_resize = resize is not False\n        if self.should_resize:\n            self.height = int(HEIGHT * resize)\n            self.width = int(WIDTH * resize)\n        else:\n            self.height = HEIGHT\n            self.width = WIDTH\n        \n        self.image_info = collections.defaultdict(dict)\n        temp_df = self.df.groupby('id')[['annotation','cell_type']].agg(lambda x: list(x)).reset_index()\n        for index, row in temp_df.iterrows():\n            if int(row['id'] < 10):\n                self.image_info[index] = {\n                        'image_id': row['id'],\n                        'label': row['cell_type'],\n                        'image_path': os.path.join(self.image_dir,\"0\" + str(row['id']) + '.jpg'),\n                        'annotations': row[\"annotation\"]\n                        }\n            else:\n                self.image_info[index] = {\n                    'image_id': row['id'],\n                    'label': label_dict[str(row['cell_type'][0])],\n                    'image_path': os.path.join(self.image_dir, str(row['id']) + '.jpg'),\n                    'annotations': row[\"annotation\"]\n                    }\n#         print(self.image_info[3])\n            \n    \n    def get_box(self, a_mask):\n        ''' Get the bounding box of a given mask '''\n        pos = np.where(a_mask)\n        xmin = np.min(pos[1])\n        xmax = np.max(pos[1])\n        ymin = np.min(pos[0])\n        ymax = np.max(pos[0])\n        return [xmin, ymin, xmax, ymax]\n\n    def __getitem__(self, idx):\n        ''' Get the image and the target'''\n        \n        img_path = self.image_info[idx][\"image_path\"]\n        label = self.image_info[idx][\"label\"]\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        if self.should_resize:\n            img = img.resize((self.width, self.height), resample=Image.BILINEAR)\n\n        info = self.image_info[idx]\n        n_objects = len(info['annotations'])\n#         n_objects = len(info['annotations'])\n        masks = np.zeros((len(info['annotations']), self.height, self.width), dtype=np.uint8)\n        boxes = []\n        \n        for i, annotation in enumerate(info['annotations']):\n            path_mask = os.path.join('../input/resize-tooth-panoramic/mask/train', annotation)\n            a_mask = cv2.imread(path_mask,0)\n            a_mask = Image.fromarray(a_mask*255)\n            \n            if self.should_resize:\n                a_mask = a_mask.resize((self.width, self.height), resample=Image.BILINEAR)\n            a_mask = np.array(a_mask) > 0\n            masks[i, :, :] = a_mask\n            boxes.append(self.get_box(a_mask))\n\n        # dummy labels\n#         labels = [1 for _ in range(n_objects)]\n        labels = [0,1,2,3,4,5,6,7,8]\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((n_objects,), dtype=torch.int64)\n\n        # This is the required target for the Mask R-CNN\n        target = {\n            'boxes': boxes,\n            'labels': labels,\n            'masks': masks,\n            'image_id': image_id,\n            'area': area,\n            'iscrowd': iscrowd\n        }\n#         print(target['labels'])\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n        else:\n            print(\" it is None\")\n\n        return img, target\n\n    def __len__(self):\n        return len(self.image_info)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:19.858683Z","iopub.execute_input":"2022-06-15T03:41:19.858928Z","iopub.status.idle":"2022-06-15T03:41:19.892439Z","shell.execute_reply.started":"2022-06-15T03:41:19.858891Z","shell.execute_reply":"2022-06-15T03:41:19.891277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(TRAIN_CSV, nrows=5000 if TEST else None)\nds_train = CellDataset(TRAIN_PATH, df_train, resize=False, transforms=get_transform(train=True))\ndl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, \n                      num_workers=2, collate_fn=lambda x: tuple(zip(*x)))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:19.893768Z","iopub.execute_input":"2022-06-15T03:41:19.894131Z","iopub.status.idle":"2022-06-15T03:41:19.937519Z","shell.execute_reply.started":"2022-06-15T03:41:19.89408Z","shell.execute_reply":"2022-06-15T03:41:19.936573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = []\nfor i in range(10):\n    classes.append('tooth' +str(i))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:19.939576Z","iopub.execute_input":"2022-06-15T03:41:19.939997Z","iopub.status.idle":"2022-06-15T03:41:19.945863Z","shell.execute_reply.started":"2022-06-15T03:41:19.939958Z","shell.execute_reply":"2022-06-15T03:41:19.944316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.utils import draw_bounding_boxes\n\n# Lets view a sample\nsample = ds_train[2]\nimg_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\n# for i in sample[1]['labels']:\n#     print(classes[i])\n# print(sample[1]['boxes'][0])\nplt.imshow(draw_bounding_boxes(\n    img_int, sample[1]['boxes'], [classes[i] for i in sample[1]['labels']], width=2\n).permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:19.947784Z","iopub.execute_input":"2022-06-15T03:41:19.948107Z","iopub.status.idle":"2022-06-15T03:41:20.261451Z","shell.execute_reply.started":"2022-06-15T03:41:19.948067Z","shell.execute_reply":"2022-06-15T03:41:20.260927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train loop","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# Override pythorch checkpoint with an \"offline\" version of the file\n!mkdir -p /root/.cache/torch/hub/checkpoints/\n!cp ../input/cocopre/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:20.262255Z","iopub.execute_input":"2022-06-15T03:41:20.262512Z","iopub.status.idle":"2022-06-15T03:41:21.592829Z","shell.execute_reply.started":"2022-06-15T03:41:20.262489Z","shell.execute_reply":"2022-06-15T03:41:21.592201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    # This is just a dummy value for the classification head\n    NUM_CLASSES = 8\n    \n    if NORMALIZE:\n        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True, \n                                                                   box_detections_per_img=BOX_DETECTIONS_PER_IMG,\n                                                                   image_mean=RESNET_MEAN, \n                                                                   image_std=RESNET_STD)\n    else:\n        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n                                                                  box_detections_per_img=BOX_DETECTIONS_PER_IMG)\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, NUM_CLASSES)\n    return model\n\n\n# Get the Mask R-CNN model\n# The model does classification, bounding boxes and MASKs for individuals, all at the same time\n# We only care about MASKS\nmodel = get_model()\nmodel.to(DEVICE)\n\n# TODO: try removing this for\nfor param in model.parameters():\n    param.requires_grad = True\n    \nmodel.train();","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:21.594207Z","iopub.execute_input":"2022-06-15T03:41:21.594408Z","iopub.status.idle":"2022-06-15T03:41:22.794415Z","shell.execute_reply.started":"2022-06-15T03:41:21.594383Z","shell.execute_reply":"2022-06-15T03:41:22.793246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training loop!","metadata":{}},{"cell_type":"code","source":"import cv2\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\nn_batches = len(dl_train)\n\nfor epoch in range(1, NUM_EPOCHS + 1):\n    print(f\"Starting epoch {epoch} of {NUM_EPOCHS}\")\n    \n    time_start = time.time()\n    loss_accum = 0.0\n    loss_mask_accum = 0.0\n    \n    for batch_idx, (images, targets) in enumerate(dl_train, 1):\n    \n        # Predict\n        images = list(image.to(DEVICE) for image in images)\n        try:\n            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n        except:\n            print(\"e\")\n\n        loss_dict = model(images, targets)\n        print(loss_dict)\n        loss = sum(loss for loss in loss_dict.values())\n        \n        # Backprop\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Logging\n        loss_mask = loss_dict['loss_mask'].item()\n        loss_accum += loss.item()\n        loss_mask_accum += loss_mask\n        \n        if batch_idx % 50 == 0:\n            print(f\"    [Batch {batch_idx:3d} / {n_batches:3d}] Batch train loss: {loss.item():7.3f}. Mask-only loss: {loss_mask:7.3f}\")\n    \n    if USE_SCHEDULER:\n        lr_scheduler.step()\n    \n    # Train losses\n    train_loss = loss_accum / n_batches\n    train_loss_mask = loss_mask_accum / n_batches\n    \n    \n    elapsed = time.time() - time_start\n    \n    \n    torch.save(model.state_dict(), f\"pytorch_model-e{epoch}.bin\")\n    prefix = f\"[Epoch {epoch:2d} / {NUM_EPOCHS:2d}]\"\n    print(f\"{prefix} Train mask-only loss: {train_loss_mask:7.3f}\")\n    print(f\"{prefix} Train loss: {train_loss:7.3f}. [{elapsed:.0f} secs]\")\n     ","metadata":{"execution":{"iopub.status.busy":"2022-06-15T03:41:22.79587Z","iopub.execute_input":"2022-06-15T03:41:22.796116Z","iopub.status.idle":"2022-06-15T04:59:53.439176Z","shell.execute_reply.started":"2022-06-15T03:41:22.796085Z","shell.execute_reply":"2022-06-15T04:59:53.437857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyze prediction results for train set","metadata":{}},{"cell_type":"code","source":"def visualize_bbox(img, bbox, color=(255, 0, 255), thickness=2):  \n    \"\"\"Helper to add bboxes to images \n    Args:\n        img : image as open-cv numpy array\n        bbox : boxes as a list or numpy array in pascal_voc fromat [x_min, y_min, x_max, y_max]  \n        color=(255, 255, 0): boxes color \n        thickness=2 : boxes line thickness\n    \"\"\"\n    x_min, y_min, x_max, y_max = bbox\n    x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n    return img\n\nfrom skimage.color import label2rgb\n# Plots: the image, The image + the ground truth mask, The image + the predicted mask\ndef analyze_train_sample(model, ds_train, sample_index):\n    \n    img, targets = ds_train[sample_index]\n    plt.imshow(img.numpy().transpose((1,2,0)))\n    plt.title(\"Image\")\n    plt.show()\n    \n    masks = np.zeros((HEIGHT, WIDTH))\n    for mask in targets['masks']:\n        masks = np.logical_or(masks, mask)\n    plt.imshow(img.numpy().transpose((1,2,0)))\n    plt.imshow(masks, alpha=0.3)\n    plt.title(\"Ground truth\")\n    plt.show()\n    \n    model.eval()\n    with torch.no_grad():\n        preds = model([img.to(DEVICE)])[0]\n\n    plt.imshow(img.cpu().numpy().transpose((1,2,0)))\n    \n    all_preds_masks = np.zeros((HEIGHT, WIDTH))\n    image = img.numpy().transpose((1,2,0))\n    for box in preds['boxes'].cpu().detach().numpy():\n        #print(box)\n \n        image = visualize_bbox(np.ascontiguousarray(image), box)  \n\n    plt.imshow(image)\n    plt.title(\"box\")\n    plt.show()\n    \n    all_preds_masks = np.zeros_like(masks)\n    for i,mask in enumerate(preds['masks'].cpu().detach().numpy()):\n#         print(preds['labels'])\n#         print(all_preds_masks.shape)\n#         all_preds_masks = np.add(all_preds_masks, (mask[0] > MASK_THRESHOLD), out=all_preds_masks, casting=\"unsafe\")\n        all_preds_masks = np.logical_or(all_preds_masks, mask[0] > MASK_THRESHOLD)\n#     mask_rgb = label2rgb(all_preds_masks, bg_label=0) \n\n    plt.imshow(all_preds_masks,alpha = 0.3)\n    plt.title(\"Predictions\")\n    plt.show()\n    \n    all_preds_masks = np.zeros_like(masks)\n    for i,mask in enumerate(preds['masks'].cpu().detach().numpy()):\n#         print(preds['labels'])\n#         print(all_preds_masks.shape)\n        all_preds_masks = np.add(all_preds_masks, (mask[0] > MASK_THRESHOLD), out=all_preds_masks, casting=\"unsafe\")\n#         all_preds_masks = np.logical_or(all_preds_masks, mask[0] > MASK_THRESHOLD)\n        mask_rgb = label2rgb(all_preds_masks, bg_label=0) \n\n    plt.imshow(all_preds_masks,alpha = 0.6)\n    plt.title(\"Predictions\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:59:53.442657Z","iopub.execute_input":"2022-06-15T04:59:53.442921Z","iopub.status.idle":"2022-06-15T04:59:53.77379Z","shell.execute_reply.started":"2022-06-15T04:59:53.442893Z","shell.execute_reply":"2022-06-15T04:59:53.77273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_iou(labels, y_pred, verbose=0):\n    \"\"\"\n    Computes the IoU for instance labels and predictions.\n\n    Args:\n        labels (np array): Labels.\n        y_pred (np array): predictions\n\n    Returns:\n        np array: IoU matrix, of size true_objects x pred_objects.\n    \"\"\"\n\n    true_objects = len(np.unique(labels))\n    pred_objects = len(np.unique(y_pred))\n\n    if verbose:\n        print(\"Number of true objects: {}\".format(true_objects))\n        print(\"Number of predicted objects: {}\".format(pred_objects))\n\n    # Compute intersection between all objects\n    intersection = np.histogram2d(\n        labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects)\n    )[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins=true_objects)[0]\n    area_pred = np.histogram(y_pred, bins=pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n    intersection = intersection[1:, 1:] # exclude background\n    union = union[1:, 1:]\n    union[union == 0] = 1e-9\n    iou = intersection / union\n    \n    return iou  \n\ndef precision_at(threshold, iou):\n    \"\"\"\n    Computes the precision at a given threshold.\n\n    Args:\n        threshold (float): Threshold.\n        iou (np array): IoU matrix.\n\n    Returns:\n        int: Number of true positives,\n        int: Number of false positives,\n        int: Number of false negatives.\n    \"\"\"\n    matches = iou > threshold\n    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n    tp, fp, fn = (\n        np.sum(true_positives),\n        np.sum(false_positives),\n        np.sum(false_negatives),\n    )\n    return tp, fp, fn\n\ndef iou_map(truths, preds, verbose=0):\n    \"\"\"\n    Computes the metric for the competition.\n    Masks contain the segmented pixels where each object has one value associated,\n    and 0 is the background.\n\n    Args:\n        truths (list of masks): Ground truths.\n        preds (list of masks): Predictions.\n        verbose (int, optional): Whether to print infos. Defaults to 0.\n\n    Returns:\n        float: mAP.\n    \"\"\"\n    ious = [compute_iou(truth, pred, verbose) for truth, pred in zip(truths, preds)]\n\n    if verbose:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        tps, fps, fns = 0, 0, 0\n        for iou in ious:\n            tp, fp, fn = precision_at(t, iou)\n            tps += tp\n            fps += fp\n            fns += fn\n\n        p = tps / (tps + fps + fns)\n        prec.append(p)\n\n        if verbose:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tps, fps, fns, p))\n\n    if verbose:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n\n    return np.mean(prec)\n\n\ndef get_score(ds, mdl):\n    \"\"\"\n    Get average IOU mAP score for a dataset\n    \"\"\"\n    mdl.eval()\n    iouscore = 0\n    for i in tqdm(range(len(ds))):\n        img, targets = ds[i]\n        with torch.no_grad():\n            result = mdl([img.to(DEVICE)])[0]\n            \n        masks = combine_masks(targets['masks'], 0.5)\n        labels = pd.Series(result['labels'].cpu().numpy()).value_counts()\n\n        mask_threshold = mask_threshold_dict[labels.sort_values().index[-1]]\n        pred_masks = combine_masks(get_filtered_masks(result), mask_threshold)\n        iouscore += iou_map([masks],[pred_masks])\n    return iouscore / len(ds)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:59:53.775166Z","iopub.execute_input":"2022-06-15T04:59:53.775366Z","iopub.status.idle":"2022-06-15T04:59:53.792662Z","shell.execute_reply.started":"2022-06-15T04:59:53.775343Z","shell.execute_reply":"2022-06-15T04:59:53.792059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef remove_overlapping_pixels(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            mask[np.logical_and(mask, other_mask)] = 0\n    return mask\n\ndef combine_masks(masks, mask_threshold):\n    \"\"\"\n    combine masks into one image\n    \"\"\"\n    maskimg = np.zeros((HEIGHT, WIDTH))\n    # print(len(masks.shape), masks.shape)\n    for m, mask in enumerate(masks,1):\n        maskimg[mask>mask_threshold] = m\n    return maskimg\n\n\ndef get_filtered_masks(pred):\n    \"\"\"\n    filter masks using MIN_SCORE for mask and MAX_THRESHOLD for pixels\n    \"\"\"\n    use_masks = []   \n    for i, mask in enumerate(pred[\"masks\"]):\n\n        # Filter-out low-scoring results. Not tried yet.\n        scr = pred[\"scores\"][i].cpu().item()\n        label = pred[\"labels\"][i].cpu().item()\n        if scr > min_score_dict[label]:\n            mask = mask.cpu().numpy().squeeze()\n            # Keep only highly likely pixels\n            binary_mask = mask > mask_threshold_dict[label]\n            binary_mask = remove_overlapping_pixels(binary_mask, use_masks)\n            use_masks.append(binary_mask)\n\n    return use_masks\n","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:59:53.79396Z","iopub.execute_input":"2022-06-15T04:59:53.794277Z","iopub.status.idle":"2022-06-15T04:59:53.811549Z","shell.execute_reply.started":"2022-06-15T04:59:53.79424Z","shell.execute_reply":"2022-06-15T04:59:53.811025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport matplotlib.patches as patches\nmask_threshold_dict = {1: 0.55, 2: 0.75, 3:  0.6,4: 0.5, 5: 0.5, 6:  0.5,7: 0.5, 8: 0.5, 9:  0.5}\nmin_score_dict = {1: 0.19, 2: 0.75, 3: 0.5, 4: 0.55, 5: 0.55, 6:  0.55,7: 0.55, 8: 0.55, 9:  0.55}\ndef combine_masks(masks, mask_threshold):\n    \"\"\"\n    combine masks into one image\n    \"\"\"\n    maskimg = np.zeros((HEIGHT, WIDTH))\n    # print(len(masks.shape), masks.shape)\n    for m, mask in enumerate(masks,1):\n\n        maskimg[mask>mask_threshold] = m\n\n    return maskimg\n\ndef get_filtered_masks(pred):\n    \"\"\"\n    filter masks using MIN_SCORE for mask and MAX_THRESHOLD for pixels\n    \"\"\"\n    use_masks = []   \n    mask_scr = np.zeros(9)\n    use_i = []\n    for i, mask in enumerate(pred[\"masks\"]):\n\n        # Filter-out low-scoring results. Not tried yet.\n        scr = pred[\"scores\"][i].cpu().item()\n        label = pred[\"labels\"][i].cpu().item()\n#         print(f'get{scr} of {min_score_dict[label]}')\n        if mask_scr[label] < scr:\n            mask_scr[label] = scr\n            mask = mask.cpu().numpy().squeeze()\n            # Keep only highly likely pixels\n            binary_mask = mask > mask_threshold_dict[label]\n            binary_mask = remove_overlapping_pixels(binary_mask, use_masks)\n            use_masks.append(binary_mask)\n            use_i.append(i)\n    return [use_masks , use_i] \n\ndef analyze_train_sample(model, ds_train, sample_index):\n    \n    img, targets = ds_train[sample_index]\n    #print(img.shape)\n    l = np.unique(targets[\"labels\"])\n    ig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,60), facecolor=\"#fefefe\")\n    ax[0].imshow(img.numpy().transpose((1,2,0)))\n    ax[0].set_title(f\"cell type {l}\")\n   \n    \n    masks = combine_masks(targets['masks'], 0.1)\n    #plt.imshow(img.numpy().transpose((1,2,0)))\n    ax[1].imshow(masks)\n    ax[1].set_title(f\"Ground truth, {len(targets['masks'])} cells\")\n    \n\n\n    model.eval()\n    with torch.no_grad():\n        preds = model([img.to(DEVICE)])[0]\n    \n    l = pd.Series(preds['labels'].cpu().numpy()).value_counts()\n    lstr = \"\"\n    for i in l.index:\n        lstr += f\"{l[i]}x{i} \"\n    #print(l, l.sort_values().index[-1])\n    #plt.imshow(img.cpu().numpy().transpose((1,2,0)))\n    mask_threshold = mask_threshold_dict[l.sort_values().index[-1]]\n    #print(mask_threshold)\n    ig, axx = plt.subplots(nrows=1, ncols=2, figsize=(20,60), facecolor=\"#fefefe\")\n    mask_filt = get_filtered_masks(preds)\n    pred_masks = combine_masks(mask_filt[0], 0.2)\n    find_label = \"found :\"\n    for lb in mask_filt[1]:\n        find_label += str(lb)\n    axx[0].imshow(img.numpy().transpose((1,2,0)))\n    axx[0].imshow(pred_masks,alpha = 0.9)\n    axx[0].set_title('label' + find_label)\n\n\n    img = img.numpy().transpose((1,2,0))\n    m = img.copy() \n    for i in mask_filt[1]:\n        label = str(preds['labels'][i].cpu().item())\n        box = preds['boxes'][i].cpu().detach().numpy()\n        m = cv2.putText(m,str(label), \n                (int(box[0]+1),int(box[1]+1)), cv2.FONT_HERSHEY_SIMPLEX, \n               1, (255,255,255),1, cv2.LINE_AA) \n#         print(\"box:\" , box)\n#         start = (int(box[0]) , int(box[1]))\n#         stop  = (int(box[0])+int(box[2]) , int(box[1])+int(box[3]))\n#         print(start,stop)\n\n        rect = patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1],  linewidth=1, edgecolor='r', facecolor='none')\n        axx[1].add_patch(rect)\n        \n    axx[1].imshow(m)\n    plt.show()\n    \n    ax[1].axis(\"off\")\n\n           \n#         img = cv2.rectangle(img, x, y, (255,0,0), thickness = 1)\n#     ax[1,1].imshow(img) \n\n    \n    #print(masks.shape, pred_masks.shape)\n    score = iou_map([masks],[pred_masks])\n#     print(preds)\n    print(\"IOU Score:\", score)    \nanalyze_train_sample(model, ds_train, 3)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T05:31:17.779969Z","iopub.execute_input":"2022-06-15T05:31:17.780243Z","iopub.status.idle":"2022-06-15T05:31:22.917702Z","shell.execute_reply.started":"2022-06-15T05:31:17.780216Z","shell.execute_reply":"2022-06-15T05:31:22.91651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nmodel = get_model()\n# model= torch.load(('../input/resize-tooth-panoramic/pytorch_model-e21.bin'), map_location='cpu')\n\n# pretrained_dict = torch.load(('../input/resize-tooth-panoramic/pytorch_model-e21.bin'), map_location='cpu')\n# model_dict = model.state_dict()\n\n# # 1. filter out unnecessary keys\n# pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n# # 2. overwrite entries in the existing state dict\n# model_dict.update(pretrained_dict) \n# # 3. load the new state dict\n# model.load_state_dict(pretrained_dict)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:59:59.45404Z","iopub.execute_input":"2022-06-15T04:59:59.454482Z","iopub.status.idle":"2022-06-15T05:00:01.168977Z","shell.execute_reply.started":"2022-06-15T04:59:59.454452Z","shell.execute_reply":"2022-06-15T05:00:01.167552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.roi_heads.mask_predictor = MaskRCNNPredictor(in_channels = 256,dim_reduced = 256,\n#                                                 num_classes = 9)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T05:00:01.170391Z","iopub.execute_input":"2022-06-15T05:00:01.170605Z","iopub.status.idle":"2022-06-15T05:00:01.176066Z","shell.execute_reply.started":"2022-06-15T05:00:01.170576Z","shell.execute_reply":"2022-06-15T05:00:01.174512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(10):\n\n#     analyze_train_sample(model, ds_train, i)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-15T05:00:01.177533Z","iopub.execute_input":"2022-06-15T05:00:01.177794Z","iopub.status.idle":"2022-06-15T05:00:01.190769Z","shell.execute_reply.started":"2022-06-15T05:00:01.177757Z","shell.execute_reply":"2022-06-15T05:00:01.189936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analyze_train_sample(model, ds_train, 1)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T05:00:01.192687Z","iopub.execute_input":"2022-06-15T05:00:01.19304Z","iopub.status.idle":"2022-06-15T05:00:06.610658Z","shell.execute_reply.started":"2022-06-15T05:00:01.192985Z","shell.execute_reply":"2022-06-15T05:00:06.609847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analyze_train_sample(model, ds_train, 2)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T05:00:06.612041Z","iopub.execute_input":"2022-06-15T05:00:06.612809Z","iopub.status.idle":"2022-06-15T05:00:12.097996Z","shell.execute_reply.started":"2022-06-15T05:00:06.612773Z","shell.execute_reply":"2022-06-15T05:00:12.09725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"markdown","source":"## Test Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"class CellTestDataset(Dataset):\n    def __init__(self, image_dir, transforms=None):\n        self.transforms = transforms\n        self.image_dir = image_dir\n        self.image_ids = [f[:-4]for f in os.listdir(self.image_dir)]\n    \n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_path = os.path.join(self.image_dir, image_id + '.jpg')\n        image = Image.open(image_path).convert(\"RGB\")\n\n        if self.transforms is not None:\n            image, _ = self.transforms(image=image, target=None)\n        return {'image': image, 'image_id': image_id}\n\n    def __len__(self):\n        return len(self.image_ids)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T05:00:12.099154Z","iopub.execute_input":"2022-06-15T05:00:12.099385Z","iopub.status.idle":"2022-06-15T05:00:12.108634Z","shell.execute_reply.started":"2022-06-15T05:00:12.099355Z","shell.execute_reply":"2022-06-15T05:00:12.107858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_test = CellTestDataset(TEST_PATH, transforms=get_transform(train=False))\nds_test[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T05:00:12.109834Z","iopub.execute_input":"2022-06-15T05:00:12.11006Z","iopub.status.idle":"2022-06-15T05:00:12.145735Z","shell.execute_reply.started":"2022-06-15T05:00:12.110011Z","shell.execute_reply":"2022-06-15T05:00:12.1451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utilities","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_encoding(x):\n    dots = np.where(x.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return ' '.join(map(str, run_lengths))\n\n\ndef remove_overlapping_pixels(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            mask[np.logical_and(mask, other_mask)] = 0\n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-06-15T05:00:12.146971Z","iopub.execute_input":"2022-06-15T05:00:12.147503Z","iopub.status.idle":"2022-06-15T05:00:12.156359Z","shell.execute_reply.started":"2022-06-15T05:00:12.147472Z","shell.execute_reply":"2022-06-15T05:00:12.154777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run predictions","metadata":{}},{"cell_type":"code","source":"model.eval();\n\nsubmission = []\nfor sample in ds_test:\n    img = sample['image']\n    image_id = sample['image_id']\n    with torch.no_grad():\n        result = model([img.to(DEVICE)])[0]\n    \n    previous_masks = []\n    for i, mask in enumerate(result[\"masks\"]):\n        \n        # Filter-out low-scoring results. Not tried yet.\n        score = result[\"scores\"][i].cpu().item()\n        if score < MIN_SCORE:\n            continue\n        \n        mask = mask.cpu().numpy()\n        # Keep only highly likely pixels\n        binary_mask = mask > MASK_THRESHOLD\n        binary_mask = remove_overlapping_pixels(binary_mask, previous_masks)\n        previous_masks.append(binary_mask)\n        rle = rle_encoding(binary_mask)\n        submission.append((image_id, rle))\n    \n    # Add empty prediction if no RLE was generated for this image\n    all_images_ids = [image_id for image_id, rle in submission]\n    if image_id not in all_images_ids:\n        submission.append((image_id, \"\"))\n\ndf_sub = pd.DataFrame(submission, columns=['id', 'predicted'])\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T05:00:12.158451Z","iopub.execute_input":"2022-06-15T05:00:12.159085Z","iopub.status.idle":"2022-06-15T05:00:28.922467Z","shell.execute_reply.started":"2022-06-15T05:00:12.158995Z","shell.execute_reply":"2022-06-15T05:00:28.921845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub['predicted'][0]\ntest  = rle_decode(df_sub['predicted'][0], (400,576))\nplt.imshow(test)\ndef rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n    return img.reshape(shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T05:00:28.923806Z","iopub.execute_input":"2022-06-15T05:00:28.924213Z","iopub.status.idle":"2022-06-15T05:00:29.137048Z","shell.execute_reply.started":"2022-06-15T05:00:28.924181Z","shell.execute_reply":"2022-06-15T05:00:29.13614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}