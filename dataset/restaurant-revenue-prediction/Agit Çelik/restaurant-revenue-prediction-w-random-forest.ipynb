{"cells":[{"metadata":{"_uuid":"b15e5e24584f71843bb79b459035729dda9b93c0"},"cell_type":"markdown","source":"# Restaurant Revenue Prediction Using With Random Forest Regressor\n\n\n-Agit Çelik\n-Oğulcan Gök\n-Selim Kundakçıoğlu\n\n\n\n\n## Contents\n- [Pre-Processing](#pre)\n\n- [Classification](#class)\n\n- [Standartizaiton](#std)\n\n- [PCA](#pca)\n- [RBF](#rbf)\n- [RandomForestRegressor is used to predict \"revenues\"](#rnd)\n- [Score of worst model](#scr)\n- [Conclution](#conc)"},{"metadata":{"_uuid":"18394f8a780bdb40b015eecc490e410cc020105c"},"cell_type":"markdown","source":"### Importing required libraries"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"5f3c2a35504e563c0023442735d05bb05091eace"},"cell_type":"code","source":"\nfrom sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nimport numpy as np \nimport pandas as pd \n\n#reading the csv data\ntrainData = pd.read_csv('../input/train.csv')\ntrainData.info()\n\ntrainData.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee075656cdde927fc0e342ac6e5388bf7449ef14"},"cell_type":"markdown","source":"## PRE-PROCESSING  & SOME  ANALYSIS <a name=\"pre\"></a>"},{"metadata":{"_uuid":"689087ffd21d43bac9b2811336e32f101e838c5d"},"cell_type":"markdown","source":"### Converting Open Date column to Open Days; day count of the restaurant since the beginning and dropping the Open Date Columns"},{"metadata":{"trusted":true,"_uuid":"646689f368bf16a7298885202534d615fb352c22"},"cell_type":"code","source":"trainData['Open Date'] = pd.to_datetime(trainData['Open Date'], format='%m/%d/%Y')   \ntrainData['OpenDays']=\"\"\n\ndateLastTrain = pd.DataFrame({'Date':np.repeat(['01/01/2018'],[len(trainData)]) })\ndateLastTrain['Date'] = pd.to_datetime(dateLastTrain['Date'], format='%m/%d/%Y')  \n\ntrainData['OpenDays'] = dateLastTrain['Date'] - trainData['Open Date']\ntrainData['OpenDays'] = trainData['OpenDays'].astype('timedelta64[D]').astype(int)\n\ntrainData = trainData.drop('Open Date', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b34dba636ef5295f3737287930bf18762e84775b"},"cell_type":"markdown","source":"### Comparing the revenues of big cities and other cities"},{"metadata":{"trusted":true,"_uuid":"2c44e44c3b59e526a74c9a9526febd796df564c8"},"cell_type":"code","source":"cityPerc = trainData[[\"City Group\", \"revenue\"]].groupby(['City Group'],as_index=False).mean()\n\nsns.barplot(x='City Group', y='revenue', data=cityPerc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c06300eb692577e678b65f66a37e8f5a01cd587"},"cell_type":"markdown","source":"# Plots\n### Sorting the cities by revenue; getting the max earned cities"},{"metadata":{"trusted":true,"_uuid":"05617c483fd12e10f877757ac857a792eeb7806f"},"cell_type":"code","source":"cityPerc = trainData[[\"City\", \"revenue\"]].groupby(['City'],as_index=False).mean()\n\nnewDF = cityPerc.sort_values([\"revenue\"],ascending= False)\nsns.barplot(x='City', y='revenue', data=newDF.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc9abbe3bb7de13c7bc62e8c7012d59e43ae556b"},"cell_type":"code","source":"cityPerc = trainData[[\"City\", \"revenue\"]].groupby(['City'],as_index=False).mean()\nnewDF = cityPerc.sort_values([\"revenue\"],ascending= True)\nsns.barplot(x='City', y='revenue', data=newDF.head(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d41efb71a98bd4e48767f7f49f0e23840ca81771"},"cell_type":"markdown","source":"### Getting an insight of which restaurant type earns more"},{"metadata":{"trusted":true,"_uuid":"e580de701d80230e6755af2ae5a70b43e3c21e97"},"cell_type":"code","source":"cityPerc = trainData[[\"Type\", \"revenue\"]].groupby(['Type'],as_index=False).mean()\nsns.barplot(x='Type', y='revenue', data=cityPerc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8d0b1f8be065bae05d3af0527337d47dbd89963"},"cell_type":"markdown","source":"### Plot about working days of specific restaurant types"},{"metadata":{"trusted":true,"_uuid":"b3b4ba28bed35ddd8c80bb05d6748cd6a55d5abe"},"cell_type":"code","source":"cityPerc = trainData[[\"Type\", \"OpenDays\"]].groupby(['Type'],as_index=False).mean()\nsns.barplot(x='Type', y='OpenDays', data=cityPerc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7dc6af2656964711c5e05a6db6399bf0ebcc7e4"},"cell_type":"markdown","source":"### Dropping the Id and Type columns since they are irrevelant for our predictions"},{"metadata":{"trusted":true,"_uuid":"e80592a391e6cdddc148ac327fe29dcda62cef39"},"cell_type":"code","source":"trainData = trainData.drop('Id', axis=1)\ntrainData = trainData.drop('Type', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6324ea0595b8574f5ee91c60ed1558d0e69d2b12"},"cell_type":"markdown","source":"### Creating dummy variables to represent City Groups. After doing dummy variables for City Group we dropped it"},{"metadata":{"trusted":true,"_uuid":"137c7c11fea7685ddb0080e8d1ad00a7a8b8f92e"},"cell_type":"code","source":"citygroupDummy = pd.get_dummies(trainData['City Group'])\ntrainData = trainData.join(citygroupDummy)\n\n\ntrainData = trainData.drop('City Group', axis=1)\n\ntrainData = trainData.drop('City', axis=1)\n\ntempRev = trainData['revenue']\ntrainData = trainData.drop('revenue', axis=1)\n\n\ntrainData = trainData.join(tempRev)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24d324d66019e3a21c13551635fce56642794025"},"cell_type":"code","source":"trainData.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abbb56700d41139ede8337ffca08a5c6e7b16dcd"},"cell_type":"markdown","source":"# Train and  Test Split for RandomForestClassifier <a name=\"class\"></a>\n\n### Using SKLEARN's train test split library for splitting train data"},{"metadata":{"trusted":true,"_uuid":"311ffc4f58b4074c64edfb284e794cbb0b87e958"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX, y = trainData.iloc[:, 1:40].values, trainData.iloc[:, 40].values\n\nX_trainForBestFeatures, X_testForBestFeatures, y_trainForBestFeatures, y_testForBestFeatures =\\\n    train_test_split(X, y, \n                     test_size=0.3, \n                     random_state=0, \n                )\n    \nX_trainForBestFeatures.shape, X_testForBestFeatures.shape, y_trainForBestFeatures.shape, y_testForBestFeatures.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85e3b2e9ead0b28ef695c44d251bbcebc33fc490"},"cell_type":"code","source":"y[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd3d8c890e0e5f374569eaa4bbbbbf50eb8d92b4"},"cell_type":"code","source":"y_trainForBestFeatures[:20]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a48f27d3c19772deb4ac6b8261d746de2a5a0ac3"},"cell_type":"markdown","source":"### For finding best features among others. We used random forest classifier in order to get the best features. We observed that using the first 19 features give us the best results"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"3f069f50bcc6df1f6e27fa402a4df87f0b9bcf9d"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n#To label our features form best to wors \nfeat_labels = trainData.columns[1:40]\n\nforest = RandomForestClassifier(n_estimators=500,\n                                random_state=1)\nforest.fit(X_trainForBestFeatures, y_trainForBestFeatures)\n\n\n\nimportances = forest.feature_importances_\n\nindices = np.argsort(importances)[::-1]\n\nfor f in range(X_trainForBestFeatures.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, \n                            feat_labels[indices[f]], \n                            importances[indices[f]]))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f265088459ee7f6dd3463814ec18f08c7a47b648"},"cell_type":"markdown","source":"### Plotting the importance of the features in a barplot"},{"metadata":{"trusted":true,"_uuid":"43b844f54ccca425aea58d637583d6ab5f6f46af"},"cell_type":"code","source":"plt.title('Feature Importance')\nplt.bar(range(X_trainForBestFeatures.shape[1]), \n        importances[indices],\n        align='center')\n\nplt.xticks(range(X_trainForBestFeatures.shape[1]), \n           feat_labels[indices], rotation=90)\nplt.xlim([-1, X_trainForBestFeatures.shape[1]])\nplt.tight_layout()\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"954c43efd6dd85ffa95c5ae9d77ae45589ca5ca6"},"cell_type":"code","source":"trainData[feat_labels[indices[0:39]]].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6e1fae56031b4358a7a9c2d7a6d834c9cf459b5"},"cell_type":"markdown","source":"### We take the natural logarithm of the OpenDays column in order to make it more easy for model to predict"},{"metadata":{"trusted":true,"_uuid":"e0a26805f5546e544c3eb433a6568cbe5ea58717"},"cell_type":"code","source":"import numpy as numpy \nopenDaysLog = trainData[feat_labels[indices[0:1]]].apply(numpy.log)\nopenDaysLog.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0112d887fe0b9c428f27186671a23449c32077f"},"cell_type":"markdown","source":"### Test and Train model created over best 19 features.\n"},{"metadata":{"trusted":true,"_uuid":"223a980952286da78c4a4b2bb325039a14a3156e"},"cell_type":"code","source":"bestDataFeaturesTrain = trainData[feat_labels[indices[1:19]]]\n\n#insert after takeing log of OpenDays feature.\nbestDataFeaturesTrain.insert(loc=0, column='OpenDays', value=openDaysLog)\n\nbestDataFeaturesTrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bd08ae5ba0dd9eb2a82e0126be70689e3ecec94"},"cell_type":"markdown","source":"# Model will predict output by using best 19 features.\ntrain_test_split method of sklearn is used to split data into %30 of Test and %70 of Train data"},{"metadata":{"trusted":true,"_uuid":"60f847bedb3f938302e4f955e266f0223d605c37"},"cell_type":"code","source":"# take the natural logarithm of the 'revenue' column in order to make it more easy for model to predict\ny = trainData['revenue'].apply(numpy.log)\n\nfrom sklearn.model_selection import train_test_split\n\nX, y = trainData.iloc[:, 1:40].values, trainData.iloc[:, 40].values\n\nX_train, X_test, y_train, y_test =\\\n    train_test_split(bestDataFeaturesTrain, y, \n                     test_size=0.3, \n                     random_state=0, \n                )\n\n\n\n    \nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6fd88f35dca99c1e417ba89b4287018c1a676ec"},"cell_type":"markdown","source":"# Standardize features by removing the mean and scaling to unit variance\n<a name=\"std\"></a>\n### Standart Scaling for model efficiency"},{"metadata":{"trusted":true,"_uuid":"736b7980b2f7c61df5b0f9e1fb03be4edb47b7f3"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler(with_std  = True ,with_mean = True, copy = True)\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edd104fe2302604aa36513137ede20efa320728e"},"cell_type":"code","source":"X_train_std[:1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62e5c339fd0a23e8d2ec7d0b41baa08d65f6e271"},"cell_type":"markdown","source":"# PCA is used due to dimension reduction <a name=\"pca\"></a>\n### Applied PCA in order to make it more efficient and reducing te dimentions"},{"metadata":{"trusted":true,"_uuid":"3e7d02d1c08de7a7ff41b1420190efbc1f295460"},"cell_type":"code","source":"from sklearn.decomposition import PCA,KernelPCA\n\npca = PCA(n_components=2,svd_solver='full')\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\npca.explained_variance_ratio_\n\nkpca = KernelPCA(kernel=\"rbf\", gamma=1)\nX_kpca_train = kpca.fit_transform(X_train_pca)\nX_kpca_test = kpca.transform(X_test_pca)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e84db84d54ff0f0bd038d755101886f96f27d76"},"cell_type":"markdown","source":"# RBF is applied for linearity (since we have non-linear data) <a name=\"rbf\"></a>\n\n### After RBF, increase in sample variance can be observed (blue plot)"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"223fb1174f5818f44a4b57cb1d426b5450bc54f2"},"cell_type":"code","source":"X_train_pca[:1]\nfig, ax = plt.subplots(nrows=1,ncols=2, figsize=(10,5))\nax[0].scatter(X_train_pca[:, 0], X_train_pca[:, 1],color='red',marker='o')\nax[1].scatter(X_kpca_train[:, 0], X_kpca_train[:, 1])\nax[0].set_xlabel('Before RBF')\nax[1].set_yticks([])\nax[1].set_xlabel('After RBF')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"0a3fe21e7818f62a202895c3ac87bb5bab2ac100"},"cell_type":"code","source":"X_test_pca[:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28d25a9e41259ba146b9f0826248354d3e3960d3"},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"bef1d219afd5a82728e437347da996887271bcc0"},"cell_type":"code","source":"X_train_std[:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb1a072eceb094792c8a449600d68b1e8bdc6880"},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef3ff5b3fdb3487297ce5d89e3d8c3db2d18e453"},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cbb7881c673f5a335eedd78c0b3acba5e4cfc69"},"cell_type":"code","source":"y_test[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cabcb54488b0e3081ea08c6f6efe1827412ff2f"},"cell_type":"markdown","source":"# RandomForestRegressor is used to predict \"revenues\" <a name=\"rnd\"></a>\n\n### Finally after pre-processing now we can begin to predict with RandomForestRegressor.\n\n#### Our model works on 86% accuracy."},{"metadata":{"trusted":true,"_uuid":"8952aafc23b46c4701e4ced653bcb91c4c9c575c"},"cell_type":"code","source":"\nimport numpy\nfrom sklearn import linear_model\ncls = RandomForestRegressor(n_estimators=250, criterion='mse', max_depth=30)#cls = RandomForestRegressor(n_estimators=150)\n\ncls.fit(X_kpca_train, y_train)#We are training the model with RBF'ed data\n\nscoreOfModel = cls.score(X_kpca_train, y_train)\n\n\nprint(\"Score is calculated as: \",scoreOfModel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dee102d9ebaee947979943aca79da1777ec63711"},"cell_type":"code","source":"pred = cls.predict(X_kpca_test)\n\npred","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"439623041f4a60bbe598be1495477d8dc7bdb88b"},"cell_type":"markdown","source":"# Our prediction and actual value with their differences and their score"},{"metadata":{"trusted":true,"_uuid":"f5f5ccc4fd9420abf3866b19383ac431f5cab06c"},"cell_type":"code","source":"for z in zip(y_test, pred):\n    print(z, (z[0]-z[1]) /z[0] )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee92619d1f58e77c42956ecf0e4aedfd9de15720"},"cell_type":"markdown","source":"## Plot Revenues(orange line) and Predicted Revenues(blue line)\n\n### plotting the real revenues and the predicted values"},{"metadata":{"trusted":true,"_uuid":"0359db9beba0b401f32c6b8a9a23f51c7926907b"},"cell_type":"code","source":"\nr = []\nfor pair in  zip(pred, y_test):\n    r.append(pair)\n\nplt.plot(r)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ab0f114577b8ca99a5777b0504e0660092416b0"},"cell_type":"markdown","source":"## Effect of estimators on score \n\n### With np.range we tried to find the most efficient estimator value. As can be seen on the plot, around 160 is the highest"},{"metadata":{"trusted":true,"_uuid":"4a63f75df18907b970a591ccd3826f49264657e4"},"cell_type":"code","source":"\nestimators = np.arange(10, 250, 10) # 10 to 250 increased with 10\nscores = []\nfor n in estimators:\n    cls.set_params(n_estimators=n)\n    cls.fit(X_kpca_train, y_train)\n    scores.append(cls.score(X_kpca_train, y_train))\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea0ee6bf430c9814c916777acc806486087216b1"},"cell_type":"code","source":"estimators = np.arange(10, 250, 10) # 10 to 250 increased with 10\nscores = []\nfor n in estimators:\n    cls.set_params(n_estimators=n)\n    cls.fit(X_kpca_train, y_train)\n    scores.append(cls.score(X_kpca_test, y_test))\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1b6854348a6f5be948fcd3afb4255d5d6bce564"},"cell_type":"code","source":"pred[:20]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6e8d1c03be411ca808c436937a1989ccd443120"},"cell_type":"markdown","source":"# Score of worst model <a name=\"scr\"></a>\n### We took the worst 19 feautres and buiilt a model accordingly to see the effect to the model score."},{"metadata":{"trusted":true,"_uuid":"22e583abd570fe608602150a37d557cd90192052"},"cell_type":"code","source":"worstDataFeaturesTrain = trainData[feat_labels[indices[19:39]]]\nworstDataFeaturesTrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b82e33b2607b5c84fa3a5c604aaae4b116c4e7b3"},"cell_type":"markdown","source":"### Splitting the data with respect to worst features\ntrain_test_split method of sklearn is used to split data into %30 of Test and %70 of Train data"},{"metadata":{"trusted":true,"_uuid":"1cf26f143ff5ffdccabdd0f9a2e4cdf0a2cbdbde"},"cell_type":"code","source":"\ny = trainData['revenue'].values\n\nfrom sklearn.model_selection import train_test_split\n\nX, y = trainData.iloc[:, 1:40].values, trainData.iloc[:, 40].values\n\nX_trainWorst, X_testWorst, y_trainWorst, y_testWorst =\\\n    train_test_split(worstDataFeaturesTrain, y, \n                     test_size=0.3, \n                     random_state=0, \n                )\n\n\n\n    \nX_trainWorst.shape, X_testWorst.shape, y_trainWorst.shape, y_testWorst.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89ce42f01232322372ccf65a0ee472baaf59ba64"},"cell_type":"markdown","source":"### Fitting the model.\n### As can be seen below, model score is 31%. Less than 50% so it is worse."},{"metadata":{"trusted":true,"_uuid":"5f8b4262c631a2d7cd8e2745f98f6f1b0251d700"},"cell_type":"code","source":"\nimport numpy\nfrom sklearn import linear_model\ncls = RandomForestRegressor(n_estimators=250, criterion='mse', max_depth=30)#cls = RandomForestRegressor(n_estimators=150)\n\ncls.fit(X_trainWorst, y_trainWorst)\n\nscoreOfModel = cls.score(X_trainWorst, y_trainWorst)\n\nprint(\"Score is calculated as: \",scoreOfModel)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecfbc7a963a236765d351bfd905c586d0790bda0"},"cell_type":"markdown","source":"### Predicted values from the 'worse' model"},{"metadata":{"trusted":true,"_uuid":"001fbd88b837b8c406e9d5d2a8ebd437fc2f42b6"},"cell_type":"code","source":"\npred = cls.predict(X_testWorst)\n\npred","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f21a9f859b7ce483afe9f045826302096af86ef"},"cell_type":"markdown","source":"### Again Trying to find the best estimator values for the model"},{"metadata":{"trusted":true,"_uuid":"bbc24c2c9663d18f4fd30ca69aafe8438a6d63a7"},"cell_type":"code","source":"\nestimators = np.arange(10, 250, 10) # 10 to 250 increased with 10\nscores = []\nfor n in estimators:\n    cls.set_params(n_estimators=n)\n    cls.fit(X_trainWorst, y_trainWorst)\n    scores.append(cls.score(X_trainWorst, y_trainWorst))\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85460726886aa8ff53519be39c0627125eae92e8"},"cell_type":"code","source":"estimators = np.arange(10, 250, 10) # 10 to 250 increased with 10\nscores = []\nfor n in estimators:\n    cls.set_params(n_estimators=n)\n    cls.fit(X_trainWorst, y_trainWorst)\n    scores.append(cls.score(X_testWorst, y_testWorst))\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf65361e5d5465a6a1ff911f4a3755d7df6cceec"},"cell_type":"code","source":"import numpy as numpy \nopenDaysLog = trainData[feat_labels[indices[0:1]]].apply(numpy.log)\nopenDaysLog.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be898b69b40146c03b68c65aaef5591a942b34c3"},"cell_type":"code","source":"bestDataFeaturesTrain = trainData[feat_labels[indices[1:2]]]\n\n#insert after takeing log of OpenDays feature.\nbestDataFeaturesTrain.insert(loc=0, column='OpenDays', value=openDaysLog)\n\nbestDataFeaturesTrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e42253557d09450db7fa08c0f0b356f5b22f7c7"},"cell_type":"code","source":"# take the natural logarithm of the 'revenue' column in order to make it more easy for model to predict\ny = trainData['revenue'].apply(numpy.log)\n\nfrom sklearn.model_selection import train_test_split\n\nX, y = trainData.iloc[:, 1:40].values, trainData.iloc[:, 40].values\n\nX_train, X_test, y_train, y_test =\\\n    train_test_split(bestDataFeaturesTrain, y, \n                     test_size=0.3, \n                     random_state=0, \n                )\n\n\n\n    \nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cedd73c94423918522d491b56251ee4abbdc14c8"},"cell_type":"code","source":"from sklearn import linear_model\n\n\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\nlinear_predictions = regr.predict(X_test)\n\nlinear_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad1771d30b33f1a0a6b9a095015f30858638fc27"},"cell_type":"code","source":"regr.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"851d20a9ed761698425b48d7b189dc23ead42411"},"cell_type":"code","source":"\nr = []\nfor pair in  zip(linear_predictions, y_test):\n    r.append(pair)\n\nplt.plot(r)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f389077699b9f8f505814576e5903b010e2d9b45"},"cell_type":"markdown","source":"# Conclusion <a name=\"conc\"></a>\n\nDuring this project we learned a lot of skills of Data Science and Machine Learning with using different techniques. \nWe observed the effects of PCA and RBF on our model. Also we had a chance to decide which technique to use for prediction.\nWe choose random forest regressor because it is more usefull in our case of Data among other algorithms. \nWe saw the advantages of standartization on model score also the logarithm. \nWe achived 86% accuracy but we belive that we can improve it."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}