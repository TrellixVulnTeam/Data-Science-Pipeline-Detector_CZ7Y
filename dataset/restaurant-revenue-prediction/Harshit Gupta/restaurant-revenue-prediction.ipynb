{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os,gc\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nfrom math import sqrt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/restaurant-revenue-prediction/train.csv')\ntest = pd.read_csv('../input/restaurant-revenue-prediction/test.csv')\nsample = pd.read_csv('../input/restaurant-revenue-prediction/sampleSubmission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. We have just 137 rows to train the model.\n2. The test data is pretty huge compared to the train data."},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing and EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(n=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. The target column is 'revenue'.\n2. Dataset is anonymised.\n3. This is a regression problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here,\n1. There are no missing values.\n2. We have 4 categorical columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(6,6))\nsns.distplot(train['revenue'], kde=True, bins=20)\nplt.title('Number of Restaurants vs Revenue')\nplt.xlabel('Revenue')\nplt.ylabel('Number of Restaurants')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most restaurant generate revenue between 0.25e7 to 0.5e7. Now, let's see how the city affects the restaurant's revenue,"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['City'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(8,4))\ntrain['City'].value_counts().plot(kind='bar')\nplt.title('No of restaurants vs City')\nplt.xlabel('City')\nplt.ylabel('No of restaurants')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Istanbul has the maximum number of restaurants.\n2. Second is Ankara and then Izimir.\n3. Rest of the cities has less than 10 restaurants."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['City','revenue']].groupby('City').mean().plot(kind='bar')\nplt.title('Mean Revenue Generated vs City')\nplt.xlabel('City')\nplt.ylabel('Mean Revenue Generated')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, \n1. Mean Revenue Generated is over 5M for a few cities.\n2. MRG is between 2M to 4M for most cities.\n3. It is less than 2M for just 2 cities.\n\nWe can't use label encoding on this column,it will mislead the model.We can bin the cities based on Mean Revenue Generated."},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_revenue_per_city = train[['City', 'revenue']].groupby('City', as_index=False).mean()\nmean_revenue_per_city['revenue'] = mean_revenue_per_city['revenue'].apply(lambda x: int(x/1e6)) \nmean_revenue_per_city\n\nmean_dict = dict(zip(mean_revenue_per_city.City, mean_revenue_per_city.revenue))\nmean_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.replace({\"City\":mean_dict}, inplace=True)\ntest.replace({\"City\":mean_dict}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['City'] = test['City'].apply(lambda x: 6 if isinstance(x,str) else x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's see the 'City Group' column."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['City Group'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['City Group'])\nplt.ylabel('No. of Restaurants')\nplt.title('No of Restaurants vs City Group')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of restaurants located in Big Cities is more."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['City Group', 'revenue']].groupby('City Group').mean().plot(kind='bar')\nplt.ylabel('Mean Revenue Generated')\nplt.title('Mean Revenue Generated vs City Group')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean revenue generated by restaurants in 'Big Cities' is close to 5M whereas in 'Other' cities it is close to 4M. We can use label encoding on this column."},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LabelEncoder()\ntrain['City Group'] = lr.fit_transform(train['City Group'])\ntest['City Group'] = lr.transform(test['City Group'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the 'Type' column."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Type'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['Type'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here,\n1. We have three types of restaurants, but in the test set another type 'MB' is present. We'll have to fit the label encoder on the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['Type', 'revenue']].groupby('Type').mean().plot(kind='bar')\nplt.title('Mean Revenue per Type')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Type'] = lr.fit_transform(test['Type'])\ntrain['Type'] = lr.transform(train['Type'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, only 'Open Date' categorical column is left. We'll ignore it for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_correlations = train.drop([\"revenue\"], axis=1).corr()\ntrain_correlations = train_correlations.values.flatten()\ntrain_correlations = train_correlations[train_correlations != 1]\n\ntest_correlations = test.corr()\ntest_correlations = test_correlations.values.flatten()\ntest_correlations = test_correlations[test_correlations != 1]\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_correlations, color=\"Red\", label=\"train\")\nsns.distplot(test_correlations, color=\"Green\", label=\"test\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features in train dataset are highly correlated as compared to the test set. Let's create a baseline and check the most important features using Permutation Importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(train.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['revenue', 'Id', 'Open Date'],axis=1)\ny = train['revenue']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearRegression(normalize=True)\nmodel.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perm = PermutationImportance(model, random_state=1).fit(X,y)\neli5.show_weights(perm, feature_names = X.columns.to_list())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The values towards the top are the most important features, and those towards the bottom matter least. P26, P9, P16, P36, P8, P18 and City are important features. Now, let's plot their graphs."},{"metadata":{"trusted":true},"cell_type":"code","source":"important_features = ['P26', 'P9', 'P16', 'P36', 'P8', 'P18']\n\nf, axes = plt.subplots(3,2, figsize=(12,12), sharex=True)\nf.suptitle('Distribution Plots of Important Features')\n\nfor ax,feature in zip(axes.flatten(), important_features):\n    sns.distplot(X[feature], ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(train[important_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"Brute force feature engineering."},{"metadata":{"trusted":true},"cell_type":"code","source":"important_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['P26_to_City_mean'] = train.groupby('City')['P26'].transform('mean')\ntrain['P9_to_City_mean'] = train.groupby('City')['P9'].transform('mean')\ntrain['P16_to_City_mean'] = train.groupby('City')['P16'].transform('mean')\ntrain['P36_to_City_mean'] = train.groupby('City')['P36'].transform('mean')\ntrain['P8_to_City_mean'] = train.groupby('City')['P8'].transform('mean')\ntrain['P18_to_City_mean'] = train.groupby('City')['P18'].transform('mean')\n\ntest['P26_to_City_mean'] = test.groupby('City')['P26'].transform('mean')\ntest['P9_to_City_mean'] = test.groupby('City')['P9'].transform('mean')\ntest['P16_to_City_mean'] = test.groupby('City')['P16'].transform('mean')\ntest['P36_to_City_mean'] = test.groupby('City')['P36'].transform('mean')\ntest['P8_to_City_mean'] = test.groupby('City')['P8'].transform('mean')\ntest['P18_to_City_mean'] = test.groupby('City')['P18'].transform('mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['P26_to_City_group_mean'] = train.groupby('City Group')['P26'].transform('mean')\ntrain['P9_to_City_group_mean'] = train.groupby('City Group')['P9'].transform('mean')\ntrain['P16_to_City_group_mean'] = train.groupby('City Group')['P16'].transform('mean')\ntrain['P36_to_City_group_mean'] = train.groupby('City Group')['P36'].transform('mean')\ntrain['P8_to_City_group_mean'] = train.groupby('City Group')['P8'].transform('mean')\ntrain['P18_to_City_group_mean'] = train.groupby('City Group')['P18'].transform('mean')\n\ntest['P26_to_City_group_mean'] = test.groupby('City Group')['P26'].transform('mean')\ntest['P9_to_City_group_mean'] = test.groupby('City Group')['P9'].transform('mean')\ntest['P16_to_City_group_mean'] = test.groupby('City Group')['P16'].transform('mean')\ntest['P36_to_City_group_mean'] = test.groupby('City Group')['P36'].transform('mean')\ntest['P8_to_City_group_mean'] = test.groupby('City Group')['P8'].transform('mean')\ntest['P18_to_City_group_mean'] = test.groupby('City Group')['P18'].transform('mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['revenue', 'Id', 'Open Date'],axis=1)\ny = train['revenue']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline Submission"},{"metadata":{},"cell_type":"markdown","source":"Using KFold cross-validation, because the size of training data is very small."},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=10, shuffle=True, random_state=108)\nmodel = LGBMRegressor(n_estimators=200, learning_rate=0.01, subsample=0.7, colsample_bytree=0.8)\n\nscores = []\nfor train_idx, test_idx in cv.split(X):\n    X_train = X.iloc[train_idx]\n    X_val = X.iloc[test_idx]\n    y_train = y.iloc[train_idx]\n    y_val = y.iloc[test_idx]\n    \n    model.fit(X_train,y_train)\n    preds = model.predict(X_val)\n    \n    rmse = sqrt(mean_squared_error(y_val, preds))\n    print(rmse)\n    scores.append(rmse)\n\nprint(\"\\nMean score %d\"%np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(test.drop(['Id', 'Open Date'], axis=1))\nsample['Prediction'] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(predictions, bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}