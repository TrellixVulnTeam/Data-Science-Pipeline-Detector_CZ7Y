{"cells":[{"metadata":{"_cell_guid":"bb3b48f7-0e1c-4474-af64-96f4e7019f8a","_uuid":"c5a994496ad58eab89bbf32da41a95a5c4c56b57"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport math as mh\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# For visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n% config InlineBackend.figure_format = 'png'\n% matplotlib inline\n\n# For data parsing\nfrom datetime import datetime\n\n# For choosing attributes that have good gaussian distribution\nfrom scipy.stats import shapiro\n\n# Needed for getting parameters for models\nfrom sklearn.cross_validation import LeaveOneOut\nfrom sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n\n# Models\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesClassifier\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn import cluster\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# For scaling/normalizing values\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"f14654a2-39ab-48b2-b826-1372aab78354","_uuid":"be8d54871acbff0e965c228f95f5dd26299ef55a"},"source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\nprint(\"Train :\",train.shape)\nprint(\"Test:\",test.shape)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"699fbcfa-2348-4ae6-8f8b-71842f24c763","_uuid":"1c2c681da53bd083f2d253a19faee6854c247fad"},"source":"# Calculate number of samples in training and test datasets\nnum_train = train.shape[0]\nnum_test = test.shape[0]\nprint(num_train, num_test)\n\n# For feature engineering, combine train and test data\ndata = pd.concat((train.loc[:, \"Open Date\" : \"P37\"],\n                  test.loc[:, \"Open Date\" : \"P37\"]), ignore_index=True)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"af735b66-e309-4f71-8e86-1c6eca4282e2","_uuid":"e007c07ffaf5f4011d6169bccd9d307bb29833c3"},"source":"data.tail()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"6c2c342d-96cb-4641-8ace-769aa14a3754","_uuid":"bfd22a66de0868180cd9086073d2178f95114be4"},"source":"#Get name of all headers of data frame\n#list(data)\ndata.columns.values.tolist()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"cdb4a0c6-477f-48f1-93a1-bc6cd4d564b0","_uuid":"05873c89aeec681b5bfd0cf50bba28ede486d270"},"cell_type":"markdown","source":"**Check for Missing values and plot for each column**"},{"metadata":{"_cell_guid":"4618cf09-8181-41a0-86c5-e7dbd22fd2c6","_uuid":"0e9560bf34ce732cbfd588997253edd95b34654e"},"source":"print(data.isnull().sum().T) #No null values in any column, so no imputation and removal of rows\nmissing_df = data.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.loc[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\n\nind = np.arange(missing_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,18))\nrects = ax.barh(ind, missing_df.missing_count.values, color='blue')\nax.set_yticks(ind)\nax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_title(\"Number of missing values in each column\")\nplt.show()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"cdedd0ad-71b1-4fba-aaca-464ad6134f45","_uuid":"b3def039c1c33641fea429880ec84f489cf06a57"},"source":"# Convert date to days\n# Have to drop date \nimport time\nfrom datetime import datetime as dt\n# train\nall_diff = []\nfor date in data[\"Open Date\"]:\n    diff = dt.now() - dt.strptime(date, \"%m/%d/%Y\")\n    all_diff.append(int(diff.days/1000))\n\ndata['Days_from_open'] = pd.Series(all_diff)\nprint(data.head())","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e358274f-9a12-4343-b18e-93051a186acb","_uuid":"48d0d1b21acd2ba87d8bd9692900e085120cede5"},"source":"#Drop Open Date Column\ndata = data.drop('Open Date', 1)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"scrolled":true,"_cell_guid":"e4c49225-dee9-4b1c-9166-2dd99f89c576","_uuid":"a19b6fcf5be0129e15afa9d7d52368d55accf8a3"},"source":"# Plotting mean of P-variables over each city helps us see which P-variables are highly related to City\n# since we are given that one class of P-variables is geographical attributes.\ndistinct_cities = train.loc[:, \"City\"].unique()\n\n# Get the mean of each p-variable for each city\nmeans = []\nfor col in train.columns[5:42]:\n    temp = []\n    for city in distinct_cities:\n        temp.append(train.loc[train.City == city, col].mean())     \n    means.append(temp)\n    \n# Construct data frame for plotting\ncity_pvars = pd.DataFrame(columns=[\"city_var\", \"means\"])\nfor i in range(37):\n    for j in range(len(distinct_cities)):\n        city_pvars.loc[i+37*j] = [\"P\"+str(i+1), means[i][j]]\n        \n# Plot boxplot\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nsns.boxplot(x=\"city_var\", y=\"means\", data=city_pvars)\n\n# From this we observe that P1, P2, P11, P19, P20, P23, and P30 are approximately a good\n# proxy for geographical location.","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1a456fd6-07f1-43d2-8815-1a03e6103c50","_uuid":"a145d232f8a433df8dbe3caf66bf35e658f6f405"},"source":"# K Means treatment for city (mentioned in the paper)\ndef adjust_cities(data, train, k):\n    \n    # As found by box plot of each city's mean over each p-var\n    relevant_pvars =  [\"P1\", \"P2\", \"P11\", \"P19\", \"P20\", \"P23\", \"P30\"]\n    train = train.loc[:, relevant_pvars]\n    \n    # Optimal k is 20 as found by DB-Index plot    \n    kmeans = cluster.KMeans(n_clusters=k)\n    kmeans.fit(train)\n    \n    # Get the cluster centers and classify city of each data instance to one of the centers\n    data['City Cluster'] = kmeans.predict(data.loc[:, relevant_pvars])\n    del data[\"City\"]\n    \n    return data\n\n# Convert unknown cities in test data to clusters based on known cities using KMeans\ndata = adjust_cities(data, train, 20)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"063c627f-6c41-46b5-950d-662568f6bc9d","_uuid":"5cd710ce35609725efc62e12919d529c1f093a13"},"source":"# The two categories of City Group both appear very frequently\nplt.rcParams['figure.figsize'] = (6.0, 6.0)\nsns.countplot(x='City Group', data=train, palette=\"Greens_d\")","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e3abd2f8-a32f-414c-8fe7-8e0d4c9d36d8","_uuid":"6ee823095527b7ae57f716c005dedb9badfed500"},"source":"# One hot encode City Group\ndata = data.join(pd.get_dummies(data['City Group'], prefix=\"CG\"))\n\n# Since only n-1 columns are needed to binarize n categories, drop one of the new columns.  \n# And drop the original columns.\ndata = data.drop([\"City Group\", \"CG_Other\"], axis=1)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"fdc8909b-c5ea-461a-a99b-e3b50e1d1036","_uuid":"4de568b690f7d054be2300bedbbcb9daa9ed3f6c"},"source":"#Check the data type of all columns\ndata.dtypes","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"4a2f8d08-44e0-457d-8272-35b633db41dc","_uuid":"1dc3284aa77b030603f003f16dd47d6300c98ef6"},"source":"#Check the type column \n# Two of the four Restaurant Types (DT and MB), are extremely rare\nsns.countplot(x='Type', data=data, palette=\"Greens_d\")","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"06de99ec-9905-4c4b-bd67-49bb0802bcdd","_uuid":"8f69d7c9a0a4381f46a5627d2b1061ecc8e72473"},"source":"# One hot encode Restaurant Type\ndata = data.join(pd.get_dummies(data['Type'], prefix=\"T\"))\n \n# Drop the original column\ndata = data.drop([\"Type\"], axis=1)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"fb095f07-41a3-40b3-8487-9feb3b4f2ecd","_uuid":"e4567a98f89d8abe933889c09bf29ee1432b32ae"},"source":"data.head()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"78c08065-2794-4179-98cd-16e2c5830355","_uuid":"c0778fa12495d54da13ca85664b675e24536b8ba"},"source":"#Count distinct values for each column in Data frame\ndata.apply(lambda x: len(x.unique()))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"572b1009-ff82-4c07-8f15-c8a99bc9c5d8","_uuid":"bdeb3aad8f2c9dfc1a7ac5dc25e0aacea477bce0"},"source":"# Scale all input features to between 0 and 1.\nmin_max_scaler = MinMaxScaler()\ndata = pd.DataFrame(data=min_max_scaler.fit_transform(data),columns=data.columns, index=data.index)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"356b99ad-e521-407e-b9dc-2572cb52b3f1","_uuid":"dc5eb661c12d650f7db0df4b88768e4e1678eaec"},"source":"train.head()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"2577af7f-c95f-427b-a749-86eb044f62ed","_uuid":"2ea9332bee6ad9acecba3cb3ec6fe9f42c3ce009"},"source":"#Revenue Distribution of Train Set\n# Check distribution of revenue and log(revenue) (Other Transformation could be Sqrt Transformation)\nplt.rcParams['figure.figsize'] = (16.0, 6.0)\npvalue_before = shapiro(train[\"revenue\"])[1]\npvalue_after = shapiro(np.log(train[\"revenue\"]))[1]\ngraph_data = pd.DataFrame(\n        {\n            (\"Revenue\\n P-value:\" + str(pvalue_before)) : train[\"revenue\"],\n            (\"Log(Revenue)\\n P-value:\" + str(pvalue_after)) : np.log(train[\"revenue\"])\n        }\n    )\ngraph_data.hist()\n\n#Shapiro Wilks test for normality\n# log transform revenue as it is approximately normal. If this distribution for revenue holds in the test set,\n# log transforming the variable before training models will improve performance vastly.\n# However, we cannot be completely certain that this distribution will hold in the test set.\ntrain[\"revenue\"] = np.log(train[\"revenue\"])","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8e24c6f2-9a4f-4a9b-a287-48435f080218","_uuid":"e778a05cda532a09f70f19ae8722810c366c9bad"},"source":"# Split into train and test datasets\ntrain_processed = data[:num_train]\ntest_processed = data[num_train:]","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"from sklearn import cross_validation, linear_model,ensemble\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\nfrom sklearn.linear_model.stochastic_gradient import SGDRegressor\nfrom sklearn.svm import SVR\n\nregr = linear_model.LinearRegression()\nregr.get_params()\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_kg_hide-input":false,"_cell_guid":"e421d915-6c43-4580-9369-e8e07d44cd1b","_uuid":"db1262cac127dbf277a663c4fe5d2a48d4d49f46"},"source":"# build model\nfrom sklearn import cross_validation,linear_model,ensemble\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import make_scorer, mean_squared_error\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\nfrom sklearn.linear_model.stochastic_gradient import SGDRegressor\nfrom sklearn.svm import SVR\n\n# simple regression\nprint(\"Simple regression\")\n\n#create linear regression model object\nregr = linear_model.LinearRegression()\n#regr.get_params() -- Check the list of paramters for the given model\n\n# create a parameter grid: map the parameter names to the values that should be searched\nparameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n\ndef RMSE(y_true,y_pred):\n    rmse = mh.sqrt(mean_squared_error(y_true, y_pred))\n    print('RMSE: %2.3f' % rmse)\n    return rmse\n\n'''def R2(y_true,y_pred):    \n     r2 = r2_score(y_true, y_pred)\n     print('R2: %2.3f' % r2)\n     return r2\n'''\n    \ndef two_score(y_true,y_pred):\n    score = RMSE(y_true,y_pred) #set score here and not below if using MSE in GridCV\n    #score = R2(y_true,y_pred)\n    return score\n\nmy_score = make_scorer(two_score, greater_is_better=False) # change for false if using MSE\n\n# instantiate the grid\ngrid = GridSearchCV(regr, parameters, cv=LeaveOneOut(train.shape[0]), scoring='mean_squared_error')\n\n# fit the grid with data\ngrid.fit(train_processed, train[\"revenue\"])\n\n# Re-train on full training set using the best parameters found in the last step.\n# examine the best model\nprint(\"Best score :\",grid.best_score_)\nprint(\"Best params :\",grid.best_params_)\nprint(\"Best estimator:\",grid.best_estimator_)\nregr.set_params(**grid.best_params_)\nregr.fit(train_processed, train[\"revenue\"])\n\n# results\nresults_regr = regr.predict(test_processed)\nresults_regr_exp=np.exp(results_regr)\nprint(results_regr_exp)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"submission_lin_reg = pd.DataFrame(columns=['Prediction'],index=test.index, data=results_regr_exp)\nsubmission_lin_reg.index.name = 'Id'","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true},"source":"submission_lin_reg.describe().astype(int)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"# Ridge model\nmodel_grid = [{'normalize': [True, False], 'alpha': np.logspace(0,10)}]\nridge_clf = Ridge()\n\n# Use a grid search and leave-one-out CV on the train set to find the best regularization parameter to use.\ngrid = GridSearchCV(ridge_clf, model_grid, cv=LeaveOneOut(train.shape[0]), scoring='mean_squared_error')\ngrid.fit(train_processed, train[\"revenue\"])\n\n# Re-train on full training set using the best parameters found in the last step.\n# examine the best model\nprint(\"Best score :\",grid.best_score_)\nprint(\"Best params :\",grid.best_params_)\nprint(\"Best estimator:\",grid.best_estimator_)\nridge_clf.set_params(**grid.best_params_)\nridge_clf.fit(train_processed, train[\"revenue\"])\n\n# results_ridge = np.exp(ridge_clf.predict(test_processed))\nresults_ridge = ridge_clf.predict(test_processed)\nresults_ridge_exp=np.exp(results_ridge)\nprint(results_ridge_exp)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"# Lasso model\nmodel_grid = [{'normalize': [True, False], 'alpha': np.logspace(0,10)}]\nlasso_clf = Lasso()\n\n# Use a grid search and leave-one-out CV on the train set to find the best regularization parameter to use.\ngrid = GridSearchCV(lasso_clf, model_grid, cv=LeaveOneOut(train.shape[0]), scoring='mean_squared_error')\ngrid.fit(train_processed, train[\"revenue\"])\n\n# Re-train on full training set using the best parameters found in the last step.\nprint(\"Best score :\",grid.best_score_)\nprint(\"Best params :\",grid.best_params_)\nprint(\"Best estimator:\",grid.best_estimator_)\nlasso_clf.set_params(**grid.best_params_)\nlasso_clf.fit(train_processed, train[\"revenue\"])\n\n#Predict the test set\nresults_lasso = lasso_clf.predict(test_processed)\nresults_lasso_exp = np.exp(results_lasso)\nprint(results_lasso_exp)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"#SVR()\nfrom sklearn.svm import SVR, LinearSVR\n\nsvr = SVR(C=1, epsilon=0.1)\nsvr.fit(train_processed, train[\"revenue\"])\nresults_svm = svr.predict(test_processed)\nresults_svm_exp = np.exp(results_svm)\nprint(results_svm_exp)","execution_count":null,"cell_type":"code","outputs":[]}],"nbformat":4,"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","version":"3.6.1"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1}