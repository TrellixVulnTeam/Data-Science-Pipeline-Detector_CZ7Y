{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 分析内容\n1. データの概要確認\n1. 目的変数の確認\n    1. 目的変数であるrevenueの要約統計量を表示\n1. 説明変数の確認\n    1. 名義変数の分布確認\n    1. 数値変数の分布確認\n    1. 名義変数とターゲットの関係を確認\n    1. 数値変数とターゲットの関係を確認\n1. 新たな特徴量の作成\n1. カテゴリー変数のエンコード\n1. 外れ値の調査\n1. 数値型変数の歪度を確認\n1. 数値変数の相関を確認\n1. モデルの相互検証\n1. パラメータチューニング\n1. モデル学習\n1. スコア提出"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv('/kaggle/input/restaurant-revenue-prediction/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/restaurant-revenue-prediction/test.csv.zip')\n\n# Idは不要なので、削除して別に変数化し、スコア提出時に使用\ntrain_Id = train.Id\ntest_Id = test.Id\n\n# Id列削除\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## 必要なライブラリをインポート"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#importing the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# import seaborn as sns\nimport seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n\nfrom datetime import datetime\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\n# 最大カラム数を100に拡張(デフォルトだと省略されてしまうので)\n# 常に全ての列（カラム）を表示\npd.options.display.max_columns = None\npd.options.display.max_rows = 80\n\n# 小数点2桁で表示(指数表記しないように)\npd.options.display.float_format = '{:.2f}'.format\n%matplotlib inline\n#ワーニングを抑止\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* id ：レストランID。\n* オープン日 ：レストランのオープン日\n* City： レストランがある都市。名前にはUnicodeが含まれていることに注意。\n* 都市グループ： 都市のタイプ。大都市、またはその他。\n* タイプ：レストランのタイプ。FC：フードコート 、IL：インライン、DT：ドライブスルー、MB：モバイル\n* P1、P2-P37：これら の難読化されたデータには3つのカテゴリがある。\n    1. 人口統計データは、GISシステムを使用してサードパーティのプロバイダーから収集される。これらには、特定の地域の人口、年齢と性別の分布、開発スケールが含まれる。\n    1. 不動産データは、主に場所のm2、場所の正面ファサード、駐車場の空き状況に関連している。\n    1. 商業データには、主に学校、銀行、その他のQSRスキャナーを含む関心のあるポイントの存在が含まれる。\n* 収益： 収益の列は、特定の年のレストランの（変換された）収益を示し、予測分析のターゲットである。\n"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print('Size of train data', train.shape)\nprint('Size of test data', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.データの概要確認"},{"metadata":{},"cell_type":"markdown","source":"## データのレコード数とカラム数を確認"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## データのデータ定義を確認"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## データの数値項目の統計要約量を表示"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## オブジェクト型のレコード数、ユニーク数、最頻値の出現回数を表示"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.describe(include='O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# import pandas_profiling as pdp\n# pdp.ProfileReport(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.目的変数の確認"},{"metadata":{},"cell_type":"markdown","source":"## 目的変数であるrevenueの要約統計量を表示"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train[\"revenue\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#目的変数であるrevenueのヒストグラムとQ-Qプロットを表示する\n# 分布確認\nfig = plt.figure(figsize=(10, 4))\nplt.subplots_adjust(wspace=0.4)\n\n# ヒストグラム\nax = fig.add_subplot(1, 2, 1)\nsns.distplot(train['revenue'], ax=ax)\n\n# QQプロット\nax2 = fig.add_subplot(1, 2, 2)\nstats.probplot(train['revenue'], plot=ax2)\n\nplt.show()\n\n# 変換後の要約統計量表示\nprint(train['revenue'].describe())\nprint(\"------------------------------\")\nprint(\"歪度: %f\" % train['revenue'].skew())\nprint(\"尖度: %f\" % train['revenue'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"線形回帰モデルも使用するかもしれないので、対数変換で正規分布に近づける。"},{"metadata":{},"cell_type":"markdown","source":"## revenueを対数変換"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 学習データをコピーし、新たなdataframeで検証\ndf = train.copy()\n\n#目的変数の対数log(x+1)をとる\ndf['revenue'] = np.log1p(df['revenue'])\n\n# 標準化(平均0, 分散1)\nscaler=StandardScaler()\ndf['revenue']=scaler.fit_transform(df[['revenue']])\n\n# 分布確認\nfig = plt.figure(figsize=(10, 4))\nplt.subplots_adjust(wspace=0.4)\n\n# ヒストグラム\nax = fig.add_subplot(1, 2, 1)\nsns.distplot(df['revenue'], ax=ax)\n\n# QQプロット\nax2 = fig.add_subplot(1, 2, 2)\nstats.probplot(df['revenue'], plot=ax2)\n\nplt.show()\n\n# 変換後の要約統計量表示\nprint(df['revenue'].describe())\nprint(\"------------------------------\")\nprint(\"歪度: %f\" % df['revenue'].skew())\nprint(\"尖度: %f\" % df['revenue'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## revenueを標準化(平均0, 分散1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 学習データをコピーし、新たなdataframeで検証\ndf = train.copy()\n\n# 標準化(平均0, 分散1)\nscaler=StandardScaler()\ndf['revenue']=scaler.fit_transform(df[['revenue']])\n\n\n# 分布確認\nfig = plt.figure(figsize=(10, 4))\nplt.subplots_adjust(wspace=0.4)\n\n# ヒストグラム\nax = fig.add_subplot(1, 2, 1)\nsns.distplot(df['revenue'], ax=ax)\n\n# QQプロット\nax2 = fig.add_subplot(1, 2, 2)\nstats.probplot(df['revenue'], plot=ax2)\n\nplt.show()\n\n# 変換後の要約統計量表示\nprint(df['revenue'].describe())\nprint(\"------------------------------\")\nprint(\"歪度: %f\" % df['revenue'].skew())\nprint(\"尖度: %f\" % df['revenue'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## revenueを正規化(最大1, 最小0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 学習データをコピーし、新たなdataframeで検証\ndf = train.copy()\n\n# Min-Max変換(正規化(最大1, 最小0))\nscaler=MinMaxScaler()\ndf['revenue']=scaler.fit_transform(df[['revenue']])\n\n# 分布確認\nfig = plt.figure(figsize=(10, 4))\nplt.subplots_adjust(wspace=0.4)\n\n# ヒストグラム\nax = fig.add_subplot(1, 2, 1)\nsns.distplot(df['revenue'], ax=ax)\n\n# QQプロット\nax2 = fig.add_subplot(1, 2, 2)\nstats.probplot(df['revenue'], plot=ax2)\n\nplt.show()\n\n# 変換後の要約統計量表示\nprint(df['revenue'].describe())\nprint(\"------------------------------\")\nprint(\"歪度: %f\" % df['revenue'].skew())\nprint(\"尖度: %f\" % df['revenue'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"対数変換が一番良いので、対数変換することにする\n"},{"metadata":{},"cell_type":"markdown","source":"## オープン日を日付形式に変換し、オープン年とオープン月に分ける"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 学習データ\n# Open Dateを日付型に変換\ntrain['pd_date'] = pd.to_datetime(train['Open Date'], format='%m/%d/%Y')\n# 年のみを抽出\ntrain['Open_Year'] = train['pd_date'].dt.strftime('%Y')\n# 月のみを抽出\ntrain['Open_Month'] = train['pd_date'].dt.strftime('%m')\n\ntrain = train.drop('pd_date',axis=1)\ntrain = train.drop('Open Date',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# テストデータ\n# Open Dateを日付型に変換\ntest['pd_date'] = pd.to_datetime(test['Open Date'], format='%m/%d/%Y')\n# 年のみを抽出\ntest['Open_Year'] = test['pd_date'].dt.strftime('%Y')\n# 月のみを抽出\ntest['Open_Month'] = test['pd_date'].dt.strftime('%m')\n\ntest = test.drop('pd_date',axis=1)\ntest = test.drop('Open Date',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.説明変数の確認"},{"metadata":{},"cell_type":"markdown","source":"##  データ型を確認"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#カテゴリ変数と数値変数に分ける\ncats = list(train.select_dtypes(include=['object']).columns)\nnums = list(train.select_dtypes(exclude=['object']).columns)\nprint(f'categorical variables:  {cats}')\nprint(f'numerical variables:  {nums}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ユニーク数を表示"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.nunique(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 値の追加\n# cats.extend([''])\n\n# 値の削除\n# nums.remove('')\n\nprint(f'categorical variables:  {cats}')\nprint(f'numerical variables:  {nums}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 名義変数、順序変数、連続変数に分ける\nカテゴリ変数は全て名義変数と定義する\n"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# 名義変数\nnominal_list =cats\n               \n# 順序変数\n# ordinal_list = []\n\n# 数値変数\nnum_list = nums","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 名義変数の分布確認"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"columns = len(nominal_list)/2+1\n\nfig = plt.figure(figsize=(30, 20))\nplt.subplots_adjust(hspace=0.6, wspace=0.4)\n\nfor i in range(len(nominal_list)):\n    ax = fig.add_subplot(columns, 2, i+1)\n    sns.countplot(x=nominal_list[i], data=train, ax=ax)\n    plt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 数値変数の分布確認"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = len(num_list)/3+1\n\nfig = plt.figure(figsize=(30, 40))\nplt.subplots_adjust(hspace=0.6, wspace=0.4)\n\nfor i in range(len(num_list)):\n    ax = fig.add_subplot(columns, 3, i+1)\n\n    train[num_list[i]].hist(ax=ax)\n    ax2 = train[num_list[i]].plot.kde(ax=ax, secondary_y=True,title=num_list[i])\n    ax2.set_ylim(0)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 名義変数とターゲットの関係を確認\n名義変数として分けた変数の中で、重み付けが必要な変数が無いか確認"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"columns = len(nominal_list)/2+1\n\nfig = plt.figure(figsize=(20, 10))\nplt.subplots_adjust(hspace=0.6, wspace=0.4)\n\nfor i in range(len(nominal_list)):\n    ax = fig.add_subplot(columns, 2, i+1)\n\n    # 回帰の場合    \n    sns.boxplot(x=nominal_list[i], y=train.revenue, data=train, ax=ax)\n    plt.xticks(rotation=45)\n    # 分類の場合\n#     sns.barplot(x = nominal_list[i], y = train.revenue, data=train, ax=ax)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"オープン月はあまり差異がみられないため、削除する"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop('Open_Month',axis=1)\ntest= test.drop('Open_Month',axis=1)\nnominal_list.remove('Open_Month')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 数値変数とターゲットの関係を確認"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"columns = len(num_list)/4+1\n\nfig = plt.figure(figsize=(30, 35))\nplt.subplots_adjust(hspace=0.6, wspace=0.4)\n\nfor i in range(len(num_list)):\n    ax = fig.add_subplot(columns, 4, i+1)\n\n    # 回帰の場合    \n    sns.regplot(x=num_list[i],y='revenue',data=train, ax=ax)\n    plt.xticks(rotation=45)\n    # 分類の場合\n#     sns.barplot(x = nominal_list[i], y = train.revenue, data=train, ax=ax)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.新たな特徴量の作成"},{"metadata":{},"cell_type":"markdown","source":"## Cityごとのrevenueの平均値を算出"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['City','revenue']].groupby('City').mean().plot(kind='bar')\nplt.title('Mean Revenue Generated vs City')\nplt.xlabel('City')\nplt.ylabel('Mean Revenue Generated')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 一部の都市では、平均収益が500万を超えている。\n* ほとんどの都市で200万から400万の間である。\n* 2つの都市で200万未満である。\n* 線形な値となっていなので、この列でラベルエンコーディングを使用することはできない。\n* 生成された平均収益に基づいて都市をビニング(平均収益を100万単位)する。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cityごとのrevenue平均値を1000000単位とする\nmean_revenue_per_city = train[['City', 'revenue']].groupby('City', as_index=False).mean()\nmean_revenue_per_city.head()\nmean_revenue_per_city['revenue'] = mean_revenue_per_city['revenue'].apply(lambda x: int(x/1e6)) \n\nmean_revenue_per_city\n\nmean_dict = dict(zip(mean_revenue_per_city.City, mean_revenue_per_city.revenue))\nmean_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# city_rev = []\n\n# for i in train['City']:\n#     for key, value in mean_dict.items():\n#         if i == key:\n#             city_rev.append(value)\n            \n# df_city_rev = pd.DataFrame({'city_rev':city_rev})\n# train = pd.concat([train,df_city_rev],axis=1)\n# train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.replace({\"City\":mean_dict}, inplace=True)\n# test.replace({\"City\":mean_dict}, inplace=True)\n# test['City'] = test['City'].apply(lambda x: 6 if isinstance(x,str) else x)\n\n# train['City_rev'] = train['City']\n# test['City_rev'] = test['City']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 学習データとテストデータにてCity差異がないか確認"},{"metadata":{},"cell_type":"markdown","source":"## 学習データに存在するCityを表示\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(train['City'].sort_values().unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## テストデータに存在するCityを表示"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['City'].sort_values().unique()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cityについて、学習データとテストデータを重複削除し、リスト化"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cityについて、学習データとテストデータにて重複削除し、リスト化\ncity_train_list = list(train['City'].unique())\ncity_test_list = list(test['City'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 共通のCityを抽出"},{"metadata":{"trusted":true},"cell_type":"code","source":"l1_l2_and = set(city_train_list) & set(city_test_list)\nprint(l1_l2_and)\nprint(len(l1_l2_and))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## どちらかにしかないCityを抽出\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# どちらかにしかないCityを抽出\nl1_l2_sym_diff = set(city_test_list) ^ set(city_train_list)\nprint(l1_l2_sym_diff)\nprint(len(l1_l2_sym_diff))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## テストデータのみ存在するCityの件数\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# テストデータのみ存在するCityの件数\nlen(set(city_test_list).difference(city_train_list))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## 学習データのみ存在するCityの件数\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 学習データのみ存在するCityの件数\nlen(set(city_train_list).difference(city_test_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"テストデータにしかないCityが29件存在するため、クラスタリングを使用し、存在しないCityについて補完する。"},{"metadata":{},"cell_type":"markdown","source":"P変数(P1～P37)のうち、どれかが地理的属性であると記載されているため、CityごとにP変数の変数の平均値の平均をとり、変化量が多い(分散が大きい)P変数を使用し(それが地理的属性を表しているとみなし)、クラスタリングする。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# P変数の1つのクラスは地理的属性であると指定されているため\n# 各都市のP変数の平均をプロットすると、どのP変数が都市と関連性が高いかが分かる\ndistinct_cities = train.loc[:, \"City\"].unique()\n\n# P変数のcityごとの平均値を取得\nmeans = []\nfor i in range(len(num_list)):\n    temp = []\n    for city in distinct_cities:\n        temp.append(train.loc[train.City == city, num_list[i]].mean())  \n    means.append(temp)\n    \ncity_pvars = pd.DataFrame(columns=[\"city_var\", \"means\"])\nfor i in range(37):\n    for j in range(len(distinct_cities)):\n        city_pvars.loc[i+37*j] = [\"P\"+str(i+1), means[i][j]]\n\nprint(city_pvars)            \n# 箱ひげ図を表示\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nsns.boxplot(x=\"city_var\", y=\"means\", data=city_pvars)\n\n# From this we observe that P1, P2, P11, P19, P20, P23, and P30 are approximately a good\n# proxy for geographical location.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import cluster\n\ndef adjust_cities(full_full_data, train, k):\n    \n    # As found by box plot of each city's mean over each p-var\n    relevant_pvars =  [\"P1\", \"P2\", \"P11\", \"P19\", \"P20\", \"P23\",\"P30\"]\n    train = train.loc[:, relevant_pvars]\n    \n    # Optimal k is 20 as found by DB-Index plot    \n    kmeans = cluster.KMeans(n_clusters=k)\n    kmeans.fit(train)\n    \n    # Get the cluster centers and classify city of each full_data instance to one of the centers\n    full_data['City_Cluster'] = kmeans.predict(full_data.loc[:, relevant_pvars])\n    \n    return full_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 学習デートとテストデータを集約"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train = train.shape[0]\nnum_test = test.shape[0]\nprint(num_train, num_test)\n\nfull_data = pd.concat([train, test], ignore_index=True)                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 学習データを使用しクラスタリングを行い、その学習結果を全データに適用させる\nfull_data = adjust_cities(full_data, train, 20)\nfull_data\n\n# City項目は不要なので削除\nfull_data = full_data.drop(['City'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 再度データを分割"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into train and test datasets\ntrain = full_data[:num_train]\ntest = full_data[num_train:]\n# check the shapes \nprint(\"Train :\",train.shape)\nprint(\"Test:\",test.shape)\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['City_Cluster','revenue']].groupby('City_Cluster').mean().plot(kind='bar')\nplt.title('Mean Revenue Generated vs City Cluster')\nplt.xlabel('City Cluster')\nplt.ylabel('Mean Revenue Generated')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_revenue_per_city = train[['City_Cluster', 'revenue']].groupby('City_Cluster', as_index=False).mean()\nmean_revenue_per_city.head()\nmean_revenue_per_city['revenue'] = mean_revenue_per_city['revenue'].apply(lambda x: int(x/1e6)) \n\nmean_revenue_per_city\n\nmean_dict = dict(zip(mean_revenue_per_city.City_Cluster, mean_revenue_per_city.revenue))\nmean_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"city_rev = []\n\nfor i in full_data['City_Cluster']:\n    for key, value in mean_dict.items():\n        if i == key:\n            city_rev.append(value)\n            \ndf_city_rev = pd.DataFrame({'city_rev':city_rev})\nfull_data = pd.concat([full_data,df_city_rev],axis=1)\nfull_data.head\n\n# 値の追加\nnominal_list.extend(['City_Cluster'])\n# 値の削除\nnominal_list.remove('City')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.カテゴリー変数のエンコード\nカテゴリが2つしかないカテゴリ変数にはラベルエンコーディングを使用し、カテゴリが3つ以上のカテゴリ変数にはワンホットエンコーディングを使用する"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\n# for col in application_full_data:\nfor i in range(len(nominal_list)):    \n    \n#     if application_full_data[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(full_data[nominal_list[i]].unique())) <= 2:\n            # full_data on the full_dataing data\n            le.fit(full_data[nominal_list[i]])\n            # Transform both full_dataing and testing data\n            full_data[nominal_list[i]] = le.transform(full_data[nominal_list[i]])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ラベルエンコードした変数以外の数値型以外の変数をone-hot encodingする。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encoding of categorical variables\nfull_data = pd.get_dummies(full_data)\nprint('full_dataing Features shape: ', full_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.外れ値の調査"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tukey_outliers(x):\n    q1 = np.percentile(x,25)\n    q3 = np.percentile(x,75)\n    \n    iqr = q3-q1\n    \n    min_range = q1 - iqr*1.5\n    max_range = q3 + iqr*1.5\n    \n    outliers = x[(x<min_range) | (x>max_range)]\n    return outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 外れ値の詳細レコードを表示\n# for col in num_list:\n#     outliers = tukey_outliers(train[col])\n#     if len(outliers):\n#         print(f\"* {col} has these tukey outliers,\\n{outliers}\\n\")\n#     else:\n#         print(f\"* {col} doesn't have any tukey outliers.\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.iloc[list(tukey_outliers(df_num.acceleration).index)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 外れ値を表示"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"columns = len(num_list)/4+1\n\n# boxplot\nfig = plt.figure(figsize=(15,20))\nplt.subplots_adjust(hspace=0.2, wspace=0.8)\nfor i in range(len(num_list)):\n    ax = fig.add_subplot(columns, 4, i+1)\n    sns.boxplot(y=full_data[num_list[i]], data=full_data, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 外れ値を変換　←今回は実施しない\n1.5IQR超える数値は95%tile値で埋める、下回る数値は5%tile値で埋める"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# 学習データを置き換え\n# for i in range(len(num_list)):\n#      # 置き換え値\n#     upper_lim = full_data[num_list[i]].quantile(.95)\n#     lower_lim = full_data[num_list[i]].quantile(.05)\n    \n#     # IQR\n#     Q1 = full_data[num_list[i]].quantile(.25)\n#     Q3 = full_data[num_list[i]].quantile(.75)\n#     IQR = Q3 - Q1\n#     outlier_step = 1.5 * IQR\n    \n#     # 1.5IQR超える数値は95%tile値で埋める、下回る数値は5%tile値で埋める\n#     full_data.loc[(full_data[num_list[i]] > (Q3 + outlier_step)), num_list[i]] =upper_lim\n#     full_data.loc[(full_data[num_list[i]] < (Q1 - outlier_step)), num_list[i]] = lower_lim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns = len(num_list)/4+1\n\n# # boxplot\n# fig = plt.figure(figsize=(15,20))\n# plt.subplots_adjust(hspace=0.2, wspace=0.8)\n# for i in range(len(num_list)):\n#     ax = fig.add_subplot(columns, 4, i+1)\n#     sns.boxplot(y=full_data[num_list[i]], data=full_data, ax=ax)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7.数値型変数の歪度を確認"},{"metadata":{},"cell_type":"markdown","source":"* 数値型変数の歪度を調べて、歪度が一定値を超える変数を対数変換する\n* 歪度が10以上の変数を対象に対数変換で左右対称に近づける"},{"metadata":{"trusted":true},"cell_type":"code","source":"skewed_data = train[num_list].apply(lambda x: skew(x)).sort_values(ascending=False)\nskewed_data[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"今回は、10以上の変数が存在しないため処理の必要なし"},{"metadata":{"trusted":true},"cell_type":"code","source":"# skew_col = skewed_data[skewed_data > 10].index\n\n# # 可視化\n# fig = plt.figure(figsize=(10, 8))\n# for i in range(len(skew_col)):\n#     ax = fig.add_subplot(2, 3, i+1)\n#     try:\n#         sns.distplot(combined_df[skew_col[i]], fit=norm, ax=ax)\n#     except:\n#         # kde計算できない時は、kde=False\n#         sns.distplot(combined_df[skew_col[i]], fit=norm, kde=False, ax=ax)\n# plt.show()\n\n# # 対数変換\n# for i in range(len(skew_col)):\n#     combined_df[skew_col[i]] = np.log1p(combined_df[skew_col[i]])\n    \n#     # 可視化\n# # 可視化\n# fig = plt.figure(figsize=(10, 8))\n# for i in range(len(skew_col)):\n#     ax = fig.add_subplot(2, 3, i+1)\n#     try:\n#         sns.distplot(combined_df[skew_col[i]], fit=norm, ax=ax)\n#     except:\n#         # kde計算できない時は、kde=False\n#         sns.distplot(combined_df[skew_col[i]], fit=norm, kde=False, ax=ax)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## データを学習データとテストデータに再分割"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into train and test datasets\ntrain = full_data[:num_train]\ntest = full_data[num_train:]\n# check the shapes \nprint(\"Train :\",train.shape)\nprint(\"Test:\",test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8.数値変数の相関を確認"},{"metadata":{},"cell_type":"markdown","source":"## 連続変数間の関係：連続変数間で相関の高い変数の確認"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.set(font_scale=1.1)\ncorrelation_train = train.corr()\nmask = np.triu(correlation_train.corr())\nfig = plt.figure(figsize=(50,50))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n#             mask=mask,\n            linewidths=1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## revenueと相関の高い変数トップ10を確認\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = train.corr()['revenue'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 相関が高い10項目のみ抽出\ncorrelations = train.corr()\n# 絶対値で取得\ncorrelations = abs(correlations)\n\ncols = correlations.nlargest(10,'revenue')['revenue'].index\ncols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 相関が高い10項目のみ抽出\ntrain = train[cols]\n\n#学習データを目的変数とそれ以外に分ける\ntrain_X = train.drop(\"revenue\",axis=1)\ntrain_y = train[\"revenue\"]\n\n#revenueを対数変換する \ntrain_y = np.log1p(train_y)\n\n#テストデータを学習データのカラムのみにする \ntmp_cols = train_X.columns\ntest_X = test[tmp_cols]\n\n#それぞれのデータのサイズを確認\nprint(\"train_X: \"+str(train_X.shape))\nprint(\"train_y: \"+str(train_y.shape))\nprint(\"test_X: \"+str(test_X.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#訓練データとモデル評価用データに分けるライブラリ\nfrom sklearn.model_selection import train_test_split\n\n#フォールドアウト法により、学習データとテストデータに分割 \n(X_train, X_test, y_train, y_test) = train_test_split(train_X, train_y , test_size = 0.3 , random_state = 0)\n\nprint(\"X_train: \"+str(X_train.shape))\nprint(\"X_test: \"+str(X_test.shape))\nprint(\"y_train: \"+str(y_train.shape))\nprint(\"y_test: \"+str(y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9.モデルの相互検証"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### リッジ回帰(Ridge)\n* 基本は通常の線形回帰\n* 過学習を抑制するために重みに対してペナルティが与えられる\n* ペナルティには L2 正則化 が使われる\n* 突出した重みが出にくくなる\n* トレーニングデータが少ない場合に有効\n* トレーニングデータが大量にある場合には効果が薄くなる\n\n### ラッソ回帰(Lasso)\n* 基本は通常の線形回帰\n* 過学習を抑制するために重みに対してペナルティが与えられる\n* ペナルティには L1 正則化 が使われる\n* いくつかの重みが完全に０となる\n* 重みが０となった特徴量の入力は無視される\n* 特徴量が多く、重要なものがわずかしかないと予想される場合に向いている\n* 突出した重みが出にくくなる\n\n### ElasticNet(ElasticNet)\n* リッジ回帰とLassoが組み合わさった回帰。\n* 基本は通常の線形回帰\n* 過学習を抑制するために重みに対してペナルティが与えられる\n* 正則化としての L1 と L2 が組み合わされたもの\n\n### K-近傍法(KNeighborsRegressor)\n* 通称 K-NN（K-Nearest Neighbor Algorithm の略称）\n* 特徴空間上において、近くにある K個 オブジェクトのうち、最も一般的なクラスに分類する。\n* 距離の算出には、一般的にユークリッド距離が使われる。（他にマンハッタン距離などがある）\n\n### サポートベクター回帰(SVR)\n### 勾配ブースティング(GradientBoostingRegressor)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#機械学習モデルをリストに格納\nrandom_state = 2\nclassifiers = []\nclassifiers.append(Lasso(random_state=random_state))\nclassifiers.append(LinearRegression())\nclassifiers.append(Ridge(random_state=random_state))\nclassifiers.append(ElasticNet(random_state=random_state))\nclassifiers.append(KNeighborsRegressor())\nclassifiers.append(SVR())\nclassifiers.append(RandomForestRegressor(random_state=random_state))\nclassifiers.append(GradientBoostingRegressor())\nclassifiers.append(AdaBoostRegressor(random_state = random_state))\nclassifiers.append(DecisionTreeRegressor())\nclassifiers.append(XGBRegressor())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#複数のclassifier の適用\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y_train, scoring='neg_mean_squared_error', cv =10, n_jobs=4))\n\n#適用したclassifierのスコアを取得    \ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"Lasso\",\"LinearRegression\",\"Ridge\",\n\"ElasticNet\",\"KNeighborsRegressor\",\"SVR\",\"RandomForestRegressor\",\"GradientBoostingRegressor\",\"AdaBoostRegressor\",\"DecisionTreeRegressor\", \"XGBRegressor\"]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_res.sort_values(ascending=False, by='CrossValMeans')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 10.パラメータチューニング\n* Optunaを使用してみる\n* Optuna はハイパーパラメータの最適化を自動化するためのソフトウェアフレームワーク。\n* ハイパーパラメータの値に関する試行錯誤を自動的に行いながら、優れた性能を発揮するハイパーパラメータの値を自動的に発見する\n* Tree-structured Parzen Estimator というベイズ最適化アルゴリズムの一種を用いている。\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport optuna\n \ndef objective(trial):\n    params = {\n        'alpha': trial.suggest_loguniform(\"alpha\", 0.1, 5), \n        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n        'normalize': trial.suggest_categorical('normalize', [True, False]),\n    }\n \n    reg = Ridge(**params)\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n \n    mae = mean_absolute_error(y_test, y_pred)\n    return mae\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optuna によるハイパーパラメータ最適化\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)\n\n# 結果を表示\nprint(f'best score: {study.best_value:.4f}, best params: {study.best_params}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 11.モデル学習"},{"metadata":{},"cell_type":"markdown","source":"## Optunaで最適化されたパラメータ使用し、モデル学習する\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'alpha': 1.9510706324753746, 'fit_intercept': True, 'normalize': True}\n\nreg = Ridge(**params)\nreg.fit(X_train, y_train)\nprediction_log = reg.predict(test_X)\nprediction =np.exp(prediction_log) \nprint(prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 12.テストデータにて予測しスコア提出"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 予測した値を提出用CSVファイル(submissionファイル)に書き出し\nsubmission = pd.DataFrame({\"Id\":test_Id, \"Prediction\":prediction})\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #LightGBMライブラリ\n# import lightgbm as lgb\n# #ハイパーパラメータチューニング自動化ライブラリ\n# import optuna\n\n# lgb_train = lgb.Dataset(X_train, y_train)\n# lgb_eval = lgb.Dataset(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def objective(trial):\n#     params = {'metric': {'rmse'},\n#               'max_depth' : trial.suggest_int('max_depth', 1, 10),\n#               'subsumple' : trial.suggest_uniform('subsumple', 0.0, 1.0),\n#               'subsample_freq' : trial.suggest_int('subsample_freq', 0, 1),\n#               'leaning_rate' : trial.suggest_loguniform('leaning_rate', 1e-5, 1),\n#               'feature_fraction' : trial.suggest_uniform('feature_fraction', 0.0, 1.0),\n#               'lambda_l1' : trial.suggest_uniform('lambda_l1' , 0.0, 1.0),\n#               'lambda_l2' : trial.suggest_uniform('lambda_l2' , 0.0, 1.0)}\n \n#     gbm = lgb.train(params,\n#                     lgb_train,\n#                     valid_sets=(lgb_train, lgb_eval),\n#                     num_boost_round=10000,\n#                     early_stopping_rounds=100,\n#                     verbose_eval=50)\n#     predicted = gbm.predict(X_test)\n#     RMSE = np.sqrt(mean_squared_error(y_test, predicted))\n    \n#     pruning_callback = optuna.integration.LightGBMPruningCallback(trial, 'rmse')\n#     return RMSE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# study = optuna.create_study()\n# study.optimize(objective, timeout=360)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Best trial:')\n# trial = study.best_trial\n# print('Value:{}'.format(trial.value))\n# print('Params:')\n# for key, value in trial.params.items():\n#     print('\"{}\" : {}'.format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# #Optunaで最適化されたパラメータ\n# params = {\"metric\": {'rmse'},\n#           \"max_depth\" : 7,\n#           \"subsumple\" : 0.0527053286950852,\n#           \"subsample_freq\" : 0,\n#           \"leaning_rate\" : 0.00012337315517641352,\n#           \"feature_fraction\" : 0.27094712699951107,\n#           \"lambda_l1\" : 0.4567708349707908,\n#           \"lambda_l2\" :6.452511288039886e-07\n#          }\n \n# #LightGBMのモデル構築\n# gbm = lgb.train(params,\n#                 lgb_train,\n#                 valid_sets=(lgb_train, lgb_eval),\n#                 num_boost_round=10000,\n#                 early_stopping_rounds=100,\n#                 verbose_eval=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #特徴量の重要度\n# lgb.plot_importance(gbm, height=0.5, figsize=(8,16))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# テストデータにて予測\n# prediction_log = gbm.predict(test_X)\n# print(prediction_log)\n# prediction =np.exp(prediction_log) \n# print(prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}