{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Predict Restaurant Revenue Using Regression**\n**By : Garry Ariel**"},{"metadata":{},"cell_type":"markdown","source":"We will try to predict restaurant revenue based on given information. The steps can be divided into the following.\n1. Analyzing and preprocessing the dataset\n2. Feed the data into regression model\n3. Use trained regression model from previous step to predict other restaurant revenue\n\nBefore we begin analyzing and preprocessing the dataset, we need to import some packages and read the dataset in."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import some packages needed\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn import linear_model\nfrom datetime import datetime\n\n# Read the train and test dataset\ntrain_df = pd.read_csv(\"../input/restaurant-revenue-prediction/train.csv.zip\")\ntest_df = pd.read_csv(\"../input/restaurant-revenue-prediction/test.csv.zip\")\n\n# Remove Id field from train dataset\ntrain_df = train_df.drop([\"Id\"], axis = 1)\n\n# Look at some train data examples\ntrain_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Analyze and Preprocess the Dataset**"},{"metadata":{},"cell_type":"markdown","source":"In this first step, we will gather as much information we need from the dataset. We will start from Open Date field. As we see, Open Date field gives us information about when the restaurant started their business. In this form, we only get categorical type variable with so many unique values. So, we will try to convert it first into an integer that have meaningful measurement. We can do it in the following way. First, we take an anchor date. In this case we can take the date when this competition launched. Then, we can calculate how old that restaurant by counting how many days are there between the open date and anchor date. So now, the Open Date gives us information about the age of the restaurant, which is measurable. After that, we will plot the Open Date field with the revenue Field using scatter plot, and try to gain information from their relationship."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turn Open Date field into datetime type\ntrain_df[\"Open Date\"] = train_df[\"Open Date\"].astype('datetime64[ns]')\n\n# Get competition date\ncompetition_date = datetime.strptime('2015-09-14', '%Y-%m-%d')\n\n# Convert Open Date field into integer\ntrain_df[\"Dummy\"] = competition_date\ntrain_df[\"Open Date\"] = train_df[\"Open Date\"] - train_df[\"Dummy\"]\ntrain_df[\"Open Date\"] = train_df[\"Open Date\"].dt.days\ntrain_df = train_df.drop([\"Dummy\"], axis = 1)\ntrain_df[\"Open Date\"] = train_df[\"Open Date\"].abs()\n\n# Look the Open Date converted field\ntrain_df[[\"Open Date\"]].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw scatter plot of Open Date vs Revenue\nplt.scatter(train_df[\"Open Date\"], train_df[\"revenue\"])\nplt.xlabel(\"Open Date\")\nplt.ylabel(\"Revenue\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all, we notice that there are 3 outliar data (the point which revenue above 1.25). We will remove those 3 data from train dataset in next step. Other than those outliar data, this plot doesn't give us any reliable information. It is more likely that there is a poor to no correlation between Open Date and Revenue. We will talk about correlation more later."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get index of those outliars\nindex_out = train_df[train_df['revenue'] > 12500000].index\n\n# Remove outliar data from train dataset\ntrain_df = train_df.drop(index_out)\n\n# Copy train_df for later observations\nfor_corr_df = train_df\n\n# To confirm, we now only have 134 rows\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will try to gather information from City field. First, we will count how many unique values are there, and what is the occurence frequency of each value. We can use value_counts syntax as the following."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the frequency of each unique value on City field\ntrain_df[[\"City\"]].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 37 unique values, where most of them only appear once. And if we want to turn them into one-hot-encoding form (which is a needed step to deal with categorical type data), it will be too much if we convert them to be 37 different features. So, we can try to make it becomes only 8 features. Those cities who have occurence frequency greater than or equal to 4 will have their own class (so there are 7 classes), and for the rest will be categorized as \"other\". Following syntax will do our target and turn them into one-hot-encoding form."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turn City field into one-hot-encoding form\ncity_dummy_df = pd.get_dummies(train_df[[\"City\"]], prefix = ['City'])\n\n# Create new column titled City_Other\ncity_dummy_df[\"City_Other\"] = 0\nfor index, rows in city_dummy_df.iterrows():\n    if (\n        rows[\"City_İstanbul\"] == 0 and\n        rows[\"City_Ankara\"] == 0 and\n        rows[\"City_İzmir\"] == 0 and\n        rows[\"City_Bursa\"] == 0 and\n        rows[\"City_Samsun\"] == 0 and\n        rows[\"City_Antalya\"] == 0 and\n        rows[\"City_Sakarya\"] == 0\n    ):\n        city_dummy_df[\"City_Other\"][index] = 1\n\n# Choose only 8 features\ncity_dummy_df = city_dummy_df[[\"City_İstanbul\", \"City_Ankara\", \"City_İzmir\", \"City_Bursa\", \"City_Samsun\", \"City_Antalya\", \"City_Sakarya\", \"City_Other\"]]\n\n# Merge that one-hot-encoding dataframe to train_df\ntrain_df = pd.merge(train_df, city_dummy_df, left_index = True, right_index = True)\n\n# Look at the result\ntrain_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will take a look at City Group field. We will draw a countplot for that field, which will give us information about what unique values are there, and how much the appearance of each value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw a chart about the frequency of each value of City Group field\nax = sns.countplot(x = \"City Group\", data = train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are only 2 unique values, where those values appear almost balance. So we will just turn it into standard one-hot-encoding form using following syntax."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turn City Group field into one-hot-encoding form\ngroup_dummy_df = pd.get_dummies(train_df[[\"City Group\"]], prefix = ['Group'])\n\n# Merge that one-hot-encoding dataframe to train_df\ntrain_df = pd.merge(train_df, group_dummy_df, left_index = True, right_index = True)\n\n# Look at the result\ntrain_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will take a look at Type field. We will draw a countplot for that field, which will give us information about what unique values are there, and how much the appearance of each value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw a chart about the frequency of each value of Type field (from train dataset and test dataset)\nplt.figure(1)\nax = sns.countplot(x = \"Type\", data = train_df)\nplt.figure(2)\nax = sns.countplot(x = \"Type\", data = test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there are 3 unique values in train dataset. But, one of them has frequency of appearance so small. So we will change all DT values into one of IL or FC. To do this we can use the same strategy as we used before for City field, using revenue average. Furthermore, if we look at the test dataset, there are 4 unique values for this field. This means we don't have any information about MB type from train dataset. And since this is added in test dataset, we cannot turn it into FC or IL based on revenue. So we will just simply turn them into FC type (the most common type) later."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get information about revenue's average in each type\nrev_avg_df = train_df[[\"Type\", \"revenue\"]].groupby(\"Type\").mean()\ntype_freq_df = train_df[[\"Type\", \"revenue\"]].groupby(\"Type\").count()\nrev_info_by_type_df = pd.merge(type_freq_df, rev_avg_df, on = \"Type\").sort_values(by = ['revenue_x'], ascending = False)\nrev_info_by_type_df = rev_info_by_type_df.rename(columns = {\"revenue_y\": \"Average Rev\", \"revenue_x\": \"Frequency\"})\nrev_info_by_type_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above result, we can see that DT's average revenue is closer to IL rather than FC, so we will replace DT into IL."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace all DT with IL\nfor index, rows in train_df.iterrows():\n    if rows[\"Type\"] == \"DT\":\n        train_df[\"Type\"][index] = \"IL\"\n\n# Turn into one-hot-encoding form\ntype_dummy_df = pd.get_dummies(train_df[[\"Type\"]], prefix = ['Type'])\n\n# Merge that one-hot-encoding dataframe to train_df\ntrain_df = pd.merge(train_df, type_dummy_df, left_index = True, right_index = True)\n\n# Look at the result\ntrain_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will look at correlation between each feature pair (Open Date, P1 - P37, and Revenue), and draw a heatmap. But before we calculate the correlation, we will normalize the dataset first."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get sub dataframe of all numerical type fields\nnumerical_df = for_corr_df.drop([\"City\", \"City Group\", \"Type\"], axis = 1)\n\n# Normalize the dataset\nnumerical_df_val = numerical_df.values # Returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nnumerical_scaled = min_max_scaler.fit_transform(numerical_df_val)\nnormal_numerical_df = pd.DataFrame(numerical_scaled)\n\n# Get correlation matrix\ncorr_df = normal_numerical_df.corr()\ncorr_df = corr_df.abs()\n\n# Draw correlation heatmap for all numerical type fields\nplt.figure(figsize = (39, 39))\nax = sns.heatmap(corr_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are interested to find a good correlation from any features to revenue. Darker tiles mean weaker correlation, and brighter tiles mean stronger correlation. Notice that row 38 is about the correlation of revenue to any other features. We can see there that most of the tiles is almost totally black (If we look closer, it is more that likely they only have around maximum 0.2 correlation with revenue). But among them, there still some features who get \"brighter\" than the other. We will focus our observation to such features, which are column 0, 2, and 28. Column 0 for Open Date feature, column 2 and 28 for P2 and P28 features respectively. Next we will look at the scatter plot of P2 vs Revenue and P28 vs Revenue."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw scatter plot for each feature vs revenue\nfeatures_index = [2, 28]\ncounter = 1\nfor index in features_index:\n    plt.figure(counter)\n    x_field_name = \"P\" + str(index)\n    plt.scatter(train_df[x_field_name], train_df[\"revenue\"])\n    plt.xlabel(x_field_name)\n    plt.ylabel(\"Revenue\")\n    counter += 1\n\n# Draw the plot in a frame\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Feed the Dataset Into Regression Model**"},{"metadata":{},"cell_type":"markdown","source":"Before we build the model, we will do feature selection. From our observation, we get that there are no strong correlation between P1 - P37 to revenue. But at least, we find that P2 and P28 is better than the other, so we will choose P2 and P28 as our features. We also know that Open Date is still better than those P1-P37 columns, so we will include Open Date too. We also have turned some categorical variables into one-hot-encoding form, which we will use as our features too."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature selection\nto_drop = []\nfor index in list(range(1, 38)):\n    if index not in features_index:\n        to_drop.append(\"P\" + str(index))\ntrain_df = train_df.drop(to_drop, axis = 1)\n\n# Look at the result\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will build and train our model. We will simply use simple linear regression model from sklearn packages. But before we fit in the dataset into the model, we separate the features and target first, then convert them into numpy array."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create regresson object and prepare the dataset\nregr = linear_model.LinearRegression()\ntrain_x = np.asanyarray(train_df.drop(['revenue', 'City', 'City Group', 'Type'], axis = 1))\ntrain_y = np.asanyarray(train_df[['revenue']])\n\n# Feed the data into the model\nregr.fit(train_x, train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Predict Restaurant Revenue**"},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to use the model to predict restaurant revenue based on information given in test dataset. But before we can predict, first we need to preprocess the data like we did before to train dataset (convert open date, turn city, city group, and type to one-hot-encoding form, etc)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------- Convert Open Date ---------------\n# Turn Open Date field into datetime type\ntest_df[\"Open Date\"] = test_df[\"Open Date\"].astype('datetime64[ns]')\n\n# Convert Open Date field into integer\ntest_df[\"Dummy\"] = competition_date\ntest_df[\"Open Date\"] = test_df[\"Open Date\"] - test_df[\"Dummy\"]\ntest_df[\"Open Date\"] = test_df[\"Open Date\"].dt.days\ntest_df = test_df.drop([\"Dummy\"], axis = 1)\ntest_df[\"Open Date\"] = test_df[\"Open Date\"].abs()\n\n# Look the Open Date converted field\ntest_df[[\"Open Date\"]].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------- Convert City ---------------\n# Turn City field into one-hot-encoding form\ncity_dummy_df = pd.get_dummies(test_df[[\"City\"]], prefix = ['City'])\n\n# Create new column titled City_Other\ncity_dummy_df[\"City_Other\"] = 0\nfor index, rows in city_dummy_df.iterrows():\n    if (\n        rows[\"City_İstanbul\"] == 0 and\n        rows[\"City_Ankara\"] == 0 and\n        rows[\"City_İzmir\"] == 0 and\n        rows[\"City_Bursa\"] == 0 and\n        rows[\"City_Samsun\"] == 0 and\n        rows[\"City_Antalya\"] == 0 and\n        rows[\"City_Sakarya\"] == 0\n    ):\n        city_dummy_df[\"City_Other\"][index] = 1\n\n# Choose only 8 features\ncity_dummy_df = city_dummy_df[[\"City_İstanbul\", \"City_Ankara\", \"City_İzmir\", \"City_Bursa\", \"City_Samsun\", \"City_Antalya\", \"City_Sakarya\", \"City_Other\"]]\n\n# Merge that one-hot-encoding dataframe to train_df\ntest_df = pd.merge(test_df, city_dummy_df, left_index = True, right_index = True)\n\n# Look at the result\ntest_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------- Convert City Group ---------------\n# Turn City Group field into one-hot-encoding form\ngroup_dummy_df = pd.get_dummies(test_df[[\"City Group\"]], prefix = ['Group'])\n\n# Merge that one-hot-encoding dataframe to train_df\ntest_df = pd.merge(test_df, group_dummy_df, left_index = True, right_index = True)\n\n# Look at the result\ntest_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------- Convert Type ---------------\n# Replace all DT with IL\nfor index, rows in test_df.iterrows():\n    if rows[\"Type\"] == \"DT\":\n        test_df[\"Type\"][index] = \"IL\"\n    elif rows[\"Type\"] == \"MB\":\n        test_df[\"Type\"][index] = \"FC\"\n\n# Turn into one-hot-encoding form\ntype_dummy_df = pd.get_dummies(test_df[[\"Type\"]], prefix = ['Type'])\n\n# Merge that one-hot-encoding dataframe to train_df\ntest_df = pd.merge(test_df, type_dummy_df, left_index = True, right_index = True)\n\n# Look at the result\ntest_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------- Feature Selection ---------------\nto_drop = []\nfor index in list(range(1, 38)):\n    if index not in features_index:\n        to_drop.append(\"P\" + str(index))\ntest_df = test_df.drop(to_drop, axis = 1)\n\n# Look at the result\ntest_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare the dataset\ntest_x = np.asanyarray(test_df.drop(['Id', 'City', 'City Group', 'Type'], axis = 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are ready to predict restaurant revenue using our trained regression model. After we get the prediction, we will convert it into pandas dataframe and save it to csv."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict restaurant revenue\ny_predict = regr.predict(test_x)\n\n# Save into csv format\ntest_df[\"Prediction\"] = y_predict\nsubmit_df = test_df[[\"Id\", \"Prediction\"]]\nsubmit_df.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}