{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datetime\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import r2_score\nimport warnings\nwarnings.warn = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/restaurant-revenue-prediction/train.csv')\ntest  = pd.read_csv('../input/restaurant-revenue-prediction/test.csv')\n\nprint(\"Training set = \",train.shape)\nprint(\"Testing set = \",test.shape)\nprint(\"Sum of Missing Values (Train/Test)= \",train.isna().sum().sum(),\"(\",test.isna().sum().sum(),\")\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"quater_id=[1,1,1,1,2,2,2,3,3,3,4,4,4]\n\n# Working with Training Data\ntrain['year']  = pd.DatetimeIndex(train['Open Date']).year\ntrain['month'] = pd.DatetimeIndex(train['Open Date']).month\ntrain['day']   = pd.DatetimeIndex(train['Open Date']).day\ntrain = train.drop(columns=['Open Date'])\n\ntrain['quater']=[quater_id[i] for i in train.month]\ntrain['first_week'] = train.day.apply(lambda x: 1 if x<= 7 else 0)\ntrain['last_week']  = train.day.apply(lambda x: 1 if x>=21 else 0)\ntrain['rev_slot'] = pd.qcut(train['revenue'], 20, labels=False)\n\n\n# Working with Testing Data\ntest['year']  = pd.DatetimeIndex(test['Open Date']).year\ntest['month'] = pd.DatetimeIndex(test['Open Date']).month\ntest['day']   = pd.DatetimeIndex(test['Open Date']).day\ntest = test.drop(columns=['Open Date'])\n\ntest['quater']=[quater_id[i] for i in test.month]\ntest['first_week'] = test.day.apply(lambda x: 1 if x<= 7 else 0)\ntest['last_week']  = test.day.apply(lambda x: 1 if x>=21 else 0)\n\n\n# Display changes in Training Data\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_weekdays(data):\n   \n  data['day_value'] = pd.DataFrame([datetime.date(data.year[i],data.month[i],data.day[i]).weekday() for i in range(data.shape[0])])\n  data['weekdays'] = np.where(data.day_value<5, 1,0)\n  data['weekend'] = 1-data['weekdays']\n  return data\n\ntrain = define_weekdays(train) \ntest  = define_weekdays(test) \n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_feature_count(data):\n  col_names = ['City','City Group','Type','year','quater','month','first_week','last_week','day','weekdays','weekend','rev_slot']\n  df_all=pd.DataFrame()\n  for i in col_names:\n    u = data[i].unique()\n    temp=pd.DataFrame()\n    for j in u:\n      m = (data[i]==j).sum()\n      temp = temp.append([[j,m]])\n    temp['col_name'] = i    \n    df_all = df_all.append(temp)\n\n  df_all.columns = ['X','Y','Feature']\n  return df_all\n\ndf = get_feature_count(train)\n\nfig=px.bar(data_frame=df, x='X',y='Y',color='Y',facet_col='Feature',facet_col_wrap=4,height=600)\nfig.update_xaxes(matches=None)\nfig.update_yaxes(matches=None)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_feature_count(data):\n  col_names = ['City','City Group','Type','year','quater','month','first_week','last_week','day','weekdays','weekend']\n  df_all=pd.DataFrame()\n  for i in col_names:\n    u = data[i].unique()\n    temp=pd.DataFrame()\n    for j in u:\n      m = data.revenue[data[i]==j].mean()\n      temp = temp.append([[j,m]])\n    temp['col_name'] = i\n    df_all = df_all.append(temp)\n\n  df_all.columns = ['X','Y','Feature']\n  return df_all\n\ndf = get_feature_count(train)\n\n\nfig=px.bar(data_frame=df, x='X',y='Y',color='Y',facet_col='Feature',facet_col_wrap=4,height=600)\nfig.update_xaxes(matches=None)\nfig.update_yaxes(matches=None)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-Hot Encoding\nprint(\"Prior to One-Hot Encoding\",train.shape,\" : \",test.shape)\nOHE_cols = ['City','City Group','Type','weekdays']\ntrain = pd.get_dummies(train, columns = OHE_cols, drop_first = True)\ntest  = pd.get_dummies(test,  columns = OHE_cols, drop_first = True)\nprint(\"Post to One-Hot Encoding\",train.shape,\" : \",test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regression Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-Test Dataset Ready\ny_train = train.revenue\nX_train = train.drop(['Id','revenue','rev_slot'],axis=1)\ntest    = test.drop(['Id'],axis=1)\n\nprint(train.shape,test.shape) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove columns which exists in Training data but is missing in Testing Data or Vice-Versa\n\ndef remove_columns(columns_a,columns_b):    \n  col_list = []\n  for i in columns_a:\n    flag=0\n    for j in columns_b:\n      if i==j:              \n        flag=1\n        break\n    if flag==0:            \n      col_list.append(i)              \n  return col_list\n    \ncol_list_1 = remove_columns(X_train.columns,test.columns)\ncol_list_2 = remove_columns(test.columns,X_train.columns)\n\nX_train = X_train.drop(columns=col_list_1,axis=1)\ntest = test.drop(columns=col_list_2,axis=1)\n\nprint(\"Post dropping columns from Training Data\",X_train.shape)\nprint(\"Post dropping columns from Testing Data\",test.shape) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression(fit_intercept=True,normalize=False)\nlr.fit(X_train,y_train)\ny_pred1 = lr.predict(X_train)\n\nprint(\"Training performance from Linear Regression = \",abs(r2_score(y_train,y_pred1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cannot use Lasso or Elastic Net: Variables are highly correlated, so LASSO may eliminate wrongly\n\nfrom sklearn.linear_model import Ridge\nrd = Ridge(alpha=1.0, max_iter=100, tol=0.0001, random_state=100)\nrd.fit(X_train,y_train)\ny_pred2 = rd.predict(X_train)\n\nprint(\"Training performance from Ridge Regression = \",abs(r2_score(y_train,y_pred2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Average Response"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = (0.7*y_pred1+0.3*y_pred2)\nprint(\"Training performance from Linear and Ridge Regression = \",abs(r2_score(y_train,y_pred)))\ny_pred = (0.5*y_pred1+0.5*y_pred2)\nprint(\"Training performance from Linear and Ridge Regression = \",abs(r2_score(y_train,y_pred)))\ny_pred = (0.3*y_pred1+0.7*y_pred2)\nprint(\"Training performance from Linear and Ridge Regression = \",abs(r2_score(y_train,y_pred)))\n\n# Performance will be obtained in-between Linear and Ridge Regression. \n# Thus, one or more models must be considered for taking an average or perform second stage regression.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}