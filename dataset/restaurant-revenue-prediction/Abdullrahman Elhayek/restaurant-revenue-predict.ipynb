{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Kaggle Scores\n#### Top Scores \n+ XGBOOSTING Score : ** 1885869.33899 ** RMSE  with  XGBOOSTING regressor 09/09/2019\n+ RandomForestScore :** 1769240.65800 ** RMSE with RandomForest regressor 10/09/2019\n\n## Update Notes \n#### 9/11/2019 \n+ Removing Outliers reduced the Training error to 0.17, yet increased the kaggle score to 1.8e6, which probably means that the number of resutrants making more than 1e7 is more frequently showing up in the test dataset, on contrary to their occurance in the training datase \n\n+ Column City seems to have slight effect on Kaggle RMSE\n\n+ fixed some Keras code error, Keras makes INF predictions ..... HOLLY MOLLY"},{"metadata":{},"cell_type":"markdown","source":"==============================================================================="},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1 - Data Loading and Essential exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load data\ntrain = pd.read_csv('/kaggle/input/restaurant-revenue-prediction/train.csv')\ntest = pd.read_csv('/kaggle/input/restaurant-revenue-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns\n#Drop sequences \n#train = train.drop('Id', axis=1)\n#test = test.drop('Id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train :\",train.shape)\nprint(\"Test:\",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Open Date'] = pd.to_datetime(train['Open Date'])\ntest['Open Date'] = pd.to_datetime(test['Open Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get column with null values  \ntrain.columns[train.isna().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns[test.isna().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen in the above text, there is no null values in any of the columns which is good, yet we have to explore the relation between the predictors and predicted through EDA "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seperate categorical from numberical variables for analysis \nnumerical_features = train.select_dtypes([np.number]).columns.tolist()\ncategorical_features = train.select_dtypes(exclude = [np.number,np.datetime64]).columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[numerical_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[categorical_features].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2 - Data Exploration\nIn this phase we are going to explore the features and their relations "},{"metadata":{},"cell_type":"markdown","source":"## Questions to be answered \n\n+ Which is the best city to Open a resutrant ? \n\n+ What is the best time in year to open a resutrant in that city ? \n\n+ What is the best type of resturant to be opened ? \n\n"},{"metadata":{},"cell_type":"markdown","source":"## Data Columns\n+   #### Id * :  Restaurant id. *\n+   #### Open Date *  : opening date for a restaurant * \n+   #### City : *  City that the restaurant is in. Note that there are unicode in the names.  * \n+   #### City Group:  * Type of the city. Big cities, or Other.  * \n+   #### Type: Type of the restaurant. FC: *  Food Court, IL: Inline, DT: Drive Thru, MB: Mobile * \n+   #### P1, P2 - P37: *  There are three categories of these obfuscated data. Demographic data are gathered from third party providers with GIS systems. These include population in any given area, age and gender distribution, development scales. Real estate data mainly relate to the m2 of the location, front facade of the location, car park availability. Commercial data mainly include the existence of points of interest including schools, banks, other QSR operators. * \n+   #### Revenue: *  The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis. Please note that the values are transformed so they don't mean real dollar values.  * "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing Numerical Variables\n"},{"metadata":{},"cell_type":"markdown","source":"Check the target column distribution ['revenue']"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['revenue'].describe())\nsns.set(style='whitegrid', palette=\"deep\", font_scale=1.1, rc={\"figure.figsize\": [8, 5]})\nsns.distplot(\n    train['revenue'], norm_hist=False, kde=True\n).set(xlabel='revenue', ylabel='P(revenue)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" + As the distribution shows that the data is not mostly left scewed as obvious that the data values are almost ranges between 1.149870e+06 and 1e+07, while the values larger than this number can be problimatic ( anamonlies or outlayer and has to be studied carefully ) "},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['revenue'] > 10000000 ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop outlayers\ntrain = train[train['revenue'] < 10000000 ]\ntrain.reset_index(drop=True).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ with a single look in the eye, it is clear that these three resturants have made a significant renvenue despite the fact that they opened in the years 2000 and 2005, unlike other resturants. there has to be some reasons behind this that has to be studied carefully"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train[numerical_features].hist(figsize=(30, 35), layout=(12, 4));\n\nk = len(train[numerical_features].columns)\nn = 3\nm = (k - 1) // n + 1 ## Floor Division (also called Integer Division)\nfig, axes = plt.subplots(m, n, figsize=(n * 5, m * 3))\nfor i, (name, col) in enumerate(train[numerical_features].iteritems()):\n    r, c = i // n, i % n\n    ax = axes[r, c]\n    col.hist(ax=ax)\n    ax2 = col.plot.kde(ax=ax, secondary_y=True, title=name)\n    ax2.set_ylim(0)\n\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ as the distributions clearly states that there are clear anamolies in some of the features in hand, these anamonlies has to be studied carefully with an expert in the field, also the distributions above describes certain biases that made that data distribution either not fair or skewed for some reason \n+ For instance if we look at P37 : value zero is more likely to show among the feature values, possibly this is a data entry error or there something important that number zero represents "},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['P37']  == 0].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ## Analyzing Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3, 1, figsize=(40, 30))\nfor variable, subplot in zip(categorical_features, ax.flatten()):\n    df_2 = train[[variable,'revenue']].groupby(variable).revenue.sum().reset_index()\n    df_2.columns = [variable,'total_revenue']\n    sns.barplot(x=variable, y='total_revenue', data=df_2 , ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ As the categorical variables chart shows that Instanbul is making the most revenues compared to other cities, So that Opening a resturant in Instanbul would be a great Idea, the question is when and why .. this questions we will answer in the next segement of our data exploration \n+ It is also clear that big cities are making more revnues than smaller cities yet the gap between them is narrowing thanks to increased number of turists visting them \n+ if you are about to open a resutrant its better be and FC Food court and IL (inline) "},{"metadata":{},"cell_type":"markdown","source":"## Analyzing Relationships Between Numerical Variables and the target ['revneue']\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(10, 4, figsize=(30, 35))\nfor variable, subplot in zip(numerical_features, ax.flatten()):\n    sns.regplot(x=train[variable], y=train['revenue'], ax=subplot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(45,25))\nsns.heatmap(train.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" + As the heatmap and the scatter distribution clearly shows that there is no obvious releation between numerical variables independtly with the target, thought the corrleation can be hidden and we will explore it through machine learning "},{"metadata":{},"cell_type":"markdown","source":"## Analyzing Relationships between the Categorical Variables and the target ['revenue']"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3, 1, figsize=(40, 30))\nfor var, subplot in zip(categorical_features, ax.flatten()):\n    sns.boxplot(x=var, y='revenue', data=train, ax=subplot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ these box plots proves our first conclusion about the best places to invest money and the worst and which types of resturant are more likely to make money than others, in addition, it gives us more insight, as the graph clearly shows that the city of Izmair has a wider range of distributed values in revneue with slighly lower median than istanbul, Ankara could be a good place for opening a resurant and making renveue in time shorther than required if the same resturant is opened in Istanbul "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Not Applicable \n## STudy relation between P2 and revenue in light of Categorical Variables\n#cond_plot = sns.FacetGrid(data=train, col='City', hue='Type', col_wrap=4)\n#cond_plot.map(sns.scatterplot, 'P2', 'revenue').add_legend();\n#cond_plot = sns.FacetGrid(data=train, col='City Group', hue='Type', col_wrap=4)\n#cond_plot.map(sns.scatterplot, 'P2', 'revenue').add_legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Case Study : Analyzing Relationships between the Cities ['İstanbul','İzmir','Ankara','Bursa','Samsun'] feature and the target ['revenue'] with respect to Open_Date and the Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"cats = train[\"City\"].unique()\ncats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tem = train.copy()\n#cats = train[\"City\"].unique().tolist() \n#fig, ax = plt.subplots(34, 1, figsize=(25, 400))\ncats = ['İstanbul','İzmir','Ankara','Bursa','Samsun']\nfig, ax = plt.subplots(5, 1, figsize=(25, 45))\nfor variable, subplot in zip(cats, ax.flatten()):\n    #x = tem.where(train[\"City\"]==variable, inplace = False)\n    x = tem[train[\"City\"]==variable]\n    x = x.sort_values(by=['Open Date'])\n    if len(x) <= 4:        \n        g = sns.barplot(x=\"Open Date\", y=\"revenue\",hue=\"Type\", data=x, ax=subplot)\n        g.title.set_text(variable)\n    else:\n        g = sns.lineplot(x=\"Open Date\", y=\"revenue\", style = \"Type\",label=variable, linestyle=\"-\", data=x, ax=subplot)\n        g.title.set_text(variable)\n        for label in subplot.get_xticklabels():\n            label.set_rotation(90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ The graphs clearly shows that the best option is to open an inline resturant in Istanbul as all the renuvues are plummeting in Turkey since 2006 - 2008, yet there is hope that inline resutrants will start making profits as their renvues grow up slighly in 2014 "},{"metadata":{},"cell_type":"markdown","source":"## Answers to the above questions \n\n+ Which is the best city to Open a resutrant ?  # Istanbu\n\n+ What is the best time in year to open a resutrant in that city ?  # first quarter of the year\n\n+ What is the best type of resturant to be opened ? #IL inline \n\n"},{"metadata":{},"cell_type":"markdown","source":"## 3 - Data preparation\nIn this phase we are going to prepare the data to be machine learned, we will deal with categircal variables and numberical veriables in addition to the dates  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n##from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## as can be seen that test has more rows than the training dataset \n## as the viuslization clearly express no clear relation in the train\nnum_train = train.shape[0]\nnum_test = test.shape[0]\nprint(num_train, num_test)\n\n# For feature engineering, combine train and test data\ndata = pd.concat((train.loc[:, \"Id\" : \"P37\"],\n                  test.loc[:, \"Id\" : \"P37\"]), ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting mean of P-variables over each city helps us see which P-variables are highly related to City\n# since we are given that one class of P-variables is geographical attributes.\ndistinct_cities = train.loc[:, \"City\"].unique()\n\n# Get the mean of each p-variable for each city\nmeans = []\nfor col in train.columns[5:42]:\n    temp = []\n    for city in distinct_cities:\n        temp.append(train.loc[train.City == city, col].mean())     \n    means.append(temp)\n    \n# Construct data frame for plotting\ncity_pvars = pd.DataFrame(columns=[\"city_var\", \"means\"])\nfor i in range(37):\n    for j in range(len(distinct_cities)):\n        city_pvars.loc[i+37*j] = [\"P\"+str(i+1), means[i][j]]\n#print(city_pvars)        \n# Plot boxplot\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nsns.boxplot(x=\"city_var\", y=\"means\", data=city_pvars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import cluster\n# K Means treatment for city (mentioned in the paper)\ndef adjust_cities(data, train, k):\n    \n    # As found by box plot of each city's mean over each p-var\n    relevant_pvars =  [\"P1\", \"P2\", \"P11\", \"P19\", \"P20\", \"P23\", \"P30\"]\n    train = train.loc[:, relevant_pvars]\n    \n    # Optimal k is 20 as found by DB-Index plot    \n    kmeans = cluster.KMeans(n_clusters=k)\n    kmeans.fit(train)\n    \n    # Get the cluster centers and classify city of each data instance to one of the centers\n    data['City Cluster'] = kmeans.predict(data.loc[:, relevant_pvars])\n    del data[\"City\"]\n    \n    return data\n\ndef one_hot_ecoding(data,col,pref):\n    # One hot encode City Group\n    data = data.join(pd.get_dummies(data[col], prefix=pref))\n    # Since only n-1 columns are needed to binarize n categories, drop one of the new columns.  \n    # And drop the original columns.\n    data = data.drop([col], axis=1)\n    return data ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert unknown cities in test data to clusters based on known cities using KMeans\ndata = adjust_cities(data, train, 20)\n#data = data.drop(['City'], axis=1)\ndata = one_hot_ecoding(data,'City Group',\"CG\")\ndata = one_hot_ecoding(data,'Type',\"T\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into train and test datasets\ntrain_processed = data[:num_train]\ntest_processed = data[num_train:]\n# check the shapes \nprint(\"Train :\",train.shape)\nprint(\"Test:\",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sav_train = pd.DataFrame()\n#sav_train = train[\"revenue\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"revenue\"] = [np.log(num) for num in train[\"revenue\"]]\nlen(train[\"revenue\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_processed[\"revenue\"] = train[\"revenue\"].values ## if you do not put this values , then you are in complete danger\n#train_processed[\"Id\"] = train[\"Id\"]\n#test_processed[\"Id\"] = test[\"Id\"]\ntrain = train_processed\ntest = test_processed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the shapes \nprint(\"Train :\",train.shape)\nprint(\"Test:\",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nfrom datetime import datetime as dt\n\n## prepartion function \ndef prepare_data_frame(dataframe, target):\n    df = dataframe.copy()\n    ## Splittin the date column into three columns \n    df['Open Date Year']  = df['Open Date'].dt.year\n    df['Open Date Month']  = df['Open Date'].dt.month\n    df['Open Date Day']  = df['Open Date'].dt.day\n    ##---------------------------------------\n    ## feature Engineering Begining \n    ##--------------------------------------\n    ## feature Engineering a diff column \n    ## since when this resturant was opened\n    all_diff = []\n    for date in df[\"Open Date\"]:\n        diff = dt.now() - date\n        all_diff.append(int(diff.days/1000))\n    df['Days_from_open'] = pd.Series(all_diff)\n    ##---------------------------------------\n    ## feature Engineering End \n    ##--------------------------------------\n    # drop Open Date column s\n    df = df.drop(['Open Date'], axis=1)\n    # drop target column s\n    if target in df.columns:\n        tar = df[target]\n        df = df.drop([target], axis=1)\n    else:\n        tar = None\n    # get numberical variables \n    num_vars = df.select_dtypes([np.number]).columns.tolist()\n    # encoding categrical variables \n    #categorical variables already encoded in one hot encoding\n    # get cat variables \n    cat_vars = df.select_dtypes(include='object').columns.tolist()\n    df[cat_vars] = df[cat_vars].apply(preprocessing.LabelEncoder().fit_transform)\n    #df.loc[:, \"P1\" :\"P37\"] = preprocessing.MinMaxScaler().fit_transform(df.loc[:, \"P1\" : \"P37\"])\n    return (df,tar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,y_train = prepare_data_frame(train,'revenue')\nX_test,y_test = prepare_data_frame(test,'revenue') ## there is no revnue in test\\","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4 - Data Modeling\nDefines a machine learning algorithm to regress the relationship between all features and the target  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#! pip install tensorflow==2.0.0-beta1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#install tensorflow if not installed\n## import tensorflow \nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom xgboost import XGBRegressor\n#from sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LassoCV\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_reg_mse(model,in_parameters):\n    # Define the model\n    my_model_1 = model # Your code here\n    clf = GridSearchCV(my_model_1, in_parameters, cv=2, scoring='neg_mean_squared_error')\n    # Fit the model\n    clf.fit(X_train, y_train) # Your code here\n    #predictions_1 = clf.predict(X_valid) # Your code here\n    #mae_1 = mean_absolute_error(y_true=y_valid, y_pred=predictions_1) # Your code here\n    print('best_params_',clf.best_params_)\n    mae_1 = clf.best_score_ * -1\n    return mae_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def baseline_model():\n    dim = len(X_train.columns.tolist())\n    if not isinstance(dim, int):\n        return 0\n    model = tf.keras.Sequential([\n        layers.Dense(dim, input_dim=dim, kernel_initializer='normal', activation='relu'),\n        layers.Dense(int(round(dim/2)), kernel_initializer='normal', activation='relu'), ## \n        layers.Dense(int(round(dim/4)), kernel_initializer='normal', activation='relu'), ## \n        layers.Dense(int(round(dim/8)), kernel_initializer='normal', activation='relu'), ## \n        layers.Dense(1, kernel_initializer='normal')\n    ])\n    #model.compile(loss='mean_squared_error', optimizer='adam')111\n    model.compile(optimizer='adam',\n                  loss='mse',\n                  metrics=['mse'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.preprocessing import StandardScaler\n\ndef get_reg_keras_mse(in_parameters):\n    # Define the model\n    # evaluate model with standardized dataset\n    my_model_1 = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=baseline_model, verbose=1)\n    reg = GridSearchCV(my_model_1, in_parameters, cv=2, scoring='neg_mean_squared_error')\n    X_ktrain = StandardScaler().fit_transform(X_train.values)  \n    arr = sav_train['rev_save'].values \n    y_ktrain = StandardScaler().fit_transform(arr[:, np.newaxis])\n    \n    reg.fit(X_ktrain, y_train.values) # Your code here\n    # Fit the model\n    print('best_params_',reg.best_params_)\n    mse_1 = reg.best_score_ * -1\n    return mse_1\n\n# A utility method to create a tf.data dataset from a Pandas Dataframe\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32, target_name='target'):\n    dataframe = dataframe.copy()\n    labels = dataframe.pop(target_name)\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    return ds\n\ndef input_fn(features, labels, training=True, batch_size=32):\n    \"\"\"An input function for training or evaluating\"\"\"\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n\n    # Shuffle and repeat if you are in training mode.\n    if training:\n        #dataset = dataset.shuffle(1000).repeat()\n        dataset = dataset.shuffle(buffer_size=len(features))\n    \n    return dataset.batch(batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ## XGBoost Mean S Error: 0.25096676125467654"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Parameters for tuning model \n#n_esitmators = list(range(100, 1001, 100))\n#learning_rates = [x / 100 for x in range(5, 101, 5)]\n#parameters = {'n_esitmators':n_esitmators, 'learning_rates':learning_rates}\n#mse_1 = get_reg_mse(XGBRegressor(),parameters)\n# Uncomment to print MAE\n#print(\"Mean Squared Error:\" , mse_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Regression.0.229699386793816"},{"metadata":{"trusted":true},"cell_type":"code","source":"#c = list(np.arange(1, 10, 0.1))\n#cache_size=list(range(100, 1001, 100))\n#parameters = {'kernel':('linear', 'rbf'), 'C':c,'cache_size':cache_size}\n#mse_1 = get_reg_mse(SVR(gamma='scale'),parameters)\n# Uncomment to print MAE\n#print(\"Mean Squared Error:\" , mse_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ## RandomForestRegressor : 0.21991174394344817"},{"metadata":{"trusted":true},"cell_type":"code","source":"#n_estimators=list(range(100, 501, 100))\n#max_depth = list(range(1, 5, 1))\n#parameters = {'n_estimators':n_estimators,'max_depth':max_depth}\n#mse_1 = get_reg_mse(RandomForestRegressor(random_state=400),parameters)\n# Uncomment to print MAE\n#print(\"Mean Squared Error:\" , mse_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LassoCV regressor :  0.23536168489619116"},{"metadata":{"trusted":true},"cell_type":"code","source":"#random_state=list(range(0, 100, 1))\n#parameters = {'random_state':random_state}\n#mse_1 = get_reg_mse(LassoCV(cv=2),parameters)\n# Uncomment to print MAE\n#print(\"Mean Squared Error:\" , mse_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KerasRegressor (Neural Network)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#seed = 1\n#epochs=list(range(100, 101, 100))\n#batch_size = [1,5]\n#parameters = {'epochs':epochs,'batch_size':batch_size}\n#mse_1 = get_reg_keras_mse(parameters)\n# Uncomment to print MAE\n#print(\"Mean Squared Error:\" , mse_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4 - Data Test Prediction\nLets make predictions "},{"metadata":{"trusted":true},"cell_type":"code","source":"samp = pd.read_csv('/kaggle/input/restaurant-revenue-prediction/sampleSubmission.csv')\nsamp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# other regressors\n#reg = RandomForestRegressor(random_state = 400, max_depth = 1, n_estimators = 500)\n#reg.fit(X_train,y_train)\n#print (reg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## keras regressor\n#reg = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=baseline_model,batch_size=5,epochs=100, verbose=1)\n#X_ktrain = StandardScaler().fit_transform(X_train.values)  \n#arr = sav_train['rev_save'].values \n#y_ktrain = StandardScaler().fit_transform(arr[:, np.newaxis])\n#reg.fit(X_ktrain,y_ktrain)\n#طprint (reg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## deal with null values in test dataset\n#cols3 = X_test.columns[X_test.isna().any()].tolist()\n#print(cols3)\n#X_test['Days_from_open'] = X_test['Days_from_open'].fillna(int(round(X_test['Days_from_open'].mean())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict \n#id_vals = X_test['Id'].values\n#output = reg.predict(X_test) #data=X_test for other Regressors\n#output = np.exp(output)\n#final_df = pd.DataFrame()\n#final_df[\"Id\"] = id_vals\n#final_df[\"Prediction\"] = output.round(1)\n#final_df.to_csv(\"Output_Keras.csv\", index=False)\n#print('Check for Na : ',final_df.isna().sum())\n#print('Check for Inf : ',np.isfinite(final_df).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}