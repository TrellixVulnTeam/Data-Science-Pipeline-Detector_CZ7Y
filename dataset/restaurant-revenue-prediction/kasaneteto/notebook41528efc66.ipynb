{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime\nimport matplotlib.pyplot as pylab\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import (GridSearchCV, RandomizedSearchCV)\nfrom sklearn.metrics import (mean_squared_error, mean_absolute_error)\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import ExtraTreesClassifier # Used for imputing rare / missing values\n\n# Regressors considered:\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import Ridge # only model used for final submission\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-12T10:12:27.08901Z","iopub.execute_input":"2021-07-12T10:12:27.089651Z","iopub.status.idle":"2021-07-12T10:12:28.52318Z","shell.execute_reply.started":"2021-07-12T10:12:27.089537Z","shell.execute_reply":"2021-07-12T10:12:28.521753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FAKE_DATA_RATIO = 311.5\n# Set a Random Seed\nSEED = 777\n# Read Kaggle Provided Data\ntrain = pd.read_csv('/kaggle/input/restaurant-revenue-prediction/train.csv.zip', index_col = 0, parse_dates=[1])\ntest = pd.read_csv('/kaggle/input/restaurant-revenue-prediction/test.csv.zip', index_col = 0, parse_dates=[1])\nprint (\"Train Dimensions:\")\nprint (train.shape)\nprint (\"Test Dimensions:\")\nprint (test.shape)\n\n# Concatenate train and test together to pre-process and featurize both consistently.\ndf = pd.concat((test, train), ignore_index=True)\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:16:08.944784Z","iopub.execute_input":"2021-07-12T10:16:08.945171Z","iopub.status.idle":"2021-07-12T10:16:09.895804Z","shell.execute_reply.started":"2021-07-12T10:16:08.945139Z","shell.execute_reply":"2021-07-12T10:16:09.894877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Open Date\"] = df[\"Open Date\"].apply(pd.to_datetime)\nlast_date = df[\"Open Date\"].max()\ndf[\"Open Date\"] = last_date - df[\"Open Date\"] # This becomes a datetime delta object\ndf[\"Open Date\"] = df[\"Open Date\"].dt.days + 1 # converts the delta object to an int\n\n# Scale \"days since opened\" so that the marginal impact decreases over time\n# This and the similar log transform of City Count below are the modifications that \n# were not in our official competition submission\ndf[\"Log Days Opened\"] = df[\"Open Date\"].apply(np.log)\ndf = df.drop([\"Open Date\"], axis=1)\npylab.rcParams['figure.figsize'] = (8, 6) # Resizes plots\ndf[[\"Log Days Opened\", \"revenue\"]].plot(x=\"Log Days Opened\", y=\"revenue\", kind='scatter', title=\"Log (Days Opened) vs Revenue\")","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:16:48.172247Z","iopub.execute_input":"2021-07-12T10:16:48.172664Z","iopub.status.idle":"2021-07-12T10:16:49.248297Z","shell.execute_reply.started":"2021-07-12T10:16:48.172629Z","shell.execute_reply":"2021-07-12T10:16:49.247313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zero_cols = ['P14', 'P15', 'P16', 'P17', 'P18', 'P24', 'P25', 'P26', 'P27', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']\n\n# We make a feature that holds this count of zero columns in the above list\ndf['zeros'] = (df[zero_cols] == 0).sum(1)\n\npylab.rcParams['figure.figsize'] = (20, 8)\nfig, axs = plt.subplots(1,2)\n\nprint (\"Distribution of new Zeros features:\")\n# We find there is only 1 row with a zero count between 0 and 17 in the train set, \ndf['zeros'].loc[pd.notnull(df.revenue)].value_counts().plot(title=\"Train Set\", kind='bar', ax=axs[0])\n\n# But in the test set there are many rows with an intermediate count of zeros. \n# This is probably an artifact of how the fake test data was generated (conditional \n# dependence between columns was not preserved).\ndf['zeros'].loc[pd.isnull(df.revenue)].value_counts().plot(title=\"Test Set\", kind='bar', ax=axs[1], color='red')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:17:09.811732Z","iopub.execute_input":"2021-07-12T10:17:09.81213Z","iopub.status.idle":"2021-07-12T10:17:10.276419Z","shell.execute_reply.started":"2021-07-12T10:17:09.812094Z","shell.execute_reply":"2021-07-12T10:17:10.275638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pylab.rcParams['figure.figsize'] = (6, 4) # Resizes plots\n\n# The two categories of City Group both appear very frequently\ntrain[\"City Group\"].value_counts().plot(title=\"City Group Distribution in the Train Set\", kind='bar')\nplt.show()\n\n# But two of the four Restaurant Types (DT and FC), are extremely rare\ntrain[\"Type\"].value_counts().plot(title=\"Restaurant Type Distribution in the Train Set\", kind='bar')\nplt.show()\n\n(test[\"Type\"].value_counts() / FAKE_DATA_RATIO).plot(title=\"Approximate Restaurant Type Distribution in True Test Set\", kind='bar', color='Red')\nplt.show()\n\ndf = df.join(pd.get_dummies(df['City Group'], prefix=\"CG\"))\ndf = df.join(pd.get_dummies(df['Type'], prefix=\"T\"))\n\n# Since only n-1 columns are needed to binarize n categories, drop one of the new columns.  \n# And drop the original columns.\n# And also drop the extremely rare restaurant types (which we handleed especially below)\ndf = df.drop([\"City Group\", \"Type\", \"CG_Other\", \"T_MB\", \"T_DT\"], axis=1)\nprint (df.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:17:24.138979Z","iopub.execute_input":"2021-07-12T10:17:24.139628Z","iopub.status.idle":"2021-07-12T10:17:24.668663Z","shell.execute_reply.started":"2021-07-12T10:17:24.139576Z","shell.execute_reply":"2021-07-12T10:17:24.667542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Replace city names with\n# count of their frequency in the train + estimated frequency in the test set.\ncity_counts = (test[\"City\"].value_counts() / FAKE_DATA_RATIO).add(train[\"City\"].value_counts(), fill_value=0)\ndf[\"City\"] = df[\"City\"].replace(city_counts)\nprint (\"Some example estimated counts of restaurants per city:\")\nprint (city_counts.head())\n\n# Take log of city count so that the marginal effect decreases\ndf[\"Log City Count\"] = df[\"City\"].apply(np.log) \ndf = df.drop([\"City\"], axis=1)\n\n# That last vertical spread of points are restaurants from Istanbul.\npylab.rcParams['figure.figsize'] = (8, 6) # Resizes plots\ndf[[\"Log City Count\", \"revenue\"]].plot(x=\"Log City Count\", y=\"revenue\", kind='scatter', title=\"Log City Count vs Revenue\")\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:17:44.284986Z","iopub.execute_input":"2021-07-12T10:17:44.285354Z","iopub.status.idle":"2021-07-12T10:17:44.890691Z","shell.execute_reply.started":"2021-07-12T10:17:44.285322Z","shell.execute_reply":"2021-07-12T10:17:44.889593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute values for the very rare restaurant types. \n# Instead of trying to predict with values that appear only 1 or 0 times in the train set, \n# we will replace them with one of the other commonly appearing categories by fitting a \n# model that predicts which common category they \"should\" be.\n\n# tofit are the rows in the train set that belong to one of the common restaurnat types\ntofit = df.loc[((df.T_FC==1) | (df.T_IL==1)) & (pd.notnull(df.revenue))]\n# tofill are rows in either train or test that belong to one of the rare types\ntofill = df.loc[((df.T_FC==0) & (df.T_IL==0))]\n\nprint('type training set shape:'), tofit.shape\nprint('data to impute:'), tofill.shape\n\n# Resaruants with type FC are labeled 1, those with type IL are labeled 0.\ny = tofit.T_FC\n# Drop the label columns and revenue (which is not in the test set, so can't be used here)\nX = tofit.drop([\"T_FC\", \"T_IL\", \"revenue\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:17:55.260239Z","iopub.execute_input":"2021-07-12T10:17:55.260665Z","iopub.status.idle":"2021-07-12T10:17:55.27571Z","shell.execute_reply.started":"2021-07-12T10:17:55.260631Z","shell.execute_reply":"2021-07-12T10:17:55.274736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define and train a model to impute restaurant type\n# The grid below just has a range of values that I've found commonly\n# work well with random forest type models (of which ExtraTrees is one).\nmodel_grid = {'max_depth': [None, 8], 'min_samples_split': [4,9,16], 'min_samples_leaf':[1,4], 'max_features':['sqrt', 0.5, None]}\ntype_model = ExtraTreesClassifier(n_estimators=25, random_state=SEED)\n\ngrid = RandomizedSearchCV(type_model, model_grid, n_iter=10, cv=5, scoring=\"roc_auc\")\ngrid.fit(X, y)\n\nprint(\"Best parameters for Type Model:\")\nprint(grid.best_params_)\n\ntype_model.set_params(**grid.best_params_)\ntype_model.fit(X, y)\n\nimputations = type_model.predict(tofill.drop([\"T_FC\", \"T_IL\", \"revenue\"], axis=1))\ndf.loc[(df.T_FC==0) & (df.T_IL==0), \"T_FC\"] = imputations\ndf = df.drop([\"T_IL\"], axis=1)\n\nprint (\"% labeled FC in the training set:\"), df.T_FC.mean()\nprint (\"% of imputed values labeled FC:\"), np.mean(imputations)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:18:18.161619Z","iopub.execute_input":"2021-07-12T10:18:18.162123Z","iopub.status.idle":"2021-07-12T10:18:20.448258Z","shell.execute_reply.started":"2021-07-12T10:18:18.16209Z","shell.execute_reply":"2021-07-12T10:18:20.447383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Now binarize the \"P\" columns with dummy variables\nprint (\"Pre-binarizing columns:\"), len(df.columns)\nfor col in df.columns:\n    if col[0] == 'P':\n        print (col), len(df[col].unique()), \"unique values\"\n        df = df.join(pd.get_dummies(df[col], prefix=col))\n        df = df.drop([col, df.columns[-1]], axis=1)\nprint (\"Post-binarizing columns:\"), len(df.columns)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:19:11.32491Z","iopub.execute_input":"2021-07-12T10:19:11.325284Z","iopub.status.idle":"2021-07-12T10:19:13.369341Z","shell.execute_reply.started":"2021-07-12T10:19:11.32525Z","shell.execute_reply":"2021-07-12T10:19:13.368401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Scale all input features to between 0 and 1, critical to do this for KNN or SVR models.\nmin_max_scaler = MinMaxScaler()\n# Don't scale the output - drop it temporarily\nrev = df.revenue\ndf = df.drop(['revenue'], axis=1)\n\ndf = pd.DataFrame(data = min_max_scaler.fit_transform(df), columns = df.columns, index=df.index)\ndf = df.join(rev)\n\n# Done with preprocessing. Let's take a last look at the data before modeling with it.\ndf.describe()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:19:22.839816Z","iopub.execute_input":"2021-07-12T10:19:22.840179Z","iopub.status.idle":"2021-07-12T10:19:24.721483Z","shell.execute_reply.started":"2021-07-12T10:19:22.840144Z","shell.execute_reply":"2021-07-12T10:19:24.720562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Recover original train/train rows based on revenue (which is null for test rows)\ntrain = df.loc[pd.notnull(df.revenue)]\ntest = df.loc[pd.isnull(df.revenue)].drop(['revenue'], axis=1)\n\n# Scale revenue by sqrt. \n# The purpose is to decrease the influence of the few very large revenue values.\ny = train.revenue.apply(np.sqrt)\nX = train.drop([\"revenue\"], axis=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:19:34.742383Z","iopub.execute_input":"2021-07-12T10:19:34.742943Z","iopub.status.idle":"2021-07-12T10:19:34.918506Z","shell.execute_reply.started":"2021-07-12T10:19:34.742893Z","shell.execute_reply":"2021-07-12T10:19:34.917517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Now define and train a Ridge Regression model. We tested others from the sklearn package:\n# SVR, RandomForest, K-nearest Neighbors, but found Ridge consistantly gave the strongest \n# leaderboard results. When training data is small, simplest is often best.\nmodel_grid = [{'normalize': [True, False], 'alpha': np.logspace(0,10)}]\nmodel = Ridge()\n\n# Use a grid search and leave-one-out CV on the train set to find the best regularization parameter to use.\n# (might take a minute or two)\ngrid = GridSearchCV(model, model_grid, cv=LeaveOneOut(), scoring='neg_mean_squared_error')\ngrid.fit(X, y)\nprint(\"Best parameters set found on development set:\")\nprint(grid.best_params_)\n\n# Re-train on full training set using the best parameters found in the last step.\nmodel.set_params(**grid.best_params_)\nmodel.fit(X, y)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:19:45.657798Z","iopub.execute_input":"2021-07-12T10:19:45.658191Z","iopub.status.idle":"2021-07-12T10:24:33.180812Z","shell.execute_reply.started":"2021-07-12T10:19:45.658159Z","shell.execute_reply":"2021-07-12T10:24:33.17957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on the test set with the trained model.\nsubmission = pd.DataFrame(columns=['Prediction'],index=test.index, data=model.predict(test))\n# Convert back to revenue from sqrt(revenue)\nsubmission.Prediction = submission.Prediction.apply(np.square)\n# Add required column name for Kaggle's submission parser:\nsubmission.index.name='Id'\n# Write out the submission\nsubmission.to_csv(\"TFI_Ridge.csv\")\n# Quick sanity check on the submission\nsubmission.describe().astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:29:06.556097Z","iopub.execute_input":"2021-07-12T10:29:06.556684Z","iopub.status.idle":"2021-07-12T10:29:07.065367Z","shell.execute_reply.started":"2021-07-12T10:29:06.556634Z","shell.execute_reply":"2021-07-12T10:29:07.064643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Revenue from train set for comparison\ntrain[['revenue']].describe().astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:29:18.841662Z","iopub.execute_input":"2021-07-12T10:29:18.842171Z","iopub.status.idle":"2021-07-12T10:29:18.858736Z","shell.execute_reply.started":"2021-07-12T10:29:18.842138Z","shell.execute_reply":"2021-07-12T10:29:18.857687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Another quick comparision. Note the x-axis scale change: the predictions are \n# more conservative and tend to be closer to the mean than the real revenues. \n# This is pretty standard behavior when using RMSE - there are big penalties for \n# being very wrong, so the model will tend towards more moderate predictions.\ntrain[['revenue']].plot(kind='kde', title=\"Train Revenue Distribution\")\nsubmission.columns = [\"predicted revenue\"]\nsubmission.plot(kind='kde', title=\"Prediction Revenue Distribution\", color='r')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:29:33.874652Z","iopub.execute_input":"2021-07-12T10:29:33.875314Z","iopub.status.idle":"2021-07-12T10:29:36.522458Z","shell.execute_reply.started":"2021-07-12T10:29:33.875254Z","shell.execute_reply":"2021-07-12T10:29:36.521243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"seed値を0から777に変更しただけでは結果変わらず。⑧をいじる？","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}