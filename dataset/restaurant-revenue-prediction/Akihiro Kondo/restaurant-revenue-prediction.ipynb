{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Data file\nimport numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns \n\nfrom sklearn.svm import SVR\nfrom sklearn.svm import LinearSVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor #  KneighborsRegressorではない\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import RANSACRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/restaurant-revenue-prediction/train.csv.zip', header=0)\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/restaurant-revenue-prediction/test.csv.zip')\ntest.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/restaurant-revenue-prediction/sampleSubmission.csv')\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mid = train.copy()\ntrain_mid['train_or_test'] = 'train'\n\ntest_mid = test.copy()\ntest_mid['train_or_test'] = 'test'\n\ntest_mid['revenue'] = 9\n\nalldata = pd.concat([train_mid, test_mid], sort=False, axis=0).reset_index(drop=True)\n\nprint('The size of the train data:' + str(train.shape))\nprint('The size of the test data:' + str(test.shape))\nprint('The size of the submission data:' + str(submission.shape))\nprint('The size of the alldata data:' + str(alldata.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()\ntest.describe()\nalldata.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('=====Train=====')\ntrain.info()\nprint('\\n=====Test=====')\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for duplicates\nidsUnique = len(set(alldata['Id']))\nidsTotal = alldata.shape[0]\nidsDupli = idsTotal - idsUnique\nprint(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing data in Alldata\ndef Missing_table(df):\n    # null_val = df.isnull().sum()\n    null_val = df.isnull().sum()[df.isnull().sum()>0].sort_values(ascending=False)\n    percent = 100 * null_val/len(df)\n    na_col_list = df.isnull().sum()[df.isnull().sum()>0].index.tolist() # 欠損を含むカラムをリスト化\n    list_type = df[na_col_list].dtypes.sort_values(ascending=False) #データ型\n    Missing_table = pd.concat([null_val, percent, list_type], axis = 1)\n    missing_table_len = Missing_table.rename(\n    columns = {0:'Missing data', 1:'%', 2:'type'})\n    return missing_table_len.sort_values(by=['Missing data'], ascending=False)\n\nMissing_table(alldata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EDA\n# Histogram\nalldata.hist(figsize = (12,12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heatmap. Understand feature related to survived\nfig, ax = plt.subplots(figsize=(15,15))\nsns.heatmap(train.corr(),annot=True, center=0, square=True, linewidths=0.1, vmax=1.0, linecolor='white', cmap=\"RdBu\")\nplt.title('Restaurant Revenue Prediction', fontsize = 20)\nplt.xlabel('x-axis', fontsize = 15)\nplt.ylabel('y-axis', fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata['City'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata['City Group'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata['Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata['City'].replace(['İstanbul', 'Ankara', 'İzmir'], 0,inplace=True)\nalldata['City'].replace(['Bursa', 'Samsun', 'Antalya', 'Sakarya', 'Kayseri', 'Diyarbakır', 'Tekirdağ', 'Eskişehir', 'Adana', 'Aydın', 'Muğla', 'Konya', 'Trabzon', 'Amasya', 'Uşak', 'Kastamonu', 'Karabük', 'Kütahya', 'Bolu', 'Şanlıurfa', 'Edirne', 'Kırklareli', 'Afyonkarahisar', 'Osmaniye', 'Denizli', 'Tokat', 'Balıkesir', 'Gaziantep', 'Kocaeli', 'Elazığ', 'Isparta', 'Mersin', 'Manisa', 'Çanakkale', 'Hatay', 'Zonguldak', 'Aksaray', 'Yalova', 'Kırıkkale', 'Malatya', 'Mardin', 'Batman', 'Rize', 'Artvin', 'Bilecik', 'Nevşehir', 'Sivas', 'Kırşehir', 'Erzincan', 'Erzurum', 'Ordu', 'Kahramanmaraş', 'Siirt', 'Niğde', 'Giresun', 'Çankırı', 'Çorum', 'Düzce', 'Tanımsız', 'Kars'], 1,inplace=True)\n\nalldata['City Group'] = alldata['City Group'].replace(\"Big Cities\",0).replace(\"Other\",1)\n\nalldata['Type'] = alldata['Type'].replace(\"FC\",0).replace(\"IL\",1).replace(\"DT\",2).replace(\"MB\",3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata[\"Open Date\"] = pd.to_datetime(alldata[\"Open Date\"])\nalldata[\"Year\"] = alldata[\"Open Date\"].apply(lambda x:x.year)\nalldata[\"Month\"] = alldata[\"Open Date\"].apply(lambda x:x.month)\nalldata[\"Day\"] = alldata[\"Open Date\"].apply(lambda x:x.day)\nalldata[\"kijun\"] = \"2015-04-27\"\nalldata[\"kijun\"] = pd.to_datetime(alldata[\"kijun\"])\nalldata[\"BusinessPeriod\"] = (alldata[\"kijun\"] - alldata[\"Open Date\"]).apply(lambda x: x.days)\n\nalldata = alldata.drop('Open Date', axis=1)\nalldata = alldata.drop('kijun', axis=1)\n\nalldata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check all of datatype\nalldata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = alldata.query('train_or_test == \"train\"')\ntest = alldata.query('train_or_test == \"test\"')\n\ntarget_column = 'revenue'\ntrain_target = train[target_column]\n\ntrain_target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_column = ['Id', 'train_or_test', 'revenue']\ntrain_feature = train.drop(columns=drop_column)\n\ntrain_feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before deleting the Id column of test data, extract only the Id column used for output. The first time I merged train and test, the first index started at 137 and the index is off by that amount. It's not a problem in the final output, but I don't like the way it looks, so I'll just reindex it\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Index reset\ntest = test.reset_index()\n# Delete unnecessary index column\ndel test[\"index\"]\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Idカラムを、submission_idとしてだけ抜き出しておく\nsubmission_id = test['Id']\n\n# 最後のテスト出力用の説明変数データを作成。学習データとカラムを合わせて、Id, train_or_test, 9のデータが入っているrevenueを削除し、学習に必要な特徴量のみを保持\ntest_feature = test.drop(columns=drop_column)\ntest_feature\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 有効な特微量を探す（SelectKBestの場合）\nfrom sklearn.feature_selection import SelectKBest, f_regression\n# 特に重要な4つの特徴量のみを探すように設定してみる\nselector = SelectKBest(score_func=f_regression, k=4) \nselector.fit(train_feature, train_target)\nmask_SelectKBest = selector.get_support()    # 各特徴量を選択したか否かのmaskを取得\n\n# 有効な特微量を探す（SelectPercentileの場合）\nfrom sklearn.feature_selection import SelectPercentile, f_regression\n# 特徴量のうち40%を選択\nselector = SelectPercentile(score_func=f_regression, percentile=40) \nselector.fit(train_feature, train_target)\nmask_SelectPercentile = selector.get_support()\n\n# 有効な特微量を探す（モデルベース選択の場合：SelectFromModel）\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestRegressor\n# estimator として RandomForestRegressor を使用。重要度が median 以上のものを選択\nselector = SelectFromModel(RandomForestRegressor(n_estimators=100, random_state=42), threshold=\"median\")    \nselector.fit(train_feature, train_target)\nmask_SelectFromModel = selector.get_support()\n\n# 有効な特微量を探す（RFE：再帰的特徴量削減 : n_features_to_select）\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestRegressor\n# estimator として RandomForestRegressor を使用。特徴量を2個選択させる\nselector = RFE(RandomForestRegressor(n_estimators=100, random_state=42), n_features_to_select=2)\nselector.fit(train_feature, train_target)\nmask_RFE = selector.get_support()\n\nprint(train.columns)\nprint(mask_SelectKBest)\nprint(mask_SelectPercentile)\nprint(mask_SelectFromModel)\nprint(mask_RFE)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"important_feature = pd.DataFrame({\"Index\":train.columns[1], \"SelectKBest\":mask_SelectKBest, \"SelectPercentile\":mask_SelectPercentile, \"SelectFromModelKBest\":mask_SelectFromModel, \"RFE\":mask_RFE})\nimportant_feature.to_csv(\"important_feature.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 読み込む\nresult = pd.read_csv('important_feature.csv')\nresult.head(50)\n# 新しいカラムを作成して合計のTrue数を記載する。その後ソートで表示する\nresult[\"Total_True_Number\"] = result.sum(axis=1)\nresult.sort_values('Total_True_Number', ascending = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ホールドアウト法で検証するため、あらかじめデータを学習用と検証用に分割\nX_train, X_test, y_train, y_test = train_test_split(train_feature, train_target, test_size=0.2, random_state=0, shuffle=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 警告が多いので、いったん警告を表示されないようにする\n# 本来は表示を消すのはお勧めしない。廃止予定の関数や例外が表示されるほうが良い\nimport warnings\nwarnings.filterwarnings('ignore')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RandomForest==============\n\nrf = RandomForestRegressor(n_estimators=200, max_depth=5, max_features=0.5,  verbose=True, random_state=0, n_jobs=-1) # RandomForest のオブジェクトを用意する\nrf.fit(X_train, y_train)\nprint('='*20)\nprint('RandomForestRegressor')\nprint(f'accuracy of train set: {rf.score(X_train, y_train)}')\nprint(f'accuracy of test set: {rf.score(X_test, y_test)}')\n\n# 学習させたRandomForestをtestデータに適用して、売上を予測しましょう\nrf_prediction = rf.predict(test_feature)\nrf_prediction\n\n'''\n# Create submission data\nrf_submission = pd.DataFrame({\"Id\":submission_id, \"Prediction\":rf_prediction})\nrf_submission.to_csv(\"RandomForest_submission.csv\", index=False)\n'''\n\n# SVR（Support Vector Regression）==============\n# ※[LibSVM]や[LibLinear]は台湾国立大学の方で開発されたらしくどうしてもその表示が入るようになっている\n\nsvr = SVR(verbose=True)\nsvr.fit(X_train, y_train)\nprint('='*20)\nprint('SVR')\nprint(f'accuracy of train set: {svr.score(X_train, y_train)}')\nprint(f'accuracy of test set: {svr.score(X_test, y_test)}')\n\nsvr_prediction = svr.predict(test_feature)\nsvr_prediction\n\n# LinearSVR==============\n\nlsvr = LinearSVR(verbose=True, random_state=0)\nlsvr.fit(X_train, y_train)\nprint('='*20)\nprint('LinearSVR')\nprint(f'accuracy of train set: {lsvr.score(X_train, y_train)}')\nprint(f'accuracy of test set: {lsvr.score(X_test, y_test)}')\n\nlsvr_prediction = lsvr.predict(test_feature)\nlsvr_prediction\n\n# SGDRegressor==============\n\nsgd = SGDRegressor(verbose=0, random_state=0)\nsgd.fit(X_train, y_train)\nprint('='*20)\nprint('SGDRegressor')\nprint(f'accuracy of train set: {sgd.score(X_train, y_train)}')\nprint(f'accuracy of test set: {sgd.score(X_test, y_test)}')\n\nsgd_prediction = sgd.predict(test_feature)\nsgd_prediction\n\n# k-近傍法（k-NN）==============\n\nknn = KNeighborsRegressor()\nknn.fit(X_train, y_train)\nprint('='*20)\nprint('KNeighborsRegressor')\nprint(f'accuracy of train set: {knn.score(X_train, y_train)}')\nprint(f'accuracy of test set: {knn.score(X_test, y_test)}')\n\nknn_prediction = knn.predict(test_feature)\nknn_prediction\n\n# 決定木==============\n\ndecisiontree = DecisionTreeRegressor(max_depth=3, random_state=0)\ndecisiontree.fit(X_train, y_train)\nprint('='*20)\nprint('DecisionTreeRegressor')\nprint(f'accuracy of train set: {decisiontree.score(X_train, y_train)}')\nprint(f'accuracy of test set: {decisiontree.score(X_test, y_test)}')\n\ndecisiontree_prediction = decisiontree.predict(test_feature)\ndecisiontree_prediction\n\n# LinearRegression (線形回帰)==============\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nprint('='*20)\nprint('LinearRegression')\nprint(f'accuracy of train set: {lr.score(X_train, y_train)}')\nprint(f'accuracy of test set: {lr.score(X_test, y_test)}')\n# 回帰係数とは、回帰分析において座標平面上で回帰式で表される直線の傾き。 原因となる変数x（説明変数）と結果となる変数y（目的変数）の平均的な関係を、一次式y＝ax＋bで表したときの、係数aを指す。\nprint(\"回帰係数:\",lr.coef_)\nprint(\"切片:\",lr.intercept_)\n\nlr_prediction = lr.predict(test_feature)\nlr_prediction\n\n\n# RANSACRegressor==============\n\n# ロバスト回帰を行う（自然界のデータにはたくさんノイズがある。ノイズなどの外れ値があると、法則性をうまく見つけられないことがある。そんなノイズをうまく無視してモデルを学習させるのがRANSAC）\n#線形モデルをRANSACでラッピング　（外れ値の影響を抑える）\nfrom sklearn.linear_model import RANSACRegressor\n \nransac=RANSACRegressor(lr,#基本モデルは、LinearRegressionを流用\n                       max_trials=100,#イテレーションの最大数100\n                       min_samples=50,#ランダムに選択されるサンプル数を最低50に設定\n                       loss=\"absolute_loss\",#学習直線に対するサンプル店の縦の距離の絶対数を計算\n                       residual_threshold=5.0,#学習直線に対する縦の距離が5以内のサンプルだけを正常値\n                       random_state=0)\n \nransac.fit(X_train, y_train)\nprint('='*20)\nprint('RANSACRegressor')\nprint(f'accuracy of train set: {lr.score(X_train, y_train)}')\nprint(f'accuracy of test set: {lr.score(X_test, y_test)}')\nprint(\"RANSAC回帰係数:\",ransac.estimator_.coef_[0])\nprint(\"RANSAC切片:\",ransac.estimator_.intercept_)\n\nransac_prediction = ransac.predict(test_feature)\nransac_prediction\n\n# RIDGE回帰==============\n\nridge = Ridge(random_state=0)\nridge.fit(X_train, y_train)\nprint('='*20)\nprint('Ridge')\nprint(f'accuracy of train set: {ridge.score(X_train, y_train)}')\nprint(f'accuracy of test set: {ridge.score(X_test, y_test)}')\n\nridge_prediction = ridge.predict(test_feature)\nridge_prediction\n\nridge_submission = pd.DataFrame({\"Id\":submission_id, \"Prediction\":ridge_prediction})\nridge_submission.to_csv(\"Ridge_submission.csv\", index=False)\n\n\n\n# LASSO回帰==============\n\nlasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005], verbose=True, random_state=0)\nlasso.fit(X_train, y_train)\nprint('='*20)\nprint('LassoCV')\nprint(f'accuracy of train set: {lasso.score(X_train, y_train)}')\nprint(f'accuracy of test set: {lasso.score(X_test, y_test)}')\n\nlasso_prediction = lasso.predict(test_feature)\nlasso_prediction\n\n\n# ElasticNet==============\n\nen = ElasticNet(random_state=0)\nen.fit(X_train, y_train)\nprint('='*20)\nprint('ElasticNet')\nprint(f'accuracy of train set: {en.score(X_train, y_train)}')\nprint(f'accuracy of test set: {en.score(X_test, y_test)}')\n\nen_prediction = en.predict(test_feature)\nen_prediction\n\n# Kernel Ridge Regression(l2制約付き最小二乗学習)==============\n\nkernelridge = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nkernelridge.fit(X_train, y_train)\nprint('='*20)\nprint('KernelRidge')\nprint(f'accuracy of train set: {kernelridge.score(X_train, y_train)}')\nprint(f'accuracy of test set: {kernelridge.score(X_test, y_test)}')\n\nkernelridge_prediction = kernelridge.predict(test_feature)\nkernelridge_prediction\n\n\n# Gradient Boosting Regression==============\n# Boostingとは弱学習器をたくさん集めて強学習器を作ろうという話が出発点で、PAC Learningと呼ばれています\n\ngradientboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', verbose=0, random_state=0)\ngradientboost.fit(X_train, y_train)\nprint('='*20)\nprint('GradientBoostingRegressor')\nprint(f'accuracy of train set: {gradientboost.score(X_train, y_train)}')\nprint(f'accuracy of test set: {gradientboost.score(X_test, y_test)}')\n\ngradientboost_prediction = gradientboost.predict(test_feature)\ngradientboost_prediction\n\n\n# XGB==============\n\nxgb = XGBRegressor(objective ='reg:squarederror', verbose=True, random_state=0)  \nxgb.fit(X_train, y_train) \nprint('='*20)\nprint('XGBClassifier')\nprint(f'accuracy of train set: {xgb.score(X_train, y_train)}')\nprint(f'accuracy of test set: {xgb.score(X_test, y_test)}')\n\nxgb_prediction = xgb.predict(test_feature)\nxgb_prediction\n\n\n# lightgbm==============\n\nlgbm = LGBMRegressor(random_state=0)\nlgbm.fit(X_train, y_train)\nprint('='*20)\nprint('LGBMRegressor')\nprint(f'accuracy of train set: {lgbm.score(X_train, y_train)}')\nprint(f'accuracy of test set: {lgbm.score(X_test, y_test)}')\n\nlgbm_prediction = lgbm.predict(test_feature)\nlgbm_prediction\n\n\n# catboost==============\n\ncatboost = CatBoostRegressor(verbose=0, random_state=0)\ncatboost.fit(X_train, y_train)\nprint('='*20)\nprint('CatBoostRegressor')\nprint(f'accuracy of train set: {catboost.score(X_train, y_train)}')\nprint(f'accuracy of test set: {catboost.score(X_test, y_test)}')\n\ncatboost_prediction = catboost.predict(test_feature)\ncatboost_prediction\n\n\n# VotingRegressor==============\n\n# voting に使う分類器を用意する\nestimators = [\n  (\"rf\", rf),\n  (\"svr\", svr),\n  (\"lsvr\", lsvr),\n  (\"sgd\", sgd),\n  (\"knn\", knn),\n  (\"decisiontree\", decisiontree),\n  (\"lr\", lr),\n  (\"ransac\", ransac),\n  (\"ridge\", ridge),\n  (\"lasso\", lasso),\n  (\"en\", en),\n  (\"kernelridge\", kernelridge),\n  (\"gradientboost\", gradientboost),\n  (\"xgb\", xgb),\n  (\"lgbm\", lgbm),\n  (\"catboost\", catboost),\n]\n\nvote = VotingRegressor(estimators=estimators)\nvote.fit(X_train, y_train)\nprint('='*20)\nprint('VotingRegressor')\nprint(f'accuracy of train set: {vote.score(X_train, y_train)}')\nprint(f'accuracy of test set: {vote.score(X_test, y_test)}')\n\nvote_prediction = vote.predict(test_feature)\nvote_prediction\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ※重要な特微量を探す（RandomForestを利用する）\nplt.figure(figsize=(20,10))\nplt.barh(\n    X_train.columns[np.argsort(rf.feature_importances_)],\n    rf.feature_importances_[np.argsort(rf.feature_importances_)],\n    label='RandomForestRegressor'\n)\nplt.title('RandomForestRegressor feature importance')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ※重要な特微量を探す（決定木やXGBを利用する）\n\n\nfrom sklearn import tree\ntext_representation = tree.export_text(decisiontree)\nprint(text_representation)\n\nwith open(\"decistion_tree.log\", \"w\") as fout:\n    fout.write(text_representation)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(decisiontree, \n                   feature_names=X_train.columns,  \n                   class_names=target_column,\n                   filled=True)\nfig.savefig(\"decistion_tree.png\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz\n# DOT data\ndot_data = tree.export_graphviz(decisiontree, out_file=None, \n                                feature_names=X_train.columns,  \n                                class_names=target_column,\n                                filled=True)\n\n# Draw graph\ngraph = graphviz.Source(dot_data, format=\"png\") \ngraph\ngraph.render(\"decision_tree_graphivz\")\n'decision_tree_graphivz.png'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 重要度を出力\nfor n, v in zip(X_train.columns, decisiontree.feature_importances_):\n    print(f'importance of {n} is :{v}')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n# RandomForest==============\n\nkf = KFold(n_splits = 5, shuffle = True, random_state=0)\n\nrf = RandomForestRegressor(n_estimators=200, max_depth=5, max_features=0.5,  verbose=True, random_state=0, n_jobs=-1) # RandomForest のオブジェクトを用意する\nrf_cross_score = cross_validate(rf, train_feature, train_target, cv=kf)\nrf_cross_score\nprint('='*20)\nprint('RandomForestRegressor 交差検証(Cross-validation)')\nprint(f'平均値 mean:{rf_cross_score[\"test_score\"].mean()}, 標準偏差 std:{rf_cross_score[\"test_score\"].std()}')\nprint(\"交差検証トレーニングのscore:\",format(rf_cross_score))\n#print(\"交差検証テストのscore:\",format(np.mean(rf_cross_score)))\n\n# SVR（Support Vector Regression）==============\n# ※[LibSVM]や[LibLinear]は台湾国立大学の方で開発されたらしくどうしてもその表示が入るようになっている\n\nsvr = SVR(verbose=True)\nsvr_cross_score = cross_validate(svr, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('SVR 交差検証(Cross-validation)')\nprint(f'平均値 mean:{svr_cross_score[\"test_score\"].mean()}, 標準偏差 std:{svr_cross_score[\"test_score\"].std()}')\n\n# LinearSVR==============\n\nlsvr = LinearSVR(verbose=True, random_state=0)\nlsvr_cross_score = cross_validate(lsvr, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('LinearSVR 交差検証(Cross-validation)')\nprint(f'平均値 mean:{lsvr_cross_score[\"test_score\"].mean()}, 標準偏差 std:{lsvr_cross_score[\"test_score\"].std()}')\n\n\n# SGDRegressor==============\n\nsgd = SGDRegressor(verbose=0, random_state=0)\nsgd_cross_score = cross_validate(sgd, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('SGDRegressor 交差検証(Cross-validation)')\nprint(f'平均値 mean:{sgd_cross_score[\"test_score\"].mean()}, 標準偏差 std:{sgd_cross_score[\"test_score\"].std()}')\n\n\n# k-近傍法（k-NN）==============\n\nknn = KNeighborsRegressor()\nknn_cross_score = cross_validate(knn, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('KNeighborsRegressor 交差検証(Cross-validation)')\nprint(f'平均値 mean:{knn_cross_score[\"test_score\"].mean()}, 標準偏差 std:{knn_cross_score[\"test_score\"].std()}')\n\n\n\n# 決定木==============\n\ndecisiontree = DecisionTreeRegressor(max_depth=3, random_state=0)\ndecisiontree_cross_score = cross_validate(decisiontree, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('DecisionTreeRegressor 交差検証(Cross-validation)')\nprint(f'平均値 mean:{decisiontree_cross_score[\"test_score\"].mean()}, 標準偏差 std:{decisiontree_cross_score[\"test_score\"].std()}')\n\n\n\n\n# LinearRegression (線形回帰)==============\n\nlr = LinearRegression()\nlr_cross_score = cross_validate(lr, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('LinearRegression 交差検証(Cross-validation)')\nprint(f'平均値 mean:{lr_cross_score[\"test_score\"].mean()}, 標準偏差 std:{lr_cross_score[\"test_score\"].std()}')\n\n\n# RANSACRegressor==============\n\n# ロバスト回帰を行う（自然界のデータにはたくさんノイズがある。ノイズなどの外れ値があると、法則性をうまく見つけられないことがある。そんなノイズをうまく無視してモデルを学習させるのがRANSAC）\n#線形モデルをRANSACでラッピング　（外れ値の影響を抑える）\nfrom sklearn.linear_model import RANSACRegressor\n \nransac=RANSACRegressor(lr,#基本モデルは、LinearRegressionを流用\n                       max_trials=100,#イテレーションの最大数100\n                       min_samples=50,#ランダムに選択されるサンプル数を最低50に設定\n                       loss=\"absolute_loss\",#学習直線に対するサンプル店の縦の距離の絶対数を計算\n                       residual_threshold=5.0,#学習直線に対する縦の距離が5以内のサンプルだけを正常値\n                       random_state=0)\n \nransac_cross_score = cross_validate(ransac, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('RANSACRegressor 交差検証(Cross-validation)')\nprint(f'平均値 mean:{ransac_cross_score[\"test_score\"].mean()}, 標準偏差 std:{ransac_cross_score[\"test_score\"].std()}')\n\n\n# RIDGE回帰==============\n\nridge = Ridge(random_state=0)\nridge_cross_score = cross_validate(ridge, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('Ridge 交差検証(Cross-validation)')\nprint(f'平均値 mean:{ridge_cross_score[\"test_score\"].mean()}, 標準偏差 std:{ridge_cross_score[\"test_score\"].std()}')\n\n\n\n\n# LASSO回帰==============\n\nlasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005], verbose=True, random_state=0)\nlasso_cross_score = cross_validate(lasso, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('LassoCV 交差検証(Cross-validation)')\nprint(f'平均値 mean:{lasso_cross_score[\"test_score\"].mean()}, 標準偏差 std:{lasso_cross_score[\"test_score\"].std()}')\n\n\n# ElasticNet==============\n\nen = ElasticNet(random_state=0)\nen_cross_score = cross_validate(en, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('ElasticNet 交差検証(Cross-validation)')\nprint(f'平均値 mean:{en_cross_score[\"test_score\"].mean()}, 標準偏差 std:{en_cross_score[\"test_score\"].std()}')\n\n\n# Kernel Ridge Regression(l2制約付き最小二乗学習)==============\n\nkernelridge = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nkernelridge_cross_score = cross_validate(kernelridge, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('KernelRidge 交差検証(Cross-validation)')\nprint(f'平均値 mean:{kernelridge_cross_score[\"test_score\"].mean()}, 標準偏差 std:{kernelridge_cross_score[\"test_score\"].std()}')\n\n\n# Gradient Boosting Regression==============\n# Boostingとは弱学習器をたくさん集めて強学習器を作ろうという話が出発点で、PAC Learningと呼ばれています\n\ngradientboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', verbose=0, random_state=0)\ngradientboost_cross_score = cross_validate(gradientboost, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('GradientBoostingRegressor 交差検証(Cross-validation)')\nprint(f'平均値 mean:{gradientboost_cross_score[\"test_score\"].mean()}, 標準偏差 std:{gradientboost_cross_score[\"test_score\"].std()}')\n\n\n# XGB==============\n\nxgb = XGBRegressor(objective ='reg:squarederror', verbose=True, random_state=0)  \nxgb_cross_score = cross_validate(xgb, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('XGBClassifier 交差検証(Cross-validation)')\nprint(f'平均値 mean:{xgb_cross_score[\"test_score\"].mean()}, 標準偏差 std:{xgb_cross_score[\"test_score\"].std()}')\n\n\n# lightgbm==============\n\nlgbm = LGBMRegressor(random_state=0)\nlgbm_cross_score = cross_validate(lgbm, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('LGBMRegressor 交差検証(Cross-validation)')\nprint(f'平均値 mean:{lgbm_cross_score[\"test_score\"].mean()}, 標準偏差 std:{lgbm_cross_score[\"test_score\"].std()}')\n\n\n# catboost==============\n\ncatboost = CatBoostRegressor(verbose=0, random_state=0)\ncatboost_cross_score = cross_validate(catboost, train_feature, train_target, cv=kf)\nprint('='*20)\nprint('CatBoostRegressor 交差検証(Cross-validation)')\nprint(f'平均値 mean:{catboost_cross_score[\"test_score\"].mean()}, 標準偏差 std:{catboost_cross_score[\"test_score\"].std()}')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}