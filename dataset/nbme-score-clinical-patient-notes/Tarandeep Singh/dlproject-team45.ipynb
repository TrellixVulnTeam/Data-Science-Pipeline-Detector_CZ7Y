{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport spacy\nimport re\nfrom ast import literal_eval\nfrom itertools import chain\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom tqdm.notebook import tqdm, trange\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel, AutoTokenizer\n\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-01T11:29:26.382075Z","iopub.execute_input":"2022-05-01T11:29:26.383007Z","iopub.status.idle":"2022-05-01T11:29:37.307453Z","shell.execute_reply.started":"2022-05-01T11:29:26.382872Z","shell.execute_reply":"2022-05-01T11:29:37.306587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = \"/kaggle/input/nbme-score-clinical-patient-notes/\"\ntrain = pd.read_csv(base_path+\"train.csv\")\ntest = pd.read_csv(base_path+\"test.csv\")\nfeatures = pd.read_csv(base_path+\"features.csv\")\npatient_notes = pd.read_csv(base_path+\"patient_notes.csv\")\nsubmission = pd.read_csv(base_path+\"sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:37.309321Z","iopub.execute_input":"2022-05-01T11:29:37.31001Z","iopub.status.idle":"2022-05-01T11:29:37.969847Z","shell.execute_reply.started":"2022-05-01T11:29:37.309968Z","shell.execute_reply":"2022-05-01T11:29:37.969118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Total number of Patients\nlen(np.unique(train['case_num']))","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:37.972686Z","iopub.execute_input":"2022-05-01T11:29:37.973198Z","iopub.status.idle":"2022-05-01T11:29:37.98633Z","shell.execute_reply.started":"2022-05-01T11:29:37.973159Z","shell.execute_reply":"2022-05-01T11:29:37.98563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For each patient we have 100 patient notes\nlen(np.unique(train[train['case_num']==3]['pn_num']))","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:37.988832Z","iopub.execute_input":"2022-05-01T11:29:37.989315Z","iopub.status.idle":"2022-05-01T11:29:37.999078Z","shell.execute_reply.started":"2022-05-01T11:29:37.989275Z","shell.execute_reply":"2022-05-01T11:29:37.99797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#One patient note in training set\ntrain[train['pn_num']==16]","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:38.000891Z","iopub.execute_input":"2022-05-01T11:29:38.001172Z","iopub.status.idle":"2022-05-01T11:29:38.019399Z","shell.execute_reply.started":"2022-05-01T11:29:38.001137Z","shell.execute_reply":"2022-05-01T11:29:38.018638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Text Preprocessing\ndef process_feature_text(text):\n    text = re.sub('I-year', '1-year', text)\n    text = re.sub('-OR-', \" or \", text)\n    text = re.sub('-', ' ', text)\n    return text\n\ndef clean_spaces(text):\n    text = re.sub('\\n', ' ', text)\n    text = re.sub('\\t', ' ', text)\n    text = re.sub('\\r', ' ', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:38.020921Z","iopub.execute_input":"2022-05-01T11:29:38.021346Z","iopub.status.idle":"2022-05-01T11:29:38.027575Z","shell.execute_reply.started":"2022-05-01T11:29:38.021308Z","shell.execute_reply":"2022-05-01T11:29:38.026541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Merging all data in training dataframe\ntrain_info = pd.merge(train,features, how='left')\ntrain_info = pd.merge(train_info, patient_notes,how='left')\ntrain_info['pn_history'] = train_info['pn_history'].apply(lambda x:x.strip())\ntrain_info['pn_history'] = train_info['pn_history'].apply(clean_spaces)\ntrain_info['pn_history'] = train_info['pn_history'].apply(str.lower)\ntrain_info[\"annotation_list\"] = [literal_eval(x) for x in train_info[\"annotation\"]]\ntrain_info[\"location_list\"] = [literal_eval(x) for x in train_info[\"location\"]]\ntrain_info['feature_text'] = train_info['feature_text'].apply(process_feature_text)\ntrain_info['feature_text'] = train_info['feature_text'].apply(str.lower)\ntrain_info['feature_text'] = train_info['feature_text'].apply(clean_spaces)\n\nskf = StratifiedKFold(n_splits = 5)\ntrain_info[\"stratify_on\"] = train_info[\"case_num\"].astype(str) + train_info[\"feature_num\"].astype(str)\ntrain_info[\"fold\"]=-1\nfor fold, (_, valid_idx) in enumerate(skf.split(train_info[\"id\"], y = train_info[\"stratify_on\"])):\n    print(fold,valid_idx)\n    train_info.loc[valid_idx, \"fold\"] = fold\n    \ntrain_info.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:38.029079Z","iopub.execute_input":"2022-05-01T11:29:38.029348Z","iopub.status.idle":"2022-05-01T11:29:38.982303Z","shell.execute_reply.started":"2022-05-01T11:29:38.029312Z","shell.execute_reply":"2022-05-01T11:29:38.98157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#A Look at one patient note\ntrain_info[train_info['pn_num']==16]","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:38.983624Z","iopub.execute_input":"2022-05-01T11:29:38.98405Z","iopub.status.idle":"2022-05-01T11:29:39.013602Z","shell.execute_reply.started":"2022-05-01T11:29:38.98401Z","shell.execute_reply":"2022-05-01T11:29:39.012789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using ClinicalBert, Can be replaced with PubMedBert\ntokenizer = AutoTokenizer.from_pretrained(\"../input/pubmedbert/BiomedNLP-PubMedBERT-base-uncased-abstract\")","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:39.015168Z","iopub.execute_input":"2022-05-01T11:29:39.015432Z","iopub.status.idle":"2022-05-01T11:29:39.101909Z","shell.execute_reply.started":"2022-05-01T11:29:39.015397Z","shell.execute_reply":"2022-05-01T11:29:39.101148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\n\ndef tokenize_and_add_labels(tokenizer, example):\n    tokenized_inputs = tokenizer(\n        example[\"feature_text\"],\n        example[\"pn_history\"],\n        truncation = \"only_second\",\n        max_length = 416, # max length is 406\n        padding = \"max_length\",\n        return_offsets_mapping = True\n    )\n    labels = [0.0] * len(tokenized_inputs[\"input_ids\"])\n    tokenized_inputs[\"location_int\"] = loc_list_to_ints(example[\"location_list\"])\n    tokenized_inputs[\"sequence_ids\"] = tokenized_inputs.sequence_ids()\n\n    for idx, (seq_id, offsets) in enumerate(zip(tokenized_inputs[\"sequence_ids\"], tokenized_inputs[\"offset_mapping\"])):\n        if seq_id is None or seq_id == 0:\n            labels[idx] = -100\n            continue\n        exit = False\n        token_start, token_end = offsets\n        for feature_start, feature_end in tokenized_inputs[\"location_int\"]:\n            if exit:\n                break\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                exit = True\n    tokenized_inputs[\"labels\"] = labels\n    \n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:39.105274Z","iopub.execute_input":"2022-05-01T11:29:39.10575Z","iopub.status.idle":"2022-05-01T11:29:39.114842Z","shell.execute_reply.started":"2022-05-01T11:29:39.105716Z","shell.execute_reply":"2022-05-01T11:29:39.113943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first = train_info.loc[0]\nexample = {\n    \"feature_text\": first.feature_text,\n    \"pn_history\": first.pn_history,\n    \"location_list\": first.location_list,\n    \"annotation_list\": first.annotation_list\n}\n\ntokenize_and_add_labels(tokenizer,example)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:39.116066Z","iopub.execute_input":"2022-05-01T11:29:39.116714Z","iopub.status.idle":"2022-05-01T11:29:39.138536Z","shell.execute_reply.started":"2022-05-01T11:29:39.11667Z","shell.execute_reply":"2022-05-01T11:29:39.137789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Setting up data\nclass NBMEData(torch.utils.data.Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = tokenize_and_add_labels(self.tokenizer, example)\n\n        input_ids = np.array(tokenized[\"input_ids\"]) # for input BERT\n        attention_mask = np.array(tokenized[\"attention_mask\"]) # for input BERT\n        labels = np.array(tokenized[\"labels\"]) # for calculate loss and cv score\n\n        offset_mapping = np.array(tokenized[\"offset_mapping\"]) # for calculate cv score\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\") # for calculate cv score\n        \n        return input_ids, attention_mask, labels, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:39.139696Z","iopub.execute_input":"2022-05-01T11:29:39.139959Z","iopub.status.idle":"2022-05-01T11:29:39.146495Z","shell.execute_reply.started":"2022-05-01T11:29:39.13992Z","shell.execute_reply":"2022-05-01T11:29:39.145833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining our model\nclass NBMEModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = AutoModel.from_pretrained('../input/pubmedbert/BiomedNLP-PubMedBERT-base-uncased-abstract') # ClinicalBERT model\n        self.dropout = torch.nn.Dropout(p = 0.2)\n        self.rnn = nn.GRU(input_size=768, hidden_size=384,num_layers=1, bidirectional=True, batch_first=True)\n        self.classifier = torch.nn.Linear(768, 1)\n    \n    def forward(self, input_ids, attention_mask):\n        pooler_outputs = self.backbone(input_ids = input_ids, attention_mask = attention_mask)\n        enc, _  = self.rnn(self.dropout(pooler_outputs[0]))\n        logits = self.classifier(enc).squeeze(-1)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:39.148356Z","iopub.execute_input":"2022-05-01T11:29:39.149103Z","iopub.status.idle":"2022-05-01T11:29:39.161139Z","shell.execute_reply.started":"2022-05-01T11:29:39.149061Z","shell.execute_reply":"2022-05-01T11:29:39.160379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold = 0\nBATCH_SIZE = 16\nEPOCHS = 3\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = NBMEModel().to(DEVICE)\ncriterion = torch.nn.BCEWithLogitsLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr = 1e-5)\n\ntrain = train_info.loc[train_info[\"fold\"] != fold].reset_index(drop = True)\nvalid = train_info.loc[train_info[\"fold\"] == fold].reset_index(drop = True)\ntrain_ds = NBMEData(train, tokenizer)\nvalid_ds = NBMEData(valid, tokenizer)\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size = BATCH_SIZE, pin_memory = True, shuffle = True, drop_last = True)\nvalid_dl = torch.utils.data.DataLoader(valid_ds, batch_size = BATCH_SIZE * 2, pin_memory = True, shuffle = False, drop_last = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:39.162658Z","iopub.execute_input":"2022-05-01T11:29:39.162933Z","iopub.status.idle":"2022-05-01T11:29:51.700967Z","shell.execute_reply.started":"2022-05-01T11:29:39.162886Z","shell.execute_reply":"2022-05-01T11:29:51.700143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n = 1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef get_location_predictions(preds, offset_mapping, sequence_ids, test = False):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        pred = sigmoid(pred)\n        start_idx = None\n        current_preds = []\n        for p, o, s_id in zip(pred, offsets, seq_ids):\n            if s_id is None or s_id == 0:\n                continue\n            if p > 0.5:\n                if start_idx is None:\n                    start_idx = o[0]\n                end_idx = o[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n    return all_predictions\n\ndef calculate_char_CV(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros((num_chars))\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n        char_preds = np.zeros((num_chars))\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n    results = precision_recall_fscore_support(all_labels, all_preds, average = \"binary\")\n    return {\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }\n\ndef compute_metrics(p):\n    predictions, y_true = p\n    y_true = y_true.astype(int)\n    y_pred = [\n        [int(p > 0.5) for (p, l) in zip(pred, label) if l != -100]\n        for pred, label in zip(predictions, y_true)\n    ]\n    y_true = [\n        [l for l in label if l != -100] for label in y_true\n    ]\n    results = precision_recall_fscore_support(list(chain(*y_true)), list(chain(*y_pred)), average = \"binary\")\n    return {\n        \"token_precision\": results[0],\n        \"token_recall\": results[1],\n        \"token_f1\": results[2]\n    }","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:51.702333Z","iopub.execute_input":"2022-05-01T11:29:51.702574Z","iopub.status.idle":"2022-05-01T11:29:51.735506Z","shell.execute_reply.started":"2022-05-01T11:29:51.702541Z","shell.execute_reply":"2022-05-01T11:29:51.734222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = {\"train\": [], \"valid\": []}\nbest_loss = np.inf\n\nfor epoch in range(EPOCHS):\n    #training\n    model.train()\n    train_loss = AverageMeter()\n    pbar = tqdm(train_dl)\n    for batch in pbar:\n        optimizer.zero_grad()\n        input_ids = batch[0].to(DEVICE)\n        attention_mask = batch[1].to(DEVICE)\n        labels = batch[2].to(DEVICE)\n        offset_mapping = batch[3]\n        sequence_ids = batch[4]\n        logits = model(input_ids, attention_mask)\n        loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n        loss = loss_fct(logits, labels)\n        loss = torch.masked_select(loss, labels > -1).mean() # we should calculate at \"pn_history\"; labels at \"feature_text\" are -100 < -1\n        loss.backward()\n        optimizer.step()\n        train_loss.update(val = loss.item(), n = len(input_ids))\n        pbar.set_postfix(Loss = train_loss.avg)\n    print(epoch, train_loss.avg)\n    history[\"train\"].append(train_loss.avg)\n\n    #evaluation\n    model.eval()\n    valid_loss = AverageMeter()\n    with torch.no_grad():\n        for batch in tqdm(valid_dl):\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            labels = batch[2].to(DEVICE)\n            offset_mapping = batch[3]\n            sequence_ids = batch[4]\n            logits = model(input_ids, attention_mask)\n            loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n            loss = loss_fct(logits, labels)\n            loss = torch.masked_select(loss, labels > -1).mean()\n            valid_loss.update(val = loss.item(), n = len(input_ids))\n            pbar.set_postfix(Loss = valid_loss.avg)\n    print(epoch, valid_loss.avg)\n    history[\"valid\"].append(valid_loss.avg)\n\n    # save model\n    if valid_loss.avg < best_loss:\n        best_loss = valid_loss.avg\n        torch.save(model.state_dict(), \"nbme.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:29:51.737069Z","iopub.execute_input":"2022-05-01T11:29:51.737581Z","iopub.status.idle":"2022-05-01T11:59:50.472116Z","shell.execute_reply.started":"2022-05-01T11:29:51.737541Z","shell.execute_reply":"2022-05-01T11:59:50.471207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"nbme.pth\", map_location = DEVICE))\n\nmodel.eval()\npreds = []\noffsets = []\nseq_ids = []\nlbls = []\nwith torch.no_grad():\n    for batch in tqdm(valid_dl):\n        input_ids = batch[0].to(DEVICE)\n        attention_mask = batch[1].to(DEVICE)\n        labels = batch[2].to(DEVICE)\n        offset_mapping = batch[3]\n        sequence_ids = batch[4]\n        logits = model(input_ids, attention_mask)\n        preds.append(logits.cpu().numpy())\n        offsets.append(offset_mapping.numpy())\n        seq_ids.append(sequence_ids.numpy())\n        lbls.append(labels.cpu().numpy())\npreds = np.concatenate(preds, axis = 0)\noffsets = np.concatenate(offsets, axis = 0)\nseq_ids = np.concatenate(seq_ids, axis = 0)\nlbls = np.concatenate(lbls, axis = 0)\nlocation_preds = get_location_predictions(preds, offsets, seq_ids, test = False)\nscore = calculate_char_CV(location_preds, offsets, seq_ids, lbls)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:59:50.477202Z","iopub.execute_input":"2022-05-01T11:59:50.479416Z","iopub.status.idle":"2022-05-01T12:00:54.945711Z","shell.execute_reply.started":"2022-05-01T11:59:50.47937Z","shell.execute_reply":"2022-05-01T12:00:54.944855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NBMETestData(torch.utils.data.Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = self.tokenizer(\n            example[\"feature_text\"],\n            example[\"pn_history\"],\n            truncation = \"only_second\",\n            max_length = 416,\n            padding = \"max_length\",\n            return_offsets_mapping = True\n        )\n        tokenized[\"sequence_ids\"] = tokenized.sequence_ids()\n\n        input_ids = np.array(tokenized[\"input_ids\"])\n        attention_mask = np.array(tokenized[\"attention_mask\"])\n        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\")\n\n        return input_ids, attention_mask, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:00:54.947122Z","iopub.execute_input":"2022-05-01T12:00:54.947962Z","iopub.status.idle":"2022-05-01T12:00:54.956925Z","shell.execute_reply.started":"2022-05-01T12:00:54.947898Z","shell.execute_reply":"2022-05-01T12:00:54.955945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_info = pd.merge(test,features,how='left')\ntest_info = pd.merge(test_info, patient_notes,how='left')\ntest_info['pn_history'] = test_info['pn_history'].apply(lambda x:x.strip())\ntest_info['pn_history'] = test_info['pn_history'].apply(clean_spaces)\ntest_info['pn_history'] = test_info['pn_history'].apply(str.lower)\ntest_info['feature_text'] = test_info['feature_text'].apply(process_feature_text)\ntest_info['feature_text'] = test_info['feature_text'].apply(clean_spaces)\ntest_info['feature_text'] = test_info['feature_text'].apply(str.lower)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:00:54.958103Z","iopub.execute_input":"2022-05-01T12:00:54.958728Z","iopub.status.idle":"2022-05-01T12:00:54.987259Z","shell.execute_reply.started":"2022-05-01T12:00:54.958684Z","shell.execute_reply":"2022-05-01T12:00:54.986487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = NBMETestData(test_info, tokenizer)\ntest_dl = torch.utils.data.DataLoader(test_ds, batch_size = BATCH_SIZE * 2, pin_memory = True, shuffle = False, drop_last = False)\n\nmodel.eval()\npreds = []\noffsets = []\nseq_ids = []\nwith torch.no_grad():\n    for batch in tqdm(test_dl):\n        input_ids = batch[0].to(DEVICE)\n        attention_mask = batch[1].to(DEVICE)\n        offset_mapping = batch[2]\n        sequence_ids = batch[3]\n        logits = model(input_ids, attention_mask)\n        preds.append(logits.cpu().numpy())\n        offsets.append(offset_mapping.numpy())\n        seq_ids.append(sequence_ids.numpy())\n\npreds = np.concatenate(preds, axis = 0)\noffsets = np.concatenate(offsets, axis = 0)\nseq_ids = np.concatenate(seq_ids, axis = 0)\n\nlocation_preds = get_location_predictions(preds, offsets, seq_ids, test = True)\ntest_info[\"location\"] = location_preds\ntest_info[[\"id\", \"location\"]].to_csv(\"submission.csv\", index = False)\npd.read_csv(\"submission.csv\").head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:00:54.988482Z","iopub.execute_input":"2022-05-01T12:00:54.988839Z","iopub.status.idle":"2022-05-01T12:00:55.171792Z","shell.execute_reply.started":"2022-05-01T12:00:54.9888Z","shell.execute_reply":"2022-05-01T12:00:55.170942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We want to acknowledge various notebooks of this particular challenge for their creative approaches to the problem which are also adapted in our code.","metadata":{}}]}