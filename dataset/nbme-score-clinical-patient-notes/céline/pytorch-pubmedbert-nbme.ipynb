{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Thanks to SHUDIPTO TRAFDER's kernel https://www.kaggle.com/code/iamsdt/pytorch-bert-baseline-nbme/notebook\n#Check pep8 format\n#!pip install pycodestyle\n#!pip install --index-url https://test.pypi.org/simple/ nbpep8\n","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:35.775319Z","iopub.execute_input":"2022-03-23T11:26:35.775893Z","iopub.status.idle":"2022-03-23T11:26:35.780418Z","shell.execute_reply.started":"2022-03-23T11:26:35.775853Z","shell.execute_reply":"2022-03-23T11:26:35.77981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from nbpep8.nbpep8 import pep8\nfrom ast import literal_eval\nfrom itertools import chain\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModel, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:35.797291Z","iopub.execute_input":"2022-03-23T11:26:35.797686Z","iopub.status.idle":"2022-03-23T11:26:35.805017Z","shell.execute_reply.started":"2022-03-23T11:26:35.797653Z","shell.execute_reply":"2022-03-23T11:26:35.804059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#display options\npd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:35.813863Z","iopub.execute_input":"2022-03-23T11:26:35.814489Z","iopub.status.idle":"2022-03-23T11:26:35.821264Z","shell.execute_reply.started":"2022-03-23T11:26:35.814425Z","shell.execute_reply":"2022-03-23T11:26:35.82065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NBME - Score Clinical Patient Notes : analysis","metadata":{}},{"cell_type":"code","source":"path_patient_note = (\"../input/nbme-score-clinical-patient-notes/\"\n                     \"patient_notes.csv\")\npatient_notes = pd.read_csv(path_patient_note)\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:35.833649Z","iopub.execute_input":"2022-03-23T11:26:35.834417Z","iopub.status.idle":"2022-03-23T11:26:36.226535Z","shell.execute_reply.started":"2022-03-23T11:26:35.834375Z","shell.execute_reply":"2022-03-23T11:26:36.225581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_notes.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:36.228073Z","iopub.execute_input":"2022-03-23T11:26:36.228276Z","iopub.status.idle":"2022-03-23T11:26:36.23744Z","shell.execute_reply.started":"2022-03-23T11:26:36.228252Z","shell.execute_reply":"2022-03-23T11:26:36.236897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_notes.iloc[5000:5003, :]","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:36.238305Z","iopub.execute_input":"2022-03-23T11:26:36.238948Z","iopub.status.idle":"2022-03-23T11:26:36.255196Z","shell.execute_reply.started":"2022-03-23T11:26:36.238915Z","shell.execute_reply":"2022-03-23T11:26:36.254223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_features = (\"../input/nbme-score-clinical-patient-notes/\"\n                \"/features.csv\")\nfeatures = pd.read_csv(path_features)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:36.257686Z","iopub.execute_input":"2022-03-23T11:26:36.258297Z","iopub.status.idle":"2022-03-23T11:26:36.269834Z","shell.execute_reply.started":"2022-03-23T11:26:36.258252Z","shell.execute_reply":"2022-03-23T11:26:36.26852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:36.27074Z","iopub.execute_input":"2022-03-23T11:26:36.271486Z","iopub.status.idle":"2022-03-23T11:26:36.284555Z","shell.execute_reply.started":"2022-03-23T11:26:36.271432Z","shell.execute_reply":"2022-03-23T11:26:36.283612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:36.285876Z","iopub.execute_input":"2022-03-23T11:26:36.286301Z","iopub.status.idle":"2022-03-23T11:26:36.301574Z","shell.execute_reply.started":"2022-03-23T11:26:36.286261Z","shell.execute_reply":"2022-03-23T11:26:36.300648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features[features['case_num'] == 5]\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:36.302928Z","iopub.execute_input":"2022-03-23T11:26:36.303242Z","iopub.status.idle":"2022-03-23T11:26:36.315734Z","shell.execute_reply.started":"2022-03-23T11:26:36.303201Z","shell.execute_reply":"2022-03-23T11:26:36.314771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=features, x='case_num')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:36.317302Z","iopub.execute_input":"2022-03-23T11:26:36.317544Z","iopub.status.idle":"2022-03-23T11:26:36.557558Z","shell.execute_reply.started":"2022-03-23T11:26:36.317517Z","shell.execute_reply":"2022-03-23T11:26:36.556678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"body = '-'.join(features['feature_text'].apply(lambda x: x.lower()).tolist())","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:36.558829Z","iopub.execute_input":"2022-03-23T11:26:36.559039Z","iopub.status.idle":"2022-03-23T11:26:36.564768Z","shell.execute_reply.started":"2022-03-23T11:26:36.559013Z","shell.execute_reply":"2022-03-23T11:26:36.563881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import package\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Generate word cloud\nwordcloud = WordCloud(width=3000,\n                      height=2000,\n                      random_state=1, \n                      background_color='black',\n                      colormap='Pastel1',\n                      collocations=False,\n                      stopwords=['-', 'or', 'of', 'with', 'ago'])\\\n                      .generate(body)\n# Plot\nplt.rcParams[\"figure.figsize\"] = (15, 10)\nplt.imshow(wordcloud)\nplt.title('Wordcloud of features', fontsize=24)\nplt.axis('off')\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:36.567075Z","iopub.execute_input":"2022-03-23T11:26:36.567285Z","iopub.status.idle":"2022-03-23T11:26:52.984094Z","shell.execute_reply.started":"2022-03-23T11:26:36.56726Z","shell.execute_reply":"2022-03-23T11:26:52.983271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NBME - Score Clinical Patient Notes : modeling\n- **Framework:** Pytorch\n- **Model Architecture:**\n    - BERT\n    - Linear(768, 512)\n    - Linear(512, 1)\n- **LR:** 1e-5\n- **Batch Size:** 8\n- **Epoch:** 6\n- **Dropout:** 0.2\n- **Criterion:** BCEWithLogitsLoss\n- **Optimizer:** AdamW\n\n# Tokenizer params\n- **Max Lenght:** 416\n- **Padding:** max_lenght\n- **Truncation:** only_scond\n","metadata":{}},{"cell_type":"markdown","source":"# Helper Functions\n### 1. Datasets Helper Function\nneed to merge `features.csv`, `patient_notes.csv` with `train.csv`","metadata":{}},{"cell_type":"code","source":"BASE_URL = \"../input/nbme-score-clinical-patient-notes\"\n\n\ndef process_feature_text(text):\n    return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n\n\ndef prepare_datasets():\n    features = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    df = pd.read_csv(f\"{BASE_URL}/train.csv\")\n    df[\"annotation_list\"] = [literal_eval(x) for x in df[\"annotation\"]]\n    df[\"location_list\"] = [literal_eval(x) for x in df[\"location\"]]\n\n    merged = df.merge(notes, how=\"left\")\n    merged = merged.merge(features, how=\"left\")\n\n    merged[\"feature_text\"] = [process_feature_text(x) for x in\n                              merged[\"feature_text\"]]\n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n\n    return merged\n\n\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:52.985497Z","iopub.execute_input":"2022-03-23T11:26:52.985913Z","iopub.status.idle":"2022-03-23T11:26:52.997043Z","shell.execute_reply.started":"2022-03-23T11:26:52.985875Z","shell.execute_reply":"2022-03-23T11:26:52.995769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Tokenizer Helper Function","metadata":{}},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\n\ndef tokenize_and_add_labels(tokenizer, data, config):\n    out = tokenizer(\n        data[\"feature_text\"],\n        data[\"pn_history\"],\n        truncation=config['truncation'],\n        max_length=config['max_length'],\n        padding=config['padding'],\n        return_offsets_mapping=config['return_offsets_mapping']\n    )\n    labels = [0.0] * len(out[\"input_ids\"])\n    out[\"location_int\"] = loc_list_to_ints(data[\"location_list\"])\n    out[\"sequence_ids\"] = out.sequence_ids()\n\n    for idx, (seq_id, offsets) in enumerate(zip(out[\"sequence_ids\"],\n                                                out[\"offset_mapping\"])):\n        if not seq_id or seq_id == 0:\n            labels[idx] = -1\n            continue\n\n        token_start, token_end = offsets\n        for feature_start, feature_end in out[\"location_int\"]:\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                break\n\n    out[\"labels\"] = labels\n\n    return out\n\n\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:52.99869Z","iopub.execute_input":"2022-03-23T11:26:52.998946Z","iopub.status.idle":"2022-03-23T11:26:53.016106Z","shell.execute_reply.started":"2022-03-23T11:26:52.998913Z","shell.execute_reply":"2022-03-23T11:26:53.015335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Prediction and Score Helper Function","metadata":{}},{"cell_type":"code","source":"def get_location_predictions(preds, offset_mapping, sequence_ids, test=False):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        pred = 1 / (1 + np.exp(-pred))\n        start_idx = None\n        end_idx = None\n        current_preds = []\n        for pred, offset, seq_id in zip(pred, offsets, seq_ids):\n            if seq_id is None or seq_id == 0:\n                continue\n\n            if pred > 0.5:\n                if start_idx is None:\n                    start_idx = offset[0]\n                end_idx = offset[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n            \n    return all_predictions\n\n\n\ndef calculate_char_cv(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping,\n                                               sequence_ids, labels):\n\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros(num_chars)\n\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n\n        char_preds = np.zeros(num_chars)\n\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n            \n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n\n    results = precision_recall_fscore_support(all_labels,\n                                              all_preds,\n                                              average=\"binary\",\n                                              labels=np.unique(all_preds))\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n\n    return {\n        \"Accuracy\": accuracy,\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }\n\n\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:53.01726Z","iopub.execute_input":"2022-03-23T11:26:53.017643Z","iopub.status.idle":"2022-03-23T11:26:53.037436Z","shell.execute_reply.started":"2022-03-23T11:26:53.017601Z","shell.execute_reply":"2022-03-23T11:26:53.036429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        data = self.data.iloc[idx]\n        tokens = tokenize_and_add_labels(self.tokenizer, data, self.config)\n\n        input_ids = np.array(tokens[\"input_ids\"])\n        attention_mask = np.array(tokens[\"attention_mask\"])\n        token_type_ids = np.array(tokens[\"token_type_ids\"])\n\n        labels = np.array(tokens[\"labels\"])\n        offset_mapping = np.array(tokens['offset_mapping'])\n        sequence_ids = np.array(tokens['sequence_ids']).astype(\"float16\")\n        \n        return input_ids, attention_mask, token_type_ids, labels, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:53.038985Z","iopub.execute_input":"2022-03-23T11:26:53.039229Z","iopub.status.idle":"2022-03-23T11:26:53.054991Z","shell.execute_reply.started":"2022-03-23T11:26:53.039197Z","shell.execute_reply":"2022-03-23T11:26:53.05429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- Lets use **Pub med BERT** base Architecture downloaded at https://www.kaggle.com/jpmiller/layoutlm\n- Also Used 2 FC layers\n","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\nclass CustomModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(config['model_name']) \n        self.dropout = nn.Dropout(p=config['dropout'])\n        self.config = config\n        self.fc1 = nn.Linear(768, 512)\n        self.fc2 = nn.Linear(512, 1)\n        \n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids)\n        logits = F.relu(self.fc1(outputs[0]))\n        logits = self.fc2(self.dropout(logits)).squeeze(-1)\n        return logits\n    \n    \n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:53.056065Z","iopub.execute_input":"2022-03-23T11:26:53.056389Z","iopub.status.idle":"2022-03-23T11:26:53.074969Z","shell.execute_reply.started":"2022-03-23T11:26:53.056354Z","shell.execute_reply":"2022-03-23T11:26:53.073962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters\n","metadata":{}},{"cell_type":"code","source":"hyperparameters = {\n    \"max_length\": 416,\n    \"padding\": \"max_length\",\n    \"return_offsets_mapping\": True,\n    \"truncation\": \"only_second\",\n    \"model_name\": (\"../input/layoutlm/BiomedNLP-PubMedBERT\"\n                   \"-base-uncased-abstract-fulltext\"),\n    \"dropout\": 0.2,\n    \"lr\": 1e-5,\n    \"test_size\": 0.2,\n    \"seed\": 1268,\n    \"batch_size\": 8\n}\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:53.076528Z","iopub.execute_input":"2022-03-23T11:26:53.077185Z","iopub.status.idle":"2022-03-23T11:26:53.088942Z","shell.execute_reply.started":"2022-03-23T11:26:53.077152Z","shell.execute_reply":"2022-03-23T11:26:53.088011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Datasets\nTrain and Test split: 20%\n\nTotal Data:\n- Train: 11440\n- Test: 2860","metadata":{}},{"cell_type":"code","source":"train_df = prepare_datasets()\n\nX_train, X_test = train_test_split(train_df,\n                                   test_size=hyperparameters['test_size'],\n                                   random_state=hyperparameters['seed'])\n\n\nprint(\"Train size\", len(X_train))\nprint(\"Test Size\", len(X_test))\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:53.090116Z","iopub.execute_input":"2022-03-23T11:26:53.090343Z","iopub.status.idle":"2022-03-23T11:26:53.950217Z","shell.execute_reply.started":"2022-03-23T11:26:53.090316Z","shell.execute_reply":"2022-03-23T11:26:53.949179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_test[['id', 'case_num', 'pn_num', 'feature_num']]","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:53.951647Z","iopub.execute_input":"2022-03-23T11:26:53.951959Z","iopub.status.idle":"2022-03-23T11:26:53.956633Z","shell.execute_reply.started":"2022-03-23T11:26:53.951919Z","shell.execute_reply":"2022-03-23T11:26:53.955768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_test[['id', 'case_num', 'pn_num', 'feature_num']].to_csv('test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:53.958068Z","iopub.execute_input":"2022-03-23T11:26:53.958441Z","iopub.status.idle":"2022-03-23T11:26:53.980312Z","shell.execute_reply.started":"2022-03-23T11:26:53.9584Z","shell.execute_reply":"2022-03-23T11:26:53.979531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(hyperparameters['model_name'])\n\ntraining_data = CustomDataset(X_train,\n                              tokenizer,\n                              hyperparameters)\n\ntrain_dataloader = DataLoader(training_data,\n                              batch_size=hyperparameters['batch_size'],\n                              shuffle=True)\n\ntest_data = CustomDataset(X_test, tokenizer, hyperparameters)\n\ntest_dataloader = DataLoader(test_data,\n                             batch_size=hyperparameters['batch_size'],\n                             shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:53.981995Z","iopub.execute_input":"2022-03-23T11:26:53.982369Z","iopub.status.idle":"2022-03-23T11:26:54.045369Z","shell.execute_reply.started":"2022-03-23T11:26:53.982336Z","shell.execute_reply":"2022-03-23T11:26:54.044333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train\nLets train the model\nwith BCEWithLogitsLoss and AdamW as optimizer\n\n**Notes:** on BCEWithLogitsLoss, the default value for reduction is `mean` (the sum of the output will be divided by the number of elements in the output). If we use this default value, it will produce negative loss. Because we have some negative labels. To fix this negative loss issue, we can use `none` as parameter. To calculate the mean, first, we have to filter out the negative values. [DOC](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = CustomModel(hyperparameters).to(DEVICE)\n\ncriterion = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\noptimizer = optim.AdamW(model.parameters(), lr=hyperparameters['lr'])\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:54.04741Z","iopub.execute_input":"2022-03-23T11:26:54.047789Z","iopub.status.idle":"2022-03-23T11:26:55.810343Z","shell.execute_reply.started":"2022-03-23T11:26:54.047759Z","shell.execute_reply":"2022-03-23T11:26:55.809465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, criterion):\n        model.train()\n        train_loss = []\n\n        for batch in tqdm(dataloader):\n            optimizer.zero_grad()\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            token_type_ids = batch[2].to(DEVICE)\n            labels = batch[3].to(DEVICE)\n\n            logits = model(input_ids,\n                           attention_mask,\n                           token_type_ids)\n            loss = criterion(logits, labels)\n            # since, we have\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            train_loss.append(loss.item() * input_ids.size(0))\n            loss.backward()\n            # clip the the gradients to 1.0.\n            #It helps in preventing the exploding gradient problem\n            # it's also improve f1 accuracy slightly\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n        return sum(train_loss)/len(train_loss)\n\n    \n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:55.812495Z","iopub.execute_input":"2022-03-23T11:26:55.812807Z","iopub.status.idle":"2022-03-23T11:26:55.820625Z","shell.execute_reply.started":"2022-03-23T11:26:55.812765Z","shell.execute_reply":"2022-03-23T11:26:55.819794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, dataloader, criterion):\n        model.eval()\n        valid_loss = []\n        preds = []\n        offsets = []\n        seq_ids = []\n        valid_labels = []\n\n        for batch in tqdm(dataloader):\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            token_type_ids = batch[2].to(DEVICE)\n            labels = batch[3].to(DEVICE)\n            offset_mapping = batch[4]\n            sequence_ids = batch[5]\n\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss = criterion(logits, labels)\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            valid_loss.append(loss.item() * input_ids.size(0))\n\n            preds.append(logits.detach().cpu().numpy())\n            offsets.append(offset_mapping.numpy())\n            seq_ids.append(sequence_ids.numpy())\n            valid_labels.append(labels.detach().cpu().numpy())\n\n        preds = np.concatenate(preds, axis=0)\n        offsets = np.concatenate(offsets, axis=0)\n        seq_ids = np.concatenate(seq_ids, axis=0)\n        valid_labels = np.concatenate(valid_labels, axis=0)\n        location_preds = get_location_predictions(preds,\n                                                  offsets,\n                                                  seq_ids,\n                                                  test=False)\n        score = calculate_char_cv(location_preds,\n                                  offsets,\n                                  seq_ids,\n                                  valid_labels)\n\n        return sum(valid_loss)/len(valid_loss), score\n\n\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:55.822076Z","iopub.execute_input":"2022-03-23T11:26:55.822374Z","iopub.status.idle":"2022-03-23T11:26:55.839785Z","shell.execute_reply.started":"2022-03-23T11:26:55.822336Z","shell.execute_reply":"2022-03-23T11:26:55.838901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\ntrain_loss_data, valid_loss_data = [], []\nscore_data_list = []\nvalid_loss_min = np.Inf\nsince = time.time()\nepochs = 25","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:25:26.258975Z","iopub.execute_input":"2022-03-23T11:25:26.259617Z","iopub.status.idle":"2022-03-23T11:25:26.272958Z","shell.execute_reply.started":"2022-03-23T11:25:26.259574Z","shell.execute_reply":"2022-03-23T11:25:26.272337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_loss = np.inf\n\nfor i in range(epochs):\n    print(\"Epoch: {}/{}\".format(i + 1, epochs))\n    # first train model\n    train_loss = train_model(model, train_dataloader, optimizer, criterion)\n    train_loss_data.append(train_loss)\n    print(f\"Train loss: {train_loss}\")\n    # evaluate model\n    valid_loss, score = eval_model(model, test_dataloader, criterion)\n    valid_loss_data.append(valid_loss)\n    score_data_list.append(score)\n    print(f\"Valid loss: {valid_loss}\")\n    print(f\"Valid score: {score}\")\n    \n    if valid_loss < best_loss:\n        best_loss = valid_loss\n        torch.save(model.state_dict(), \"nbme_pubmed_bert.pth\")\n\n    \ntime_elapsed = time.time() - since\nprint('Training completed in {:.0f}m {:.0f}s'.format(\n    time_elapsed // 60, time_elapsed % 60))","metadata":{"execution":{"iopub.status.busy":"2022-03-22T23:17:30.198777Z","iopub.execute_input":"2022-03-22T23:17:30.198973Z","iopub.status.idle":"2022-03-23T04:30:20.617958Z","shell.execute_reply.started":"2022-03-22T23:17:30.198945Z","shell.execute_reply":"2022-03-23T04:30:20.617186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nplt.plot(train_loss_data, label=\"Training loss\")\nplt.plot(valid_loss_data, label=\"validation loss\")\nplt.legend(frameon=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T04:30:20.619361Z","iopub.execute_input":"2022-03-23T04:30:20.619809Z","iopub.status.idle":"2022-03-23T04:30:20.864914Z","shell.execute_reply.started":"2022-03-23T04:30:20.61977Z","shell.execute_reply":"2022-03-23T04:30:20.864247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nscore_df = pd.DataFrame.from_dict(score_data_list)\nscore_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T04:30:20.866278Z","iopub.execute_input":"2022-03-23T04:30:20.866554Z","iopub.status.idle":"2022-03-23T04:30:20.880935Z","shell.execute_reply.started":"2022-03-23T04:30:20.866519Z","shell.execute_reply":"2022-03-23T04:30:20.880271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare For Testing","metadata":{}},{"cell_type":"markdown","source":"Load best model","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"nbme_pubmed_bert.pth\", map_location = DEVICE))","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:26:55.843472Z","iopub.execute_input":"2022-03-23T11:26:55.843904Z","iopub.status.idle":"2022-03-23T11:26:56.305567Z","shell.execute_reply.started":"2022-03-23T11:26:55.843868Z","shell.execute_reply":"2022-03-23T11:26:56.304733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_df():\n    feats = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    test = pd.read_csv(f\"{BASE_URL}/test.csv\")\n\n    merged = test.merge(notes, how=\"left\")\n    merged = merged.merge(feats, how=\"left\")\n\n    def process_feature_text(text):\n        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n    \n    merged[\"feature_text\"] = [process_feature_text(x) \n                              for x in merged[\"feature_text\"]]\n    \n    return merged\n\n\nclass SubmissionDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = self.tokenizer(\n            example[\"feature_text\"],\n            example[\"pn_history\"],\n            truncation = self.config['truncation'],\n            max_length = self.config['max_length'],\n            padding = self.config['padding'],\n            return_offsets_mapping = self.config['return_offsets_mapping']\n        )\n        tokenized[\"sequence_ids\"] = tokenized.sequence_ids()\n\n        input_ids = np.array(tokenized[\"input_ids\"])\n        attention_mask = np.array(tokenized[\"attention_mask\"])\n        token_type_ids = np.array(tokenized[\"token_type_ids\"])\n        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n        sequence_ids = np.array(tokenized[\"sequence_ids\"])\\\n                        .astype(\"float16\")\n\n        return input_ids, attention_mask, token_type_ids, offset_mapping, sequence_ids\n\n\ntest_df = create_test_df()\n\nsubmission_data = SubmissionDataset(test_df, tokenizer, hyperparameters)\nsubmission_dataloader = DataLoader(submission_data,\n                                   batch_size=hyperparameters['batch_size'],\n                                   shuffle=False)\n\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:27:00.175121Z","iopub.execute_input":"2022-03-23T11:27:00.175517Z","iopub.status.idle":"2022-03-23T11:27:00.56108Z","shell.execute_reply.started":"2022-03-23T11:27:00.175347Z","shell.execute_reply":"2022-03-23T11:27:00.560051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\npreds = []\noffsets = []\nseq_ids = []\n\nfor batch in tqdm(submission_dataloader):\n    input_ids = batch[0].to(DEVICE)\n    attention_mask = batch[1].to(DEVICE)\n    token_type_ids = batch[2].to(DEVICE)\n    offset_mapping = batch[3]\n    sequence_ids = batch[4]\n\n    logits = model(input_ids, attention_mask, token_type_ids)\n    \n    preds.append(logits.detach().cpu().numpy())\n    offsets.append(offset_mapping.numpy())\n    seq_ids.append(sequence_ids.numpy())\n\npreds = np.concatenate(preds, axis=0)\noffsets = np.concatenate(offsets, axis=0)\nseq_ids = np.concatenate(seq_ids, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:27:09.070472Z","iopub.execute_input":"2022-03-23T11:27:09.070755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_preds = get_location_predictions(preds, offsets, seq_ids, test=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:22:00.019177Z","iopub.execute_input":"2022-03-23T11:22:00.019526Z","iopub.status.idle":"2022-03-23T11:22:00.0312Z","shell.execute_reply.started":"2022-03-23T11:22:00.019483Z","shell.execute_reply":"2022-03-23T11:22:00.030201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(location_preds), len(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:22:00.502782Z","iopub.execute_input":"2022-03-23T11:22:00.503668Z","iopub.status.idle":"2022-03-23T11:22:00.510305Z","shell.execute_reply.started":"2022-03-23T11:22:00.503614Z","shell.execute_reply":"2022-03-23T11:22:00.509536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"location\"] = location_preds","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:22:01.565182Z","iopub.execute_input":"2022-03-23T11:22:01.565677Z","iopub.status.idle":"2022-03-23T11:22:01.570223Z","shell.execute_reply.started":"2022-03-23T11:22:01.565623Z","shell.execute_reply":"2022-03-23T11:22:01.569334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[[\"id\", \"location\"]].to_csv(\"submission.csv\", index = False)\npd.read_csv(\"submission.csv\").head()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:22:02.199016Z","iopub.execute_input":"2022-03-23T11:22:02.200054Z","iopub.status.idle":"2022-03-23T11:22:02.217395Z","shell.execute_reply.started":"2022-03-23T11:22:02.200013Z","shell.execute_reply":"2022-03-23T11:22:02.216736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(f\"{BASE_URL}/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:22:49.760434Z","iopub.execute_input":"2022-03-23T11:22:49.760752Z","iopub.status.idle":"2022-03-23T11:22:49.775234Z","shell.execute_reply.started":"2022-03-23T11:22:49.760718Z","shell.execute_reply":"2022-03-23T11:22:49.774352Z"},"trusted":true},"execution_count":null,"outputs":[]}]}