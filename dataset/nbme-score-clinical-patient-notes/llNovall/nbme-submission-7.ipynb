{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport tqdm.notebook as tqdm\nimport ast\nimport re\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport transformers\nfrom transformers import RobertaTokenizerFast\nimport numpy as np\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-21T15:05:15.783137Z","iopub.execute_input":"2022-04-21T15:05:15.783449Z","iopub.status.idle":"2022-04-21T15:05:22.500412Z","shell.execute_reply.started":"2022-04-21T15:05:15.783383Z","shell.execute_reply":"2022-04-21T15:05:22.499699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:22.501907Z","iopub.execute_input":"2022-04-21T15:05:22.502145Z","iopub.status.idle":"2022-04-21T15:05:22.507333Z","shell.execute_reply.started":"2022-04-21T15:05:22.502112Z","shell.execute_reply":"2022-04-21T15:05:22.506168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.tqdm_notebook.pandas()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:22.508726Z","iopub.execute_input":"2022-04-21T15:05:22.50899Z","iopub.status.idle":"2022-04-21T15:05:22.520555Z","shell.execute_reply.started":"2022-04-21T15:05:22.50895Z","shell.execute_reply":"2022-04-21T15:05:22.519805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device('cuda')\n\nMAX_LENGTH = 384\nDOC_STRIDE = 128\nBATCH_SIZE = 1","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:22.524181Z","iopub.execute_input":"2022-04-21T15:05:22.524376Z","iopub.status.idle":"2022-04-21T15:05:22.530528Z","shell.execute_reply.started":"2022-04-21T15:05:22.524348Z","shell.execute_reply":"2022-04-21T15:05:22.529719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_CONFIG = transformers.RobertaConfig(\n                  attention_probs_dropout_prob= 0.1,\n                  bos_token_id= 0,\n                  classifier_dropout= None,\n                  eos_token_id= 2,\n                  hidden_act= \"gelu\",\n                  hidden_dropout_prob= 0.1,\n                  hidden_size= 768,\n                  initializer_range= 0.02,\n                  intermediate_size= 3072,\n                  layer_norm_eps= 1e-05,\n                  max_position_embeddings= 514,\n                  model_type= \"roberta\",\n                  num_attention_heads= 12,\n                  num_hidden_layers= 12,\n                  pad_token_id= 1,\n                  position_embedding_type= \"absolute\",\n                  transformers_version= \"4.17.0\",\n                  type_vocab_size= 1,\n                  use_cache= True,\n                  vocab_size= 50265\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:22.531845Z","iopub.execute_input":"2022-04-21T15:05:22.532149Z","iopub.status.idle":"2022-04-21T15:05:22.541201Z","shell.execute_reply.started":"2022-04-21T15:05:22.532103Z","shell.execute_reply":"2022-04-21T15:05:22.540528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_data = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\npatient_notes_data = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\ntest_data = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\ntrain_data = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')\nsubmission_data = pd.read_csv('../input/nbme-score-clinical-patient-notes/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:22.542655Z","iopub.execute_input":"2022-04-21T15:05:22.542944Z","iopub.status.idle":"2022-04-21T15:05:23.19549Z","shell.execute_reply.started":"2022-04-21T15:05:22.54291Z","shell.execute_reply":"2022-04-21T15:05:23.194761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NBMETestTensorDataset:\n    def __init__(self, tokenized_data):\n        self.input_ids = tokenized_data['input_ids']\n        self.attention_masks = tokenized_data['attention_mask']\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        \n        data = {}\n        data['input_ids'] = torch.tensor(self.input_ids[idx])\n        data['attention_mask'] = torch.tensor(self.attention_masks[idx])\n        return data","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:23.197462Z","iopub.execute_input":"2022-04-21T15:05:23.197664Z","iopub.status.idle":"2022-04-21T15:05:23.204422Z","shell.execute_reply.started":"2022-04-21T15:05:23.19764Z","shell.execute_reply":"2022-04-21T15:05:23.203771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_start_end_of_second_sequence(sequence):\n    \n    is_searching_start = True\n    is_searching_end = False\n    \n    sequence_start = -1\n    sequence_end = -1\n    \n    for n in range(len(sequence)):\n        \n        if is_searching_end == True:\n            if sequence[n] == None:\n                sequence_end = n\n                break\n        if is_searching_start == True:\n            if sequence[n] == 1:\n                sequence_start = n\n                is_searching_start = False\n                is_searching_end = True\n                \n    return sequence_start, sequence_end\ndef tokenize_test_data(datas):\n    datas = datas.reset_index().to_dict(orient='index')\n    \n    tokenized_datas = {\n                        'input_ids' : [],\n                        'attention_mask' : []\n                      }\n    for data_idx in tqdm.tqdm_notebook(range(len(datas))):\n        data = datas[data_idx]\n\n        tokenized_data = TOKENIZER(text = data['feature_text'], \n                                   text_pair=data['pn_history'],\n                                   return_offsets_mapping=True, \n                                   padding='max_length', \n                                   max_length=MAX_LENGTH, \n                                   truncation = 'only_second',stride=DOC_STRIDE\n                                      )\n        tokenized_datas[\"input_ids\"].append(tokenized_data[\"input_ids\"])\n        tokenized_datas[\"attention_mask\"].append(tokenized_data[\"attention_mask\"])\n        \n    return tokenized_datas","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:23.206071Z","iopub.execute_input":"2022-04-21T15:05:23.206638Z","iopub.status.idle":"2022-04-21T15:05:23.218795Z","shell.execute_reply.started":"2022-04-21T15:05:23.206601Z","shell.execute_reply":"2022-04-21T15:05:23.21817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_location(row):\n    matches = re.findall('(\\d+)', str(row))\n    start_positions = []\n    end_positions = []\n    if len(matches) > 0:\n\n        for n in range(0, len(matches), 2):\n            start_positions.append(int(matches[n]))\n            end_positions.append(int(matches[n+1]))\n            \n        \n    return start_positions, end_positions\n\ndef preprocess_features_data(features_data : pd.DataFrame):\n    \n    data = features_data.copy()\n\n    print(f'Cleaning Feature_text...')\n    data['feature_text'] = data['feature_text'].progress_apply(lambda x: re.sub('-OR-', ' or ', x))\n    data['feature_text'] = data['feature_text'].progress_apply(lambda x: re.sub('-I-year', ' 1 year', x))\n    data['feature_text'] = data['feature_text'].progress_apply(lambda x: re.sub('-', ' ', x))\n    data['feature_text'] = data['feature_text'].progress_apply(lambda x: str.strip(x))\n    \n    return data\n\ndef preprocess_patient_notes_data(patient_notes_data : pd.DataFrame):\n\n    data = patient_notes_data.copy()\n    print(f'Cleaning Patient_notes...')\n\n    \n    data['pn_history'] = data['pn_history'].progress_apply(lambda x : re.sub(' mM ', 'M', x))\n    data['pn_history'] = data['pn_history'].progress_apply(lambda x : re.sub('YOF', 'YO F', x))\n    data['pn_history'] = data['pn_history'].progress_apply(lambda x : re.sub('yof', 'yo f', x))\n    data['pn_history'] = data['pn_history'].progress_apply(lambda x : re.sub('malepresents', 'male presents', x))\n    data['pn_history'] = data['pn_history'].progress_apply(lambda x : re.sub('AAF', ' F ', x))\n    data['pn_history'] = data['pn_history'].progress_apply(lambda x : re.sub('YOM', 'YO M', x))\n    data['pn_history'] = data['pn_history'].progress_apply(lambda x : re.sub(' FM ', ' F ', x))\n    data['pn_history'] = data['pn_history'].progress_apply(lambda x : re.sub('17yoMotherwise', '17 yo M otherwise', x))\n    \n    return data\n\ndef process_data(train_data: pd.DataFrame, features_data: pd.DataFrame, patient_notes_data: pd.DataFrame):\n    data = train_data.copy()\n    \n    print(f'Adding Feature_text to data...')\n    data['feature_text'] = data[['case_num','feature_num']].progress_apply(lambda x: \n                            features_data.loc[(features_data['case_num'] == x['case_num']) & (features_data['feature_num'] == x['feature_num'])].values[0][2], \n                                                                        axis=1)\n    \n    print(f'Adding pn_history to data...')\n    data['pn_history'] = data[['case_num','pn_num']].progress_apply(lambda x: \n                            patient_notes_data.loc[(patient_notes_data['case_num'] == x['case_num']) & (patient_notes_data['pn_num'] == x['pn_num'])].values[0][2], \n                                                                        axis=1)\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:23.220205Z","iopub.execute_input":"2022-04-21T15:05:23.220499Z","iopub.status.idle":"2022-04-21T15:05:23.239711Z","shell.execute_reply.started":"2022-04-21T15:05:23.220464Z","shell.execute_reply":"2022-04-21T15:05:23.239051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TOKENIZER = RobertaTokenizerFast(vocab_file='../input/robertatokenizer/vocab.json',\n                                     merges_file='../input/robertatokenizer/merges.txt', \n                                     tokenizer_file='../input/robertatokenizer/tokenizer.json')","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:23.242843Z","iopub.execute_input":"2022-04-21T15:05:23.243277Z","iopub.status.idle":"2022-04-21T15:05:23.362497Z","shell.execute_reply.started":"2022-04-21T15:05:23.243245Z","shell.execute_reply":"2022-04-21T15:05:23.361645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_features_data = preprocess_features_data(features_data)\np_patient_notes_data = preprocess_patient_notes_data(patient_notes_data)\np_test_data = process_data(test_data,p_features_data, p_patient_notes_data)\n# p_train_data = process_data(train_data,p_features_data, p_patient_notes_data)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:23.364475Z","iopub.execute_input":"2022-04-21T15:05:23.364715Z","iopub.status.idle":"2022-04-21T15:05:25.246225Z","shell.execute_reply.started":"2022-04-21T15:05:23.364688Z","shell.execute_reply":"2022-04-21T15:05:25.24552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_test_data = tokenize_test_data(p_test_data)\n# t_train_data = tokenize_test_data(p_train_data)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:25.247484Z","iopub.execute_input":"2022-04-21T15:05:25.247891Z","iopub.status.idle":"2022-04-21T15:05:25.300682Z","shell.execute_reply.started":"2022-04-21T15:05:25.24784Z","shell.execute_reply":"2022-04-21T15:05:25.300018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_nbme_dataset = NBMETestTensorDataset(t_test_data)\n# train_nbme_dataset = NBMETestTensorDataset(t_train_data)\n\ntest_dl = DataLoader(test_nbme_dataset, BATCH_SIZE)\n# train_dl = DataLoader(train_nbme_dataset, BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:25.30207Z","iopub.execute_input":"2022-04-21T15:05:25.302508Z","iopub.status.idle":"2022-04-21T15:05:25.306813Z","shell.execute_reply.started":"2022-04-21T15:05:25.302474Z","shell.execute_reply":"2022-04-21T15:05:25.30593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NBMEModel(nn.Module):\n    def __init__(self,roberta_config):\n        super(NBMEModel, self).__init__()\n        \n        self.loss_fn = nn.functional.binary_cross_entropy_with_logits\n\n        self.roberta = transformers.RobertaModel(roberta_config)\n        self.roberta.gradient_checkpointing_enable = True\n        \n        self.start_pos = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(in_features=768, out_features=1)\n        )\n        \n        self.end_pos = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(in_features=768, out_features=1)\n        )\n\n        self.sigmoid_layer = nn.Sigmoid()\n\n    def forward(self, data):\n        \n        output= self.roberta(data['input_ids'], data['attention_mask'])\n\n        start_logits = self.start_pos(output.last_hidden_state)\n        end_logits = self.end_pos(output.last_hidden_state)\n        \n        loss = None\n\n        loss_start = self.loss_fn(start_logits, data['start_positions'].view(len(data['start_positions']), MAX_LENGTH, -1), reduction='mean')\n        loss_end = self.loss_fn(end_logits, data['end_positions'].view(len(data['end_positions']), MAX_LENGTH, -1),reduction='mean')\n            \n        loss = loss_start + loss_end\n\n        return loss\n    \n    def predict(self, data):\n        \n        output= self.roberta(data['input_ids'], data['attention_mask'])\n        \n        start_logits = self.start_pos(output.last_hidden_state)\n        end_logits = self.end_pos(output.last_hidden_state)\n\n        start_sig = self.sigmoid_layer(start_logits)\n        end_sig = self.sigmoid_layer(end_logits)\n        \n        return start_sig, end_sig\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:25.308178Z","iopub.execute_input":"2022-04-21T15:05:25.308718Z","iopub.status.idle":"2022-04-21T15:05:25.321431Z","shell.execute_reply.started":"2022-04-21T15:05:25.308667Z","shell.execute_reply":"2022-04-21T15:05:25.320743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = NBMEModel(ROBERTA_CONFIG)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:25.322734Z","iopub.execute_input":"2022-04-21T15:05:25.323224Z","iopub.status.idle":"2022-04-21T15:05:27.351187Z","shell.execute_reply.started":"2022-04-21T15:05:25.323181Z","shell.execute_reply":"2022-04-21T15:05:27.350366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:27.354396Z","iopub.execute_input":"2022-04-21T15:05:27.354609Z","iopub.status.idle":"2022-04-21T15:05:32.278527Z","shell.execute_reply.started":"2022-04-21T15:05:27.354582Z","shell.execute_reply":"2022-04-21T15:05:32.27782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('../input/nbme-roberta-final/Model_15.pt', map_location=DEVICE))","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:32.279624Z","iopub.execute_input":"2022-04-21T15:05:32.280039Z","iopub.status.idle":"2022-04-21T15:05:36.89046Z","shell.execute_reply.started":"2022-04-21T15:05:32.280001Z","shell.execute_reply":"2022-04-21T15:05:36.889761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(raw_data, data_dl, model, num_pred, acc_req, device):\n    num = 0\n    \n    submission = {'id': [],\n                  'location' : []\n                 }\n    model.eval()\n    \n    with torch.no_grad():\n        for data in tqdm.tqdm_notebook(data_dl, total=len(data_dl)):\n            for k, v in data.items():\n                    data[k] = v.to(device)\n            preds_start, preds_end = model.predict(data)\n\n            for count in range(len(preds_start)):\n\n                start_confidence, start_prediction = preds_start[count].topk(num_pred, dim=0)\n                end_confidence, end_prediction = preds_end[count].topk(num_pred, dim=0)\n\n                possible_start = []\n                possible_end = []\n\n                for n in range(len(start_confidence)):\n                    if (start_confidence[n] >= acc_req) & (end_confidence[n] >= acc_req):\n                        possible_start.append(start_prediction[n].cpu().detach().numpy()[0])\n                        possible_end.append(end_prediction[n].cpu().detach().numpy()[0])\n                \n                pred_size = len(possible_start) if len(possible_start) < len(possible_end) else len(possible_end)\n#                 print(f'PRED : {pred_size}, len_start : {len(possible_start)}, len_end : {len(possible_end)}')\n                locations = []\n                feature_text = raw_data.loc[num,]['feature_text']\n                pn_history = raw_data.loc[num,]['pn_history']\n\n                tokenized_data = TOKENIZER(\n#                     feature_text, \n                                            pn_history, \n                                           return_offsets_mapping=True,\n#                                            max_length=MAX_LENGTH, \n#                                            truncation = 'only_second'\n#                                            ,stride=DOC_STRIDE\n                                          )\n                offset_mapping = tokenized_data.pop('offset_mapping')\n\n#                 sequence = tokenized_data.sequence_ids()\n\n#                 sequence_start , sequence_end = get_start_end_of_second_sequence(sequence)\n\n                if pred_size > 0:\n                    for pred_idx in range(pred_size):\n                        if (possible_start[pred_idx] <= possible_end[pred_idx]) & (len(offset_mapping) > (possible_end[pred_idx])):\n\n                                start = possible_start[pred_idx]\n                                end = possible_end[pred_idx]\n\n                                char_start = offset_mapping[start][0]\n                                char_end = offset_mapping[end][1]\n                                if char_end == 0:\n                                    char_end = offset_mapping[end - 1][1]\n#                                 print(f'Start : {start}, End : {end}, Char_Start : {char_start}, Char_end : {char_end}')\n                                if char_start < char_end:\n                                    locations.append(f'{char_start} {char_end}')\n\n\n                submission['id'].append(raw_data.loc[num,]['id'])\n                if len(locations) == 0:\n                    submission['location'].append(np.nan)\n                else:\n                    submission['location'].append(\";\".join(locations))\n                num += 1\n            \n    return submission","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:36.891867Z","iopub.execute_input":"2022-04-21T15:05:36.892341Z","iopub.status.idle":"2022-04-21T15:05:36.907908Z","shell.execute_reply.started":"2022-04-21T15:05:36.892303Z","shell.execute_reply":"2022-04-21T15:05:36.907115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[:5]","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:05:36.909007Z","iopub.execute_input":"2022-04-21T15:05:36.909329Z","iopub.status.idle":"2022-04-21T15:05:36.929491Z","shell.execute_reply.started":"2022-04-21T15:05:36.909293Z","shell.execute_reply":"2022-04-21T15:05:36.928711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = predict(p_test_data,test_dl, model, 6, 0.10, DEVICE)\ndf_submission = pd.DataFrame(submission)\ndf_submission.to_csv('submission.csv', index=False)\ndf_submission","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:06:24.787988Z","iopub.execute_input":"2022-04-21T15:06:24.788246Z","iopub.status.idle":"2022-04-21T15:06:24.930455Z","shell.execute_reply.started":"2022-04-21T15:06:24.788216Z","shell.execute_reply":"2022-04-21T15:06:24.929668Z"},"trusted":true},"execution_count":null,"outputs":[]}]}