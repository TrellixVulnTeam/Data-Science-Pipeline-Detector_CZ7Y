{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    \n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-03T13:52:42.013489Z","iopub.execute_input":"2022-05-03T13:52:42.013801Z","iopub.status.idle":"2022-05-03T13:52:42.043272Z","shell.execute_reply.started":"2022-05-03T13:52:42.013714Z","shell.execute_reply":"2022-05-03T13:52:42.042615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport gc\nimport ast\nimport sys\nimport copy\nimport json\nimport math\nimport string\nimport pickle\nimport random\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:52:42.045296Z","iopub.execute_input":"2022-05-03T13:52:42.045572Z","iopub.status.idle":"2022-05-03T13:52:43.953616Z","shell.execute_reply.started":"2022-05-03T13:52:42.045536Z","shell.execute_reply":"2022-05-03T13:52:43.952802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    '''\n    Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.\n    '''\n    random.seed(seed)\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        # When running on the CuDNN backend, two further options must be set\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:52:43.955139Z","iopub.execute_input":"2022-05-03T13:52:43.955624Z","iopub.status.idle":"2022-05-03T13:52:43.965214Z","shell.execute_reply.started":"2022-05-03T13:52:43.955585Z","shell.execute_reply":"2022-05-03T13:52:43.964479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    \n    return f1_score(truths, preds)\n\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n        \n    return binary\n\n\ndef span_micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n        \n    return micro_f1(bin_preds, bin_truths)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:52:43.969629Z","iopub.execute_input":"2022-05-03T13:52:43.970325Z","iopub.status.idle":"2022-05-03T13:52:43.981913Z","shell.execute_reply.started":"2022-05-03T13:52:43.970284Z","shell.execute_reply":"2022-05-03T13:52:43.981219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_labels_for_scoring(df):\n    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n        if len(location_list) > 0:\n            location = location_list[0]\n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n        \n    return truths\n\n\ndef get_char_probs(texts, predictions, tokenizer):\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n            \n    return results\n\n\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n        \n    return results\n\n\ndef get_predictions(results):\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n        \n    return predictions\n\n\ndef get_score(y_true, y_pred):\n    return span_micro_f1(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:52:43.985132Z","iopub.execute_input":"2022-05-03T13:52:43.987058Z","iopub.status.idle":"2022-05-03T13:52:44.002609Z","shell.execute_reply.started":"2022-05-03T13:52:43.987023Z","shell.execute_reply":"2022-05-03T13:52:44.001893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_feature_text(text):\n    text = re.sub('I-year', '1-year', text)\n    text = re.sub('-OR-', \" or \", text)\n    text = re.sub('-', ' ', text)\n    return text\n\n\ndef clean_spaces(txt):\n    txt = re.sub('\\n', ' ', txt)\n    txt = re.sub('\\t', ' ', txt)\n    txt = re.sub('\\r', ' ', txt)\n#     txt = re.sub(r'\\s+', ' ', txt)\n    return txt","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:52:44.004142Z","iopub.execute_input":"2022-05-03T13:52:44.00476Z","iopub.status.idle":"2022-05-03T13:52:44.014479Z","shell.execute_reply.started":"2022-05-03T13:52:44.004723Z","shell.execute_reply":"2022-05-03T13:52:44.013739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_dir=\"../input/nbme-score-clinical-patient-notes/\"\n\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\n\n\ntest = pd.read_csv(main_dir+'test.csv')\nsubmission = pd.read_csv(main_dir+'sample_submission.csv')\nfeatures = pd.read_csv(main_dir+'features.csv')\npatient_notes = pd.read_csv(main_dir+'patient_notes.csv')\n\nfeatures = preprocess_features(features)\n\nprint(f\"test.shape: {test.shape}\")\ndisplay(test.head())\nprint(f\"features.shape: {features.shape}\")\ndisplay(features.head())\nprint(f\"patient_notes.shape: {patient_notes.shape}\")\ndisplay(patient_notes.head())","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:52:44.017542Z","iopub.execute_input":"2022-05-03T13:52:44.018405Z","iopub.status.idle":"2022-05-03T13:52:44.703423Z","shell.execute_reply.started":"2022-05-03T13:52:44.018347Z","shell.execute_reply":"2022-05-03T13:52:44.702589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.merge(features, on=['feature_num', 'case_num'], how='left')\ntest = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n\n\n# test['pn_history'] = test['pn_history'].apply(lambda x: x.strip())\n# test['feature_text'] = test['feature_text'].apply(process_feature_text)\n\n# test['feature_text'] = test['feature_text'].apply(clean_spaces)\n# test['pn_history'] = test['pn_history'].apply(clean_spaces)\n\n\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:52:44.704737Z","iopub.execute_input":"2022-05-03T13:52:44.705076Z","iopub.status.idle":"2022-05-03T13:52:44.73239Z","shell.execute_reply.started":"2022-05-03T13:52:44.705038Z","shell.execute_reply":"2022-05-03T13:52:44.731558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input_fast(cfg, text, feature_text, batch_max_len):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           max_length=batch_max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDatasetFast(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n        self.batch_max_len = df['batch_max_length'].values\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input_fast(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item],\n                               self.batch_max_len[item],\n                              )\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:52:44.733731Z","iopub.execute_input":"2022-05-03T13:52:44.734072Z","iopub.status.idle":"2022-05-03T13:52:44.743631Z","shell.execute_reply.started":"2022-05-03T13:52:44.734034Z","shell.execute_reply":"2022-05-03T13:52:44.742809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        \n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:52:44.744895Z","iopub.execute_input":"2022-05-03T13:52:44.745161Z","iopub.status.idle":"2022-05-03T13:52:44.759816Z","shell.execute_reply.started":"2022-05-03T13:52:44.745126Z","shell.execute_reply":"2022-05-03T13:52:44.759074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn_fast(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n    # for inputs in test_loader:\n        bs = len(inputs['input_ids'])\n        pred_w_pad = np.zeros((bs, CFG.max_len, 1))\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        y_preds = y_preds.sigmoid().to('cpu').numpy()\n        pred_w_pad[:, :y_preds.shape[1]] = y_preds\n        preds.append(pred_w_pad)\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:52:44.761509Z","iopub.execute_input":"2022-05-03T13:52:44.762037Z","iopub.status.idle":"2022-05-03T13:52:44.772272Z","shell.execute_reply.started":"2022-05-03T13:52:44.761987Z","shell.execute_reply":"2022-05-03T13:52:44.771558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# deberta v3 large","metadata":{}},{"cell_type":"code","source":"# class CFG:\n#     num_workers=4\n#     path=\"../input/nbme-deberta-v3-large-3/\"\n#     config_path=path+'config.pth'\n#     model=\"microsoft/deberta-v3-large\"\n#     batch_size=32\n#     fc_dropout=0.2\n#     max_len=354\n#     seed=42\n#     n_fold=5\n#     trn_fold=[0, 1, 2, 3, 4]\n\n# from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n\n# tokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\n# CFG.tokenizer = tokenizer\n\n# input_lengths = []\n# tk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\n# for text, feature_text in tk0:\n#     length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n#     input_lengths.append(length)\n# test['input_lengths'] = input_lengths\n# length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# # sort dataframe\n# sort_df = test.iloc[length_sorted_idx]\n\n# # calc max_len per batch\n# sorted_input_length = sort_df['input_lengths'].values\n# batch_max_length = np.zeros_like(sorted_input_length)\n# bs = CFG.batch_size\n# for i in range((len(sorted_input_length)//bs)+1):\n#     batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \n# sort_df['batch_max_length'] = batch_max_length\n\n# test_dataset = TestDatasetFast(CFG, sort_df)\n# test_loader = DataLoader(test_dataset,\n#                       batch_size=CFG.batch_size,\n#                       shuffle=False,\n#                       num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    \n#     state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n#                        map_location=torch.device('cpu'))\n       \n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn_fast(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n    \n#     ## data re-sort ## \n#     prediction = prediction[np.argsort(length_sorted_idx)]\n    \n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs\n#     gc.collect()\n#     torch.cuda.empty_cache()\n    \n# predictions_v3_large_1 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:52:44.774676Z","iopub.execute_input":"2022-05-03T13:52:44.775999Z","iopub.status.idle":"2022-05-03T13:52:44.784562Z","shell.execute_reply.started":"2022-05-03T13:52:44.775947Z","shell.execute_reply":"2022-05-03T13:52:44.783843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# v3 large 4","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=\"../input/nbme-v3-large-6/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=32\n    fc_dropout=0.2\n    max_len=512\n    seed=42\n    n_fold=7\n    trn_fold=[0, 2, 3, 4, 5, 6, 7]\n\nfrom transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n\ntokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\nCFG.tokenizer = tokenizer\n\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length\n\ntest_dataset = TestDatasetFast(CFG, sort_df)\ntest_loader = DataLoader(test_dataset,\n                      batch_size=CFG.batch_size,\n                      shuffle=False,\n                      num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    \n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n       \n    model.load_state_dict(state['model'])\n    prediction = inference_fn_fast(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    \n    ## data re-sort ## \n    prediction = prediction[np.argsort(length_sorted_idx)]\n    \n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs\n    gc.collect()\n    torch.cuda.empty_cache()\n    \npredictions_v3_large_2 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:52:44.788057Z","iopub.execute_input":"2022-05-03T13:52:44.788285Z","iopub.status.idle":"2022-05-03T13:55:56.549697Z","shell.execute_reply.started":"2022-05-03T13:52:44.788253Z","shell.execute_reply":"2022-05-03T13:55:56.547997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# deberta large","metadata":{}},{"cell_type":"code","source":"# class CFG:\n#     num_workers=4\n#     path=\"../input/nbme-deberta-large-fold-5/\"\n#     config_path=path+'config.pth'\n#     model=\"microsoft/deberta-large\"\n#     batch_size=24\n#     fc_dropout=0.2\n#     max_len=466\n#     seed=42\n#     n_fold=5\n#     trn_fold=[0, 1, 2, 3, 4]\n\n# CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\n# input_lengths = []\n# tk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\n# for text, feature_text in tk0:\n#     length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n#     input_lengths.append(length)\n# test['input_lengths'] = input_lengths\n# length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# # sort dataframe\n# sort_df = test.iloc[length_sorted_idx]\n\n# # calc max_len per batch\n# sorted_input_length = sort_df['input_lengths'].values\n# batch_max_length = np.zeros_like(sorted_input_length)\n# bs = CFG.batch_size\n# for i in range((len(sorted_input_length)//bs)+1):\n#     batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \n# sort_df['batch_max_length'] = batch_max_length\n\n# test_dataset = TestDatasetFast(CFG, sort_df)\n# test_loader = DataLoader(test_dataset,\n#                       batch_size=CFG.batch_size,\n#                       shuffle=False,\n#                       num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n#     state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n#                        map_location=torch.device('cpu'))\n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn_fast(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n    \n#     prediction = prediction[np.argsort(length_sorted_idx)]\n    \n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs; gc.collect()\n#     torch.cuda.empty_cache()\n# predictions_v1_l = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:55:56.55402Z","iopub.execute_input":"2022-05-03T13:55:56.555887Z","iopub.status.idle":"2022-05-03T13:55:56.56382Z","shell.execute_reply.started":"2022-05-03T13:55:56.555847Z","shell.execute_reply":"2022-05-03T13:55:56.563179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# large 2","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=\"../input/nbme-large-4/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-large\"\n    batch_size=24\n    fc_dropout=0.2\n    max_len=466\n    seed=42\n    n_fold=6\n    trn_fold=[0, 2, 3, 5, 6, 7]\n\nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length\n\ntest_dataset = TestDatasetFast(CFG, sort_df)\ntest_loader = DataLoader(test_dataset,\n                      batch_size=CFG.batch_size,\n                      shuffle=False,\n                      num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn_fast(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    \n    prediction = prediction[np.argsort(length_sorted_idx)]\n    \n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_v1_2 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:55:56.56756Z","iopub.execute_input":"2022-05-03T13:55:56.569579Z","iopub.status.idle":"2022-05-03T13:58:43.513156Z","shell.execute_reply.started":"2022-05-03T13:55:56.569541Z","shell.execute_reply":"2022-05-03T13:58:43.512238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# deberta-large-mnli","metadata":{}},{"cell_type":"code","source":"# class CFG:\n#     num_workers=4\n#     path=\"../input/nbme-deberta-large-mnli/\"\n#     config_path=path+'config.pth'\n#     model=\"microsoft/deberta-large-mnli\"\n#     batch_size=24\n#     fc_dropout=0.2\n#     max_len=466\n#     seed=1024\n#     n_fold=5\n#     trn_fold=[0, 1, 2, 3, 4]\n\n# CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\n# input_lengths = []\n# tk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\n# for text, feature_text in tk0:\n#     length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n#     input_lengths.append(length)\n# test['input_lengths'] = input_lengths\n# length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# # sort dataframe\n# sort_df = test.iloc[length_sorted_idx]\n\n# # calc max_len per batch\n# sorted_input_length = sort_df['input_lengths'].values\n# batch_max_length = np.zeros_like(sorted_input_length)\n# bs = CFG.batch_size\n# for i in range((len(sorted_input_length)//bs)+1):\n#     batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \n# sort_df['batch_max_length'] = batch_max_length\n\n# test_dataset = TestDatasetFast(CFG, sort_df)\n# test_loader = DataLoader(test_dataset,\n#                       batch_size=CFG.batch_size,\n#                       shuffle=False,\n#                       num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n#     state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n#                        map_location=torch.device('cpu'))\n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn_fast(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n    \n#     prediction = prediction[np.argsort(length_sorted_idx)]\n    \n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs; gc.collect()\n#     torch.cuda.empty_cache()\n# predictions_large_mnli_1 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:58:43.517618Z","iopub.execute_input":"2022-05-03T13:58:43.519497Z","iopub.status.idle":"2022-05-03T13:58:43.527604Z","shell.execute_reply.started":"2022-05-03T13:58:43.519452Z","shell.execute_reply":"2022-05-03T13:58:43.52681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# large-mnli-3","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=\"../input/nbme-large-mnli-3/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-large-mnli\"\n    batch_size=24\n    fc_dropout=0.2\n    max_len=466\n    seed=1024\n    n_fold=7\n    trn_fold=[0, 2, 3, 4, 5, 6, 7]\n\nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length\n\ntest_dataset = TestDatasetFast(CFG, sort_df)\ntest_loader = DataLoader(test_dataset,\n                      batch_size=CFG.batch_size,\n                      shuffle=False,\n                      num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn_fast(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    \n    prediction = prediction[np.argsort(length_sorted_idx)]\n    \n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_large_mnli_2 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T13:58:43.531502Z","iopub.execute_input":"2022-05-03T13:58:43.533289Z","iopub.status.idle":"2022-05-03T14:01:55.636168Z","shell.execute_reply.started":"2022-05-03T13:58:43.533254Z","shell.execute_reply":"2022-05-03T14:01:55.635324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# deberta-xlarge","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=\"../input/nbme-xlarge-3/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-xlarge\"\n    batch_size=24\n    fc_dropout=0.2\n    max_len=466\n    seed=42\n    n_fold=6\n    trn_fold=[0, 2, 3, 5, 6, 7]\n\nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length\n\ntest_dataset = TestDatasetFast(CFG, sort_df)\ntest_loader = DataLoader(test_dataset,\n                      batch_size=CFG.batch_size,\n                      shuffle=False,\n                      num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn_fast(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    ## data re-sort ## \n    prediction = prediction[np.argsort(length_sorted_idx)]\n\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_xl = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T14:01:55.640911Z","iopub.execute_input":"2022-05-03T14:01:55.64282Z","iopub.status.idle":"2022-05-03T14:06:13.941286Z","shell.execute_reply.started":"2022-05-03T14:01:55.642774Z","shell.execute_reply":"2022-05-03T14:06:13.940499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# large-base","metadata":{}},{"cell_type":"code","source":"# # ====================================================\n# # CFG\n# # ====================================================\n# class CFG:\n#     num_workers=4\n#     path=\"../input/nbme-deberta-base/\"\n#     config_path=path+'config.pth'\n#     model=\"microsoft/deberta-base\"\n#     batch_size=24\n#     fc_dropout=0.2\n#     max_len=466\n#     seed=42\n#     n_fold=5\n#     trn_fold=[0, 1, 2, 3, 4]\n\n# CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\n# input_lengths = []\n# tk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\n# for text, feature_text in tk0:\n#     length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n#     input_lengths.append(length)\n# test['input_lengths'] = input_lengths\n# length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# # sort dataframe\n# sort_df = test.iloc[length_sorted_idx]\n\n# # calc max_len per batch\n# sorted_input_length = sort_df['input_lengths'].values\n# batch_max_length = np.zeros_like(sorted_input_length)\n# bs = CFG.batch_size\n# for i in range((len(sorted_input_length)//bs)+1):\n#     batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \n# sort_df['batch_max_length'] = batch_max_length\n\n# test_dataset = TestDatasetFast(CFG, sort_df)\n# test_loader = DataLoader(test_dataset,\n#                       batch_size=CFG.batch_size,\n#                       shuffle=False,\n#                       num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n#     state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n#                        map_location=torch.device('cpu'))\n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn_fast(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n#     ## data re-sort ## \n#     prediction = prediction[np.argsort(length_sorted_idx)]\n\n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs; gc.collect()\n#     torch.cuda.empty_cache()\n# predictions_b_1 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T14:06:13.945542Z","iopub.execute_input":"2022-05-03T14:06:13.947427Z","iopub.status.idle":"2022-05-03T14:06:13.955395Z","shell.execute_reply.started":"2022-05-03T14:06:13.947385Z","shell.execute_reply":"2022-05-03T14:06:13.954539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# combine","metadata":{}},{"cell_type":"code","source":"# 0.55-0.40-0.10-0.15\nw1 = 0.35     # v3 large    8825, 882\nw2 = 0.25     # large       8783, 880\nw3 = 0.35     # large-mnli  8795, 882\nw4 = 0.25     # xlarge      8785, 882\n\n# predictions_v3_large_2      0.8826\t0.882\n# predictions_v1_l            0.8783\t0.88\n# predictions_v1_2            0.8789\t0.883\n# predictions_large_mnli_2    0.8806\t0.882\n# predictions_xl              0.8785\t0.882\n# predictions_b_1             \n\n# w1, w2, w3, w4 = 0.55, 0.15, 0.35, 0.15              0.886\n# w1, w2, w3, w4, w5 = 0.55, 0.15, 0.25, 0.15, 0.1     0.885\n# w1, w2, w3, w4, w5 = 0.45, 0.15, 0.35, 0.15, 0.1     0.885\n# w1, w2, w3, w4, w5 = 0.45, 0.15, 0.35, 0.15, 0.05    0.886\n# w1, w2, w3, w4, w5 = 0.55, 0.10, 0.40, 0.10, 0.05    0.885\n# w1, w2, w3, w4 = 0.65, 0.15, 0.25, 0.15              0.886\n# w1, w2, w3, w4 = 0.65, 0.10, 0.35, 0.10              0.885\n# w1, w2, w3, w4 = 0.75, 0.10, 0.25, 0.10              0.885\n# w1, w2, w3, w4 = 0.55, 0.15, 0.35, 0.15              0.886\n# w1, w2, w3, w4 = 0.65, 0.15, 0.25, 0.15              0.886\n# w1, w2, w3, w4 = 0.65, 0.15, 0.15, 0.10              0.885\n\n# w1, w2, w3 = 0.55, 0.35, 0.20                        0.887\n\n# 0.886\n# w1, w2, w3, w4 = 0.55, 0.25, 0.15, 0.10\n\n# 0.887\nw1, w2, w3, w4 = 0.55, 0.20, 0.15, 0.10\n\n\n\npredictions = []\nfor p1, p2, p3, p4 in zip(predictions_v3_large_2, predictions_xl, predictions_large_mnli_2, predictions_v1_2):\n    predictions.append(w1 * p1 + w2 * p2 + w3 * p3 + w4 * p4)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T14:06:13.959143Z","iopub.execute_input":"2022-05-03T14:06:13.961254Z","iopub.status.idle":"2022-05-03T14:06:13.971637Z","shell.execute_reply.started":"2022-05-03T14:06:13.961215Z","shell.execute_reply":"2022-05-03T14:06:13.971018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = get_results(predictions)\nsubmission['location'] = results\ndisplay(submission.head())\nsubmission[['id', 'location']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T14:06:13.975717Z","iopub.execute_input":"2022-05-03T14:06:13.977988Z","iopub.status.idle":"2022-05-03T14:06:14.002742Z","shell.execute_reply.started":"2022-05-03T14:06:13.977951Z","shell.execute_reply":"2022-05-03T14:06:14.00209Z"},"trusted":true},"execution_count":null,"outputs":[]}]}