{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro\n\nThe competation home page: https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes\n\nWe use Spark NLP to train a NER model.\n* https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb\n* https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.3.prepare_CoNLL_from_annotations_for_NER.ipynb\n\nNote: You need to set up your own licenses!!! Rerun this notebook will not work.","metadata":{}},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nfrom kaggle_secrets import UserSecretsClient\nimport warnings\nwarnings.filterwarnings('ignore')\n\nuser_secrets = UserSecretsClient()\nlicense_keys={}\nlicense_keys['SECRET'] = user_secrets.get_secret(\"SECRET\")\nlicense_keys['SPARK_NLP_LICENSE'] = user_secrets.get_secret(\"SPARK_NLP_LICENSE\")\nlicense_keys[\"PUBLIC_VERSION\"] = user_secrets.get_secret(\"PUBLIC_VERSION\")\nlicense_keys[\"JSL_VERSION\"] = user_secrets.get_secret(\"JSL_VERSION\")\nlicense_keys[\"AWS_ACCESS_KEY_ID\"] = user_secrets.get_secret(\"AWS_ACCESS_KEY_ID\")\nlicense_keys[\"AWS_SECRET_ACCESS_KEY\"] = user_secrets.get_secret(\"AWS_SECRET_ACCESS_KEY\")\nlicense_keys[\"AWS_SESSION_TOKEN\"] = user_secrets.get_secret(\"AWS_SESSION_TOKEN\")\nos.environ.update(license_keys)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-11T18:51:16.984342Z","iopub.execute_input":"2022-04-11T18:51:16.984743Z","iopub.status.idle":"2022-04-11T18:51:18.07067Z","shell.execute_reply.started":"2022-04-11T18:51:16.984641Z","shell.execute_reply":"2022-04-11T18:51:18.069628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install --upgrade -q pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION\n\n# Installing Spark NLP Healthcare\n! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n\n# Installing Spark NLP Display Library for visualization\n! pip install -q spark-nlp-display","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:51:18.072778Z","iopub.execute_input":"2022-04-11T18:51:18.07322Z","iopub.status.idle":"2022-04-11T18:52:14.609003Z","shell.execute_reply.started":"2022-04-11T18:51:18.07317Z","shell.execute_reply":"2022-04-11T18:52:14.608182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml import Pipeline,PipelineModel\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom sparknlp.annotator import *\nfrom sparknlp_jsl.annotator import *\nfrom sparknlp.base import *\nimport sparknlp_jsl\nimport sparknlp\nfrom sparknlp_jsl.compatibility import Compatibility \nfrom sparknlp_display import NerVisualizer\nfrom sparknlp.training import CoNLL\nfrom sparknlp_jsl.training import tf_graph\nfrom sparknlp.common import *\nfrom tqdm import tqdm\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:52:14.610808Z","iopub.execute_input":"2022-04-11T18:52:14.611178Z","iopub.status.idle":"2022-04-11T18:52:15.056309Z","shell.execute_reply.started":"2022-04-11T18:52:14.611126Z","shell.execute_reply":"2022-04-11T18:52:15.05449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Start spark session","metadata":{}},{"cell_type":"code","source":"params = {\"spark.driver.memory\":\"16G\", \n          \"spark.kryoserializer.buffer.max\":\"2000M\", \n          \"spark.driver.maxResultSize\":\"2000M\"} \nspark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\nprint (\"Spark NLP Version :\", sparknlp.version())\nprint (\"Spark NLP_JSL Version :\", sparknlp_jsl.version())","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:52:15.059266Z","iopub.execute_input":"2022-04-11T18:52:15.059909Z","iopub.status.idle":"2022-04-11T18:53:41.24767Z","shell.execute_reply.started":"2022-04-11T18:52:15.059843Z","shell.execute_reply":"2022-04-11T18:53:41.246341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities","metadata":{}},{"cell_type":"code","source":"# Generic NER Function with LightPipeline\ndef get_light_model (embeddings, model_name = 'ner_clinical'):\n\n    documentAssembler = DocumentAssembler()\\\n      .setInputCol(\"text\")\\\n      .setOutputCol(\"document\")\n\n    sentenceDetector = SentenceDetector()\\\n      .setInputCols([\"document\"])\\\n      .setOutputCol(\"sentence\")\n\n    tokenizer = Tokenizer()\\\n      .setInputCols([\"sentence\"])\\\n      .setOutputCol(\"token\")\n\n    word_embeddings = WordEmbeddingsModel.pretrained(embeddings, \"en\", \"clinical/models\")\\\n      .setInputCols([\"sentence\", \"token\"])\\\n      .setOutputCol(\"embeddings\")\n\n    loaded_ner_model = MedicalNerModel.pretrained(model_name, \"en\", \"clinical/models\") \\\n      .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n      .setOutputCol(\"ner\")\n\n    ner_converter = NerConverter() \\\n      .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n      .setOutputCol(\"ner_chunk\")\n\n    nlpPipeline = Pipeline(stages=[\n      documentAssembler,\n      sentenceDetector,\n      tokenizer,\n      word_embeddings,\n      loaded_ner_model,\n      ner_converter])\n\n    model = nlpPipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))\n\n    return LightPipeline(model)\n\n\n# Get NER Results with fullAnnotate Method\ndef get_light_result (light_model, text, chunk_name=\"ner_chunk\"):\n\n    light_result = light_model.fullAnnotate(text)\n\n    chunks = []\n    entities = []\n    sentence= []\n    begin = []\n    end = []\n\n    for n in light_result[0][chunk_name]:\n        begin.append(n.begin)\n        end.append(n.end)\n        chunks.append(n.result)\n        entities.append(n.metadata['entity']) \n        sentence.append(n.metadata['sentence']) \n\n    pd_df = pd.DataFrame({'sentence_id':sentence, \n                          'begin': begin, \n                          'end':end, \n                          'chunks':chunks,  \n                          'entities':entities})\n    \n    visualiser = NerVisualizer()\n    visualiser.display(light_result[0], label_col='ner_chunk', document_col='document')\n    \n    return pd_df\n\n\n# Convert to conll format\ndef make_conll(text:pd.DataFrame, entity:pd.DataFrame, \n               save_tag:bool=None, \n               save_conll:bool=None, \n               verbose:bool=None, \n               begin_deviation:int=0, \n               end_deviation:int=0 )->str:\n\n    df_text = text.iloc[:,[0,1]]\n    df_entity = entity.iloc[:,[0,1,2,3,4]]\n    df_text.columns = ['text_id','text']\n    df_entity.columns = ['text_id','begin','end','chunk','entity']\n    entity_list = list(df_entity.entity.unique())\n\n\n    ########--------------1.tag transformation function------------########\n\n    def transform_text(text, entities, verbose=None):\n\n        tag_list=[]\n        for entity in entities.iterrows():\n\n            begin = entity[1][1] + begin_deviation \n            end = entity[1][2] + end_deviation\n            chunk = entity[1][3]\n            tag = entity[1][4]\n            text = text[:end] + f' </END_NER:{tag}> ' + text[end:]\n            text = text[:begin] + f' <START_NER:{tag}> ' + text[begin:]\n            tag_list.append(tag)\n\n        sum_of_added_entity = Counter(tag_list)\n        sum_of_entity = Counter(entities['entity'].values)\n\n        if verbose:\n            print(f'Processed text id   : {entities.text_id.values[:1]}')\n            print(f'Original Entities   : {sum_of_entity}\\nAdded Entities      : {sum_of_added_entity}')\n            print(f'Number Equality     : {sum_of_added_entity == sum_of_entity}')\n            print(\"==\"*40)\n\n        if not sum_of_entity == sum_of_added_entity:\n            print(\"There is a problem in text id:\")\n            print(entities.text_id.values[0])\n            raise Exception(\"Check this text!\")\n\n        return text\n\n\n    ######---------------2.apply_transform_text function ----------------#######\n\n    def apply_tag_ner(df_text, df_entity, save=None, verbose=None):\n\n        for text_id in tqdm(df_text.text_id):\n            text  = df_text.loc[df_text['text_id']==text_id]['text'].values[0] \n            entities  = df_entity.loc[(df_entity['text_id']==text_id)].sort_values(by='begin',ascending=False) \n\n            df_text.loc[df_text['text_id']==text_id, 'text'] = transform_text(text, entities, verbose=verbose)\n\n        if save:\n            df_text.to_csv(\"text_with_ner_tag.csv\", index=False, encoding='utf8')\n\n        return df_text\n\n\n    ##########----------------3.RUNNING TAG FUNCTION---------------#############\n    \n    print(\"Text tagging starting. Applying entities to whole text...\\n\")\n    df = apply_tag_ner(df_text, df_entity, save=save_tag, verbose=verbose)\n\n\n    ###########---------------4.Spark Pipeline-----------------------###########\n\n    def spark_pipeline(df):\n        spark_df = spark.createDataFrame(df)\n\n        documentAssembler = DocumentAssembler()\\\n            .setInputCol(\"text\")\\\n            .setOutputCol(\"document\")\\\n            .setCleanupMode(\"shrink\")\n\n        sentenceDetector = SentenceDetector()\\\n            .setInputCols(['document'])\\\n            .setOutputCol('sentences')\\\n            .setExplodeSentences(True)\n\n        tokenizer = Tokenizer() \\\n            .setInputCols([\"sentences\"]) \\\n            .setOutputCol(\"token\")\n\n        nlpPipeline = Pipeline(stages=[documentAssembler, sentenceDetector, tokenizer ])\n\n        empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n        pipelineModel = nlpPipeline.fit(empty_df)\n\n        result = pipelineModel.transform(spark_df.select(['text']))\n\n\n        return result.select('token.result').toPandas()\n    print(\"\\n\\nSpark pipeline is running...\")\n    df_final = spark_pipeline(df)\n\n\n    #########--------------5.CoNLL Function--------------------#############\n\n    def build_conll(df_final, tag_list, save=None):\n\n        header = \"-DOCSTART- -X- -X- O\\n\\n\"\n        conll_text = \"\"\n        chunks = []\n        tag_list = tag_list\n        tag = 'O'      # token tag \n        ct = 'B'       # chunk tag part B or I\n\n        for sentence_tokens in tqdm(df_final.result[:]):\n            for token in sentence_tokens:\n                if token.startswith(\"<START_NER:\"):\n                    tag = token.split(':')[1][:-1]\n                    if tag not in tag_list:\n                        tag = 'O'\n                        conll_text += f'{token} NN NN {tag}\\n'\n\n                    continue\n\n                if token.startswith(\"</END_NER:\") and tag != 'O':\n                    for i, chunk in enumerate(chunks):\n                        ct = 'B' if i == 0 else 'I' \n                        conll_text += f'{chunk} NNP NNP {ct}-{tag}\\n'\n                    \n                    chunks=[]\n                    tag='O'\n                    continue\n\n                if tag != 'O':    \n                    chunks.append(token)\n                    continue\n\n                if tag == 'O':\n                    conll_text += f'{token} NN NN {tag}\\n'             \n                    continue\n\n            conll_text += '\\n'                                         \n\n        if save:\n            with open(\"conll2003_text_file.conll\", \"w+\", encoding='utf8') as f:\n                f.write(header)\n                f.write(conll_text)\n\n        print(\"\\nDONE!\")    \n        return conll_text\n\n        \n    ########----------------6.RUNNING CONLL FUNCTION--------------------########\n\n    print(\"Conll file is being created...\\n\")\n    return build_conll(df_final, tag_list=entity_list, save=save_conll)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:53:41.250991Z","iopub.execute_input":"2022-04-11T18:53:41.251527Z","iopub.status.idle":"2022-04-11T18:53:41.302758Z","shell.execute_reply.started":"2022-04-11T18:53:41.251472Z","shell.execute_reply":"2022-04-11T18:53:41.30158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test pipeline with samples","metadata":{}},{"cell_type":"code","source":"\ntext ='''The patient was prescribed 1 capsule of Parol with meals . \nHe was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day . \nIt was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months .'''\n\nembeddings = 'embeddings_clinical'\n\nmodel_name = 'ner_posology'\n\n# Uncomment the two lines to test. Note doing this will increase the memory usage.\n#light_model = get_light_model (embeddings, model_name)\n#get_light_result (light_model, text, chunk_name=\"ner_chunk\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:53:41.304731Z","iopub.execute_input":"2022-04-11T18:53:41.30514Z","iopub.status.idle":"2022-04-11T18:53:41.324842Z","shell.execute_reply.started":"2022-04-11T18:53:41.305095Z","shell.execute_reply":"2022-04-11T18:53:41.323651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process training data\n\nConvert csv to conll format accepted by Spark NLP.","metadata":{}},{"cell_type":"code","source":"# Create a text file\ndfNotes = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/patient_notes.csv\",\n                     usecols=[\"pn_num\",\"pn_history\"])\ndfNotes.columns = ['text_id','text']\ndfNotes.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:53:41.326495Z","iopub.execute_input":"2022-04-11T18:53:41.32682Z","iopub.status.idle":"2022-04-11T18:53:42.195669Z","shell.execute_reply.started":"2022-04-11T18:53:41.326783Z","shell.execute_reply":"2022-04-11T18:53:42.194648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an entity file\ndfFeatures = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/features.csv\",\n                        usecols=[\"feature_num\",\"feature_text\"])\ndfTrain = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/train.csv\")\ndfEnt = dfTrain.join(dfFeatures,on=\"feature_num\",rsuffix='_other')\ndfEnt = dfEnt[[\"pn_num\",\"location\",\"annotation\",\"feature_text\"]]\ndfEnt = dfEnt.dropna()\ndfEnt = dfEnt[dfEnt[\"annotation\"]!=\"[]\"]\ndfEnt = dfEnt[~dfEnt.location.str.contains(\";\")]\ndfEnt['location'] = dfEnt.location.apply(lambda x: x[2:-2].split(\"', '\"))\ndfEnt['annotation'] = dfEnt.annotation.apply(lambda x: x[2:-2].split(\"', '\"))\ndfEnt['locationLen'] = dfEnt.location.apply(lambda x: len(x))\ndfEnt['annotationLen'] = dfEnt.annotation.apply(lambda x: len(x))\ndfEnt = dfEnt[dfEnt[\"annotationLen\"]==dfEnt[\"locationLen\"]].explode([\"location\",\"annotation\"])\ndfEnt['begin'] = dfEnt.location.apply(lambda x: x.split(\" \")[0])\ndfEnt['end'] = dfEnt.location.apply(lambda x: x.split(\" \")[1])\ndfEnt = dfEnt[[\"pn_num\",\"begin\",\"end\",\"annotation\",\"feature_text\"]]\ndfEnt.columns = ['text_id','begin','end','chunk','entity']\ndfEnt = dfEnt.astype({'begin': 'int32','end': 'int32'})\ndfEnt.head()\n\n# Sample for debug. Using the entire training data will lead to OOM.\ndfEnt = dfEnt.sample(n=20) \ndfNotes = dfNotes.join(dfEnt,how=\"inner\",on=\"text_id\",rsuffix='_other')\ndfNotes = dfNotes[['text_id','text']]","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:53:42.197349Z","iopub.execute_input":"2022-04-11T18:53:42.197591Z","iopub.status.idle":"2022-04-11T18:53:42.359259Z","shell.execute_reply.started":"2022-04-11T18:53:42.197563Z","shell.execute_reply":"2022-04-11T18:53:42.35816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to conll and save\nconll_text = make_conll(dfNotes,dfEnt,save_conll=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:53:42.360786Z","iopub.execute_input":"2022-04-11T18:53:42.3611Z","iopub.status.idle":"2022-04-11T18:53:51.807764Z","shell.execute_reply.started":"2022-04-11T18:53:42.361053Z","shell.execute_reply":"2022-04-11T18:53:51.806783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load conll   \ndata = CoNLL().readDataset(spark, \"./conll2003_text_file.conll\")\ndata.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:53:51.811757Z","iopub.execute_input":"2022-04-11T18:53:51.812476Z","iopub.status.idle":"2022-04-11T18:53:54.204316Z","shell.execute_reply.started":"2022-04-11T18:53:51.812421Z","shell.execute_reply":"2022-04-11T18:53:54.203316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"clinical_embeddings = WordEmbeddingsModel.pretrained('embeddings_clinical', \"en\", \"clinical/models\")\\\n                        .setInputCols([\"sentence\", \"token\"])\\\n                        .setOutputCol(\"embeddings\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:53:54.205759Z","iopub.execute_input":"2022-04-11T18:53:54.20611Z","iopub.status.idle":"2022-04-11T18:55:11.867213Z","shell.execute_reply.started":"2022-04-11T18:53:54.206044Z","shell.execute_reply":"2022-04-11T18:55:11.86584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_graph.print_model_params(\"ner_dl\")\n\ntf_graph.build(\"ner_dl\", \n               build_params={\"embeddings_dim\": 200, \n                             \"nchars\": 83, \n                             \"ntags\": 12, \n                             \"is_medical\": 1}, \n               model_location=\"./medical_ner_graphs\", \n               model_filename=\"auto\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:55:11.869908Z","iopub.execute_input":"2022-04-11T18:55:11.8703Z","iopub.status.idle":"2022-04-11T18:55:31.894635Z","shell.execute_reply.started":"2022-04-11T18:55:11.870251Z","shell.execute_reply":"2022-04-11T18:55:31.893875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nerTagger = MedicalNerApproach()\\\n    .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n    .setLabelColumn(\"label\")\\\n    .setOutputCol(\"ner\")\\\n    .setMaxEpochs(30)\\\n    .setBatchSize(64)\\\n    .setRandomSeed(0)\\\n    .setVerbose(1)\\\n    .setValidationSplit(0.2)\\\n    .setEvaluationLogExtended(True) \\\n    .setEnableOutputLogs(True)\\\n    .setIncludeConfidence(True)\\\n    .setOutputLogsPath('ner_logs')\\\n    .setGraphFolder('medical_ner_graphs')\\\n    .setUseBestModel(True)\\\n    .setEarlyStoppingCriterion(0.04)\\\n    .setEarlyStoppingPatience(3)\\\n    .setEnableMemoryOptimizer(True) #>> if you have a limited memory and a large conll file, you can set this True to train batch by batch       \n\nner_pipeline = Pipeline(stages=[\n          clinical_embeddings,\n          nerTagger\n ])","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:57:53.594807Z","iopub.execute_input":"2022-04-11T18:57:53.595781Z","iopub.status.idle":"2022-04-11T18:57:53.618759Z","shell.execute_reply.started":"2022-04-11T18:57:53.59573Z","shell.execute_reply":"2022-04-11T18:57:53.61787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nner_model = ner_pipeline.fit(data)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:58:00.327126Z","iopub.execute_input":"2022-04-11T18:58:00.327468Z","iopub.status.idle":"2022-04-11T18:58:38.084395Z","shell.execute_reply.started":"2022-04-11T18:58:00.32743Z","shell.execute_reply":"2022-04-11T18:58:38.083437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner_model.stages[1].getTrainingClassDistribution()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:58:52.97554Z","iopub.execute_input":"2022-04-11T18:58:52.97627Z","iopub.status.idle":"2022-04-11T18:58:53.003452Z","shell.execute_reply.started":"2022-04-11T18:58:52.976219Z","shell.execute_reply":"2022-04-11T18:58:53.002606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_file= os.listdir(\"ner_logs\")[0]\n\nwith open (f\"./ner_logs/{log_file}\") as f:\n    print(f.read())","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:58:59.616147Z","iopub.execute_input":"2022-04-11T18:58:59.616992Z","iopub.status.idle":"2022-04-11T18:58:59.623731Z","shell.execute_reply.started":"2022-04-11T18:58:59.616941Z","shell.execute_reply":"2022-04-11T18:58:59.622789Z"},"trusted":true},"execution_count":null,"outputs":[]}]}