{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.system('pip uninstall -y transformers')\nos.system('python -m pip install --no-index --find-links=../input/nbme-pip-wheels transformers')","metadata":{"execution":{"iopub.status.busy":"2022-04-15T11:56:56.716031Z","iopub.execute_input":"2022-04-15T11:56:56.716356Z","iopub.status.idle":"2022-04-15T11:57:09.368624Z","shell.execute_reply.started":"2022-04-15T11:56:56.716282Z","shell.execute_reply":"2022-04-15T11:57:09.36788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\nimport shutil\nfrom pathlib import Path\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in [\n    'tokenization_deberta_v2.py',\n    'tokenization_deberta_v2_fast.py',\n    \"deberta__init__.py\"]:\n    if str(filename).startswith(\"deberta\"):\n        filepath = deberta_v2_path/str(filename).replace(\"deberta\", \"\")\n    else:\n        filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T11:57:09.370332Z","iopub.execute_input":"2022-04-15T11:57:09.371735Z","iopub.status.idle":"2022-04-15T11:57:09.40209Z","shell.execute_reply.started":"2022-04-15T11:57:09.371693Z","shell.execute_reply":"2022-04-15T11:57:09.401435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =================================\n# Library\n# =================================\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import DataCollatorWithPadding\n%env TOKENIZERS_PARALLELISM=true\nfrom transformers.models.deberta_v2 import DebertaV2TokenizerFast\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.cuda.amp import autocast, GradScaler\nimport gc\nimport ast\nimport itertools\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-04-15T11:57:09.403286Z","iopub.execute_input":"2022-04-15T11:57:09.403711Z","iopub.status.idle":"2022-04-15T11:57:15.653127Z","shell.execute_reply.started":"2022-04-15T11:57:09.403675Z","shell.execute_reply":"2022-04-15T11:57:15.652378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =================================\n# Constant\n# =================================\nSUB_PATH = \"../input/feedback-prize-2021/sample_submission.csv\"\nTEST_PATH = \"../input/nbme-score-clinical-patient-notes/test.csv\"\nFEATURE_PATH = \"./input/nbme-score-clinical-patient-notes/features.csv\"\nPATIENT_NOTES_PATH = \"../input/nbme-score-clinical-patient-notes/patient_notes.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-04-15T11:57:15.654935Z","iopub.execute_input":"2022-04-15T11:57:15.655721Z","iopub.status.idle":"2022-04-15T11:57:15.660393Z","shell.execute_reply.started":"2022-04-15T11:57:15.655678Z","shell.execute_reply":"2022-04-15T11:57:15.659738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =================================\n# Settings\n# =================================\nw1 = 0.25\nw2 = 0.1\nw3 = 0.35\nw4 = 0.3\n\nth_dict = {0: [0.49, 0.8976772190005388],\n 1: [0.525, 0.9064990886585599],\n 2: [0.475, 0.8517623923219974],\n 3: [0.485, 0.9260359498514676],\n 4: [0.53, 0.9248769561757685],\n 5: [0.47, 0.8364175195561528],\n 6: [0.47, 0.906836587356394],\n 7: [0.53, 0.8801618303571429],\n 8: [0.485, 0.9256547241005716],\n 9: [0.53, 0.9290001463914508]}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG_ex038:\n    max_len=512\n    batch_size=24\n    tokenizer_path=\"../input/deberta-v3-large-model/deberta-v3-large\"\n    model_path=\"../input/ex038-save/ex038\"\n    n_fold=5\n    \nclass CFG_ex041:\n    max_len=512\n    batch_size=16\n    tokenizer_path=\"../input/deberta/v2-xlarge\"\n    model_path=\"../input/ex041-save/ex041\"\n    n_fold=5\n    \nclass CFG_ex051:\n    max_len=512\n    batch_size=8\n    tokenizer_path=\"../input/deberta/v2-xxlarge\"\n    model_path=\"../input/nbme-ex051/ex051\"\n    n_fold=4","metadata":{"execution":{"iopub.status.busy":"2022-04-15T11:57:18.235361Z","iopub.execute_input":"2022-04-15T11:57:18.235905Z","iopub.status.idle":"2022-04-15T11:57:18.24278Z","shell.execute_reply.started":"2022-04-15T11:57:18.235866Z","shell.execute_reply":"2022-04-15T11:57:18.241645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Function\n# ====================================================\ndef get_char_probs(texts, predictions, tokenizer):\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n    return results\n\ndef get_results_raw_pp(char_probs, case_nums, th_dict):\n    results = []\n    for char_prob,case_num in zip(char_probs,case_nums):\n        result = np.where(char_prob >= th_dict[int(case_num)][0])[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n    return results\n\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        if len(result) > 0:\n            if result[0] == 1:\n                result = np.concatenate([np.array([0]),result],axis=0)\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n    return results\n\n\ndef get_predictions(results):\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n    return predictions\n\ndef get_results_from_preds_list(preds):\n    results = []\n    for pred in preds:\n        s = []\n        for p in pred:\n            s.append(' '.join(list(map(str, p))))\n        s = ';'.join(s)\n        results.append(s)\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-04-15T11:57:18.641973Z","iopub.execute_input":"2022-04-15T11:57:18.642655Z","iopub.status.idle":"2022-04-15T11:57:18.659969Z","shell.execute_reply.started":"2022-04-15T11:57:18.642607Z","shell.execute_reply":"2022-04-15T11:57:18.659263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess(texts, preds):\n    from nltk.tokenize import word_tokenize\n    preds_pp = preds.copy()\n    tk0 = tqdm(range(len(preds_pp)), total=len(preds_pp))\n    for raw_idx in tk0:\n        pred = preds[raw_idx]\n        char_prob = char_probs[raw_idx]\n        text = texts[raw_idx]\n        if len(pred) != 0:\n            # pp1: indexが1から始まる予測値は0から始まるように修正 ## +0.00123\n            if pred[0][0] == 1:\n                preds_pp[raw_idx][0][0] = 0\n            for p_index, pp in enumerate(pred):\n                start, end = pred[p_index]\n                if start == 0:\n                    break\n                # pp2: startとendが同じ予測値はstartを前に1ずらす ## +0.00012\n                if start == end:\n                    preds_pp[raw_idx][p_index][0] = start - 1\n                    break\n                # pp3: 始点が改行の場合始点を1つ後ろにずらす ## +0.00032\n                if text[start] == '\\n':\n                    preds_pp[raw_idx][p_index][0] = start + 1\n                    start = start + 1\n                # pp4: 1-2などは-2で予測されることがあるので修正 ## +0.00001\n                if text[start-1].isdigit() and text[start] == '-' and text[start+1].isdigit():\n                    preds_pp[raw_idx][p_index][0] = start - 1\n                    start = start - 1\n                if text[start-1].isdigit() and text[start] == '/' and text[start+1].isdigit():\n                    preds_pp[raw_idx][p_index][0] = start - 1\n                    start = start - 1\n                # pp5: 67などは7で予測されることがあるので修正 ## +0.00001\n                if text[start-1].isdigit() and text[start].isdigit():\n                    preds_pp[raw_idx][p_index][0] = start - 1\n                    start = start - 1\n                # pp6: 2文字前が記号でないやつに対するpp ## +0.00001\n                if text[start-2] == ',' and text[start-1] != ' ':\n                    preds_pp[raw_idx][p_index][0] = start - 1\n                    start = start - 1\n                if text[start-2] == '-' and text[start-1] != ' ':\n                    preds_pp[raw_idx][p_index][0] = start - 1\n                    start = start - 1\n                if text[start-2] == '\\\"' and text[start-1] != ' ':\n                    preds_pp[raw_idx][p_index][0] = start - 1\n                    start = start - 1\n                if text[start-2] == ':' and text[start-1] != ' ':\n                    preds_pp[raw_idx][p_index][0] = start - 1\n                    start = start - 1\n                if text[start-2] == '(' and text[start-1] != ' ':\n                    preds_pp[raw_idx][p_index][0] = start - 1\n                    start = start - 1\n                if text[start-2] == ')' and text[start-1] != ' ':\n                    preds_pp[raw_idx][p_index][0] = start - 1\n                    start = start - 1\n                # pp7: heart -> h + eart となっているようなものを汎用的に修正する ## +0.00050\n                try:\n                    text_token = word_tokenize(text[start-1:end])\n                    text_token = [text for text in text_token]\n                    first = text[start:end].split()[0]\n                    if first not in text_token:\n                        for t in text_token:\n                            if (first == t[-len(first):]) and (t[0].isalpha()):\n                                sub = len(t) - len(first)\n                                preds_pp[raw_idx][p_index][0] = start - sub\n                                start = start - sub\n                                break\n                except:\n                    None\n                # pp8: endの修正 ## +0.00001\n                if text[end-1:end] == '.':\n                    preds_pp[raw_idx][p_index][1] = end - 1\n                    end = end - 1\n                if text[end-1:end] == '-' and text[end:end+1].isnumeric():\n                    preds_pp[raw_idx][p_index][1] = end + 1\n                    end = end + 1\n    return preds_pp","metadata":{"execution":{"iopub.status.busy":"2022-04-15T11:57:18.788445Z","iopub.execute_input":"2022-04-15T11:57:18.788748Z","iopub.status.idle":"2022-04-15T11:57:18.834143Z","shell.execute_reply.started":"2022-04-15T11:57:18.788719Z","shell.execute_reply":"2022-04-15T11:57:18.833336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(CFG, text, feature_text,tokenizer):\n    inputs =  tokenizer.encode_plus(text, feature_text, \n                           add_special_tokens=True,\n#                            max_length=CFG.max_len,\n#                            padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\nclass TestDataset(Dataset):\n    def __init__(self,df,tokenizer,CFG):\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n        self.tokenizer = tokenizer\n        self.CFG = CFG\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.CFG,\n                               self.pn_historys[item], \n                               self.feature_texts[item],\n                               self.tokenizer)\n        return inputs\n\n\n    \nclass custom_model_ex038(nn.Module):\n    def __init__(self):\n        super(custom_model_ex038, self).__init__()\n        self.model = AutoModel.from_pretrained(\n            CFG_ex038.tokenizer_path, \n        )\n        self.config = AutoConfig.from_pretrained(CFG_ex038.tokenizer_path)\n        self.dropout1 = nn.Dropout(p=0.2)\n        self.ln1 = nn.LayerNorm(1024)\n        self.linear1 = nn.Linear(1024,512)\n        self.ln2 = nn.LayerNorm(512)\n        self.relu = nn.ReLU()\n        self.dropout2 = nn.Dropout(p=0.2)\n        self.linear2 = nn.Linear(512,1)\n        self._init_weights(self.linear1)\n        self._init_weights(self.linear2)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n            \n    def forward(self, ids, mask,token_type):\n        # pooler\n        emb = self.model(ids, mask,token_type)[\"last_hidden_state\"]\n        output = self.ln1(emb)\n        output = self.dropout1(output)\n        output = self.linear1(output)\n        output = self.ln2(output)\n        output = self.relu(output)\n        output = self.dropout2(output)\n        output = self.linear2(output)\n        return output\n\n    \nclass custom_model_ex041(nn.Module):\n    def __init__(self):\n        super(custom_model_ex041, self).__init__()\n        self.model = AutoModel.from_pretrained(\n            CFG_ex041.tokenizer_path, \n        )\n        self.config = AutoConfig.from_pretrained(CFG_ex041.tokenizer_path)\n        self.dropout1 = nn.Dropout(p=0.2)\n        self.ln1 = nn.LayerNorm(1536)\n        self.linear1 = nn.Linear(1536,768)\n        self.ln2 = nn.LayerNorm(768)\n        self.relu = nn.ReLU()\n        self.dropout2 = nn.Dropout(p=0.2)\n        self.linear2 = nn.Linear(768,1)\n        self._init_weights(self.linear1)\n        self._init_weights(self.linear2)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n            \n    def forward(self, ids, mask,token_type):\n        # pooler\n        emb = self.model(ids, mask,token_type)[\"last_hidden_state\"]\n        output = self.ln1(emb)\n        output = self.dropout1(output)\n        output = self.linear1(output)\n        output = self.ln2(output)\n        output = self.relu(output)\n        output = self.dropout2(output)\n        output = self.linear2(output)\n        return output\n    \n    \nclass custom_model_ex051(nn.Module):\n    def __init__(self):\n        super(custom_model_ex051, self).__init__()\n        self.model = AutoModel.from_pretrained(\n            CFG_ex051.tokenizer_path, \n        )\n        self.dropout1 = nn.Dropout(p=0.2)\n        self.ln1 = nn.LayerNorm(1536)\n        self.linear1 = nn.Linear(1536,768)\n        self.ln2 = nn.LayerNorm(768)\n        self.relu = nn.ReLU()\n        self.dropout2 = nn.Dropout(p=0.2)\n        self.linear2 = nn.Linear(768,1)\n            \n    def forward(self, ids, mask,token_type):\n        # pooler\n        emb = self.model(ids, mask,token_type)[\"last_hidden_state\"]\n        output = self.ln1(emb)\n        output = self.dropout1(output)\n        output = self.linear1(output)\n        output = self.ln2(output)\n        output = self.relu(output)\n        output = self.dropout2(output)\n        output = self.linear2(output)\n        return output\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-15T11:57:18.935902Z","iopub.execute_input":"2022-04-15T11:57:18.93641Z","iopub.status.idle":"2022-04-15T11:57:18.957413Z","shell.execute_reply.started":"2022-04-15T11:57:18.936367Z","shell.execute_reply":"2022-04-15T11:57:18.956715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SUB_PATH = \"../input/nbme-score-clinical-patient-notes/sample_submission.csv\"\nTEST_PATH = \"../input/nbme-score-clinical-patient-notes/test.csv\"\nFEATURE_PATH = \"../input/nbme-score-clinical-patient-notes/features.csv\"\nPATIENT_NOTES_PATH = \"../input/nbme-score-clinical-patient-notes/patient_notes.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-04-15T11:57:19.31673Z","iopub.execute_input":"2022-04-15T11:57:19.316955Z","iopub.status.idle":"2022-04-15T11:57:19.320856Z","shell.execute_reply.started":"2022-04-15T11:57:19.316928Z","shell.execute_reply":"2022-04-15T11:57:19.320063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===========================================\n# main\n# ===========================================\ntest = pd.read_csv(TEST_PATH)\nsubmission = pd.read_csv(SUB_PATH)\nfeatures = pd.read_csv(FEATURE_PATH)\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\nfeatures = preprocess_features(features)\npatient_notes = pd.read_csv(PATIENT_NOTES_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T11:57:19.498561Z","iopub.execute_input":"2022-04-15T11:57:19.499067Z","iopub.status.idle":"2022-04-15T11:57:20.083436Z","shell.execute_reply.started":"2022-04-15T11:57:19.499035Z","shell.execute_reply":"2022-04-15T11:57:20.082583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.merge(features, on=['feature_num', 'case_num'], how='left')\ntest = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')","metadata":{"execution":{"iopub.status.busy":"2022-04-15T11:57:20.085715Z","iopub.execute_input":"2022-04-15T11:57:20.08614Z","iopub.status.idle":"2022-04-15T11:57:20.111651Z","shell.execute_reply.started":"2022-04-15T11:57:20.086088Z","shell.execute_reply":"2022-04-15T11:57:20.111002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testの並び替え\ntoken_len = []\ntokenizer = DebertaV2TokenizerFast.from_pretrained(CFG_ex038.tokenizer_path)\nfor f,p in zip(test['feature_text'].values,test['pn_history'].values):\n    inputs =  tokenizer.encode_plus(p, f, \n                           add_special_tokens=True,\n                           return_offsets_mapping=False)\n    token_len.append(len(inputs[\"input_ids\"]))\ntest[\"token_len\"] = token_len\ntest = test.sort_values(by=\"token_len\").reset_index(drop=True)\ndisplay(test)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T11:57:20.168794Z","iopub.execute_input":"2022-04-15T11:57:20.168995Z","iopub.status.idle":"2022-04-15T11:57:21.297087Z","shell.execute_reply.started":"2022-04-15T11:57:20.16897Z","shell.execute_reply":"2022-04-15T11:57:21.296327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================================\n# exp051 deberta-v2-xxlarge\n# ================================================\nsub_preds051 = np.zeros((len(test),CFG_ex051.max_len,1))\ntokenizer051 = DebertaV2TokenizerFast.from_pretrained(CFG_ex051.tokenizer_path)\ntest_dataset = TestDataset(test, tokenizer051,CFG_ex051)\ntest_loader = DataLoader(test_dataset, \n                         batch_size=CFG_ex051.batch_size,\n                         shuffle=False, \n                         collate_fn=DataCollatorWithPadding(tokenizer=tokenizer051, padding='longest'),\n                         pin_memory=True, drop_last=False)\nmodel_list = [\"../input/nbme-ex051/ex051_0.pth\",\n              \"../input/nbme-ex051/ex051_1.pth\",\n              \"../input/nbme-ex051-2/ex051_2.pth\",\n              \"../input/nbme-ex051/ex051_3.pth\",\n              \"../input/nbme-ex051-2/ex051_4.pth\",]\nfor fold in tqdm(range(CFG_ex051.n_fold)):\n    model = custom_model_ex051()\n    model.load_state_dict(torch.load(model_list[fold]))\n    model.to(device)\n    model.eval()\n    \n    test_preds_ = np.ndarray((0,CFG_ex051.max_len,1))\n    with torch.no_grad():  \n        for d in test_loader:\n            ids = d['input_ids'].to(device)\n            mask = d['attention_mask'].to(device)\n            token_type = d[\"token_type_ids\"].to(device)\n            with autocast():\n                outputs = model(ids,mask,token_type)\n            outputs = np.concatenate([outputs.sigmoid().detach().cpu().numpy(),np.zeros([len(outputs),CFG_ex051.max_len - outputs.shape[1],1])],axis=1)\n            test_preds_ = np.concatenate([test_preds_, outputs], axis=0)\n    \n    torch.cuda.empty_cache()\n    sub_preds051 += test_preds_ / CFG_ex051.n_fold\n    del model,test_preds_; gc.collect()\nprint(sub_preds051)\ndel test_dataset,test_loader\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================================\n# exp038 deberta-v3-large\n# ================================================\nsub_preds038 = np.zeros((len(test),CFG_ex038.max_len,1))\ntokenizer038 = DebertaV2TokenizerFast.from_pretrained(CFG_ex038.tokenizer_path)\ntest_dataset = TestDataset(test, tokenizer038,CFG_ex038)\ntest_loader = DataLoader(test_dataset, \n                         batch_size=CFG_ex038.batch_size,\n                         shuffle=False, \n                         collate_fn=DataCollatorWithPadding(tokenizer=tokenizer038, padding='longest'),\n                         pin_memory=True, drop_last=False)\nfor fold in tqdm(range(CFG_ex038.n_fold)):\n    model = custom_model_ex038()\n    model.load_state_dict(torch.load(CFG_ex038.model_path + f\"_{fold}.pth\"))\n    model.to(device)\n    model.eval()\n    \n    test_preds_ = np.ndarray((0,CFG_ex038.max_len,1))\n    with torch.no_grad():  \n        for d in test_loader:\n            ids = d['input_ids'].to(device)\n            mask = d['attention_mask'].to(device)\n            token_type = d[\"token_type_ids\"].to(device)\n            with autocast():\n                outputs = model(ids,mask,token_type)\n            outputs = np.concatenate([outputs.sigmoid().detach().cpu().numpy(),np.zeros([len(outputs),CFG_ex038.max_len - outputs.shape[1],1])],axis=1)\n            test_preds_ = np.concatenate([test_preds_, outputs], axis=0)\n    \n    torch.cuda.empty_cache()\n    sub_preds038 += test_preds_ / CFG_ex038.n_fold\n    del model,test_preds_; gc.collect()\nprint(sub_preds038)\ndel test_dataset,test_loader\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T11:57:22.850499Z","iopub.execute_input":"2022-04-15T11:57:22.850839Z","iopub.status.idle":"2022-04-15T11:57:22.858522Z","shell.execute_reply.started":"2022-04-15T11:57:22.8508Z","shell.execute_reply":"2022-04-15T11:57:22.857773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================================\n# exp041 deberta-v2-xlarge\n# ================================================\nsub_preds041 = np.zeros((len(test),CFG_ex041.max_len,1))\ntokenizer041 = DebertaV2TokenizerFast.from_pretrained(CFG_ex041.tokenizer_path)\ntest_dataset = TestDataset(test, tokenizer041,CFG_ex041)\ntest_loader = DataLoader(test_dataset, \n                         batch_size=CFG_ex041.batch_size,\n                         shuffle=False, \n                         collate_fn=DataCollatorWithPadding(tokenizer=tokenizer041, padding='longest'),\n                         pin_memory=True, drop_last=False)\nfor fold in tqdm(range(CFG_ex041.n_fold)):\n    model = custom_model_ex041()\n    model.load_state_dict(torch.load(CFG_ex041.model_path + f\"_{fold}.pth\"))\n    model.to(device)\n    model.eval()\n    \n    test_preds_ = np.ndarray((0,CFG_ex041.max_len,1))\n    with torch.no_grad():  \n        for d in test_loader:\n            ids = d['input_ids'].to(device)\n            mask = d['attention_mask'].to(device)\n            token_type = d[\"token_type_ids\"].to(device)\n            with autocast():\n                outputs = model(ids,mask,token_type)\n            outputs = np.concatenate([outputs.sigmoid().detach().cpu().numpy(),np.zeros([len(outputs),CFG_ex041.max_len - outputs.shape[1],1])],axis=1)\n            test_preds_ = np.concatenate([test_preds_, outputs], axis=0)\n    \n    torch.cuda.empty_cache()\n    sub_preds041 += test_preds_ / CFG_ex041.n_fold\n    del model,test_preds_; gc.collect()\nprint(sub_preds041)\ndel test_dataset,test_loader\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T11:59:22.396013Z","iopub.execute_input":"2022-04-15T11:59:22.396272Z","iopub.status.idle":"2022-04-15T12:00:21.634415Z","shell.execute_reply.started":"2022-04-15T11:59:22.396243Z","shell.execute_reply":"2022-04-15T12:00:21.63319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_probs038 = get_char_probs(test['pn_history'].values, sub_preds038.reshape([-1,512]), tokenizer038)\nchar_probs041 = get_char_probs(test['pn_history'].values, sub_preds041.reshape([-1,512]), tokenizer041)\nchar_probs051 = get_char_probs(test['pn_history'].values, sub_preds051.reshape([-1,512]), tokenizer051)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===========================\n# nakama ex141\n# ===========================\n# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    path=\"../input/nbme-debertav3large-exp141/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=24\n    fc_dropout=0.\n    max_len=315\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]\n    losses=['bce', 'bce', 'bce', 'bce']\n    target_sizes=[1, 1, 1, 1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nif CFG.model.find(\"deberta-v2\") >= 0 or CFG.model.find(\"deberta-v3\") >= 0:\n    CFG.tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.path+'tokenizer/')\nelse:\n    CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_n = test.copy()\ntest_n['feature_text'] = test_n['feature_text'].str.lower()\ntest_n['pn_history'] = test_n['pn_history'].str.lower()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text, feature_text):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           # max_length=CFG.max_len,\n                           # padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item])\n        return inputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device, loss='bce'):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        if loss == 'bce':\n            preds.append(y_preds.sigmoid().to('cpu').numpy())\n        elif loss == 'ce':\n            preds.append(y_preds.softmax(2).to('cpu').numpy()[:,:,1])\n    predictions = preds.copy()\n    max_len = max([pred.shape[1] for pred in preds])\n    for i, pred in enumerate(preds):\n        bs = pred.shape[0]\n        p = np.zeros((bs, max_len, 1))\n        p[:,:pred.shape[1],:] = pred\n        predictions[i] = p\n    predictions = np.concatenate(predictions)\n    predictions = predictions.reshape((-1, max_len))\n    return predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test_n)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         collate_fn=DataCollatorWithPadding(tokenizer=CFG.tokenizer, padding='longest'),\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold, loss in zip(CFG.trn_fold, CFG.losses):\n    CFG.target_size = 1 if loss == 'bce' else 2\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device, loss)\n    char_probs = get_char_probs(test_n['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions = np.mean(predictions, axis=0)\nprint(predictions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_probs = []\nfor i in range(len(char_probs038)):\n    char_probs.append(char_probs038[i] * w1 + \n                      char_probs041[i] * w2 + \n                      char_probs051[i] * w3 + \n                      predictions[i] * w4)\ncase_nums = test[\"case_num\"].values\nresults = get_results_raw_pp(char_probs, case_nums, th_dict=th_dict)\nresults = get_predictions(results)\nresults_postprocess = postprocess(test['pn_history'].values, results)\nresults_postprocess = get_results_from_preds_list(results_postprocess)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['location'] = results_postprocess\ndisplay(test.head())\ntest[['id', 'location']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T07:02:14.475346Z","iopub.execute_input":"2022-03-20T07:02:14.475986Z","iopub.status.idle":"2022-03-20T07:02:14.508765Z","shell.execute_reply.started":"2022-03-20T07:02:14.475943Z","shell.execute_reply":"2022-03-20T07:02:14.508007Z"},"trusted":true},"execution_count":null,"outputs":[]}]}