{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mkdir TRAIN_DATA\n!pip install matplotlib scikit-learn pandas scipy setuptools wheel spacy[cuda110,transformers,lookups] ipython && pip install jupyter --upgrade\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display\nimport re\n\nfrom sklearn.model_selection import train_test_split\n\nimport spacy\nfrom spacy import displacy\nfrom spacy.tokens import DocBin\nimport json\nfrom tqdm import tqdm\n\ndef divide_chunks(l, n):\n    # looping till length l\n    for i in range(0, len(l), n): \n        yield l[i:i + n]\n\nclass spacy_prep:\n    def __init__(self, feature_desc, location_desc, note_corpus):\n        self.feature_desc = feature_desc\n        self.location_desc = location_desc\n        self.note_corpus = note_corpus\n        self.nlp = spacy.blank('en')\n        \n    def start_prep(self):\n        location_dict = {}\n        rels = []\n        feature_keys = self.feature_desc[:, 1]\n        \n        for key in feature_keys:\n            location_dict[key] = []\n        \n        for entry in self.location_desc:\n            for feat in self.feature_desc:\n                if entry[0] == feat[0]:\n                    if entry[1] !='[]':\n                        stripper = entry[1][0:len(entry[1])-1]\n                        stripper = stripper[1:]\n                        #stripper = re.sub(' ', '-', stripper)\n                        #stripper = re.sub(\"'\", \"\", stripper)\n                        #stripper = re.split(\";\", stripper)\n                        \n                        #stripper = re.split(\"t\", stripper)\n                        stripper = re.split(\",\", stripper)\n                        stripper = [[int(s) for s in re.findall(r'\\b\\d+\\b', sentry)] for sentry in stripper]\n                        #print(stripper)\n\n                        #stripper = re.split(\",\" , stripper)\n                        #stripper = [re.sub(\",\",\"\", ent) for ent in stripper]\n                        #stripper = [re.split(\";\", entity) for entity in stripper]\n                        #stripper = [[int(e) for e in entu] for entu in stripper]\n                        indices = []\n                        for given_list in stripper:\n                            for list_entry in given_list:\n                                #print(list_entry)\n                                \n                                indices.append(list_entry)\n                        indices = list(divide_chunks(indices, 2)) #Paired chunks\n                        #print(indices)\n                        note_num = entry[2]\n                        #print(note_num)\n                        #indices = np.split(indices, 2)\n                        \n                        #print(indices)\n                        location_dict[feat[1]].append(tuple([note_num, indices]))\n                        \n        for feat in self.feature_desc:\n            for (note_num, indexes) in location_dict[feat[1]]:\n                for entry in indexes:\n                    start = entry[0]\n                    stop = entry[1]\n                    my_note = self.note_corpus.loc[note_num]\n                    rels.append([my_note, [start, stop], feat[1]])\n        return rels\n    \n    def training_prep(self):\n        preproccd_data = self.start_prep()\n        collective_dict = {'TRAINING_DATA': [], \n                           'VALIDATION_DATA': []}\n        \n        \n        for note in self.note_corpus.values:\n            entities = []\n            for entry in preproccd_data:\n                \n                if entry[0] == note:\n                    #print(\"yes\")\n                    start = entry[1][0]\n                    stop = entry[1][1]\n                    key = entry[2]\n                    entities.append((start, stop, key))\n                            \n            results = [note, {\"entities\": entities}]\n            if results[1]['entities'] == []:\n                del results[1]\n                del results[0]\n                \n            #print(results)\n            collective_dict['TRAINING_DATA'].append(results)\n            \n        collective_dict['TRAINING_DATA'] = [x for x in collective_dict['TRAINING_DATA'] if x != []]\n        \n        collective_dict['TRAINING_DATA'], collective_dict['VALIDATION_DATA'] = train_test_split(collective_dict['TRAINING_DATA'] \n                                                                                                , test_size=0.2, random_state=42)\n        json_string = json.dumps(collective_dict)\n        \n        with open('clin_data.json', 'w') as outfile:\n            outfile.write(json_string)\n            \n        return collective_dict\n    \n    def create_training(self):\n        coll_dict = self.training_prep()\n        TRAIN_DATA = coll_dict['TRAINING_DATA']\n        db = DocBin()\n        for text, annot in tqdm(TRAIN_DATA):\n            doc = self.nlp.make_doc(text)\n            ents = []\n    \n            # create span objects\n            for start, end, label in annot[\"entities\"]:\n                span = doc.char_span(start, end, label=label, alignment_mode=\"contract\") \n    \n                # skip if the character indices do not map to a valid span\n                if span is None:\n                    #print(\"start: {}, end: {}, label: {}\".format(start, end, label))\n                    print(\"Skipping entity.\")\n                else:\n                    #print(\"start: {}, end: {}, label: {}\".format(start, end, label))\n                    ents.append(span)\n                    # handle erroneous entity annotations by removing them\n                    try:\n                        doc.ents = ents\n                    except:\n                        # print(\"BAD SPAN:\", span, \"\\n\")\n                        ents.pop()\n            doc.ents = ents\n    \n            # pack Doc objects into DocBin\n            db.add(doc)\n            \n        return db\n    \n    def create_validation(self):\n        coll_dict = self.training_prep()\n        VAL_DATA = coll_dict['VALIDATION_DATA']\n        db = DocBin()\n        for text, annot in tqdm(VAL_DATA):\n            doc = self.nlp.make_doc(text)\n            ents = []\n    \n            # create span objects\n            for start, end, label in annot[\"entities\"]:\n                span = doc.char_span(start, end, label=label, alignment_mode=\"contract\") \n    \n                # skip if the character indices do not map to a valid span\n                if span is None:\n                    #print(\"start: {}, end: {}, label: {}\".format(start, end, label))\n                    print(\"Skipping entity.\")\n                else:\n                    #print(\"start: {}, end: {}, label: {}\".format(start, end, label))\n                    ents.append(span)\n                    # handle erroneous entity annotations by removing them\n                    try:\n                        doc.ents = ents\n                    except:\n                        # print(\"BAD SPAN:\", span, \"\\n\")\n                        ents.pop()\n            doc.ents = ents\n    \n            # pack Doc objects into DocBin\n            db.add(doc)\n            \n        return db\n","metadata":{"execution":{"iopub.status.busy":"2022-03-18T08:40:08.198117Z","iopub.execute_input":"2022-03-18T08:40:08.198399Z","iopub.status.idle":"2022-03-18T08:40:23.782795Z","shell.execute_reply.started":"2022-03-18T08:40:08.19836Z","shell.execute_reply":"2022-03-18T08:40:23.781901Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data():\n    #Load raw data\n    feature_frame = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/features.csv')\n    note_frame = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv')\n    train_frame = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/train.csv')\n    print(\"Feature frame columns:\\n{}\\nNote frame columns:\\n{}\\nTrain frame columns:\\n{}\\n\\n\".format(feature_frame.columns, note_frame.columns, train_frame.columns))\n    \n    note_frame.set_index('pn_num', inplace=True)\n    note_corpus = note_frame['pn_history']\n    feature_frame = feature_frame.drop_duplicates('feature_text')\n    \n    feature_desc = feature_frame[['feature_num', 'feature_text']].values\n    location_desc = train_frame[['feature_num', 'location', 'pn_num']].values\n    \n    prepper = spacy_prep(feature_desc, location_desc, note_corpus)\n    \n    TRAIN_DATA_DOC = prepper.create_training()\n    TRAIN_DATA_DOC.to_disk(\"./TRAIN_DATA/TRAIN_DATA.spacy\")\n    \n    VAL_DATA_DOC = prepper.create_validation()\n    VAL_DATA_DOC.to_disk(\"./TRAIN_DATA/VAL_DATA.spacy\")","metadata":{"execution":{"iopub.status.busy":"2022-03-18T08:40:23.785839Z","iopub.execute_input":"2022-03-18T08:40:23.786132Z","iopub.status.idle":"2022-03-18T08:40:23.795469Z","shell.execute_reply.started":"2022-03-18T08:40:23.786079Z","shell.execute_reply":"2022-03-18T08:40:23.794231Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tester():\n    note_frame = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv')\n    note_corpus = note_frame['pn_history']\n    notes = note_corpus.values\n    \n    model_test = notes[50]\n    #print(model_test)\n    \n    nlp_output = spacy.load(\"output/model-best\")\n    doc = nlp_output(model_test)\n    displacy.render(doc, style=\"ent\")\n    \ndef setup():\n    load_data()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T08:40:23.79749Z","iopub.execute_input":"2022-03-18T08:40:23.798366Z","iopub.status.idle":"2022-03-18T08:40:23.806485Z","shell.execute_reply.started":"2022-03-18T08:40:23.798328Z","shell.execute_reply":"2022-03-18T08:40:23.805556Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ ==\"__main__\":\n    setup()\n    !python3 -m spacy init fill-config ../input/spaceyconfig/base_config.cfg config.cfg\n    !python3 -m spacy train config.cfg -g 0 --output ./output\n    tester()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T08:40:23.808869Z","iopub.execute_input":"2022-03-18T08:40:23.809253Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]}]}