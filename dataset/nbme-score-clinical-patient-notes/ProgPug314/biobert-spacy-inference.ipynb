{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mkdir TRAIN_DATA\n#!pip install matplotlib scikit-learn pandas scipy setuptools wheel spacy[cuda110,transformers,lookups] ipython && pip install jupyter --upgrade\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display\nimport re\n\nfrom itertools import chain\nfrom sklearn.model_selection import train_test_split\n\nimport spacy\nfrom spacy import displacy\nfrom spacy.tokens import DocBin\nimport json\nfrom tqdm import tqdm\n\ndef divide_chunks(l, n):\n    # looping till length l\n    for i in range(0, len(l), n): \n        yield l[i:i + n]\n\n\ndef process_feature_text(text):\n    text = re.sub('I-year', '1-year', text)\n    text = re.sub('-OR-', \" or \", text)\n    text = re.sub('-', ' ', text)\n    return text\n\n\ndef clean_spaces(txt):\n    txt = re.sub('\\n', ' ', txt)\n    txt = re.sub('\\t', ' ', txt)\n    txt = re.sub('\\r', ' ', txt)\n#     txt = re.sub(r'\\s+', ' ', txt)\n    return txt\n\nclass spacy_prep:\n    def __init__(self, feature_desc, location_desc, note_corpus):\n        self.feature_desc = feature_desc\n        self.location_desc = location_desc\n        self.note_corpus = note_corpus\n        self.nlp = spacy.blank('en')\n        \n    def start_prep(self):\n        location_dict = {}\n        rels = []\n        feature_keys = self.feature_desc[:, 1]\n        \n        for key in feature_keys:\n            location_dict[key] = []\n        \n        for entry in self.location_desc:\n            for feat in self.feature_desc:\n                if entry[0] == feat[0]:\n                    if entry[1] !='[]':\n                        stripper = entry[1][0:len(entry[1])-1]\n                        stripper = stripper[1:]\n                        #stripper = re.sub(' ', '-', stripper)\n                        #stripper = re.sub(\"'\", \"\", stripper)\n                        #stripper = re.split(\";\", stripper)\n                        \n                        #stripper = re.split(\"t\", stripper)\n                        stripper = re.split(\",\", stripper)\n                        stripper = [[int(s) for s in re.findall(r'\\b\\d+\\b', sentry)] for sentry in stripper]\n                        #print(stripper)\n\n                        #stripper = re.split(\",\" , stripper)\n                        #stripper = [re.sub(\",\",\"\", ent) for ent in stripper]\n                        #stripper = [re.split(\";\", entity) for entity in stripper]\n                        #stripper = [[int(e) for e in entu] for entu in stripper]\n                        indices = []\n                        for given_list in stripper:\n                            for list_entry in given_list:\n                                #print(list_entry)\n                                \n                                indices.append(list_entry)\n                        indices = list(divide_chunks(indices, 2)) #Paired chunks\n                        #print(indices)\n                        note_num = entry[2]\n                        #print(note_num)\n                        #indices = np.split(indices, 2)\n                        \n                        #print(indices)\n                        location_dict[feat[1]].append(tuple([note_num, indices]))\n                        \n        for feat in self.feature_desc:\n            for (note_num, indexes) in location_dict[feat[1]]:\n                for entry in indexes:\n                    start = entry[0]\n                    stop = entry[1]\n                    my_note = self.note_corpus.loc[note_num]\n                    my_note = process_feature_text(my_note)\n                    my_note = clean_spaces(my_note)\n                    rels.append([my_note, [start, stop], feat[1]])\n        return rels\n    \n    def training_prep(self):\n        preproccd_data = self.start_prep()\n        collective_dict = {'TRAINING_DATA': [], \n                           'VALIDATION_DATA': []}\n        \n        \n        for note in self.note_corpus.values:\n            entities = []\n            for entry in preproccd_data:\n                \n                if entry[0] == note:\n                    #print(\"yes\")\n                    start = entry[1][0]\n                    stop = entry[1][1]\n                    key = entry[2]\n                    entities.append((start, stop, key))\n                            \n            results = [note, {\"entities\": entities}]\n            if results[1]['entities'] == []:\n                del results[1]\n                del results[0]\n                \n            #print(results)\n            collective_dict['TRAINING_DATA'].append(results)\n            \n        collective_dict['TRAINING_DATA'] = [x for x in collective_dict['TRAINING_DATA'] if x != []]\n        \n        collective_dict['TRAINING_DATA'], collective_dict['VALIDATION_DATA'] = train_test_split(collective_dict['TRAINING_DATA'] \n                                                                                                , test_size=0.2, random_state=42)\n        json_string = json.dumps(collective_dict)\n        \n        with open('clin_data.json', 'w') as outfile:\n            outfile.write(json_string)\n            \n        return collective_dict\n    \n    def create_training(self):\n        coll_dict = self.training_prep()\n        TRAIN_DATA = coll_dict['TRAINING_DATA']\n        db = DocBin()\n        for text, annot in tqdm(TRAIN_DATA):\n            doc = self.nlp.make_doc(text)\n            ents = []\n    \n            # create span objects\n            for start, end, label in annot[\"entities\"]:\n                span = doc.char_span(start, end, label=label, alignment_mode=\"contract\") \n    \n                # skip if the character indices do not map to a valid span\n                if span is None:\n                    #print(\"start: {}, end: {}, label: {}\".format(start, end, label))\n                    print(\"Skipping entity.\")\n                else:\n                    #print(\"start: {}, end: {}, label: {}\".format(start, end, label))\n                    ents.append(span)\n                    # handle erroneous entity annotations by removing them\n                    try:\n                        doc.ents = ents\n                    except:\n                        # print(\"BAD SPAN:\", span, \"\\n\")\n                        ents.pop()\n            doc.ents = ents\n    \n            # pack Doc objects into DocBin\n            db.add(doc)\n            \n        return db\n    \n    def create_validation(self):\n        coll_dict = self.training_prep()\n        VAL_DATA = coll_dict['VALIDATION_DATA']\n        db = DocBin()\n        for text, annot in tqdm(VAL_DATA):\n            doc = self.nlp.make_doc(text)\n            ents = []\n    \n            # create span objects\n            for start, end, label in annot[\"entities\"]:\n                span = doc.char_span(start, end, label=label, alignment_mode=\"contract\") \n    \n                # skip if the character indices do not map to a valid span\n                if span is None:\n                    #print(\"start: {}, end: {}, label: {}\".format(start, end, label))\n                    print(\"Skipping entity.\")\n                else:\n                    #print(\"start: {}, end: {}, label: {}\".format(start, end, label))\n                    ents.append(span)\n                    # handle erroneous entity annotations by removing them\n                    try:\n                        doc.ents = ents\n                    except:\n                        # print(\"BAD SPAN:\", span, \"\\n\")\n                        ents.pop()\n            doc.ents = ents\n    \n            # pack Doc objects into DocBin\n            db.add(doc)\n            \n        return db\n","metadata":{"execution":{"iopub.status.busy":"2022-04-22T03:51:19.330399Z","iopub.execute_input":"2022-04-22T03:51:19.330813Z","iopub.status.idle":"2022-04-22T03:51:20.240749Z","shell.execute_reply.started":"2022-04-22T03:51:19.33078Z","shell.execute_reply":"2022-04-22T03:51:20.23972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data():\n    #Load raw data\n    feature_frame = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/features.csv')\n    note_frame = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv')\n    train_frame = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/train.csv')\n    print(\"Feature frame columns:\\n{}\\nNote frame columns:\\n{}\\nTrain frame columns:\\n{}\\n\\n\".format(feature_frame.columns, note_frame.columns, train_frame.columns))\n    \n    note_frame['pn_history'] = note_frame['pn_history'].apply(clean_spaces)\n    note_frame.set_index('pn_num', inplace=True)\n    note_corpus = note_frame['pn_history']\n    #print(note_corpus)\n    \n    feature_frame = feature_frame.drop_duplicates('feature_text')\n    feature_frame['feature_text'] = feature_frame['feature_text'].apply(process_feature_text)\n    feature_frame['feature_text'] = feature_frame['feature_text'].apply(clean_spaces)\n    \n    feature_desc = feature_frame[['feature_num', 'feature_text']].values\n    location_desc = train_frame[['feature_num', 'location', 'pn_num']].values\n    \n    prepper = spacy_prep(feature_desc, location_desc, note_corpus)\n    \n    TRAIN_DATA_DOC = prepper.create_training()\n    TRAIN_DATA_DOC.to_disk(\"./TRAIN_DATA/TRAIN_DATA.spacy\")\n    \n    VAL_DATA_DOC = prepper.create_validation()\n    VAL_DATA_DOC.to_disk(\"./TRAIN_DATA/VAL_DATA.spacy\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T03:51:20.243494Z","iopub.execute_input":"2022-04-22T03:51:20.244292Z","iopub.status.idle":"2022-04-22T03:51:20.25486Z","shell.execute_reply.started":"2022-04-22T03:51:20.244245Z","shell.execute_reply":"2022-04-22T03:51:20.25336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ndef prep_sub(notes, feature_dict, test_csv):\n    print(feature_dict)\n    test_info = test_csv[['case_num', 'pn_num', 'feature_num']].values\n    spacy.require_gpu()\n    nlp_output = spacy.load(\"../input/med-models/output_rob/model-best\")\n    entities = []\n    \n    for entry in tqdm(test_info):\n        try:\n            rel_case_num = entry[0]\n            rel_note_num = entry[1]\n            rel_feat_num = entry[2]\n            \n            if rel_feat_num == 601:\n                feature_dict['Male'] = 601\n            else:\n                feature_dict['Male'] = 11\n            \n            if rel_feat_num == 602:\n                feature_dict['17-year'] = 602\n            else:\n                feature_dict['17-year'] = 12\n                \n            #print(rel_feat_num)\n            \n            rel_note_row = np.where(notes[:, 2] == rel_note_num)\n            #print(rel_note_row)\n            rel_note = notes[rel_note_row][0][0]\n            #print(rel_note)\n            rel_doc = nlp_output(rel_note)\n            \n            entity_list = []\n            equiv = []\n            for ent in rel_doc.ents:\n                span_list = []\n                if feature_dict[ent.label_]== rel_feat_num:\n                    equiv.append(1)\n                    curr_span = str(ent.start_char)+\" \"+ str(ent.end_char)\n                    #if rel_feat_num > 10:\n                        #print(curr_span)\n                    span_list.append(curr_span)\n                    #print(curr_span)\n                span_list = [x for x in span_list if x]\n                #print(span_list)\n                if span_list != []:\n                    #print(span_list)\n                    entry_string = str(rel_note_num).zfill(5) +'_'+ str(rel_feat_num).zfill(3)\n                    entity_list.append([rel_case_num, rel_note_num, rel_feat_num, span_list])\n            #entity_list = [list(x) for x in set(tuple(x) for x in entity_list)]\n            #print(entity_list)\n            #if rel_feat_num > 9:\n                #print(entity_list)\n            if equiv == []:\n                spanny = []\n                spanny.append('-1 -1')\n                entity_list.append([rel_case_num, rel_note_num, rel_feat_num, spanny])\n            entities.append(entity_list)\n        \n        except IndexError as e:\n            rel_case_num = entry[0]\n            rel_note_num = entry[1]\n            rel_feat_num = entry[2]\n            \n            if rel_feat_num == 601:\n                feature_dict['Male'] = 601\n            else:\n                feature_dict['Male'] = 11\n            \n            if rel_feat_num == 602:\n                feature_dict['17-year'] = 602\n            else:\n                feature_dict['17-year'] = 12\n                \n            #print(rel_feat_num)\n            \n            rel_note_row = np.where(notes[:, 2] == rel_note_num)\n            #print(rel_note_row)\n            rel_note = notes[rel_note_row][0][0]\n            rel_note = rel_note + \" \" + \"err\"\n            #print(rel_note)\n            rel_doc = nlp_output(rel_note)\n            \n            entity_list = []\n            equiv = []\n            for ent in rel_doc.ents:\n                span_list = []\n                if feature_dict[ent.label_]== rel_feat_num:\n                    equiv.append(1)\n                    curr_span = str(ent.start_char)+\" \"+ str(ent.end_char)\n                    #if rel_feat_num > 10:\n                        #print(curr_span)\n                    span_list.append(curr_span)\n                    #print(curr_span)\n                span_list = [x for x in span_list if x]\n                #print(span_list)\n                if span_list != []:\n                    #print(span_list)\n                    entry_string = str(rel_note_num).zfill(5) +'_'+ str(rel_feat_num).zfill(3)\n                    entity_list.append([rel_case_num, rel_note_num, rel_feat_num, span_list])\n            #entity_list = [list(x) for x in set(tuple(x) for x in entity_list)]\n            #print(entity_list)\n            #if rel_feat_num > 9:\n                #print(entity_list)\n            if equiv == []:\n                spanny = []\n                spanny.append('-1 -1')\n                entity_list.append([rel_case_num, rel_note_num, rel_feat_num, spanny])\n            entities.append(entity_list)\n            \n        fin_ents = []\n        if entities != []:\n            for entity in entities:\n                for ent in entity:\n                    #print(ent)\n                    fin_ents.append(ent)\n            #for entry in fin_ents:\n                #print(entry[0])\n    return fin_ents\n            \n            \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T03:51:20.257207Z","iopub.execute_input":"2022-04-22T03:51:20.257904Z","iopub.status.idle":"2022-04-22T03:51:20.285286Z","shell.execute_reply.started":"2022-04-22T03:51:20.257861Z","shell.execute_reply":"2022-04-22T03:51:20.284262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display\ndef submission(entities):\n    final_list = []\n    if entities !=[]:\n        for entry in entities:\n            case_num = entry[0]\n            note_num = entry[1]\n            feat_num = entry[2]\n            #span_list = entry[3]\n            entry[3] = ''.join(entry[3])\n            span = entry[3]\n            if span == '':\n                span = np.nan\n            rel_id = str(note_num).zfill(5) +'_'+ str(feat_num).zfill(3)\n            final_list.append([rel_id,span])\n        \n    subm_df = pd.DataFrame()\n    ids = []\n    locats = []\n    if final_list != []:\n        for entry in final_list:\n            ids.append(entry[0])\n            locats.append(entry[1])\n    \n    subm_df['id'] = pd.Series(ids)\n    subm_df['location'] = pd.Series(locats)\n    \n    dup_dict = {ID: [] for ID in ids}\n    dup_mask = subm_df.id.duplicated()\n    dup_df = subm_df[dup_mask]\n    subm_df = subm_df[~dup_mask]\n    \n    dup_df.reset_index()\n    for ind, row in dup_df.iterrows():\n        row_id = row['id']\n        span = row['location']\n        for key in dup_dict.keys():\n            if row_id == key:\n                dup_dict[key].append(str(span)+ ';')\n                #dup_dict[key] = ''.join(dup_dict[key])\n                \n    for key in dup_dict.keys():\n        dup_dict[key] = ''.join(dup_dict[key])\n        dup_dict[key] = dup_dict[key][:len(dup_dict[key])-1]\n        #print(dup_dict[key])\n        \n    dup_dict = {k:v for k,v in dup_dict.items() if v}\n    subm_df['locale'] = subm_df['id'].apply(lambda x: dup_dict.get(x))\n    #display(subm_df)\n    subm_df['location'] = np.where(~subm_df['locale'].isnull(),subm_df['location'] + ';'+ subm_df['locale'],subm_df['location'])\n    subm_df = subm_df.drop(['locale'], axis=1)\n    subm_df['location'] = subm_df['location'].replace('-1 -1', np.nan)\n    \n    subm_df.to_csv('submission.csv', index=False)\n    #print(subm_df.columns.tolist())\n    \n    display(subm_df)\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-04-22T03:51:20.288259Z","iopub.execute_input":"2022-04-22T03:51:20.288882Z","iopub.status.idle":"2022-04-22T03:51:20.30751Z","shell.execute_reply.started":"2022-04-22T03:51:20.288832Z","shell.execute_reply":"2022-04-22T03:51:20.306459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndef tester():\n    note_frame = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv')\n    note_frame['pn_history'] = note_frame['pn_history'].apply(clean_spaces)\n    note_corpus = note_frame[['pn_history', 'case_num', 'pn_num']]\n    \n    notes = note_corpus.values\n    \n    feature_frame = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/features.csv')\n    feature_frame['feature_text'] = feature_frame['feature_text'].apply(process_feature_text)\n    feature_frame['feature_text'] = feature_frame['feature_text'].apply(clean_spaces)\n    \n    feature_desc = feature_frame[['feature_num', 'feature_text', 'case_num']].values\n    feature_tup = []\n    for entry in feature_desc:\n        feature_tup.append((entry[1], entry[0]))\n        \n    feature_dict = dict(feature_tup)\n    feature_dict[\"17 year\"] = 12\n    feature_dict[\"Male\"] = 11\n    \n    test_csv = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/test.csv')\n    \n    entities = prep_sub(notes, feature_dict, test_csv)\n    submit = submission(entities)\n    #print(entities[0])\n'''\n    model_test = notes[16][0]\n    pn_num = notes[16][2]\n    #print(model_test)\n    \n    nlp_output = spacy.load(\"../input/med-models/output/model-best\")\n    doc = nlp_output(model_test)\n    displacy.render(doc, style=\"ent\")\n    entity_list = []\n    \n    \n\n    for ent in doc.ents:\n        #print(\"Label: {}, Span: {}:{}\".format(feature_dict[ent.label_], ent.start_char, ent.end_char))\n        #entity_list.append([feature_dict[ent.label_], case_num, ent.start_char, ent.end_char])\n        feat_num = feature_dict[ent.label_]\n        feat_num = str(feat_num)\n        feat_num = feat_num.zfill(3)\n\n        pat_num = str(pn_num)\n        pat_num = pat_num.zfill(5)\n\n        my_id = pat_num+\"_\"+feat_num\n        entity_list.append([my_id, ent.start_char, ent.end_char])\n        \n    subm_df = pd.DataFrame()\n    ids = []\n    locats = []\n    #subm_df['id'] = pd.Series(entity_list[:, 0])\n    for entity in entity_list:\n        r_id = entity[0]\n        r_start = entity[1]\n        r_end = entity[2]\n        locale = str(r_start)+\" \"+ str(r_end)\n        ids.append(r_id)\n        locats.append(locale)\n        \n    subm_df['id'] = pd.Series(ids)\n    subm_df['location'] = pd.Series(locats)\n    \n    subm_df.to_csv('submission.csv', index=False)\n'''\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-22T03:51:20.309288Z","iopub.execute_input":"2022-04-22T03:51:20.309945Z","iopub.status.idle":"2022-04-22T03:51:20.33283Z","shell.execute_reply.started":"2022-04-22T03:51:20.309892Z","shell.execute_reply":"2022-04-22T03:51:20.331446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!apt update && pip install git+https://github.com/huggingface/transformers torch==1.10.2+cu113 torchvision==0.11.3+cu113 torchaudio==0.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html","metadata":{"execution":{"iopub.status.busy":"2022-04-22T03:51:20.334745Z","iopub.execute_input":"2022-04-22T03:51:20.335669Z","iopub.status.idle":"2022-04-22T03:51:20.347134Z","shell.execute_reply.started":"2022-04-22T03:51:20.33562Z","shell.execute_reply":"2022-04-22T03:51:20.346052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install matplotlib scikit-learn pandas scipy setuptools wheel spacy[cuda114,transformers,lookups] ipython\n#!pip download spacy_transformers\n#!rm -r ./output","metadata":{"execution":{"iopub.status.busy":"2022-04-22T03:51:20.349766Z","iopub.execute_input":"2022-04-22T03:51:20.351234Z","iopub.status.idle":"2022-04-22T03:51:20.359047Z","shell.execute_reply.started":"2022-04-22T03:51:20.351189Z","shell.execute_reply":"2022-04-22T03:51:20.357969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def setup():\n    load_data()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T03:51:20.361021Z","iopub.execute_input":"2022-04-22T03:51:20.361378Z","iopub.status.idle":"2022-04-22T03:51:20.370536Z","shell.execute_reply.started":"2022-04-22T03:51:20.361332Z","shell.execute_reply":"2022-04-22T03:51:20.369296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ ==\"__main__\":\n    !pip install --no-index --no-deps ../input/mywheels3/Packages/*.whl\n    #setup()\n    #!python3 -m spacy init fill-config ../input/spacy-params/base_config.cfg config.cfg\n    #!python3 -m spacy train config.cfg -g 0 --output ./output\n    import spacy_transformers\n    tester()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-22T03:51:20.372638Z","iopub.execute_input":"2022-04-22T03:51:20.37304Z","iopub.status.idle":"2022-04-22T03:51:37.055358Z","shell.execute_reply.started":"2022-04-22T03:51:20.372982Z","shell.execute_reply":"2022-04-22T03:51:37.054258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}