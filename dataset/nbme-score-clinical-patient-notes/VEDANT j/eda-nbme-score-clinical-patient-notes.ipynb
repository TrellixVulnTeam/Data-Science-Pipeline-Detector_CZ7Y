{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem Statement\n\n1. Use ML to evaluate a candidate doctor's notes regarding a test patient (**an actor who is made to portray some specific clinical case/disease**). \n2. Usually experienced doctors (**examiners**) are given these notes along with some rubrics (**a scoring guide used to evaluate the performance of a candidate**) that outlines the important concepts or features regarding the clinical case potrayed by the actor/test patient. \n\nThe main motive of this exercise is to see that whether the candidate doctor can identify all the important/relevant medical concepts/features related to a specific clinical case. \n\nSimple : more features found in candidate doctor's note, higher the score he/she gets. (**This implies that the candidate has developed good ability to perform correct diagnosis**). \n\nThis whole process is a part of an exam **USMLE**, to measure a candidate's ability to recognize pertinent clinical facts (**features**) during encounters with standardized patients ( **actors** ) so that a correct diagnosis is made & thus a correct assesment of the possible disease of the patient is made.\n\n# Challenge for ML :\n\n1. Symptoms/features can be written in a spectrum of ways. ( eg a feature : \"diminished appetite\" can be written as “eating less,” “clothes fit looser”, \"lost a lot of weight\", \"having only 2 meals a day\" .......) or (if there is a feature : \"17 year old\" can be written as \"17 yo\", \"17 year-old\", \"17 year o M\" .....)\n\n\n2. There could be scenarios where a paritcular case's feature might make sense semantically, when non contiguous pieces of text in the notes are combined. ( eg feature : \"Stress-due-to-caring-for-elderly-parents\" can be written in note as :  \"Feels very overwhelmed\"  & \"takes care of her mother\", the combination of both sentences conveys the fact that the feature was captured by the candiate)\n\n\n3. Capture the intent of sentences based on the context of a note in order to map it to a correct feature.\n\n\n4. Capture medical keywords for correct feature identification.\n\n\n5. Being robust to spelling mistakes (quite common human error).\n\n\n# Basic AIM :\n\n(**from the competition description**)\n\nTo develop an automated method to map clinical concepts/features from an exam rubric (e.g., “diminished appetite”) to various ways in which these concepts are expressed in candidate's notes (e.g., “eating less,” “clothes fit looser”). Great solutions will be both accurate and reliable. The model must be able to find useful information that is pertaining to a feature & ignore all noisy aspects.","metadata":{}},{"cell_type":"code","source":"# Basic imports\nimport re\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pylab import rcParams\nimport ast\nfrom wordcloud import WordCloud, STOPWORDS\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:40.168828Z","iopub.execute_input":"2022-02-20T07:25:40.169045Z","iopub.status.idle":"2022-02-20T07:25:41.117426Z","shell.execute_reply.started":"2022-02-20T07:25:40.169022Z","shell.execute_reply":"2022-02-20T07:25:41.116645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Paths & Constants**","metadata":{}},{"cell_type":"code","source":"data_dir = \"../input/nbme-score-clinical-patient-notes/\"\n\n# For code reporducability\nseed = 100\nrandom.seed(seed)\nnp.random.seed(seed)\n\n# Plotting constants\nrcParams['figure.figsize'] = 7,4\nplt.rcParams['axes.grid'] = False\nplt.style.use('seaborn')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:41.118969Z","iopub.execute_input":"2022-02-20T07:25:41.119187Z","iopub.status.idle":"2022-02-20T07:25:41.124298Z","shell.execute_reply.started":"2022-02-20T07:25:41.119156Z","shell.execute_reply":"2022-02-20T07:25:41.123497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Facts about the dataset\n\n1. **Patient Note** : After conversation with an actor/test patient, the candidate documents all relevant facts of the encounter. ( Typical note making we observe whenever we go to any doctor). The notes contain our complains, historical facts or maybe the results of some tests taken before.\n\n2. **Feature** : The item which each trained physician/examiner is trying to search in every candiate's patient note. A clinically relevant concept. A rubric of concepts describes the key features relevant to each case. So each case in this data has some unique features which are essential for correct diagnosis. The goal for every candidate is to capture most of these features in their notes.\n\n3. **Clinical Case** : The scenario (e.g., symptoms, complaints, concerns) the actor presents to the candidate. 10 cases are represented in this dataset.\n\n\n## Items in the dataset :\n\n- **patient_notes.csv** : \n    - 40k documents containing a portion of patient notes. ( Ofcourse portion beacuse every detail cannot be revealed ! ). \n    - Only a subset of these documents have features annotated in them. \n    - These notes could be used for unsupervised learning. Just to understand the expected medical lingo in testing scenario \n    - Columns in this data :  \n        1. **pn_num**     : Unique ID of the patient.\n        2. **case_num**   : Unique ID of the case.\n        3. **pn_history** : The note containing all information about the interaction.\n\n\n- **features.csv** : \n    - The rubric that contains features to be searched by trained doctors for every clinical case. \n    - Columns in this file : \n        1. **feature_num**  : Unique id of the feature/clinical concept. \n        2. **case_num**     : Unique ID of the case.\n        3. **feature_text** : Text describing the feature in detail.\n\n- **train.csv** : \n    - Feature annotations for only 1000 of the patient notes, 100 for each of the ten clinical cases is available.\n    - Columns in this file : \n        1. **id**          : a unqiue combination of patient-feature id.\n        2. **pn_num**      : Unique ID of the patient.\n        3. **feature_num** : Unique id of the feature/clinical concept.\n        4. **case_num**    : Unique ID of the case.\n        5. **annotation**  : Part of text indicating the feature in patient note. Also a note can contain repitions of one feature.\n        6. **location**    : In terms of character index, it specifies the feature.","metadata":{}},{"cell_type":"markdown","source":"# Focus & Intutions from Patient Notes\n\n**Facts Gained** : \n\n1. For every unique patient there is only one note. Therefore the data has style & little bit of fact variations but no missing entries. (**All the entries in this csv are unique therefore we should not care about who the patient was or who the candidate doctor was.**)\n\n\n2. Each actor/test patient would be given the true scenario with detailed information about a specific case & its clinical concepts/features ( **symptoms which actor will have to potray using his/her skills**), but the candidate has no clue about this. \n    - Thus each entry for a specific case in this csv would be an estimate of the ground reality (**complete information given to all actors potraying a specific case**) based on what the candidate doctor understands & how well the actor/test patient performs in front of them (** lol, if the actor forgets a fact how will candidate mention that in their note.**) \n    - Every note belonging to a particular case, irrespective of patient num would share quite a lot of similarities because the ground facts of the specific case are fixed. \n    - Also the same 10 clinical cases will be asked in the test set.\n\n\n3. We don't need to know which candidate wrote which note, but clearly since the data was taken from an exam each candidate would have their own style of writing notes. \n    - Some would prefer mentioning facts in bullet points.\n    - Some would prefer paragraphs.\n    - Some would use a lot of short forms (**MLH, FH ...**).\n    - Some would give emphasis on use of correct punctuations.\n    - Some would give weightage to certain features more. (**mind develops biases in mysterious ways !**)\n    \n    \n4. The beauty about this problem statement is that the set of features i.e. **clinical concepts** to be searched in each note, when a case is given, is fixed. It is the **context/patient notes** in which we have to search a specific set of features, is the item that keeps on changing. This is due to the fact that each candidate has their own version of seeing the ground truth & may see different actor/test patient who could portay the whole case in a slighlty different way.","metadata":{}},{"cell_type":"code","source":"pat_notes_df = pd.read_csv(data_dir + \"patient_notes.csv\")\n\nprint(\"Number of patients    : \",pat_notes_df[\"pn_num\"].nunique())\nprint(\"Total number of notes : \",len(pat_notes_df))\nprint(\"Total number of cases : \",pat_notes_df[\"case_num\"].nunique())\n\npat_notes_df[\"text_len\"] = pat_notes_df[\"pn_history\"].map(lambda x : len(x))\nprint(\"Average note length (number of characters)   : \",pat_notes_df[\"text_len\"].mean())\nprint(\"Min text length : %d | Max text length : %d\"%(pat_notes_df[\"text_len\"].min(),pat_notes_df[\"text_len\"].max()))\nprint(\"Number of missing entries : \",pat_notes_df[\"pn_history\"].isna().sum())\n\n# Print patient notes....\npat_notes_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:41.125421Z","iopub.execute_input":"2022-02-20T07:25:41.12577Z","iopub.status.idle":"2022-02-20T07:25:41.882106Z","shell.execute_reply.started":"2022-02-20T07:25:41.125746Z","shell.execute_reply":"2022-02-20T07:25:41.881389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Notes Distribution for each case**\n\n- The labelled dataset is balanced (**100 annotations for each case**) but the total notes available is clearly skewed.\n- One has to be careful if they are trying to levarage unsupervised learning because the underlying skewed distribtuion could lead the model astray by making it learn more of the dominant case features/terminology, even though we perform a case wise independent feature search !","metadata":{}},{"cell_type":"code","source":"# Notes distribution per case\ncase_to_notes_len = {grp.iloc[0][\"case_num\"] : len(grp) for tmp, grp in pat_notes_df.groupby('case_num')}\nplt.bar(case_to_notes_len.keys(), case_to_notes_len.values(), 1, color='r',edgecolor = \"black\")\nplt.ylabel(\"Number of notes\")\nplt.yticks(np.arange(0,10000,500))\nplt.xlabel(\"Clinical case number\")\nplt.xticks(np.arange(0,10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:41.8839Z","iopub.execute_input":"2022-02-20T07:25:41.884172Z","iopub.status.idle":"2022-02-20T07:25:42.091507Z","shell.execute_reply.started":"2022-02-20T07:25:41.884145Z","shell.execute_reply":"2022-02-20T07:25:42.090799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Popular Words in each Clinical Case**\n\n 1. The goal of these word clouds generated by merging all notes case wise, is to get an idea about the most common words found in each note. PS. since in the test setup we will be given the case num & thus we can perform search on each set of case features independently.\n \n 2. This also gives us an idea about the ground truth because if all the notes capture a term/token it must have some weightage (since every candidate found it) which is captured by the size of the term in world cloud.\n \n 3. But this strategy has the problem of attending to only a specific word frequency therefore it would not be able to give us a true idea about tokens/words that have similar/same semantic meaning or became slightly different due to spelling mistake or short form writing.","metadata":{}},{"cell_type":"code","source":"word_cloud_list = []\nfor t, group in tqdm(pat_notes_df.groupby('case_num'), position = 0, leave = True):\n    combine_all_group_text = \"\"\n    for i in range(len(group)):\n        combine_all_group_text+=group.iloc[i][\"pn_history\"].lower() + \" \"\n    \n    word_cloud_list.append(WordCloud(width = 800, height = 800,\n                                     background_color ='black',\n                                     stopwords = set(STOPWORDS),\n                                     min_font_size = 10).generate(combine_all_group_text))\n    \nfig, ax = plt.subplots(nrows=2, ncols=5, figsize=(27, 12))\nfig.subplots_adjust(hspace=0.05, wspace = 0.03)\nctr = 0\nfor r in range(2):\n    for c in range(5):\n        ax[r,c].imshow(word_cloud_list[ctr])\n        ax[r,c].title.set_text('Case {}'.format(ctr))\n        ax[r,c].axis(\"off\")\n        ctr+=1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:25:42.092514Z","iopub.execute_input":"2022-02-20T07:25:42.093166Z","iopub.status.idle":"2022-02-20T07:26:19.92771Z","shell.execute_reply.started":"2022-02-20T07:25:42.093129Z","shell.execute_reply":"2022-02-20T07:26:19.925517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Text Length Distribution**\n- Most of the notes are long in nature (**this could be due to newline i.e. bullet point style or use of special characters**).\n\n- In this case while preprocessing we need to be careful because the annotations provided in train_df would be w.r.t. to the original uncleaned text.\n\n- So this plot also gives us an idea that usually we have complete information available & less cases of abrupt end of data due to faulty data collection.\n\n- Also I don't feel space based token count histograms would be beneficial because subword tokenisation schemes in today's transformers break the words in their own ways, thus character count gives us an upper bound estimate of the maximum length scenario.","metadata":{}},{"cell_type":"code","source":"# note length distribution original\nsns.displot(pat_notes_df[\"text_len\"], kde = True, height = 6, aspect = 2)\nplt.xticks(np.arange(0, 1000,50))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:26:19.929073Z","iopub.execute_input":"2022-02-20T07:26:19.929304Z","iopub.status.idle":"2022-02-20T07:26:20.566585Z","shell.execute_reply.started":"2022-02-20T07:26:19.929277Z","shell.execute_reply":"2022-02-20T07:26:20.56571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Case wise text length distribution** :\n\n- As we have seen the number of notes for each case is different therefore the sample size of each box & violin plots is different, therefore different plot comparison is futile.\n\n- The median size of all case notes is generally between 800 to 1000 characters.\n\n- Most of them have similar size.\n\n- The box plot contains few outliers but if we plan to use supervised learning we should see the other plots.\n\n**Labelled Case** :\n- The notes that are labelled have far fewer outliers when observed case wise. (Data is quite clean during training)\n\n- The distribution is more uniform therefore more variation in terms of length observed for a specific case's note.\n\n- Also comparison across violin & box plots for this data makes sense because the sample size is same i.e. 100 annotations for each case, balanced training data.\n\n**Note**\n\nThe whisker length on the top is not same as the bottom because it extends only till the maximum data point between Q3 & Q3+ 1.5\\*IQR in this dataset for each clinical case there are no exceptionally set of text but surely there are small text when compared to the median or the mean !","metadata":{}},{"cell_type":"code","source":"# case wise length distribution\nprint(\"\")\nplt.figure(figsize=(17,10))\nplt.suptitle('FULL DATA')\nplt.subplot(211)\nsns.violinplot(x=\"case_num\", y=\"text_len\", data = pat_notes_df)\n\nplt.subplot(212)\nsns.boxplot(x=\"case_num\", y=\"text_len\", data = pat_notes_df)\nplt.show()\n\n# case wise length distribution\ntmp_df = pd.read_csv(data_dir + \"train.csv\")\nlabelled_patient_cases = tmp_df[\"pn_num\"].unique()\npat_notes_df[\"label_status\"] = pat_notes_df[\"pn_num\"].apply(lambda x: True if x in labelled_patient_cases else False)\n\nplt.figure(figsize=(17,10))\nplt.suptitle('SUPERVISED DATA')\nplt.subplot(211)\nsns.violinplot(x=\"case_num\", y=\"text_len\", data = pat_notes_df[pat_notes_df[\"label_status\"] == True])\n\nplt.subplot(212)\nsns.boxplot(x=\"case_num\", y=\"text_len\", data = pat_notes_df[pat_notes_df[\"label_status\"] == True])\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-02-20T07:26:20.567657Z","iopub.execute_input":"2022-02-20T07:26:20.56839Z","iopub.status.idle":"2022-02-20T07:26:21.913553Z","shell.execute_reply.started":"2022-02-20T07:26:20.568364Z","shell.execute_reply":"2022-02-20T07:26:21.91254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As mentioned each candidate has a way of writing notes:**\n\n- One candiate prefers to write the patient's name while others only focus on gender & age.\n\n- Bullet Pointers for terms like PMH (Past medical history), FH (family history) etc. is commonly seen for this random sample (change the seed to explore different samples).\n\n- Some candidates prefer to use heavy medical lingo like \"burning epigastric pain\" etc.\n\n- As mentioned earlier since all the text entries in this sample belong to the same case therefore they share the ground truth facts. This clinical case involves a 35 year old male who has some form of stomach pain that burns & does not improve throughout the day.\n\nThe thing that makes the task so challenging for NLP-ML is that there is no single way of intepreting the actor's clinical case/concepts/features correctly. We ought to build a robust model which has the ability to search for each clinical case specific features if they are present in any context.","metadata":{}},{"cell_type":"code","source":"# for reproducability & my for my text box to make sense\nrandom.seed(15)\nfew_samples = 4\npick_case_num = random.sample(range(pat_notes_df[\"case_num\"].nunique()),1)[0]\n# Visualise few samples from a specific case ....\nprint(\"All patient notes belong to the same case ..\")\nprint(\"Case number : %d\"%pick_case_num)\nprint(\"----------------------------\")\n\nsub_df = pat_notes_df[pat_notes_df[\"case_num\"] == pick_case_num]\nrandom_samples = random.sample(range(len(sub_df)), few_samples)\n\nfor cnt,i in enumerate(random_samples):\n    print(\"Text : \",cnt)\n    print(\"----------------------------\")\n    print(sub_df.iloc[i][\"pn_history\"])\n    print(\"----------------------------\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:26:21.91453Z","iopub.execute_input":"2022-02-20T07:26:21.914701Z","iopub.status.idle":"2022-02-20T07:26:21.929217Z","shell.execute_reply.started":"2022-02-20T07:26:21.914677Z","shell.execute_reply":"2022-02-20T07:26:21.92792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Focus & Intutions from features\n\nThis is the exam rubric which is only available to the expert physician & not to the candidate appearing for the exam. Thus this is the set of ground truth labels for each of the 10 cases which is going to stay constant throughout this whole problem statement even in the test setup. \n\nOur goal is to pick a set of features for a clinical case ( which we recieve in the test setup ) & search them in the candidate's patient note & find it anyhow if it is present in that note. \n\nThe challenge is that each feature:\n   - It can be written multiple times. ( Detecting the location of every such excerpt )\n   \n   - In many different ways. ( Plethora of ways to write the same sentence )\n   \n   - Non contigous pieces of text could semantically mean a feature/clinical concept. ( Joining the chunks to make a feature )","metadata":{}},{"cell_type":"code","source":"features_df  = pd.read_csv(data_dir + \"features.csv\")\n\nprint(\"Number of unique clinical cases : \",features_df[\"case_num\"].nunique())\nprint(\"Number of unique features : \",features_df[\"feature_num\"].nunique())\nfeatures_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:26:21.930627Z","iopub.execute_input":"2022-02-20T07:26:21.93084Z","iopub.status.idle":"2022-02-20T07:26:21.953137Z","shell.execute_reply.started":"2022-02-20T07:26:21.930812Z","shell.execute_reply":"2022-02-20T07:26:21.952722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Case Feature distribution**\n\n- This plot is just to get a feel for the number of rows for each patient case in the train.csv because labeling has been done in such a way that for each labelled patient note we have mentioned every feature, if present then all the character index (start/stop location) chunks are specified & if absent then that feature is mentioned in the csv but the character index is blank.\n\n- There are atleast 9 features for each case in this problem statement.","metadata":{}},{"cell_type":"code","source":"# exploring the number of features in each clinical case\ncase_to_features_cnt = {}\nfor t, case_fet in features_df.groupby('case_num'):\n    case_to_features_cnt[case_fet[\"case_num\"].iloc[0]] = case_fet[\"feature_num\"].nunique()\n\nplt.bar(case_to_features_cnt.keys(), case_to_features_cnt.values(), 1, color='r',edgecolor = \"black\")\nplt.xticks(np.arange(0, 10))\nplt.yticks(np.arange(0,19))\nplt.ylabel(\"Number of features\")\nplt.xlabel(\"Clinical case number\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:26:21.954522Z","iopub.execute_input":"2022-02-20T07:26:21.954679Z","iopub.status.idle":"2022-02-20T07:26:22.143717Z","shell.execute_reply.started":"2022-02-20T07:26:21.954659Z","shell.execute_reply":"2022-02-20T07:26:22.143033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just a plot to get the feel for label size. (not important)","metadata":{}},{"cell_type":"code","source":"# Word token distribution in features \ncleaned_features = features_df[\"feature_text\"].map(lambda x : x.replace(\"-\", \" \"))\nsns.displot([len(i.split()) for i in cleaned_features])","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:26:22.144641Z","iopub.execute_input":"2022-02-20T07:26:22.144923Z","iopub.status.idle":"2022-02-20T07:26:22.370872Z","shell.execute_reply.started":"2022-02-20T07:26:22.144886Z","shell.execute_reply":"2022-02-20T07:26:22.370379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Since we get the case num in the test setup therefore each case's feature are searched independently but there are some features repeated across the cases. This should cause no trouble while building the model.\n\n2. Also no feature is repeated in each case therefore no problem with training data.","metadata":{}},{"cell_type":"code","source":"# just to check whether any feature/concept repeats or not in a particular case\nrep_cnt_grp_wise = 0\ntmp = features_df['feature_text'].map(lambda x: x.strip().lower())\nfor t, grp in features_df.groupby('case_num'):\n    tmp_grp = grp['feature_text'].map(lambda x: x.strip().lower())\n    rep_cnt_grp_wise += len(set(tmp_grp[tmp_grp.duplicated()]))\nprint(\"Number of features repeated in a group : \",rep_cnt_grp_wise)\n\n# This is not a problem becasue once a clinical case is given we don't need to worry about the other features\nprint(\"Number of unqiue features overall : \",tmp.nunique())\nprint(\"Duplicate features are : \")\nset(tmp[tmp.duplicated()])","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-02-20T07:26:22.371694Z","iopub.execute_input":"2022-02-20T07:26:22.37263Z","iopub.status.idle":"2022-02-20T07:26:22.388941Z","shell.execute_reply.started":"2022-02-20T07:26:22.372562Z","shell.execute_reply":"2022-02-20T07:26:22.388292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Focus & Intutions from train file\n\n\nAs mentioned in the data section we have 100 annotations for each of the 10 clinical cases therefore we have a total of 1000 labels for our overall problem statement. **While splitting the data for train & validation keep in mind to use \"*case_num*\" for stratified splits so that we have equal represenatations for both scenarios.**\n\nAfter observing the data the 2 most tricky aspects observed are :\n1. Multiple spans of the same feature in the patient note. The index for this case is specified as feature_1 -> [(start_1, end_1),(start_2, end_2),(start_3, end_3)]. Simple start stop indicies to show multiple excerpts.\n\n2. Tring to link different, non contiguous spans to map to a feature. The index for this case is specified as feature_1 -> [(start_1_1, end_1_1 ; start_1_2, end_1_2) ....]. Use of semicolons to specify disjoint chunks to map to a feature.","metadata":{}},{"cell_type":"code","source":"# if len of any tuple is >2 then it is part of one seperated text in (start_1,end_1,start_2,end_2...) format \ndef order_func(x):\n    coll_list = []\n    for i in x:\n        i = i.strip()\n        if \";\" in i:\n            tmp = []\n            set_list = i.split(\";\")\n            for j in set_list:\n                j = j.strip()\n                tmp.extend((int(j.split()[0]), int(j.split()[-1])))\n            coll_list.append(tuple(tmp))\n        else:\n            coll_list.append((int(i.split()[0]), int(i.split()[-1])))\n    \n    return coll_list","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:26:22.389948Z","iopub.execute_input":"2022-02-20T07:26:22.390102Z","iopub.status.idle":"2022-02-20T07:26:22.402916Z","shell.execute_reply.started":"2022-02-20T07:26:22.390082Z","shell.execute_reply":"2022-02-20T07:26:22.401839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- A helper function is used to read the list of indices for multiple repitions & multiple chunks case as tuples of start & end. \n\n- Also the each features text description is added to the annotation csv to get a true idea about the feature to search for.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(data_dir + \"train.csv\")\nprint(\"Number of patients for which we have annotations : \",len(train_df.groupby('pn_num')))\n\n# also for every patient we exactly have the same number of entries as there are features belonging to that case.\nmismatch = 0\nfor t, grp in train_df.groupby('pn_num'):\n    if len(grp) != case_to_features_cnt[grp.iloc[0][\"case_num\"]]:\n        mismatch+=1\n        \nprint(\"Number of patients with missing features         :\",mismatch)\n# since the data is present in the df as string list\ntrain_df[\"location\"] = (train_df[\"location\"].map(lambda x : ast.literal_eval(x))).map(lambda x : order_func(x))\ntrain_df[\"annotation\"] = train_df[\"annotation\"].map(lambda x : ast.literal_eval(x))\ntrain_df[\"feature_text\"] = train_df[\"feature_num\"].map(lambda x : features_df[features_df[\"feature_num\"] == x][\"feature_text\"].iloc[0])\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:26:22.403968Z","iopub.execute_input":"2022-02-20T07:26:22.404158Z","iopub.status.idle":"2022-02-20T07:26:27.578898Z","shell.execute_reply.started":"2022-02-20T07:26:22.404136Z","shell.execute_reply":"2022-02-20T07:26:27.577786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Case-wise feature repitition distribution**\n\nThe goal of these set of plots is to get an idea about a single feature repition in our data.\n\n1. For almost all the cases there is clearly a skew in feature presence. It could be that understadning that feature was quite hard & only the best candidates could see that.\n2. For case 2,4,8,9 there is 1 feature whose presence in the labels is extremely small.","metadata":{}},{"cell_type":"code","source":"# case wise plot\n\nfig, ax = plt.subplots(nrows=10, ncols=1, figsize=(18, 70))\nfig.subplots_adjust(hspace=0.2)\ni = 0\nfor t, grp in train_df.groupby('case_num'):\n    feature_to_count = {}\n    feature_num_to_text = {}\n    for t_j, fet_grp in grp.groupby('feature_num'):\n        feature_to_count[fet_grp.iloc[0]['feature_num']] = 0\n        if len(fet_grp.iloc[0]['feature_text'].split(\"-\")) > 5:\n            feature_num_to_text[fet_grp.iloc[0]['feature_num']] = \"-\".join(fet_grp.iloc[0]['feature_text'].split(\"-\")[0:5])\n        else:\n            feature_num_to_text[fet_grp.iloc[0]['feature_num']] = fet_grp.iloc[0]['feature_text']\n        for ctr in range(len(fet_grp)):\n            feature_to_count[fet_grp.iloc[0]['feature_num']] += len(fet_grp.iloc[ctr]['annotation'])\n    \n\n    ax[i].set_title('Case {}'.format(grp.iloc[0]['case_num']), fontweight =\"bold\")\n    ax[i].bar(feature_to_count.keys(), feature_to_count.values(), 1, color='r',edgecolor = \"black\")\n    ax[i].set_ylabel(\"Label Count\")\n    ax[i].set_xticklabels([])\n    \n    for index, value in enumerate(feature_to_count):\n        ax[i].text(value, index,str(feature_num_to_text[value]) + \" = ( \" + str(feature_to_count[value]) + \" )\", rotation = 90)\n \n    i+=1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:54:31.436141Z","iopub.execute_input":"2022-02-20T07:54:31.436412Z","iopub.status.idle":"2022-02-20T07:54:38.711705Z","shell.execute_reply.started":"2022-02-20T07:54:31.436384Z","shell.execute_reply":"2022-02-20T07:54:38.71065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scatter plot to get an idea about the ","metadata":{}},{"cell_type":"markdown","source":"To get a feel for full text vs the present & absent features.","metadata":{}},{"cell_type":"code","source":"# Random Viz\nrandom.seed(40)\nunq_patients = random.sample(list(train_df[\"pn_num\"].unique()),1)[0]\npatient_note = pat_notes_df[pat_notes_df[\"pn_num\"] == unq_patients][\"pn_history\"].iloc[0]\nprint(\"Case number :\",pat_notes_df[pat_notes_df[\"pn_num\"] == unq_patients][\"case_num\"].iloc[0])\n\nprint(\"--------------------------------------\")\nprint(\"Original Text ........\")\nprint(patient_note)\nprint(\"--------------------------------------\")\n\nprint(\"Labelled features ...... {feature : text}\")\ntmp_df = train_df[train_df[\"pn_num\"] == unq_patients]\nfor i in range(len(tmp_df)):\n    location_list = tmp_df.iloc[i][\"location\"]\n    print(\"--------------------------------------\")\n    print(\"Feature : \",tmp_df.iloc[i][\"feature_text\"])\n    if len(location_list) == 0:\n        print(\"Value   :  Not found\")\n        \n    for j in location_list:\n        tmp = \"\"\n        if len(j) > 2:\n            for k in range(0,len(j),2):\n                tmp+= patient_note[j[k]:j[k+1]] + \" + \"\n        else:\n            tmp+= patient_note[j[0]:j[1]]\n        print(\"Value   :  %s\"%(tmp))\n    print(\"--------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:00:49.049667Z","iopub.execute_input":"2022-02-20T06:00:49.049932Z","iopub.status.idle":"2022-02-20T06:00:49.0776Z","shell.execute_reply.started":"2022-02-20T06:00:49.049905Z","shell.execute_reply":"2022-02-20T06:00:49.076709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[train_df[\"pn_num\"] == unq_patients]","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:00:53.751346Z","iopub.execute_input":"2022-02-20T06:00:53.751659Z","iopub.status.idle":"2022-02-20T06:00:53.777067Z","shell.execute_reply.started":"2022-02-20T06:00:53.75163Z","shell.execute_reply":"2022-02-20T06:00:53.776168Z"},"trusted":true},"execution_count":null,"outputs":[]}]}