{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### This noteboos is based on:\n-  [Roberta Strikes Back !](https://www.kaggle.com/code/theoviel/roberta-strikes-back) \n-  [Pytorch Bert baseline to RoBERTa NBME\n](https://www.kaggle.com/code/iamsdt/pytorch-bert-baseline-to-roberta-nbme)\n-  [【Train】Deberta-v3-large baseline\n](https://www.kaggle.com/code/librauee/train-deberta-v3-large-baseline)\n\n\n##### Apply trained model from this book can get 0.876 LB. I know in Roberta Strikes Back ! the author use Roberta-large get 0.882, hope someone can use it to get higher LB.","metadata":{}},{"cell_type":"markdown","source":"# Library","metadata":{"papermill":{"duration":0.016162,"end_time":"2021-11-16T19:32:40.221507","exception":false,"start_time":"2021-11-16T19:32:40.205345","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn import metrics, model_selection\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"papermill":{"duration":30.77583,"end_time":"2021-11-16T19:33:11.013554","exception":false,"start_time":"2021-11-16T19:32:40.237724","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-10T06:17:34.418622Z","iopub.execute_input":"2022-04-10T06:17:34.419132Z","iopub.status.idle":"2022-04-10T06:17:41.813003Z","shell.execute_reply.started":"2022-04-10T06:17:34.419049Z","shell.execute_reply":"2022-04-10T06:17:41.811529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"config = dict(\n    seed = 2020,\n    num_labels=2,\n    num_folds=1,\n    fold_to_train = [0],\n    device = 'cuda' if torch.cuda.is_available() else 'cpu',\n    model_checkpoint = 'roberta-large', \n    learning_rate = 1e-5,\n    weight_decay = 1e-2,\n    max_length = 512,\n    train_batch_size = 4,\n    valid_batch_size = 8,\n    epochs_to_train = 1,\n    total_epochs = 1,\n    grad_acc_steps = 4,\n    num_cycles=0.5,\n    scheduler='linear', # ['linear', 'cosine']\n    output_dir = '',\n    debug = None,\n    precompute_tokens = True\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:17:41.814897Z","iopub.execute_input":"2022-04-10T06:17:41.815171Z","iopub.status.idle":"2022-04-10T06:17:41.823813Z","shell.execute_reply.started":"2022-04-10T06:17:41.815135Z","shell.execute_reply":"2022-04-10T06:17:41.821035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{"papermill":{"duration":0.018406,"end_time":"2021-11-16T19:33:11.150174","exception":false,"start_time":"2021-11-16T19:33:11.131768","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\ntrain = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')\ntrain['annotation'] = train['annotation'].apply(ast.literal_eval)\ntrain['location'] = train['location'].apply(ast.literal_eval)\nfeatures = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\nfeatures = preprocess_features(features)\npatient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n\nprint(f\"train.shape: {train.shape}\")\ndisplay(train.head())\nprint(f\"features.shape: {features.shape}\")\ndisplay(features.head())\nprint(f\"patient_notes.shape: {patient_notes.shape}\")\ndisplay(patient_notes.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:17:41.824772Z","iopub.execute_input":"2022-04-10T06:17:41.825018Z","iopub.status.idle":"2022-04-10T06:17:42.9155Z","shell.execute_reply.started":"2022-04-10T06:17:41.824965Z","shell.execute_reply":"2022-04-10T06:17:42.914693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_feature_text(text):\n    text = re.sub('I-year', '1-year', text)\n    text = re.sub('-OR-', \" or \", text)\n    text = re.sub('-', ' ', text)\n    return text\n\n\ndef clean_spaces(txt):\n    txt = re.sub('\\n', ' ', txt)\n    txt = re.sub('\\t', ' ', txt)\n    txt = re.sub('\\r', ' ', txt)\n#     txt = re.sub(r'\\s+', ' ', txt)\n    return txt","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:17:42.917499Z","iopub.execute_input":"2022-04-10T06:17:42.917809Z","iopub.status.idle":"2022-04-10T06:17:42.925545Z","shell.execute_reply.started":"2022-04-10T06:17:42.917773Z","shell.execute_reply":"2022-04-10T06:17:42.924711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.merge(features, on=['feature_num', 'case_num'], how='left')\ntrain = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\ntrain['pn_history'] = train['pn_history'].apply(lambda x: x.strip())\ntrain['feature_text'] = train['feature_text'].apply(process_feature_text)\n\ntrain['feature_text'] = train['feature_text'].apply(clean_spaces)\ntrain['clean_text'] = train['pn_history'].apply(clean_spaces)\ndisplay(train.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:17:42.926917Z","iopub.execute_input":"2022-04-10T06:17:42.927473Z","iopub.status.idle":"2022-04-10T06:17:43.167409Z","shell.execute_reply.started":"2022-04-10T06:17:42.927436Z","shell.execute_reply":"2022-04-10T06:17:43.166578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# incorrect annotation\ntrain.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\ntrain.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n\ntrain.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\ntrain.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n\ntrain.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\ntrain.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n\ntrain.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\ntrain.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n\ntrain.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\ntrain.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n\ntrain.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\ntrain.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n\ntrain.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\ntrain.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n\ntrain.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\ntrain.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n\ntrain.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\ntrain.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n\ntrain.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\ntrain.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n\ntrain.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\ntrain.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n\ntrain.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\ntrain.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n\ntrain.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\ntrain.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n\ntrain.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\ntrain.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n\ntrain.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\ntrain.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n\ntrain.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\ntrain.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n\ntrain.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\ntrain.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n\ntrain.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\ntrain.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n\ntrain.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\ntrain.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n\ntrain.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\ntrain.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n\ntrain.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\ntrain.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n\ntrain.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\ntrain.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n\ntrain.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\ntrain.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n\ntrain.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\ntrain.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n\ntrain.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\ntrain.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n\ntrain.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\ntrain.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n\ntrain.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\ntrain.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n\ntrain.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\ntrain.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n\ntrain.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\ntrain.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n\ntrain.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\ntrain.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n\ntrain.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\ntrain.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n\ntrain.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\ntrain.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n\ntrain.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\ntrain.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n\ntrain.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\ntrain.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n\ntrain.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\ntrain.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n\ntrain.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\ntrain.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n\ntrain.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\ntrain.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n\ntrain.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\ntrain.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n\ntrain.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\ntrain.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n\ntrain.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\ntrain.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n\ntrain.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\ntrain.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n\ntrain.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\ntrain.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:17:43.169084Z","iopub.execute_input":"2022-04-10T06:17:43.169336Z","iopub.status.idle":"2022-04-10T06:17:43.264675Z","shell.execute_reply.started":"2022-04-10T06:17:43.169302Z","shell.execute_reply":"2022-04-10T06:17:43.26402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['annotation_length'] = train['annotation'].apply(len)\ndisplay(train['annotation_length'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:17:43.26628Z","iopub.execute_input":"2022-04-10T06:17:43.266499Z","iopub.status.idle":"2022-04-10T06:17:43.282459Z","shell.execute_reply.started":"2022-04-10T06:17:43.266474Z","shell.execute_reply":"2022-04-10T06:17:43.281725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CV split","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CV split\n# ====================================================\nFold = GroupKFold(n_splits=5)\ngroups = train['pn_num'].values\nfor n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n    train.loc[val_index, 'kfold'] = int(n)\ntrain['kfold'] = train['kfold'].astype(int)\ndisplay(train.groupby('kfold').size())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:17:43.283703Z","iopub.execute_input":"2022-04-10T06:17:43.284589Z","iopub.status.idle":"2022-04-10T06:17:43.306423Z","shell.execute_reply.started":"2022-04-10T06:17:43.284552Z","shell.execute_reply":"2022-04-10T06:17:43.305664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom transformers import AutoTokenizer\n\n\ndef get_tokenizer(name, precompute=False, df=None, folder=None):\n    if folder is None:\n        tokenizer = AutoTokenizer.from_pretrained(name)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(folder)\n\n    tokenizer.name = name\n    tokenizer.special_tokens = {\n        \"sep\": tokenizer.sep_token_id,\n        \"cls\": tokenizer.cls_token_id,\n        \"pad\": tokenizer.pad_token_id,\n    }\n\n    if precompute:\n        tokenizer.precomputed = precompute_tokens(df, tokenizer)\n    else:\n        tokenizer.precomputed = None\n\n    return tokenizer\n\n\ndef precompute_tokens(df, tokenizer):\n    feature_texts = df[\"feature_text\"].unique()\n\n    ids = {}\n    offsets = {}\n\n    for feature_text in feature_texts:\n        encoding = tokenizer(\n            feature_text,\n            return_token_type_ids=True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        ids[feature_text] = encoding[\"input_ids\"]\n        offsets[feature_text] = encoding[\"offset_mapping\"]\n\n    texts = df[\"clean_text\"].unique()\n\n    for text in texts:\n        encoding = tokenizer(\n            text,\n            return_token_type_ids=True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        ids[text] = encoding[\"input_ids\"]\n        offsets[text] = encoding[\"offset_mapping\"]\n\n    return {\"ids\": ids, \"offsets\": offsets}","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:17:43.307668Z","iopub.execute_input":"2022-04-10T06:17:43.307965Z","iopub.status.idle":"2022-04-10T06:17:43.318178Z","shell.execute_reply.started":"2022-04-10T06:17:43.30793Z","shell.execute_reply":"2022-04-10T06:17:43.31743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\ndef tokenize_and_add_labels(tokenizer, example):\n    tokenized_inputs = tokenizer(\n        example[\"feature_text\"],\n        example[\"pn_history\"],\n        truncation=\"only_second\",\n        max_length=config['max_length'],\n        padding=\"max_length\",\n        return_offsets_mapping=True,\n        return_token_type_ids=True\n    )\n    labels = [0.0] * len(tokenized_inputs[\"input_ids\"])\n    tokenized_inputs[\"location_int\"] = loc_list_to_ints(example[\"location\"])\n    tokenized_inputs[\"sequence_ids\"] = tokenized_inputs.sequence_ids()\n    \n    for idx, (seq_id, offsets) in enumerate(zip(tokenized_inputs[\"sequence_ids\"], tokenized_inputs[\"offset_mapping\"])):\n        if seq_id is None or seq_id == 0:\n            labels[idx] = -100\n            continue\n        exit = False\n        token_start, token_end = offsets\n        for feature_start, feature_end in tokenized_inputs[\"location_int\"]:\n            if exit:\n                break\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                exit = True\n    tokenized_inputs[\"labels\"] = labels\n    \n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:17:43.321103Z","iopub.execute_input":"2022-04-10T06:17:43.322124Z","iopub.status.idle":"2022-04-10T06:17:43.333576Z","shell.execute_reply.started":"2022-04-10T06:17:43.322085Z","shell.execute_reply":"2022-04-10T06:17:43.332792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\nclass NBMEData(torch.utils.data.Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = tokenize_and_add_labels(self.tokenizer, example)\n        \n        input_ids = np.array(tokenized[\"input_ids\"])\n        attention_mask = np.array(tokenized[\"attention_mask\"])\n        token_type_ids = np.array(tokenized[\"token_type_ids\"])\n\n        labels = np.array(tokenized[\"labels\"])\n        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\")\n        \n        return {\n            'input_ids': input_ids, \n            'attention_mask': attention_mask,\n            'token_type_ids':token_type_ids,\n            'targets': labels, \n            'offset_mapping': offset_mapping, \n            'sequence_ids': sequence_ids,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:17:43.336536Z","iopub.execute_input":"2022-04-10T06:17:43.336791Z","iopub.status.idle":"2022-04-10T06:17:43.345599Z","shell.execute_reply.started":"2022-04-10T06:17:43.336767Z","shell.execute_reply":"2022-04-10T06:17:43.344922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"\nclass NBMEModel(nn.Module):\n    def __init__(self, num_labels, path=None):\n        super().__init__()\n                \n        self.path = path\n        self.num_labels = num_labels\n        self.config = transformers.AutoConfig.from_pretrained(config['model_checkpoint'],output_hidden_states=True)\n\n        self.model = transformers.AutoModel.from_pretrained(config['model_checkpoint'], config=self.config)\n        self.dropout = nn.Dropout(0.2)\n        self.output = nn.Linear(self.config.hidden_size, 1)\n        \n        if self.path is not None:\n            self.load_state_dict(torch.load(self.path)['model'])\n\n    def forward(self, data):\n        \n        ids = data['input_ids']\n        mask = data['attention_mask']\n        token_type_ids = data['token_type_ids']\n        \n        try:\n            target = data['targets']\n        except:\n            target = None\n\n        transformer_out = self.model(ids,mask,token_type_ids)\n        sequence_output = transformer_out[0]\n        sequence_output = self.dropout(sequence_output)\n        logits = self.output(sequence_output)\n\n        output = {\n            \"logits\": torch.sigmoid(logits),\n        }\n        \n        if target is not None:\n            loss = get_loss(logits, target)\n            output['loss'] = loss\n            output['targets'] = target\n\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:17:43.34695Z","iopub.execute_input":"2022-04-10T06:17:43.347471Z","iopub.status.idle":"2022-04-10T06:17:43.360083Z","shell.execute_reply.started":"2022-04-10T06:17:43.347413Z","shell.execute_reply":"2022-04-10T06:17:43.359194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helpler functions","metadata":{}},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.max = 0\n        self.min = 1e5\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        if val > self.max:\n            self.max = val\n        if val < self.min:\n            self.min = val\n            \ndef train_fn(model, train_loader, optimizer, scheduler, device, current_epoch):  \n    losses = AverageMeter()\n    optimizer.zero_grad()\n\n    with tqdm(train_loader, unit=\"batch\") as tepoch:\n        for batch_idx, data in enumerate(tepoch):\n            for k, v in data.items():\n                if k != 'offset_mapping':\n                    data[k] = v.to(config['device'])\n\n            model.train()\n            loss = model(data)['loss'] / config['grad_acc_steps']\n                \n            loss.backward()\n            losses.update(loss.item(), len(train_loader))\n            tepoch.set_postfix(train_loss=losses.avg)\n            \n            if batch_idx % config['grad_acc_steps'] == 0:\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad() \n            \n            \ndef eval_fn(model, valid_loader, device, current_epoch):\n    losses = AverageMeter()\n\n    final_targets = []\n    final_predictions = []\n    offset_mapping = []\n    sequence_ids = []\n\n    model.eval()\n    \n    with torch.no_grad():\n        \n        with tqdm(valid_loader, unit=\"batch\") as tepoch:\n\n            for batch_idx, data in enumerate(tepoch):\n                for k, v in data.items():\n                    if k not in  ['offset_mapping', 'sequence_id']:\n                        data[k] = v.to(config['device'])\n                \n                x = model(data)\n                loss = x['loss']\n                losses.update(loss.item(), len(valid_loader))\n\n                o = x['logits'].detach().cpu().numpy()\n                final_predictions.extend(o)\n                \n                y = data['targets'].detach().cpu().numpy()\n                final_targets.extend(y)\n                \n                offset_mapping.extend(data['offset_mapping'].tolist())\n                sequence_ids.extend(data['sequence_ids'].tolist())\n    \n    predicted_locations = decode_predictions(final_predictions, offset_mapping, sequence_ids, test=False)\n    scores = get_score(predicted_locations, offset_mapping, sequence_ids, final_targets)\n\n    return round(losses.avg, 4), round(scores['f1'], 4)\n\n\ndef decode_predictions(preds, offset_mapping, sequence_ids, test=False):\n    \n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        start_idx = None\n        current_preds = []\n        \n        for p, o, s_id in zip(pred, offsets, seq_ids):\n            if s_id is None or s_id == 0:\n                continue\n                \n            # if class = 1, track start and end location from offset map\n            if p > 0.5:\n                if start_idx is None:\n                    start_idx = o[0]\n                end_idx = o[1]\n            \n            # if class 0, record previously tracked predictions if not done already\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None # reset\n                \n        if test:\n            all_predictions.append(\"; \".join(current_preds)) # delimiting with semi-colon for submission requirement\n        else:\n            all_predictions.append(current_preds)\n            \n    return all_predictions\n\n\ndef get_score(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    \n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n        num_chars = max(list(itertools.chain(*offsets)))\n        char_labels = np.zeros((num_chars))\n        \n        # formatting ground truth for evaluation\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            # do nothing if sequence id is not 1\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n            \n        # formatting predictions for evaluation\n        char_preds = np.zeros((num_chars))\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n            \n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n        \n    results = metrics.precision_recall_fscore_support(all_labels, all_preds, average = \"binary\")\n    return {\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }\n\ndef save_checkpoint(model, optimizer, scheduler, epoch, score, best_score, name):\n    print('saving model of this epoch as:', name)\n    \n    name = config['output_dir'] + name\n    torch.save(\n        {\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n            'scheduler': scheduler.state_dict(),\n            'epoch': epoch,\n            'score': score,\n            'best_score': best_score,\n        },\n        name\n    )","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:17:43.361638Z","iopub.execute_input":"2022-04-10T06:17:43.362148Z","iopub.status.idle":"2022-04-10T06:17:43.392719Z","shell.execute_reply.started":"2022-04-10T06:17:43.362109Z","shell.execute_reply":"2022-04-10T06:17:43.392009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# scheduler\n# ====================================================\ndef get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n    if config[\"scheduler\"]=='linear':\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n        )\n    elif config[\"scheduler\"]=='cosine':\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps, num_cycles=config[\"num_cycles\"]\n        )\n    return scheduler\n\ndef make_optimizer(self,model, optimizer_name=\"AdamW\"):\n        kwargs = {\n                'lr':5e-5,\n                'weight_decay':0.01\n        }\n        \n        if optimizer_name == \"AdamW\":\n            optimizer = AdamW(self.parameters(),**kwargs)\n            return optimizer\n        else:\n            raise Exception('Unknown optimizer: {}'.format(optimizer_name))\n        \n\ndef get_loss(output, target):\n    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n    loss = loss_fn(output.view(-1, 1), target.view(-1, 1))\n    loss = torch.masked_select(loss, target.view(-1, 1) != -100).mean()\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:18:47.282428Z","iopub.execute_input":"2022-04-10T06:18:47.282693Z","iopub.status.idle":"2022-04-10T06:18:47.291225Z","shell.execute_reply.started":"2022-04-10T06:18:47.282665Z","shell.execute_reply":"2022-04-10T06:18:47.290478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run","metadata":{}},{"cell_type":"code","source":"def run(df, fold, tokenizer, device):\n\n    print('Fold:', fold)\n\n    print('\\npreparing training data...')\n    train_df = df[df['kfold'] != fold].reset_index(drop=True)\n    train_dataset = NBMEData(train_df, tokenizer)\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['train_batch_size'],\n        shuffle=True,\n    )\n    \n    print('\\npreparing validation data...')\n    valid_df = df[df['kfold'] == fold].reset_index(drop=True)\n    valid_dataset = NBMEData(valid_df, tokenizer)\n    valid_loader = DataLoader(\n        valid_dataset,\n        batch_size=config['valid_batch_size'],\n        shuffle=False,\n    )\n\n    model = NBMEModel(config['num_labels'])\n    torch.save(model.config, 'config.pth')\n    model.to(device)\n    \n    optimizer = make_optimizer(model, \"AdamW\")\n\n    num_training_steps = (len(train_dataset) // (config['train_batch_size'] * config['grad_acc_steps'])) * config['total_epochs']\n    num_warmup_steps = int(num_training_steps * 0.01)\n    scheduler = get_scheduler(optimizer, num_warmup_steps, num_training_steps)\n    config['num_training_steps'] =  num_training_steps\n    config['num_warmup_steps'] =  num_warmup_steps \n\n    epoch_start = 0\n    best_score = -1\n    start = time.time()\n\n    \n    for epoch in range(epoch_start, epoch_start + config['epochs_to_train']):   \n\n        print(f'\\n\\n\\nTraining Epoch: {epoch}')\n        train_fn(model, train_loader, optimizer, scheduler, device, epoch)\n        \n        print('Evaluation...')\n        val_loss, val_score = eval_fn(\n            model=model, \n            valid_loader=valid_loader, \n            device=device,\n            current_epoch=epoch,\n        )\n        \n        if val_score > best_score:\n            best_score = val_score\n\n            save_checkpoint(model, optimizer, scheduler, epoch, val_score, best_score, f'best_model_{config[\"model_checkpoint\"]}_{fold}.pth')\n\n        save_checkpoint(model, optimizer, scheduler, epoch, val_score, best_score, f'last_model_{config[\"model_checkpoint\"]}_{fold}.pth')\n\n        print('Valid Score:', val_score, 'Valid Loss:', val_loss, 'Best Score:', best_score)\n        \n    print(f'Best Score: {best_score}, Time Taken: {round(time.time() - start, 4)}s')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T06:18:47.843749Z","iopub.execute_input":"2022-04-10T06:18:47.845209Z","iopub.status.idle":"2022-04-10T06:18:47.865584Z","shell.execute_reply.started":"2022-04-10T06:18:47.845161Z","shell.execute_reply":"2022-04-10T06:18:47.863984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = get_tokenizer(\n    config['model_checkpoint'], precompute=config['precompute_tokens'], df=train, folder=None)\n\nif config['debug']:\n    train = train.sample(config['debug']).reset_index(drop=True)\n\nfor fold in config['fold_to_train']:\n    run(\n        df=train, \n        fold=fold,\n        tokenizer=tokenizer,\n        device=config['device']\n    )\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"papermill":{"duration":4899.627687,"end_time":"2021-11-16T20:55:00.880155","exception":false,"start_time":"2021-11-16T19:33:21.252468","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-10T06:18:48.711715Z","iopub.execute_input":"2022-04-10T06:18:48.712233Z","iopub.status.idle":"2022-04-10T06:23:48.303258Z","shell.execute_reply.started":"2022-04-10T06:18:48.712196Z","shell.execute_reply":"2022-04-10T06:23:48.301848Z"},"trusted":true},"execution_count":null,"outputs":[]}]}