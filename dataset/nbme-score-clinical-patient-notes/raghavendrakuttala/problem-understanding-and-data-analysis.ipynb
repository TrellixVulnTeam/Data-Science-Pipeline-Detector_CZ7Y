{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-11T11:49:23.79927Z","iopub.execute_input":"2022-04-11T11:49:23.799642Z","iopub.status.idle":"2022-04-11T11:49:23.831805Z","shell.execute_reply.started":"2022-04-11T11:49:23.799541Z","shell.execute_reply":"2022-04-11T11:49:23.831141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:49:23.837033Z","iopub.execute_input":"2022-04-11T11:49:23.837398Z","iopub.status.idle":"2022-04-11T11:49:25.762307Z","shell.execute_reply.started":"2022-04-11T11:49:23.837366Z","shell.execute_reply":"2022-04-11T11:49:25.7614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Problem\n> develop an automated method to map clinical concepts from an exam rubric (e.g., “diminished appetite”) to various ways in which these concepts are expressed in clinical patient notes written\n\nGiven an clinical patient note map out clinical concepts in it.\n\nLets try to understand data files given to get intuitive understanding of what the problem is. See if we can map input and output of the problem at the end of analysis\n\n\n* ### [Patient Notes](#section-one)\n* ### [Features](#section-two)\n* ### [Train](#section-three)\n* ### [Conclusion](#section-four)\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n### Patient notes\n\nThis is nothing but patient individual notes","metadata":{}},{"cell_type":"code","source":"df_patient = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:49:28.925309Z","iopub.execute_input":"2022-04-11T11:49:28.925734Z","iopub.status.idle":"2022-04-11T11:49:29.595552Z","shell.execute_reply.started":"2022-04-11T11:49:28.925703Z","shell.execute_reply":"2022-04-11T11:49:29.59469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_patient.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:49:34.125657Z","iopub.execute_input":"2022-04-11T11:49:34.125941Z","iopub.status.idle":"2022-04-11T11:49:34.145444Z","shell.execute_reply.started":"2022-04-11T11:49:34.125908Z","shell.execute_reply":"2022-04-11T11:49:34.144243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can see number of patients, case_nums\ndf_patient.pn_num.nunique(),df_patient.case_num.nunique(), df_patient.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:49:36.52171Z","iopub.execute_input":"2022-04-11T11:49:36.522216Z","iopub.status.idle":"2022-04-11T11:49:36.536327Z","shell.execute_reply.started":"2022-04-11T11:49:36.52218Z","shell.execute_reply":"2022-04-11T11:49:36.535044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_patient[df_patient['pn_num'].isin([1,0])]['pn_history'].values","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:49:47.0339Z","iopub.execute_input":"2022-04-11T11:49:47.034202Z","iopub.status.idle":"2022-04-11T11:49:47.045771Z","shell.execute_reply.started":"2022-04-11T11:49:47.034169Z","shell.execute_reply":"2022-04-11T11:49:47.045152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### we can see there are specific fields they are collecting for every patients like age, symptom, family history, previous treatment, its effects on the life of patient, usual routine (diet and excersize.. etc). In later analysis we will analyse these patterns.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n## Features\n\nLets understand what each case represents","metadata":{}},{"cell_type":"code","source":"df_features = pd.read_csv(\"/kaggle/input/nbme-score-clinical-patient-notes/features.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:50:01.12704Z","iopub.execute_input":"2022-04-11T11:50:01.12769Z","iopub.status.idle":"2022-04-11T11:50:01.13688Z","shell.execute_reply.started":"2022-04-11T11:50:01.127628Z","shell.execute_reply":"2022-04-11T11:50:01.13602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:50:02.440858Z","iopub.execute_input":"2022-04-11T11:50:02.441302Z","iopub.status.idle":"2022-04-11T11:50:02.451402Z","shell.execute_reply.started":"2022-04-11T11:50:02.441254Z","shell.execute_reply":"2022-04-11T11:50:02.450595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_features.shape, df_features.feature_num.nunique(), df_features.case_num.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:50:03.820951Z","iopub.execute_input":"2022-04-11T11:50:03.821241Z","iopub.status.idle":"2022-04-11T11:50:03.828917Z","shell.execute_reply.started":"2022-04-11T11:50:03.821212Z","shell.execute_reply":"2022-04-11T11:50:03.828344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As we can see, we have 10 cases and each case has some feature associated with it","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=df_features, x='case_num')","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:50:41.306864Z","iopub.execute_input":"2022-04-11T11:50:41.307194Z","iopub.status.idle":"2022-04-11T11:50:41.580812Z","shell.execute_reply.started":"2022-04-11T11:50:41.307151Z","shell.execute_reply":"2022-04-11T11:50:41.579964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lets see top words in each cases to see what kind of symptoms we are talking about\n\nWe use tfidf because we already know the no cases so : build Tfidf vectorizer and sort through the features based on their weight for each of the case","metadata":{}},{"cell_type":"code","source":"def clean(txt):\n    txt = txt.replace('-', ' ')\n    return txt.lower()\ndf_features['feature_text'] = df_features['feature_text'].apply(clean)\ndf_features['feature_text'][:10].values","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:50:46.607338Z","iopub.execute_input":"2022-04-11T11:50:46.609344Z","iopub.status.idle":"2022-04-11T11:50:46.61805Z","shell.execute_reply.started":"2022-04-11T11:50:46.609297Z","shell.execute_reply":"2022-04-11T11:50:46.617156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp_df = df_features.groupby('case_num')['feature_text'].agg(lambda x: ' '.join(x)).reset_index()\ntmp_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:50:48.360813Z","iopub.execute_input":"2022-04-11T11:50:48.361105Z","iopub.status.idle":"2022-04-11T11:50:48.3748Z","shell.execute_reply.started":"2022-04-11T11:50:48.361061Z","shell.execute_reply":"2022-04-11T11:50:48.374045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf = TfidfVectorizer(stop_words=stop_words)\ntf_vector = tfidf.fit_transform(tmp_df['feature_text'])","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:50:49.777143Z","iopub.execute_input":"2022-04-11T11:50:49.777551Z","iopub.status.idle":"2022-04-11T11:50:49.793745Z","shell.execute_reply.started":"2022-04-11T11:50:49.777513Z","shell.execute_reply":"2022-04-11T11:50:49.792586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_topk_words(tf_vector, top_k):\n    for i in range(tf_vector.shape[0]):\n        row = tf_vector[i].toarray()\n        row_vector = np.squeeze(row)\n        sort_features = np.argsort(row_vector)[::-1][:top_k]\n        features = tfidf.get_feature_names()\n        li = [features[i] for i in sort_features]\n        print(f\"cases number: {i} and its top words : {li}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:50:50.399223Z","iopub.execute_input":"2022-04-11T11:50:50.39952Z","iopub.status.idle":"2022-04-11T11:50:50.405931Z","shell.execute_reply.started":"2022-04-11T11:50:50.399488Z","shell.execute_reply":"2022-04-11T11:50:50.40514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_topk_words(tf_vector, 10) ## we could see each of cases belong to sympton or complaint or concern.","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:50:51.754762Z","iopub.execute_input":"2022-04-11T11:50:51.755682Z","iopub.status.idle":"2022-04-11T11:50:51.766306Z","shell.execute_reply.started":"2022-04-11T11:50:51.755637Z","shell.execute_reply":"2022-04-11T11:50:51.764875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### we can see tfidf words associated with each case and figureout associated symptom with it:\n* 1 st one talks about heart rate.\n* 2 nd about addominal, diarrhea.\n* 3 rd about permenstrual\n* etc..","metadata":{}},{"cell_type":"markdown","source":"#### Do cluster of feature to see what kind of patterns we have","metadata":{}},{"cell_type":"code","source":"df_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:51:11.447644Z","iopub.execute_input":"2022-04-11T11:51:11.447926Z","iopub.status.idle":"2022-04-11T11:51:11.457427Z","shell.execute_reply.started":"2022-04-11T11:51:11.447898Z","shell.execute_reply":"2022-04-11T11:51:11.456355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets intialize tfidf again this time on feature_text itself.\ntfidf = TfidfVectorizer(stop_words=stop_words)\ntfidf_vector = tfidf.fit_transform(df_features['feature_text'])\ntfidf_vector.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:51:12.784517Z","iopub.execute_input":"2022-04-11T11:51:12.784777Z","iopub.status.idle":"2022-04-11T11:51:12.795696Z","shell.execute_reply.started":"2022-04-11T11:51:12.784751Z","shell.execute_reply":"2022-04-11T11:51:12.794874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_features.groupby('case_num')['feature_num'].count().mean()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:51:13.180703Z","iopub.execute_input":"2022-04-11T11:51:13.181204Z","iopub.status.idle":"2022-04-11T11:51:13.188622Z","shell.execute_reply.started":"2022-04-11T11:51:13.181172Z","shell.execute_reply":"2022-04-11T11:51:13.188023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we could see each case has on average 14 features, let's see if we can see pattern in the features are collecting for each case\ntrue_k = 14\nkmeans = KMeans(n_clusters=true_k, random_state=42)\nkmeans.fit(tfidf_vector)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:51:14.514185Z","iopub.execute_input":"2022-04-11T11:51:14.514604Z","iopub.status.idle":"2022-04-11T11:51:14.694188Z","shell.execute_reply.started":"2022-04-11T11:51:14.514573Z","shell.execute_reply":"2022-04-11T11:51:14.693406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"order_centers = kmeans.cluster_centers_.argsort()[:,::-1]\nterms = tfidf.get_feature_names()\nfor i in range(true_k):\n    terms_out = []\n    for ind in order_centers[i, :10]:\n        terms_out.append(terms[ind])\n    print(f\"cluster {i}, and its words: {','.join(terms_out)}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:51:15.214392Z","iopub.execute_input":"2022-04-11T11:51:15.214677Z","iopub.status.idle":"2022-04-11T11:51:15.223811Z","shell.execute_reply.started":"2022-04-11T11:51:15.214646Z","shell.execute_reply":"2022-04-11T11:51:15.223139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We could see cluster\n* 0,1,5,3,6,7,8 symptoms\n* 5 about family history \n* 11,10, 9talks about habits\n* 1,4 duration, how many days was it present.\n* 2 date info\n* etc..","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n## Train\n\nonly 1000 patient data is annotated.","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nbme-score-clinical-patient-notes/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:51:26.468785Z","iopub.execute_input":"2022-04-11T11:51:26.469265Z","iopub.status.idle":"2022-04-11T11:51:26.507767Z","shell.execute_reply.started":"2022-04-11T11:51:26.469227Z","shell.execute_reply":"2022-04-11T11:51:26.507136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:51:27.435954Z","iopub.execute_input":"2022-04-11T11:51:27.4368Z","iopub.status.idle":"2022-04-11T11:51:27.448629Z","shell.execute_reply.started":"2022-04-11T11:51:27.436736Z","shell.execute_reply":"2022-04-11T11:51:27.448081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape, df_train.pn_num.nunique(), df_train.case_num.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:51:27.797681Z","iopub.execute_input":"2022-04-11T11:51:27.798068Z","iopub.status.idle":"2022-04-11T11:51:27.804268Z","shell.execute_reply.started":"2022-04-11T11:51:27.798038Z","shell.execute_reply":"2022-04-11T11:51:27.803616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### doe's patients have only one case associated with it\n\nconform's the beow code that each patient belong to one case only","metadata":{}},{"cell_type":"code","source":"df_groupby_pt = df_train.groupby(['pn_num'])['case_num'].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:51:35.14484Z","iopub.execute_input":"2022-04-11T11:51:35.145251Z","iopub.status.idle":"2022-04-11T11:51:35.151674Z","shell.execute_reply.started":"2022-04-11T11:51:35.145217Z","shell.execute_reply":"2022-04-11T11:51:35.150835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(df_groupby_pt != 1).sum() # all are false so when you sum it it's 0","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:51:36.074821Z","iopub.execute_input":"2022-04-11T11:51:36.075082Z","iopub.status.idle":"2022-04-11T11:51:36.081843Z","shell.execute_reply.started":"2022-04-11T11:51:36.075055Z","shell.execute_reply":"2022-04-11T11:51:36.081186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Compare case number and their features","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=df_train, x='case_num') ## We could see each cases has same features represented in features file","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:53:21.860518Z","iopub.execute_input":"2022-04-11T11:53:21.861365Z","iopub.status.idle":"2022-04-11T11:53:22.224129Z","shell.execute_reply.started":"2022-04-11T11:53:21.861318Z","shell.execute_reply":"2022-04-11T11:53:22.223265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test file","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/nbme-score-clinical-patient-notes/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:53:27.055333Z","iopub.execute_input":"2022-04-11T11:53:27.055613Z","iopub.status.idle":"2022-04-11T11:53:27.066555Z","shell.execute_reply.started":"2022-04-11T11:53:27.055582Z","shell.execute_reply":"2022-04-11T11:53:27.065905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:53:27.700516Z","iopub.execute_input":"2022-04-11T11:53:27.701069Z","iopub.status.idle":"2022-04-11T11:53:27.713587Z","shell.execute_reply.started":"2022-04-11T11:53:27.701018Z","shell.execute_reply":"2022-04-11T11:53:27.712645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### sample submission file","metadata":{}},{"cell_type":"code","source":"df_sub = pd.read_csv(\"/kaggle/input/nbme-score-clinical-patient-notes/sample_submission.csv\")\ndf_sub","metadata":{"execution":{"iopub.status.busy":"2022-04-11T11:53:28.695633Z","iopub.execute_input":"2022-04-11T11:53:28.695895Z","iopub.status.idle":"2022-04-11T11:53:28.708975Z","shell.execute_reply.started":"2022-04-11T11:53:28.695859Z","shell.execute_reply":"2022-04-11T11:53:28.708154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n\n### Conclusion/Analysis\n\n\n* Given a case number and patient number along with feature num we have to find reference of it expressed in patient notes and retrieve the index.\n* how analysis of cases, feature names help us understand how/what kind of symptom data we are dealing with. Since we see number of different features: we can have different mechanisms to extract different features.\n* Since only 1000 patients has annotations. by using pseudo labelling schema we can see for other patient history labells can be generated or not\n\n\nThis is on going document, as and when I find more analysis I will be updaing it!. Fork and feel free to add your analysis. \n\n                                            \n","metadata":{}},{"cell_type":"markdown","source":"#                                                                             **Kindly upvote if you find it useful**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}