{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport scipy as sp\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-03T23:47:35.89559Z","iopub.execute_input":"2022-05-03T23:47:35.896229Z","iopub.status.idle":"2022-05-03T23:47:35.996499Z","shell.execute_reply.started":"2022-05-03T23:47:35.896105Z","shell.execute_reply":"2022-05-03T23:47:35.995751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Exploration**","metadata":{}},{"cell_type":"code","source":"!pip install sty","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:47:35.99898Z","iopub.execute_input":"2022-05-03T23:47:35.999398Z","iopub.status.idle":"2022-05-03T23:47:45.511848Z","shell.execute_reply.started":"2022-05-03T23:47:35.999357Z","shell.execute_reply":"2022-05-03T23:47:45.511031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nfrom pathlib import Path\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\nif conversion_path.exists():\n    conversion_path.unlink()\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n    shutil.copy(input_dir/filename, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:47:45.514342Z","iopub.execute_input":"2022-05-03T23:47:45.514563Z","iopub.status.idle":"2022-05-03T23:47:45.541315Z","shell.execute_reply.started":"2022-05-03T23:47:45.514533Z","shell.execute_reply":"2022-05-03T23:47:45.540536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport ast\nimport sys\nimport copy\nimport json\nimport math\nimport string\nimport pickle\nimport random\nimport itertools\nimport warnings\nimport wordcloud\nfrom sty import bg, rs\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple\nimport plotly.express as px\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:47:45.544884Z","iopub.execute_input":"2022-05-03T23:47:45.545329Z","iopub.status.idle":"2022-05-03T23:47:50.577097Z","shell.execute_reply.started":"2022-05-03T23:47:45.545296Z","shell.execute_reply":"2022-05-03T23:47:50.576361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = pd.read_csv(\"/kaggle/input/nbme-score-clinical-patient-notes/features.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:47:50.578508Z","iopub.execute_input":"2022-05-03T23:47:50.578793Z","iopub.status.idle":"2022-05-03T23:47:50.592495Z","shell.execute_reply.started":"2022-05-03T23:47:50.578753Z","shell.execute_reply":"2022-05-03T23:47:50.591409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/train.csv\")\ntrain['annotation'] = train['annotation'].apply(ast.literal_eval)\ntrain['location'] = train['location'].apply(ast.literal_eval)\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:47:50.595933Z","iopub.execute_input":"2022-05-03T23:47:50.596404Z","iopub.status.idle":"2022-05-03T23:47:50.971801Z","shell.execute_reply.started":"2022-05-03T23:47:50.596374Z","shell.execute_reply":"2022-05-03T23:47:50.971116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cases = []\nnum_features = []\nfor x in set(features['case_num']):\n    cases.append(x)\n    num_features.append(features['case_num'].tolist().count(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:47:50.973057Z","iopub.execute_input":"2022-05-03T23:47:50.975127Z","iopub.status.idle":"2022-05-03T23:47:50.983921Z","shell.execute_reply.started":"2022-05-03T23:47:50.975085Z","shell.execute_reply":"2022-05-03T23:47:50.982561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_notes = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/patient_notes.csv\")\nall_notes = []\nall_notes_len = []\nfor notes in patient_notes['pn_history']:\n    all_notes.append(notes)\n    all_notes_len.append(len(notes))\nprint(\"Average length of Patient History - \",np.mean(all_notes_len))\nfig = px.histogram(x = all_notes_len,  marginal=\"violin\",nbins = 100)\nfig.update_layout(template=\"plotly_white\")\nfig.update_xaxes(title = \"Lenght of patient Notes\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:47:50.984904Z","iopub.execute_input":"2022-05-03T23:47:50.985163Z","iopub.status.idle":"2022-05-03T23:47:53.18703Z","shell.execute_reply.started":"2022-05-03T23:47:50.985129Z","shell.execute_reply":"2022-05-03T23:47:53.186415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud_notes = wordcloud.WordCloud(stopwords=wordcloud.STOPWORDS, max_font_size=120, max_words=5000,\n                      width = 600, height = 400,\n                      background_color='white').generate(\" \".join(all_notes))\nfig, ax = plt.subplots(figsize=(14,10))\nax.imshow(wordcloud_notes, interpolation='bilinear')\nax.set_axis_off()\nplt.imshow(wordcloud_notes);","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:47:53.188026Z","iopub.execute_input":"2022-05-03T23:47:53.188282Z","iopub.status.idle":"2022-05-03T23:48:11.929356Z","shell.execute_reply.started":"2022-05-03T23:47:53.188246Z","shell.execute_reply":"2022-05-03T23:48:11.927841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for note in patient_notes['pn_history'][:1]:\n    print(type(note))\nnote_lengths = [len(note) for note in patient_notes['pn_history']]\nnum_notes = []\nfor x in set(patient_notes['case_num']):\n    cases.append(x)\n    num_notes.append(patient_notes['case_num'].tolist().count(x))\nannotation_lengths = [len(x) for an in train['annotation'].tolist() for x in an]","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:11.932483Z","iopub.execute_input":"2022-05-03T23:48:11.933134Z","iopub.status.idle":"2022-05-03T23:48:11.978083Z","shell.execute_reply.started":"2022-05-03T23:48:11.933084Z","shell.execute_reply":"2022-05-03T23:48:11.977211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cases = list(set(train['case_num']))\npatient_notes_num = [0 for x in cases]\nfor x in set(train['pn_num']):\n    case_num = train[train['pn_num'] == x]['case_num'].tolist()[0]\n    patient_notes_num[case_num]+=1","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:11.979563Z","iopub.execute_input":"2022-05-03T23:48:11.979823Z","iopub.status.idle":"2022-05-03T23:48:12.378516Z","shell.execute_reply.started":"2022-05-03T23:48:11.979788Z","shell.execute_reply":"2022-05-03T23:48:12.377661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CaseData = namedtuple('CaseData', 'feature location num')\ndef getData(patient_note_num, case_num):\n    patient_note=\"\"\n    for index, row in patient_notes.iterrows():\n        if row['pn_num'] == patient_note_num and row['case_num'] == case_num:\n            patient_note = row['pn_history']\n    case_datas = []\n    for index, row in train.iterrows():\n        if row['pn_num'] == patient_note_num and row['case_num'] == case_num:\n            ft_num = row['feature_num']\n            for i in range(len(row['annotation'])):\n                case_datas.append(CaseData(row['annotation'][i], list(map(int, row['location'][i].split(\" \"))), ft_num))\n    return patient_note, case_datas","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:12.379863Z","iopub.execute_input":"2022-05-03T23:48:12.380185Z","iopub.status.idle":"2022-05-03T23:48:12.389273Z","shell.execute_reply.started":"2022-05-03T23:48:12.380145Z","shell.execute_reply":"2022-05-03T23:48:12.387833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_note, case_datas = getData(patient_note_num = 16, case_num = 0)\nprint(len(case_datas))\nlast_idx=0\nfor data in sorted(case_datas, key = lambda case_data : case_data.location[0]):\n    print(patient_note[last_idx : data.location[0]], end=\"\")\n    print(bg.blue + patient_note[data.location[0] : data.location[1]] + bg.rs, end=\"\")\n    last_idx = data.location[1]","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:12.390648Z","iopub.execute_input":"2022-05-03T23:48:12.391115Z","iopub.status.idle":"2022-05-03T23:48:15.042299Z","shell.execute_reply.started":"2022-05-03T23:48:12.391073Z","shell.execute_reply":"2022-05-03T23:48:15.041553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)   \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        # When running on the CuDNN backend, two further options must be set\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:15.043693Z","iopub.execute_input":"2022-05-03T23:48:15.043939Z","iopub.status.idle":"2022-05-03T23:48:15.053466Z","shell.execute_reply.started":"2022-05-03T23:48:15.043904Z","shell.execute_reply":"2022-05-03T23:48:15.052679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def micro_f1(preds, truths):\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    return f1_score(truths, preds)\ndef spans_to_binary(spans, length=None):\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1    \n    return binary\ndef span_micro_f1(preds, truths):\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n    return micro_f1(bin_preds, bin_truths)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:15.056688Z","iopub.execute_input":"2022-05-03T23:48:15.056905Z","iopub.status.idle":"2022-05-03T23:48:15.06624Z","shell.execute_reply.started":"2022-05-03T23:48:15.056879Z","shell.execute_reply":"2022-05-03T23:48:15.065416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_labels_for_scoring(df):\n    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n        if len(location_list) > 0:\n            location = location_list[0]\n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n    return truths\ndef get_char_probs(texts, predictions, tokenizer):\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n    return results\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n    return results\ndef get_predictions(results):\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n    return predictions\ndef get_score(y_true, y_pred):\n    return span_micro_f1(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:15.067745Z","iopub.execute_input":"2022-05-03T23:48:15.068066Z","iopub.status.idle":"2022-05-03T23:48:15.086023Z","shell.execute_reply.started":"2022-05-03T23:48:15.068026Z","shell.execute_reply":"2022-05-03T23:48:15.084039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_dir=\"../input/nbme-score-clinical-patient-notes/\"\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\ntest = pd.read_csv(main_dir+'test.csv')\nsubmission = pd.read_csv(main_dir+'sample_submission.csv')\nfeatures = pd.read_csv(main_dir+'features.csv')\npatient_notes = pd.read_csv(main_dir+'patient_notes.csv')\nfeatures = preprocess_features(features)\nprint(f\"test.shape: {test.shape}\")\nprint(f\"features.shape: {features.shape}\")\nprint(f\"patient_notes.shape: {patient_notes.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:15.087679Z","iopub.execute_input":"2022-05-03T23:48:15.088055Z","iopub.status.idle":"2022-05-03T23:48:15.419522Z","shell.execute_reply.started":"2022-05-03T23:48:15.08799Z","shell.execute_reply":"2022-05-03T23:48:15.418778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.merge(features, on=['feature_num', 'case_num'], how='left')\ntest = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:15.420883Z","iopub.execute_input":"2022-05-03T23:48:15.421383Z","iopub.status.idle":"2022-05-03T23:48:15.442949Z","shell.execute_reply.started":"2022-05-03T23:48:15.42134Z","shell.execute_reply":"2022-05-03T23:48:15.442255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=\"../input/deberta-v3-large-5-folds-public/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=32\n    fc_dropout=0.2\n    max_len=354\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:15.444256Z","iopub.execute_input":"2022-05-03T23:48:15.444539Z","iopub.status.idle":"2022-05-03T23:48:15.449702Z","shell.execute_reply.started":"2022-05-03T23:48:15.444502Z","shell.execute_reply":"2022-05-03T23:48:15.448734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\ntokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\nCFG.tokenizer = tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:15.451666Z","iopub.execute_input":"2022-05-03T23:48:15.451981Z","iopub.status.idle":"2022-05-03T23:48:16.586949Z","shell.execute_reply.started":"2022-05-03T23:48:15.451945Z","shell.execute_reply":"2022-05-03T23:48:16.586243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text, feature_text):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           max_length=CFG.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n    def __len__(self):\n        return len(self.feature_texts)\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item])\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:16.58826Z","iopub.execute_input":"2022-05-03T23:48:16.588495Z","iopub.status.idle":"2022-05-03T23:48:16.598376Z","shell.execute_reply.started":"2022-05-03T23:48:16.588461Z","shell.execute_reply":"2022-05-03T23:48:16.597716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ScoringModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0) \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0] \n        return last_hidden_states\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:16.599881Z","iopub.execute_input":"2022-05-03T23:48:16.600406Z","iopub.status.idle":"2022-05-03T23:48:16.614873Z","shell.execute_reply.started":"2022-05-03T23:48:16.600366Z","shell.execute_reply":"2022-05-03T23:48:16.614178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:16.616387Z","iopub.execute_input":"2022-05-03T23:48:16.616657Z","iopub.status.idle":"2022-05-03T23:48:16.627935Z","shell.execute_reply.started":"2022-05-03T23:48:16.616623Z","shell.execute_reply":"2022-05-03T23:48:16.627147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = ScoringModel(CFG, config_path=CFG.config_path, pretrained=False)\n    \n    state = torch.load(CFG.path+f\"{CFG.model.split('/')[1]}_fold{fold}_best.pth\",\n                           map_location=torch.device('cpu')) \n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs\n    gc.collect()\n    torch.cuda.empty_cache()\npredictions_v3_l = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:48:16.629597Z","iopub.execute_input":"2022-05-03T23:48:16.629875Z","iopub.status.idle":"2022-05-03T23:50:40.279865Z","shell.execute_reply.started":"2022-05-03T23:48:16.629829Z","shell.execute_reply":"2022-05-03T23:50:40.279063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=\"../input/debertalarge/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-large\"\n    batch_size=24\n    fc_dropout=0.2\n    max_len=466\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:50:40.284311Z","iopub.execute_input":"2022-05-03T23:50:40.284612Z","iopub.status.idle":"2022-05-03T23:50:40.293836Z","shell.execute_reply.started":"2022-05-03T23:50:40.28457Z","shell.execute_reply":"2022-05-03T23:50:40.293153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:50:40.297653Z","iopub.execute_input":"2022-05-03T23:50:40.297848Z","iopub.status.idle":"2022-05-03T23:50:40.460425Z","shell.execute_reply.started":"2022-05-03T23:50:40.297824Z","shell.execute_reply":"2022-05-03T23:50:40.459661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)    \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:50:40.461556Z","iopub.execute_input":"2022-05-03T23:50:40.461801Z","iopub.status.idle":"2022-05-03T23:50:40.47319Z","shell.execute_reply.started":"2022-05-03T23:50:40.461769Z","shell.execute_reply":"2022-05-03T23:50:40.4725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:50:40.478419Z","iopub.execute_input":"2022-05-03T23:50:40.478911Z","iopub.status.idle":"2022-05-03T23:50:40.49163Z","shell.execute_reply.started":"2022-05-03T23:50:40.478872Z","shell.execute_reply":"2022-05-03T23:50:40.490742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_v1_l = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:50:40.492587Z","iopub.execute_input":"2022-05-03T23:50:40.492777Z","iopub.status.idle":"2022-05-03T23:52:28.905793Z","shell.execute_reply.started":"2022-05-03T23:50:40.492754Z","shell.execute_reply":"2022-05-03T23:52:28.90497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=\"../input/nbme-deberta-base-baseline-train/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-base\"\n    batch_size=24\n    fc_dropout=0.2\n    max_len=466\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:52:28.90717Z","iopub.execute_input":"2022-05-03T23:52:28.907633Z","iopub.status.idle":"2022-05-03T23:52:28.913832Z","shell.execute_reply.started":"2022-05-03T23:52:28.907587Z","shell.execute_reply":"2022-05-03T23:52:28.913148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:52:28.915377Z","iopub.execute_input":"2022-05-03T23:52:28.915674Z","iopub.status.idle":"2022-05-03T23:52:29.054588Z","shell.execute_reply.started":"2022-05-03T23:52:28.915637Z","shell.execute_reply":"2022-05-03T23:52:29.053734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_v1_b = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:52:29.056164Z","iopub.execute_input":"2022-05-03T23:52:29.056428Z","iopub.status.idle":"2022-05-03T23:53:38.563717Z","shell.execute_reply.started":"2022-05-03T23:52:29.056391Z","shell.execute_reply":"2022-05-03T23:53:38.562878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1 = 1\nw2 = 0\nw3 = 0","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:53:38.565388Z","iopub.execute_input":"2022-05-03T23:53:38.568534Z","iopub.status.idle":"2022-05-03T23:53:38.574858Z","shell.execute_reply.started":"2022-05-03T23:53:38.568478Z","shell.execute_reply":"2022-05-03T23:53:38.57414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor p1, p2, p3 in zip(predictions_v3_l, predictions_v1_l, predictions_v1_b):\n    predictions.append(w1*p1 + w2*p2 + w3*p3)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:53:38.576188Z","iopub.execute_input":"2022-05-03T23:53:38.576781Z","iopub.status.idle":"2022-05-03T23:53:38.587394Z","shell.execute_reply.started":"2022-05-03T23:53:38.576743Z","shell.execute_reply":"2022-05-03T23:53:38.586565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = get_results(predictions)\nsubmission['location'] = results\ndisplay(submission.head())\nsubmission[['id', 'location']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:53:38.588761Z","iopub.execute_input":"2022-05-03T23:53:38.589126Z","iopub.status.idle":"2022-05-03T23:53:38.619938Z","shell.execute_reply.started":"2022-05-03T23:53:38.58909Z","shell.execute_reply":"2022-05-03T23:53:38.619212Z"},"trusted":true},"execution_count":null,"outputs":[]}]}