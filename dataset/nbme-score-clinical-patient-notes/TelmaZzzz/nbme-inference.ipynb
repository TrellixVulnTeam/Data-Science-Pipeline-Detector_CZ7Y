{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-03T08:43:50.441707Z","iopub.execute_input":"2022-05-03T08:43:50.442297Z","iopub.status.idle":"2022-05-03T08:43:50.471628Z","shell.execute_reply.started":"2022-05-03T08:43:50.442208Z","shell.execute_reply":"2022-05-03T08:43:50.470953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\nimport shutil\n\ninput_dir = Path(\"../input/deberta-v23-fix\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T08:43:50.473267Z","iopub.execute_input":"2022-05-03T08:43:50.473741Z","iopub.status.idle":"2022-05-03T08:43:50.510402Z","shell.execute_reply.started":"2022-05-03T08:43:50.473704Z","shell.execute_reply":"2022-05-03T08:43:50.509773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_deberta_tokenizer(model_name, use_trim_offsets=False):        \n    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n    return DebertaV2TokenizerFast.from_pretrained(model_name, trim_offsets=use_trim_offsets)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T08:43:50.511422Z","iopub.execute_input":"2022-05-03T08:43:50.511743Z","iopub.status.idle":"2022-05-03T08:43:50.516703Z","shell.execute_reply.started":"2022-05-03T08:43:50.511705Z","shell.execute_reply":"2022-05-03T08:43:50.515756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoModel, AutoTokenizer, T5EncoderModel, AutoConfig\nfrom tqdm import tqdm\nimport gc\nimport itertools","metadata":{"execution":{"iopub.status.busy":"2022-05-03T08:43:50.520418Z","iopub.execute_input":"2022-05-03T08:43:50.520957Z","iopub.status.idle":"2022-05-03T08:43:52.421974Z","shell.execute_reply.started":"2022-05-03T08:43:50.52092Z","shell.execute_reply":"2022-05-03T08:43:52.42119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class ModelConfig(object):\n    def __init__(self, args):\n        self.pretrain_path = args.pretrain_path\n        self.device = args.device\n        self.dropout = 0.1\n\n\nclass Swish(nn.Module):\n    def __init__(self, inplace=True):\n        super(Swish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        if self.inplace:\n            x.mul_(torch.sigmoid(x))\n            return x\n        else:\n            return x * torch.sigmoid(x)\n\n\nclass Activation(nn.Module):\n    def __init__(self, name=\"swish\"):\n        super(Activation, self).__init__()\n        if name not in [\"swish\", \"relu\", \"gelu\"]:\n            raise\n        if name == \"swish\":\n            self.net = Swish()\n        elif name == \"relu\":\n            self.net = nn.ReLU()\n        elif name == \"gelu\":\n            self.net = nn.GELU()\n    \n    def forward(self, x):\n        return self.net(x)\n\n\nclass Dence(nn.Module):\n    def __init__(self, i_dim, o_dim, activation=\"swish\"):\n        super(Dence, self).__init__()\n        self.dence = nn.Sequential(\n            nn.Linear(i_dim, o_dim),\n            # nn.ReLU(),\n            Activation(activation),\n        )\n\n    def forward(self, x):\n        return self.dence(x)\n\n\nclass NBMEModel(nn.Module):\n    def __init__(self, args):\n        super(NBMEModel, self).__init__()\n        config = AutoConfig.from_pretrained(args.pretrain_path)\n        if \"t5\" in args.pretrain_path:\n            self.transformer = T5EncoderModel.from_pretrained(args.pretrain_path)\n        else:\n            self.transformer = AutoModel.from_pretrained(args.pretrain_path)\n        self.dropout = nn.Dropout(args.dropout)\n        self.output = nn.Linear(config.hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n        if token_type_ids:\n            transformer_out = self.transformer(input_ids, attention_mask, token_type_ids)\n        else:\n            transformer_out = self.transformer(input_ids, attention_mask)\n        sequence_output = transformer_out.last_hidden_state\n        sequence_output = self.dropout(sequence_output)\n\n        logits = self.output(sequence_output)\n        logits_out = torch.sigmoid(logits)\n        loss = 0\n        if labels is not None:\n            loss = self.loss(logits, labels)\n\n        return logits_out, loss\n    \n    def loss(self, logits, labels):\n        loss_fct = nn.BCEWithLogitsLoss()\n        logits = logits.squeeze(-1)\n        labels_mask = (labels != -1)\n        # logging.debug(f\"labels shape: {labels.shape}\")\n        # logging.debug(f\"logits shape: {logits.shape}\")\n        # logging.debug(f\"labels_mask shape: {labels_mask.shape}\")\n        active_logits = torch.masked_select(logits, labels_mask)\n        true_labels = torch.masked_select(labels, labels_mask)\n        loss = loss_fct(active_logits, true_labels)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-05-03T08:43:52.424389Z","iopub.execute_input":"2022-05-03T08:43:52.424913Z","iopub.status.idle":"2022-05-03T08:43:52.442813Z","shell.execute_reply.started":"2022-05-03T08:43:52.424874Z","shell.execute_reply":"2022-05-03T08:43:52.442027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Datasets","metadata":{}},{"cell_type":"code","source":"class NBMEDataset:\n    def __init__(self, samples, tokenizer, mask_prob=0.0, mask_ratio=0.0):\n        self.samples = samples\n        self.length = len(samples)\n        self.tokenizer = tokenizer\n        self.mask_prob = mask_prob\n        self.mask_ratio = mask_ratio\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        attention_mask = self.samples[idx][\"attention_mask\"]\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n        }\n\n    \nclass Collate:\n    def __init__(self, tokenizer, fix_length=-1, fixed=False):\n        self.tokenizer = tokenizer\n        self.fix_length = fix_length\n        self.fixed = fixed\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n        if self.fix_length != -1:\n            batch_max = min(batch_max, self.fix_length)\n            if self.fixed:\n                batch_max = self.fix_length\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n        else:\n            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"input_ids\"]]\n            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n\n        # convert to tensors\n        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-05-03T08:43:52.44399Z","iopub.execute_input":"2022-05-03T08:43:52.444425Z","iopub.status.idle":"2022-05-03T08:43:52.460103Z","shell.execute_reply.started":"2022-05-03T08:43:52.444387Z","shell.execute_reply":"2022-05-03T08:43:52.459292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicter","metadata":{}},{"cell_type":"code","source":"class TrainerConfig(object):\n    def __init__(self, args):\n        self.model_load = args.model_load\n        self.valid_batch_size = args.valid_batch_size\n        self.fix_length = args.fix_length\n\n\nclass Predicter(object):\n    def __init__(self, args):\n        self.trainer_config = TrainerConfig(args)\n        self.model_config = ModelConfig(args)\n        self.device = args.device\n    \n    def build_model(self):\n        self.model = NBMEModel(self.model_config)\n    \n    def model_init(self):\n        self.build_model()\n        self.model.to(self.device)\n        self.model.eval()\n    \n    def model_load(self, path):\n        self.model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n        self.model.to(self.device)\n        self.model.eval()\n    \n    def get_logits(self, batch, return_loss=False):\n        input_ids = batch[\"input_ids\"].to(self.device)\n        attention_mask = batch[\"attention_mask\"].to(self.device)\n        # logging.debug(f\"attention_mask: {attention_mask}\")\n        if return_loss:\n            labels = batch[\"labels\"].to(self.device)\n            # logging.debug(f\"labels: {labels.tolist()}\")\n            # logging.debug(f\"ids: {input_ids.tolist()}\")\n            logits, loss = self.model(input_ids, attention_mask, labels=labels)\n            return logits, loss\n        else:\n            logits, _ = self.model(input_ids, attention_mask)\n            return logits\n\n    @torch.no_grad()\n    def predict(self, valid_datasets, valid_collate):\n        self.model.eval()\n        valid_iter = torch.utils.data.DataLoader(valid_datasets, batch_size=self.trainer_config.valid_batch_size, collate_fn=valid_collate)\n        preds = []\n        PAD = torch.tensor([0], dtype=torch.float).unsqueeze(0)\n        for batch in tqdm(valid_iter):\n            logits = self.get_logits(batch).cpu().squeeze(-1)\n            bs, length = logits.shape\n            batch_pad = torch.cat([PAD] * bs, dim=0)\n            logits = torch.cat([logits] + [batch_pad] * (512 - length), dim=1)\n            preds.append(logits)\n        preds = torch.cat(preds, dim=0)\n        return preds","metadata":{"execution":{"iopub.status.busy":"2022-05-03T08:43:52.46257Z","iopub.execute_input":"2022-05-03T08:43:52.462754Z","iopub.status.idle":"2022-05-03T08:43:52.481123Z","shell.execute_reply.started":"2022-05-03T08:43:52.462732Z","shell.execute_reply":"2022-05-03T08:43:52.480379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def prepare_test_data(df, tokenizer, max_len):\n    test_sample = []\n    for _, row in df.iterrows():\n        # if row[\"id\"] == \"10075_100\":\n        #     debug = True\n        # else:\n        #     debug = False\n        pn_history = row[\"pn_history\"]\n        feature_text = row[\"feature_text\"]\n        encoder = tokenizer.encode_plus(\n            pn_history, feature_text,\n            add_special_tokens=True,\n            return_offsets_mapping=False,\n        )\n        decoder = tokenizer.encode_plus(\n            pn_history,\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n        )\n        offset_mapping = decoder.offset_mapping\n        test_sample.append({\n            \"input_ids\": encoder.input_ids,\n            \"attention_mask\": encoder.attention_mask,\n            \"offset_mapping\": offset_mapping,\n            \"id\": row[\"id\"],\n            \"length\": len(pn_history),\n        })\n    return test_sample\n\n\ndef micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    return f1_score(truths, preds)\n\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n    return binary\n\n\ndef span_micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n    return micro_f1(bin_preds, bin_truths)\n\n\ndef create_labels_for_scoring(df):\n    df[\"location_for_create_labels\"] = df[\"location\"]\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        location_list = eval(location_list)\n        truth = []\n        for location in location_list:\n            for loc in [s.split(\" \") for s in location.split(\";\")]:\n                truth.append([int(loc[0]), int(loc[1])])\n        truths.append(truth)\n    return truths\n\n\ndef get_char_probs(samples, predictions):\n    results = [np.zeros(item[\"length\"]) for item in samples]\n    for i, (sample, prediction) in enumerate(zip(samples, predictions)):\n        offset_mapping = sample[\"offset_mapping\"]\n        for idx, (offset_mapping, pred) in enumerate(zip(offset_mapping, prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n    return results\n\n\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n    return results\n\n\ndef get_results_v2(char_probs, valid_texts, th=0.5):\n    results = []\n    for i, char_prob in enumerate(char_probs):\n        result = []\n        end = 0\n        while end < len(char_prob):\n            if char_prob[end] < th:\n                end += 1\n            else:\n                start = end\n                while end < len(char_prob) and char_prob[end] >= th:\n                    end += 1\n                if valid_texts[i][start].isspace():\n                    result.extend(list(range(start+1, end+1)))\n                else:\n                    result.extend(list(range(start, end+1)))\n\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n    return results\n\n\ndef get_predictions(results):\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n    return predictions\n\n\ndef text_to_word(text):\n    word = text.split()\n    word_offset = []\n\n    start = 0\n    for w in word:\n        r = text[start:].find(w)\n\n        if r == -1:\n            raise NotImplementedError\n        else:\n            start = start + r\n            end = start + len(w)\n            word_offset.append((start, end))\n            # print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])\n        start = end\n\n    return word, word_offset\n\n\ndef get_score(y_true, y_pred):\n    score = span_micro_f1(y_true, y_pred)\n    return score\n\n\ndef get_results_v3(char_probs, valid_texts, valid_id, th_mp={}, th=0.5):\n    results = []\n    for i, char_prob in enumerate(char_probs):\n        result = []\n        end = 0\n        feature_num = valid_id[i].split(\"_\")[-1]\n        fth = th\n        ### post 2\n        fth = th_mp.get(feature_num, 0.5)\n        ### /post 2\n        while end < len(char_prob):\n            if char_prob[end] < fth:\n                end += 1\n            else:\n                start = end\n                while end < len(char_prob) and char_prob[end] >= fth:\n                    end += 1\n                ### post 1\n                if feature_num == \"708\":\n                    if \"5\" in valid_texts[i][start:end+1] or \"3\" in valid_texts[i][start:end+1]:\n                        continue\n                ### /post 1\n                if valid_texts[i][start].isspace():\n                    result.extend(list(range(start+1, end+1)))\n                else:\n                    result.extend(list(range(start, end+1)))\n\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-05-03T08:43:52.48378Z","iopub.execute_input":"2022-05-03T08:43:52.484475Z","iopub.status.idle":"2022-05-03T08:43:52.521084Z","shell.execute_reply.started":"2022-05-03T08:43:52.484436Z","shell.execute_reply":"2022-05-03T08:43:52.520372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"class Config:\n    valid_batch_size = 32\n    fix_length = 512\n    device = None\n    model_load = \"\"\n    pretrain_path = \"\"\n\n\nargs = Config\nargs.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n# th_mp = {\n#     '103': 0.33,\n#     '911': 0.31,\n#     '003': 0.35,\n#     '313': 0.35,\n#     '904': 0.63,\n#     '200': 0.31,\n#     '207': 0.35,\n#     '510': 0.69,\n#     '508': 0.31,\n#     \"516\": 0.38,\n#     \"708\": 0.51,\n#     \"206\": 0.32,\n#     \"702\": 0.66,\n# }\n# no del\n# th_mp = {'103': 0.33, '911': 0.31, '003': 0.35, '313': 0.35, '904': 0.63, '200': 0.31, '207': 0.35, '510': 0.69, '508': 0.31, '516': 0.38, '708': 0.51, '206': 0.32, '702': 0.66, '703': 0.5, '203': 0.42, '807': 0.5, '812': 0.5, '215': 0.62, '000': 0.6, '108': 0.41000000000000003, '816': 0.4, '010': 0.64, '004': 0.56, '502': 0.5, '815': 0.41000000000000003, '914': 0.35000000000000003, '112': 0.5, '403': 0.6, '900': 0.49, '802': 0.5, '915': 0.45, '111': 0.64, '214': 0.37, '208': 0.5700000000000001, '201': 0.53, '314': 0.5, '504': 0.5, '505': 0.36, '101': 0.5, '801': 0.54, '404': 0.43, '808': 0.36, '513': 0.5, '309': 0.5, '408': 0.38, '608': 0.5, '012': 0.5, '211': 0.5, '814': 0.54, '601': 0.56, '507': 0.4, '512': 0.36, '511': 0.64, '800': 0.5, '002': 0.39, '210': 0.5, '301': 0.61, '311': 0.46, '907': 0.5, '302': 0.4, '100': 0.39, '501': 0.36, '600': 0.5, '304': 0.5, '804': 0.5, '705': 0.5, '102': 0.41000000000000003, '707': 0.38, '811': 0.37, '110': 0.36, '906': 0.58, '401': 0.5, '500': 0.35000000000000003, '701': 0.59, '609': 0.64, '312': 0.63, '611': 0.49, '006': 0.6, '310': 0.5, '105': 0.39, '107': 0.35000000000000003, '603': 0.62, '809': 0.5, '916': 0.39, '913': 0.5, '606': 0.5700000000000001, '104': 0.56, '605': 0.45, '213': 0.55, '602': 0.45, '305': 0.5, '517': 0.5, '704': 0.35000000000000003, '908': 0.5, '005': 0.38, '910': 0.5, '514': 0.55, '300': 0.35000000000000003, '803': 0.56, '706': 0.58, '001': 0.64, '901': 0.5, '813': 0.45, '202': 0.56, '204': 0.61, '604': 0.56, '007': 0.5, '905': 0.36, '607': 0.43, '109': 0.5, '810': 0.62, '902': 0.4, '402': 0.5700000000000001, '405': 0.5, '205': 0.59, '303': 0.52, '506': 0.55, '610': 0.45, '806': 0.63, '308': 0.5, '307': 0.64, '315': 0.5, '306': 0.6, '209': 0.58, '407': 0.5, '515': 0.5, '212': 0.39, '903': 0.59, '216': 0.35000000000000003, '406': 0.61, '805': 0.5, '106': 0.5, '008': 0.5, '009': 0.43, '909': 0.42, '912': 0.39, '011': 0.61, '700': 0.54, '503': 0.38, '400': 0.54, '409': 0.55, '817': 0.35000000000000003, '509': 0.55}\n# have del\n# th_mp = {  \"103\": 0.33,  \"911\": 0.31,  \"003\": 0.35,  \"313\": 0.35,  \"904\": 0.63,  \"200\": 0.31,  \"207\": 0.35,  \"510\": 0.69,  \"508\": 0.31,  \"516\": 0.38,  \"708\": 0.51,  \"206\": 0.32,  \"702\": 0.66,  \"703\": 0.5,  \"203\": 0.42,  \"807\": 0.5,  \"812\": 0.5,  \"215\": 0.62,  \"000\": 0.6,  \"108\": 0.41000000000000003,  \"816\": 0.4,  \"010\": 0.64,  \"502\": 0.5,  \"815\": 0.41000000000000003,  \"914\": 0.35000000000000003,  \"112\": 0.5,  \"403\": 0.6,  \"900\": 0.49,  \"802\": 0.5,  \"915\": 0.45,  \"111\": 0.64,  \"214\": 0.37,  \"208\": 0.5700000000000001,  \"201\": 0.53,  \"314\": 0.5,  \"504\": 0.5,  \"505\": 0.36,  \"101\": 0.5,  \"801\": 0.54,  \"404\": 0.43,  \"513\": 0.5,  \"309\": 0.5,  \"408\": 0.38,  \"608\": 0.5,  \"012\": 0.5,  \"211\": 0.5,  \"601\": 0.56,  \"507\": 0.4,  \"512\": 0.36,  \"511\": 0.64,  \"800\": 0.5,  \"002\": 0.39,  \"210\": 0.5,  \"301\": 0.61,  \"907\": 0.5,  \"302\": 0.4,  \"100\": 0.39,  \"501\": 0.36,  \"600\": 0.5,  \"304\": 0.5,  \"804\": 0.5,  \"705\": 0.5,  \"102\": 0.41000000000000003,  \"811\": 0.37,  \"110\": 0.36,  \"906\": 0.58,  \"401\": 0.5,  \"500\": 0.35000000000000003,  \"609\": 0.64,  \"312\": 0.63,  \"006\": 0.6,  \"310\": 0.5,  \"105\": 0.39,  \"107\": 0.35000000000000003,  \"603\": 0.62,  \"809\": 0.5,  \"916\": 0.39,  \"913\": 0.5,  \"606\": 0.5700000000000001,  \"605\": 0.45,  \"213\": 0.55,  \"305\": 0.5,  \"517\": 0.5,  \"704\": 0.35000000000000003,  \"908\": 0.5,  \"005\": 0.38,  \"910\": 0.5,  \"514\": 0.55,  \"300\": 0.35000000000000003,  \"803\": 0.56,  \"706\": 0.58,  \"001\": 0.64,  \"901\": 0.5,  \"813\": 0.45,  \"204\": 0.61,  \"604\": 0.56,  \"007\": 0.5,  \"905\": 0.36,  \"607\": 0.43,  \"109\": 0.5,  \"810\": 0.62,  \"902\": 0.4,  \"405\": 0.5,  \"205\": 0.59,  \"506\": 0.55,  \"610\": 0.45,  \"806\": 0.63,  \"308\": 0.5,  \"307\": 0.64,  \"315\": 0.5,  \"306\": 0.6,  \"209\": 0.58,  \"407\": 0.5,  \"515\": 0.5,  \"212\": 0.39,  \"903\": 0.59,  \"406\": 0.61,  \"805\": 0.5,  \"106\": 0.5,  \"008\": 0.5,  \"009\": 0.43,  \"909\": 0.42,  \"912\": 0.39,  \"700\": 0.54,  \"503\": 0.38,  \"400\": 0.54,  \"817\": 0.35000000000000003,  \"509\": 0.55 }\n# deep del\nth_mp = { \"103\": 0.33, \"911\": 0.31, \"003\": 0.35, \"313\": 0.35, \"904\": 0.63, \"200\": 0.31, \"207\": 0.35, \"508\": 0.31, \"516\": 0.38, \"708\": 0.51, \"206\": 0.32, \"702\": 0.66, \"000\": 0.6, \"108\": 0.41, \"816\": 0.4, \"010\": 0.64, \"914\": 0.35, \"403\": 0.6, \"111\": 0.64, \"214\": 0.37, \"505\": 0.36, \"801\": 0.54, \"404\": 0.43, \"408\": 0.38, \"601\": 0.56, \"507\": 0.4, \"512\": 0.36, \"511\": 0.64, \"002\": 0.39, \"301\": 0.61, \"302\": 0.4, \"100\": 0.39, \"501\": 0.36, \"102\": 0.41, \"811\": 0.37, \"110\": 0.36, \"906\": 0.58, \"500\": 0.35, \"312\": 0.63, \"006\": 0.6, \"105\": 0.39, \"107\": 0.35, \"603\": 0.62, \"916\": 0.39, \"213\": 0.55, \"704\": 0.35, \"514\": 0.55, \"300\": 0.35, \"803\": 0.56, \"706\": 0.58, \"813\": 0.45, \"204\": 0.61, \"604\": 0.56, \"607\": 0.43, \"810\": 0.62, \"205\": 0.59, \"610\": 0.45, \"307\": 0.64, \"306\": 0.6, \"903\": 0.59, \"406\": 0.61, \"009\": 0.43, \"909\": 0.42, \"912\": 0.39, \"503\": 0.38, \"400\": 0.54, \"817\": 0.35, }\n# 0.001\n# th_mp = {'103': 0.33, '911': 0.31, '003': 0.35, '313': 0.35, '904': 0.63, '200': 0.31, '207': 0.35, '510': 0.69, '508': 0.31, '516': 0.38, '708': 0.51, '206': 0.32, '702': 0.66, '909': 0.5, '606': 0.5, '306': 0.5, '805': 0.5, '305': 0.5, '603': 0.62, '506': 0.5, '107': 0.51, '212': 0.5, '812': 0.5, '916': 0.5, '405': 0.5, '404': 0.5, '605': 0.5, '814': 0.5, '100': 0.5, '309': 0.5, '906': 0.5, '000': 0.5, '806': 0.5, '304': 0.5, '310': 0.5, '802': 0.5, '701': 0.5, '813': 0.49, '913': 0.5, '815': 0.5, '112': 0.5, '201': 0.5, '502': 0.5, '801': 0.5, '503': 0.4, '109': 0.5, '008': 0.5, '101': 0.5, '007': 0.5, '002': 0.5, '908': 0.5, '300': 0.47000000000000003, '905': 0.5, '406': 0.5, '308': 0.5, '203': 0.5, '110': 0.48, '816': 0.5, '213': 0.51, '401': 0.5, '803': 0.54, '501': 0.5, '602': 0.5, '006': 0.5, '408': 0.5, '914': 0.5, '509': 0.5, '600': 0.5, '507': 0.5, '513': 0.5, '804': 0.5, '303': 0.5, '901': 0.5, '216': 0.5, '106': 0.5, '314': 0.5, '817': 0.36, '004': 0.5, '809': 0.5, '604': 0.5, '312': 0.5, '705': 0.5, '514': 0.5, '900': 0.49, '409': 0.5, '005': 0.5, '607': 0.5, '301': 0.51, '210': 0.5, '009': 0.43, '307': 0.5, '609': 0.5, '700': 0.5, '704': 0.5, '211': 0.5, '315': 0.5, '400': 0.54, '108': 0.5, '500': 0.5, '204': 0.5, '915': 0.5, '407': 0.5, '311': 0.5, '511': 0.5, '703': 0.5, '903': 0.5, '810': 0.5, '517': 0.5, '102': 0.5, '902': 0.5, '111': 0.53, '707': 0.5, '209': 0.5, '205': 0.5, '706': 0.49, '800': 0.5, '808': 0.5, '011': 0.5, '608': 0.5, '012': 0.5, '215': 0.5, '512': 0.5, '505': 0.51, '910': 0.5, '907': 0.5, '403': 0.5, '202': 0.5, '504': 0.5, '001': 0.5, '811': 0.5, '515': 0.5, '611': 0.5, '601': 0.5, '807': 0.5, '214': 0.5, '010': 0.5, '402': 0.5, '610': 0.5, '302': 0.48, '912': 0.5, '208': 0.5, '104': 0.5, '105': 0.5}\n# 0.0005\n# th_mp = {'103': 0.33, '911': 0.31, '003': 0.35, '313': 0.35, '904': 0.63, '200': 0.31, '207': 0.35, '510': 0.69, '508': 0.31, '516': 0.38, '708': 0.51, '206': 0.32, '702': 0.66, '901': 0.5, '402': 0.5, '902': 0.5, '611': 0.49, '211': 0.5, '408': 0.5, '900': 0.49, '913': 0.5, '707': 0.5, '107': 0.48, '405': 0.5, '906': 0.5, '813': 0.45, '511': 0.64, '404': 0.43, '809': 0.5, '805': 0.5, '001': 0.5, '804': 0.5, '507': 0.5, '610': 0.47000000000000003, '608': 0.5, '512': 0.5, '000': 0.5, '912': 0.5, '102': 0.41000000000000003, '600': 0.5, '400': 0.54, '406': 0.5, '704': 0.37, '011': 0.5, '903': 0.54, '604': 0.56, '803': 0.54, '602': 0.5, '008': 0.5, '701': 0.5, '105': 0.41000000000000003, '910': 0.5, '802': 0.5, '007': 0.5, '308': 0.5, '505': 0.51, '806': 0.5, '010': 0.53, '514': 0.55, '311': 0.5, '703': 0.5, '407': 0.5, '814': 0.5, '108': 0.41000000000000003, '302': 0.48, '315': 0.5, '908': 0.5, '817': 0.35000000000000003, '208': 0.5, '403': 0.53, '203': 0.5, '603': 0.62, '914': 0.5, '104': 0.5, '601': 0.5, '916': 0.5, '706': 0.55, '009': 0.43, '606': 0.49, '005': 0.5, '312': 0.5, '213': 0.51, '800': 0.5, '109': 0.5, '504': 0.5, '607': 0.43, '209': 0.5, '609': 0.5, '909': 0.42, '112': 0.5, '100': 0.5, '816': 0.4, '502': 0.5, '310': 0.5, '305': 0.5, '705': 0.5, '210': 0.5, '501': 0.5, '309': 0.5, '905': 0.5, '409': 0.5, '006': 0.5, '303': 0.5, '509': 0.5, '300': 0.35000000000000003, '101': 0.5, '304': 0.5, '314': 0.5, '500': 0.52, '204': 0.54, '517': 0.5, '812': 0.5, '700': 0.52, '004': 0.5, '106': 0.5, '515': 0.5, '205': 0.59, '801': 0.54, '815': 0.5, '012': 0.5, '201': 0.5, '306': 0.5, '216': 0.5, '307': 0.5, '212': 0.5, '915': 0.5, '002': 0.5, '401': 0.5, '807': 0.5, '506': 0.5, '215': 0.5, '301': 0.61, '811': 0.38, '503': 0.4, '810': 0.61, '202': 0.5, '111': 0.53, '808': 0.5, '513': 0.5, '907': 0.5, '214': 0.5, '605': 0.5, '110': 0.48}\n# pt5 th f1 <= 0.90\n# th_mp = {'207': 0.37, '209': 0.61, '911': 0.5, '103': 0.35000000000000003, '003': 0.36, '215': 0.61, '603': 0.63, '510': 0.64, '809': 0.51, '509': 0.52, '206': 0.45, '513': 0.48, '402': 0.5, '508': 0.35000000000000003, '200': 0.5, '505': 0.47000000000000003, '504': 0.35000000000000003, '702': 0.62, '708': 0.39, '313': 0.47000000000000003, '516': 0.58, '008': 0.63, '503': 0.35000000000000003, '216': 0.5, '514': 0.61, '212': 0.44, '403': 0.63, '904': 0.63, '204': 0.5, '201': 0.44, '500': 0.42, '511': 0.63, '111': 0.6, '902': 0.39, '609': 0.51, '813': 0.37, '517': 0.5, '811': 0.36, '010': 0.63, '512': 0.38, '203': 0.45, '213': 0.58, '214': 0.5, '604': 0.64, '706': 0.55, '205': 0.5, '310': 0.51, '801': 0.41000000000000003, '306': 0.63, '009': 0.35000000000000003, '005': 0.5, '903': 0.61, '803': 0.37, '312': 0.36, '810': 0.5, '607': 0.51}\n# pt5 th f1 <= 0.90 up > 1e-5\n# th_mp = {'207': 0.37, '911': 0.5, '103': 0.35000000000000003, '003': 0.36, '215': 0.61, '603': 0.63, '510': 0.64, '509': 0.52, '206': 0.45, '513': 0.48, '402': 0.5, '508': 0.35000000000000003, '200': 0.5, '505': 0.47000000000000003, '504': 0.35000000000000003, '702': 0.62, '708': 0.39, '313': 0.47000000000000003, '503': 0.35000000000000003, '216': 0.5, '514': 0.61, '212': 0.44, '403': 0.63, '204': 0.5, '201': 0.44, '511': 0.63, '111': 0.6, '902': 0.39, '813': 0.37, '517': 0.5, '811': 0.36, '010': 0.63, '512': 0.38, '203': 0.45, '214': 0.5, '604': 0.64, '706': 0.55, '205': 0.5, '801': 0.41000000000000003, '009': 0.35000000000000003, '005': 0.5, '903': 0.61, '810': 0.5}\n# punish_th = {\n#     \"207\": 0.35,\n#     \"911\": 0.31,\n#     \"103\": 0.33,\n#     \"003\": 0.35,\n#     \"206\": 0.32,\n#     \"603\": 0.62,\n#     \"508\": 0.31,\n#     \"200\": 0.31,\n#     \"702\": 0.66,\n#     \"505\": 0.36,\n#     \"313\": 0.35,\n#     \"516\": 0.38,\n#     \"708\": 0.51,\n#     \"503\": 0.38,\n#     \"403\": 0.6,\n#     \"904\": 0.63,\n#     \"514\": 0.55,\n#     \"500\": 0.35,\n#     \"204\": 0.61,\n#     \"111\": 0.64,\n#     \"511\": 0.64,\n#     \"811\": 0.37,\n#     \"512\": 0.36,\n#     \"813\": 0.45,\n#     \"213\": 0.55,\n#     \"010\": 0.64,\n#     \"214\": 0.37,\n#     \"706\": 0.58,\n#     \"810\": 0.62,\n#     \"205\": 0.59,\n#     \"604\": 0.56,\n#     \"803\": 0.56,\n# }\n\nparams = [96, 79, 40, 88, 48, 39]\n# params = [99, 69, 72, 15, 1, 2, 19, 75] # pt5\n# params = [70, 10, 86, 39, 96, 1, 3, 94] # pt6\nmodel_list = [\n    {\n        \"model_path\": [\n            # deberta-xlarge\n            (\"../input/nbme-deberta-xlarge-4fold/dxl00_4fold.bin\", params[2]/4),\n            (\"../input/nbme-deberta-xlarge-4fold/dxl10_4fold.bin\", params[2]/4),\n            (\"../input/nbme-deberta-xlarge-4fold/dxl20_4fold.bin\", params[2]/4),\n            (\"../input/nbme-deberta-xlarge-4fold/dxl30_4fold.bin\", params[2]/4),\n            # pseudo deberta-xlarge\n            (\"../input/nbme-deberta-xlarge-pretrain/dxl03.bin\", params[3]/4),\n            (\"../input/nbme-deberta-xlarge-pretrain/dxl13.bin\", params[3]/4),\n            (\"../input/nbme-deberta-xlarge-pretrain/dxl23.bin\", params[3]/4),\n            (\"../input/nbme-deberta-xlarge-pretrain/dxl33.bin\", params[3]/4),\n        ],\n        \"pretrain_path\": \"../input/d/datasets/shujun717/deberta-xlarge\",\n    },\n    {\n        \"model_path\": [\n            # deberta-v3-large\n            (\"../input/nbme-deberta-v3-large-4fold/dvl01_4fold.bin\", params[0]/4),\n            (\"../input/nbme-deberta-v3-large-4fold/dvl12_4fold.bin\", params[0]/4),\n            (\"../input/nbme-deberta-v3-large-4fold/dvl22_4fold.bin\", params[0]/4),\n            (\"../input/nbme-deberta-v3-large-4fold/dvl30_4fold.bin\", params[0]/4),\n            # pseudo deberta-v3-large\n            (\"../input/nbme-deberta-v3-large-4fold/dvl03_4fold.bin\", params[1]/4),\n            (\"../input/nbme-deberta-xlarge-pretrain/dvl14.bin\", params[1]/4),\n            (\"../input/nbme-deberta-v3-large-4fold/dvl23_fold4.bin\", params[1]/4),\n            (\"../input/nbme-deberta-v3-large-4fold/dvl33_fold4.bin\", params[1]/4),\n            # pseudo round 2 deberta-v3-large\n#             (\"../input/nbme-deberta-v3-large/dvl05.bin\", params[2]/4),\n#             (\"../input/nbme-deberta-v3-large/dvl15.bin\", params[2]/4),\n#             (\"../input/nbme-deberta-v3-large/dvl25.bin\", params[2]/4),\n#             (\"../input/nbme-deberta-v3-large/dvl35.bin\", params[2]/4),\n        ],\n        \"pretrain_path\": \"../input/deberta-v3-large/deberta-v3-large\",\n    },\n    {\n        \"model_path\": [\n            # pretrain roberta-large\n            (\"../input/nbme-roberta-large/Fold_0.bin\", params[4]/4),\n            (\"../input/nbme-roberta-large/Fold_1.bin\", params[4]/4),\n            (\"../input/nbme-roberta-large/Fold_2.bin\", params[4]/4),\n            (\"../input/nbme-roberta-large/Fold_3.bin\", params[4]/4),\n            # pretrain + pseudo roberta-large\n            (\"../input/nbme-roberta-large/rl02.bin\", params[5]/4),\n            (\"../input/nbme-roberta-large/rl12.bin\", params[5]/4),\n            (\"../input/nbme-roberta-large/rl22.bin\", params[5]/4),\n            (\"../input/nbme-roberta-large/rl32.bin\", params[5]/4),\n            # pretrain + pseudo round 2 roberta-large\n#             (\"../input/nbme-roberta-large/rl04.bin\", params[7]/4),\n#             (\"../input/nbme-roberta-large/rl14.bin\", params[7]/4),\n#             (\"../input/nbme-roberta-large/rl24.bin\", params[7]/4),\n#             (\"../input/nbme-roberta-large/rl34.bin\", params[7]/4),\n        ],\n        \"pretrain_path\": \"../input/robertalarge\",\n    },\n]\ndefault_list = [209, 807, 911, 809, 207, 907, 215, 515, 502, 804, 910, 8, 100, 201, 206, 403, 903, 311, 310, 609, 905, 101, 510, 915, 506, 909, 5, 705, 511, 108, 505, 202, 703, 4, 404, 210, 312, 512, 305, 405, 803, 904, 608, 810, 402, 204, 106, 815, 813, 814, 514, 105, 610, 200, 704, 916, 109, 2, 508, 816, 900, 908, 7, 313, 706, 211, 306, 509, 513, 800, 812, 203, 214, 304, 307, 600, 516, 701, 806, 811]\np2r = [911, 510, 508, 200, 505, 313, 516, 201, 904, 511, 512, 813, 706, 803, 4, 608, 916, 502, 703, 2, 900, 100, 814, 910, 405]\nr2p = [207, 215, 509, 809, 402, 513, 8, 514, 204, 811, 214, 203, 810, 310, 306, 905, 903, 312, 311, 515, 307, 109, 610, 202, 600, 506, 815, 305, 806, 211, 101]\no = [807, 209, 206, 403, 609, 5, 106, 915, 108, 105, 704, 701, 404, 800, 816, 909, 7, 304, 210, 812, 705, 907, 908, 804]\ndefault_list = [\"0\" * (3 - len(str(item))) + str(item) for item in default_list]\np2r = [\"0\" * (3 - len(str(item))) + str(item) for item in p2r]\nr2p = [\"0\" * (3 - len(str(item))) + str(item) for item in r2p]\no = [\"0\" * (3 - len(str(item))) + str(item) for item in o]\n# for key in default_list:\n#     th_mp[str(key)] = 0.419\nfor key in p2r:\n    th_mp[str(key)] = 0.385\nfor key in r2p:\n    th_mp[str(key)] = 0.503\nfor key in o:\n    th_mp[str(key)] = 0.408\nmodel_num = 0\nfor model_config in model_list:\n    for model in model_config[\"model_path\"]:\n        model_num += model[1]\nfeatures_df = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/features.csv\")\npatient_df = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/patient_notes.csv\")\nvalid_df = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/test.csv\")\n# valid_df = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/train.csv\")\nvalid_df = valid_df.merge(features_df, on=[\"feature_num\", \"case_num\"], how=\"left\")\nvalid_df = valid_df.merge(patient_df, on=[\"pn_num\", \"case_num\"], how=\"left\")\nvalid_df['text_len'] = valid_df['pn_history'].apply(lambda x: len(x))\nvalid_df = valid_df.sort_values(by=\"text_len\").reset_index(drop=True)\ntext = valid_df[\"pn_history\"].tolist()\nresults = [np.zeros(len(item)) for item in text]\nfor model_config in model_list:\n    model_path = model_config[\"model_path\"]\n    pretrain_path = model_config[\"pretrain_path\"]\n    args.pretrain_path = pretrain_path\n    args.tokenizer_path = pretrain_path\n    predicter = Predicter(args)\n    predicter.model_init()\n    if \"deberta-v\" in args.tokenizer_path:\n        tokenizer = get_deberta_tokenizer(args.tokenizer_path)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path, trim_offsets=False)\n    valid_samples = prepare_test_data(valid_df, tokenizer, args.fix_length)\n    valid_datasets = NBMEDataset(valid_samples, tokenizer)\n    valid_collate = Collate(tokenizer)\n    for path in model_path:\n        predicter.model_load(path[0])\n        preds = predicter.predict(valid_datasets, valid_collate)\n        preds = preds * (path[1] / model_num)\n        char_probs = get_char_probs(valid_samples, preds)\n        for i in range(len(results)):\n            results[i] += char_probs[i]\n            # logging.info(f\"{len(results[i])} {len(char_probs[i])}\")\n        torch.cuda.empty_cache()\n        del preds, char_probs\n        gc.collect()\n    del valid_samples, valid_datasets\n    del predicter.model\n    del predicter\n    gc.collect()\nsubmission = valid_df\n# results = get_results_v2(results, text, th=0.50)\nresults = get_results_v3(results, text, valid_df[\"id\"].values.tolist(), th_mp, th=0.5)\nsubmission['location'] = results\ndisplay(submission.head())\nsubmission[['id', 'location']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T08:43:52.525736Z","iopub.execute_input":"2022-05-03T08:43:52.526032Z","iopub.status.idle":"2022-05-03T08:52:42.743961Z","shell.execute_reply.started":"2022-05-03T08:43:52.525997Z","shell.execute_reply":"2022-05-03T08:52:42.743152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[[\"id\", \"location\"]].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T08:52:42.745161Z","iopub.execute_input":"2022-05-03T08:52:42.745918Z","iopub.status.idle":"2022-05-03T08:52:42.756955Z","shell.execute_reply.started":"2022-05-03T08:52:42.745879Z","shell.execute_reply":"2022-05-03T08:52:42.756201Z"},"trusted":true},"execution_count":null,"outputs":[]}]}