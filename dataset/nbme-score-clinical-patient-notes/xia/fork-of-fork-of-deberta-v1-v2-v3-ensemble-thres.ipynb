{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this notebook\n- Deberta-base starter code\n- pip wheels is [here](https://www.kaggle.com/yasufuminakama/nbme-pip-wheels)\n- Training notebook is [here](https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train)\n\nIf this notebook is helpful, feel free to upvote :)","metadata":{}},{"cell_type":"markdown","source":"# CFG","metadata":{}},{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/debertav23fasttokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    \n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:59:29.064822Z","iopub.execute_input":"2022-04-19T16:59:29.065249Z","iopub.status.idle":"2022-04-19T16:59:29.09545Z","shell.execute_reply.started":"2022-04-19T16:59:29.06519Z","shell.execute_reply":"2022-04-19T16:59:29.094714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=1\n    path=\"../input/xlarge-5foldpsudo-4000/\"\n    tokenizer_path=\"../input/xlarge-5foldpsudo-4000/\"\n    config_path='../input/xlarge-5foldpsudo-4000/config.pth'\n    model=\"../input/deberta-xlarge/\"\n    batch_size=24\n    fc_dropout=0.2\n    max_len=466\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\nclass CFG1:\n    num_workers=1\n    path=\"../input/large-5foldpsudo-v3/\"\n    config_path='../input/large-5foldpsudo-v3/config.pth'\n    model=\"../input/debertalarge/\"\n    batch_size=30\n    fc_dropout=0.2\n    max_len=466\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n    \nclass CFG2:\n    num_workers=1\n    path=\"../input/v3-large-5fold-psudo-4000/\"\n    config_path='../input/v3-large-5fold-psudo-4000/config.pth'\n    model=\"../input/deberta-v3-large/deberta-v3-large\"\n    batch_size=64\n    fc_dropout=0.2\n    max_len=364\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n    \nclass CFG3:\n    num_workers=1\n    path=\"../input/v2-xlarge-psudo-v2-400/\"\n    config_path='../input/v2-xlarge-psudo-v2-400/config.pth'\n    model=\"../input/deberta-v2-xlarge\"\n    batch_size=30\n    fc_dropout=0.2\n    max_len=358\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n    \nclass CFG4:\n    num_workers=1\n    path=\"../input/d/datasets/xiamaozi11/v2-xxlarge/\"\n    config_path='../input/d/datasets/xiamaozi11/v2-xxlarge/config.pth'\n    model=\"../input/deberta-v2-xxlarge\"\n    batch_size=10\n    fc_dropout=0.2\n    max_len=358\n    seed=42\n    n_fold=4\n    trn_fold=[0, 2]\n\nclass CFG5:\n    num_workers=1\n    path=\"../input/v3-base-pretrained/\"\n    config_path='../input/v3-base-pretrained/config.pth'\n    model=\"../input/deberta-v3-base/deberta-v3-base\"\n    batch_size=64\n    fc_dropout=0.2\n    max_len=364\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]","metadata":{"papermill":{"duration":0.02543,"end_time":"2021-11-16T19:32:30.040766","exception":false,"start_time":"2021-11-16T19:32:30.015336","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T16:59:29.140445Z","iopub.execute_input":"2022-04-19T16:59:29.140655Z","iopub.status.idle":"2022-04-19T16:59:29.151833Z","shell.execute_reply.started":"2022-04-19T16:59:29.14062Z","shell.execute_reply":"2022-04-19T16:59:29.149371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# # This must be done before importing transformers\n# import shutil\n# from pathlib import Path\n\n# transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\n# input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\n# convert_file = input_dir / \"convert_slow_tokenizer.py\"\n# conversion_path = transformers_path/convert_file.name\n\n# if conversion_path.exists():\n#     conversion_path.unlink()\n\n# shutil.copy(convert_file, transformers_path)\n# deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\n# for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n#     filepath = deberta_v2_path/filename\n#     if filepath.exists():\n#         filepath.unlink()\n\n#     shutil.copy(input_dir/filename, filepath)\n# from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:59:29.209104Z","iopub.execute_input":"2022-04-19T16:59:29.209606Z","iopub.status.idle":"2022-04-19T16:59:29.214042Z","shell.execute_reply.started":"2022-04-19T16:59:29.209578Z","shell.execute_reply":"2022-04-19T16:59:29.21313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Library","metadata":{"papermill":{"duration":0.016162,"end_time":"2021-11-16T19:32:40.221507","exception":false,"start_time":"2021-11-16T19:32:40.205345","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\n# os.system('pip uninstall -y transformers')\n# os.system('python -m pip install --no-index --find-links=../input/nbme-pip-wheels transformers')\nimport tokenizers\nimport transformers\n# print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n# print(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"papermill":{"duration":30.77583,"end_time":"2021-11-16T19:33:11.013554","exception":false,"start_time":"2021-11-16T19:32:40.237724","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T16:59:29.288966Z","iopub.execute_input":"2022-04-19T16:59:29.289271Z","iopub.status.idle":"2022-04-19T16:59:29.303538Z","shell.execute_reply.started":"2022-04-19T16:59:29.289237Z","shell.execute_reply":"2022-04-19T16:59:29.302797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tokenizer","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\ntokenizer = AutoTokenizer.from_pretrained(\"../input/xlarge-pretrained-15epoch\")\nCFG.tokenizer = tokenizer\n\ntokenizer1 = AutoTokenizer.from_pretrained(\"../input/large-5foldpsudo-v3\")\nCFG1.tokenizer = tokenizer1\n\nfrom transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n\ntokenizer2 = DebertaV2TokenizerFast.from_pretrained('../input/v3-large-psudo-2000')\nCFG2.tokenizer = tokenizer2\n\ntokenizer3 = DebertaV2TokenizerFast.from_pretrained('../input/v2-xlarge-psudo-v2-400')\nCFG3.tokenizer = tokenizer3\n\ntokenizer4 = DebertaV2TokenizerFast.from_pretrained('../input/d/datasets/xiamaozi11/v2-xxlarge')\nCFG4.tokenizer = tokenizer4\n\ntokenizer5 = DebertaV2TokenizerFast.from_pretrained('../input/v3-base-pretrained')\nCFG5.tokenizer = tokenizer5","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:59:29.357878Z","iopub.execute_input":"2022-04-19T16:59:29.358167Z","iopub.status.idle":"2022-04-19T16:59:31.75172Z","shell.execute_reply.started":"2022-04-19T16:59:29.358141Z","shell.execute_reply":"2022-04-19T16:59:31.750967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions for scoring","metadata":{"papermill":{"duration":0.01915,"end_time":"2021-11-16T19:33:11.052159","exception":false,"start_time":"2021-11-16T19:33:11.033009","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n\ndef micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    return f1_score(truths, preds)\n\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n    return binary\n\n\ndef span_micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n    return micro_f1(bin_preds, bin_truths)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:59:31.753403Z","iopub.execute_input":"2022-04-19T16:59:31.753681Z","iopub.status.idle":"2022-04-19T16:59:31.762463Z","shell.execute_reply.started":"2022-04-19T16:59:31.753643Z","shell.execute_reply":"2022-04-19T16:59:31.7618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_labels_for_scoring(df):\n    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n        if len(location_list) > 0:\n            location = location_list[0]\n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n    return truths\n\n\ndef get_char_probs(texts, predictions, tokenizer):\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n    return results\n\n\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n    return results\ndef convert_offsets_to_word_indices(preds_offsets,texts, case_nums, feature_nums, th=0.5):\n    predicts = []\n    for text,preds,case_num, feature_num in zip(texts,preds_offsets, case_nums, feature_nums):\n#     text = oof['pn_history'][num]\n#         print(text)\n#         try:\n            encoded_text = tokenizer(text,\n                                         add_special_tokens=True,\n                                         max_length=CFG.max_len,\n                                         padding=\"max_length\",\n                                         return_offsets_mapping=True)\n            offset_mapping = encoded_text['offset_mapping']\n            sep_index = encoded_text[\"input_ids\"].index(tokenizer.sep_token_id)\n#             sample_pred_scores = preds\n            result=np.zeros(len(preds))\n#             results = np.zeros(sep_index)\n            \n            results = np.zeros(sep_index)\n            for idx, (offset, pred) in enumerate(zip(offset_mapping[:sep_index], preds)):\n                    start = offset[0]\n            #         end = offset_mapping[1]\n                    results[idx] = preds[start]\n            sample_pred_scores = results\n            if str(feature_num)[-1] == '3' and (str(case_num)=='0' or str(case_num)=='3' ):\n                result = [1 if s >= 0.54  else 0 for s in results]\n            elif    str(feature_num)[-1] == '3' and (str(case_num)=='1' ):\n                result = [1 if s >= 0.45 else 0 for s in results]\n            elif    str(feature_num)[-1] == '3' and   str(case_num)=='6':\n                result = [1 if s >= 0.52 else 0 for s in results]\n            elif str(case_num)=='5' and ( str(feature_num) == '503' ):\n                result = [1 if s >= 0.49  else 0 for s in results]\n            elif str(case_num)=='5' and ( str(feature_num) == '504' ):\n                result = [1 if s >= 0.4  else 0 for s in results]\n            elif str(case_num)=='5' and ( str(feature_num) == '508' ):\n                result = [1 if s >= 0.49 else 0 for s in results]\n            elif str(case_num)=='5' and ( str(feature_num) == '509' ):\n                result = [1 if s >= 0.4 else 0 for s in results]\n            elif str(case_num)=='5' and ( str(feature_num) == '510' ):\n                result = [1 if s >=0.55 else 0 for s in results]\n            elif str(case_num)=='5' and ( str(feature_num) == '511' ):\n                result = [1 if s >=0.55 else 0 for s in results]\n            elif str(case_num)=='5' and ( str(feature_num) == '512' ):\n                result = [1 if s >=0.45 else 0 for s in results]\n            elif str(case_num)=='5' and ( str(feature_num) == '513' ):\n                result = [1 if s >=0.59 else 0 for s in results]\n            elif str(case_num)=='5' and ( str(feature_num) == '514' ):\n                result = [1 if s >=0.49 else 0 for s in results]\n            elif str(case_num)=='5' and ( str(feature_num) == '516' ):\n                result = [1 if s >=0.41 else 0 for s in results]\n            elif str(case_num)=='7' and ( str(feature_num) == '702' ):\n                result = [1 if s >=0.56 else 0 for s in results]\n#             elif str(case_num)=='9':\n#                 result = [1 if s >=0.54 else 0 for s in results]\n#             elif str(case_num)=='0':\n#                 result = [1 if s >=0.4 else 0 for s in results]\n#             elif str(case_num)=='3':\n#                 result = [1 if s >=0.52 else 0 for s in results]\n#             elif str(case_num)=='4':\n#                 result = [1 if s >=0.52 else 0 for s in results]\n#             elif str(case_num)=='7':\n#                 result = [1 if s >=0.55 else 0 for s in results]\n            else:\n\n                result = [1 if s >= 0.47  else 0 for s in results]\n\n#             result = [1 if s >= 0.47  else 0 for s in results]\n            preds = result\n            \n            sample_text = text\n            sample_input_ids = encoded_text[\"input_ids\"]\n\n            sample_preds = []\n\n            if len(preds) < len(offset_mapping):\n                preds = preds + [0] * (len(offset_mapping) - len(preds))\n                sample_pred_scores = list(sample_pred_scores) + [0] * (len(offset_mapping) - len(sample_pred_scores))\n\n            idx = 0\n            phrase_preds = []\n            while idx < sep_index:\n                start, _ = offset_mapping[idx]\n            #     print(start,idx)\n                if preds[idx] != 0:\n                    label = preds[idx]\n                else:\n                    label = 0\n                phrase_scores = []\n                phrase_scores.append(sample_pred_scores[idx])\n            #     idx += 1\n                while idx < sep_index:\n                    if label == 0:\n                        matching_label = 0\n                    else:\n                        matching_label = 1\n                    if preds[idx] == matching_label:\n                        _, end = offset_mapping[idx]\n                        phrase_scores.append(sample_pred_scores[idx])\n                        idx += 1\n                    else:\n                        break\n                if \"end\" in locals():\n                    phrase = sample_text[start:end]\n                    phrase_preds.append((phrase, start, end, label, phrase_scores))\n            temp = []\n            for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n                newlabel = ''\n                word_start = len(sample_text[:start].split())\n                word_end = word_start + len(sample_text[start:end].split())\n                word_end = min(word_end, len(sample_text.split()))\n                ps = \" \".join([str(x) for x in range(word_start, word_end)])\n\n                if label != 0:\n                    if sum(phrase_scores) / len(phrase_scores) >= th and word_end !=0:\n                        if start!=0 and sample_text[start]==' ':\n                            start+=1\n\n                        temp.append(np.array([start,end]))\n            if len(temp)==0:\n                temp = [list(g) for _, g in itertools.groupby(temp, key=lambda n, c=itertools.count(): n - next(c))]\n                temp = [f\"{min(r)} {max(r)}\" for r in temp]\n            #                 print(temp)\n            else:\n            #                 print(temp)\n            #                 print(type(temp[0][0]))\n                temp = [f\"{min(r)} {max(r)}\" for r in temp]\n            #                 print(temp)\n            predict = \";\".join(temp)\n            predicts.append(predict)\n    return predicts\n\ndef get_predictions(results):\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:59:31.76378Z","iopub.execute_input":"2022-04-19T16:59:31.764171Z","iopub.status.idle":"2022-04-19T16:59:31.796941Z","shell.execute_reply.started":"2022-04-19T16:59:31.764136Z","shell.execute_reply":"2022-04-19T16:59:31.796308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = span_micro_f1(y_true, y_pred)\n    return score\n\n\ndef get_logger(filename='inference'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"papermill":{"duration":0.034649,"end_time":"2021-11-16T19:33:11.105766","exception":false,"start_time":"2021-11-16T19:33:11.071117","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T16:59:31.799013Z","iopub.execute_input":"2022-04-19T16:59:31.799415Z","iopub.status.idle":"2022-04-19T16:59:31.811087Z","shell.execute_reply.started":"2022-04-19T16:59:31.799381Z","shell.execute_reply":"2022-04-19T16:59:31.810363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OOF","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# oof\n# ====================================================\n# oof = pd.read_pickle(CFG.tokenizer_path+'/oof_df.pkl')\n\n# truths = create_labels_for_scoring(oof)\n# char_probs = get_char_probs(oof['pn_history'].values,\n#                             oof[[i for i in range(CFG.max_len)]].values, \n#                             CFG.tokenizer)\nbest_th = 0.5\nbest_score = 0.\n# for th in np.arange(0.45, 0.55, 0.01):\n#     th = np.round(th, 2)\n#     results = get_results(char_probs, th=th)\n#     preds = get_predictions(results)\n#     score = get_score(preds, truths)\n#     if best_score < score:\n#         best_th = th\n#         best_score = score\n#     LOGGER.info(f\"th: {th}  score: {score:.5f}\")\n# LOGGER.info(f\"best_th: {best_th}  score: {best_score:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:59:31.814207Z","iopub.execute_input":"2022-04-19T16:59:31.814453Z","iopub.status.idle":"2022-04-19T16:59:31.821961Z","shell.execute_reply.started":"2022-04-19T16:59:31.814429Z","shell.execute_reply":"2022-04-19T16:59:31.821199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# oof = pd.read_pickle('../input/nnme-deberta-large/deberta-largeoof_df.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:59:31.823501Z","iopub.execute_input":"2022-04-19T16:59:31.823926Z","iopub.status.idle":"2022-04-19T16:59:31.830187Z","shell.execute_reply.started":"2022-04-19T16:59:31.823889Z","shell.execute_reply":"2022-04-19T16:59:31.829281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{"papermill":{"duration":0.018406,"end_time":"2021-11-16T19:33:11.150174","exception":false,"start_time":"2021-11-16T19:33:11.131768","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\ntest = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\nsubmission = pd.read_csv('../input/nbme-score-clinical-patient-notes/sample_submission.csv')\nfeatures = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\nfeatures = preprocess_features(features)\npatient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n\nprint(f\"test.shape: {test.shape}\")\ndisplay(test.head())\nprint(f\"features.shape: {features.shape}\")\ndisplay(features.head())\nprint(f\"patient_notes.shape: {patient_notes.shape}\")\ndisplay(patient_notes.head())","metadata":{"papermill":{"duration":0.637101,"end_time":"2021-11-16T19:33:11.805349","exception":false,"start_time":"2021-11-16T19:33:11.168248","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T16:59:31.831583Z","iopub.execute_input":"2022-04-19T16:59:31.832317Z","iopub.status.idle":"2022-04-19T16:59:32.549301Z","shell.execute_reply.started":"2022-04-19T16:59:31.832282Z","shell.execute_reply":"2022-04-19T16:59:32.548484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.merge(features, on=['feature_num', 'case_num'], how='left')\ntest = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:59:32.550635Z","iopub.execute_input":"2022-04-19T16:59:32.550974Z","iopub.status.idle":"2022-04-19T16:59:32.591613Z","shell.execute_reply.started":"2022-04-19T16:59:32.550935Z","shell.execute_reply":"2022-04-19T16:59:32.590813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\n# def prepare_input(cfg, text, feature_text):\n#     inputs = cfg.tokenizer(text, feature_text, \n#                            add_special_tokens=True,\n#                            max_length=cfg.max_len,\n#                            padding=\"max_length\",\n#                            return_offsets_mapping=False)\n#     for k, v in inputs.items():\n#         inputs[k] = torch.tensor(v, dtype=torch.long)\n#     return inputs\n\n\n# class TestDataset(Dataset):\n#     def __init__(self, cfg, df):\n#         self.cfg = cfg\n#         self.feature_texts = df['feature_text'].values\n#         self.pn_historys = df['pn_history'].values\n\n#     def __len__(self):\n#         return len(self.feature_texts)\n\n#     def __getitem__(self, item):\n#         inputs = prepare_input(self.cfg, \n#                                self.pn_historys[item], \n#                                self.feature_texts[item])\n#         return inputs","metadata":{"papermill":{"duration":0.040128,"end_time":"2021-11-16T19:33:20.931029","exception":false,"start_time":"2021-11-16T19:33:20.890901","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T16:59:32.593051Z","iopub.execute_input":"2022-04-19T16:59:32.593326Z","iopub.status.idle":"2022-04-19T16:59:32.597314Z","shell.execute_reply.started":"2022-04-19T16:59:32.593291Z","shell.execute_reply":"2022-04-19T16:59:32.596624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text, feature_text, batch_max_len):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           max_length=batch_max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n        self.batch_max_len = df['batch_max_length'].values\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item],\n                               self.batch_max_len[item],\n                              )\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:59:32.600487Z","iopub.execute_input":"2022-04-19T16:59:32.600958Z","iopub.status.idle":"2022-04-19T16:59:32.610837Z","shell.execute_reply.started":"2022-04-19T16:59:32.600921Z","shell.execute_reply":"2022-04-19T16:59:32.610043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.02209,"end_time":"2021-11-16T19:33:20.978793","exception":false,"start_time":"2021-11-16T19:33:20.956703","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"papermill":{"duration":0.032939,"end_time":"2021-11-16T19:33:21.034275","exception":false,"start_time":"2021-11-16T19:33:21.001336","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T16:59:32.613902Z","iopub.execute_input":"2022-04-19T16:59:32.614399Z","iopub.status.idle":"2022-04-19T16:59:32.627084Z","shell.execute_reply.started":"2022-04-19T16:59:32.61437Z","shell.execute_reply":"2022-04-19T16:59:32.626382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomModel1(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n        self.config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        self.output = nn.Linear(self.config.hidden_size, 1)\n\n        LSTM_SIZE = 1024\n        self.Lstm = nn.LSTM(self.config.hidden_size * 3, LSTM_SIZE // 2, bidirectional=True, batch_first=True) # , num_layers=2\n\n        self._init_weights(self.output)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs):\n#         feature = self.feature(inputs)\n#         output = self.fc(self.fc_dropout(feature))\n        \n        outputs = self.model(**inputs)\n        concat_output = outputs.hidden_states[-1]\n        # print('transformer_out: ', len(transformer_out[2]))\n#         concat_output = torch.cat((hidden_states[-1], hidden_states[-2], hidden_states[-3]), 2)\n        concat_output = self.dropout(concat_output)\n#         self.Lstm.flatten_parameters()\n#         lstm_output, _ = self.Lstm(concat_output)\n#         concat_output = torch.cat((concat_output, lstm_output), 2)\n\n        logits1 = self.output(self.dropout1(concat_output))\n        logits2 = self.output(self.dropout2(concat_output))\n        logits3 = self.output(self.dropout3(concat_output))\n        logits4 = self.output(self.dropout4(concat_output))\n        logits5 = self.output(self.dropout5(concat_output))\n\n        output = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n        \n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:59:32.628392Z","iopub.execute_input":"2022-04-19T16:59:32.629015Z","iopub.status.idle":"2022-04-19T16:59:32.647903Z","shell.execute_reply.started":"2022-04-19T16:59:32.628979Z","shell.execute_reply":"2022-04-19T16:59:32.647081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# inference","metadata":{"papermill":{"duration":0.022058,"end_time":"2021-11-16T19:33:21.081885","exception":false,"start_time":"2021-11-16T19:33:21.059827","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\n# def inference_fn(test_loader, model, device):\n#     preds = []\n#     model.eval()\n#     model.to(device)\n#     tk0 = tqdm(test_loader, total=len(test_loader))\n#     for inputs in tk0:\n#         for k, v in inputs.items():\n#             inputs[k] = v.to(device)\n#         with torch.no_grad():\n#             y_preds = model(inputs)\n#         preds.append(y_preds.sigmoid().to('cpu').numpy())\n#     predictions = np.concatenate(preds)\n#     return predictions","metadata":{"papermill":{"duration":0.044153,"end_time":"2021-11-16T19:33:21.148373","exception":false,"start_time":"2021-11-16T19:33:21.10422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T16:59:32.6508Z","iopub.execute_input":"2022-04-19T16:59:32.650983Z","iopub.status.idle":"2022-04-19T16:59:32.660192Z","shell.execute_reply.started":"2022-04-19T16:59:32.650959Z","shell.execute_reply":"2022-04-19T16:59:32.659349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device,cfg):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n    # for inputs in test_loader:\n        bs = len(inputs['input_ids'])\n        pred_w_pad = np.zeros((bs, cfg.max_len, 1))\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        y_preds = y_preds.sigmoid().to('cpu').numpy()\n        pred_w_pad[:, :y_preds.shape[1]] = y_preds\n        preds.append(pred_w_pad)\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:59:32.661588Z","iopub.execute_input":"2022-04-19T16:59:32.662159Z","iopub.status.idle":"2022-04-19T16:59:32.671056Z","shell.execute_reply.started":"2022-04-19T16:59:32.662122Z","shell.execute_reply":"2022-04-19T16:59:32.67027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Reduce Padding Inference ######\n\n# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG4.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG4.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG4, sort_df)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG4.batch_size,\n                         shuffle=False,\n                         num_workers=CFG4.num_workers, pin_memory=True, drop_last=False)\npredictions_v2_xxlarge = []\n# predictions1_v2 = []\nfor fold in CFG4.trn_fold:\n    model = CustomModel1(CFG4, config_path=CFG4.config_path, pretrained=False)\n    \n    \n    state = torch.load(CFG4.path+f\"user_data-deberta-v2-xxlarge-pretrained-20epoch_fold{fold}_best.pth\")\n       \n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device, CFG4)\n    prediction = prediction.reshape((len(test), CFG4.max_len))\n    prediction = prediction[np.argsort(length_sorted_idx)]\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG4.tokenizer)\n    predictions_v2_xxlarge.append(char_probs)\n#     predictions1_v3.append(prediction)\n    del model, state, prediction, char_probs\n    gc.collect()\n    torch.cuda.empty_cache()\n    \npredictions_v2_xxlarge = np.mean(predictions_v2_xxlarge, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Reduce Padding Inference ######\n\n# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length\n# test = sort_df","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:59:32.672206Z","iopub.execute_input":"2022-04-19T16:59:32.672929Z","iopub.status.idle":"2022-04-19T16:59:32.727032Z","shell.execute_reply.started":"2022-04-19T16:59:32.672893Z","shell.execute_reply":"2022-04-19T16:59:32.726409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, sort_df)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions_xlarge = []\n# predictions1_xlarge = []\nfor fold in CFG.trn_fold:\n    model = CustomModel1(CFG, config_path=CFG.config_path, pretrained=False)\n    \n    \n    state = torch.load(CFG.path+f\"user_data-deberta-xlarge-pretrained-15epoch_fold{fold}_best.pth\")\n       \n       \n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device,CFG)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    prediction = prediction[np.argsort(length_sorted_idx)]\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions_xlarge.append(char_probs)\n#     predictions1_xlarge.append(prediction)\n    del model, state, prediction, char_probs\n    gc.collect()\n#     torch.cuda.empty_cache()\n    \npredictions_xlarge = np.mean(predictions_xlarge, axis=0)\n# predictions1_xlarge = np.mean(predictions1_xlarge, axis=0)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-19T16:59:32.728156Z","iopub.execute_input":"2022-04-19T16:59:32.728595Z","iopub.status.idle":"2022-04-19T17:05:33.074362Z","shell.execute_reply.started":"2022-04-19T16:59:32.728557Z","shell.execute_reply":"2022-04-19T17:05:33.073493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG1.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG1.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:05:33.079794Z","iopub.execute_input":"2022-04-19T17:05:33.081755Z","iopub.status.idle":"2022-04-19T17:05:33.171767Z","shell.execute_reply.started":"2022-04-19T17:05:33.081712Z","shell.execute_reply":"2022-04-19T17:05:33.171182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG1, sort_df)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG1.batch_size,\n                         shuffle=False,\n                         num_workers=CFG1.num_workers, pin_memory=True, drop_last=False)\npredictions_large = []\npredictions1_large = []\nfor fold in CFG1.trn_fold:\n    model = CustomModel1(CFG1, config_path=CFG1.config_path, pretrained=False)\n    \n    state = torch.load(CFG1.path+f\"user_data-deberta-large-pretrained_fold{fold}_best.pth\")\n       \n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device,CFG1)\n    prediction = prediction.reshape((len(test), CFG1.max_len))\n    prediction = prediction[np.argsort(length_sorted_idx)]\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG1.tokenizer)\n    predictions_large.append(char_probs)\n#     predictions1_large.append(prediction)\n    del model, state, prediction, char_probs\n    gc.collect()\n    torch.cuda.empty_cache()\n    \npredictions_large = np.mean(predictions_large, axis=0)\n# predictions1_large = np.mean(predictions1_large, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:05:33.175059Z","iopub.execute_input":"2022-04-19T17:05:33.177078Z","iopub.status.idle":"2022-04-19T17:08:21.384131Z","shell.execute_reply.started":"2022-04-19T17:05:33.177044Z","shell.execute_reply":"2022-04-19T17:08:21.383293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG2.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG2.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:08:21.388553Z","iopub.execute_input":"2022-04-19T17:08:21.38891Z","iopub.status.idle":"2022-04-19T17:08:21.440596Z","shell.execute_reply.started":"2022-04-19T17:08:21.388877Z","shell.execute_reply":"2022-04-19T17:08:21.439936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG2, sort_df)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG2.batch_size,\n                         shuffle=False,\n                         num_workers=CFG2.num_workers, pin_memory=True, drop_last=False)\npredictions_v3 = []\npredictions1_v3 = []\nfor fold in CFG2.trn_fold:\n    model = CustomModel1(CFG2, config_path=CFG2.config_path, pretrained=False)\n    \n    state = torch.load(CFG2.path+f\"deberta-v3-large-pretrained_fold{fold}_best.pth\")\n       \n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device, CFG2)\n    prediction = prediction.reshape((len(test), CFG2.max_len))\n    prediction = prediction[np.argsort(length_sorted_idx)]\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG2.tokenizer)\n    predictions_v3.append(char_probs)\n#     predictions1_v3.append(prediction)\n    del model, state, prediction, char_probs\n    gc.collect()\n    torch.cuda.empty_cache()\n    \npredictions_v3 = np.mean(predictions_v3, axis=0)\n# predictions1_v3 = np.mean(predictions1_v3, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:08:21.441964Z","iopub.execute_input":"2022-04-19T17:08:21.44242Z","iopub.status.idle":"2022-04-19T17:10:54.840087Z","shell.execute_reply.started":"2022-04-19T17:08:21.442383Z","shell.execute_reply":"2022-04-19T17:10:54.839201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sort by token num\n# input_lengths = []\n# tk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\n# for text, feature_text in tk0:\n#     length = len(CFG5.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n#     input_lengths.append(length)\n# test['input_lengths'] = input_lengths\n# length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# # sort dataframe\n# sort_df = test.iloc[length_sorted_idx]\n\n# # calc max_len per batch\n# sorted_input_length = sort_df['input_lengths'].values\n# batch_max_length = np.zeros_like(sorted_input_length)\n# bs = CFG5.batch_size\n# for i in range((len(sorted_input_length)//bs)+1):\n#     batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \n# sort_df['batch_max_length'] = batch_max_length\n# test_dataset = TestDataset(CFG5, sort_df)\n# test_loader = DataLoader(test_dataset,\n#                          batch_size=CFG5.batch_size,\n#                          shuffle=False,\n#                          num_workers=CFG5.num_workers, pin_memory=True, drop_last=False)\n# predictions_v3_base = []\n# # predictions1_v3 = []\n# for fold in CFG5.trn_fold:\n#     model = CustomModel1(CFG5, config_path=CFG5.config_path, pretrained=False)\n    \n#     state = torch.load(CFG5.path+f\"user_data-deberta-v3-base-pretrained_fold{fold}_best.pth\")\n       \n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn(test_loader, model, device, CFG5)\n#     prediction = prediction.reshape((len(test), CFG5.max_len))\n#     prediction = prediction[np.argsort(length_sorted_idx)]\n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG5.tokenizer)\n#     predictions_v3_base.append(char_probs)\n# #     predictions1_v3.append(prediction)\n#     del model, state, prediction, char_probs\n#     gc.collect()\n#     torch.cuda.empty_cache()\n    \n# predictions_v3_base = np.mean(predictions_v3_base, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG3.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG3.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:10:54.849105Z","iopub.execute_input":"2022-04-19T17:10:54.853509Z","iopub.status.idle":"2022-04-19T17:10:54.950605Z","shell.execute_reply.started":"2022-04-19T17:10:54.853458Z","shell.execute_reply":"2022-04-19T17:10:54.949966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG3, sort_df)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG3.batch_size,\n                         shuffle=False,\n                         num_workers=CFG3.num_workers, pin_memory=True, drop_last=False)\npredictions_v2 = []\n# predictions1_v2 = []\nfor fold in CFG3.trn_fold:\n    model = CustomModel1(CFG3, config_path=CFG3.config_path, pretrained=False)\n    \n        \n    state = torch.load(CFG3.path+f\"user_data-deberta-v2-xlarge-pretrained-20epoch_fold{fold}_best.pth\")\n       \n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device, CFG3)\n    prediction = prediction.reshape((len(test), CFG3.max_len))\n    prediction = prediction[np.argsort(length_sorted_idx)]\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG3.tokenizer)\n    predictions_v2.append(char_probs)\n#     predictions1_v3.append(prediction)\n    del model, state, prediction, char_probs\n    gc.collect()\n    torch.cuda.empty_cache()\n    \npredictions_v2 = np.mean(predictions_v2, axis=0)\n# predictions1_v3 = np.mean(predictions1_v3, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:10:54.951672Z","iopub.execute_input":"2022-04-19T17:10:54.952022Z","iopub.status.idle":"2022-04-19T17:17:52.664434Z","shell.execute_reply.started":"2022-04-19T17:10:54.951988Z","shell.execute_reply":"2022-04-19T17:17:52.66364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = predictions_large\nfor i in range(len(predictions)):\n#     predictions[i] = (predictions_xlarge[i]+ predictions_large[i]+ predictions_v3[i])/3\n        predictions[i] = (predictions_large[i]+ predictions_xlarge[i] + predictions_v3[i]  + predictions_v2[i] + predictions_v2_xxlarge[i])/5","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:17:52.666474Z","iopub.execute_input":"2022-04-19T17:17:52.666746Z","iopub.status.idle":"2022-04-19T17:17:52.672184Z","shell.execute_reply.started":"2022-04-19T17:17:52.666709Z","shell.execute_reply":"2022-04-19T17:17:52.67147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_n_space(text):\n    count = 0\n    cont = True\n    \n    if text[0]==' ':\n        count+=1\n    return count\n\n\n\ndef get_results(char_probs, texts,th=0.5):\n    results = []\n    for idx,char_prob in enumerate(char_probs):\n        result = np.where(char_prob >= th)[0] #+ 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        new_res = []\n        text = texts[idx]\n        for r in result:\n            start = min(r)\n            end = max(r)\n            word_start = len(text[:start].split())\n            \n            sample_text =  text[start:end+1]\n            n_space = count_n_space(sample_text)\n            start = start+n_space\n            new_res.append(f\"{start} {end+1}\")   \n        result = new_res        \n        result = \";\".join(result)\n        results.append(result)\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:17:52.673568Z","iopub.execute_input":"2022-04-19T17:17:52.674098Z","iopub.status.idle":"2022-04-19T17:17:52.684932Z","shell.execute_reply.started":"2022-04-19T17:17:52.674002Z","shell.execute_reply":"2022-04-19T17:17:52.684055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"results = convert_offsets_to_word_indices(predictions, test['pn_history'].values, test['case_num'].values,test['feature_num'].values,th=0.48)\n# results = get_results(predictions,test['pn_history'].values,th=0.48)\ntemp = []\nfor pred in results: \n    if len(pred)>0 and pred[0:2] == '1 ': \n        pred = '0' + pred[1:]\n    temp.append(pred)\nresults = temp\n\nsubmission['location'] = results\ndisplay(submission.head())\nsubmission[['id', 'location']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:17:52.686237Z","iopub.execute_input":"2022-04-19T17:17:52.686556Z","iopub.status.idle":"2022-04-19T17:17:52.735123Z","shell.execute_reply.started":"2022-04-19T17:17:52.686516Z","shell.execute_reply":"2022-04-19T17:17:52.734499Z"},"trusted":true},"execution_count":null,"outputs":[]}]}