{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 概要\nこのノートブックは[Pytorch Bert baseline NBME by Shudipto Trafder](https://www.kaggle.com/iamsdt/pytorch-bert-baseline-nbme/notebook)を非常に参考にしながら，各コードの動作や意図についての私の理解やメモをまとめたものです．\n\n私が上のノートブックを理解するまでに学んだことや調べたことを書いています．私と同じような初心者の方の参考になれば幸いです．\n\n間違いの指摘や質問などあればコメントからお願いします．\n\nもしこのノートブックを気に入っていただけたら，UP VOTEをお願いします🎉","metadata":{}},{"cell_type":"code","source":"from ast import literal_eval\nfrom itertools import chain\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModel, AutoTokenizer","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-03-02T02:59:11.109094Z","iopub.execute_input":"2022-03-02T02:59:11.109499Z","iopub.status.idle":"2022-03-02T02:59:13.534794Z","shell.execute_reply.started":"2022-03-02T02:59:11.109471Z","shell.execute_reply":"2022-03-02T02:59:13.533924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# データの確認","metadata":{}},{"cell_type":"code","source":"features = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/features.csv\")\nnotes = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/patient_notes.csv\")\ntrain = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/train.csv\")\ntest = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-02T02:59:13.537513Z","iopub.execute_input":"2022-03-02T02:59:13.538022Z","iopub.status.idle":"2022-03-02T02:59:14.19689Z","shell.execute_reply.started":"2022-03-02T02:59:13.537982Z","shell.execute_reply":"2022-03-02T02:59:14.196141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`patient_notes.csv`にはpatient notes（カルテ）が入っています．各カルテには`pn_num`（カルテ番号），`case_num`（症状番号）が割り振られています．","metadata":{}},{"cell_type":"code","source":"notes","metadata":{"execution":{"iopub.status.busy":"2022-03-02T02:59:14.198058Z","iopub.execute_input":"2022-03-02T02:59:14.198334Z","iopub.status.idle":"2022-03-02T02:59:14.219929Z","shell.execute_reply.started":"2022-03-02T02:59:14.198294Z","shell.execute_reply":"2022-03-02T02:59:14.219199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`features.csv`には，各caseごとに重要なfeature（特徴，症状）が入っています．`feature_text`はそのfeatureの説明で，例えば\"Chest-pressure\"（胸の圧迫感）や\"Photophobia\"（強い光を受けた際に不快感や眼の痛みなどを生じること）などがあります．\n\n`feature_num`の先頭は`case_num`に対応しています（`case_num`が0の場合を除く）．","metadata":{}},{"cell_type":"code","source":"features","metadata":{"execution":{"iopub.status.busy":"2022-03-02T02:59:14.222031Z","iopub.execute_input":"2022-03-02T02:59:14.22299Z","iopub.status.idle":"2022-03-02T02:59:14.235993Z","shell.execute_reply.started":"2022-03-02T02:59:14.222956Z","shell.execute_reply":"2022-03-02T02:59:14.234942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`train.csv`には，カルテの中で各featureに対応する記述`annotation`とその位置`location`が含まれています．\n\nまたannotaionやlocationの中身は`\"['dad with recent heart attcak']\"`や`\"['696 724']\"`のような文字列になっています．\n","metadata":{}},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2022-03-02T02:59:14.237615Z","iopub.execute_input":"2022-03-02T02:59:14.238215Z","iopub.status.idle":"2022-03-02T02:59:14.254897Z","shell.execute_reply.started":"2022-03-02T02:59:14.238126Z","shell.execute_reply":"2022-03-02T02:59:14.254122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"例えば，`id`が`00016_002`の`annotaion`は`\"chest pressure\"`，`location`は`[203 217]`となっています．\n\n実際，カルテの中から`location`の範囲を取り出してみると，`annnotaion`に一致しています．","metadata":{}},{"cell_type":"code","source":"notes[notes[\"pn_num\"]==16][\"pn_history\"].iloc[0][203:217]","metadata":{"execution":{"iopub.status.busy":"2022-03-02T02:59:14.256215Z","iopub.execute_input":"2022-03-02T02:59:14.256488Z","iopub.status.idle":"2022-03-02T02:59:14.266433Z","shell.execute_reply.started":"2022-03-02T02:59:14.256455Z","shell.execute_reply":"2022-03-02T02:59:14.265624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"featureによってはカルテ中にannotationが存在しない場合もあります．\n\n逆に１つのfeatureに対して複数のannotaionがあったり，1つのannotationが複数箇所に別れて存在している場合もあります．","metadata":{}},{"cell_type":"code","source":"train[(train[\"id\"]==\"00211_000\")|(train[\"id\"]==\"00100_010\")]","metadata":{"execution":{"iopub.status.busy":"2022-03-02T02:59:14.267914Z","iopub.execute_input":"2022-03-02T02:59:14.268445Z","iopub.status.idle":"2022-03-02T02:59:14.287231Z","shell.execute_reply.started":"2022-03-02T02:59:14.268407Z","shell.execute_reply":"2022-03-02T02:59:14.285843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"また，すべてのカルテに対してannotationが用意されているわけではないようです．`train.csv`に含まれている，つまりannotationが存在しているカルテの数を調べてみます．","metadata":{}},{"cell_type":"code","source":"len(train[\"pn_num\"].unique())","metadata":{"execution":{"iopub.status.busy":"2022-03-02T02:59:14.289021Z","iopub.execute_input":"2022-03-02T02:59:14.289214Z","iopub.status.idle":"2022-03-02T02:59:14.298456Z","shell.execute_reply.started":"2022-03-02T02:59:14.289192Z","shell.execute_reply":"2022-03-02T02:59:14.297682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`test.csv`には予測対象のデータが入っています．各行にはカルテ番号`pn_num`，case番号`case_num`，feature番号`feature_num`が含まれています．\n\nなおこのファイルはダミーで，提出時に本物のファイルに置き換えられます．","metadata":{}},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2022-03-02T02:59:14.299866Z","iopub.execute_input":"2022-03-02T02:59:14.300653Z","iopub.status.idle":"2022-03-02T02:59:14.310221Z","shell.execute_reply.started":"2022-03-02T02:59:14.300616Z","shell.execute_reply":"2022-03-02T02:59:14.309503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ここではデータの中身やフォーマットを確認しました．詳細やEDAに関してはすでに様々なノートブックが存在しています．\n\n* https://www.kaggle.com/drcapa/nbme-starter\n* https://www.kaggle.com/utcarshagrawal/nbme-complete-eda\n* https://www.kaggle.com/yufuin/nbme-japanese","metadata":{}},{"cell_type":"markdown","source":"この先，自然言語処理とHuggingFaceの各種ライブラリ（主にTransformers）の知識がある程度必要になってきます．\n\n私は完全に素人ですが，このコンペに参加する直前に偶然読んでいた[HuggingFaceのコース](https://huggingface.co/course)のおかげで自然言語処理や🤗 Transformersの最低限の知識が得られました（得られた気がしています）．長すぎずわかりやすい内容なので非常におすすめです．\n\nより詳細な説明は[🤗のドキュメント](https://huggingface.co/docs)や，[Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098103231/)などが参考になります．","metadata":{}},{"cell_type":"markdown","source":"# 前処理・Dataset作成\n## QA/NERハイブリッドアプローチ\n\n前処理やDataset作成の前に，モデルの入出力を把握する必要があります．\n\nこのノートブックのベースになっている[Pytorch Bert baseline NBME](https://www.kaggle.com/iamsdt/pytorch-bert-baseline-nbme/notebook)では[QA/NER hybrid train 🚆](https://www.kaggle.com/nbroad/qa-ner-hybrid-train-nbme)のノートブックで紹介されているQA(Question Answering)とNER(Named Entity Recognition)を組み合わせたアプローチを利用しています．\n\nこのアプローチでは，feature_text（featureの説明）とカルテを分類モデルに入力します．\nこの分類モデルはカルテのあるtokenがfeatureに関して重要な記述であるか，つまりannotationに含まれるかどうかを判定（0 or 1）します．\nカルテのすべてのtokenに対して判定を行い，連続して重要であると判定されたtokenをまとめて1つの予測annotationを作ります．\n\nつまりDataset作成では入力としてfeature_textとカルテのテキスト（それぞれをtokenizeしたもの）と，ラベルとしてカルテの各tokenがannotationに含まれているかどうかを表す0 or 1のリストを作成します．","metadata":{}},{"cell_type":"markdown","source":"## 前処理\n\n3つのcsvファイルを読み込んでマージし，次の前処理を行います．\n\n1. `ast.literal_eval`で`trian.csv`のannotationとlocationを文字列からリストに変換\n1. feature_textに含まれている`\"-OR-\"`を`\"; \"`に，`\"-\"`を`\" \"`に置き換える\n1. feature_textとpn_history（カルテ文）に含まれれる文字をすべて小文字に変換．\n\n今回利用するモデルはuncasedのBERTなので，大文字と小文字を区別しません．","metadata":{}},{"cell_type":"code","source":"BASE_URL = \"../input/nbme-score-clinical-patient-notes\"\n\n\ndef process_feature_text(text):\n    return text.replace(\"-OR-\", \"; \").replace(\"-\", \" \")\n\n\ndef prepare_datasets():\n    features = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    train = pd.read_csv(f\"{BASE_URL}/train.csv\")\n\n    merged = train.merge(notes, how=\"left\")\n    merged = merged.merge(features, how=\"left\")\n\n    merged[\"annotation_list\"] = [literal_eval(x) for x in merged[\"annotation\"]]\n    merged[\"location_list\"] = [literal_eval(x) for x in merged[\"location\"]]\n    \n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    \n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n\n    return merged\n\ntrain_df = prepare_datasets()\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T02:59:14.31258Z","iopub.execute_input":"2022-03-02T02:59:14.313063Z","iopub.status.idle":"2022-03-02T02:59:15.087768Z","shell.execute_reply.started":"2022-03-02T02:59:14.312995Z","shell.execute_reply":"2022-03-02T02:59:15.087076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset作成\n\n`train_df`からPytorch Datasetを作成します．最初に説明したように入力としてfeature_textとカルテのテキスト（それぞれをtokenizeしたもの）と，ラベルとしてカルテのテキストのtokenがannotationに含まれているかどうかを表す0 or 1のリストを作成します．\n\n複雑な処理なので，まずは次のような簡単な例を使って処理の流れを説明します．","metadata":{}},{"cell_type":"code","source":"example = train_df.loc[14242]\nexample","metadata":{"execution":{"iopub.status.busy":"2022-03-02T02:59:19.175947Z","iopub.execute_input":"2022-03-02T02:59:19.176195Z","iopub.status.idle":"2022-03-02T02:59:19.189629Z","shell.execute_reply.started":"2022-03-02T02:59:19.176166Z","shell.execute_reply":"2022-03-02T02:59:19.188828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## token分割","metadata":{}},{"cell_type":"markdown","source":"まずtransformersのTokenizerでpn_historyとfeature_textをtokenに分割します．\n\n本番submit時にはネットワークが使えないらしく，HuggingFace Hubからモデルをダウンロードできないため，Kaggleにある[Huggingface BERT](https://www.kaggle.com/xhlulu/huggingface-bert)をNotebookに追加して使用します．（右のタブの\"+Add data\"から追加します）\n","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"../input/huggingface-bert/bert-base-uncased\")\n\n# submitしないのであれば，Hubから直接ダウンロードできる\n# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2022-03-02T02:59:21.401231Z","iopub.execute_input":"2022-03-02T02:59:21.401913Z","iopub.status.idle":"2022-03-02T02:59:21.472625Z","shell.execute_reply.started":"2022-03-02T02:59:21.401877Z","shell.execute_reply":"2022-03-02T02:59:21.471962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"tokenizerには文のペアを渡すことができます．ここではfeature_textとpn_historyをこの順番でtokenizerに渡してtokenに変換します．\n\n[Transformers Docs](https://huggingface.co/docs/transformers/preprocessing)\n\n私が気になったのは，`max_length=416`としている点です．ここで利用している[bert_base_uncasedのモデルカード](https://huggingface.co/bert-base-uncased#:~:text=The%20only%20constrain%20is%20that%20the%20result%20with%20the%20two%20%22sentences%22%20has%20a%20combined%20length%20of%20less%20than%20512%20tokens.)には「２つの文を合わせた長さは最大512tokenに制限される」と書かれていますが，ここではなぜかそれよりも短い416という値を使っています．","metadata":{}},{"cell_type":"code","source":"out = tokenizer(example[\"feature_text\"], example[\"pn_history\"], max_length=416, truncation=\"only_second\", padding=\"max_length\", return_offsets_mapping=True)\nout.keys()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T02:59:24.414529Z","iopub.execute_input":"2022-03-02T02:59:24.415119Z","iopub.status.idle":"2022-03-02T02:59:24.424206Z","shell.execute_reply.started":"2022-03-02T02:59:24.415084Z","shell.execute_reply":"2022-03-02T02:59:24.42323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"input_idsはfeature_textとpn_historyをtokenに分割したものです．tokenそのものではなくtokenのidが入っています．\n\n`tokenizer.convert_ids_to_tokens`でtoken idをtokenに変換できます．token idをtokenに戻してみるともとの文が確認できます．\nfeature_textとpn_historyは結合され`[SEP]`で区切られています．","metadata":{}},{"cell_type":"code","source":"print(len(out[\"input_ids\"]))\nprint(out[\"input_ids\"])\nprint(tokenizer.convert_ids_to_tokens(out[\"input_ids\"]))","metadata":{"execution":{"iopub.status.busy":"2022-03-02T02:59:40.446107Z","iopub.execute_input":"2022-03-02T02:59:40.446859Z","iopub.status.idle":"2022-03-02T02:59:40.454049Z","shell.execute_reply.started":"2022-03-02T02:59:40.446808Z","shell.execute_reply":"2022-03-02T02:59:40.453328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"tokenizerで`tokenizer(..., return_offsets_mapping=True)`としていたので，tokenに変換した際にoffset_mappingも帰ってきます．\n\noffset_mappingを見ると，各tokenがもとの文のどの部分に対応するかがわかります．\n例えば\"lives\"は11番目のtokenで，offset_mappingは(13, 18)となっています．もとの文（pn_history）で13文字目から17文字目までを見ると，確かに\"lives\"となっています．","metadata":{}},{"cell_type":"code","source":"print(out[\"offset_mapping\"][:15]) #長いので最初の15個を表示\nprint(example[\"pn_history\"][13:18])","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:21:28.787971Z","iopub.execute_input":"2022-03-01T15:21:28.788263Z","iopub.status.idle":"2022-03-01T15:21:28.799636Z","shell.execute_reply.started":"2022-03-01T15:21:28.788223Z","shell.execute_reply":"2022-03-01T15:21:28.798569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`out.sequence_ids()`を見ることで，各tokenが入力した２つの文（feature_text，pn_history）のどちらから作られたかを知ることができます．\n\n416個の各tokenについて，`0`はfeature_text，`1`はpn_historyに属しているtokenであることを表しています．`None`となっているのは`[SEP]`や`[PAD]`などの特殊なtokenです．\n\n`out[\"sequence_ids\"]`に入れておきます．","metadata":{}},{"cell_type":"code","source":"print(len(out.sequence_ids()))\nprint(out.sequence_ids())\n\nout[\"sequence_ids\"] = out.sequence_ids()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:21:28.801002Z","iopub.execute_input":"2022-03-01T15:21:28.801506Z","iopub.status.idle":"2022-03-01T15:21:28.809729Z","shell.execute_reply.started":"2022-03-01T15:21:28.801469Z","shell.execute_reply":"2022-03-01T15:21:28.808879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ラベル作成","metadata":{}},{"cell_type":"markdown","source":"次に，annotaionの位置`example[\"location_list\"]`をもとにしてラベルを作成します．\n\n前処理で文字列からリストへ変換しましたが，まだ中身は文字列なのでさらに変換を行います．`loc_list_to_ints`という関数で変換を行います．\nまた変換したものを`out[\"location_int\"]`に入れておきます．","metadata":{}},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\n\nprint(f\"{example['location_list']} -> {loc_list_to_ints(example['location_list'])}\")\n\nprint(f\"{['682 688;695 697']} -> {loc_list_to_ints(['682 688;695 697'])}\")\n\n\nout[\"location_int\"] = loc_list_to_ints(example[\"location_list\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:21:28.811457Z","iopub.execute_input":"2022-03-01T15:21:28.811968Z","iopub.status.idle":"2022-03-01T15:21:28.821226Z","shell.execute_reply.started":"2022-03-01T15:21:28.81193Z","shell.execute_reply":"2022-03-01T15:21:28.820483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`out[\"sequence_ids\"]`と`out[\"offset_mapping\"]`，そして`out[\"location_int\"]`を使って，各tokenにがfeatureに関する重要な記述であるかどうか（annotationに含むまれるか）を表すラベルを作成します．","metadata":{}},{"cell_type":"code","source":"labels = [0.0] * len(out[\"input_ids\"]) # tokenの数と同じ長さのラベルのリストを用意\n\n# 各tokenのseq_id（どちらの文に属しているか）とoffsets（もとの文のどの範囲にあるか）\nfor idx, (seq_id, offsets) in enumerate(zip(out[\"sequence_ids\"], out[\"offset_mapping\"])):\n    # tokenが特殊なtoken（[PAD]や[SEQ]）である場合，あるいは１つ目の文（feature_text）に属している場合はラベルを付けたくないので-1としておく．\n    # （ラベルが-1のtokenは，後でlossを計算する際に無視されます）\n    if not seq_id or seq_id == 0:\n        labels[idx] = -1\n        continue\n\n    # tokenがannotaionの範囲に含まれている場合，そのtokenのラベルを1.0にする．\n    token_start, token_end = offsets\n    for feature_start, feature_end in out[\"location_int\"]:\n        if token_start >= feature_start and token_end <= feature_end:\n            labels[idx] = 1.0\n            break\n\nout[\"labels\"] = labels\n\nprint(out[\"labels\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:21:28.822311Z","iopub.execute_input":"2022-03-01T15:21:28.822613Z","iopub.status.idle":"2022-03-01T15:21:28.833907Z","shell.execute_reply.started":"2022-03-01T15:21:28.822579Z","shell.execute_reply":"2022-03-01T15:21:28.832961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"これでデータセットを作成できます．改めてコードをまとめます．","metadata":{}},{"cell_type":"code","source":"hyperparameters = {\n    \"max_length\": 416,\n    \"padding\": \"max_length\",\n    \"return_offsets_mapping\": True,\n    \"truncation\": \"only_second\",\n    \"model_name\": \"../input/huggingface-bert/bert-base-uncased\",\n    \"dropout\": 0.2,\n    \"lr\": 1e-5,\n    \"test_size\": 0.2,\n    \"seed\": 1268,\n    \"batch_size\": 8\n}\n\ndef loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\n\ndef tokenize_and_add_labels(tokenizer, data, config):\n    out = tokenizer(\n        data[\"feature_text\"],\n        data[\"pn_history\"],\n        truncation=config['truncation'],\n        max_length=config['max_length'],\n        padding=config['padding'],\n        return_offsets_mapping=config['return_offsets_mapping']\n    )\n    labels = [0.0] * len(out[\"input_ids\"])\n    out[\"location_int\"] = loc_list_to_ints(data[\"location_list\"])\n    out[\"sequence_ids\"] = out.sequence_ids()\n\n    for idx, (seq_id, offsets) in enumerate(zip(out[\"sequence_ids\"], out[\"offset_mapping\"])):\n        if not seq_id or seq_id == 0:\n            labels[idx] = -1\n            continue\n\n        token_start, token_end = offsets\n        for feature_start, feature_end in out[\"location_int\"]:\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                break\n\n    out[\"labels\"] = labels\n\n    return out\n\nclass CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        data = self.data.iloc[idx]\n        tokens = tokenize_and_add_labels(self.tokenizer, data, self.config)\n\n        input_ids = np.array(tokens[\"input_ids\"])\n        attention_mask = np.array(tokens[\"attention_mask\"])\n        token_type_ids = np.array(tokens[\"token_type_ids\"])\n\n        labels = np.array(tokens[\"labels\"])\n        offset_mapping = np.array(tokens['offset_mapping'])\n        sequence_ids = np.array(tokens['sequence_ids']).astype(\"float16\")\n        \n        return input_ids, attention_mask, token_type_ids, labels, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:21:28.835664Z","iopub.execute_input":"2022-03-01T15:21:28.835943Z","iopub.status.idle":"2022-03-01T15:21:28.853678Z","shell.execute_reply.started":"2022-03-01T15:21:28.835894Z","shell.execute_reply":"2022-03-01T15:21:28.852762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"dataloaderを作成します．","metadata":{}},{"cell_type":"code","source":"train_df = prepare_datasets()\n\nX_train, X_test = train_test_split(train_df, test_size=hyperparameters['test_size'],\n                                   random_state=hyperparameters['seed'])\n\nprint(\"Train size\", len(X_train))\nprint(\"Test Size\", len(X_test))\n\ntokenizer = AutoTokenizer.from_pretrained(hyperparameters['model_name'])\n\ntraining_data = CustomDataset(X_train, tokenizer, hyperparameters)\ntrain_dataloader = DataLoader(training_data, batch_size=hyperparameters['batch_size'], shuffle=True)\n\ntest_data = CustomDataset(X_test, tokenizer, hyperparameters)\ntest_dataloader = DataLoader(test_data, batch_size=hyperparameters['batch_size'], shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:21:28.855133Z","iopub.execute_input":"2022-03-01T15:21:28.855378Z","iopub.status.idle":"2022-03-01T15:21:29.495075Z","shell.execute_reply.started":"2022-03-01T15:21:28.855347Z","shell.execute_reply":"2022-03-01T15:21:29.493398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"試しにbatchを１つ取り出してみます．","metadata":{}},{"cell_type":"code","source":"for batch in train_dataloader:\n    input_ids, attention_mask, token_type_ids, labels, offset_mapping, sequence_ids = batch\n    print(input_ids.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:21:29.496566Z","iopub.execute_input":"2022-03-01T15:21:29.496826Z","iopub.status.idle":"2022-03-01T15:21:29.556362Z","shell.execute_reply.started":"2022-03-01T15:21:29.496792Z","shell.execute_reply":"2022-03-01T15:21:29.555274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"次にモデルを作成します．BERTの出力にFC層をつなげたモデルになっています．","metadata":{}},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(config['model_name'])  # BERT model\n        self.dropout = nn.Dropout(p=config['dropout'])\n        self.config = config\n        self.fc1 = nn.Linear(768, 512)\n        self.fc2 = nn.Linear(512, 512)\n        self.fc3 = nn.Linear(512, 1)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        logits = self.fc1(outputs[0])\n        logits = self.fc2(self.dropout(logits))\n        logits = self.fc3(self.dropout(logits)).squeeze(-1)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:21:29.55768Z","iopub.execute_input":"2022-03-01T15:21:29.557954Z","iopub.status.idle":"2022-03-01T15:21:29.565879Z","shell.execute_reply.started":"2022-03-01T15:21:29.55789Z","shell.execute_reply":"2022-03-01T15:21:29.564831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"モデルに入力されたテンソルの形がどのように変わっていくかを見てみます．\n\nBERTの出力の形は`(batch_size, n_tokens, hidden_dim) = (8, 416, 768)`で，FC層を通すことで最終的に`(8, 416)`の形になります．各batchの各tokenに対して１つの値（logit）が出力されています．これをsigmoidに通すことで確率を計算します．\n\nモデル作成時に\"Some weights of the ...\"という警告が出ていますが正常です．[参考](https://huggingface.co/course/chapter3/3?fw=pt#:~:text=You%20will%20notice,to%20do%20now.)","metadata":{}},{"cell_type":"code","source":"model = CustomModel(hyperparameters)\n\nfor batch in train_dataloader:\n    input_ids, attention_mask, token_type_ids, labels, offset_mapping, sequence_ids = batch\n    bert_output = model.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n    print(bert_output.last_hidden_state.size())\n    \n    logits = model.fc1(bert_output[0]) # bert_output[0]とbert_output.last_hidden_stateは同じ\n    logits = model.fc2(model.dropout(logits))\n    logits = model.fc3(model.dropout(logits))\n    print(logits.size())\n    \n    logits = logits.squeeze(-1)\n    print(logits.size())\n    break","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:21:29.567446Z","iopub.execute_input":"2022-03-01T15:21:29.567743Z","iopub.status.idle":"2022-03-01T15:21:45.312594Z","shell.execute_reply.started":"2022-03-01T15:21:29.567706Z","shell.execute_reply":"2022-03-01T15:21:45.311564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"criterion = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n\ndef train_model(model, dataloader, optimizer, criterion):\n        model.train()\n        train_loss = []\n\n        for batch in tqdm(dataloader):\n            optimizer.zero_grad()\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            token_type_ids = batch[2].to(DEVICE)\n            labels = batch[3].to(DEVICE)\n\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss = criterion(logits, labels)\n            \n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            train_loss.append(loss.item() * input_ids.size(0))\n            loss.backward()\n            # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n            # it's also improve f1 accuracy slightly\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n        return sum(train_loss)/len(train_loss)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:21:45.313853Z","iopub.execute_input":"2022-03-01T15:21:45.314114Z","iopub.status.idle":"2022-03-01T15:21:45.323091Z","shell.execute_reply.started":"2022-03-01T15:21:45.314077Z","shell.execute_reply":"2022-03-01T15:21:45.322038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"先ほどと同様に`train_dataloader`からbatchを１つ取り出して流れを確認します．\n\n-1.0のラベルを付けたtoken（feature_textに属している or 特殊なtoken）に対するモデルのlossは訓練に使いたくないため，`torch.masked_select`でフィルタリングしています．\n\nまた`BCEWithLogitsLoss`で`reduction=\"none\"`とすることで，各tokenに対するlossを計算できます．","metadata":{}},{"cell_type":"code","source":"for batch in train_dataloader:\n    input_ids, attention_mask, token_type_ids, labels, offset_mapping, sequence_ids = batch\n\n    logits = model(input_ids, attention_mask, token_type_ids)\n    print(logits.size())\n    \n    # criterion = BCEWithLogitsLoss(reduction = \"none\") とすることで，各batchの各tokenのlossがわかる．\n    # デフォルトではreduction = \"mean\"\n    loss = criterion(logits, labels)\n    print(loss.size())\n\n    # ラベル作成時に-1.0とした部分のlossは無視する．\n    loss = torch.masked_select(loss, labels > -1.0)\n    print(loss.size()) # ラベルが 0 or 1 のlossのみがのこる．\n    \n    print(loss.mean())\n    \n    break","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:21:45.324388Z","iopub.execute_input":"2022-03-01T15:21:45.324885Z","iopub.status.idle":"2022-03-01T15:21:54.314143Z","shell.execute_reply.started":"2022-03-01T15:21:45.324831Z","shell.execute_reply":"2022-03-01T15:21:54.313266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ラベルが 0 or 1 のlossの数\nnp.sum(labels.numpy()>-1.0)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:21:54.315775Z","iopub.execute_input":"2022-03-01T15:21:54.316118Z","iopub.status.idle":"2022-03-01T15:21:54.322217Z","shell.execute_reply.started":"2022-03-01T15:21:54.316075Z","shell.execute_reply":"2022-03-01T15:21:54.321506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"🤗 Transformersには訓練に必要な処理をまとめたTrainerクラスが存在しますが，今回は上で説明したような処理を組み込む必要があったため使用していないようです．","metadata":{}},{"cell_type":"markdown","source":"次に評価用の関数です．ほとんど訓練用の関数と同じですが，token単位の予測結果を文字単位の範囲に変換する処理とmetricの計算が追加されています．\n\n[Evaluation方法](https://www.kaggle.com/c/nbme-score-clinical-patient-notes/overview/evaluation)","metadata":{}},{"cell_type":"code","source":"def eval_model(model, dataloader, criterion):\n        model.eval()\n        valid_loss = []\n        preds = []\n        offsets = []\n        seq_ids = []\n        valid_labels = []\n\n        for batch in tqdm(dataloader):\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            token_type_ids = batch[2].to(DEVICE)\n            labels = batch[3].to(DEVICE)\n            offset_mapping = batch[4]\n            sequence_ids = batch[5]\n\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss = criterion(logits, labels)\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            valid_loss.append(loss.item() * input_ids.size(0))\n\n            preds.append(logits.detach().cpu().numpy())\n            offsets.append(offset_mapping.numpy())\n            seq_ids.append(sequence_ids.numpy())\n            valid_labels.append(labels.detach().cpu().numpy())\n\n        preds = np.concatenate(preds, axis=0)\n        offsets = np.concatenate(offsets, axis=0)\n        seq_ids = np.concatenate(seq_ids, axis=0)\n        valid_labels = np.concatenate(valid_labels, axis=0)\n        location_preds = get_location_predictions(preds, offsets, seq_ids, test=False)\n        score = calculate_char_cv(location_preds, offsets, seq_ids, valid_labels)\n\n        return sum(valid_loss)/len(valid_loss), score\n\n    \n# token単位の予測結果を文字単位の範囲に変換する．\ndef get_location_predictions(preds, offset_mapping, sequence_ids, test=False):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        pred = 1 / (1 + np.exp(-pred)) # logitsからprobabilityを計算\n        \n        start_idx = None\n        end_idx = None\n        current_preds = []\n        for pred, offset, seq_id in zip(pred, offsets, seq_ids):\n            if seq_id is None or seq_id == 0:\n                continue\n\n            # probability > 0.5 が連続している部分をtoken単位で探し，\n            # そのstart位置とstop位置の（文字単位での）idxを記録する．\n            if pred > 0.5:\n                if start_idx is None:\n                    start_idx = offset[0]\n                end_idx = offset[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n            \n    return all_predictions\n\n\n# 文字単位で評価値を計算する．\ndef calculate_char_cv(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros(num_chars)\n\n        # ラベル\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n\n        char_preds = np.zeros(num_chars)\n\n        # 予測結果\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n\n    results = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", labels=np.unique(all_preds))\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n\n    return {\n        \"Accuracy\": accuracy,\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:21:54.323613Z","iopub.execute_input":"2022-03-01T15:21:54.324138Z","iopub.status.idle":"2022-03-01T15:21:54.346615Z","shell.execute_reply.started":"2022-03-01T15:21:54.324089Z","shell.execute_reply":"2022-03-01T15:21:54.345797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"実際に訓練を行うコードです．（実行に時間がかかるのでコメントアウトしています）","metadata":{}},{"cell_type":"code","source":"# train_df = prepare_datasets()\n# X_train, X_test = train_test_split(train_df, test_size=hyperparameters['test_size'], random_state=hyperparameters['seed'])\n\n# tokenizer = AutoTokenizer.from_pretrained(hyperparameters['model_name'])\n# training_data = CustomDataset(X_train, tokenizer, hyperparameters)\n# train_dataloader = DataLoader(training_data, batch_size=hyperparameters['batch_size'], shuffle=True)\n# test_data = CustomDataset(X_test, tokenizer, hyperparameters)\n# test_dataloader = DataLoader(test_data, batch_size=hyperparameters['batch_size'], shuffle=False)\n\n# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# model = CustomModel(hyperparameters).to(DEVICE)\n# criterion = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n# optimizer = optim.AdamW(model.parameters(), lr=hyperparameters['lr'])\n\n# train_loss_data, valid_loss_data = [], []\n# score_data_list = []\n# valid_loss_min = np.Inf\n# since = time.time()\n# epochs = 3\n\n# best_loss = np.inf\n\n# for i in range(epochs):\n#     print(\"Epoch: {}/{}\".format(i + 1, epochs))\n#     # first train model\n#     train_loss = train_model(model, train_dataloader, optimizer, criterion)\n#     train_loss_data.append(train_loss)\n#     print(f\"Train loss: {train_loss}\")\n#     # evaluate model\n#     valid_loss, score = eval_model(model, test_dataloader, criterion)\n#     valid_loss_data.append(valid_loss)\n#     score_data_list.append(score)\n#     print(f\"Valid loss: {valid_loss}\")\n#     print(f\"Valid score: {score}\")\n    \n#     if valid_loss < best_loss:\n#         best_loss = valid_loss\n#         torch.save(model.state_dict(), \"nbme_bert_v2.pth\")\n\n    \n# time_elapsed = time.time() - since\n# print('Training completed in {:.0f}m {:.0f}s'.format(\n#     time_elapsed // 60, time_elapsed % 60))","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:21:54.348053Z","iopub.execute_input":"2022-03-01T15:21:54.348565Z","iopub.status.idle":"2022-03-01T15:21:54.358972Z","shell.execute_reply.started":"2022-03-01T15:21:54.348528Z","shell.execute_reply":"2022-03-01T15:21:54.35829Z"},"trusted":true},"execution_count":null,"outputs":[]}]}