{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this notebook\n- Deberta-base starter code\n- pip wheels is [here](https://www.kaggle.com/yasufuminakama/nbme-pip-wheels)\n- Inference notebook is [here](https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-inference)\n\nIf this notebook is helpful, feel free to upvote :)","metadata":{}},{"cell_type":"markdown","source":"# Directory settings","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:54:32.306531Z","iopub.execute_input":"2022-04-19T10:54:32.30676Z","iopub.status.idle":"2022-04-19T10:54:32.331754Z","shell.execute_reply.started":"2022-04-19T10:54:32.306697Z","shell.execute_reply":"2022-04-19T10:54:32.331099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{"papermill":{"duration":0.015756,"end_time":"2021-11-16T19:32:29.999526","exception":false,"start_time":"2021-11-16T19:32:29.98377","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    wandb=False \n    competition='NBME'\n    _wandb_kernel='nakama'\n    debug=False\n    apex=True\n    print_freq=100\n    num_workers=6\n#     model='roberta-large'\n#     model=\"microsoft/deberta-large\"\n#     model=\"microsoft/deberta-base\"\n#     model=\"ZZ99/NBME_TAPT_deberta_base\"\n#     tokenizer_model=\"microsoft/deberta-base\"\n#     model=\"ZZ99/NBME_TAPT_deberta_base\"\n    tokenizer_model=\"microsoft/deberta-v3-base\"\n#     tokenizer_model=\"../input/deberta-v3-base\"  \n    model=\"ZZ99/tapt_nbme_deberta_v3_base\"\n#     model=\"emilyalsentzer/Bio_ClinicalBERT\"\n#     model=\"Tsubasaz/clinical-pubmed-bert-base-128\"\n    scheduler='cosine' # ['linear', 'cosine']\n    batch_scheduler=True\n    num_cycles=0.5\n    num_warmup_steps=0\n    epochs=5\n    encoder_lr=2e-5\n    decoder_lr=2e-5\n    min_lr=1e-6\n    eps=1e-6\n    betas=(0.9, 0.999)\n    batch_size=12\n    fc_dropout=0.2\n    max_len=512\n    weight_decay=0.01\n    gradient_accumulation_steps=1\n    max_grad_norm=1000\n    seed=42\n    \n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n#     n_fold=2\n#     trn_fold=[0, 1]\n    train=True\n    \nif CFG.debug:\n    CFG.epochs = 2\n    CFG.trn_fold = [0]","metadata":{"papermill":{"duration":0.02543,"end_time":"2021-11-16T19:32:30.040766","exception":false,"start_time":"2021-11-16T19:32:30.015336","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T10:54:32.333221Z","iopub.execute_input":"2022-04-19T10:54:32.33351Z","iopub.status.idle":"2022-04-19T10:54:32.341678Z","shell.execute_reply.started":"2022-04-19T10:54:32.333475Z","shell.execute_reply":"2022-04-19T10:54:32.340832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# wandb\n# ====================================================\nif CFG.wandb:\n    \n    import wandb\n\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n        wandb.login(key=secret_value_0)\n        anony = None\n    except:\n        anony = \"must\"\n        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n\n\n    def class2dict(f):\n        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n\n    run = wandb.init(project='NBME-Public', \n                     name=CFG.model,\n                     config=class2dict(CFG),\n                     group=CFG.model,\n                     job_type=\"train\",\n                     anonymous=anony)","metadata":{"papermill":{"duration":10.132708,"end_time":"2021-11-16T19:32:40.188665","exception":false,"start_time":"2021-11-16T19:32:30.055957","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T10:54:32.343643Z","iopub.execute_input":"2022-04-19T10:54:32.344038Z","iopub.status.idle":"2022-04-19T10:54:32.352008Z","shell.execute_reply.started":"2022-04-19T10:54:32.343994Z","shell.execute_reply":"2022-04-19T10:54:32.35131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Library","metadata":{"papermill":{"duration":0.016162,"end_time":"2021-11-16T19:32:40.221507","exception":false,"start_time":"2021-11-16T19:32:40.205345","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Library\n# Install required dependencies\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\n#abstract syntax tree\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\n# os.system('pip uninstall -y transformers')\n# os.system('python -m pip install --no-index --find-links=../input/nbme-pip-wheels transformers')\n# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/d/datasets/nbroad/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n    print(\"conversion_path unlinked\")\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in [\n    \"tokenization_deberta_v2.py\",\n    \"tokenization_deberta_v2_fast.py\",\n    \"deberta__init__.py\",\n]:\n    if str(filename).startswith(\"deberta\"):\n        filepath = deberta_v2_path / str(filename).replace(\"deberta\", \"\")\n    else:\n        filepath = deberta_v2_path / filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir / filename, filepath)\nimport tokenizers\nimport transformers \nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"papermill":{"duration":30.77583,"end_time":"2021-11-16T19:33:11.013554","exception":false,"start_time":"2021-11-16T19:32:40.237724","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T10:54:32.353734Z","iopub.execute_input":"2022-04-19T10:54:32.35414Z","iopub.status.idle":"2022-04-19T10:54:39.65215Z","shell.execute_reply.started":"2022-04-19T10:54:32.354106Z","shell.execute_reply":"2022-04-19T10:54:39.651396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nif (\n    \"v2\" in CFG.model\n    or \"v3\" in CFG.model\n):\n    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n    tokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.tokenizer_model)\nelse: \n    tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n# tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_model)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\nCFG.tokenizer = tokenizer","metadata":{"papermill":{"duration":8.833709,"end_time":"2021-11-16T19:33:20.817889","exception":false,"start_time":"2021-11-16T19:33:11.98418","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T10:54:39.655506Z","iopub.execute_input":"2022-04-19T10:54:39.655732Z","iopub.status.idle":"2022-04-19T10:54:47.277411Z","shell.execute_reply.started":"2022-04-19T10:54:39.655704Z","shell.execute_reply":"2022-04-19T10:54:47.276678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions for scoring","metadata":{"papermill":{"duration":0.01915,"end_time":"2021-11-16T19:33:11.052159","exception":false,"start_time":"2021-11-16T19:33:11.033009","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n\ndef micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    return f1_score(truths, preds)\n\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n    return binary\n\n\ndef span_micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n    return micro_f1(bin_preds, bin_truths)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:54:47.278753Z","iopub.execute_input":"2022-04-19T10:54:47.279007Z","iopub.status.idle":"2022-04-19T10:54:47.288534Z","shell.execute_reply.started":"2022-04-19T10:54:47.278962Z","shell.execute_reply":"2022-04-19T10:54:47.287789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_labels_for_scoring(df):\n    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    #df.values returns values in numpy form\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n        #if non-empty\n        if len(location_list) > 0:\n            location = location_list[0]\n            #['0 1;3 4']-> ['0 1','3 4']\n            \n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n    return truths\n\n\ndef get_char_probs(texts, predictions, tokenizer):\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n    return results\n\n# Difficulty: How to turn separate word probability into a branch of words as output?\n# char_probs shape= set size*history note word size\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n    return results\n\n\ndef get_predictions(results):\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:54:47.291098Z","iopub.execute_input":"2022-04-19T10:54:47.291409Z","iopub.status.idle":"2022-04-19T10:54:47.307982Z","shell.execute_reply.started":"2022-04-19T10:54:47.291373Z","shell.execute_reply":"2022-04-19T10:54:47.307264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\n#return f1 score\ndef get_score(y_true, y_pred):\n    score = span_micro_f1(y_true, y_pred)\n    return score\n\n\ndef get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"papermill":{"duration":0.034649,"end_time":"2021-11-16T19:33:11.105766","exception":false,"start_time":"2021-11-16T19:33:11.071117","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T10:54:47.309353Z","iopub.execute_input":"2022-04-19T10:54:47.309876Z","iopub.status.idle":"2022-04-19T10:54:47.322046Z","shell.execute_reply.started":"2022-04-19T10:54:47.309844Z","shell.execute_reply":"2022-04-19T10:54:47.32133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{"papermill":{"duration":0.018406,"end_time":"2021-11-16T19:33:11.150174","exception":false,"start_time":"2021-11-16T19:33:11.131768","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\ntrain = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')\ntrain['annotation'] = train['annotation'].apply(ast.literal_eval)\ntrain['location'] = train['location'].apply(ast.literal_eval)\nfeatures = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\nfeatures = preprocess_features(features)\npatient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n\nprint(f\"train.shape: {train.shape}\")\ndisplay(train.head())\nprint(f\"features.shape: {features.shape}\")\ndisplay(features.head())\nprint(f\"patient_notes.shape: {patient_notes.shape}\")\ndisplay(patient_notes.head())","metadata":{"papermill":{"duration":0.637101,"end_time":"2021-11-16T19:33:11.805349","exception":false,"start_time":"2021-11-16T19:33:11.168248","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T10:54:47.323326Z","iopub.execute_input":"2022-04-19T10:54:47.323912Z","iopub.status.idle":"2022-04-19T10:54:48.392867Z","shell.execute_reply.started":"2022-04-19T10:54:47.32387Z","shell.execute_reply":"2022-04-19T10:54:48.392094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.merge(features, on=['feature_num', 'case_num'], how='left')\ntrain = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\ndisplay(train.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:54:48.394085Z","iopub.execute_input":"2022-04-19T10:54:48.394415Z","iopub.status.idle":"2022-04-19T10:54:48.432253Z","shell.execute_reply.started":"2022-04-19T10:54:48.394375Z","shell.execute_reply":"2022-04-19T10:54:48.431461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# incorrect annotation\ntrain.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\ntrain.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n\ntrain.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\ntrain.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n\ntrain.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\ntrain.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n\ntrain.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\ntrain.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n\ntrain.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\ntrain.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n\ntrain.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\ntrain.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n\ntrain.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\ntrain.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n\ntrain.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\ntrain.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n\ntrain.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\ntrain.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n\ntrain.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\ntrain.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n\ntrain.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\ntrain.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n\ntrain.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\ntrain.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n\ntrain.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\ntrain.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n\ntrain.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\ntrain.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n\ntrain.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\ntrain.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n\ntrain.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\ntrain.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n\ntrain.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\ntrain.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n\ntrain.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\ntrain.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n\ntrain.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\ntrain.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n\ntrain.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\ntrain.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n\ntrain.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\ntrain.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n\ntrain.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\ntrain.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n\ntrain.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\ntrain.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n\ntrain.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\ntrain.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n\ntrain.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\ntrain.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n\ntrain.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\ntrain.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n\ntrain.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\ntrain.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n\ntrain.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\ntrain.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n\ntrain.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\ntrain.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n\ntrain.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\ntrain.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n\ntrain.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\ntrain.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n\ntrain.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\ntrain.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n\ntrain.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\ntrain.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n\ntrain.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\ntrain.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n\ntrain.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\ntrain.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n\ntrain.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\ntrain.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n\ntrain.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\ntrain.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n\ntrain.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\ntrain.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n\ntrain.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\ntrain.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n\ntrain.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\ntrain.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n\ntrain.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\ntrain.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n\ntrain.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\ntrain.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:54:48.433732Z","iopub.execute_input":"2022-04-19T10:54:48.433982Z","iopub.status.idle":"2022-04-19T10:54:48.530769Z","shell.execute_reply.started":"2022-04-19T10:54:48.433948Z","shell.execute_reply":"2022-04-19T10:54:48.530139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['annotation_length'] = train['annotation'].apply(len)\ndisplay(train['annotation_length'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:54:48.531741Z","iopub.execute_input":"2022-04-19T10:54:48.531966Z","iopub.status.idle":"2022-04-19T10:54:48.547065Z","shell.execute_reply.started":"2022-04-19T10:54:48.531931Z","shell.execute_reply":"2022-04-19T10:54:48.546312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CV split","metadata":{"papermill":{"duration":0.02099,"end_time":"2021-11-16T19:33:11.849001","exception":false,"start_time":"2021-11-16T19:33:11.828011","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# CV split\n# ====================================================\nFold = GroupKFold(n_splits=CFG.n_fold)\ngroups = train['pn_num'].values\nfor n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n    train.loc[val_index, 'fold'] = int(n)\ntrain['fold'] = train['fold'].astype(int)\ndisplay(train.groupby('fold').size())\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:54:48.550651Z","iopub.execute_input":"2022-04-19T10:54:48.550837Z","iopub.status.idle":"2022-04-19T10:54:48.581633Z","shell.execute_reply.started":"2022-04-19T10:54:48.550814Z","shell.execute_reply":"2022-04-19T10:54:48.580983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.debug:\n    display(train.groupby('fold').size())\n    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n    display(train.groupby('fold').size())","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:54:48.582742Z","iopub.execute_input":"2022-04-19T10:54:48.583134Z","iopub.status.idle":"2022-04-19T10:54:48.5882Z","shell.execute_reply.started":"2022-04-19T10:54:48.583101Z","shell.execute_reply":"2022-04-19T10:54:48.587353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tokenizer","metadata":{"papermill":{"duration":0.02017,"end_time":"2021-11-16T19:33:11.963889","exception":false,"start_time":"2021-11-16T19:33:11.943719","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Define max_len\n# ====================================================\nfor text_col in ['pn_history']:\n    pn_history_lengths = []\n    tk0 = tqdm(patient_notes[text_col].fillna(\"\").values, total=len(patient_notes))\n    for text in tk0:\n        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n        pn_history_lengths.append(length)\n#     pn_history max(lengths): 433\n    LOGGER.info(f'{text_col} max(lengths): {max(pn_history_lengths)}')\n\nfor text_col in ['feature_text']:\n    features_lengths = []\n    tk0 = tqdm(features[text_col].fillna(\"\").values, total=len(features))\n    for text in tk0:\n        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n        features_lengths.append(length)\n    LOGGER.info(f'{text_col} max(lengths): {max(features_lengths)}')\n# feature_text max(lengths): 30\n# max_len: 466\nCFG.max_len = max(pn_history_lengths) + max(features_lengths) + 3 # cls & sep & sep\nLOGGER.info(f\"max_len: {CFG.max_len}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:54:48.589493Z","iopub.execute_input":"2022-04-19T10:54:48.589808Z","iopub.status.idle":"2022-04-19T10:55:14.367595Z","shell.execute_reply.started":"2022-04-19T10:54:48.589773Z","shell.execute_reply":"2022-04-19T10:55:14.366912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\n# tokenize and standardlize inputs\ndef prepare_input(cfg, text, feature_text):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           max_length=CFG.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\ndef create_label(cfg, text, annotation_length, location_list):\n    encoded = cfg.tokenizer(text,\n                            add_special_tokens=True,\n                            max_length=CFG.max_len,\n                            padding=\"max_length\",\n                            return_offsets_mapping=True)\n    offset_mapping = encoded['offset_mapping']\n    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n    label = np.zeros(len(offset_mapping))\n    label[ignore_idxes] = -1\n    if annotation_length != 0:\n        for location in location_list:\n            for loc in [s.split() for s in location.split(';')]:\n                start_idx = -1\n                end_idx = -1\n                start, end = int(loc[0]), int(loc[1])\n                for idx in range(len(offset_mapping)):\n                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n                        start_idx = idx - 1\n                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n                        end_idx = idx + 1\n                if start_idx == -1:\n                    start_idx = end_idx\n                if (start_idx != -1) & (end_idx != -1):\n                    label[start_idx:end_idx] = 1\n    return torch.tensor(label, dtype=torch.float)\n\n\nclass TrainDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n        self.annotation_lengths = df['annotation_length'].values\n        self.locations = df['location'].values\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item])\n        label = create_label(self.cfg, \n                             self.pn_historys[item], \n                             self.annotation_lengths[item], \n                             self.locations[item])\n        return inputs, label","metadata":{"papermill":{"duration":0.040128,"end_time":"2021-11-16T19:33:20.931029","exception":false,"start_time":"2021-11-16T19:33:20.890901","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T10:55:14.36905Z","iopub.execute_input":"2022-04-19T10:55:14.369522Z","iopub.status.idle":"2022-04-19T10:55:14.385822Z","shell.execute_reply.started":"2022-04-19T10:55:14.369484Z","shell.execute_reply":"2022-04-19T10:55:14.384989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.02209,"end_time":"2021-11-16T19:33:20.978793","exception":false,"start_time":"2021-11-16T19:33:20.956703","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel(self.config)\n            \n        s = self.config.hidden_size\n        print(\"size###\",s)\n        self.lstm = nn.LSTM(input_size=768, hidden_size=384,\n                            num_layers=2, bidirectional=True, batch_first=True)\n        \n#         self.lstm = nn.LSTM(self.model.config.hidden_size, self.lstm_hidden_size, bidirectional=True)\n\n        \n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n# Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n# - This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n# - This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).     \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        \n        last_hidden_states = outputs[0]\n#         enc, _ = self.lstm(last_hidden_states)\n        return last_hidden_states\n#         with torch.no_grad():\n#           embeds, _  = self.model(**inputs)\n#         enc, _ = self.lstm(embeds)\n#         return enc\n\n    def forward(self, inputs):\n        \n        feature_outputs = self.model(**inputs)\n        feature = feature_outputs[0]\n        enc, _ = self.lstm(feature)\n        enc = self.fc_dropout(enc)\n        output = self.fc(enc)\n        return output","metadata":{"papermill":{"duration":0.032939,"end_time":"2021-11-16T19:33:21.034275","exception":false,"start_time":"2021-11-16T19:33:21.001336","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T10:55:14.386599Z","iopub.execute_input":"2022-04-19T10:55:14.386786Z","iopub.status.idle":"2022-04-19T10:55:14.403761Z","shell.execute_reply.started":"2022-04-19T10:55:14.386762Z","shell.execute_reply":"2022-04-19T10:55:14.402984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helpler functions","metadata":{"papermill":{"duration":0.022058,"end_time":"2021-11-16T19:33:21.081885","exception":false,"start_time":"2021-11-16T19:33:21.059827","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# train loop\n# ====================================================\ndef train_loop(folds, fold):\n    \n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================\n    #reset training set and vaildation set according to the folds\n#     valid_folds:           id  case_num  pn_num  feature_num                                         annotation                                           location                    feature_text                                         pn_history  annotation_length  fold                         location_for_create_labels\n# 0  20540_202         2   20540          202                                                 []                                                 []                             IUD  Mrs. Montgomery is a 44yo female presenting wi...                  0     0                                                 []\n# 1  43252_400         4   43252          400  [no h/o tremors, no palpitations, no heat into...  [346 360, 346 348;362 374, 346 348;411 427, 80...  Lack-of-other-thyroid-symptoms  45 y/o f with a h/o nervousness since few week...                  4     0  [346 360;346 348;362 374;346 348;411 427;809 8...\n# 2  10620_109         1   10620          109                               [decreased appetite]                                          [249 267]             Diminished-appetite  Ms. Powelton is a 20yo F with no sig PMHx who ...                  1     0                                          [249 267]\n# 3  01581_003         0    1581            3                               [episodes, episdoes]                                 [105 113, 350 358]           Intermittent-symptoms  Dillon cleaveland is a 17 yo M presenting for ...                  2     0                                  [105 113;350 358]\n# 4  83843_800         8   83843          800                           [eating more than usual]                                          [348 370]              Increased-appetite  HO1 67 F presenting with difficulty sleeping p...                  1     0                                          [348 370]\n\n# valid shape:((204, 11),), training shape:((796, 10),) valid_texts shape: (204,)\n    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n\n# valid_text: ['Mrs. Montgomery is a 44yo female presenting with irregular periods for the last 3 years. These range from 2-6 days in length with her cycles lasting from 3 weeks to 4 months. Her menstrual flow can be either heavy or light ranging fro 2-3 pads/day to 5-6 pads/day. Her periods are not painful. She has also had night sweats and hot flashes during this time period. She has not experienced dizziness, palpitations, intermentrual bleeding or vaginal discharge. She is sexually active with her husband (monogamous) and has no dyspurinia but does experience some vaginal dryness which she uses lubrication for. 1 weeks ago did have some nausea and vomitting for a day which resolved on its own. \\r\\nDenies changes in mood or energy.\\r\\nPMHx: HTN, takes HCTZ, no surgeries\\r\\nFMHX: OA, obsesity in mom and HTN in brother'\n#  '45 y/o f with a h/o nervousness since few weeks. Patient has recently changed her job from research to lecturing her audience. The nervousness starts suddenly and is more on mondays and sundays. She thinks frequently about her work and performance on mondays. It is however also present throughout the week. It has never happened before. She has no h/o tremors, palpitations, SOB, headaches, sensation of doom, heat intolerance or weight loss. Patient has been eating less due to anxiety. She consumes 5-6 cups of caffeine everyday. She takes tylenol and was hospitalized for delivery. NKDA, she has reached menopause. Patient has no h/o smoking, alcohol consumption 1 drink socially. Husband has had a vasectomy. She is sexually active. \\r\\nROS- occasional headaches+, No h/o Abdominal pain, nausea, vomiting, bowel or bladder changes, facial pain, sleep changes'\n#  'Ms. Powelton is a 20yo F with no sig PMHx who presents to the ED with 8-10 hour of RLQ abdominal pain. Pt describes this pain as dull/alchy, without radiation. Patient does endorse similar episodes ~3/4 times though with less severeity. Pt endorses decreased appetite, diarrhea of 2 days without hematochezia or melena. Pt denies subjective fever, chills, nausea, vomitting. Patient denies dysuria. Patient has 28 day menstrual cycle with 4 days of moderate flow, last one was 2 weeks ago. Patient is currently no on contraceptive, and was last sexually active with a male partner 9 months ago, using condom consistently.\\r\\nPMHx: Negative, uses ibuprofen for pain currently\\r\\nPast Surgical Hx: negative\\r\\nFHx: healthy parents\\r\\nSHx: does not smoke, drinks EtOH a couple times a month, does not use illicit drugs, has not travelled anywhere recently'\n#  'Dillon cleaveland is a 17 yo M presenting for heart pounding. He noticed this 2-3 mo ago. He has had 5-6 episodes of this over that timeframe. No kown triggers, nothing stops them. He gets lightheaded during them. He feels like his heart is bearting really fast \" beating out of my chest\". They last for 3-4 minutes. Yesterday was the worst of these episdoes. it lasted for 10 minutes, and in additon to the pounding, he has pressure in the middle of his chest that didn\\'t radiate, he was also SOB that that time. Those symtpoms resolved after the pounding went away. He does endorse a historyof taking his friend\\'s adderall prescription but hx does not coincide with symtpoms. No headache, vision changes, diarrhea, fever, chills, or temperature issues.\\r\\nPMH: none- born at term, immunizations complete, growth curve normal\\r\\nmed: adderall\\r\\nallergiesl none\\r\\nFH: mom-thyroid problem, dad, heart attack-52\\r\\nSH: no smoking, alcohol 3-4drinks, drug- MJ'\n#  'HO1 67 F presenting with difficulty sleeping past 3 weeks, diufficulty initialting sleep and getting up early. previosuly no similar episodes . son passed away 3 weeks ago and has been notices lack of energy and low mood since visual and auditory hallucinations once since then.lack of interest in daily activities,no change in weight but has been eating more than usual.\\r\\ndenies snoring.or any urinary or bowl complaints.\\r\\nros -ve except as above \\r\\npmhx htn on medication past 15 year s\\r\\npshx nil \\r\\nfhx mother alive and well has a history of major depression \\r\\nmedications lisinopril and hctz \\r\\nshx non smoker \\r\\ndrink 2-3 glasses of wine a week cage 0/4 monogamous with hysband retired receptionist excersise regularly till 3 weeks ago']\n    valid_texts = valid_folds['pn_history'].values\n    #valid_labels [[], [[346, 360], [346, 348], [362, 374], [346, 348], [411, 427], [809, 814], [826, 833], [768, 770]], [[249, 267]], [[105, 113], [350, 358]], [[348, 370]]] length= 204\n    valid_labels = create_labels_for_scoring(valid_folds)\n        \n        \n    train_dataset = TrainDataset(CFG, train_folds)\n    valid_dataset = TrainDataset(CFG, valid_folds)\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=True,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=False,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    model = CustomModel(CFG, config_path=None, pretrained=True)\n    torch.save(model.config, OUTPUT_DIR+'config.pth')\n    model.to(device)\n    \n    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            # n for name and p for params\n            #loop over the model named parameters, if the name of the parameter in no_dacay, add the param to the params list\n            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': weight_decay},\n            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': 0.0},\n            \n            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n             'lr': decoder_lr, 'weight_decay': 0.0}\n        ]\n        return optimizer_parameters\n    \n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=CFG.encoder_lr, \n                                                decoder_lr=CFG.decoder_lr,\n                                                weight_decay=CFG.weight_decay)\n    #[{'params': [Parameter containing:\n# tensor([[ 0.0067, -0.0069, -0.0080,  ..., -0.0221, -0.0057, -0.0004],\n#         [-0.0079,  0.0098, -0.0234,  ..., -0.0141, -0.0073,  0.0069],\n#         [-0.0089,  0.0216, -0.0072,  ..., -0.0187, -0.0202,  0.0146],\n#         ...,\n#         [-0.0033,  0.0059, -0.0197,  ..., -0.0177,  0.0091,  0.0102],\n#         [-0.0126, -0.0349, -0.0075,  ..., -0.0281,  0.0034, -0.0225],\n#         [ 0.0065, -0.0107, -0.0003,  ..., -0.0150,  0.0082,  0.0031]],\n#        device='cuda:0', requires_grad=True), Parameter containing:\n# tensor([[-0.1014, -0.0048, -0.0295,  ..., -0.0334,  0.0616,  0.0006],\n#         [-0.0174,  0.0850, -0.0525,  ...,  0.0057,  0.0261, -0.0012],\n#         [-0.0089,  0.0334, -0.0349,  ...,  0.0391, -0.0177, -0.0127],\n#         ...,\n#         [ 0.0303,  0.0198, -0.0292,  ...,  0.0812,  0.0044,  0.0423],\n#         [ 0.0944, -0.0252, -0.0389,  ..., -0.0740, -0.0243,  0.0389],\n#         [ 0.0155, -0.0573, -0.0324,  ..., -0.0103, -0.0305,  0.0228]],\n#        device='cuda:0', requires_grad=True), Parameter containing:\n    ## for debug\n    if fold==0:\n#         print('valid_labels',valid_labels[0:5],'length=',len(valid_labels))\n#'valid_text:',valid_texts[0:5],'valid_folds:',valid_folds.head(),\n        print('valid shape:%s, training shape:%s' %((valid_folds.shape,),(train_folds.shape,)),'valid_texts shape:',valid_texts.shape)\n#         print(\"optimizier params shape:\",len(optimizer_parameters))\n              \n    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n    \n    # ====================================================\n    # scheduler\n    # ====================================================\n    def get_scheduler(cfg, optimizer, num_train_steps):\n        if cfg.scheduler=='linear':\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n            )\n        elif cfg.scheduler=='cosine':\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n            )\n        return scheduler\n    \n    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n    \n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n    best_score = 0.\n    for epoch in range(CFG.epochs):\n        start_time = time.time()\n        # train\n        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n        # eval\n        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n        predictions = predictions.reshape((len(valid_folds), CFG.max_len))\n        # scoring\n        #char_probs.shape=(204,809), 204=test set size, 809=dic size? word size in current pn_history\n        char_probs = get_char_probs(valid_texts, predictions, CFG.tokenizer)\n\n        results = get_results(char_probs, th=0.5)\n#         preds= [[], [], [[249, 267]], [], [], [[18, 19]], [[625, 633]], [[29, 34], [52, 58], [71, 79]], [[435, 447]], [[235, 249], [251, 253], [257, 260]], [[1, 5]], [[294, 305]], [], [[13, 31]], [[831, 846]], [[202, 211], [223, 228]], [], [[58, 77]], [], [[356, 360], [367, 370]], [], [], [], [[85, 90], [122, 127], [135, 155]], [], [[413, 422]], [], [[81, 90]], [[10, 15]], [[877, 893]], [], [[17, 25]], [[242, 248], [262, 266], [307, 316]], [], [], [[56, 77]], [[657, 672]], [[502, 511], [514, 517]], [], [[139, 150]], [], [[266, 272], [277, 285]], [[689, 697]], [[84, 116], [685, 689]], [[649, 650], [655, 681]], [[447, 455], [460, 465]], [[395, 397], [399, 406]], [[434, 437]], [[338, 354]], [[339, 356], [422, 433], [451, 459]], [], [[443, 457]], [[35, 45]], [], [[227, 235]], [[845, 847], [855, 861]], [[182, 205]], [], [], [[331, 346]], [[240, 254]], [[343, 345]], [[45, 61], [71, 85], [98, 104]], [], [], [[252, 255], [257, 273]], [], [[61, 68]], [[368, 392]], [[491, 502], [515, 519]], [], [], [], [], [], [[53, 56]], [], [], [], [[488, 510], [542, 547]], [[360, 378]], [[326, 328], [617, 623], [633, 638], [640, 658]], [], [[81, 93], [141, 149]], [[418, 422]], [], [], [], [], [], [[242, 249], [253, 266]], [[249, 264]], [], [[299, 301]], [[365, 369]], [], [], [[21, 27]], [[76, 80], [85, 99], [123, 151]], [[202, 213], [275, 287]], [], [[250, 263]], [[165, 183]], [[1, 4]], [], [[5, 17]], [], [[533, 552]], [], [[300, 306]], [[266, 273]], [], [[30, 39]], [], [[200, 205], [209, 214], [414, 421], [426, 434], [469, 481]], [[384, 387]], [[1, 7]], [], [[15, 25]], [[27, 42], [56, 64]], [[292, 303]], [], [[346, 362], [364, 397]], [[33, 49]], [[258, 262], [291, 297]], [[561, 591]], [[300, 331]], [], [[154, 164], [168, 182]], [], [[15, 26]], [[281, 286], [291, 296], [447, 459]], [[540, 565]], [], [], [[125, 136]], [[351, 354], [366, 388]], [[76, 90], [96, 116]], [[359, 378]], [[50, 65]], [[276, 279], [282, 284], [292, 299]], [], [[165, 175], [223, 240]], [], [], [], [], [], [], [[33, 43]], [[828, 838]], [[79, 93]], [], [[351, 354]], [[250, 258]], [[288, 293]], [[631, 632]], [], [[66, 74], [80, 92], [110, 114], [340, 348]], [[16, 27]], [], [], [[24, 29]], [[73, 97], [200, 216]], [], [[525, 536]], [[43, 58]], [], [[1, 5]], [], [[110, 113], [115, 131]], [], [[251, 266]], [[292, 300]], [[363, 372], [377, 388]], [[284, 303]], [], [[65, 80]], [[483, 488], [516, 529], [544, 547]], [], [[125, 133]], [], [], [], [[8, 18]], [[308, 319]], [[44, 58], [70, 74], [81, 84]], [], [], [[134, 142]], [[15, 20]], [[178, 193]], [[84, 101]], [], [], [[113, 129]], [[137, 147], [163, 168]], [], [[199, 219]], [], [], [[416, 423], [459, 467]], [[5, 9]], []] \n#         shape=(204,)\n        preds = get_predictions(results)\n        score = get_score(valid_labels, preds)\n#         if epoch==0 and fold==0:\n#             tensor_char_probs=torch.tensor(char_probs)\n#             print(char_probs)\n#             print('char_probs.shape=(%d,%d)'%(len(char_probs),len(char_probs[0])))\n            \n        elapsed = time.time() - start_time\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n        if CFG.wandb:\n            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n                       f\"[fold{fold}] score\": score})\n        \n        if best_score < score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'model': model.state_dict(),\n                        'predictions': predictions},\n                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n    #load best result model\n    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n                             map_location=torch.device('cpu'))['predictions']\n    #[i for i in range()] means [0,1,2,...,CFG.max_len]\n    #CFG.max_len=512\n    valid_folds[[i for i in range(CFG.max_len)]] = predictions\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return valid_folds","metadata":{"papermill":{"duration":0.051076,"end_time":"2021-11-16T19:33:21.225819","exception":false,"start_time":"2021-11-16T19:33:21.174743","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T10:55:14.405447Z","iopub.execute_input":"2022-04-19T10:55:14.405748Z","iopub.status.idle":"2022-04-19T10:55:14.438848Z","shell.execute_reply.started":"2022-04-19T10:55:14.405712Z","shell.execute_reply":"2022-04-19T10:55:14.438216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Helper functions\n# ====================================================\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\ndef train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    model.train()\n    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n    losses = AverageMeter()\n    start = end = time.time()\n    global_step = 0\n    for step, (inputs, labels) in enumerate(train_loader):\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        with torch.cuda.amp.autocast(enabled=CFG.apex):\n            y_preds = model(inputs)\n        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        scaler.scale(loss).backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            global_step += 1\n            if CFG.batch_scheduler:\n                scheduler.step()\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f}  '\n                  'LR: {lr:.8f}  '\n                  .format(epoch+1, step, len(train_loader), \n                          remain=timeSince(start, float(step+1)/len(train_loader)),\n                          loss=losses,\n                          grad_norm=grad_norm,\n                          lr=scheduler.get_lr()[0]))\n        if CFG.wandb:\n            wandb.log({f\"[fold{fold}] loss\": losses.val,\n                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n    return losses.avg\n\n\ndef valid_fn(valid_loader, model, criterion, device):\n    losses = AverageMeter()\n    model.eval()\n    preds = []\n    start = end = time.time()\n    for step, (inputs, labels) in enumerate(valid_loader):\n        #load vaildation data to gpu\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}/{1}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(step, len(valid_loader),\n                          loss=losses,\n                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n    predictions = np.concatenate(preds)\n    return losses.avg, predictions\n\n\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"papermill":{"duration":0.044153,"end_time":"2021-11-16T19:33:21.148373","exception":false,"start_time":"2021-11-16T19:33:21.10422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T10:55:14.440029Z","iopub.execute_input":"2022-04-19T10:55:14.440282Z","iopub.status.idle":"2022-04-19T10:55:14.466037Z","shell.execute_reply.started":"2022-04-19T10:55:14.440248Z","shell.execute_reply":"2022-04-19T10:55:14.465173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    \n    def get_result(oof_df):\n        labels = create_labels_for_scoring(oof_df)\n        predictions = oof_df[[i for i in range(CFG.max_len)]].values\n        char_probs = get_char_probs(oof_df['pn_history'].values, predictions, CFG.tokenizer)\n        results = get_results(char_probs, th=0.5)\n        preds = get_predictions(results)\n        score = get_score(labels, preds)\n        LOGGER.info(f'Score: {score:<.4f}')\n    \n    if CFG.train:\n        oof_df = pd.DataFrame()\n        #    n_fold=5\n        #    trn_fold=[0, 1, 2, 3, 4]\n        for fold in range(CFG.n_fold):\n            if fold in CFG.trn_fold:\n#train.head() is like:                 \n#id\tcase_num\tpn_num\tfeature_num\tannotation\tlocation\tfeature_text\tpn_history-> fold\n# 0\t00016_000\t0\t16\t0\t[dad with recent heart attcak]\t[696 724]\tFamily-history-of-MI-OR-Family-history-of-myoc...\tHPI: 17yo M presents with palpitations. Patien...\n# 1\t00016_001\t0\t16\t1\t[mom with \"thyroid disease]\t[668 693]\tFamily-history-of-thyroid-disorder\tHPI: 17yo M presents with palpitations. Patien...\n# 2\t00016_002\t0\t16\t2\t[chest pressure]\t[203 217]\tChest-pressure\tHPI: 17yo M presents with palpitations. Patien...\n# 3\t00016_003\t0\t16\t3\t[intermittent episodes, episode]\t[70 91, 176 183]\tIntermittent-symptoms\tHPI: 17yo M presents with palpitations. Patien...\n# 4\t00016_004\t0\t16\t4\t[felt as if he were going to pass out]\t[222 258]\tLightheaded\tHPI: 17yo M presents with palpitations. Patien...\n                _oof_df = train_loop(train, fold)\n                oof_df = pd.concat([oof_df, _oof_df])\n                LOGGER.info(f\"========== fold: {fold} result ==========\")\n                get_result(_oof_df)\n        oof_df = oof_df.reset_index(drop=True)\n#         print(\"oof_df shape=\"+oof_df.shape)\n        LOGGER.info(f\"========== CV ==========\")\n        get_result(oof_df)\n        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n#         oof_df.to_csv(OUTPUT_DIR+'submission.csv',index=False)\n#         model.predcit \n    if CFG.wandb:\n        wandb.finish()\n        ","metadata":{"papermill":{"duration":4899.627687,"end_time":"2021-11-16T20:55:00.880155","exception":false,"start_time":"2021-11-16T19:33:21.252468","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-19T10:55:14.467592Z","iopub.execute_input":"2022-04-19T10:55:14.467841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred,pred.shape,labels\n# # with \n# !cd ../input/nbme-deberta-base-baseline-train/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}