{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Roberta Strikes Back !\n\nThis notebook demonstates that you can reach decent performances with Roberta, if you process predictions correctly.\n\nThe training procedure will not be shared as it would most likely destroy the leaderboard.","metadata":{}},{"cell_type":"markdown","source":"# Initialization","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport ast\nimport json\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-03T15:27:39.510177Z","iopub.execute_input":"2022-03-03T15:27:39.510834Z","iopub.status.idle":"2022-03-03T15:27:39.586664Z","shell.execute_reply.started":"2022-03-03T15:27:39.510735Z","shell.execute_reply":"2022-03-03T15:27:39.586014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Paths","metadata":{}},{"cell_type":"code","source":"DATA_PATH = \"../input/nbme-score-clinical-patient-notes/\"\nOUT_PATH = \"../input/nbme-roberta-large/\"\nWEIGHTS_FOLDER = \"../input/nbme-roberta-large/\"\n\nNUM_WORKERS = 2","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-03T15:27:39.635986Z","iopub.execute_input":"2022-03-03T15:27:39.636179Z","iopub.status.idle":"2022-03-03T15:27:39.639757Z","shell.execute_reply.started":"2022-03-03T15:27:39.636155Z","shell.execute_reply":"2022-03-03T15:27:39.639093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"markdown","source":"## Preparation","metadata":{}},{"cell_type":"code","source":"def process_feature_text(text):\n    text = re.sub('I-year', '1-year', text)\n    text = re.sub('-OR-', \" or \", text)\n    text = re.sub('-', ' ', text)\n    return text\n\n\ndef clean_spaces(txt):\n    txt = re.sub('\\n', ' ', txt)\n    txt = re.sub('\\t', ' ', txt)\n    txt = re.sub('\\r', ' ', txt)\n#     txt = re.sub(r'\\s+', ' ', txt)\n    return txt\n\n\ndef load_and_prepare_test(root=\"\"):\n    patient_notes = pd.read_csv(root + \"patient_notes.csv\")\n    features = pd.read_csv(root + \"features.csv\")\n    df = pd.read_csv(root + \"test.csv\")\n\n    df = df.merge(features, how=\"left\", on=[\"case_num\", \"feature_num\"])\n    df = df.merge(patient_notes, how=\"left\", on=['case_num', 'pn_num'])\n\n    df['pn_history'] = df['pn_history'].apply(lambda x: x.strip())\n    df['feature_text'] = df['feature_text'].apply(process_feature_text)\n\n    df['feature_text'] = df['feature_text'].apply(clean_spaces)\n    df['clean_text'] = df['pn_history'].apply(clean_spaces)\n\n    df['target'] = \"\"\n    return df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-03T15:27:39.646154Z","iopub.execute_input":"2022-03-03T15:27:39.646514Z","iopub.status.idle":"2022-03-03T15:27:39.655323Z","shell.execute_reply.started":"2022-03-03T15:27:39.646488Z","shell.execute_reply":"2022-03-03T15:27:39.654476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Processing","metadata":{}},{"cell_type":"code","source":"import itertools\n\n\ndef token_pred_to_char_pred(token_pred, offsets):\n    char_pred = np.zeros((np.max(offsets), token_pred.shape[1]))\n    for i in range(len(token_pred)):\n        s, e = int(offsets[i][0]), int(offsets[i][1])  # start, end\n        char_pred[s:e] = token_pred[i]\n\n        if token_pred.shape[1] == 3:  # following characters cannot be tagged as start\n            s += 1\n            char_pred[s: e, 1], char_pred[s: e, 2] = (\n                np.max(char_pred[s: e, 1:], 1),\n                np.min(char_pred[s: e, 1:], 1),\n            )\n\n    return char_pred\n\n\ndef labels_to_sub(labels):\n    all_spans = []\n    for label in labels:\n        indices = np.where(label > 0)[0]\n        indices_grouped = [\n            list(g) for _, g in itertools.groupby(\n                indices, key=lambda n, c=itertools.count(): n - next(c)\n            )\n        ]\n\n        spans = [f\"{min(r)} {max(r) + 1}\" for r in indices_grouped]\n        all_spans.append(\";\".join(spans))\n    return all_spans\n\n\ndef char_target_to_span(char_target):\n    spans = []\n    start, end = 0, 0\n    for i in range(len(char_target)):\n        if char_target[i] == 1 and char_target[i - 1] == 0:\n            if end:\n                spans.append([start, end])\n            start = i\n            end = i + 1\n        elif char_target[i] == 1:\n            end = i + 1\n        else:\n            if end:\n                spans.append([start, end])\n            start, end = 0, 0\n    return spans","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-03T15:27:39.657146Z","iopub.execute_input":"2022-03-03T15:27:39.657394Z","iopub.status.idle":"2022-03-03T15:27:39.673602Z","shell.execute_reply.started":"2022-03-03T15:27:39.65736Z","shell.execute_reply":"2022-03-03T15:27:39.672908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom transformers import AutoTokenizer\n\n\ndef get_tokenizer(name, precompute=False, df=None, folder=None):\n    if folder is None:\n        tokenizer = AutoTokenizer.from_pretrained(name)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(folder)\n\n    tokenizer.name = name\n    tokenizer.special_tokens = {\n        \"sep\": tokenizer.sep_token_id,\n        \"cls\": tokenizer.cls_token_id,\n        \"pad\": tokenizer.pad_token_id,\n    }\n\n    if precompute:\n        tokenizer.precomputed = precompute_tokens(df, tokenizer)\n    else:\n        tokenizer.precomputed = None\n\n    return tokenizer\n\n\ndef precompute_tokens(df, tokenizer):\n    feature_texts = df[\"feature_text\"].unique()\n\n    ids = {}\n    offsets = {}\n\n    for feature_text in feature_texts:\n        encoding = tokenizer(\n            feature_text,\n            return_token_type_ids=True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        ids[feature_text] = encoding[\"input_ids\"]\n        offsets[feature_text] = encoding[\"offset_mapping\"]\n\n    texts = df[\"clean_text\"].unique()\n\n    for text in texts:\n        encoding = tokenizer(\n            text,\n            return_token_type_ids=True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        ids[text] = encoding[\"input_ids\"]\n        offsets[text] = encoding[\"offset_mapping\"]\n\n    return {\"ids\": ids, \"offsets\": offsets}\n\n\ndef encodings_from_precomputed(feature_text, text, precomputed, tokenizer, max_len=300):\n    tokens = tokenizer.special_tokens\n\n    # Input ids\n    if \"roberta\" in tokenizer.name:\n        qa_sep = [tokens[\"sep\"], tokens[\"sep\"]]\n    else:\n        qa_sep = [tokens[\"sep\"]]\n\n    input_ids = [tokens[\"cls\"]] + precomputed[\"ids\"][feature_text] + qa_sep\n    n_question_tokens = len(input_ids)\n\n    input_ids += precomputed[\"ids\"][text]\n    input_ids = input_ids[: max_len - 1] + [tokens[\"sep\"]]\n\n    # Token type ids\n    if \"roberta\" not in tokenizer.name:\n        token_type_ids = np.ones(len(input_ids))\n        token_type_ids[:n_question_tokens] = 0\n        token_type_ids = token_type_ids.tolist()\n    else:\n        token_type_ids = [0] * len(input_ids)\n\n    # Offsets\n    offsets = [(0, 0)] * n_question_tokens + precomputed[\"offsets\"][text]\n    offsets = offsets[: max_len - 1] + [(0, 0)]\n\n    # Padding\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([tokens[\"pad\"]] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        offsets = offsets + ([(0, 0)] * padding_length)\n\n    encoding = {\n        \"input_ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"offset_mapping\": offsets,\n    }\n\n    return encoding\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-03T15:27:39.676545Z","iopub.execute_input":"2022-03-03T15:27:39.677014Z","iopub.status.idle":"2022-03-03T15:27:41.387994Z","shell.execute_reply.started":"2022-03-03T15:27:39.67698Z","shell.execute_reply":"2022-03-03T15:27:41.387258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import Dataset\n\n\nclass PatientNoteDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n\n        self.texts = df['clean_text'].values\n        self.feature_text = df['feature_text'].values\n        self.char_targets = df['target'].values.tolist()\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        feature_text = self.feature_text[idx]\n        char_target = self.char_targets[idx]\n\n        # Tokenize\n        if self.tokenizer.precomputed is None:\n            encoding = self.tokenizer(\n                feature_text,\n                text,\n                return_token_type_ids=True,\n                return_offsets_mapping=True,\n                return_attention_mask=False,\n                truncation=\"only_second\",\n                max_length=self.max_len,\n                padding='max_length',\n            )\n            raise NotImplementedError(\"fix issues with question offsets\")\n        else:\n            encoding = encodings_from_precomputed(\n                feature_text,\n                text,\n                self.tokenizer.precomputed,\n                self.tokenizer,\n                max_len=self.max_len\n            )\n\n        return {\n            \"ids\": torch.tensor(encoding[\"input_ids\"], dtype=torch.long),\n            \"token_type_ids\": torch.tensor(encoding[\"token_type_ids\"], dtype=torch.long),\n            \"target\": torch.tensor([0], dtype=torch.float),\n            \"offsets\": np.array(encoding[\"offset_mapping\"]),\n            \"text\": text,\n        }\n\n    def __len__(self):\n        return len(self.texts)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-03T15:27:41.391154Z","iopub.execute_input":"2022-03-03T15:27:41.391349Z","iopub.status.idle":"2022-03-03T15:27:41.402216Z","shell.execute_reply.started":"2022-03-03T15:27:41.391324Z","shell.execute_reply":"2022-03-03T15:27:41.401452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot predictions","metadata":{}},{"cell_type":"code","source":"import spacy\nimport numpy as np\n\ndef plot_annotation(df, pn_num):\n    options = {\"colors\": {}}\n\n    df_text = df[df[\"pn_num\"] == pn_num].reset_index(drop=True)\n\n    text = df_text[\"pn_history\"][0]\n    ents = []\n\n    for spans, feature_text, feature_num in df_text[[\"span\", \"feature_text\", \"feature_num\"]].values:\n        for s in spans:\n            ents.append({\"start\": int(s[0]), \"end\": int(s[1]), \"label\": feature_text})\n\n        options[\"colors\"][feature_text] =  f\"rgb{tuple(np.random.randint(100, 255, size=3))}\"\n\n    doc = {\"text\": text, \"ents\": sorted(ents, key=lambda i: i[\"start\"])}\n\n    spacy.displacy.render(doc, style=\"ent\", options=options, manual=True, jupyter=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-03T15:27:41.404783Z","iopub.execute_input":"2022-03-03T15:27:41.405345Z","iopub.status.idle":"2022-03-03T15:27:50.10609Z","shell.execute_reply.started":"2022-03-03T15:27:41.405307Z","shell.execute_reply":"2022-03-03T15:27:50.105297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport transformers\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModel\n\n\nclass NERTransformer(nn.Module):\n    def __init__(\n        self,\n        model,\n        num_classes=1,\n        config_file=None,\n        pretrained=True,\n    ):\n        super().__init__()\n        self.name = model\n        self.pad_idx = 1 if \"roberta\" in self.name else 0\n\n        transformers.logging.set_verbosity_error()\n\n        if config_file is None:\n            config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n        else:\n            config = torch.load(config_file)\n\n        if pretrained:\n            self.transformer = AutoModel.from_pretrained(model, config=config)\n        else:\n            self.transformer = AutoModel.from_config(config)\n\n        self.nb_features = config.hidden_size\n\n#         self.cnn = nn.Identity()\n        self.logits = nn.Linear(self.nb_features, num_classes)\n\n    def forward(self, tokens, token_type_ids):\n        \"\"\"\n        Usual torch forward function\n\n        Arguments:\n            tokens {torch tensor} -- Sentence tokens\n            token_type_ids {torch tensor} -- Sentence tokens ids\n        \"\"\"\n        hidden_states = self.transformer(\n            tokens,\n            attention_mask=(tokens != self.pad_idx).long(),\n            token_type_ids=token_type_ids,\n        )[-1]\n\n        features = hidden_states[-1]\n\n        logits = self.logits(features)\n\n        return logits","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-03T15:27:50.107662Z","iopub.execute_input":"2022-03-03T15:27:50.107918Z","iopub.status.idle":"2022-03-03T15:27:50.126337Z","shell.execute_reply.started":"2022-03-03T15:27:50.107884Z","shell.execute_reply":"2022-03-03T15:27:50.125623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"## Loads weights","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef load_model_weights(model, filename, verbose=1, cp_folder=\"\", strict=True):\n    \"\"\"\n    Loads the weights of a PyTorch model. The exception handles cpu/gpu incompatibilities.\n\n    Args:\n        model (torch model): Model to load the weights to.\n        filename (str): Name of the checkpoint.\n        verbose (int, optional): Whether to display infos. Defaults to 1.\n        cp_folder (str, optional): Folder to load from. Defaults to \"\".\n        strict (bool, optional): Whether to allow missing/additional keys. Defaults to False.\n\n    Returns:\n        torch model: Model with loaded weights.\n    \"\"\"\n    if verbose:\n        print(f\"\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n\")\n\n    try:\n        model.load_state_dict(\n            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n            strict=strict,\n        )\n    except RuntimeError:\n        model.encoder.fc = torch.nn.Linear(model.nb_ft, 1)\n        model.load_state_dict(\n            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n            strict=strict,\n        )\n\n    return model","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-03T15:27:50.127743Z","iopub.execute_input":"2022-03-03T15:27:50.128235Z","iopub.status.idle":"2022-03-03T15:27:50.136078Z","shell.execute_reply.started":"2022-03-03T15:27:50.128194Z","shell.execute_reply":"2022-03-03T15:27:50.135423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\n\n\ndef predict(model, dataset, data_config, activation=\"softmax\"):\n    \"\"\"\n    Usual predict torch function\n    \"\"\"\n    model.eval()\n\n    loader = DataLoader(\n        dataset,\n        batch_size=data_config['val_bs'],\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n\n    preds = []\n    with torch.no_grad():\n        for data in tqdm(loader):\n            ids, token_type_ids = data[\"ids\"], data[\"token_type_ids\"]\n\n            y_pred = model(ids.cuda(), token_type_ids.cuda())\n\n            if activation == \"sigmoid\":\n                y_pred = y_pred.sigmoid()\n            elif activation == \"softmax\":\n                y_pred = y_pred.softmax(-1)\n\n            preds += [\n                token_pred_to_char_pred(y, offsets) for y, offsets\n                in zip(y_pred.detach().cpu().numpy(), data[\"offsets\"].numpy())\n            ]\n\n    return preds\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-03T15:27:50.137313Z","iopub.execute_input":"2022-03-03T15:27:50.137792Z","iopub.status.idle":"2022-03-03T15:27:50.149248Z","shell.execute_reply.started":"2022-03-03T15:27:50.137754Z","shell.execute_reply":"2022-03-03T15:27:50.148455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def inference_test(df, exp_folder, config, cfg_folder=None):\n    preds = []\n\n    if cfg_folder is not None:\n        model_config_file = cfg_folder + config.name.split('/')[-1] + \"/config.pth\"\n        tokenizer_folder = cfg_folder + config.name.split('/')[-1] + \"/tokenizers/\"\n    else:\n        model_config_file, tokenizer_folder = None, None\n\n    tokenizer = get_tokenizer(\n        config.name, precompute=config.precompute_tokens, df=df, folder=tokenizer_folder\n    )\n\n    dataset = PatientNoteDataset(\n        df,\n        tokenizer,\n        max_len=config.max_len,\n    )\n\n    model = NERTransformer(\n        config.name,\n        num_classes=config.num_classes,\n        config_file=model_config_file,\n        pretrained=False\n    ).cuda()\n    model.zero_grad()\n\n    weights = sorted(glob.glob(exp_folder + \"*.pt\"))\n    for weight in weights:\n        model = load_model_weights(model, weight)\n\n        pred = predict(\n            model,\n            dataset,\n            data_config=config.data_config,\n            activation=config.loss_config[\"activation\"]\n        )\n        preds.append(pred)\n\n    return preds","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-03T15:27:50.151658Z","iopub.execute_input":"2022-03-03T15:27:50.152158Z","iopub.status.idle":"2022-03-03T15:27:50.162376Z","shell.execute_reply.started":"2022-03-03T15:27:50.152122Z","shell.execute_reply":"2022-03-03T15:27:50.161539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    # Architecture\n    name = \"roberta-large\"\n    num_classes = 1\n\n    # Texts\n    max_len = 310\n    precompute_tokens = True\n\n    # Training    \n    loss_config = {\n        \"activation\": \"sigmoid\",\n    }\n\n    data_config = {\n        \"val_bs\": 16 if \"large\" in name else 32,\n        \"pad_token\": 1 if \"roberta\" in name else 0,\n    }\n\n    verbose = 1","metadata":{"execution":{"iopub.status.busy":"2022-03-03T15:27:50.163779Z","iopub.execute_input":"2022-03-03T15:27:50.164045Z","iopub.status.idle":"2022-03-03T15:27:50.17508Z","shell.execute_reply.started":"2022-03-03T15:27:50.164008Z","shell.execute_reply":"2022-03-03T15:27:50.174327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"code","source":"df_test = load_and_prepare_test(root=DATA_PATH)\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T15:27:50.179024Z","iopub.execute_input":"2022-03-03T15:27:50.179225Z","iopub.status.idle":"2022-03-03T15:27:51.1212Z","shell.execute_reply.started":"2022-03-03T15:27:50.1792Z","shell.execute_reply":"2022-03-03T15:27:51.120511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"preds = inference_test(\n    df_test,\n    WEIGHTS_FOLDER,\n    Config,\n    cfg_folder=OUT_PATH\n)[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-03T15:27:51.122392Z","iopub.execute_input":"2022-03-03T15:27:51.122671Z","iopub.status.idle":"2022-03-03T15:36:09.199855Z","shell.execute_reply.started":"2022-03-03T15:27:51.122642Z","shell.execute_reply":"2022-03-03T15:36:09.198766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['preds'] = preds\ndf_test['preds'] = df_test.apply(lambda x: x['preds'][:len(x['clean_text'])], 1)\n\ndf_test['preds'] = df_test['preds'].apply(lambda x: (x > 0.5).flatten())","metadata":{"execution":{"iopub.status.busy":"2022-03-03T15:38:24.715432Z","iopub.execute_input":"2022-03-03T15:38:24.716194Z","iopub.status.idle":"2022-03-03T15:38:25.036669Z","shell.execute_reply.started":"2022-03-03T15:38:24.716156Z","shell.execute_reply":"2022-03-03T15:38:25.035924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot predictions","metadata":{}},{"cell_type":"code","source":"try:\n    df_test['span'] = df_test['preds'].apply(char_target_to_span)\n    plot_annotation(df_test, df_test['pn_num'][0])\nexcept:\n    pass","metadata":{"execution":{"iopub.status.busy":"2022-03-03T15:38:26.645771Z","iopub.execute_input":"2022-03-03T15:38:26.646323Z","iopub.status.idle":"2022-03-03T15:39:13.191209Z","shell.execute_reply.started":"2022-03-03T15:38:26.646284Z","shell.execute_reply":"2022-03-03T15:39:13.190477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unlike for deberta, spaces are not included in the offsets, we need to add them manually otherwise this will hurt performances.","metadata":{}},{"cell_type":"markdown","source":"## Post-processing","metadata":{}},{"cell_type":"code","source":"def post_process_spaces(target, text):\n    target = np.copy(target)\n\n    if len(text) > len(target):\n        padding = np.zeros(len(text) - len(target))\n        target = np.concatenate([target, padding])\n    else:\n        target = target[:len(text)]\n\n    if text[0] == \" \":\n        target[0] = 0\n    if text[-1] == \" \":\n        target[-1] = 0\n\n    for i in range(1, len(text) - 1):\n        if text[i] == \" \":\n            if target[i] and not target[i - 1]:  # space before\n                target[i] = 0\n\n            if target[i] and not target[i + 1]:  # space after\n                target[i] = 0\n\n            if target[i - 1] and target[i + 1]:\n                target[i] = 1\n\n    return target","metadata":{"execution":{"iopub.status.busy":"2022-03-03T15:40:01.858078Z","iopub.execute_input":"2022-03-03T15:40:01.858343Z","iopub.status.idle":"2022-03-03T15:40:01.868163Z","shell.execute_reply.started":"2022-03-03T15:40:01.858312Z","shell.execute_reply":"2022-03-03T15:40:01.86748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['preds_pp'] = df_test.apply(lambda x: post_process_spaces(x['preds'], x['clean_text']), 1)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T15:40:04.86552Z","iopub.execute_input":"2022-03-03T15:40:04.86623Z","iopub.status.idle":"2022-03-03T15:40:06.809997Z","shell.execute_reply.started":"2022-03-03T15:40:04.866191Z","shell.execute_reply":"2022-03-03T15:40:06.809209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    df_test['span'] = df_test['preds_pp'].apply(char_target_to_span)\n    plot_annotation(df_test, df_test['pn_num'][0])\nexcept:\n    pass","metadata":{"execution":{"iopub.status.busy":"2022-03-03T15:40:11.071676Z","iopub.execute_input":"2022-03-03T15:40:11.071995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"df_test['location'] = labels_to_sub(df_test['preds_pp'].values)\n\nsub = pd.read_csv(DATA_PATH + 'sample_submission.csv')\n\nsub = sub[['id']].merge(df_test[['id', \"location\"]], how=\"left\", on=\"id\")\n\nsub.to_csv('submission.csv', index=False)\n\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Done ! ","metadata":{}}]}