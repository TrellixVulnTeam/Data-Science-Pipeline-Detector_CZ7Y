{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from ast import literal_eval\nfrom itertools import chain\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModel, AutoTokenizer, RobertaTokenizerFast, RobertaModel\nimport pickle","metadata":{"id":"fkd3brizLJDa","outputId":"ef475ed1-b685-4b08-e0eb-b8076cdfe9fe","execution":{"iopub.status.busy":"2022-04-11T14:09:46.667981Z","iopub.execute_input":"2022-04-11T14:09:46.668331Z","iopub.status.idle":"2022-04-11T14:09:49.367622Z","shell.execute_reply.started":"2022-04-11T14:09:46.668244Z","shell.execute_reply":"2022-04-11T14:09:49.366873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\n#os.system('pip uninstall -y transformers')\n#os.system('python -m pip install --no-index --find-links=../input/nbme-pip-wheels transformers')\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:09:49.368902Z","iopub.execute_input":"2022-04-11T14:09:49.369086Z","iopub.status.idle":"2022-04-11T14:09:54.117897Z","shell.execute_reply.started":"2022-04-11T14:09:49.369064Z","shell.execute_reply":"2022-04-11T14:09:54.117007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import pickle\n#from transformers import AutoTokenizer, AutoModel\n#tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")  # BERT model\n#with open(\"RobertaTokenizerFast.pkl\", \"wb\") as at:\n#    pickle.dump(tokenizer, at)\n#bert = RobertaModel.from_pretrained(\"roberta-base\")  # BERT model\n#with open(\"RobertaModel.pkl\", \"wb\") as am:\n#    pickle.dump(bert, am)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import pickle\n#from transformers import AutoTokenizer, AutoModel\n#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")  # BERT model\n#with open(\"debertaTokenizerFast.pkl\", \"wb\") as at:\n#   pickle.dump(tokenizer, at)\n#bert = AutoModel.from_pretrained(\"microsoft/deberta-base\")  # BERT model\n#with open(\"debertaModel.pkl\", \"wb\") as am:\n#    pickle.dump(bert, am)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:57:01.666347Z","iopub.execute_input":"2022-03-26T00:57:01.666681Z","iopub.status.idle":"2022-03-26T00:57:58.047234Z","shell.execute_reply.started":"2022-03-26T00:57:01.666645Z","shell.execute_reply":"2022-03-26T00:57:58.046266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    path=\"../input/nbme-deberta-base-baseline-train/\"\n    config_path='../input/20220410/nbme_deberta_config.pth'\n    model=\"microsoft/deberta-base\"\n    batch_size=24\n    fc_dropout=0.2\n    max_len=316\n    seed=42\n    n_fold=2\n    trn_fold=[0,1]","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:20:26.370654Z","iopub.execute_input":"2022-04-11T14:20:26.37272Z","iopub.status.idle":"2022-04-11T14:20:26.382514Z","shell.execute_reply.started":"2022-04-11T14:20:26.372626Z","shell.execute_reply":"2022-04-11T14:20:26.381671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/20220410/debertaTokenizerFast.pkl\", \"rb\") as t:\n    CFG.tokenizer = pickle.load(t)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:20:29.054999Z","iopub.execute_input":"2022-04-11T14:20:29.055705Z","iopub.status.idle":"2022-04-11T14:20:29.193889Z","shell.execute_reply.started":"2022-04-11T14:20:29.055659Z","shell.execute_reply":"2022-04-11T14:20:29.19273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n\ndef micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    return f1_score(truths, preds)\n\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n    return binary\n\n\ndef span_micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n    return micro_f1(bin_preds, bin_truths)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:10:20.103589Z","iopub.execute_input":"2022-04-11T14:10:20.103876Z","iopub.status.idle":"2022-04-11T14:10:20.122256Z","shell.execute_reply.started":"2022-04-11T14:10:20.103846Z","shell.execute_reply":"2022-04-11T14:10:20.121346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_labels_for_scoring(df):\n    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n        if len(location_list) > 0:\n            location = location_list[0]\n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n    return truths\n\n\ndef get_char_probs(texts, predictions, tokenizer):\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n    return results\n\n\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n    return results\n\n\ndef get_predictions(results):\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:10:22.330931Z","iopub.execute_input":"2022-04-11T14:10:22.331235Z","iopub.status.idle":"2022-04-11T14:10:22.346246Z","shell.execute_reply.started":"2022-04-11T14:10:22.331205Z","shell.execute_reply":"2022-04-11T14:10:22.345452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = span_micro_f1(y_true, y_pred)\n    return score\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:10:24.014742Z","iopub.execute_input":"2022-04-11T14:10:24.014986Z","iopub.status.idle":"2022-04-11T14:10:24.022678Z","shell.execute_reply.started":"2022-04-11T14:10:24.014961Z","shell.execute_reply":"2022-04-11T14:10:24.021988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text, feature_text):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           max_length=CFG.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item])\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:10:25.304845Z","iopub.execute_input":"2022-04-11T14:10:25.305275Z","iopub.status.idle":"2022-04-11T14:10:25.312479Z","shell.execute_reply.started":"2022-04-11T14:10:25.30523Z","shell.execute_reply":"2022-04-11T14:10:25.311847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            with open(\"../input/20220410/nbme_deberta_config.pth\", \"rb\") as t:\n                self.config = torch.load(config_path)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            with open(\"../input/20220410/nbme_deberta_config.pth\", \"rb\") as t:\n                self.model = torch.load(\"../input/20220410/debertaModel.pkl\")\n            #self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:10:27.005967Z","iopub.execute_input":"2022-04-11T14:10:27.007011Z","iopub.status.idle":"2022-04-11T14:10:27.018108Z","shell.execute_reply.started":"2022-04-11T14:10:27.006972Z","shell.execute_reply":"2022-04-11T14:10:27.017281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:10:28.6522Z","iopub.execute_input":"2022-04-11T14:10:28.652478Z","iopub.status.idle":"2022-04-11T14:10:28.658297Z","shell.execute_reply.started":"2022-04-11T14:10:28.65245Z","shell.execute_reply":"2022-04-11T14:10:28.65733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_URL = \"../input/nbme-score-clinical-patient-notes\"\n\n\ndef process_feature_text(text):\n    return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n\n\ndef prepare_datasets():\n    print(\"prepare_datasets\")\n    features = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    df = pd.read_csv(f\"{BASE_URL}/test.csv\")\n    #df['annotation'] = df['annotation'].apply(ast.literal_eval) #中身をstr→kist型に変換\n    #df['location'] = df['location'].apply(ast.literal_eval)\n\n    merged = df.merge(notes, how=\"left\")\n    merged = merged.merge(features, how=\"left\")\n\n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    #[]も含めてstr形で格納されているためリスト型に変更\n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n\n    return merged","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:10:44.37674Z","iopub.execute_input":"2022-04-11T14:10:44.377035Z","iopub.status.idle":"2022-04-11T14:10:44.384972Z","shell.execute_reply.started":"2022-04-11T14:10:44.377004Z","shell.execute_reply":"2022-04-11T14:10:44.383934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = prepare_datasets()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:20:37.923358Z","iopub.execute_input":"2022-04-11T14:20:37.923662Z","iopub.status.idle":"2022-04-11T14:20:38.421408Z","shell.execute_reply.started":"2022-04-11T14:20:37.923628Z","shell.execute_reply":"2022-04-11T14:20:38.420432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:10:47.577865Z","iopub.execute_input":"2022-04-11T14:10:47.57814Z","iopub.status.idle":"2022-04-11T14:10:47.583842Z","shell.execute_reply.started":"2022-04-11T14:10:47.578099Z","shell.execute_reply":"2022-04-11T14:10:47.583189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(\"../input/20220410/nbme_deberta_v3.pth\",map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:20:39.61529Z","iopub.execute_input":"2022-04-11T14:20:39.616373Z","iopub.status.idle":"2022-04-11T14:21:07.385896Z","shell.execute_reply.started":"2022-04-11T14:20:39.616329Z","shell.execute_reply":"2022-04-11T14:21:07.384291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/nbme-score-clinical-patient-notes/sample_submission.csv')\nresults = get_results(predictions, th=0.48)\nsubmission['location'] = results\ndisplay(submission.head())\nsubmission[['id', 'location']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:22:52.340968Z","iopub.execute_input":"2022-04-11T14:22:52.341367Z","iopub.status.idle":"2022-04-11T14:22:52.362159Z","shell.execute_reply.started":"2022-04-11T14:22:52.34133Z","shell.execute_reply":"2022-04-11T14:22:52.36121Z"},"trusted":true},"execution_count":null,"outputs":[]}]}