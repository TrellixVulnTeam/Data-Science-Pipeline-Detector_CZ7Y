{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 개요\n의사가 되기 위해서는 환자 노트를 작성하는 시험을 본다.\n롤 플레이처럼 환자역할을 하는 사람(특정 질병에 대해서 묘사하도록 훈련된사람)에게 증상을 듣고 환자 노트를 작성하는 방식으로 시험을 본다.\n환자 노트를 작성하는 **시험** 을 본다는 의미는 -> 누군가 채점을 해야한다는 것..\n\n이런 시험은 채점이 정말 오래걸리고 힘들다. 돈도 많이든다. 따라서 자연어처리를 통해서 해결해보려고 여러가지 노력이 있었지.\n하지만 이 마저도 여의치않아. 표현이 너무 다양하기 떄문에\n예를들어 \"활동을 안하고싶어한다\"와 \"이제 테니스를 안칠꺼다\" 는 같은 뜻이 될 수 있다\n또, \"손이차고, 머리가 빠지고, 심장이 뛰고 등등등\"라는 문장은 \"갑상선에 trouble이 있음\" 으로 표현되기도 한다.\n\n우리는 특정 증상과 그를 표현하는 다양한 문장들을 매핑하는 솔루션을 만들어야함\nex) \"식욕 떨어짐\" == \"식욕 없다\", \"옷이 점점 루즈핏이 된다\" and etc","metadata":{}},{"cell_type":"markdown","source":"# 데이터 description\n\n## 중요한 용어\n- Clinical Case: 표준화된 환자가 테스트보는사람에게 제공되는 시나리오. 10개의 사례가 있음\n- Patient Note: 환자와 관련된 중요한 정보 디테일하게 작성한거. (몸 검사 및 인터뷰)\n- Feature: clinically 한 개념. rublic은 각 케이스와 관련이 있는 주요 개념을 설명함.\n\n## training Data\n- patient_notes.csv - 40000개의 환자노트 기록 모음. 이중에 일부만 annotated features를 갖고있음. 아 어노테이션 빼고 unsupervised learning 하고싶어할수도있음. 테스트셋에있는 환자노트는 public version에 없습니다.(당연)\n  - pn_num - 환자노트 id\n  - case_num - 환자노트의 clinical case의 id\n  - pn_history - 시험본사람이 기록한 text\n\n- features.csv - 각 clinical case의 채점기준표.\n  - feature_num - feature의 id\n  - case_num - clinical case의 id.\n  - feature_text - 특징들의 description\n\n- train.csv - 환자노트 1000개의 특징 annotation. 각 10개에 100개씩 있음.\n  - id - id. pn_num + feature_num 을 pair로 해서 만들어짐\n  - pn_num - 환자노트id.\n  - feature_num - feature id\n  - case_num - case num\n  - annotation - feature를 가리키는 환자노트의 text. 하나의 노트에서 feature 하나가 여러번 나올 수 있음.\n  - location - annotation의 idx 범위. 세미콜론(;)으로 구문되며 from,to 로 되어있는 듯?\n  \n## Example Test Data\n테스트데이터 넣어줌\n- test.csv - \n- sample_submission.csv - ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-26T15:34:49.527097Z","iopub.execute_input":"2022-04-26T15:34:49.527435Z","iopub.status.idle":"2022-04-26T15:34:49.622936Z","shell.execute_reply.started":"2022-04-26T15:34:49.527339Z","shell.execute_reply":"2022-04-26T15:34:49.62221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# import","metadata":{}},{"cell_type":"code","source":"# import\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\nfrom tqdm.notebook import tqdm\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom nltk.stem import WordNetLemmatizer\nfrom itertools import chain\nimport warnings\nimport json\nfrom ast import literal_eval\nimport spacy\n\nimport scipy as sp\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.metrics import precision_recall_fscore_support\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport re\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:34:49.624434Z","iopub.execute_input":"2022-04-26T15:34:49.624691Z","iopub.status.idle":"2022-04-26T15:35:01.545214Z","shell.execute_reply.started":"2022-04-26T15:34:49.624658Z","shell.execute_reply":"2022-04-26T15:35:01.544423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"# Library\n# Transformer\nos.system('pip uninstall -y transformers')\nos.system('python -m pip install --no-index --find-links=../input/my-whl transformers')\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"device :\", device)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:01.548954Z","iopub.execute_input":"2022-04-26T15:35:01.549156Z","iopub.status.idle":"2022-04-26T15:35:15.316402Z","shell.execute_reply.started":"2022-04-26T15:35:01.549132Z","shell.execute_reply":"2022-04-26T15:35:15.315652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\nunmasker = pipeline('fill-mask', model='../input/huggingface-roberta-variants/roberta-large/roberta-large')\nunmasker(\"Paris is the <mask> of France.\")","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:15.317615Z","iopub.execute_input":"2022-04-26T15:35:15.318309Z","iopub.status.idle":"2022-04-26T15:35:37.245587Z","shell.execute_reply.started":"2022-04-26T15:35:15.318263Z","shell.execute_reply":"2022-04-26T15:35:37.244918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n데이터를 한번씩 까봐야겠음","metadata":{}},{"cell_type":"code","source":"patient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\npatient_notes","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:37.247614Z","iopub.execute_input":"2022-04-26T15:35:37.247927Z","iopub.status.idle":"2022-04-26T15:35:37.895972Z","shell.execute_reply.started":"2022-04-26T15:35:37.247889Z","shell.execute_reply":"2022-04-26T15:35:37.895281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\nfeatures","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:37.897314Z","iopub.execute_input":"2022-04-26T15:35:37.897586Z","iopub.status.idle":"2022-04-26T15:35:37.914934Z","shell.execute_reply.started":"2022-04-26T15:35:37.897552Z","shell.execute_reply":"2022-04-26T15:35:37.9143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:37.916184Z","iopub.execute_input":"2022-04-26T15:35:37.91642Z","iopub.status.idle":"2022-04-26T15:35:37.957813Z","shell.execute_reply.started":"2022-04-26T15:35:37.916388Z","shell.execute_reply":"2022-04-26T15:35:37.956991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.loc[0])\nprint(train.loc[338])\nprint(train.loc[338, \"annotation\"])\nprint(train.loc[338, \"location\"])\n\nprint(train.loc[655])\nprint(train.loc[655, \"annotation\"])\nprint(train.loc[655, \"location\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:37.959282Z","iopub.execute_input":"2022-04-26T15:35:37.959578Z","iopub.status.idle":"2022-04-26T15:35:37.975803Z","shell.execute_reply.started":"2022-04-26T15:35:37.95954Z","shell.execute_reply":"2022-04-26T15:35:37.974986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# incorrect annotation\ntrain.loc[338, 'annotation'] = \"['father heart attack']\"\ntrain.loc[338, 'location'] = \"['764 783']\"\n\ntrain.loc[621, 'annotation'] = \"['for the last 2-3 months']\"\ntrain.loc[621, 'location'] = \"['77 100']\"\n\ntrain.loc[655, 'annotation'] = \"['no heat intolerance', 'no cold intolerance']\"\ntrain.loc[655, 'location'] = \"['285 292;301 312', '285 287;296 312']\"\n\ntrain.loc[1262, 'annotation'] = \"['mother thyroid problem']\"\ntrain.loc[1262, 'location'] = \"['551 557;565 580']\"\n\ntrain.loc[1265, 'annotation'] = \"['felt like he was going to \\\"pass out\\\"']\"\ntrain.loc[1265, 'location'] = \"['131 135;181 212']\"\n\ntrain.loc[1396, 'annotation'] = \"['stool , with no blood']\"\ntrain.loc[1396, 'location'] = \"['259 280']\"\n\ntrain.loc[1591, 'annotation'] = \"['diarrhoe non blooody']\"\ntrain.loc[1591, 'location'] = \"['176 184;201 212']\"\n\ntrain.loc[1615, 'annotation'] = \"['diarrhea for last 2-3 days']\"\ntrain.loc[1615, 'location'] = \"['249 257;271 288']\"\n\ntrain.loc[1664, 'annotation'] = \"['no vaginal discharge']\"\ntrain.loc[1664, 'location'] = \"['822 824;907 924']\"\n\ntrain.loc[1714, 'annotation'] = \"['started about 8-10 hours ago']\"\ntrain.loc[1714, 'location'] = \"['101 129']\"\n\ntrain.loc[1929, 'annotation'] = \"['no blood in the stool']\"\ntrain.loc[1929, 'location'] = \"['531 539;549 561']\"\n\ntrain.loc[2134, 'annotation'] = \"['last sexually active 9 months ago']\"\ntrain.loc[2134, 'location'] = \"['540 560;581 593']\"\n\ntrain.loc[2191, 'annotation'] = \"['right lower quadrant pain']\"\ntrain.loc[2191, 'location'] = \"['32 57']\"\n\ntrain.loc[2553, 'annotation'] = \"['diarrhoea no blood']\"\ntrain.loc[2553, 'location'] = \"['308 317;376 384']\"\n\ntrain.loc[3124, 'annotation'] = \"['sweating']\"\ntrain.loc[3124, 'location'] = \"['549 557']\"\n\ntrain.loc[3858, 'annotation'] = \"['previously as regular', 'previously eveyr 28-29 days', 'previously lasting 5 days', 'previously regular flow']\"\ntrain.loc[3858, 'location'] = \"['102 123', '102 112;125 141', '102 112;143 157', '102 112;159 171']\"\n\ntrain.loc[4373, 'annotation'] = \"['for 2 months']\"\ntrain.loc[4373, 'location'] = \"['33 45']\"\n\ntrain.loc[4763, 'annotation'] = \"['35 year old']\"\ntrain.loc[4763, 'location'] = \"['5 16']\"\n\ntrain.loc[4782, 'annotation'] = \"['darker brown stools']\"\ntrain.loc[4782, 'location'] = \"['175 194']\"\n\ntrain.loc[4908, 'annotation'] = \"['uncle with peptic ulcer']\"\ntrain.loc[4908, 'location'] = \"['700 723']\"\n\ntrain.loc[6016, 'annotation'] = \"['difficulty falling asleep']\"\ntrain.loc[6016, 'location'] = \"['225 250']\"\n\ntrain.loc[6192, 'annotation'] = \"['helps to take care of aging mother and in-laws']\"\ntrain.loc[6192, 'location'] = \"['197 218;236 260']\"\n\ntrain.loc[6380, 'annotation'] = \"['No hair changes', 'No skin changes', 'No GI changes', 'No palpitations', 'No excessive sweating']\"\ntrain.loc[6380, 'location'] = \"['480 482;507 519', '480 482;499 503;512 519', '480 482;521 531', '480 482;533 545', '480 482;564 582']\"\n\ntrain.loc[6562, 'annotation'] = \"['stressed due to taking care of her mother', 'stressed due to taking care of husbands parents']\"\ntrain.loc[6562, 'location'] = \"['290 320;327 337', '290 320;342 358']\"\n\ntrain.loc[6862, 'annotation'] = \"['stressor taking care of many sick family members']\"\ntrain.loc[6862, 'location'] = \"['288 296;324 363']\"\n\ntrain.loc[7022, 'annotation'] = \"['heart started racing and felt numbness for the 1st time in her finger tips']\"\ntrain.loc[7022, 'location'] = \"['108 182']\"\n\ntrain.loc[7422, 'annotation'] = \"['first started 5 yrs']\"\ntrain.loc[7422, 'location'] = \"['102 121']\"\n\ntrain.loc[8876, 'annotation'] = \"['No shortness of breath']\"\ntrain.loc[8876, 'location'] = \"['481 483;533 552']\"\n\ntrain.loc[9027, 'annotation'] = \"['recent URI', 'nasal stuffines, rhinorrhea, for 3-4 days']\"\ntrain.loc[9027, 'location'] = \"['92 102', '123 164']\"\n\ntrain.loc[9938, 'annotation'] = \"['irregularity with her cycles', 'heavier bleeding', 'changes her pad every couple hours']\"\ntrain.loc[9938, 'location'] = \"['89 117', '122 138', '368 402']\"\n\ntrain.loc[9973, 'annotation'] = \"['gaining 10-15 lbs']\"\ntrain.loc[9973, 'location'] = \"['344 361']\"\n\ntrain.loc[10513, 'annotation'] = \"['weight gain', 'gain of 10-16lbs']\"\ntrain.loc[10513, 'location'] = \"['600 611', '607 623']\"\n\ntrain.loc[11551, 'annotation'] = \"['seeing her son knows are not real']\"\ntrain.loc[11551, 'location'] = \"['386 400;443 461']\"\n\ntrain.loc[11677, 'annotation'] = \"['saw him once in the kitchen after he died']\"\ntrain.loc[11677, 'location'] = \"['160 201']\"\n\ntrain.loc[12124, 'annotation'] = \"['tried Ambien but it didnt work']\"\ntrain.loc[12124, 'location'] = \"['325 337;349 366']\"\n\ntrain.loc[12279, 'annotation'] = \"['heard what she described as a party later than evening these things did not actually happen']\"\ntrain.loc[12279, 'location'] = \"['405 459;488 524']\"\n\ntrain.loc[12289, 'annotation'] = \"['experienced seeing her son at the kitchen table these things did not actually happen']\"\ntrain.loc[12289, 'location'] = \"['353 400;488 524']\"\n\ntrain.loc[13238, 'annotation'] = \"['SCRACHY THROAT', 'RUNNY NOSE']\"\ntrain.loc[13238, 'location'] = \"['293 307', '321 331']\"\n\ntrain.loc[13297, 'annotation'] = \"['without improvement when taking tylenol', 'without improvement when taking ibuprofen']\"\ntrain.loc[13297, 'location'] = \"['182 221', '182 213;225 234']\"\n\ntrain.loc[13299, 'annotation'] = \"['yesterday', 'yesterday']\"\ntrain.loc[13299, 'location'] = \"['79 88', '409 418']\"\n\ntrain.loc[13845, 'annotation'] = \"['headache global', 'headache throughout her head']\"\ntrain.loc[13845, 'location'] = \"['86 94;230 236', '86 94;237 256']\"\n\ntrain.loc[14083, 'annotation'] = \"['headache generalized in her head']\"\ntrain.loc[14083, 'location'] = \"['56 64;156 179']\"","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:37.977709Z","iopub.execute_input":"2022-04-26T15:35:37.978161Z","iopub.status.idle":"2022-04-26T15:35:38.052497Z","shell.execute_reply.started":"2022-04-26T15:35:37.978123Z","shell.execute_reply":"2022-04-26T15:35:38.051739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.loc[0])\nprint(train.loc[338])\nprint(train.loc[338, \"annotation\"])\nprint(train.loc[338, \"location\"])\n\nprint(train.loc[655])\nprint(train.loc[655, \"annotation\"])\nprint(train.loc[655, \"location\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:38.053857Z","iopub.execute_input":"2022-04-26T15:35:38.054125Z","iopub.status.idle":"2022-04-26T15:35:38.066031Z","shell.execute_reply.started":"2022-04-26T15:35:38.054088Z","shell.execute_reply":"2022-04-26T15:35:38.065199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train 데이터의 0번 index 한번 열어보기","metadata":{}},{"cell_type":"code","source":"train[train[\"pn_num\"] == 16]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:38.070506Z","iopub.execute_input":"2022-04-26T15:35:38.070743Z","iopub.status.idle":"2022-04-26T15:35:38.086174Z","shell.execute_reply.started":"2022-04-26T15:35:38.070721Z","shell.execute_reply":"2022-04-26T15:35:38.085421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"annotation이 어떤 feature name인지 같이 보고싶음","metadata":{}},{"cell_type":"code","source":"train_merge_feature = train.merge(features[['feature_num', 'feature_text']], on='feature_num')\ntrain_merge_feature","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:38.087786Z","iopub.execute_input":"2022-04-26T15:35:38.088075Z","iopub.status.idle":"2022-04-26T15:35:38.116686Z","shell.execute_reply.started":"2022-04-26T15:35:38.088037Z","shell.execute_reply":"2022-04-26T15:35:38.115869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"pn_num이 16인거 한번 열어보기","metadata":{}},{"cell_type":"code","source":"patient_notes.loc[16]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:38.118195Z","iopub.execute_input":"2022-04-26T15:35:38.118468Z","iopub.status.idle":"2022-04-26T15:35:38.127057Z","shell.execute_reply.started":"2022-04-26T15:35:38.118435Z","shell.execute_reply":"2022-04-26T15:35:38.126146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_notes.loc[16, \"pn_num\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:38.128284Z","iopub.execute_input":"2022-04-26T15:35:38.128596Z","iopub.status.idle":"2022-04-26T15:35:38.138571Z","shell.execute_reply.started":"2022-04-26T15:35:38.128561Z","shell.execute_reply":"2022-04-26T15:35:38.137545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"location의 from / to 한번 열어보기","metadata":{}},{"cell_type":"code","source":"tmp = patient_notes.loc[16, \"pn_history\"]\nprint(tmp)\nprint('*' * 50)\ntmp[696:724]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:38.140352Z","iopub.execute_input":"2022-04-26T15:35:38.140839Z","iopub.status.idle":"2022-04-26T15:35:38.148255Z","shell.execute_reply.started":"2022-04-26T15:35:38.140804Z","shell.execute_reply":"2022-04-26T15:35:38.147393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"color visualization 해서 한번 보자.","metadata":{}},{"cell_type":"code","source":"def print_colored_patient_note(train_data, feature_data, patient_data, pn_num):\n    train_merge_feature = train.merge(features[['feature_num', 'feature_text']], on='feature_num')\n    train_specific_patient = train_merge_feature[train_merge_feature[\"pn_num\"] == pn_num]\n    \n    ents = []\n    for i, row in train_specific_patient.iterrows():\n        # print(row[\"location\"])\n        locs = literal_eval(row[\"location\"])\n        for loc in locs:\n            # print(loc)\n            begin = int(loc.split(' ')[0])\n            end = int(loc.split(' ')[-1])\n            ents.append({\n                'start' : begin,\n                'end' : end,\n                'label' : row['feature_text']\n            }\n            )\n            \n    doc = {\n        'text' : patient_data[patient_data[\"pn_num\"] == pn_num][\"pn_history\"].iloc[0],\n        \"ents\" : ents\n    }\n\n    # colors = {\"Annotation\" :\"#33FAFA\" } \n    colors = {} \n    options = {\"colors\": colors}\n    spacy.displacy.render(doc, style=\"ent\", options = options , manual=True, jupyter=True);\n    \nprint_colored_patient_note(train, features, patient_notes, 16)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:38.149712Z","iopub.execute_input":"2022-04-26T15:35:38.150646Z","iopub.status.idle":"2022-04-26T15:35:38.174873Z","shell.execute_reply.started":"2022-04-26T15:35:38.150603Z","shell.execute_reply":"2022-04-26T15:35:38.174044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"17yo, M 이런것도 유의미한 데이터 맞나..?","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\nprint(test.shape)\ntest","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:38.176217Z","iopub.execute_input":"2022-04-26T15:35:38.176543Z","iopub.status.idle":"2022-04-26T15:35:38.194197Z","shell.execute_reply.started":"2022-04-26T15:35:38.176508Z","shell.execute_reply":"2022-04-26T15:35:38.193537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/nbme-score-clinical-patient-notes/sample_submission.csv')\nsub","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:38.196936Z","iopub.execute_input":"2022-04-26T15:35:38.197137Z","iopub.status.idle":"2022-04-26T15:35:38.21296Z","shell.execute_reply.started":"2022-04-26T15:35:38.197112Z","shell.execute_reply":"2022-04-26T15:35:38.212289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = patient_notes.loc[16, \"pn_history\"]\nprint(tmp[0:100])\nprint(tmp[200:250])\nprint(tmp[300:400])\nprint(tmp[75:110])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:38.214125Z","iopub.execute_input":"2022-04-26T15:35:38.214365Z","iopub.status.idle":"2022-04-26T15:35:38.221111Z","shell.execute_reply.started":"2022-04-26T15:35:38.214333Z","shell.execute_reply":"2022-04-26T15:35:38.220322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CFG\n일단 만들어둠","metadata":{}},{"cell_type":"code","source":"class CFG:\n    debug = False\n    n_fold = 3\n    model = \"../input/huggingface-roberta-variants/roberta-large/roberta-large\"\n    dropout = 0.2\n    batch_size = 4\n    lr = 1e-5\n    max_length = 480\n    epochs = 5\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:38.22253Z","iopub.execute_input":"2022-04-26T15:35:38.222822Z","iopub.status.idle":"2022-04-26T15:35:38.228506Z","shell.execute_reply.started":"2022-04-26T15:35:38.222788Z","shell.execute_reply":"2022-04-26T15:35:38.227807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Learning DataFrame\n학습에 넣을 df 만들기","metadata":{}},{"cell_type":"code","source":"def process_feature_text(text):\n    text = re.sub('I-year', '1-year', text)\n    text = re.sub('-OR-', \" or \", text)\n    text = re.sub('-', ' ', text)\n    return text\n\n\ndef clean_spaces(txt):\n    txt = re.sub('\\n', ' ', txt)\n    txt = re.sub('\\t', ' ', txt)\n    txt = re.sub('\\r', ' ', txt)\n    return txt\n\ndef build_learning_df(train_data, feature_data, patient_data):\n    train_data[\"annotation_list\"] = [literal_eval(x) for x in train[\"annotation\"]]\n    train_data[\"location_list\"] = [literal_eval(x) for x in train[\"location\"]]\n    merged = train_data.merge(patient_data, how = \"left\")\n    merged = merged.merge(feature_data, how = \"left\")\n    merged = merged.loc[merged[\"annotation\"] != \"[]\"].copy().reset_index(drop = True) # comment out if you train all samples\n    \n#     def process_feature_text(text):\n#         return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n#     merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    \n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n    \n    merged['pn_history'] = merged['pn_history'].apply(lambda x: x.strip())\n    merged['feature_text'] = merged['feature_text'].apply(process_feature_text)\n    merged['feature_text'] = merged['feature_text'].apply(clean_spaces)\n\n    if CFG.debug:\n        merged = merged.sample(frac = 0.5).reset_index(drop = True)\n\n#     skf = StratifiedKFold(n_splits = CFG.n_fold)\n#     merged[\"stratify_on\"] = merged[\"case_num\"].astype(str) + merged[\"feature_num\"].astype(str)\n#     merged[\"fold\"] = -1\n#     for fold, (_, valid_idx) in enumerate(skf.split(merged[\"id\"], y = merged[\"stratify_on\"])):\n#         merged.loc[valid_idx, \"fold\"] = fold\n        \n    Fold = GroupKFold(n_splits=CFG.n_fold)\n    groups = merged['pn_num'].values\n    for n, (train_index, val_index) in enumerate(Fold.split(merged, merged['location'], groups)):\n        merged.loc[val_index, 'fold'] = int(n)\n    merged['fold'] = merged['fold'].astype(int)\n    \n    print(merged.shape)\n    return merged\n\nlearning_df = build_learning_df(train, features, patient_notes)\nlearning_df","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:38.230089Z","iopub.execute_input":"2022-04-26T15:35:38.230616Z","iopub.status.idle":"2022-04-26T15:35:38.747247Z","shell.execute_reply.started":"2022-04-26T15:35:38.230574Z","shell.execute_reply":"2022-04-26T15:35:38.746531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\nprint(learning_df.loc[338][\"location_list\"])\nexample_loc_ints = loc_list_to_ints(learning_df.loc[338][\"location_list\"])[0]\nprint(example_loc_ints)\nprint(learning_df.loc[338][\"pn_history\"][example_loc_ints[0] : example_loc_ints[1]])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:38.74857Z","iopub.execute_input":"2022-04-26T15:35:38.748965Z","iopub.status.idle":"2022-04-26T15:35:38.762551Z","shell.execute_reply.started":"2022-04-26T15:35:38.748928Z","shell.execute_reply":"2022-04-26T15:35:38.760008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\ntokenizer = AutoTokenizer.from_pretrained(CFG.model)\ntokenizer.save_pretrained('tokenizer/')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:38.763989Z","iopub.execute_input":"2022-04-26T15:35:38.764372Z","iopub.status.idle":"2022-04-26T15:35:39.053935Z","shell.execute_reply.started":"2022-04-26T15:35:38.764339Z","shell.execute_reply":"2022-04-26T15:35:39.053253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_row = learning_df.loc[338]\none_row","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:39.055133Z","iopub.execute_input":"2022-04-26T15:35:39.055372Z","iopub.status.idle":"2022-04-26T15:35:39.062696Z","shell.execute_reply.started":"2022-04-26T15:35:39.055339Z","shell.execute_reply":"2022-04-26T15:35:39.062004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_add_labels(tokenizer, one_row):\n    tokenized_inputs = tokenizer(\n        one_row[\"feature_text\"],\n        one_row[\"pn_history\"],\n        truncation = \"only_second\",\n        max_length = CFG.max_length, # max length is 406\n        padding = \"max_length\",\n        return_offsets_mapping = True\n    )\n    labels = [0.0] * len(tokenized_inputs[\"input_ids\"])\n    tokenized_inputs[\"location_int\"] = loc_list_to_ints(one_row[\"location_list\"])\n    tokenized_inputs[\"sequence_ids\"] = tokenized_inputs.sequence_ids()\n\n    for idx, (seq_id, offsets) in enumerate(zip(tokenized_inputs[\"sequence_ids\"], tokenized_inputs[\"offset_mapping\"])):\n        if seq_id is None or seq_id == 0:\n            labels[idx] = -100\n            continue\n            \n        token_start, token_end = offsets\n        for feature_start, feature_end in tokenized_inputs[\"location_int\"]:\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                break\n                \n    tokenized_inputs[\"labels\"] = labels\n    \n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:39.064055Z","iopub.execute_input":"2022-04-26T15:35:39.064785Z","iopub.status.idle":"2022-04-26T15:35:39.074664Z","shell.execute_reply.started":"2022-04-26T15:35:39.06475Z","shell.execute_reply":"2022-04-26T15:35:39.073872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_inputs = tokenize_and_add_labels(tokenizer, one_row)\nfor key in tokenized_inputs.keys():\n    print(key)\n    print(tokenized_inputs[key])\n    print(\"=\" * 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:39.077603Z","iopub.execute_input":"2022-04-26T15:35:39.077792Z","iopub.status.idle":"2022-04-26T15:35:39.092865Z","shell.execute_reply.started":"2022-04-26T15:35:39.077767Z","shell.execute_reply":"2022-04-26T15:35:39.092012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        tokens = tokenize_and_add_labels(self.tokenizer, row)\n\n        input_ids = np.array(tokens[\"input_ids\"])\n        attention_mask = np.array(tokens[\"attention_mask\"])\n\n        labels = np.array(tokens[\"labels\"])\n        offset_mapping = np.array(tokens['offset_mapping'])\n        sequence_ids = np.array(tokens['sequence_ids']).astype(\"float16\")\n        \n        return input_ids, attention_mask, labels, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:39.094322Z","iopub.execute_input":"2022-04-26T15:35:39.094643Z","iopub.status.idle":"2022-04-26T15:35:39.104128Z","shell.execute_reply.started":"2022-04-26T15:35:39.094606Z","shell.execute_reply":"2022-04-26T15:35:39.103399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(CFG.model)  # BERT model\n        self.dropout = torch.nn.Dropout(p=CFG.dropout)\n        self.fc1 = nn.Linear(1024, 1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, 512)\n        self.classifier = torch.nn.Linear(512, 1) # BERT has pooler_output(size: 768)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)[0]\n        logits = self.fc1(outputs)\n        logits = self.fc2(self.dropout(logits))\n        logits = self.fc3(self.dropout(logits))\n        logits = self.classifier(self.dropout(logits)).squeeze(-1)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:39.105012Z","iopub.execute_input":"2022-04-26T15:35:39.105204Z","iopub.status.idle":"2022-04-26T15:35:39.114878Z","shell.execute_reply.started":"2022-04-26T15:35:39.105182Z","shell.execute_reply":"2022-04-26T15:35:39.114117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train & Evaluation","metadata":{}},{"cell_type":"code","source":"from itertools import chain\nfrom sklearn.metrics import precision_recall_fscore_support","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:39.118801Z","iopub.execute_input":"2022-04-26T15:35:39.119008Z","iopub.status.idle":"2022-04-26T15:35:39.133768Z","shell.execute_reply.started":"2022-04-26T15:35:39.118985Z","shell.execute_reply":"2022-04-26T15:35:39.132924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n = 1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef get_location_predictions(preds, offset_mapping, sequence_ids, test = False):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        pred = sigmoid(pred)\n        start_idx = None\n        current_preds = []\n        for p, o, s_id in zip(pred, offsets, seq_ids):\n            if s_id is None or s_id == 0:\n                continue\n            if p > 0.5:\n                if start_idx is None:\n                    start_idx = o[0]\n                end_idx = o[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n    return all_predictions\n\ndef calculate_char_CV(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros((num_chars))\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n        char_preds = np.zeros((num_chars))\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n    results = precision_recall_fscore_support(all_labels, all_preds, average = \"binary\")\n    return {\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }\n\ndef compute_metrics(p):\n    predictions, y_true = p\n    y_true = y_true.astype(int)\n    y_pred = [\n        [int(p > 0.5) for (p, l) in zip(pred, label) if l != -100]\n        for pred, label in zip(predictions, y_true)\n    ]\n    y_true = [\n        [l for l in label if l != -100] for label in y_true\n    ]\n    results = precision_recall_fscore_support(list(chain(*y_true)), list(chain(*y_pred)), average = \"binary\")\n    return {\n        \"token_precision\": results[0],\n        \"token_recall\": results[1],\n        \"token_f1\": results[2]\n    }","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:39.134897Z","iopub.execute_input":"2022-04-26T15:35:39.135231Z","iopub.status.idle":"2022-04-26T15:35:39.154458Z","shell.execute_reply.started":"2022-04-26T15:35:39.135197Z","shell.execute_reply":"2022-04-26T15:35:39.153755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:39.155735Z","iopub.execute_input":"2022-04-26T15:35:39.156138Z","iopub.status.idle":"2022-04-26T15:35:39.432289Z","shell.execute_reply.started":"2022-04-26T15:35:39.156103Z","shell.execute_reply":"2022-04-26T15:35:39.43142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(CFG.n_fold):\n    torch.cuda.empty_cache()\n    history = {\"train\": [], \"valid\": []}\n    best_loss = np.inf\n    \n    # Folding\n    val_fold = fold\n\n    model = CustomModel().to(CFG.device)\n    criterion = torch.nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr = CFG.lr)\n\n    train = learning_df.loc[learning_df[\"fold\"] != val_fold].reset_index(drop = True)\n    valid = learning_df.loc[learning_df[\"fold\"] == val_fold].reset_index(drop = True)\n\n    train_ds = CustomDataset(train, tokenizer)\n    valid_ds = CustomDataset(valid, tokenizer)\n    train_dl = torch.utils.data.DataLoader(train_ds, batch_size = CFG.batch_size, pin_memory = True, shuffle = True, drop_last = True)\n    valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size = CFG.batch_size * 2, pin_memory = True, shuffle = False, drop_last = False)\n    \n    print(\"Train Fold :\", fold)\n    for epoch in range(CFG.epochs):\n        print(\"Epoch :\", epoch)\n        \n        #training\n        model.train()\n        train_loss = AverageMeter()\n        pbar = tqdm(train_dl)\n        # input_ids, attention_mask, labels, offset_mapping, sequence_ids\n        for batch in pbar:\n            optimizer.zero_grad()\n            input_ids = batch[0].to(CFG.device)\n            attention_mask = batch[1].to(CFG.device)\n    #         print(input_ids)\n    #         print(input_ids.shape)\n    #         print(attention_mask)\n    #         print(attention_mask.shape)\n            labels = batch[2].to(CFG.device)\n            offset_mapping = batch[3]\n            sequence_ids = batch[4]\n            logits = model(input_ids, attention_mask)\n            loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n            loss = loss_fct(logits, labels)\n            loss = torch.masked_select(loss, labels > -1).mean() # we should calculate at \"pn_history\"; labels at \"feature_text\" are -100 < -1\n            loss.backward()\n            optimizer.step()\n            train_loss.update(val = loss.item(), n = len(input_ids))\n            pbar.set_postfix(Loss = train_loss.avg)\n        print(\"epoch :\", epoch, \"train loss :\", train_loss.avg)\n        history[\"train\"].append(train_loss.avg)\n\n        #evaluation\n        model.eval()\n        valid_loss = AverageMeter()\n        with torch.no_grad():\n            for batch in tqdm(valid_dl):\n                input_ids = batch[0].to(CFG.device)\n                attention_mask = batch[1].to(CFG.device)\n                labels = batch[2].to(CFG.device)\n                offset_mapping = batch[3]\n                sequence_ids = batch[4]\n                logits = model(input_ids, attention_mask)\n                loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n                loss = loss_fct(logits, labels)\n                loss = torch.masked_select(loss, labels > -1).mean()\n                valid_loss.update(val = loss.item(), n = len(input_ids))\n                pbar.set_postfix(Loss = valid_loss.avg)\n        print(\"epoch :\", epoch, \"val loss :\", valid_loss.avg)\n        history[\"valid\"].append(valid_loss.avg)\n\n        # save model\n        if valid_loss.avg < best_loss:\n            print(\"model performance is improved. %f -> %f\" % (best_loss, valid_loss.avg))\n            print(\"Model saved\")\n            best_loss = valid_loss.avg\n            torch.save(model.state_dict(), \"nbme_%d.pth\" % (fold))\n\n    # EVALUATION\n    model.load_state_dict(torch.load(\"nbme_%d.pth\" % (fold), map_location = CFG.device))\n\n    model.eval()\n    preds = []\n    offsets = []\n    seq_ids = []\n    lbls = []\n    with torch.no_grad():\n        for batch in tqdm(valid_dl):\n            input_ids = batch[0].to(CFG.device)\n            attention_mask = batch[1].to(CFG.device)\n            labels = batch[2].to(CFG.device)\n            offset_mapping = batch[3]\n            sequence_ids = batch[4]\n            logits = model(input_ids, attention_mask)\n            preds.append(logits.cpu().numpy())\n            offsets.append(offset_mapping.numpy())\n            seq_ids.append(sequence_ids.numpy())\n            lbls.append(labels.cpu().numpy())\n    preds = np.concatenate(preds, axis = 0)\n    offsets = np.concatenate(offsets, axis = 0)\n    seq_ids = np.concatenate(seq_ids, axis = 0)\n    lbls = np.concatenate(lbls, axis = 0)\n    location_preds = get_location_predictions(preds, offsets, seq_ids, test = False)\n    score = calculate_char_CV(location_preds, offsets, seq_ids, lbls)\n    print(\"Fold %d CV score :\" % (fold), score)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:35:39.433689Z","iopub.execute_input":"2022-04-26T15:35:39.433948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def build_test_df():\n    merged = test.merge(patient_notes, how = \"left\")\n    merged = merged.merge(features, how = \"left\")\n\n#     def process_feature_text(text):\n#         return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n#     merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    merged[\"feature_text\"] = [x for x in merged[\"feature_text\"]]\n    \n    print(merged.shape)\n    return merged","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomTestData(torch.utils.data.Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = self.tokenizer(\n            example[\"feature_text\"],\n            example[\"pn_history\"],\n            truncation = \"only_second\",\n            max_length = 416,\n            padding = \"max_length\",\n            return_offsets_mapping = True\n        )\n        tokenized[\"sequence_ids\"] = tokenized.sequence_ids()\n\n        input_ids = np.array(tokenized[\"input_ids\"])\n        attention_mask = np.array(tokenized[\"attention_mask\"])\n        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\")\n\n        return input_ids, attention_mask, offset_mapping, sequence_ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = build_test_df()\ntest_ds = CustomTestData(test_df, tokenizer)\ntest_dl = torch.utils.data.DataLoader(test_ds, batch_size = CFG.batch_size * 2, pin_memory = True, shuffle = False, drop_last = False)\n\nall_preds = None\nfor fold in range(CFG.n_fold):\n    model = CustomModel().to(CFG.device)\n    model.load_state_dict(torch.load(\"nbme_%d.pth\" % (fold), map_location = CFG.device))\n    model.eval()\n    \n    preds = []\n    offsets = []\n    seq_ids = []\n    with torch.no_grad():\n        for batch in tqdm(test_dl):\n            input_ids = batch[0].to(CFG.device)\n            attention_mask = batch[1].to(CFG.device)\n            offset_mapping = batch[2]\n            sequence_ids = batch[3]\n            logits = model(input_ids, attention_mask)\n            preds.append(logits.cpu().numpy())\n            offsets.append(offset_mapping.numpy())\n            seq_ids.append(sequence_ids.numpy())\n\n    preds = np.concatenate(preds, axis = 0)\n    \n    if all_preds is None:\n        all_preds = np.array(preds).astype(np.float32)\n    else:\n        all_preds += np.array(preds).astype(np.float32)\n        \n    torch.cuda.empty_cache()\n\nall_preds /= CFG.n_fold\nall_preds = all_preds.squeeze()\n\noffsets = np.concatenate(offsets, axis = 0)\nseq_ids = np.concatenate(seq_ids, axis = 0)\n\n# location_preds = get_location_predictions(preds, offsets, seq_ids, test = True)\nlocation_preds = get_location_predictions(all_preds, offsets, seq_ids, test = True)\ntest[\"location\"] = location_preds\ntest[[\"id\", \"location\"]].to_csv(\"submission.csv\", index = False)\npd.read_csv(\"submission.csv\").head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}