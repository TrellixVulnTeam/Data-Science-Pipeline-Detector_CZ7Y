{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom ast import literal_eval\nfrom itertools import chain\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom tqdm.notebook import tqdm, trange\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nfrom transformers import AutoModel, AutoTokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-10T11:19:10.710887Z","iopub.execute_input":"2022-04-10T11:19:10.711392Z","iopub.status.idle":"2022-04-10T11:19:18.315971Z","shell.execute_reply.started":"2022-04-10T11:19:10.711353Z","shell.execute_reply":"2022-04-10T11:19:18.315232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class CFG:\n    root = \"../input/nbme-score-clinical-patient-notes\"\n    debug = False\n    n_fold= 5\n    model = \"../input/huggingface-bert/bert-base-uncased\"\n    max_length = 512\n    doc_stride = 128\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    lr = 1e-5\n    batch_size = 16\n    epochs = 3","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:19:18.317785Z","iopub.execute_input":"2022-04-10T11:19:18.318057Z","iopub.status.idle":"2022-04-10T11:19:18.372267Z","shell.execute_reply.started":"2022-04-10T11:19:18.318022Z","shell.execute_reply":"2022-04-10T11:19:18.371484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create df","metadata":{}},{"cell_type":"code","source":"def create_train_df():\n    feats = pd.read_csv(f\"{CFG.root}/features.csv\")\n    notes = pd.read_csv(f\"{CFG.root}/patient_notes.csv\")\n    train = pd.read_csv(f\"{CFG.root}/train.csv\")\n\n    train[\"annotation_list\"] = [literal_eval(x) for x in train[\"annotation\"]]\n    train[\"location_list\"] = [literal_eval(x) for x in train[\"location\"]]\n    merged = train.merge(notes, how = \"left\")\n    merged = merged.merge(feats, how = \"left\")\n    merged = merged.loc[merged[\"annotation\"] != \"[]\"].copy().reset_index(drop = True) # comment out if you train all samples\n    \n    def process_feature_text(text):\n        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    \n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n    \n    merged['location_prediction'] = -1\n    merged['token_proba'] = -1\n    merged['token_offsets'] = -1\n\n    if CFG.debug:\n        merged = merged.sample(frac = 0.2).reset_index(drop = True)\n\n    skf = StratifiedKFold(CFG.n_fold)\n    merged[\"stratify_on\"] = merged[\"case_num\"].astype(str) + merged[\"feature_num\"].astype(str)\n    merged[\"fold\"] = -1\n    for fold, (_, valid_idx) in enumerate(skf.split(merged[\"id\"], y = merged[\"stratify_on\"])):\n        merged.loc[valid_idx, \"fold\"] = fold\n    \n    print(merged.shape)\n    print(merged.loc[merged[\"fold\"]==0].shape)\n    return merged\n\ndf = create_train_df()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:19:18.373879Z","iopub.execute_input":"2022-04-10T11:19:18.374371Z","iopub.status.idle":"2022-04-10T11:19:19.571428Z","shell.execute_reply.started":"2022-04-10T11:19:18.374329Z","shell.execute_reply":"2022-04-10T11:19:19.570636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:19:19.573442Z","iopub.execute_input":"2022-04-10T11:19:19.574149Z","iopub.status.idle":"2022-04-10T11:19:19.596849Z","shell.execute_reply.started":"2022-04-10T11:19:19.574108Z","shell.execute_reply":"2022-04-10T11:19:19.596084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first = df.loc[0]\nexample = {\n    \"feature_text\": first.feature_text,\n    \"pn_history\": first.pn_history,\n    \"location_list\": first.location_list,\n    \"annotation_list\": first.annotation_list\n}\nfor key in example.keys():\n    print(key)\n    print(example[key])\n    print(\"=\" * 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:19:19.598077Z","iopub.execute_input":"2022-04-10T11:19:19.598392Z","iopub.status.idle":"2022-04-10T11:19:19.607896Z","shell.execute_reply.started":"2022-04-10T11:19:19.598357Z","shell.execute_reply":"2022-04-10T11:19:19.606957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\nprint(example[\"location_list\"])\nexample_loc_ints = loc_list_to_ints(example[\"location_list\"])[0]\nprint(example_loc_ints)\nprint(example[\"pn_history\"][example_loc_ints[0] : example_loc_ints[1]])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:19:19.609043Z","iopub.execute_input":"2022-04-10T11:19:19.609252Z","iopub.status.idle":"2022-04-10T11:19:19.619806Z","shell.execute_reply.started":"2022-04-10T11:19:19.609223Z","shell.execute_reply":"2022-04-10T11:19:19.618574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.model)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:19:19.62121Z","iopub.execute_input":"2022-04-10T11:19:19.621403Z","iopub.status.idle":"2022-04-10T11:19:19.701382Z","shell.execute_reply.started":"2022-04-10T11:19:19.621381Z","shell.execute_reply":"2022-04-10T11:19:19.700652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_add_labels(tokenizer, example):\n    tokenized_inputs = tokenizer(\n        example[\"feature_text\"],\n        example[\"pn_history\"],\n        max_length = CFG.max_length,\n        stride = CFG.doc_stride,\n        padding = \"max_length\",\n        truncation = \"only_second\",\n        return_offsets_mapping = True\n    )\n    labels = [0.0] * len(tokenized_inputs[\"input_ids\"])\n    tokenized_inputs[\"location_int\"] = loc_list_to_ints(example[\"location_list\"])\n    tokenized_inputs[\"sequence_ids\"] = tokenized_inputs.sequence_ids()\n\n    for idx, (seq_id, offsets) in enumerate(zip(tokenized_inputs[\"sequence_ids\"], tokenized_inputs[\"offset_mapping\"])):\n        if seq_id is None or seq_id == 0:\n            labels[idx] = -100\n            continue\n        exit = False\n        token_start, token_end = offsets\n        for feature_start, feature_end in tokenized_inputs[\"location_int\"]:\n            if exit:\n                break\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                exit = True\n    tokenized_inputs[\"labels\"] = labels\n    \n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:19:19.702677Z","iopub.execute_input":"2022-04-10T11:19:19.702926Z","iopub.status.idle":"2022-04-10T11:19:19.713969Z","shell.execute_reply.started":"2022-04-10T11:19:19.702892Z","shell.execute_reply":"2022-04-10T11:19:19.713107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_inputs = tokenize_and_add_labels(tokenizer, example)\nfor key in tokenized_inputs.keys():\n    print(key)\n    print(tokenized_inputs[key])\n    print(\"=\" * 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:19:19.715166Z","iopub.execute_input":"2022-04-10T11:19:19.715516Z","iopub.status.idle":"2022-04-10T11:19:19.72888Z","shell.execute_reply.started":"2022-04-10T11:19:19.715481Z","shell.execute_reply":"2022-04-10T11:19:19.726886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we need \"input_ids\" and \"attention_mask\" for BERT.\n\nlabels are 1.0 at annotation.\n\nso we can train as binary classification; does this word(token) represent the feature? -> 1 or 0","metadata":{}},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class NBMEData(torch.utils.data.Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = tokenize_and_add_labels(self.tokenizer, example)\n\n        input_ids = np.array(tokenized[\"input_ids\"]) # for input BERT\n        attention_mask = np.array(tokenized[\"attention_mask\"]) # for input BERT\n        labels = np.array(tokenized[\"labels\"]) # for calculate loss and cv score\n\n        offset_mapping = np.array(tokenized[\"offset_mapping\"]) # for calculate cv score\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\") # for calculate cv score\n        \n        return input_ids, attention_mask, labels, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:19:19.731657Z","iopub.execute_input":"2022-04-10T11:19:19.731856Z","iopub.status.idle":"2022-04-10T11:19:19.739004Z","shell.execute_reply.started":"2022-04-10T11:19:19.731827Z","shell.execute_reply":"2022-04-10T11:19:19.73822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class NBMEModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = AutoModel.from_pretrained(CFG.model) # BERT model\n        self.dropout = torch.nn.Dropout(p = 0.2)\n        self.classifier = torch.nn.Linear(768, 1) # BERT has last_hidden_state(size: sequqence_length, 768)\n    \n    def forward(self, input_ids, attention_mask):\n        last_hidden_state = self.backbone(input_ids = input_ids, attention_mask = attention_mask)[0] # idx 0 is last_hidden_state; backbone().last_hidden_state is also good\n        logits = self.classifier(self.dropout(last_hidden_state)).squeeze(-1)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:19:19.740592Z","iopub.execute_input":"2022-04-10T11:19:19.740912Z","iopub.status.idle":"2022-04-10T11:19:19.750375Z","shell.execute_reply.started":"2022-04-10T11:19:19.740878Z","shell.execute_reply":"2022-04-10T11:19:19.749619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def train_loop(fold):\n    model = NBMEModel().to(CFG.device)\n    #criterion = torch.nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), CFG.lr)\n\n    train = df.loc[df[\"fold\"] != fold].reset_index(drop = True)\n    valid = df.loc[df[\"fold\"] == fold].reset_index(drop = True)\n    train_ds = NBMEData(train, tokenizer)\n    valid_ds = NBMEData(valid, tokenizer)\n    train_dl = torch.utils.data.DataLoader(train_ds, batch_size = CFG.batch_size, pin_memory = True, shuffle = True, drop_last = True)\n    valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size = CFG.batch_size * 2, pin_memory = True, shuffle = False, drop_last = False)\n    \n    return train_dl, valid_dl, model, optimizer","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:19:19.751751Z","iopub.execute_input":"2022-04-10T11:19:19.752022Z","iopub.status.idle":"2022-04-10T11:19:19.761104Z","shell.execute_reply.started":"2022-04-10T11:19:19.751988Z","shell.execute_reply":"2022-04-10T11:19:19.760174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n = 1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef get_location_predictions(preds, offset_mapping, sequence_ids):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        pred = sigmoid(pred)\n        start_idx = None\n        current_preds = []        \n        for p, o, s_id in zip(pred, offsets, seq_ids):\n            if s_id is None or s_id == 0:\n                continue\n            if p > 0.5:\n                if start_idx is None:\n                    start_idx = o[0]\n                end_idx = o[1]\n            elif start_idx is not None:\n                current_preds.append((start_idx, end_idx))\n                start_idx = None\n        all_predictions.append(current_preds)\n    return all_predictions\n\ndef calculate_char_CV(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros((num_chars))\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n        char_preds = np.zeros((num_chars))\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n    results = precision_recall_fscore_support(all_labels, all_preds, average = \"binary\")\n    return {\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:19:19.762506Z","iopub.execute_input":"2022-04-10T11:19:19.762774Z","iopub.status.idle":"2022-04-10T11:19:19.779567Z","shell.execute_reply.started":"2022-04-10T11:19:19.762724Z","shell.execute_reply":"2022-04-10T11:19:19.778798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_loop():    \n    history = {}\n    for fold in range(CFG.n_fold):\n        print(f\"========== fold: {fold} training ==========\")\n        train_dl, valid_dl, model, optimizer = train_loop(fold)\n        history[fold] = {\"train\": [], \"valid\": []}\n        best_loss = np.inf\n        \n        for epoch in range(CFG.epochs):\n            print(f\"========== EPOCH: {epoch} training ==========\")\n            #training\n            model.train()\n            train_loss = AverageMeter()\n            pbar = tqdm(train_dl)\n            for batch in pbar:\n                optimizer.zero_grad()\n                input_ids = batch[0].to(CFG.device)\n                attention_mask = batch[1].to(CFG.device)\n                labels = batch[2].to(CFG.device)\n                offset_mapping = batch[3]\n                sequence_ids = batch[4]\n                logits = model(input_ids, attention_mask)\n                loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n                loss = loss_fct(logits, labels)\n                loss = torch.masked_select(loss, labels > -1).mean() # we should calculate at \"pn_history\"; labels at \"feature_text\" are -100 < -1\n                loss.backward()\n                optimizer.step()\n                train_loss.update(val = loss.item(), n = len(input_ids))\n                pbar.set_postfix(Loss = train_loss.avg)\n            print(epoch, train_loss.avg)\n            history[fold][\"train\"].append(train_loss.avg)\n\n            #evaluation\n            model.eval()\n            valid_loss = AverageMeter()\n            preds = []\n            offsets = []\n            seq_ids = []\n            lbls = []\n            with torch.no_grad():\n                for batch in tqdm(valid_dl):\n                    input_ids = batch[0].to(CFG.device)\n                    attention_mask = batch[1].to(CFG.device)\n                    labels = batch[2].to(CFG.device)\n                    offset_mapping = batch[3]\n                    sequence_ids = batch[4]\n                    logits = model(input_ids, attention_mask)\n                    loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n                    loss = loss_fct(logits, labels)\n                    loss = torch.masked_select(loss, labels > -1).mean()\n                    valid_loss.update(val = loss.item(), n = len(input_ids))\n                    pbar.set_postfix(Loss = valid_loss.avg)\n                    preds.append(logits.cpu().numpy())\n                    offsets.append(offset_mapping.numpy())\n                    seq_ids.append(sequence_ids.numpy())\n                    lbls.append(labels.cpu().numpy())\n            print(epoch, valid_loss.avg)\n            history[fold][\"valid\"].append(valid_loss.avg)          \n            \n            # save model\n            if valid_loss.avg < best_loss:\n                best_loss = valid_loss.avg\n                torch.save(model.state_dict(), f\"nbme_{fold}.pth\")\n                preds = np.concatenate(preds, axis = 0)\n                offsets = np.concatenate(offsets, axis = 0)\n                seq_ids = np.concatenate(seq_ids, axis = 0)\n                lbls = np.concatenate(lbls, axis = 0)\n                location_preds = get_location_predictions(preds, offsets, seq_ids)\n                index = df[df['fold'] == fold].index        \n                df.loc[index,'token_proba'] = pd.Series([list(preds[i]) for i in range(preds.shape[0])], index=index)\n                df.loc[index,'token_offsets'] = pd.Series([list(offsets[i]) for i in range(offsets.shape[0])], index=index)\n                score = calculate_char_CV(location_preds, offsets, seq_ids, lbls)\n                print(score)\n    print(history)\n    return()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:19:19.78094Z","iopub.execute_input":"2022-04-10T11:19:19.781624Z","iopub.status.idle":"2022-04-10T11:19:19.804121Z","shell.execute_reply.started":"2022-04-10T11:19:19.781586Z","shell.execute_reply":"2022-04-10T11:19:19.803328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_loop()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:19:19.807002Z","iopub.execute_input":"2022-04-10T11:19:19.807208Z","iopub.status.idle":"2022-04-10T11:44:30.320834Z","shell.execute_reply.started":"2022-04-10T11:19:19.807176Z","shell.execute_reply":"2022-04-10T11:44:30.32001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_pickle(\"df_pred.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-04-10T11:44:30.361772Z","iopub.execute_input":"2022-04-10T11:44:30.362053Z","iopub.status.idle":"2022-04-10T11:44:40.023004Z","shell.execute_reply.started":"2022-04-10T11:44:30.36202Z","shell.execute_reply":"2022-04-10T11:44:40.022005Z"},"trusted":true},"execution_count":null,"outputs":[]}]}