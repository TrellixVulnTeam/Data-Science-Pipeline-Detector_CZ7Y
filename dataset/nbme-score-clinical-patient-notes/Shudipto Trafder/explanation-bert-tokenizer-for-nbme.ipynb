{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NBME - Score Clinical Patient Notes\nIn this notebook, I explain about -\n1. How to generate tokens\n2. How to generate labels (why -1)\n3. How to get prediction and convert it back","metadata":{}},{"cell_type":"code","source":"from ast import literal_eval\nfrom itertools import chain\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModel, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-03-24T05:55:21.589574Z","iopub.execute_input":"2022-03-24T05:55:21.590158Z","iopub.status.idle":"2022-03-24T05:55:30.740106Z","shell.execute_reply.started":"2022-03-24T05:55:21.590019Z","shell.execute_reply":"2022-03-24T05:55:30.739049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Datasets Helper Function\nneed to merge `features.csv`, `patient_notes.csv` with `train.csv`\n\nalso, `annotation` and `location` these two columns are actually a list, so we need to convert them back. Let's use `ast` library for this conversion\n\n","metadata":{}},{"cell_type":"code","source":"BASE_URL = \"../input/nbme-score-clinical-patient-notes\"\n\n\ndef process_feature_text(text):\n    return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n\n\ndef prepare_datasets():\n    features = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    df = pd.read_csv(f\"{BASE_URL}/train.csv\")\n    df[\"annotation_list\"] = [literal_eval(x) for x in df[\"annotation\"]]\n    df[\"location_list\"] = [literal_eval(x) for x in df[\"location\"]]\n\n    merged = df.merge(notes, how=\"left\")\n    merged = merged.merge(features, how=\"left\")\n\n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n\n    return merged","metadata":{"execution":{"iopub.status.busy":"2022-03-24T05:55:30.74324Z","iopub.execute_input":"2022-03-24T05:55:30.74364Z","iopub.status.idle":"2022-03-24T05:55:30.753484Z","shell.execute_reply.started":"2022-03-24T05:55:30.743568Z","shell.execute_reply":"2022-03-24T05:55:30.752391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = prepare_datasets()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T05:55:30.754857Z","iopub.execute_input":"2022-03-24T05:55:30.755114Z","iopub.status.idle":"2022-03-24T05:55:31.989911Z","shell.execute_reply.started":"2022-03-24T05:55:30.755087Z","shell.execute_reply":"2022-03-24T05:55:31.988807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets Tokenize\nHere, we are going to use `BertTokenizerFast` insted of `BertTokenizer`. You can take a look on the documentation [PreTrainedTokenizerFast](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) vs [PreTrainedTokenizerFast](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer)\n\n> A tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models. Most of the tokenizers are available in two flavors: a full python implementation and a “Fast” implementation based on the Rust library 🤗 Tokenizers. The “Fast” implementations allows: a significant speed-up in particular when doing batched tokenization and\nadditional methods to map between the original string (character and words) and the token space (e.g. getting the index of the token comprising a given character or the span of characters corresponding to a given token).","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertTokenizerFast\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:02:12.256122Z","iopub.execute_input":"2022-03-24T06:02:12.256454Z","iopub.status.idle":"2022-03-24T06:02:18.231918Z","shell.execute_reply.started":"2022-03-24T06:02:12.256423Z","shell.execute_reply":"2022-03-24T06:02:18.231016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auto_tokenizer = AutoTokenizer.from_pretrained(\"../input/huggingface-bert/bert-base-uncased\")\n\nprint(\"Bert Tokenizer Type\", type(tokenizer))\nprint(\"Auto Tokenizer Type\", type(auto_tokenizer))\n\ntype(auto_tokenizer) == type(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:04:12.571836Z","iopub.execute_input":"2022-03-24T06:04:12.5722Z","iopub.status.idle":"2022-03-24T06:04:12.627322Z","shell.execute_reply.started":"2022-03-24T06:04:12.572163Z","shell.execute_reply":"2022-03-24T06:04:12.626351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so, both are using same tokenizer.\n\nNow take the first row from the datasets, and step by step prepare for model input","metadata":{}},{"cell_type":"code","source":"first = df.iloc[0]\nfirst","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:06:29.824034Z","iopub.execute_input":"2022-03-24T06:06:29.824945Z","iopub.status.idle":"2022-03-24T06:06:29.833699Z","shell.execute_reply.started":"2022-03-24T06:06:29.824901Z","shell.execute_reply":"2022-03-24T06:06:29.832832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_text, pn_history = first.feature_text, first.pn_history\nprint(\"feature_text\", feature_text)\nprint(\"\\n\")\nprint(\"pn_history\", pn_history)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:06:31.783723Z","iopub.execute_input":"2022-03-24T06:06:31.784038Z","iopub.status.idle":"2022-03-24T06:06:31.791219Z","shell.execute_reply.started":"2022-03-24T06:06:31.784004Z","shell.execute_reply":"2022-03-24T06:06:31.790268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see how the text look like after it's converted into bert token\ntokens = tokenizer.tokenize(feature_text, pn_history)\nprint(\"Total Tokens\", len(tokens))\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:06:34.331196Z","iopub.execute_input":"2022-03-24T06:06:34.331514Z","iopub.status.idle":"2022-03-24T06:06:34.342642Z","shell.execute_reply.started":"2022-03-24T06:06:34.33147Z","shell.execute_reply":"2022-03-24T06:06:34.341907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pn_history[696:724]","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:06:36.603895Z","iopub.execute_input":"2022-03-24T06:06:36.604184Z","iopub.status.idle":"2022-03-24T06:06:36.610431Z","shell.execute_reply.started":"2022-03-24T06:06:36.604155Z","shell.execute_reply":"2022-03-24T06:06:36.609788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's ask the tokenizer to return the offsets mapping, by sending argument `return_offsets_mapping` as True\n\n- return_offsets_mapping:\n> Whether or not to return (char_start, char_end) for each token. This is only available on fast tokenizers inheriting from PreTrainedTokenizerFast, if using Python’s tokenizer, this method will raise NotImplementedError.","metadata":{}},{"cell_type":"code","source":"out = tokenizer(\n        feature_text,\n        pn_history,\n        truncation=True,\n        max_length=1000,\n        padding=\"max_length\",\n        return_offsets_mapping=True\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:22:49.224749Z","iopub.execute_input":"2022-03-24T06:22:49.225055Z","iopub.status.idle":"2022-03-24T06:22:49.233299Z","shell.execute_reply.started":"2022-03-24T06:22:49.225022Z","shell.execute_reply":"2022-03-24T06:22:49.232298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, we are successfully able to generate tokens for training, but we also need Labels\n\nSteps to generate labels:\n 1. Zip sequence_ids and offset mapping\n 2. if sequence_id is None or 0 then the label is -1 (or any value you want)\n 3. and if the location (from datasets) is valid (compared with offset mapping) then its true label which is 1\n\n\n#### What is sequence ids?\nit's mapping of tokens to the id of their original sentences:\n- `None` for special tokens added around or between sequences,\n- `0` for tokens corresponding to words in the first sequence,\n- `1` for tokens corresponding to words in the second sequence when a pair of sequences was jointly encoded.\n\n#### Why -1 as label?\nif sequence id is none then it's basically a special token like ['SEP'], ['CLS'], and if it is 0 then it's coming from the first sentence, in our case, it's `feature_text`","metadata":{}},{"cell_type":"code","source":"zipped = zip(out.sequence_ids(), out[\"offset_mapping\"])\n\nidx, (seq_id, offsets) = next(enumerate(zipped))\nif not seq_id or seq_id == 0:\n    print(\"Seq ID zero, so level is -1 also\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:22:52.175482Z","iopub.execute_input":"2022-03-24T06:22:52.176001Z","iopub.status.idle":"2022-03-24T06:22:52.181619Z","shell.execute_reply.started":"2022-03-24T06:22:52.17596Z","shell.execute_reply":"2022-03-24T06:22:52.181014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_id = 1 #assume\nloc_list = [696, 724]\n\nfor idx, (seq_id, offsets)  in enumerate(zip(out.sequence_ids(), out[\"offset_mapping\"])):\n    token_start, token_end = offsets\n    for feature_start, feature_end in [loc_list]:\n        if token_start >= feature_start and token_end <= feature_end:\n            print(f\"Word {pn_history[token_start:token_end]}, label: 1\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:23:44.781546Z","iopub.execute_input":"2022-03-24T06:23:44.7825Z","iopub.status.idle":"2022-03-24T06:23:44.792166Z","shell.execute_reply.started":"2022-03-24T06:23:44.782445Z","shell.execute_reply":"2022-03-24T06:23:44.791064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction\nLet's convert it back from a pre-trained model. Here I am going to use [Pytorch Bert baseline NBME](https://www.kaggle.com/code/iamsdt/pytorch-bert-baseline-nbme) notebook model","metadata":{}},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(config['model_name'])  # BERT model\n        self.dropout = nn.Dropout(p=config['dropout'])\n        self.config = config\n        self.fc1 = nn.Linear(768, 512)\n        self.fc2 = nn.Linear(512, 512)\n        self.fc3 = nn.Linear(512, 1)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        logits = self.fc1(outputs[0])\n        logits = self.fc2(self.dropout(logits))\n        logits = self.fc3(self.dropout(logits)).squeeze(-1)\n        return logits\n    \n\nhyperparameters = {\n    \"max_length\": 416,\n    \"padding\": \"max_length\",\n    \"return_offsets_mapping\": True,\n    \"truncation\": \"only_second\",\n    \"model_name\": \"../input/huggingface-bert/bert-base-uncased\",\n    \"dropout\": 0.2,\n    \"lr\": 1e-5,\n    \"test_size\": 0.2,\n    \"seed\": 1268,\n    \"batch_size\": 8\n}\n\nmodel = CustomModel(hyperparameters)\nmodel.load_state_dict(torch.load(\"../input/pytorch-bert-baseline-nbme/nbme_bert_v2.pth\", map_location = \"cpu\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:27:33.881023Z","iopub.execute_input":"2022-03-24T06:27:33.881819Z","iopub.status.idle":"2022-03-24T06:27:43.432358Z","shell.execute_reply.started":"2022-03-24T06:27:33.881765Z","shell.execute_reply":"2022-03-24T06:27:43.431521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_df():\n    feats = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    test = pd.read_csv(f\"{BASE_URL}/test.csv\")\n\n    merged = test.merge(notes, how = \"left\")\n    merged = merged.merge(feats, how = \"left\")\n\n    def process_feature_text(text):\n        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n    \n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    \n    return merged\n\n\nclass SubmissionDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = self.tokenizer(\n            example[\"feature_text\"],\n            example[\"pn_history\"],\n            truncation = self.config['truncation'],\n            max_length = self.config['max_length'],\n            padding = self.config['padding'],\n            return_offsets_mapping = self.config['return_offsets_mapping']\n        )\n        tokenized[\"sequence_ids\"] = tokenized.sequence_ids()\n\n        input_ids = np.array(tokenized[\"input_ids\"])\n        attention_mask = np.array(tokenized[\"attention_mask\"])\n        token_type_ids = np.array(tokenized[\"token_type_ids\"])\n        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\")\n\n        return input_ids, attention_mask, token_type_ids, offset_mapping, sequence_ids\n\n\ntest_df = create_test_df()\n\nsubmission_data = SubmissionDataset(test_df, tokenizer, hyperparameters)\nsubmission_dataloader = DataLoader(submission_data, batch_size=1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:28:19.319862Z","iopub.execute_input":"2022-03-24T06:28:19.320189Z","iopub.status.idle":"2022-03-24T06:28:19.709767Z","shell.execute_reply.started":"2022-03-24T06:28:19.320153Z","shell.execute_reply":"2022-03-24T06:28:19.708581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pn_history = test_df.iloc[0]['pn_history']\nprint(test_pn_history)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:28:21.564438Z","iopub.execute_input":"2022-03-24T06:28:21.564773Z","iopub.status.idle":"2022-03-24T06:28:21.570559Z","shell.execute_reply.started":"2022-03-24T06:28:21.56474Z","shell.execute_reply":"2022-03-24T06:28:21.569597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a single batch, also used batch size as 1, so it will load only one history\nbatch = next(iter(submission_dataloader))","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:29:13.24095Z","iopub.execute_input":"2022-03-24T06:29:13.241256Z","iopub.status.idle":"2022-03-24T06:29:13.266432Z","shell.execute_reply.started":"2022-03-24T06:29:13.241224Z","shell.execute_reply":"2022-03-24T06:29:13.265487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = batch[0]\nattention_mask = batch[1]\ntoken_type_ids = batch[2]\noffset_mapping = batch[3]\nsequence_ids = batch[4]\n\nlogits = model(input_ids, attention_mask, token_type_ids)\npredicted = logits.detach().cpu().numpy()\noffset_mapping = offset_mapping.numpy()\nsequence_ids = sequence_ids.numpy()\nprint(predicted.size == hyperparameters['max_length'])","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:29:50.415435Z","iopub.execute_input":"2022-03-24T06:29:50.415738Z","iopub.status.idle":"2022-03-24T06:29:51.442708Z","shell.execute_reply.started":"2022-03-24T06:29:50.415707Z","shell.execute_reply":"2022-03-24T06:29:51.44173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Steps\n1. pass the model output into a sigmoid function\n2. for sequence_id  if it's `none` or `0` then the same logic\n3. if the prediction is greater than 0.5 (threshold) then check the offset index","metadata":{}},{"cell_type":"code","source":"for pred, offsets, seq_ids in zip(predicted, offset_mapping, sequence_ids):\n    pred = 1 / (1 + np.exp(-pred)) # which is sigmoid function    \n    start_idx = None\n    end_idx = None\n    \n    for pred, offset, seq_id in zip(pred, offsets, seq_ids):\n        if not seq_id or seq_id == 0:\n            continue\n    \n        if pred > 0.5:\n            if not start_idx:\n                start_idx = offset[0]\n            end_idx = offset[1]\n            \n        elif start_idx:\n            print(\"Current index\", f\"{start_idx} {end_idx}\")\n            print(\"Word: \", test_pn_history[start_idx:end_idx])\n            start_idx = None","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:33:29.381661Z","iopub.execute_input":"2022-03-24T06:33:29.381991Z","iopub.status.idle":"2022-03-24T06:33:29.394361Z","shell.execute_reply.started":"2022-03-24T06:33:29.381958Z","shell.execute_reply":"2022-03-24T06:33:29.393402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's it we are done. IF you any thoughts or want to add something let's use comment section\n> Thank you very much","metadata":{}}]}