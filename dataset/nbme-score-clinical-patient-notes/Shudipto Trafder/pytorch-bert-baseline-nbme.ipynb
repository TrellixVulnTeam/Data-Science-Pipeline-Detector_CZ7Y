{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NBME - Score Clinical Patient Notes \n- **Framework:** Pytorch\n- **Model Architecture:**\n    - BERT\n    - Linear(768, 512)\n    - Linear(512, 512)\n    - Linear(512, 1)\n- **LR:** 1e-5\n- **Batch Size:** 8\n- **Epoch:** 3\n- **Dropout:** 0.2\n- **Criterion:** BCEWithLogitsLoss\n- **Optimizer:** AdamW\n\n# Tokenizer params\n- **Max Lenght:** 416\n- **Padding:** max_lenght\n- **Truncation:** only_scond\n","metadata":{}},{"cell_type":"code","source":"from ast import literal_eval\nfrom itertools import chain\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModel, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:40:19.714471Z","iopub.execute_input":"2022-02-15T04:40:19.714953Z","iopub.status.idle":"2022-02-15T04:40:27.236937Z","shell.execute_reply.started":"2022-02-15T04:40:19.714912Z","shell.execute_reply":"2022-02-15T04:40:27.236174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions\n### 1. Datasets Helper Function\nneed to merge `features.csv`, `patient_notes.csv` with `train.csv`","metadata":{}},{"cell_type":"code","source":"BASE_URL = \"../input/nbme-score-clinical-patient-notes\"\n\n\ndef process_feature_text(text):\n    return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n\n\ndef prepare_datasets():\n    features = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    df = pd.read_csv(f\"{BASE_URL}/train.csv\")\n    df[\"annotation_list\"] = [literal_eval(x) for x in df[\"annotation\"]]\n    df[\"location_list\"] = [literal_eval(x) for x in df[\"location\"]]\n\n    merged = df.merge(notes, how=\"left\")\n    merged = merged.merge(features, how=\"left\")\n\n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n\n    return merged","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:40:27.238397Z","iopub.execute_input":"2022-02-15T04:40:27.238911Z","iopub.status.idle":"2022-02-15T04:40:27.246292Z","shell.execute_reply.started":"2022-02-15T04:40:27.238868Z","shell.execute_reply":"2022-02-15T04:40:27.245708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Tokenizer Helper Function","metadata":{}},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\n\ndef tokenize_and_add_labels(tokenizer, data, config):\n    out = tokenizer(\n        data[\"feature_text\"],\n        data[\"pn_history\"],\n        truncation=config['truncation'],\n        max_length=config['max_length'],\n        padding=config['padding'],\n        return_offsets_mapping=config['return_offsets_mapping']\n    )\n    labels = [0.0] * len(out[\"input_ids\"])\n    out[\"location_int\"] = loc_list_to_ints(data[\"location_list\"])\n    out[\"sequence_ids\"] = out.sequence_ids()\n\n    for idx, (seq_id, offsets) in enumerate(zip(out[\"sequence_ids\"], out[\"offset_mapping\"])):\n        if not seq_id or seq_id == 0:\n            labels[idx] = -1\n            continue\n\n        token_start, token_end = offsets\n        for feature_start, feature_end in out[\"location_int\"]:\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                break\n\n    out[\"labels\"] = labels\n\n    return out","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:40:36.425717Z","iopub.execute_input":"2022-02-15T04:40:36.425995Z","iopub.status.idle":"2022-02-15T04:40:36.435215Z","shell.execute_reply.started":"2022-02-15T04:40:36.425964Z","shell.execute_reply":"2022-02-15T04:40:36.434507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Predection and Score Helper Function","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef get_location_predictions(preds, offset_mapping, sequence_ids, test=False):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        pred = 1 / (1 + np.exp(-pred))\n        start_idx = None\n        end_idx = None\n        current_preds = []\n        for pred, offset, seq_id in zip(pred, offsets, seq_ids):\n            if seq_id is None or seq_id == 0:\n                continue\n\n            if pred > 0.5:\n                if start_idx is None:\n                    start_idx = offset[0]\n                end_idx = offset[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n            \n    return all_predictions\n\n\ndef calculate_char_cv(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros(num_chars)\n\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n\n        char_preds = np.zeros(num_chars)\n\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n\n    results = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", labels=np.unique(all_preds))\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n\n    return {\n        \"Accuracy\": accuracy,\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:40:38.832278Z","iopub.execute_input":"2022-02-15T04:40:38.832835Z","iopub.status.idle":"2022-02-15T04:40:38.846037Z","shell.execute_reply.started":"2022-02-15T04:40:38.832798Z","shell.execute_reply":"2022-02-15T04:40:38.845273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        data = self.data.iloc[idx]\n        tokens = tokenize_and_add_labels(self.tokenizer, data, self.config)\n\n        input_ids = np.array(tokens[\"input_ids\"])\n        attention_mask = np.array(tokens[\"attention_mask\"])\n        token_type_ids = np.array(tokens[\"token_type_ids\"])\n\n        labels = np.array(tokens[\"labels\"])\n        offset_mapping = np.array(tokens['offset_mapping'])\n        sequence_ids = np.array(tokens['sequence_ids']).astype(\"float16\")\n        \n        return input_ids, attention_mask, token_type_ids, labels, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:40:43.273346Z","iopub.execute_input":"2022-02-15T04:40:43.274087Z","iopub.status.idle":"2022-02-15T04:40:43.282444Z","shell.execute_reply.started":"2022-02-15T04:40:43.274046Z","shell.execute_reply":"2022-02-15T04:40:43.280415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- Lets use **BERT** base Architecture\n- Also Used 3 FC layers\n\n**Comments:** 3 layers improve accuracy 2% on public score","metadata":{}},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(config['model_name'])  # BERT model\n        self.dropout = nn.Dropout(p=config['dropout'])\n        self.config = config\n        self.fc1 = nn.Linear(768, 512)\n        self.fc2 = nn.Linear(512, 512)\n        self.fc3 = nn.Linear(512, 1)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        logits = self.fc1(outputs[0])\n        logits = self.fc2(self.dropout(logits))\n        logits = self.fc3(self.dropout(logits)).squeeze(-1)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:40:46.096554Z","iopub.execute_input":"2022-02-15T04:40:46.096808Z","iopub.status.idle":"2022-02-15T04:40:46.10548Z","shell.execute_reply.started":"2022-02-15T04:40:46.096779Z","shell.execute_reply":"2022-02-15T04:40:46.103442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters\n","metadata":{}},{"cell_type":"code","source":"hyperparameters = {\n    \"max_length\": 416,\n    \"padding\": \"max_length\",\n    \"return_offsets_mapping\": True,\n    \"truncation\": \"only_second\",\n    \"model_name\": \"../input/huggingface-bert/bert-base-uncased\",\n    \"dropout\": 0.2,\n    \"lr\": 1e-5,\n    \"test_size\": 0.2,\n    \"seed\": 1268,\n    \"batch_size\": 8\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:40:48.5828Z","iopub.execute_input":"2022-02-15T04:40:48.58356Z","iopub.status.idle":"2022-02-15T04:40:48.588294Z","shell.execute_reply.started":"2022-02-15T04:40:48.58352Z","shell.execute_reply":"2022-02-15T04:40:48.587432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Datasets\nTrain and Test split: 20%\n\nTotal Data:\n- Train: 11440\n- Test: 2860","metadata":{}},{"cell_type":"code","source":"train_df = prepare_datasets()\n\nX_train, X_test = train_test_split(train_df, test_size=hyperparameters['test_size'],\n                                   random_state=hyperparameters['seed'])\n\n\nprint(\"Train size\", len(X_train))\nprint(\"Test Size\", len(X_test))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:42:09.940468Z","iopub.execute_input":"2022-02-15T04:42:09.940734Z","iopub.status.idle":"2022-02-15T04:42:10.523551Z","shell.execute_reply.started":"2022-02-15T04:42:09.940706Z","shell.execute_reply":"2022-02-15T04:42:10.522784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(hyperparameters['model_name'])\n\ntraining_data = CustomDataset(X_train, tokenizer, hyperparameters)\ntrain_dataloader = DataLoader(training_data, batch_size=hyperparameters['batch_size'], shuffle=True)\n\ntest_data = CustomDataset(X_test, tokenizer, hyperparameters)\ntest_dataloader = DataLoader(test_data, batch_size=hyperparameters['batch_size'], shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:42:11.499292Z","iopub.execute_input":"2022-02-15T04:42:11.499795Z","iopub.status.idle":"2022-02-15T04:42:11.552835Z","shell.execute_reply.started":"2022-02-15T04:42:11.499756Z","shell.execute_reply":"2022-02-15T04:42:11.552121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train\nLets train the model\nwith BCEWithLogitsLoss and AdamW as optimizer\n\n**Notes:** on BCEWithLogitsLoss, the default value for reduction is `mean` (the sum of the output will be divided by the number of elements in the output). If we use this default value, it will produce negative loss. Because we have some negative labels. To fix this negative loss issue, we can use `none` as parameter. To calculate the mean, first, we have to filter out the negative values. [DOC](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = CustomModel(hyperparameters).to(DEVICE)\n\ncriterion = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\noptimizer = optim.AdamW(model.parameters(), lr=hyperparameters['lr'])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:42:15.355982Z","iopub.execute_input":"2022-02-15T04:42:15.356732Z","iopub.status.idle":"2022-02-15T04:42:25.713118Z","shell.execute_reply.started":"2022-02-15T04:42:15.356693Z","shell.execute_reply":"2022-02-15T04:42:25.712348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, criterion):\n        model.train()\n        train_loss = []\n\n        for batch in tqdm(dataloader):\n            optimizer.zero_grad()\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            token_type_ids = batch[2].to(DEVICE)\n            labels = batch[3].to(DEVICE)\n\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss = criterion(logits, labels)\n            # since, we have\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            train_loss.append(loss.item() * input_ids.size(0))\n            loss.backward()\n            # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n            # it's also improve f1 accuracy slightly\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n        return sum(train_loss)/len(train_loss)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:42:25.714917Z","iopub.execute_input":"2022-02-15T04:42:25.715184Z","iopub.status.idle":"2022-02-15T04:42:25.722858Z","shell.execute_reply.started":"2022-02-15T04:42:25.715156Z","shell.execute_reply":"2022-02-15T04:42:25.721721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, dataloader, criterion):\n        model.eval()\n        valid_loss = []\n        preds = []\n        offsets = []\n        seq_ids = []\n        valid_labels = []\n\n        for batch in tqdm(dataloader):\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            token_type_ids = batch[2].to(DEVICE)\n            labels = batch[3].to(DEVICE)\n            offset_mapping = batch[4]\n            sequence_ids = batch[5]\n\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss = criterion(logits, labels)\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            valid_loss.append(loss.item() * input_ids.size(0))\n\n            preds.append(logits.detach().cpu().numpy())\n            offsets.append(offset_mapping.numpy())\n            seq_ids.append(sequence_ids.numpy())\n            valid_labels.append(labels.detach().cpu().numpy())\n\n        preds = np.concatenate(preds, axis=0)\n        offsets = np.concatenate(offsets, axis=0)\n        seq_ids = np.concatenate(seq_ids, axis=0)\n        valid_labels = np.concatenate(valid_labels, axis=0)\n        location_preds = get_location_predictions(preds, offsets, seq_ids, test=False)\n        score = calculate_char_cv(location_preds, offsets, seq_ids, valid_labels)\n\n        return sum(valid_loss)/len(valid_loss), score","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:42:25.724423Z","iopub.execute_input":"2022-02-15T04:42:25.724974Z","iopub.status.idle":"2022-02-15T04:42:25.739482Z","shell.execute_reply.started":"2022-02-15T04:42:25.724935Z","shell.execute_reply":"2022-02-15T04:42:25.738749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\ntrain_loss_data, valid_loss_data = [], []\nscore_data_list = []\nvalid_loss_min = np.Inf\nsince = time.time()\nepochs = 3","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:42:25.741345Z","iopub.execute_input":"2022-02-15T04:42:25.741657Z","iopub.status.idle":"2022-02-15T04:42:25.752275Z","shell.execute_reply.started":"2022-02-15T04:42:25.74162Z","shell.execute_reply":"2022-02-15T04:42:25.751528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_loss = np.inf\n\nfor i in range(epochs):\n    print(\"Epoch: {}/{}\".format(i + 1, epochs))\n    # first train model\n    train_loss = train_model(model, train_dataloader, optimizer, criterion)\n    train_loss_data.append(train_loss)\n    print(f\"Train loss: {train_loss}\")\n    # evaluate model\n    valid_loss, score = eval_model(model, test_dataloader, criterion)\n    valid_loss_data.append(valid_loss)\n    score_data_list.append(score)\n    print(f\"Valid loss: {valid_loss}\")\n    print(f\"Valid score: {score}\")\n    \n    if valid_loss < best_loss:\n        best_loss = valid_loss\n        torch.save(model.state_dict(), \"nbme_bert_v2.pth\")\n\n    \ntime_elapsed = time.time() - since\nprint('Training completed in {:.0f}m {:.0f}s'.format(\n    time_elapsed // 60, time_elapsed % 60))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:42:27.445038Z","iopub.execute_input":"2022-02-15T04:42:27.445595Z","iopub.status.idle":"2022-02-15T04:43:00.148248Z","shell.execute_reply.started":"2022-02-15T04:42:27.445555Z","shell.execute_reply":"2022-02-15T04:43:00.146963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Experimets:\n- exp 1\nParams: Base bert with 1FC, epoch 5, lr 1e-5\n\n{'Accuracy': 0.9922235313632376, 'precision': 0.699022058288238, 'recall': 0.8327118203104674, 'f1': 0.7600327168148598}\n\n- exp 2:\nParams: Base bert with 2FC, epoch 2, lr 1e-5\n\n{'Accuracy': 0.9931995444417273, 'precision': 0.755762387079113, 'recall': 0.7980805365247304, 'f1': 0.7763452047860748}\n\n- exp 3:\nparams: 2FC, epoch 2, lr 1e-5 with gradient clip\n{'Accuracy': 0.9932764968526464, 'precision': 0.7633003963601853, 'recall': 0.7905067499205042, 'f1': 0.7766653886025079}\n\n- exp 4: 3FC, epoch 2, 1e-5 with gradient clip\n\n{'Accuracy': 0.9933637095850213, 'precision': 0.7576469952442715, 'recall': 0.8105397045645073, 'f1': 0.7832013519364255}\n","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nplt.plot(train_loss_data, label=\"Training loss\")\nplt.plot(valid_loss_data, label=\"validation loss\")\nplt.legend(frameon=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:51:19.579429Z","iopub.execute_input":"2022-02-14T17:51:19.579819Z","iopub.status.idle":"2022-02-14T17:51:19.830516Z","shell.execute_reply.started":"2022-02-14T17:51:19.579786Z","shell.execute_reply":"2022-02-14T17:51:19.829766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nscore_df = pd.DataFrame.from_dict(score_data_list)\nscore_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:51:22.937258Z","iopub.execute_input":"2022-02-14T17:51:22.937911Z","iopub.status.idle":"2022-02-14T17:51:22.949599Z","shell.execute_reply.started":"2022-02-14T17:51:22.937875Z","shell.execute_reply":"2022-02-14T17:51:22.948962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare For Testing","metadata":{}},{"cell_type":"markdown","source":"Load best model","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"nbme_bert_v2.pth\", map_location = DEVICE))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:51:33.339694Z","iopub.execute_input":"2022-02-14T17:51:33.339963Z","iopub.status.idle":"2022-02-14T17:51:33.565648Z","shell.execute_reply.started":"2022-02-14T17:51:33.339918Z","shell.execute_reply":"2022-02-14T17:51:33.564786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_df():\n    feats = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    test = pd.read_csv(f\"{BASE_URL}/test.csv\")\n\n    merged = test.merge(notes, how = \"left\")\n    merged = merged.merge(feats, how = \"left\")\n\n    def process_feature_text(text):\n        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n    \n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    \n    return merged\n\n\nclass SubmissionDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = self.tokenizer(\n            example[\"feature_text\"],\n            example[\"pn_history\"],\n            truncation = self.config['truncation'],\n            max_length = self.config['max_length'],\n            padding = self.config['padding'],\n            return_offsets_mapping = self.config['return_offsets_mapping']\n        )\n        tokenized[\"sequence_ids\"] = tokenized.sequence_ids()\n\n        input_ids = np.array(tokenized[\"input_ids\"])\n        attention_mask = np.array(tokenized[\"attention_mask\"])\n        token_type_ids = np.array(tokenized[\"token_type_ids\"])\n        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\")\n\n        return input_ids, attention_mask, token_type_ids, offset_mapping, sequence_ids\n\n\ntest_df = create_test_df()\n\nsubmission_data = SubmissionDataset(test_df, tokenizer, hyperparameters)\nsubmission_dataloader = DataLoader(submission_data, batch_size=hyperparameters['batch_size'], shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:14.28865Z","iopub.execute_input":"2022-02-15T04:43:14.288938Z","iopub.status.idle":"2022-02-15T04:43:14.615585Z","shell.execute_reply.started":"2022-02-15T04:43:14.288889Z","shell.execute_reply":"2022-02-15T04:43:14.614876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\npreds = []\noffsets = []\nseq_ids = []\n\nfor batch in tqdm(submission_dataloader):\n    input_ids = batch[0].to(DEVICE)\n    attention_mask = batch[1].to(DEVICE)\n    token_type_ids = batch[2].to(DEVICE)\n    offset_mapping = batch[3]\n    sequence_ids = batch[4]\n\n    logits = model(input_ids, attention_mask, token_type_ids)\n    \n    preds.append(logits.detach().cpu().numpy())\n    offsets.append(offset_mapping.numpy())\n    seq_ids.append(sequence_ids.numpy())\n\npreds = np.concatenate(preds, axis=0)\noffsets = np.concatenate(offsets, axis=0)\nseq_ids = np.concatenate(seq_ids, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:18.251294Z","iopub.execute_input":"2022-02-15T04:43:18.25185Z","iopub.status.idle":"2022-02-15T04:43:18.391331Z","shell.execute_reply.started":"2022-02-15T04:43:18.251811Z","shell.execute_reply":"2022-02-15T04:43:18.390536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_preds = get_location_predictions(preds, offsets, seq_ids, test=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:22.467394Z","iopub.execute_input":"2022-02-15T04:43:22.467991Z","iopub.status.idle":"2022-02-15T04:43:22.486548Z","shell.execute_reply.started":"2022-02-15T04:43:22.467953Z","shell.execute_reply":"2022-02-15T04:43:22.485852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(location_preds), len(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:24.683747Z","iopub.execute_input":"2022-02-15T04:43:24.684313Z","iopub.status.idle":"2022-02-15T04:43:24.690863Z","shell.execute_reply.started":"2022-02-15T04:43:24.684272Z","shell.execute_reply":"2022-02-15T04:43:24.689946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"location\"] = location_preds","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:26.979602Z","iopub.execute_input":"2022-02-15T04:43:26.980157Z","iopub.status.idle":"2022-02-15T04:43:26.98502Z","shell.execute_reply.started":"2022-02-15T04:43:26.980118Z","shell.execute_reply":"2022-02-15T04:43:26.984202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[[\"id\", \"location\"]].to_csv(\"submission.csv\", index = False)\npd.read_csv(\"submission.csv\").head()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:29.07465Z","iopub.execute_input":"2022-02-15T04:43:29.074938Z","iopub.status.idle":"2022-02-15T04:43:29.093071Z","shell.execute_reply.started":"2022-02-15T04:43:29.07489Z","shell.execute_reply":"2022-02-15T04:43:29.092348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Special Credits\n- [tomohiroh](https://www.kaggle.com/tomohiroh/nbme-bert-for-beginners)\n- [gazu468](https://www.kaggle.com/gazu468/nbme-details-eda)\n","metadata":{}}]}