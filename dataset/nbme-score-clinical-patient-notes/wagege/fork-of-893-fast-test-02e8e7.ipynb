{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"###### CFG","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\n\nclass CFG:\n    num_workers=4\n    path=\"../input/4-1-xlage-fake/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-xlarge\"\n    batch_size=32\n    fc_dropout=0.0\n    max_len=466\n    seed=42\n    n_fold=5\n    trn_fold=[0,1,2,3,4]\n\nclass CFG_2:\n    num_workers=4\n    path=\"../input/nbme-deberta-v3-large-fgm-pseudo/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=48\n    fc_dropout=0.0\n    max_len=370\n    seed=42\n    n_fold=5\n    trn_fold=[0,1,2,3,4]\n    \n\nclass CFG_3:\n    num_workers=4\n    path=\"../input/output-4-5-largev3-new-fgm/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=48\n    fc_dropout=0.0\n    max_len=354\n    seed=42\n    n_fold=5\n    trn_fold=[0,2,3]\n    \nclass CFG_4:\n    num_workers=4\n    path=\"../input/4-28-xlarge-fold-pseudo/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-xlarge\"\n    batch_size=32\n    fc_dropout=0.0\n    max_len=466\n    seed=42\n    n_fold=5\n    trn_fold=[0,1,2,3,4]\n\nclass CFG_5:\n    num_workers=4\n    path=\"../input/nmbe-v3large-0427-models/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=48\n    fc_dropout=0.0\n    max_len=370\n    seed=42\n    n_fold=5\n    trn_fold=[0,1,2,3,4]","metadata":{"papermill":{"duration":0.02543,"end_time":"2021-11-16T19:32:30.040766","exception":false,"start_time":"2021-11-16T19:32:30.015336","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-20T17:44:55.51389Z","iopub.execute_input":"2022-04-20T17:44:55.514462Z","iopub.status.idle":"2022-04-20T17:44:55.522343Z","shell.execute_reply.started":"2022-04-20T17:44:55.514427Z","shell.execute_reply":"2022-04-20T17:44:55.521526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/debert-largev3-token\" + \"/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py', \"deberta__init__.py\"]:\n    if str(filename).startswith(\"deberta\"):\n        filepath = deberta_v2_path/str(filename).replace(\"deberta\", \"\")\n    else:\n        filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:44:55.549072Z","iopub.execute_input":"2022-04-20T17:44:55.549262Z","iopub.status.idle":"2022-04-20T17:44:55.571429Z","shell.execute_reply.started":"2022-04-20T17:44:55.549239Z","shell.execute_reply":"2022-04-20T17:44:55.570521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Library","metadata":{"papermill":{"duration":0.016162,"end_time":"2021-11-16T19:32:40.221507","exception":false,"start_time":"2021-11-16T19:32:40.205345","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\n# os.system('pip uninstall -y transformers')\n# os.system('python -m pip install --no-index --find-links=../input/nbme-pip-wheels transformers')\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"papermill":{"duration":30.77583,"end_time":"2021-11-16T19:33:11.013554","exception":false,"start_time":"2021-11-16T19:32:40.237724","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-20T17:44:55.585241Z","iopub.execute_input":"2022-04-20T17:44:55.58556Z","iopub.status.idle":"2022-04-20T17:44:55.601222Z","shell.execute_reply.started":"2022-04-20T17:44:55.585533Z","shell.execute_reply":"2022-04-20T17:44:55.600549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\nfrom transformers.models.deberta_v2 import DebertaV2TokenizerFast\ntokenizer = DebertaV2TokenizerFast.from_pretrained('../input/large-v3-tokenizer/tokenizer')\n    \n# tokenizer = AutoTokenizer.from_pretrained(CFG.model)\nCFG_2.tokenizer = tokenizer\n\nCFG_3.tokenizer = tokenizer\n\nCFG_4.tokenizer = AutoTokenizer.from_pretrained(CFG_4.path+'tokenizer/')\n\nCFG_5.tokenizer = tokenizer\n","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:44:55.615507Z","iopub.execute_input":"2022-04-20T17:44:55.615944Z","iopub.status.idle":"2022-04-20T17:44:56.895256Z","shell.execute_reply.started":"2022-04-20T17:44:55.615917Z","shell.execute_reply":"2022-04-20T17:44:56.894394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions for scoring","metadata":{"papermill":{"duration":0.01915,"end_time":"2021-11-16T19:33:11.052159","exception":false,"start_time":"2021-11-16T19:33:11.033009","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n\ndef micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    return f1_score(truths, preds)\n\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n    return binary\n\n\ndef span_micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n    return micro_f1(bin_preds, bin_truths)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:44:56.899022Z","iopub.execute_input":"2022-04-20T17:44:56.899243Z","iopub.status.idle":"2022-04-20T17:44:56.908696Z","shell.execute_reply.started":"2022-04-20T17:44:56.899215Z","shell.execute_reply":"2022-04-20T17:44:56.907992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_labels_for_scoring(df):\n    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n        if len(location_list) > 0:\n            location = location_list[0]\n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n    return truths\n\n\ndef get_char_probs(texts, predictions, tokenizer):\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n    return results\n\n\ndef get_results(char_probs, th=0.5, texts=None):\n    results = []\n    for idx, char_prob in enumerate(char_probs):\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        \n        if len(result) > 0:\n            if result[0][0] == 1:\n                result[0][0] = 0\n                \n        te = texts[idx]\n        encoded = CFG_2.tokenizer(te,\n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n\n       \n        result_new = []\n        if len(result) > 0:\n            for r in result:\n                str_tmp = te[min(r):max(r)]\n\n                pos_b = min(r)\n                if pos_b > 0:\n                    txt_b = te[pos_b]\n                    txt_a = te[pos_b - 1]\n\n                    for idx, token in enumerate(encoded.encodings[0].tokens):\n                        map_tmp = encoded['offset_mapping'][idx]\n                        if  pos_b >= map_tmp[0] and pos_b <= map_tmp[1]:\n                            if txt_a in token and txt_b in token:\n                                r[0] = r[0] - 1\n                                break\n\n                # pos_d = max(r)\n                # if pos_d < len(te) - 1:\n                #     txt_a = te[pos_d - 1]\n                #     txt_b = te[pos_d]\n                #\n                #     for idx, token in enumerate(encoded.encodings[0].tokens):\n                #         map = encoded['offset_mapping'][idx]\n                #         if pos_d >= map[0] and pos_d <= map[1]:\n                #             if txt_a in token and txt_b in token:\n                #                 r[-1] = r[-1] + 1\n                #                 break\n\n                result_new.append(r)\n\n        result = result_new\n\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n    return results\n\n\n\ndef get_predictions(results):\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:44:56.910261Z","iopub.execute_input":"2022-04-20T17:44:56.911784Z","iopub.status.idle":"2022-04-20T17:44:56.935095Z","shell.execute_reply.started":"2022-04-20T17:44:56.911718Z","shell.execute_reply":"2022-04-20T17:44:56.934027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_results_pp2(char_probs, th=0.5, texts=None):\n    results = []\n    for idx, char_prob in enumerate(char_probs):\n        text = texts[idx]                            # 对照文本\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        temp = []\n        for r in result:\n            start = min(r)\n            end = max(r)\n            if start <= 1:           # 修复丢失文本第 0 个字符的情况\n                start = 0\n            elif start == end:\n                start -= 1\n            elif re.match(r'^[a-zA-Z0-9]$', text[start - 1]) and (text[start - 2] in ['\\t', '\\n', '\\r', ',', '.', ':', ';', '-', '+', '\"', '(', '/', '&', '*']):   # 修复没有空格而丢失单词第一个字符的情况\n                start -= 1\n            else:\n                pass\n            temp.append(f\"{start} {end}\")    \n        temp = \";\".join(temp)    \n        results.append(temp)\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:44:56.938805Z","iopub.execute_input":"2022-04-20T17:44:56.939588Z","iopub.status.idle":"2022-04-20T17:44:56.95106Z","shell.execute_reply.started":"2022-04-20T17:44:56.939543Z","shell.execute_reply":"2022-04-20T17:44:56.950145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = span_micro_f1(y_true, y_pred)\n    return score\n\n\ndef get_logger(filename='inference'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"papermill":{"duration":0.034649,"end_time":"2021-11-16T19:33:11.105766","exception":false,"start_time":"2021-11-16T19:33:11.071117","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-20T17:44:56.952855Z","iopub.execute_input":"2022-04-20T17:44:56.95321Z","iopub.status.idle":"2022-04-20T17:44:56.965408Z","shell.execute_reply.started":"2022-04-20T17:44:56.953172Z","shell.execute_reply":"2022-04-20T17:44:56.964498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OOF","metadata":{}},{"cell_type":"markdown","source":"# Data Loading","metadata":{"papermill":{"duration":0.018406,"end_time":"2021-11-16T19:33:11.150174","exception":false,"start_time":"2021-11-16T19:33:11.131768","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\ntest = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\nsubmission = pd.read_csv('../input/nbme-score-clinical-patient-notes/sample_submission.csv')\nfeatures = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\nfeatures = preprocess_features(features)\npatient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n\nprint(f\"test.shape: {test.shape}\")\ndisplay(test.head())\nprint(f\"features.shape: {features.shape}\")\ndisplay(features.head())\nprint(f\"patient_notes.shape: {patient_notes.shape}\")\ndisplay(patient_notes.head())","metadata":{"papermill":{"duration":0.637101,"end_time":"2021-11-16T19:33:11.805349","exception":false,"start_time":"2021-11-16T19:33:11.168248","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-20T17:44:56.967197Z","iopub.execute_input":"2022-04-20T17:44:56.967564Z","iopub.status.idle":"2022-04-20T17:44:57.627034Z","shell.execute_reply.started":"2022-04-20T17:44:56.967525Z","shell.execute_reply":"2022-04-20T17:44:57.626189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.merge(features, on=['feature_num', 'case_num'], how='left')\ntest = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:44:57.628463Z","iopub.execute_input":"2022-04-20T17:44:57.628775Z","iopub.status.idle":"2022-04-20T17:44:57.657361Z","shell.execute_reply.started":"2022-04-20T17:44:57.628739Z","shell.execute_reply":"2022-04-20T17:44:57.656467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input_fast(cfg, text, feature_text, batch_max_len):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           max_length=batch_max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDatasetFast(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n        self.batch_max_len = df['batch_max_length'].values\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input_fast(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item],\n                               self.batch_max_len[item],\n                              )\n        return inputs","metadata":{"papermill":{"duration":0.040128,"end_time":"2021-11-16T19:33:20.931029","exception":false,"start_time":"2021-11-16T19:33:20.890901","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-20T17:44:57.658844Z","iopub.execute_input":"2022-04-20T17:44:57.659187Z","iopub.status.idle":"2022-04-20T17:44:57.670653Z","shell.execute_reply.started":"2022-04-20T17:44:57.659148Z","shell.execute_reply":"2022-04-20T17:44:57.669773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.02209,"end_time":"2021-11-16T19:33:20.978793","exception":false,"start_time":"2021-11-16T19:33:20.956703","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"papermill":{"duration":0.032939,"end_time":"2021-11-16T19:33:21.034275","exception":false,"start_time":"2021-11-16T19:33:21.001336","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-20T17:44:57.672398Z","iopub.execute_input":"2022-04-20T17:44:57.672937Z","iopub.status.idle":"2022-04-20T17:44:57.687906Z","shell.execute_reply.started":"2022-04-20T17:44:57.672896Z","shell.execute_reply":"2022-04-20T17:44:57.686864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# inference","metadata":{"papermill":{"duration":0.022058,"end_time":"2021-11-16T19:33:21.081885","exception":false,"start_time":"2021-11-16T19:33:21.059827","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn_fast(test_loader, model, device, max_len_in):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n    # for inputs in test_loader:\n        bs = len(inputs['input_ids'])\n        pred_w_pad = np.zeros((bs, max_len_in, 1))\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        y_preds = y_preds.sigmoid().to('cpu').numpy()\n        pred_w_pad[:, :y_preds.shape[1]] = y_preds\n        preds.append(pred_w_pad)\n    predictions = np.concatenate(preds)\n    return predictions\n","metadata":{"papermill":{"duration":0.044153,"end_time":"2021-11-16T19:33:21.148373","exception":false,"start_time":"2021-11-16T19:33:21.10422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-20T17:44:57.692824Z","iopub.execute_input":"2022-04-20T17:44:57.69328Z","iopub.status.idle":"2022-04-20T17:44:57.711207Z","shell.execute_reply.started":"2022-04-20T17:44:57.693239Z","shell.execute_reply":"2022-04-20T17:44:57.710038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Reduce Padding Inference ######\n\n# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:44:57.712941Z","iopub.execute_input":"2022-04-20T17:44:57.713273Z","iopub.status.idle":"2022-04-20T17:44:57.773481Z","shell.execute_reply.started":"2022-04-20T17:44:57.713176Z","shell.execute_reply":"2022-04-20T17:44:57.772718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDatasetFast(CFG, sort_df)\ntest_loader = DataLoader(test_dataset,\n                      batch_size=CFG.batch_size,\n                      shuffle=False,\n                      num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n\nprint(len(test_dataset))\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn_fast(test_loader, model, device, CFG.max_len)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    ## data re-sort ## \n    prediction = prediction[np.argsort(length_sorted_idx)]\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_xlarge = np.mean(predictions, axis=0)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-20T17:44:57.775096Z","iopub.execute_input":"2022-04-20T17:44:57.775653Z","iopub.status.idle":"2022-04-20T17:49:26.684225Z","shell.execute_reply.started":"2022-04-20T17:44:57.775609Z","shell.execute_reply":"2022-04-20T17:49:26.683513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_2 = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\nsubmission = pd.read_csv('../input/nbme-score-clinical-patient-notes/sample_submission.csv')\n\n\nfeatures_2 = pd.read_csv('../input/nbme-feature-data/features_with_meanings.csv')\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\nfeatures_2 = preprocess_features(features_2)\nfeatures_2['feature_text'] = features_2.apply(lambda row: row['feature_text'] + '[SEP]meaning: ' + row['meaning'], axis=1)\nfeatures_2.drop(['meaning'], axis=1, inplace=True)\n\n\npatient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n\nprint(f\"test.shape: {test_2.shape}\")\ndisplay(test.head())\nprint(f\"features.shape: {features_2.shape}\")\ndisplay(features.head())\nprint(f\"patient_notes.shape: {patient_notes.shape}\")\ndisplay(patient_notes.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:49:26.688216Z","iopub.execute_input":"2022-04-20T17:49:26.690264Z","iopub.status.idle":"2022-04-20T17:49:27.40869Z","shell.execute_reply.started":"2022-04-20T17:49:26.690214Z","shell.execute_reply":"2022-04-20T17:49:27.40797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\ntest_3 = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\ntest_4 = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\ntest_5 = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\nfeatures_3 = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\nfeatures_3 = preprocess_features(features_3)\n\nprint(f\"test.shape: {test_3.shape}\")\ndisplay(test_3.head())\nprint(f\"features.shape: {features_3.shape}\")\ndisplay(features_3.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:49:27.409922Z","iopub.execute_input":"2022-04-20T17:49:27.41165Z","iopub.status.idle":"2022-04-20T17:49:27.437345Z","shell.execute_reply.started":"2022-04-20T17:49:27.411608Z","shell.execute_reply":"2022-04-20T17:49:27.436631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_3 = test_3.merge(features_3, on=['feature_num', 'case_num'], how='left')\ntest_3 = test_3.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\ndisplay(test_3.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:49:27.438722Z","iopub.execute_input":"2022-04-20T17:49:27.439612Z","iopub.status.idle":"2022-04-20T17:49:27.46832Z","shell.execute_reply.started":"2022-04-20T17:49:27.43957Z","shell.execute_reply":"2022-04-20T17:49:27.467471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_4 = test_4.merge(features_3, on=['feature_num', 'case_num'], how='left')\ntest_4 = test_4.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\ndisplay(test_4.head())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_2 = test_2.merge(features_2, on=['feature_num', 'case_num'], how='left')\ntest_2 = test_2.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:49:27.469639Z","iopub.execute_input":"2022-04-20T17:49:27.470136Z","iopub.status.idle":"2022-04-20T17:49:27.498167Z","shell.execute_reply.started":"2022-04-20T17:49:27.470092Z","shell.execute_reply":"2022-04-20T17:49:27.497441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_5 = test_5.merge(features_2, on=['feature_num', 'case_num'], how='left')\ntest_5 = test_5.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\ndisplay(test.head())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Reduce Padding Inference ######\n\n# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test_2['pn_history'].fillna(\"\").values, test_2['feature_text'].fillna(\"\").values), total=len(test_2))\nfor text, feature_text in tk0:\n    length = len(CFG_2.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest_2['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test_2.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG_2.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:49:27.499592Z","iopub.execute_input":"2022-04-20T17:49:27.499866Z","iopub.status.idle":"2022-04-20T17:49:27.554352Z","shell.execute_reply.started":"2022-04-20T17:49:27.49983Z","shell.execute_reply":"2022-04-20T17:49:27.553514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset_2 = TestDatasetFast(CFG_2, sort_df)\ntest_loader_2 = DataLoader(test_dataset_2,\n                      batch_size=CFG_2.batch_size,\n                      shuffle=False,\n                      num_workers=CFG_2.num_workers, pin_memory=True, drop_last=False)\n\nprint(len(test_dataset_2))\npredictions_v3_all = []\nfor fold in CFG_2.trn_fold:\n    model_2 = CustomModel(CFG_2, config_path=CFG_2.config_path, pretrained=False)\n    \n    state_2 = torch.load(CFG_2.path+f\"{CFG_2.model.replace('/', '-')}_fold{fold}_best.pth\",\n                   map_location=torch.device('cpu'))\n    \n    model_2.load_state_dict(state_2['model'])\n    prediction = inference_fn_fast(test_loader_2, model_2, device, CFG_2.max_len)\n    prediction = prediction.reshape((len(test_2), CFG_2.max_len))\n    ## data re-sort ## \n    prediction = prediction[np.argsort(length_sorted_idx)]\n    \n    char_probs = get_char_probs(test_2['pn_history'].values, prediction, CFG_2.tokenizer)\n    predictions_v3_all.append(char_probs)\n    del model_2, state_2, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_v3 = np.mean(predictions_v3_all, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:49:27.556012Z","iopub.execute_input":"2022-04-20T17:49:27.556323Z","iopub.status.idle":"2022-04-20T17:52:06.120583Z","shell.execute_reply.started":"2022-04-20T17:49:27.556288Z","shell.execute_reply":"2022-04-20T17:52:06.119751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Reduce Padding Inference ######\n\n# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test_3['pn_history'].fillna(\"\").values, test_3['feature_text'].fillna(\"\").values), total=len(test_3))\nfor text, feature_text in tk0:\n    length = len(CFG_3.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest_3['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test_3.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG_3.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:52:06.12508Z","iopub.execute_input":"2022-04-20T17:52:06.127097Z","iopub.status.idle":"2022-04-20T17:52:06.211759Z","shell.execute_reply.started":"2022-04-20T17:52:06.127058Z","shell.execute_reply":"2022-04-20T17:52:06.211088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset_3 = TestDatasetFast(CFG_3, sort_df)\ntest_loader_3 = DataLoader(test_dataset_3,\n                      batch_size=CFG_3.batch_size,\n                      shuffle=False,\n                      num_workers=CFG_3.num_workers, pin_memory=True, drop_last=False)\n\nprint(len(test_dataset_3))\npredictions_largev3_all_3 = []\nfor fold in CFG_3.trn_fold:\n    model_3 = CustomModel(CFG_3, config_path=CFG_3.config_path, pretrained=False)\n    \n    state_3 = torch.load(CFG_3.path+f\"{CFG_3.model.replace('/', '-')}_fold{fold}_best.pth\",\n               map_location=torch.device('cpu'))\n    \n    model_3.load_state_dict(state_3['model'])\n    \n    prediction = inference_fn_fast(test_loader_3, model_3, device, CFG_3.max_len)\n    prediction = prediction.reshape((len(test_3), CFG_3.max_len))\n    ## data re-sort ## \n    prediction = prediction[np.argsort(length_sorted_idx)]\n    \n    char_probs = get_char_probs(test_3['pn_history'].values, prediction, CFG_3.tokenizer)\n    predictions_largev3_all_3.append(char_probs)\n    del model_3, state_3, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\n\npredictions_v3_2 = np.mean(predictions_largev3_all_3, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:52:06.213Z","iopub.execute_input":"2022-04-20T17:52:06.213361Z","iopub.status.idle":"2022-04-20T17:54:45.998308Z","shell.execute_reply.started":"2022-04-20T17:52:06.213322Z","shell.execute_reply":"2022-04-20T17:54:45.99747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Reduce Padding Inference ######\n\n# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test_4['pn_history'].fillna(\"\").values, test_4['feature_text'].fillna(\"\").values), total=len(test_4))\nfor text, feature_text in tk0:\n    length = len(CFG_4.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest_4['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test_4.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG_4.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset_4 = TestDatasetFast(CFG_4, sort_df)\ntest_loader_4 = DataLoader(test_dataset_4,\n                      batch_size=CFG_4.batch_size,\n                      shuffle=False,\n                      num_workers=CFG_4.num_workers, pin_memory=True, drop_last=False)\n\nprint(len(test_dataset_4))\npredictions_xlarge_all_new = []\nfor fold in CFG_4.trn_fold:\n    model_4 = CustomModel(CFG_4, config_path=CFG_4.config_path, pretrained=False)\n    \n    state_4 = torch.load(CFG_4.path+f\"{CFG_4.model.replace('/', '-')}_fold{fold}_best.pth\",\n               map_location=torch.device('cpu'))\n    \n    model_4.load_state_dict(state_4['model'])\n    \n    prediction = inference_fn_fast(test_loader_4, model_4, device, CFG_4.max_len)\n    prediction = prediction.reshape((len(test_4), CFG_4.max_len))\n    ## data re-sort ## \n    prediction = prediction[np.argsort(length_sorted_idx)]\n    \n    char_probs = get_char_probs(test_4['pn_history'].values, prediction, CFG_4.tokenizer)\n    predictions_xlarge_all_new.append(char_probs)\n    del model_4, state_4, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\n\npredictions_xlarge_new = np.mean(predictions_xlarge_all_new, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Reduce Padding Inference ######\n\n# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test_5['pn_history'].fillna(\"\").values, test_5['feature_text'].fillna(\"\").values), total=len(test_5))\nfor text, feature_text in tk0:\n    length = len(CFG_5.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest_5['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test_5.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG_5.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset_5 = TestDatasetFast(CFG_5, sort_df)\ntest_loader_5 = DataLoader(test_dataset_5,\n                      batch_size=CFG_5.batch_size,\n                      shuffle=False,\n                      num_workers=CFG_5.num_workers, pin_memory=True, drop_last=False)\n\nprint(len(test_dataset_5))\npredictions_largev3_h_new = []\nfor fold in CFG_5.trn_fold:\n    model_5 = CustomModel(CFG_5, config_path=CFG_5.config_path, pretrained=False)\n    \n    state_5 = torch.load(CFG_5.path+f\"{CFG_5.model.replace('/', '-')}_fold{fold}_best.pth\",\n               map_location=torch.device('cpu'))\n    \n    model_5.load_state_dict(state_5['model'])\n    \n    prediction = inference_fn_fast(test_loader_5, model_5, device, CFG_5.max_len)\n    prediction = prediction.reshape((len(test_5), CFG_5.max_len))\n    ## data re-sort ## \n    prediction = prediction[np.argsort(length_sorted_idx)]\n    \n    char_probs = get_char_probs(test_5['pn_history'].values, prediction, CFG_5.tokenizer)\n    predictions_largev3_h_new.append(char_probs)\n    del model_5, state_5, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\n\npredictions_largev3_h = np.mean(predictions_largev3_h_new, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_1 = predictions_xlarge * 0.4 + predictions_v3 * 0.32 + predictions_v3_2 * 0.28\npredictions_2 = predictions_xlarge_new * 0.75 + predictions_largev3_h * 0.25\n\npredictions = predictions_1 * 0.45 + predictions_2 * 0.55\n\nresults = get_results_pp2(predictions, 0.44, test['pn_history'].values)\nsubmission['location'] = results\ndisplay(submission.head())\nsubmission[['id', 'location']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T17:54:46.003273Z","iopub.execute_input":"2022-04-20T17:54:46.005266Z","iopub.status.idle":"2022-04-20T17:54:46.028072Z","shell.execute_reply.started":"2022-04-20T17:54:46.005227Z","shell.execute_reply":"2022-04-20T17:54:46.027454Z"},"trusted":true},"execution_count":null,"outputs":[]}]}