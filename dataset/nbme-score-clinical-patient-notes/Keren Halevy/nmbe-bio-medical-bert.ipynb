{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom ast import literal_eval\nfrom itertools import chain\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom tqdm.notebook import tqdm, trange\nfrom sklearn.model_selection import StratifiedKFold\nimport torch\nfrom transformers import AutoModel, AutoTokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-09T18:05:22.963622Z","iopub.execute_input":"2022-04-09T18:05:22.964389Z","iopub.status.idle":"2022-04-09T18:05:22.970675Z","shell.execute_reply.started":"2022-04-09T18:05:22.964347Z","shell.execute_reply":"2022-04-09T18:05:22.969537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class CFG:\n    root = \"../input/nbme-score-clinical-patient-notes\"\n    debug = False\n    n_fold=5\n  #  model_path = \"emilyalsentzer/Bio_ClinicalBERT\"\n    model=\"../input/bio-clinicalbert\"\n    max_length = 512\n    doc_stride = 128\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    lr = 1e-5\n    batch_size = 16\n    epochs = 3","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:05:25.574112Z","iopub.execute_input":"2022-04-09T18:05:25.574421Z","iopub.status.idle":"2022-04-09T18:05:25.580086Z","shell.execute_reply.started":"2022-04-09T18:05:25.574387Z","shell.execute_reply":"2022-04-09T18:05:25.579311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create df","metadata":{}},{"cell_type":"code","source":"def create_train_df():\n    feats = pd.read_csv(f\"{CFG.root}/features.csv\")\n    notes = pd.read_csv(f\"{CFG.root}/patient_notes.csv\")\n    train = pd.read_csv(f\"{CFG.root}/train.csv\")\n\n    train[\"annotation_list\"] = [literal_eval(x) for x in train[\"annotation\"]]\n    train[\"location_list\"] = [literal_eval(x) for x in train[\"location\"]]\n    merged = train.merge(notes, how = \"left\")\n    merged = merged.merge(feats, how = \"left\")\n    merged = merged.loc[merged[\"annotation\"] != \"[]\"].copy().reset_index(drop = True) # comment out if you train all samples\n    \n    def process_feature_text(text):\n        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    \n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n    \n    merged['location_prediction'] = -1\n    merged['token_proba'] = -1\n    merged['token_offsets'] = -1\n\n    if CFG.debug:\n        merged = merged.sample(frac = 0.5).reset_index(drop = True)\n\n    skf = StratifiedKFold(CFG.n_fold)\n    merged[\"stratify_on\"] = merged[\"case_num\"].astype(str) + merged[\"feature_num\"].astype(str)\n    merged[\"fold\"] = -1\n    for fold, (_, valid_idx) in enumerate(skf.split(merged[\"id\"], y = merged[\"stratify_on\"])):\n        merged.loc[valid_idx, \"fold\"] = fold\n    \n    print(merged.shape)\n    return merged\n\ndf = create_train_df()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:05:29.175569Z","iopub.execute_input":"2022-04-09T18:05:29.175902Z","iopub.status.idle":"2022-04-09T18:05:30.148521Z","shell.execute_reply.started":"2022-04-09T18:05:29.175869Z","shell.execute_reply":"2022-04-09T18:05:30.147682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:05:33.263845Z","iopub.execute_input":"2022-04-09T18:05:33.264151Z","iopub.status.idle":"2022-04-09T18:05:33.284738Z","shell.execute_reply.started":"2022-04-09T18:05:33.264114Z","shell.execute_reply":"2022-04-09T18:05:33.283639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first = df.loc[0]\nexample = {\n    \"feature_text\": first.feature_text,\n    \"pn_history\": first.pn_history,\n    \"location_list\": first.location_list,\n    \"annotation_list\": first.annotation_list\n}\nfor key in example.keys():\n    print(key)\n    print(example[key])\n    print(\"=\" * 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:05:36.830353Z","iopub.execute_input":"2022-04-09T18:05:36.830644Z","iopub.status.idle":"2022-04-09T18:05:36.849462Z","shell.execute_reply.started":"2022-04-09T18:05:36.830613Z","shell.execute_reply":"2022-04-09T18:05:36.847923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\nprint(example[\"location_list\"])\nexample_loc_ints = loc_list_to_ints(example[\"location_list\"])[0]\nprint(example_loc_ints)\nprint(example[\"pn_history\"][example_loc_ints[0] : example_loc_ints[1]])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:05:39.403396Z","iopub.execute_input":"2022-04-09T18:05:39.403678Z","iopub.status.idle":"2022-04-09T18:05:39.411534Z","shell.execute_reply.started":"2022-04-09T18:05:39.403648Z","shell.execute_reply":"2022-04-09T18:05:39.410878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.model)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:05:45.467879Z","iopub.execute_input":"2022-04-09T18:05:45.468136Z","iopub.status.idle":"2022-04-09T18:05:45.556514Z","shell.execute_reply.started":"2022-04-09T18:05:45.46811Z","shell.execute_reply":"2022-04-09T18:05:45.555682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Questions: \n# 1. why not use doc_stride  -> treated\n# 2. why using duoble instead of int -> treated\ndef tokenize_and_add_labels(tokenizer, example):\n    tokenized_inputs = tokenizer(\n        example[\"feature_text\"],\n        example[\"pn_history\"],\n        max_length = CFG.max_length,\n        stride = CFG.doc_stride,\n        padding = \"max_length\",\n        truncation = \"only_second\",\n        return_offsets_mapping = True\n    )\n    labels = [0.0] * len(tokenized_inputs[\"input_ids\"])\n    tokenized_inputs[\"location_int\"] = loc_list_to_ints(example[\"location_list\"])\n    tokenized_inputs[\"sequence_ids\"] = tokenized_inputs.sequence_ids()\n\n    for idx, (seq_id, offsets) in enumerate(zip(tokenized_inputs[\"sequence_ids\"], tokenized_inputs[\"offset_mapping\"])):\n        if seq_id is None or seq_id == 0:\n            labels[idx] = -100\n            continue\n        exit = False\n        token_start, token_end = offsets\n        for feature_start, feature_end in tokenized_inputs[\"location_int\"]:\n            if exit:\n                break\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                exit = True\n    tokenized_inputs[\"labels\"] = labels\n    \n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:05:48.774237Z","iopub.execute_input":"2022-04-09T18:05:48.774516Z","iopub.status.idle":"2022-04-09T18:05:48.786091Z","shell.execute_reply.started":"2022-04-09T18:05:48.774487Z","shell.execute_reply":"2022-04-09T18:05:48.785121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_inputs = tokenize_and_add_labels(tokenizer, example)\nfor key in tokenized_inputs.keys():\n    print(key)\n    print(tokenized_inputs[key])\n    print(\"=\" * 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:05:53.236653Z","iopub.execute_input":"2022-04-09T18:05:53.23696Z","iopub.status.idle":"2022-04-09T18:05:53.255985Z","shell.execute_reply.started":"2022-04-09T18:05:53.236924Z","shell.execute_reply":"2022-04-09T18:05:53.2553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we need \"input_ids\" and \"attention_mask\" for BERT.\n\nlabels are 1.0 at annotation.\n\nso we can train as binary classification; does this word(token) represent the feature? -> 1 or 0","metadata":{}},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class NBMEData(torch.utils.data.Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = tokenize_and_add_labels(self.tokenizer, example)\n\n        input_ids = np.array(tokenized[\"input_ids\"]) # for input BERT\n        attention_mask = np.array(tokenized[\"attention_mask\"]) # for input BERT\n        labels = np.array(tokenized[\"labels\"]) # for calculate loss and cv score\n\n        offset_mapping = np.array(tokenized[\"offset_mapping\"]) # for calculate cv score\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\") # for calculate cv score\n        \n        return input_ids, attention_mask, labels, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:05:58.583653Z","iopub.execute_input":"2022-04-09T18:05:58.584502Z","iopub.status.idle":"2022-04-09T18:05:58.592668Z","shell.execute_reply.started":"2022-04-09T18:05:58.584462Z","shell.execute_reply":"2022-04-09T18:05:58.591786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class NBMEModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = AutoModel.from_pretrained(CFG.model) # BERT model\n        self.dropout = torch.nn.Dropout(p = 0.2)\n        self.classifier = torch.nn.Linear(768, 1) # BERT has last_hidden_state(size: sequqence_length, 768)\n    \n    def forward(self, input_ids, attention_mask):\n        last_hidden_state = self.backbone(input_ids = input_ids, attention_mask = attention_mask)[0] # idx 0 is last_hidden_state; backbone().last_hidden_state is also good\n        logits = self.classifier(self.dropout(last_hidden_state)).squeeze(-1)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:06:06.167291Z","iopub.execute_input":"2022-04-09T18:06:06.168024Z","iopub.status.idle":"2022-04-09T18:06:06.175626Z","shell.execute_reply.started":"2022-04-09T18:06:06.167987Z","shell.execute_reply":"2022-04-09T18:06:06.174931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def train_loop(fold):\n    model = NBMEModel().to(CFG.device)\n    #criterion = torch.nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), CFG.lr)\n\n    train = df.loc[df[\"fold\"] != fold].reset_index(drop = True)\n    valid = df.loc[df[\"fold\"] == fold].reset_index(drop = True)\n    train_ds = NBMEData(train, tokenizer)\n    valid_ds = NBMEData(valid, tokenizer)\n    train_dl = torch.utils.data.DataLoader(train_ds, batch_size = CFG.batch_size, pin_memory = True, shuffle = True, drop_last = True)\n    valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size = CFG.batch_size * 2, pin_memory = True, shuffle = False, drop_last = False)\n    \n    return train_dl, valid_dl, model, optimizer","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:06:09.581909Z","iopub.execute_input":"2022-04-09T18:06:09.582204Z","iopub.status.idle":"2022-04-09T18:06:09.591268Z","shell.execute_reply.started":"2022-04-09T18:06:09.582172Z","shell.execute_reply":"2022-04-09T18:06:09.590096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n = 1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef get_location_predictions(preds, offset_mapping, sequence_ids, test = False):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        pred = sigmoid(pred)\n        start_idx = None\n        current_preds = []\n        for p, o, s_id in zip(pred, offsets, seq_ids):\n            if s_id is None or s_id == 0:\n                continue\n            if p > 0.75:\n                if start_idx is None:\n                    start_idx = o[0]\n                end_idx = o[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n    return all_predictions\n\ndef calculate_char_CV(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros((num_chars))\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n        char_preds = np.zeros((num_chars))\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n    results = precision_recall_fscore_support(all_labels, all_preds, average = \"binary\")\n    return {\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:06:13.107049Z","iopub.execute_input":"2022-04-09T18:06:13.107942Z","iopub.status.idle":"2022-04-09T18:06:13.12583Z","shell.execute_reply.started":"2022-04-09T18:06:13.107898Z","shell.execute_reply":"2022-04-09T18:06:13.125123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_loop():    \n    history = {}\n    for fold in range(CFG.n_fold):\n        print(f\"========== fold: {fold} training ==========\")\n        train_dl, valid_dl, model, optimizer = train_loop(fold)\n        history[fold] = {\"train\": [], \"valid\": []}\n        best_loss = np.inf\n        \n        for epoch in range(CFG.epochs):\n            print(f\"========== EPOCH: {epoch} training ==========\")\n            #training\n            model.train()\n            train_loss = AverageMeter()\n            pbar = tqdm(train_dl)\n            for batch in pbar:\n                optimizer.zero_grad()\n                input_ids = batch[0].to(CFG.device)\n                attention_mask = batch[1].to(CFG.device)\n                labels = batch[2].to(CFG.device)\n                offset_mapping = batch[3]\n                sequence_ids = batch[4]\n                logits = model(input_ids, attention_mask)\n                loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n                loss = loss_fct(logits, labels)\n                loss = torch.masked_select(loss, labels > -1).mean() # we should calculate at \"pn_history\"; labels at \"feature_text\" are -100 < -1\n                loss.backward()\n                optimizer.step()\n                train_loss.update(val = loss.item(), n = len(input_ids))\n                pbar.set_postfix(Loss = train_loss.avg)\n            print(epoch, train_loss.avg)\n            history[fold][\"train\"].append(train_loss.avg)\n\n            #evaluation\n            model.eval()\n            valid_loss = AverageMeter()\n            preds = []\n            offsets = []\n            seq_ids = []\n            lbls = []\n            with torch.no_grad():\n                for batch in tqdm(valid_dl):\n                    input_ids = batch[0].to(CFG.device)\n                    attention_mask = batch[1].to(CFG.device)\n                    labels = batch[2].to(CFG.device)\n                    offset_mapping = batch[3]\n                    sequence_ids = batch[4]\n                    logits = model(input_ids, attention_mask)\n                    loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n                    loss = loss_fct(logits, labels)\n                    loss = torch.masked_select(loss, labels > -1).mean()\n                    valid_loss.update(val = loss.item(), n = len(input_ids))\n                    pbar.set_postfix(Loss = valid_loss.avg)\n                    preds.append(logits.cpu().numpy())\n                    offsets.append(offset_mapping.numpy())\n                    seq_ids.append(sequence_ids.numpy())\n                    lbls.append(labels.cpu().numpy())\n            print(epoch, valid_loss.avg)\n            history[fold][\"valid\"].append(valid_loss.avg)          \n            \n            # save model\n            if valid_loss.avg < best_loss:\n                best_loss = valid_loss.avg\n                torch.save(model.state_dict(), f\"nbme_{fold}.pth\")\n                preds = np.concatenate(preds, axis = 0)\n                # print(preds.shape)\n                # print([preds[i][0] for i in range(preds.shape[0])])\n                # print(preds)\n                offsets = np.concatenate(offsets, axis = 0)\n                seq_ids = np.concatenate(seq_ids, axis = 0)\n                lbls = np.concatenate(lbls, axis = 0)\n                location_preds= get_location_predictions(preds, offsets, seq_ids)\n                # print(offsets.shape)\n                # df.loc[df['fold'] == fold, 'location_prediction'] = written_predictions\n                index = df[df['fold'] == fold].index\n                df.loc[index,'location_prediction'] = pd.Series(location_preds, index=index)\n                df.loc[index,'token_proba'] = pd.Series([list(preds[i]) for i in range(preds.shape[0])], index=index)\n                df.loc[index,'token_offsets'] = pd.Series([list(offsets[i]) for i in range(offsets.shape[0])], index=index)\n                score = calculate_char_CV(location_preds, offsets, seq_ids, lbls)\n                # if epoch == 1:\n                #     return location_preds\n                print(score)\n    print(history)\n    return()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:56:33.862837Z","iopub.status.idle":"2022-04-09T16:56:33.863191Z","shell.execute_reply.started":"2022-04-09T16:56:33.863002Z","shell.execute_reply":"2022-04-09T16:56:33.863021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_loop()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:56:33.864922Z","iopub.status.idle":"2022-04-09T16:56:33.86531Z","shell.execute_reply.started":"2022-04-09T16:56:33.865106Z","shell.execute_reply":"2022-04-09T16:56:33.865133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_pickle(\"df_pred_medical.pkl\")","metadata":{},"execution_count":null,"outputs":[]}]}