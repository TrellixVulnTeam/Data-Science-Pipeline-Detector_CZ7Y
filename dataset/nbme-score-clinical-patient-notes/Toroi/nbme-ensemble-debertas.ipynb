{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Ensemble of the three public Deberta notebooks. Scores 0.884 on the leaderboard (scoring takes 5+ hours).\n\nPlease upvote the original notebooks:\n\n**Deberta v3 large**\n\n[inference](https://www.kaggle.com/code/lunapandachan/nbme-thanh-s-infer-add-test)\n\n[original inference](https://www.kaggle.com/code/thanhns/deberta-v3-large-0-883-lb)\n\n[model](https://www.kaggle.com/datasets/thanhns/deberta-v3-large-5-folds-public)\n\n**Deberta v1 large**\n\n[inference](https://www.kaggle.com/code/manojprabhaakr/nbme-deberta-large-baseline-inference)\n\n[model](https://www.kaggle.com/datasets/manojprabhaakr/debertalarge)\n\n**Deberta v1 base**\n\n[train](https://www.kaggle.com/code/yasufuminakama/nbme-deberta-base-baseline-train)\n\n[inference](https://www.kaggle.com/code/yasufuminakama/nbme-deberta-base-baseline-inference)","metadata":{}},{"cell_type":"code","source":"DEBUG = False","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:06.640438Z","iopub.execute_input":"2022-04-30T02:15:06.64073Z","iopub.status.idle":"2022-04-30T02:15:06.645741Z","shell.execute_reply.started":"2022-04-30T02:15:06.6407Z","shell.execute_reply":"2022-04-30T02:15:06.644089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Weights","metadata":{"execution":{"iopub.status.busy":"2022-04-01T01:01:55.256298Z","iopub.execute_input":"2022-04-01T01:01:55.256585Z","iopub.status.idle":"2022-04-01T01:01:55.275393Z","shell.execute_reply.started":"2022-04-01T01:01:55.256514Z","shell.execute_reply":"2022-04-01T01:01:55.274772Z"}}},{"cell_type":"code","source":"w1 = 0.2     # Deberta v1 large\nw2 = 0.2     # Deberta v1 base\nw3 = 0.2     # Deberta v3 large\nw4 = 0.2     # Deberta v3 psuedo 2x\nw5 = 0.2     # Roberta large","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:06.844486Z","iopub.execute_input":"2022-04-30T02:15:06.845387Z","iopub.status.idle":"2022-04-30T02:15:06.849586Z","shell.execute_reply.started":"2022-04-30T02:15:06.845338Z","shell.execute_reply":"2022-04-30T02:15:06.848795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    \n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-30T02:15:06.980319Z","iopub.execute_input":"2022-04-30T02:15:06.980872Z","iopub.status.idle":"2022-04-30T02:15:06.993401Z","shell.execute_reply.started":"2022-04-30T02:15:06.980838Z","shell.execute_reply":"2022-04-30T02:15:06.992703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport ast\nimport sys\nimport copy\nimport json\nimport math\nimport string\nimport pickle\nimport random\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport re\nimport ast\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:07.121779Z","iopub.execute_input":"2022-04-30T02:15:07.1223Z","iopub.status.idle":"2022-04-30T02:15:07.135415Z","shell.execute_reply.started":"2022-04-30T02:15:07.122264Z","shell.execute_reply":"2022-04-30T02:15:07.134464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Seed","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed=42):\n    '''\n    Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.\n    '''\n    random.seed(seed)\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        # When running on the CuDNN backend, two further options must be set\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:07.258341Z","iopub.execute_input":"2022-04-30T02:15:07.258805Z","iopub.status.idle":"2022-04-30T02:15:07.266302Z","shell.execute_reply.started":"2022-04-30T02:15:07.258772Z","shell.execute_reply":"2022-04-30T02:15:07.265472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions for scoring","metadata":{}},{"cell_type":"code","source":"def micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    \n    return f1_score(truths, preds)\n\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n        \n    return binary\n\n\ndef span_micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n        \n    return micro_f1(bin_preds, bin_truths)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:07.390264Z","iopub.execute_input":"2022-04-30T02:15:07.390753Z","iopub.status.idle":"2022-04-30T02:15:07.404944Z","shell.execute_reply.started":"2022-04-30T02:15:07.390721Z","shell.execute_reply":"2022-04-30T02:15:07.403939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_labels_for_scoring(df):\n    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n        if len(location_list) > 0:\n            location = location_list[0]\n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n        \n    return truths\n\n\ndef get_char_probs(texts, predictions, tokenizer):\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n            \n    return results\n\n\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n        \n    return results\n\n\ndef get_predictions(results):\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n        \n    return predictions\n\n\ndef get_score(y_true, y_pred):\n    return span_micro_f1(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:07.522122Z","iopub.execute_input":"2022-04-30T02:15:07.522326Z","iopub.status.idle":"2022-04-30T02:15:07.538376Z","shell.execute_reply.started":"2022-04-30T02:15:07.522301Z","shell.execute_reply":"2022-04-30T02:15:07.537284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"main_dir=\"../input/nbme-score-clinical-patient-notes/\"\n\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\n\nif DEBUG:\n    test = pd.read_csv(main_dir+'train.csv')\nelse:\n    test = pd.read_csv(main_dir+'test.csv')\nsubmission = pd.read_csv(main_dir+'sample_submission.csv')\nfeatures = pd.read_csv(main_dir+'features.csv')\npatient_notes = pd.read_csv(main_dir+'patient_notes.csv')\n\nfeatures = preprocess_features(features)\n\nprint(f\"test.shape: {test.shape}\")\ndisplay(test.head())\nprint(f\"features.shape: {features.shape}\")\ndisplay(features.head())\nprint(f\"patient_notes.shape: {patient_notes.shape}\")\ndisplay(patient_notes.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:07.682043Z","iopub.execute_input":"2022-04-30T02:15:07.68242Z","iopub.status.idle":"2022-04-30T02:15:08.008332Z","shell.execute_reply.started":"2022-04-30T02:15:07.682388Z","shell.execute_reply":"2022-04-30T02:15:08.007613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.merge(features, on=['feature_num', 'case_num'], how='left')\ntest = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:08.009829Z","iopub.execute_input":"2022-04-30T02:15:08.010529Z","iopub.status.idle":"2022-04-30T02:15:08.032789Z","shell.execute_reply.started":"2022-04-30T02:15:08.01049Z","shell.execute_reply":"2022-04-30T02:15:08.032099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deberta v3 large","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=\"../input/deberta-v3-large-5-folds-public/\"\n    config_path=path+'config.pth'\n    model=\"deberta-v3-large\"\n    batch_size=32\n    fc_dropout=0.2\n    max_len=354\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:08.03394Z","iopub.execute_input":"2022-04-30T02:15:08.03433Z","iopub.status.idle":"2022-04-30T02:15:08.041404Z","shell.execute_reply.started":"2022-04-30T02:15:08.034293Z","shell.execute_reply":"2022-04-30T02:15:08.040702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n\ntokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\nCFG.tokenizer = tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:08.070207Z","iopub.execute_input":"2022-04-30T02:15:08.070398Z","iopub.status.idle":"2022-04-30T02:15:09.073838Z","shell.execute_reply.started":"2022-04-30T02:15:08.070374Z","shell.execute_reply":"2022-04-30T02:15:09.073119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def prepare_input(cfg, text, feature_text):\n#     inputs = cfg.tokenizer(text, feature_text, \n#                            add_special_tokens=True,\n#                            max_length=CFG.max_len,\n#                            padding=\"max_length\",\n#                            return_offsets_mapping=False)\n#     for k, v in inputs.items():\n#         inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n#     return inputs\n\n\n# class TestDataset(Dataset):\n#     def __init__(self, cfg, df):\n#         self.cfg = cfg\n#         self.feature_texts = df['feature_text'].values\n#         self.pn_historys = df['pn_history'].values\n\n#     def __len__(self):\n#         return len(self.feature_texts)\n\n#     def __getitem__(self, item):\n#         inputs = prepare_input(self.cfg, \n#                                self.pn_historys[item], \n#                                self.feature_texts[item])\n        \n#         return inputs\n# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input_fast(cfg, text, feature_text, batch_max_len):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           max_length=batch_max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDatasetFast(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n        self.batch_max_len = df['batch_max_length'].values\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input_fast(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item],\n                               self.batch_max_len[item],\n                              )\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:09.075366Z","iopub.execute_input":"2022-04-30T02:15:09.075786Z","iopub.status.idle":"2022-04-30T02:15:09.08564Z","shell.execute_reply.started":"2022-04-30T02:15:09.075742Z","shell.execute_reply":"2022-04-30T02:15:09.084794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ScoringModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        \n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        \n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        \n        return output\n    \n# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:09.087058Z","iopub.execute_input":"2022-04-30T02:15:09.087428Z","iopub.status.idle":"2022-04-30T02:15:09.109274Z","shell.execute_reply.started":"2022-04-30T02:15:09.087391Z","shell.execute_reply":"2022-04-30T02:15:09.10852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # ====================================================\n# # inference\n# # ====================================================\n# def inference_fn(test_loader, model, device):\n#     preds = []\n#     model.eval()\n#     model.to(device)\n#     tk0 = tqdm(test_loader, total=len(test_loader))\n#     for inputs in tk0:\n#         for k, v in inputs.items():\n#             inputs[k] = v.to(device)\n#         with torch.no_grad():\n#             y_preds = model(inputs)\n#         preds.append(y_preds.sigmoid().to('cpu').numpy())\n#     predictions = np.concatenate(preds)\n#     return predictions\n\n# ====================================================\n# inference\n# ====================================================\ndef inference_fn_fast(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n    # for inputs in test_loader:\n        bs = len(inputs['input_ids'])\n        pred_w_pad = np.zeros((bs, CFG.max_len, 1))\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        y_preds = y_preds.sigmoid().to('cpu').numpy()\n        pred_w_pad[:, :y_preds.shape[1]] = y_preds\n        preds.append(pred_w_pad)\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:09.112565Z","iopub.execute_input":"2022-04-30T02:15:09.112975Z","iopub.status.idle":"2022-04-30T02:15:09.122598Z","shell.execute_reply.started":"2022-04-30T02:15:09.112931Z","shell.execute_reply":"2022-04-30T02:15:09.121929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Reduce Padding Inference ######\n\n# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:09.124129Z","iopub.execute_input":"2022-04-30T02:15:09.124642Z","iopub.status.idle":"2022-04-30T02:15:09.17792Z","shell.execute_reply.started":"2022-04-30T02:15:09.124604Z","shell.execute_reply":"2022-04-30T02:15:09.177091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dataset = TestDataset(CFG, test)\n# test_loader = DataLoader(test_dataset,\n#                          batch_size=CFG.batch_size,\n#                          shuffle=False,\n#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = ScoringModel(CFG, config_path=CFG.config_path, pretrained=False)\n    \n#     state = torch.load(CFG.path+f\"{CFG.model.split('/')[1]}_fold{fold}_best.pth\",\n#                            map_location=torch.device('cpu'))\n       \n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs\n#     gc.collect()\n#     torch.cuda.empty_cache()\n    \n# predictions_v3_l = np.mean(predictions, axis=0)\n\ntest_dataset = TestDatasetFast(CFG, sort_df)\ntest_loader = DataLoader(test_dataset,\n                      batch_size=CFG.batch_size,\n                      shuffle=False,\n                      num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn_fast(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    ## data re-sort ## \n    prediction = prediction[np.argsort(length_sorted_idx)]\n\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_v3_l = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:15:09.179364Z","iopub.execute_input":"2022-04-30T02:15:09.179634Z","iopub.status.idle":"2022-04-30T02:17:34.910561Z","shell.execute_reply.started":"2022-04-30T02:15:09.179597Z","shell.execute_reply":"2022-04-30T02:17:34.909833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deberta large","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    path=\"../input/debertalarge/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-large\"\n    batch_size=24\n    fc_dropout=0.2\n    max_len=466\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:17:34.914639Z","iopub.execute_input":"2022-04-30T02:17:34.916445Z","iopub.status.idle":"2022-04-30T02:17:34.923513Z","shell.execute_reply.started":"2022-04-30T02:17:34.9164Z","shell.execute_reply":"2022-04-30T02:17:34.922864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:17:34.927356Z","iopub.execute_input":"2022-04-30T02:17:34.929401Z","iopub.status.idle":"2022-04-30T02:17:35.072371Z","shell.execute_reply.started":"2022-04-30T02:17:34.929364Z","shell.execute_reply":"2022-04-30T02:17:35.071611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:17:35.073699Z","iopub.execute_input":"2022-04-30T02:17:35.073944Z","iopub.status.idle":"2022-04-30T02:17:35.085897Z","shell.execute_reply.started":"2022-04-30T02:17:35.073912Z","shell.execute_reply":"2022-04-30T02:17:35.085006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Reduce Padding Inference ######\n\n# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:17:35.08951Z","iopub.execute_input":"2022-04-30T02:17:35.090008Z","iopub.status.idle":"2022-04-30T02:17:35.144578Z","shell.execute_reply.started":"2022-04-30T02:17:35.089948Z","shell.execute_reply":"2022-04-30T02:17:35.143877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dataset = TestDataset(CFG, test)\n# test_loader = DataLoader(test_dataset,\n#                          batch_size=CFG.batch_size,\n#                          shuffle=False,\n#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = ScoringModel(CFG, config_path=CFG.config_path, pretrained=False)\n    \n#     state = torch.load(CFG.path+f\"{CFG.model.split('/')[1]}_fold{fold}_best.pth\",\n#                            map_location=torch.device('cpu'))\n       \n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs\n#     gc.collect()\n#     torch.cuda.empty_cache()\n    \n# predictions_v3_l = np.mean(predictions, axis=0)\n\ntest_dataset = TestDatasetFast(CFG, sort_df)\ntest_loader = DataLoader(test_dataset,\n                      batch_size=CFG.batch_size,\n                      shuffle=False,\n                      num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn_fast(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    ## data re-sort ## \n    prediction = prediction[np.argsort(length_sorted_idx)]\n\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_v1_l = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:17:35.145763Z","iopub.execute_input":"2022-04-30T02:17:35.146385Z","iopub.status.idle":"2022-04-30T02:19:21.121242Z","shell.execute_reply.started":"2022-04-30T02:17:35.146341Z","shell.execute_reply":"2022-04-30T02:19:21.120487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deberta base","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    path=\"../input/nbme-deberta-base-baseline-train/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-base\"\n    batch_size=24\n    fc_dropout=0.2\n    max_len=466\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:19:21.125302Z","iopub.execute_input":"2022-04-30T02:19:21.1271Z","iopub.status.idle":"2022-04-30T02:19:21.133746Z","shell.execute_reply.started":"2022-04-30T02:19:21.127053Z","shell.execute_reply":"2022-04-30T02:19:21.133146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:19:21.137632Z","iopub.execute_input":"2022-04-30T02:19:21.139413Z","iopub.status.idle":"2022-04-30T02:19:21.278625Z","shell.execute_reply.started":"2022-04-30T02:19:21.139376Z","shell.execute_reply":"2022-04-30T02:19:21.277863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Reduce Padding Inference ######\n\n# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:19:21.280104Z","iopub.execute_input":"2022-04-30T02:19:21.280342Z","iopub.status.idle":"2022-04-30T02:19:21.329238Z","shell.execute_reply.started":"2022-04-30T02:19:21.280309Z","shell.execute_reply":"2022-04-30T02:19:21.328451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dataset = TestDataset(CFG, test)\n# test_loader = DataLoader(test_dataset,\n#                          batch_size=CFG.batch_size,\n#                          shuffle=False,\n#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = ScoringModel(CFG, config_path=CFG.config_path, pretrained=False)\n    \n#     state = torch.load(CFG.path+f\"{CFG.model.split('/')[1]}_fold{fold}_best.pth\",\n#                            map_location=torch.device('cpu'))\n       \n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs\n#     gc.collect()\n#     torch.cuda.empty_cache()\n    \n# predictions_v3_l = np.mean(predictions, axis=0)\n\ntest_dataset = TestDatasetFast(CFG, sort_df)\ntest_loader = DataLoader(test_dataset,\n                      batch_size=CFG.batch_size,\n                      shuffle=False,\n                      num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn_fast(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    ## data re-sort ## \n    prediction = prediction[np.argsort(length_sorted_idx)]\n\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_v1_b = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:19:21.330776Z","iopub.execute_input":"2022-04-30T02:19:21.331251Z","iopub.status.idle":"2022-04-30T02:20:29.083678Z","shell.execute_reply.started":"2022-04-30T02:19:21.331215Z","shell.execute_reply":"2022-04-30T02:20:29.082913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deberta v3 large maxgrad 5000","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=\"../input/debertav3largemaxgrad5000/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=32\n    fc_dropout=0.2\n    max_len=354\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:20:29.08779Z","iopub.execute_input":"2022-04-30T02:20:29.089979Z","iopub.status.idle":"2022-04-30T02:20:29.097523Z","shell.execute_reply.started":"2022-04-30T02:20:29.089922Z","shell.execute_reply":"2022-04-30T02:20:29.096884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n\ntokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\nCFG.tokenizer = tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:20:29.101718Z","iopub.execute_input":"2022-04-30T02:20:29.104472Z","iopub.status.idle":"2022-04-30T02:20:30.139928Z","shell.execute_reply.started":"2022-04-30T02:20:29.10437Z","shell.execute_reply":"2022-04-30T02:20:30.139111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Reduce Padding Inference ######\n\n# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:20:30.14135Z","iopub.execute_input":"2022-04-30T02:20:30.14164Z","iopub.status.idle":"2022-04-30T02:20:30.193391Z","shell.execute_reply.started":"2022-04-30T02:20:30.141601Z","shell.execute_reply":"2022-04-30T02:20:30.192651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dataset = TestDataset(CFG, test)\n# test_loader = DataLoader(test_dataset,\n#                          batch_size=CFG.batch_size,\n#                          shuffle=False,\n#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = ScoringModel(CFG, config_path=CFG.config_path, pretrained=False)\n    \n#     state = torch.load(CFG.path+f\"{CFG.model.split('/')[1]}_fold{fold}_best.pth\",\n#                            map_location=torch.device('cpu'))\n       \n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs\n#     gc.collect()\n#     torch.cuda.empty_cache()\n    \n# predictions_v3_l = np.mean(predictions, axis=0)\n\ntest_dataset = TestDatasetFast(CFG, sort_df)\ntest_loader = DataLoader(test_dataset,\n                      batch_size=CFG.batch_size,\n                      shuffle=False,\n                      num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn_fast(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    ## data re-sort ## \n    prediction = prediction[np.argsort(length_sorted_idx)]\n\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_v3_l_mg5000 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:20:30.195092Z","iopub.execute_input":"2022-04-30T02:20:30.195593Z","iopub.status.idle":"2022-04-30T02:22:58.510858Z","shell.execute_reply.started":"2022-04-30T02:20:30.195553Z","shell.execute_reply":"2022-04-30T02:22:58.510155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deberta v3 psuedo label 2x","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=\"../input/debertav3largepsuedolabel2x/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=32\n    fc_dropout=0.2\n    max_len=354\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:22:58.514953Z","iopub.execute_input":"2022-04-30T02:22:58.516791Z","iopub.status.idle":"2022-04-30T02:22:58.523815Z","shell.execute_reply.started":"2022-04-30T02:22:58.516744Z","shell.execute_reply":"2022-04-30T02:22:58.52316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n\ntokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\nCFG.tokenizer = tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:22:58.527414Z","iopub.execute_input":"2022-04-30T02:22:58.529402Z","iopub.status.idle":"2022-04-30T02:22:59.631117Z","shell.execute_reply.started":"2022-04-30T02:22:58.529364Z","shell.execute_reply":"2022-04-30T02:22:59.630368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Reduce Padding Inference ######\n\n# sort by token num\ninput_lengths = []\ntk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\nfor text, feature_text in tk0:\n    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n    input_lengths.append(length)\ntest['input_lengths'] = input_lengths\nlength_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# sort dataframe\nsort_df = test.iloc[length_sorted_idx]\n\n# calc max_len per batch\nsorted_input_length = sort_df['input_lengths'].values\nbatch_max_length = np.zeros_like(sorted_input_length)\nbs = CFG.batch_size\nfor i in range((len(sorted_input_length)//bs)+1):\n    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \nsort_df['batch_max_length'] = batch_max_length","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:22:59.632251Z","iopub.execute_input":"2022-04-30T02:22:59.633069Z","iopub.status.idle":"2022-04-30T02:22:59.679913Z","shell.execute_reply.started":"2022-04-30T02:22:59.633037Z","shell.execute_reply":"2022-04-30T02:22:59.67927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dataset = TestDataset(CFG, test)\n# test_loader = DataLoader(test_dataset,\n#                          batch_size=CFG.batch_size,\n#                          shuffle=False,\n#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = ScoringModel(CFG, config_path=CFG.config_path, pretrained=False)\n    \n#     state = torch.load(CFG.path+f\"{CFG.model.split('/')[1]}_fold{fold}_best.pth\",\n#                            map_location=torch.device('cpu'))\n       \n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs\n#     gc.collect()\n#     torch.cuda.empty_cache()\n    \n# predictions_v3_l = np.mean(predictions, axis=0)\n\ntest_dataset = TestDatasetFast(CFG, sort_df)\ntest_loader = DataLoader(test_dataset,\n                      batch_size=CFG.batch_size,\n                      shuffle=False,\n                      num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn_fast(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    ## data re-sort ## \n    prediction = prediction[np.argsort(length_sorted_idx)]\n\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_v3_l_psuedo2x = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:22:59.681236Z","iopub.execute_input":"2022-04-30T02:22:59.681669Z","iopub.status.idle":"2022-04-30T02:25:29.871795Z","shell.execute_reply.started":"2022-04-30T02:22:59.681633Z","shell.execute_reply":"2022-04-30T02:25:29.871059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Roberta Strikes Back!","metadata":{}},{"cell_type":"code","source":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef process_feature_text(text):\n    text = re.sub('I-year', '1-year', text)\n    text = re.sub('-OR-', \" or \", text)\n    text = re.sub('-', ' ', text)\n    return text\n\n\ndef clean_spaces(txt):\n    txt = re.sub('\\n', ' ', txt)\n    txt = re.sub('\\t', ' ', txt)\n    txt = re.sub('\\r', ' ', txt)\n#     txt = re.sub(r'\\s+', ' ', txt)\n    return txt\n\n\ndef load_and_prepare_test(root=\"\"):\n    patient_notes = pd.read_csv(root + \"patient_notes.csv\")\n    features = pd.read_csv(root + \"features.csv\")\n    if DEBUG:\n        df = pd.read_csv(root + \"train.csv\")\n    else:\n        df = pd.read_csv(root + \"test.csv\")\n\n    df = df.merge(features, how=\"left\", on=[\"case_num\", \"feature_num\"])\n    df = df.merge(patient_notes, how=\"left\", on=['case_num', 'pn_num'])\n\n    df['pn_history'] = df['pn_history'].apply(lambda x: x.strip())\n    df['feature_text'] = df['feature_text'].apply(process_feature_text)\n\n    df['feature_text'] = df['feature_text'].apply(clean_spaces)\n    df['clean_text'] = df['pn_history'].apply(clean_spaces)\n\n    df['target'] = \"\"\n    return df\n\nimport itertools\n\n\ndef token_pred_to_char_pred(token_pred, offsets):\n    char_pred = np.zeros((np.max(offsets), token_pred.shape[1]))\n    for i in range(len(token_pred)):\n        s, e = int(offsets[i][0]), int(offsets[i][1])  # start, end\n        char_pred[s:e] = token_pred[i]\n\n        if token_pred.shape[1] == 3:  # following characters cannot be tagged as start\n            s += 1\n            char_pred[s: e, 1], char_pred[s: e, 2] = (\n                np.max(char_pred[s: e, 1:], 1),\n                np.min(char_pred[s: e, 1:], 1),\n            )\n\n    return char_pred\n\n\ndef labels_to_sub(labels):\n    all_spans = []\n    for label in labels:\n        indices = np.where(label > 0)[0]\n        indices_grouped = [\n            list(g) for _, g in itertools.groupby(\n                indices, key=lambda n, c=itertools.count(): n - next(c)\n            )\n        ]\n\n        spans = [f\"{min(r)} {max(r) + 1}\" for r in indices_grouped]\n        all_spans.append(\";\".join(spans))\n    return all_spans\n\n\ndef char_target_to_span(char_target):\n    spans = []\n    start, end = 0, 0\n    for i in range(len(char_target)):\n        if char_target[i] == 1 and char_target[i - 1] == 0:\n            if end:\n                spans.append([start, end])\n            start = i\n            end = i + 1\n        elif char_target[i] == 1:\n            end = i + 1\n        else:\n            if end:\n                spans.append([start, end])\n            start, end = 0, 0\n    return spans\n\nimport numpy as np\nfrom transformers import AutoTokenizer\n\n\ndef get_tokenizer(name, precompute=False, df=None, folder=None):\n    if folder is None:\n        tokenizer = AutoTokenizer.from_pretrained(name)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(folder)\n\n    tokenizer.name = name\n    tokenizer.special_tokens = {\n        \"sep\": tokenizer.sep_token_id,\n        \"cls\": tokenizer.cls_token_id,\n        \"pad\": tokenizer.pad_token_id,\n    }\n\n    if precompute:\n        tokenizer.precomputed = precompute_tokens(df, tokenizer)\n    else:\n        tokenizer.precomputed = None\n\n    return tokenizer\n\n\ndef precompute_tokens(df, tokenizer):\n    feature_texts = df[\"feature_text\"].unique()\n\n    ids = {}\n    offsets = {}\n\n    for feature_text in feature_texts:\n        encoding = tokenizer(\n            feature_text,\n            return_token_type_ids=True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        ids[feature_text] = encoding[\"input_ids\"]\n        offsets[feature_text] = encoding[\"offset_mapping\"]\n\n    texts = df[\"clean_text\"].unique()\n\n    for text in texts:\n        encoding = tokenizer(\n            text,\n            return_token_type_ids=True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        ids[text] = encoding[\"input_ids\"]\n        offsets[text] = encoding[\"offset_mapping\"]\n\n    return {\"ids\": ids, \"offsets\": offsets}\n\n\ndef encodings_from_precomputed(feature_text, text, precomputed, tokenizer, max_len=300):\n    tokens = tokenizer.special_tokens\n\n    # Input ids\n    if \"roberta\" in tokenizer.name:\n        qa_sep = [tokens[\"sep\"], tokens[\"sep\"]]\n    else:\n        qa_sep = [tokens[\"sep\"]]\n\n    input_ids = [tokens[\"cls\"]] + precomputed[\"ids\"][feature_text] + qa_sep\n    n_question_tokens = len(input_ids)\n\n    input_ids += precomputed[\"ids\"][text]\n    input_ids = input_ids[: max_len - 1] + [tokens[\"sep\"]]\n\n    # Token type ids\n    if \"roberta\" not in tokenizer.name:\n        token_type_ids = np.ones(len(input_ids))\n        token_type_ids[:n_question_tokens] = 0\n        token_type_ids = token_type_ids.tolist()\n    else:\n        token_type_ids = [0] * len(input_ids)\n\n    # Offsets\n    offsets = [(0, 0)] * n_question_tokens + precomputed[\"offsets\"][text]\n    offsets = offsets[: max_len - 1] + [(0, 0)]\n\n    # Padding\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([tokens[\"pad\"]] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        offsets = offsets + ([(0, 0)] * padding_length)\n\n    encoding = {\n        \"input_ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"offset_mapping\": offsets,\n    }\n\n    return encoding\n\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\n\n\nclass PatientNoteDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n\n        self.texts = df['clean_text'].values\n        self.feature_text = df['feature_text'].values\n        self.char_targets = df['target'].values.tolist()\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        feature_text = self.feature_text[idx]\n        char_target = self.char_targets[idx]\n\n        # Tokenize\n        if self.tokenizer.precomputed is None:\n            encoding = self.tokenizer(\n                feature_text,\n                text,\n                return_token_type_ids=True,\n                return_offsets_mapping=True,\n                return_attention_mask=False,\n                truncation=\"only_second\",\n                max_length=self.max_len,\n                padding='max_length',\n            )\n            raise NotImplementedError(\"fix issues with question offsets\")\n        else:\n            encoding = encodings_from_precomputed(\n                feature_text,\n                text,\n                self.tokenizer.precomputed,\n                self.tokenizer,\n                max_len=self.max_len\n            )\n\n        return {\n            \"ids\": torch.tensor(encoding[\"input_ids\"], dtype=torch.long),\n            \"token_type_ids\": torch.tensor(encoding[\"token_type_ids\"], dtype=torch.long),\n            \"target\": torch.tensor([0], dtype=torch.float),\n            \"offsets\": np.array(encoding[\"offset_mapping\"]),\n            \"text\": text,\n        }\n\n    def __len__(self):\n        return len(self.texts)\n    \nimport torch\nimport transformers\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModel\n\n\nclass NERTransformer(nn.Module):\n    def __init__(\n        self,\n        model,\n        num_classes=1,\n        config_file=None,\n        pretrained=True,\n    ):\n        super().__init__()\n        self.name = model\n        self.pad_idx = 1 if \"roberta\" in self.name else 0\n\n        transformers.logging.set_verbosity_error()\n\n        if config_file is None:\n            config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n        else:\n            config = torch.load(config_file)\n\n        if pretrained:\n            self.transformer = AutoModel.from_pretrained(model, config=config)\n        else:\n            self.transformer = AutoModel.from_config(config)\n\n        self.nb_features = config.hidden_size\n\n#         self.cnn = nn.Identity()\n        self.logits = nn.Linear(self.nb_features, num_classes)\n\n    def forward(self, tokens, token_type_ids):\n        \"\"\"\n        Usual torch forward function\n\n        Arguments:\n            tokens {torch tensor} -- Sentence tokens\n            token_type_ids {torch tensor} -- Sentence tokens ids\n        \"\"\"\n        hidden_states = self.transformer(\n            tokens,\n            attention_mask=(tokens != self.pad_idx).long(),\n            token_type_ids=token_type_ids,\n        )[-1]\n\n        features = hidden_states[-1]\n\n        logits = self.logits(features)\n\n        return logits\n\ndef load_model_weights(model, filename, verbose=1, cp_folder=\"\", strict=True):\n    \"\"\"\n    Loads the weights of a PyTorch model. The exception handles cpu/gpu incompatibilities.\n\n    Args:\n        model (torch model): Model to load the weights to.\n        filename (str): Name of the checkpoint.\n        verbose (int, optional): Whether to display infos. Defaults to 1.\n        cp_folder (str, optional): Folder to load from. Defaults to \"\".\n        strict (bool, optional): Whether to allow missing/additional keys. Defaults to False.\n\n    Returns:\n        torch model: Model with loaded weights.\n    \"\"\"\n    if verbose:\n        print(f\"\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n\")\n\n    try:\n        model.load_state_dict(\n            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n            strict=strict,\n        )\n    except RuntimeError:\n        model.encoder.fc = torch.nn.Linear(model.nb_ft, 1)\n        model.load_state_dict(\n            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n            strict=strict,\n        )\n\n    return model\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\n\n\ndef predict(model, dataset, data_config, activation=\"softmax\"):\n    \"\"\"\n    Usual predict torch function\n    \"\"\"\n    model.eval()\n\n    loader = DataLoader(\n        dataset,\n        batch_size=data_config['val_bs'],\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n\n    preds = []\n    with torch.no_grad():\n        for data in tqdm(loader):\n            ids, token_type_ids = data[\"ids\"], data[\"token_type_ids\"]\n\n            y_pred = model(ids.cuda(), token_type_ids.cuda())\n\n            if activation == \"sigmoid\":\n                y_pred = y_pred.sigmoid()\n            elif activation == \"softmax\":\n                y_pred = y_pred.softmax(-1)\n\n            preds += [\n                token_pred_to_char_pred(y, offsets) for y, offsets\n                in zip(y_pred.detach().cpu().numpy(), data[\"offsets\"].numpy())\n            ]\n\n    return preds\n\ndef inference_test(df, exp_folder, config, cfg_folder=None):\n    preds = []\n\n    if cfg_folder is not None:\n        model_config_file = cfg_folder + config.name.split('/')[-1] + \"/config.pth\"\n        tokenizer_folder = cfg_folder + config.name.split('/')[-1] + \"/tokenizers/\"\n    else:\n        model_config_file, tokenizer_folder = None, None\n\n    tokenizer = get_tokenizer(\n        config.name, precompute=config.precompute_tokens, df=df, folder=tokenizer_folder\n    )\n\n    dataset = PatientNoteDataset(\n        df,\n        tokenizer,\n        max_len=config.max_len,\n    )\n\n    model = NERTransformer(\n        config.name,\n        num_classes=config.num_classes,\n        config_file=model_config_file,\n        pretrained=False\n    ).cuda()\n    model.zero_grad()\n\n    weights = sorted(glob.glob(exp_folder + \"*.pt\"))\n    for weight in weights:\n        model = load_model_weights(model, weight)\n\n        pred = predict(\n            model,\n            dataset,\n            data_config=config.data_config,\n            activation=config.loss_config[\"activation\"]\n        )\n        preds.append(pred)\n\n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:29.875952Z","iopub.execute_input":"2022-04-30T02:25:29.876539Z","iopub.status.idle":"2022-04-30T02:25:29.967951Z","shell.execute_reply.started":"2022-04-30T02:25:29.876497Z","shell.execute_reply":"2022-04-30T02:25:29.967257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    # Architecture\n    name = \"roberta-large\"\n    num_classes = 1\n\n    # Texts\n    max_len = 310\n    precompute_tokens = True\n\n    # Training    \n    loss_config = {\n        \"activation\": \"sigmoid\",\n    }\n\n    data_config = {\n        \"val_bs\": 16 if \"large\" in name else 32,\n        \"pad_token\": 1 if \"roberta\" in name else 0,\n    }\n\n    verbose = 1","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:29.970946Z","iopub.execute_input":"2022-04-30T02:25:29.971187Z","iopub.status.idle":"2022-04-30T02:25:29.976058Z","shell.execute_reply.started":"2022-04-30T02:25:29.971162Z","shell.execute_reply":"2022-04-30T02:25:29.975126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = \"../input/nbme-score-clinical-patient-notes/\"\nOUT_PATH = \"../input/nbme-roberta-large/\"\nWEIGHTS_FOLDER = \"../input/nbme-roberta-large/\"\n\nNUM_WORKERS = 4","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:29.977543Z","iopub.execute_input":"2022-04-30T02:25:29.978032Z","iopub.status.idle":"2022-04-30T02:25:29.984431Z","shell.execute_reply.started":"2022-04-30T02:25:29.977988Z","shell.execute_reply":"2022-04-30T02:25:29.98368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = load_and_prepare_test(root=DATA_PATH)\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:29.985922Z","iopub.execute_input":"2022-04-30T02:25:29.986485Z","iopub.status.idle":"2022-04-30T02:25:30.348585Z","shell.execute_reply.started":"2022-04-30T02:25:29.986399Z","shell.execute_reply":"2022-04-30T02:25:30.34782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = inference_test(\n    df_test,\n    WEIGHTS_FOLDER,\n    Config,\n    cfg_folder=OUT_PATH\n)[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:30.352488Z","iopub.execute_input":"2022-04-30T02:25:30.352684Z","iopub.status.idle":"2022-04-30T02:25:50.479747Z","shell.execute_reply.started":"2022-04-30T02:25:30.352659Z","shell.execute_reply":"2022-04-30T02:25:50.478917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['preds'] = preds\ndf_test['preds'] = df_test.apply(lambda x: x['preds'][:len(x['clean_text'])], 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:50.481877Z","iopub.execute_input":"2022-04-30T02:25:50.482166Z","iopub.status.idle":"2022-04-30T02:25:50.491091Z","shell.execute_reply.started":"2022-04-30T02:25:50.482124Z","shell.execute_reply":"2022-04-30T02:25:50.489924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def post_process_spaces(target, text):\n    target = np.copy(target)\n\n    if len(text) > len(target):\n        padding = np.zeros(len(text) - len(target))\n        target = np.concatenate([target, padding])\n    else:\n        target = target[:len(text)]\n\n    if text[0] == \" \":\n        target[0] = 0\n    if text[-1] == \" \":\n        target[-1] = 0\n\n    for i in range(1, len(text) - 1):\n        if text[i] == \" \":\n            if target[i] and not target[i - 1]:  # space before\n                target[i] = 0\n\n            if target[i] and not target[i + 1]:  # space after\n                target[i] = 0\n\n            if target[i - 1] and target[i + 1]:\n                target[i] = 1\n\n    return target","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:50.492501Z","iopub.execute_input":"2022-04-30T02:25:50.492898Z","iopub.status.idle":"2022-04-30T02:25:50.50383Z","shell.execute_reply.started":"2022-04-30T02:25:50.492858Z","shell.execute_reply":"2022-04-30T02:25:50.50292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_roberta_l = np.array([k[:, 0] for k in df_test[\"preds\"].values])","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:50.505373Z","iopub.execute_input":"2022-04-30T02:25:50.505668Z","iopub.status.idle":"2022-04-30T02:25:50.513232Z","shell.execute_reply.started":"2022-04-30T02:25:50.505629Z","shell.execute_reply":"2022-04-30T02:25:50.512443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in predictions_roberta_l:\n    if k.shape[0]!=938:\n        print(k.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:50.514565Z","iopub.execute_input":"2022-04-30T02:25:50.514891Z","iopub.status.idle":"2022-04-30T02:25:50.521954Z","shell.execute_reply.started":"2022-04-30T02:25:50.514852Z","shell.execute_reply":"2022-04-30T02:25:50.521114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_v3_l_psuedo2x[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:50.523451Z","iopub.execute_input":"2022-04-30T02:25:50.523799Z","iopub.status.idle":"2022-04-30T02:25:50.532936Z","shell.execute_reply.started":"2022-04-30T02:25:50.523761Z","shell.execute_reply":"2022-04-30T02:25:50.532076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Postprocessing","metadata":{}},{"cell_type":"code","source":"def post_padding_brank(result):\n    if not result == \"\":\n        post_result = []\n        for idx, _result in enumerate(result.split(\";\")):\n            start, end = [int(_r) for _r in _result.split(\" \")]\n            if start == end:\n                # single character\n                start = end - 1\n            post_result.append(\" \".join(list(map(str, [start, end]))))\n\n    else:\n        post_result = result\n\n    return \";\".join(post_result)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:50.534342Z","iopub.execute_input":"2022-04-30T02:25:50.534773Z","iopub.status.idle":"2022-04-30T02:25:50.541884Z","shell.execute_reply.started":"2022-04-30T02:25:50.534734Z","shell.execute_reply":"2022-04-30T02:25:50.54102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combine","metadata":{}},{"cell_type":"code","source":"predictions = []\nfor p1, p2, p3, p4, p5 in zip(predictions_v1_l, predictions_v1_b, predictions_v3_l_mg5000, predictions_v3_l_psuedo2x, predictions_roberta_l):\n    # remove no-meaning character\n    min_len = min(len(p1), len(p2), len(p3), len(p4), len(p5))\n    predictions.append(w1*p1[:min_len] + \\\n                       w2*p2[:min_len] + \\\n                       w3*p3[:min_len] + \\\n                       w4*p4[:min_len] + \\\n                       w5*p5[:min_len])\n\n# for i, (p1, p2) in enumerate(zip(predictions_v3_l_psuedo2x, predictions_roberta_l)):\n#     # remove no-meaning character\n#     min_len = min(len(p1), len(p2))\n#     predictions.append(0.5*p1[:min_len] + 0.5*p2[:min_len])\n\n# for p1 in predictions_roberta_l:\n#     predictions.append(p1)\n\ndf_test[\"preds_ens\"] = predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:50.543373Z","iopub.execute_input":"2022-04-30T02:25:50.543706Z","iopub.status.idle":"2022-04-30T02:25:50.554104Z","shell.execute_reply.started":"2022-04-30T02:25:50.543668Z","shell.execute_reply":"2022-04-30T02:25:50.553412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['preds_ens'] = df_test['preds_ens'].apply(lambda x: (x > 0.5).flatten())","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:50.555266Z","iopub.execute_input":"2022-04-30T02:25:50.555537Z","iopub.status.idle":"2022-04-30T02:25:50.563889Z","shell.execute_reply.started":"2022-04-30T02:25:50.555498Z","shell.execute_reply":"2022-04-30T02:25:50.563122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['preds_ens_pp'] = df_test.apply(lambda x: post_process_spaces(x['preds_ens'], x['clean_text']), 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:50.565226Z","iopub.execute_input":"2022-04-30T02:25:50.565693Z","iopub.status.idle":"2022-04-30T02:25:50.575357Z","shell.execute_reply.started":"2022-04-30T02:25:50.565654Z","shell.execute_reply":"2022-04-30T02:25:50.574519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['location'] = labels_to_sub(df_test['preds_ens_pp'].values)\n\nsub = pd.read_csv(DATA_PATH + 'sample_submission.csv')\n\nsub = sub[['id']].merge(df_test[['id', \"location\"]], how=\"left\", on=\"id\")\n\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:25:50.57693Z","iopub.execute_input":"2022-04-30T02:25:50.577218Z","iopub.status.idle":"2022-04-30T02:25:50.6015Z","shell.execute_reply.started":"2022-04-30T02:25:50.577183Z","shell.execute_reply":"2022-04-30T02:25:50.600877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}