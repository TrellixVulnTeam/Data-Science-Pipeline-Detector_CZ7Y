{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport json\n\nimport collections","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.121917Z","iopub.execute_input":"2022-02-17T17:02:57.122831Z","iopub.status.idle":"2022-02-17T17:02:57.130533Z","shell.execute_reply.started":"2022-02-17T17:02:57.122782Z","shell.execute_reply":"2022-02-17T17:02:57.129165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I've put little effort to understand and figure out methods. Any suggestions are highly appreciated and** **please leave a comment. Upvote if you think it's worth an upvote**\n**Thank You** ","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.132577Z","iopub.execute_input":"2022-02-17T17:02:57.132849Z","iopub.status.idle":"2022-02-17T17:02:57.146357Z","shell.execute_reply.started":"2022-02-17T17:02:57.132818Z","shell.execute_reply":"2022-02-17T17:02:57.145176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_path = '/kaggle/input/nbme-score-clinical-patient-notes/'\nfeatures = pd.read_csv(f_path+'features.csv')\nsample_submission = pd.read_csv(f_path+'sample_submission.csv')\npatient_notes = pd.read_csv(f_path+'patient_notes.csv')\ntrain = pd.read_csv(f_path+'train.csv')\ntest = pd.read_csv(f_path+'test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.147859Z","iopub.execute_input":"2022-02-17T17:02:57.149483Z","iopub.status.idle":"2022-02-17T17:02:57.591215Z","shell.execute_reply.started":"2022-02-17T17:02:57.149443Z","shell.execute_reply":"2022-02-17T17:02:57.590444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploring features.csv\n\nFor a particular Case number we will have number of features, there's a verbal explanation which describes what should be marked under a particular feature.\n\n- No null value is there\n- Number of Features varies to different Cases\n- Total we have 143 features across all cases\n\n\n|Case | Number of Frquencies |\n| --- | --- |\n|0|    13|\n|1|    13|\n|2|    17|\n|3|    16|\n|4|    10|\n|5|    18|\n|6|    12|\n|7|     9|\n|8|    18|\n|9|    17|","metadata":{}},{"cell_type":"code","source":"print(features.shape)\nfeatures.info()\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-17T17:02:57.593714Z","iopub.execute_input":"2022-02-17T17:02:57.594013Z","iopub.status.idle":"2022-02-17T17:02:57.622962Z","shell.execute_reply.started":"2022-02-17T17:02:57.593971Z","shell.execute_reply":"2022-02-17T17:02:57.622093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.624822Z","iopub.execute_input":"2022-02-17T17:02:57.625136Z","iopub.status.idle":"2022-02-17T17:02:57.64368Z","shell.execute_reply.started":"2022-02-17T17:02:57.625093Z","shell.execute_reply":"2022-02-17T17:02:57.64282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.groupby(['case_num'])['feature_num'].agg('count')","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.645135Z","iopub.execute_input":"2022-02-17T17:02:57.645804Z","iopub.status.idle":"2022-02-17T17:02:57.653909Z","shell.execute_reply.started":"2022-02-17T17:02:57.645763Z","shell.execute_reply":"2022-02-17T17:02:57.652808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploring patient_notes.csv\n\nThis the notes taken on each patient. \n- Patient has unique id : pn_num\n- Complete Note is given in pn_history\n- Case is identified by case_num, which is also mentioned in features\n- No Patient is repeated in the list\n- No null values\n- Total 42146 records available\n\n#### example of note : \n17-year-old male, has come to the student health clinic complaining of heart pounding. Mr. Cleveland's mother has given verbal consent for a history, physical examination, and treatment\\\n-began 2-3 months ago,sudden,intermittent for 2 days(lasting 3-4 min),worsening,non-allev/aggrav \\\n-associated with dispnea on exersion and rest,stressed out about school \\\n-reports fe feels like his heart is jumping out of his chest \\\n-ros:denies chest pain,dyaphoresis,wt loss,chills,fever,nausea,vomiting,pedal edeam \\\n-pmh:non,meds :aderol (from a friend),nkda \\\n-fh:father had MI recently,mother has thyroid dz \\\n-sh:non-smoker,mariguana 5-6 months ago,3 beers on the weekend, basketball at school \\\n-sh:no std ","metadata":{}},{"cell_type":"code","source":"print(patient_notes.shape)\npatient_notes.info()\npatient_notes.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.65554Z","iopub.execute_input":"2022-02-17T17:02:57.65642Z","iopub.status.idle":"2022-02-17T17:02:57.689095Z","shell.execute_reply.started":"2022-02-17T17:02:57.656377Z","shell.execute_reply":"2022-02-17T17:02:57.688512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(patient_notes['pn_num'].unique())","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.690186Z","iopub.execute_input":"2022-02-17T17:02:57.690441Z","iopub.status.idle":"2022-02-17T17:02:57.701253Z","shell.execute_reply.started":"2022-02-17T17:02:57.690409Z","shell.execute_reply":"2022-02-17T17:02:57.700381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(patient_notes[patient_notes['pn_num']==0]['pn_history'][0])","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.702563Z","iopub.execute_input":"2022-02-17T17:02:57.7028Z","iopub.status.idle":"2022-02-17T17:02:57.710404Z","shell.execute_reply.started":"2022-02-17T17:02:57.702773Z","shell.execute_reply":"2022-02-17T17:02:57.709581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploring train.csv\n\nTrain CSV is a combination of Feature and Patient Note\n- There's unique ID for each row\n- Case Number, Patient Number and Feature Number is associating with previous dataframes\n- From Patients note (pn_history). features matching with features table for particular case is identified\n- in annotation the key statement is annoted and on location character wise location is given","metadata":{}},{"cell_type":"code","source":"print(train.shape)\ntrain.info()\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.713848Z","iopub.execute_input":"2022-02-17T17:02:57.714251Z","iopub.status.idle":"2022-02-17T17:02:57.741241Z","shell.execute_reply.started":"2022-02-17T17:02:57.714203Z","shell.execute_reply":"2022-02-17T17:02:57.740511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.742452Z","iopub.execute_input":"2022-02-17T17:02:57.742692Z","iopub.status.idle":"2022-02-17T17:02:57.753186Z","shell.execute_reply.started":"2022-02-17T17:02:57.742661Z","shell.execute_reply":"2022-02-17T17:02:57.752469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.754528Z","iopub.execute_input":"2022-02-17T17:02:57.755124Z","iopub.status.idle":"2022-02-17T17:02:57.768819Z","shell.execute_reply.started":"2022-02-17T17:02:57.755092Z","shell.execute_reply":"2022-02-17T17:02:57.768161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's Try to Figure out the requirement Mannually\n\nJust like search engine and tags. We can try to figure out the statement which valid using the frequently used Keys. Here I'm trying to find out most frequently used keys for each feature in each case, which we can later use for further analysis","metadata":{}},{"cell_type":"code","source":"# I'm listing out all annotations of patient no : 16 as per the training data\ntrain[train['pn_num']==16]","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.769958Z","iopub.execute_input":"2022-02-17T17:02:57.77096Z","iopub.status.idle":"2022-02-17T17:02:57.79057Z","shell.execute_reply.started":"2022-02-17T17:02:57.77091Z","shell.execute_reply":"2022-02-17T17:02:57.789876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Also I'm checking the note from which We were able to pick the annotations.\nprint(patient_notes[patient_notes['pn_num']==16]['pn_history'][16])","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.792028Z","iopub.execute_input":"2022-02-17T17:02:57.792261Z","iopub.status.idle":"2022-02-17T17:02:57.798745Z","shell.execute_reply.started":"2022-02-17T17:02:57.792232Z","shell.execute_reply":"2022-02-17T17:02:57.797912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I'm listing out all the features of the particular case patient 16 is having\nfeatures[features['case_num']==0]","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.801274Z","iopub.execute_input":"2022-02-17T17:02:57.801543Z","iopub.status.idle":"2022-02-17T17:02:57.81652Z","shell.execute_reply.started":"2022-02-17T17:02:57.801513Z","shell.execute_reply":"2022-02-17T17:02:57.815877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# expanding feature 0\nfeatures[features['case_num']==0]['feature_text'][0]","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.817745Z","iopub.execute_input":"2022-02-17T17:02:57.818092Z","iopub.status.idle":"2022-02-17T17:02:57.830039Z","shell.execute_reply.started":"2022-02-17T17:02:57.818062Z","shell.execute_reply":"2022-02-17T17:02:57.829399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# expanding feature 5\nfeatures[features['case_num']==0]['feature_text'][5]","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.831441Z","iopub.execute_input":"2022-02-17T17:02:57.831765Z","iopub.status.idle":"2022-02-17T17:02:57.843677Z","shell.execute_reply.started":"2022-02-17T17:02:57.831736Z","shell.execute_reply":"2022-02-17T17:02:57.842762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trying to Solve for one Feature \n\nI'm trying to implement a key word list for 1 particular feature. Once we finish we'll try to create a method and run for all rows in features and store it in a dataframe","metadata":{}},{"cell_type":"code","source":"# Duplicating train df\ndummy_train = train.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.844917Z","iopub.execute_input":"2022-02-17T17:02:57.84557Z","iopub.status.idle":"2022-02-17T17:02:57.853749Z","shell.execute_reply.started":"2022-02-17T17:02:57.84553Z","shell.execute_reply":"2022-02-17T17:02:57.853104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing all unnecessary characters from the data frame\ndummy_train['annotation']  = dummy_train['annotation'].str.replace('[','')\ndummy_train['annotation']  = dummy_train['annotation'].str.replace(']','')\ndummy_train['annotation']  = dummy_train['annotation'].str.replace(\"'\",\"\")\ndummy_train['annotation']  = dummy_train['annotation'].str.replace('\"','')\ndummy_train['annotation']  = dummy_train['annotation'].str.replace(',','')\ndummy_train['annotation']  = dummy_train['annotation'].str.replace('-',' ')\ndummy_train['annotation']  = dummy_train['annotation'].str.replace(':',' ')\n\n# changing all words to lower so we can avoid repetition \ndummy_train['annotation']  = dummy_train['annotation'].str.lower()\ndummy_train","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.854948Z","iopub.execute_input":"2022-02-17T17:02:57.85532Z","iopub.status.idle":"2022-02-17T17:02:57.95985Z","shell.execute_reply.started":"2022-02-17T17:02:57.855267Z","shell.execute_reply":"2022-02-17T17:02:57.959043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taking all unique values for a particular feature for a particular case and storing values to an array\nfeature_list = pd.Series(dummy_train[(train['case_num']==0) & (train['feature_num']==0)]['annotation']).unique()\nprint(type(feature_list))\nprint(feature_list)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-17T17:02:57.961251Z","iopub.execute_input":"2022-02-17T17:02:57.961756Z","iopub.status.idle":"2022-02-17T17:02:57.971661Z","shell.execute_reply.started":"2022-02-17T17:02:57.96172Z","shell.execute_reply":"2022-02-17T17:02:57.970716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From sentence I'm splitting it into words,\nfeature_word_list = ' '.join(feature_list).split()\nprint(type(feature_word_list))\nprint(feature_word_list)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.972852Z","iopub.execute_input":"2022-02-17T17:02:57.97305Z","iopub.status.idle":"2022-02-17T17:02:57.98295Z","shell.execute_reply.started":"2022-02-17T17:02:57.973025Z","shell.execute_reply":"2022-02-17T17:02:57.982185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a frequency list for each word, so we can find out most used word for this particular feature\ncounter=collections.Counter(feature_word_list)\nfeature_word_list = dict(sorted(counter.items(), key=lambda item: item[1],reverse=True))\nprint(feature_word_list)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.985394Z","iopub.execute_input":"2022-02-17T17:02:57.985929Z","iopub.status.idle":"2022-02-17T17:02:57.994128Z","shell.execute_reply.started":"2022-02-17T17:02:57.985887Z","shell.execute_reply":"2022-02-17T17:02:57.993422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's remove common used english words like a and an. For now we've taken only small set of list, \n# later we will expand it\ndrop_keys = ['had','with','in','a','an','of','his','for','has','the','at']\nfor i in drop_keys:\n    if i in feature_word_list:\n        del feature_word_list[i]","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:57.995264Z","iopub.execute_input":"2022-02-17T17:02:57.995655Z","iopub.status.idle":"2022-02-17T17:02:58.005891Z","shell.execute_reply.started":"2022-02-17T17:02:57.995623Z","shell.execute_reply":"2022-02-17T17:02:58.00503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing all words which has frequency less than 5\nfeature_word_list = {key:val for key, val in feature_word_list.items() if val > 4}","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:58.007538Z","iopub.execute_input":"2022-02-17T17:02:58.00776Z","iopub.status.idle":"2022-02-17T17:02:58.019647Z","shell.execute_reply.started":"2022-02-17T17:02:58.007733Z","shell.execute_reply":"2022-02-17T17:02:58.019036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's see the final list\nprint(feature_word_list)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:58.02094Z","iopub.execute_input":"2022-02-17T17:02:58.021374Z","iopub.status.idle":"2022-02-17T17:02:58.031511Z","shell.execute_reply.started":"2022-02-17T17:02:58.021302Z","shell.execute_reply":"2022-02-17T17:02:58.03084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now I'll create a list from the word, currently feature_word_list is a dict with vale as freq\nfinal_list = list(feature_word_list.keys())\nprint(type(final_list))\nprint(final_list)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:58.032777Z","iopub.execute_input":"2022-02-17T17:02:58.033531Z","iopub.status.idle":"2022-02-17T17:02:58.042805Z","shell.execute_reply.started":"2022-02-17T17:02:58.033494Z","shell.execute_reply":"2022-02-17T17:02:58.042025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Now let's create a Method to create Key list for all features","metadata":{}},{"cell_type":"code","source":"dummy_features = features.copy()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:58.044814Z","iopub.execute_input":"2022-02-17T17:02:58.04514Z","iopub.status.idle":"2022-02-17T17:02:58.051806Z","shell.execute_reply.started":"2022-02-17T17:02:58.045098Z","shell.execute_reply":"2022-02-17T17:02:58.051197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def GenerateKeys(string_list, freq):\n    drop_keys = ['had','with','in','a','an','of','his','for','has','the','at','no','last',\n                 'ago','to','not','past','was','her','he','his','and','is','have','when','up','but','mo\"]',\n                 'by','him']\n    string_word_list = ' '.join(string_list).split()\n    counter=collections.Counter(string_word_list)\n    string_word_list = dict(sorted(counter.items(), key=lambda item: item[1],reverse=True))\n    \n    for drop_key in drop_keys:\n        if drop_key in string_word_list:\n            del string_word_list[drop_key]\n        \n    string_word_list = {key:val for key, val in string_word_list.items() if val > freq}\n    return list(string_word_list.keys())","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:58.055065Z","iopub.execute_input":"2022-02-17T17:02:58.055687Z","iopub.status.idle":"2022-02-17T17:02:58.063809Z","shell.execute_reply.started":"2022-02-17T17:02:58.055639Z","shell.execute_reply":"2022-02-17T17:02:58.063237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummy_features.reset_index()\n\nfor idx,row in dummy_features.iterrows():\n    string_list = pd.Series(dummy_train[(dummy_train['case_num']==row['case_num']) & (dummy_train['feature_num']==row['feature_num'])]['annotation']).unique()\n    no = len(string_list)\n    keys = GenerateKeys(string_list,2)\n    dummy_features.at[idx,'keys'] = json.dumps(keys)\n    dummy_features.at[idx,'no_of_notes'] = no\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:58.065086Z","iopub.execute_input":"2022-02-17T17:02:58.065514Z","iopub.status.idle":"2022-02-17T17:02:58.233853Z","shell.execute_reply.started":"2022-02-17T17:02:58.065469Z","shell.execute_reply":"2022-02-17T17:02:58.232948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummy_features[dummy_features['case_num']==4]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-17T17:02:58.23511Z","iopub.execute_input":"2022-02-17T17:02:58.235349Z","iopub.status.idle":"2022-02-17T17:02:58.253752Z","shell.execute_reply.started":"2022-02-17T17:02:58.235299Z","shell.execute_reply":"2022-02-17T17:02:58.25319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The keys still need some adjustments as it some of them had only 2 notes per feature.\n# Apart from that I hope this will give some headstart\n# will continue with notebook and methods to predict the sentence using keys\n# find the features_with_keys.csv in data section of this notebook\ndummy_features.to_csv('features_with_keys.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:02:58.254794Z","iopub.execute_input":"2022-02-17T17:02:58.255372Z","iopub.status.idle":"2022-02-17T17:02:58.263356Z","shell.execute_reply.started":"2022-02-17T17:02:58.255332Z","shell.execute_reply":"2022-02-17T17:02:58.262678Z"},"trusted":true},"execution_count":null,"outputs":[]}]}