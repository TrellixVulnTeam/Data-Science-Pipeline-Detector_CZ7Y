{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-18T03:27:51.520925Z","iopub.execute_input":"2022-03-18T03:27:51.521339Z","iopub.status.idle":"2022-03-18T03:27:51.577765Z","shell.execute_reply.started":"2022-03-18T03:27:51.521309Z","shell.execute_reply":"2022-03-18T03:27:51.576935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport numpy as np\nimport pandas as pd \nimport os\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom termcolor import colored\nfrom wordcloud import WordCloud,STOPWORDS\nfrom spacy import displacy\nfrom nltk.tokenize import sent_tokenize, word_tokenize \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:27:51.579431Z","iopub.execute_input":"2022-03-18T03:27:51.579641Z","iopub.status.idle":"2022-03-18T03:27:52.73249Z","shell.execute_reply.started":"2022-03-18T03:27:51.579617Z","shell.execute_reply":"2022-03-18T03:27:52.731653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')\ntest = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\nss = pd.read_csv('../input/nbme-score-clinical-patient-notes/sample_submission.csv')\npn = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\nfeatures = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\ntrain = train.merge(features, on=['case_num','feature_num'], validate='m:1')\ntrain = train.merge(pn, validate='m:1')\nfeatures.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:27:52.73369Z","iopub.execute_input":"2022-03-18T03:27:52.733924Z","iopub.status.idle":"2022-03-18T03:27:54.24815Z","shell.execute_reply.started":"2022-03-18T03:27:52.733879Z","shell.execute_reply":"2022-03-18T03:27:54.247173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers\n!pip download tokenizers==0.11.0\n","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:27:54.249723Z","iopub.execute_input":"2022-03-18T03:27:54.250046Z","iopub.status.idle":"2022-03-18T03:28:06.267626Z","shell.execute_reply.started":"2022-03-18T03:27:54.250008Z","shell.execute_reply":"2022-03-18T03:28:06.26678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:06.270085Z","iopub.execute_input":"2022-03-18T03:28:06.270358Z","iopub.status.idle":"2022-03-18T03:28:06.276136Z","shell.execute_reply.started":"2022-03-18T03:28:06.270326Z","shell.execute_reply":"2022-03-18T03:28:06.275009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    wandb=True\n    competition='NBME'\n    _wandb_kernel='nakama'\n    debug=False\n    apex=True\n    print_freq=100\n    num_workers=4\n    model=\"microsoft/deberta-base\"\n    scheduler='cosine' # ['linear', 'cosine']\n    batch_scheduler=True\n    num_cycles=0.5\n    num_warmup_steps=0\n    epochs=5\n    encoder_lr=2e-5\n    decoder_lr=2e-5\n    min_lr=1e-6\n    eps=1e-6\n    betas=(0.9, 0.999)\n    batch_size=12\n    fc_dropout=0.2\n    max_len=512\n    weight_decay=0.01\n    gradient_accumulation_steps=1\n    max_grad_norm=1000\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n    train=True\n    \nif CFG.debug:\n    CFG.epochs = 2\n    CFG.trn_fold = [0]\n# ====================================================\n# wandb\n# ====================================================\nif CFG.wandb:\n    \n    import wandb\n\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n        wandb.login(key=secret_value_0)\n        anony = None\n    except:\n        anony = \"must\"\n        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n\n\n    def class2dict(f):\n        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n\n    run = wandb.init(project='NBME-Public', \n                     name=CFG.model,\n                     config=class2dict(CFG),\n                     group=CFG.model,\n                     job_type=\"train\",\n                     anonymous=anony)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:06.277719Z","iopub.execute_input":"2022-03-18T03:28:06.27807Z","iopub.status.idle":"2022-03-18T03:28:14.944816Z","shell.execute_reply.started":"2022-03-18T03:28:06.278028Z","shell.execute_reply":"2022-03-18T03:28:14.944079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\ntrain = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/train.csv\")\npatient_notes = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/patient_notes.csv\")\nbest_answer_pn_num = train.query(\"`case_num` == 0 and `location` != '[]'\").groupby(['pn_num']).size().idxmax()\n# The following cell was copied was SANSKAR HASIJA's excellent EDA\nPATIENT_IDX = best_answer_pn_num\n\npatient_df = train[train['pn_num'] == PATIENT_IDX]\nprint(f\"\\033[94mPatient Notes - \")\nprint(f'\\033[94m',patient_notes[patient_notes[\"pn_num\"] == PATIENT_IDX][\"pn_history\"].iloc[0])\nprint(\"------------\")\nprint(f'\\033[92mAnnotaions:')\nfor i in range(len(patient_df)):\n    print(f'\\033[92m',patient_df[\"annotation\"].iloc[i])","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:14.948972Z","iopub.execute_input":"2022-03-18T03:28:14.950736Z","iopub.status.idle":"2022-03-18T03:28:15.384161Z","shell.execute_reply.started":"2022-03-18T03:28:14.950694Z","shell.execute_reply":"2022-03-18T03:28:15.382839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_answer_pn_num = train.query(\"`case_num` == 1 and `location` != '[]'\").groupby(['pn_num']).size().idxmax()\n\n# The following cell was copied was SANSKAR HASIJA's excellent EDA\nPATIENT_IDX = best_answer_pn_num\n\npatient_df = train[train['pn_num'] == PATIENT_IDX]\nprint(f\"\\033[94mPatient Notes - \")\nprint(f'\\033[94m',patient_notes[patient_notes[\"pn_num\"] == PATIENT_IDX][\"pn_history\"].iloc[0])\nprint(\"------------\")\nprint(f'\\033[92mAnnotaions:')\nfor i in range(len(patient_df)):\n    print(f'\\033[92m',patient_df[\"annotation\"].iloc[i])","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:15.385729Z","iopub.execute_input":"2022-03-18T03:28:15.386047Z","iopub.status.idle":"2022-03-18T03:28:15.407716Z","shell.execute_reply.started":"2022-03-18T03:28:15.385979Z","shell.execute_reply":"2022-03-18T03:28:15.404979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.query(\"`case_num` == 1\")['feature_num'].unique()\nrandom_id = train[train['case_num'] == 1].sample(random_state=12)['pn_num']\ndisplay(random_id.iloc[0])\n\npatient_df = train[train['pn_num'] == random_id.iloc[0]]\nprint(f\"\\033[94mPatient Notes - \")\nprint(f'\\033[94m',patient_notes[patient_notes[\"pn_num\"] == PATIENT_IDX][\"pn_history\"].iloc[0])\nprint(\"------------\")\nprint(f'\\033[92mAnnotaions:')\nfor i in range(len(patient_df)):\n    print(f'\\033[92m',patient_df[\"annotation\"].iloc[i])\n#\nbest_answer_pn_num = train.query(\"`case_num` == 2 and `location` != '[]'\").groupby(['pn_num']).size().idxmax()\n\n# The following cell was copied was SANSKAR HASIJA's excellent EDA\nPATIENT_IDX = best_answer_pn_num\n\npatient_df = train[train['pn_num'] == PATIENT_IDX]\nprint(f\"\\033[94mPatient Notes - \")\nprint(f'\\033[94m',patient_notes[patient_notes[\"pn_num\"] == PATIENT_IDX][\"pn_history\"].iloc[0])\nprint(\"------------\")\nprint(f'\\033[92mAnnotaions:')\nfor i in range(len(patient_df)):\n    print(f'\\033[92m',patient_df[\"annotation\"].iloc[i])","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:15.409381Z","iopub.execute_input":"2022-03-18T03:28:15.40967Z","iopub.status.idle":"2022-03-18T03:28:15.478737Z","shell.execute_reply.started":"2022-03-18T03:28:15.40964Z","shell.execute_reply":"2022-03-18T03:28:15.477098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport random\nimport os\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\nimport dill\nimport tensorflow.keras.backend as K\nfrom tqdm.auto import tqdm\nfrom tensorflow.keras import mixed_precision\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, AutoConfig,TFAutoModel\nimport json\nimport gc\nimport string\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n%env TOKENIZERS_PARALLELISM=true\n","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:15.480395Z","iopub.execute_input":"2022-03-18T03:28:15.480645Z","iopub.status.idle":"2022-03-18T03:28:19.471783Z","shell.execute_reply.started":"2022-03-18T03:28:15.480608Z","shell.execute_reply":"2022-03-18T03:28:19.471094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try: # detect TPUs\n    tpu  = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    tf.config.experimental_connect_to_cluster(tpu )\n    tf.tpu.experimental.initialize_tpu_system(tpu )\n    strategy = tf.distribute.TPUStrategy(tpu )\n    print('Using TPU')\nexcept ValueError: # detect GPUs\n    tpu = None\n    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    #strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:19.473085Z","iopub.execute_input":"2022-03-18T03:28:19.47335Z","iopub.status.idle":"2022-03-18T03:28:33.277135Z","shell.execute_reply.started":"2022-03-18T03:28:19.473312Z","shell.execute_reply":"2022-03-18T03:28:33.276035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed= 59\nrandom.seed(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\nos.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\ntf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\nTRAIN = False ","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:33.278655Z","iopub.execute_input":"2022-03-18T03:28:33.279016Z","iopub.status.idle":"2022-03-18T03:28:33.286135Z","shell.execute_reply.started":"2022-03-18T03:28:33.278974Z","shell.execute_reply":"2022-03-18T03:28:33.285422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/features.csv\")\npatient_notes = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/patient_notes.csv\")\ntest = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/test.csv\")\ntrain= pd.read_csv(\"../input/nbme-score-clinical-patient-notes/train.csv\")\nsample_submission= pd.read_csv(\"../input/nbme-score-clinical-patient-notes/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:33.287384Z","iopub.execute_input":"2022-03-18T03:28:33.287619Z","iopub.status.idle":"2022-03-18T03:28:33.698627Z","shell.execute_reply.started":"2022-03-18T03:28:33.287592Z","shell.execute_reply":"2022-03-18T03:28:33.697664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.loc[27, 'feature_text'] = 'Last-Pap-smear-1-year-ago'","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:33.702794Z","iopub.execute_input":"2022-03-18T03:28:33.703106Z","iopub.status.idle":"2022-03-18T03:28:33.710624Z","shell.execute_reply.started":"2022-03-18T03:28:33.703074Z","shell.execute_reply":"2022-03-18T03:28:33.709664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[338, 'location'] = ('[[764 783]]')\n\ntrain.loc[621, 'location'] = ('[[77 100]]')\n\ntrain.loc[655, 'location'] = ('[[285 292;301 312], [285 287;296 312]]')\n\ntrain.loc[1262, 'location'] = ('[[551 557;565 580]]')\n\ntrain.loc[1265, 'location'] = ('[[131 135;181 212]]')\n\ntrain.loc[1396, 'location'] = ('[[259 280]]')\n\ntrain.loc[1591, 'location'] = ('[[176 184;201 212]]')\n\ntrain.loc[1615, 'location'] = ('[[249 257;271 288]]')\n\ntrain.loc[1664, 'location'] = ('[[822 824;907 924]]')\n\ntrain.loc[1714, 'location'] = ('[[101 129]]')\n\ntrain.loc[1929, 'location'] = ('[[531 539;549 561]]')\n\ntrain.loc[2134, 'location'] = ('[[540 560;581 593]]')\n\ntrain.loc[2191, 'location'] = ('[[32 57]]')\n\ntrain.loc[2553, 'location'] = ('[[308 317;376 384]]')\n\ntrain.loc[3124, 'location'] = ('[[549 557]]')\n\ntrain.loc[3858, 'location'] = ('[[102 123], [102 112;125 141], [102 112;143 157], [102 112;159 171]]')\n\ntrain.loc[4373, 'location'] = ('[[33 45]]')\n\ntrain.loc[4763, 'location'] = ('[[5 16]]')\n\ntrain.loc[4782, 'location'] = ('[[175 194]]')\n\ntrain.loc[4908, 'location'] = ('[[700 723]]')\n\ntrain.loc[6016, 'location'] = ('[[225 250]]')\n\ntrain.loc[6192, 'location'] = ('[[197 218;236 260]]')\n\ntrain.loc[6380, 'location'] = ('[[480 482;507 519], [480 482;499 503;512 519], [480 482;521 531], [480 482;533 545], [480 482;564 582]]')\n\ntrain.loc[6562, 'location'] = ('[[290 320;327 337], [290 320;342 358]]')\n\ntrain.loc[6862, 'location'] = ('[[288 296;324 363]]')\n\ntrain.loc[7022, 'location'] = ('[[108 182]]')\n\ntrain.loc[7422, 'location'] = ('[[102 121]]')\n\ntrain.loc[8876, 'location'] = ('[[481 483;533 552]]')\n\ntrain.loc[9027, 'location'] = ('[[92 102], [123 164]]')\n\ntrain.loc[9938, 'location'] = ('[[89 117], [122 138], [368 402]]')\n\ntrain.loc[9973, 'location'] = ('[[344 361]]')\n\ntrain.loc[10513, 'location'] = ('[[600 611], [607 623]]')\n\ntrain.loc[11551, 'location'] = ('[[386 400;443 461]]')\n\ntrain.loc[11677, 'location'] = ('[[160 201]]')\n\ntrain.loc[12124, 'location'] = ('[[325 337;349 366]]')\n\ntrain.loc[12279, 'location'] = ('[[405 459;488 524]]')\n\ntrain.loc[12289, 'location'] = ('[[353 400;488 524]]')\n\ntrain.loc[13238, 'location'] = ('[[293 307], [321 331]]')\n\ntrain.loc[13297, 'location'] = ('[[182 221], [182 213;225 234]]')\n\ntrain.loc[13299, 'location'] = ('[[79 88], [409 418]]')\n\ntrain.loc[13845, 'location'] = ('[[86 94;230 236], [86 94;237 256]]')\n\ntrain.loc[14083, 'location'] = ('[[56 64;156 179]]')","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:33.712094Z","iopub.execute_input":"2022-03-18T03:28:33.712306Z","iopub.status.idle":"2022-03-18T03:28:33.756424Z","shell.execute_reply.started":"2022-03-18T03:28:33.712281Z","shell.execute_reply":"2022-03-18T03:28:33.755297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.merge(patient_notes,on=['case_num','pn_num']).merge(features,on=['case_num','feature_num'])\ntrain = train.merge(patient_notes,on=['case_num','pn_num']).merge(features,on=['case_num','feature_num'])","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:33.758943Z","iopub.execute_input":"2022-03-18T03:28:33.759673Z","iopub.status.idle":"2022-03-18T03:28:33.802172Z","shell.execute_reply.started":"2022-03-18T03:28:33.759627Z","shell.execute_reply":"2022-03-18T03:28:33.801209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = \"bert-base-uncased\"\nDATA_PATH = \"../input/nbmeqatfdata\"\nDATA_EXISTS = os.path.exists(DATA_PATH)\nMODEL_NAME","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:33.803399Z","iopub.execute_input":"2022-03-18T03:28:33.803632Z","iopub.status.idle":"2022-03-18T03:28:33.811354Z","shell.execute_reply.started":"2022-03-18T03:28:33.803606Z","shell.execute_reply":"2022-03-18T03:28:33.810465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DATA_EXISTS and TRAIN:\n    ! cp -r ../input/nbmeqatfdata/my_tokenizer .\n    ! cp ../input/nbmeqatfdata/*.dill .\nnormalization = True","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:33.812449Z","iopub.execute_input":"2022-03-18T03:28:33.812796Z","iopub.status.idle":"2022-03-18T03:28:33.824386Z","shell.execute_reply.started":"2022-03-18T03:28:33.812745Z","shell.execute_reply":"2022-03-18T03:28:33.823596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DATA_EXISTS:\n    tokenizer = AutoTokenizer.from_pretrained(DATA_PATH+\"/my_tokenizer/\",normalization=normalization)\n    config = AutoConfig.from_pretrained(DATA_PATH+\"/my_tokenizer/config.json\")\nelse:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,normalization=normalization)\n    config = AutoConfig.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained('my_tokenizer')\n    config.save_pretrained('my_tokenizer')","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:33.825703Z","iopub.execute_input":"2022-03-18T03:28:33.82596Z","iopub.status.idle":"2022-03-18T03:28:33.967724Z","shell.execute_reply.started":"2022-03-18T03:28:33.825932Z","shell.execute_reply":"2022-03-18T03:28:33.967001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURES = features.feature_num.unique().tolist()\nN_FEATURES = len(FEATURES)\nSEQUENCE_LENGTH = 512\ndef decode_location(locations):\n    for x in [\"[\",\"]\",\"'\"]:\n        locations = locations.replace(x,'')\n    locations = locations.replace(',',';')\n    locations = locations.split(\";\")\n    res = []\n    for location in locations:\n        if location:\n            x,y = location.split()\n            res.append((int(x),int(y)))\n    return sorted(res,key=lambda x:x[0])\n\ndef build_data():\n    input_ids_arr,attention_mask_arr,token_type_ids_arr,answers =[],[],[],[]\n    feature_ids ,case_ids = [],[]\n    for g1 in tqdm(train.groupby('pn_num')):\n        gdf = g1[1]\n        pn_history  = gdf.iloc[0].pn_history\n\n        for index, row in gdf.iterrows():\n            question = row.feature_text\n            answer_empty = True\n            tokens = tokenizer.encode_plus(pn_history,question, max_length=SEQUENCE_LENGTH, padding='max_length',truncation=True, return_offsets_mapping=True)\n            input_ids = np.array(tokens['input_ids'],dtype=np.int32)\n            attention_mask = np.array(tokens['attention_mask'],dtype=np.uint8)\n            token_type_ids = np.array(tokens['token_type_ids'],dtype=np.uint8)\n            offsets = tokens['offset_mapping']\n            answer_mask = np.zeros(SEQUENCE_LENGTH,dtype=np.uint8)\n            # Answer mask\n            for i, (w_start, w_end) in enumerate(offsets):\n                for start,end in decode_location(row.location):\n                    if w_start < w_end and (w_start >= start) and (end >= w_end):\n                        answer_mask[i] = 1\n                        answer_empty = False\n                    if w_start >= w_end:\n                        break\n            if not answer_empty:\n                input_ids_arr.append(input_ids)\n                attention_mask_arr.append(attention_mask)\n                token_type_ids_arr.append(token_type_ids)\n                answers.append(answer_mask)\n                feature_ids.append(row.feature_num)\n                case_ids.append(row.case_num)\n    input_ids_arr = np.array(input_ids_arr,dtype=np.int32)\n    attention_mask_arr = np.array(attention_mask_arr,dtype=np.uint8)\n    token_type_ids_arr = np.array(token_type_ids_arr,dtype=np.uint8)\n    answers = np.array(answers,dtype=np.uint8)\n    feature_ids = np.array(feature_ids,dtype=np.int32)\n    case_ids = np.array(case_ids,dtype=np.int32)\n    return feature_ids,case_ids,input_ids_arr,attention_mask_arr,token_type_ids_arr,answers\nif DATA_EXISTS:\n    data = dill.load(open(DATA_PATH+\"/data.dill\",'rb'))\nelse:\n    data = build_data()\n    dill.dump(data,open('data.dill','wb'))","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:33.96912Z","iopub.execute_input":"2022-03-18T03:28:33.96944Z","iopub.status.idle":"2022-03-18T03:28:35.06806Z","shell.execute_reply.started":"2022-03-18T03:28:33.969402Z","shell.execute_reply":"2022-03-18T03:28:35.067261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:35.069847Z","iopub.execute_input":"2022-03-18T03:28:35.070187Z","iopub.status.idle":"2022-03-18T03:28:35.078387Z","shell.execute_reply.started":"2022-03-18T03:28:35.070148Z","shell.execute_reply":"2022-03-18T03:28:35.077659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_dataset(data,batch_size=32 if tpu else 12,shuffle=True):\n    ds = tf.data.Dataset.from_tensor_slices(data).map(lambda a,b,c,d:((a,b,c),d))\n    size = len(ds)\n    steps = size//batch_size\n    ds = ds.repeat()\n    if shuffle:\n        ds = ds.shuffle(size)\n    ds = ds.batch(batch_size).prefetch(buffer_size=AUTO)\n    return ds,steps\nn_splits = 5\ncv = GroupKFold(n_splits=n_splits)\nfeature_ids,case_ids,input_ids_arr,attention_mask_arr,token_type_ids_arr,answers = data\ndata_splits = []\nfor n, (train_index, val_index) in enumerate(cv.split(feature_ids , case_ids,feature_ids)):\n    train_data = input_ids_arr[train_index],attention_mask_arr[train_index],token_type_ids_arr[train_index],answers[train_index]\n    test_data = input_ids_arr[val_index],attention_mask_arr[val_index],token_type_ids_arr[val_index],answers[val_index]\n    data_splits.append((to_dataset(train_data),to_dataset(test_data,shuffle=False)))\n    del train_data,test_data","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:35.079799Z","iopub.execute_input":"2022-03-18T03:28:35.08063Z","iopub.status.idle":"2022-03-18T03:28:35.626755Z","shell.execute_reply.started":"2022-03-18T03:28:35.080587Z","shell.execute_reply":"2022-03-18T03:28:35.625837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    \n    tokens = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH,), name = 'tokens', dtype=tf.int32)\n    attention = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH,), name = 'attention', dtype=tf.int32)\n    token_type_id = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH,), name = 'token_type_id', dtype=tf.int32)\n    \n    if not TRAIN:\n        config = AutoConfig.from_pretrained(DATA_PATH+\"/my_tokenizer/config.json\")\n        backbone = TFAutoModel.from_config(config)\n    else:\n        print(f\"Loading {MODEL_NAME}...\")\n        config = AutoConfig.from_pretrained(MODEL_NAME)\n        backbone = TFAutoModel.from_pretrained(MODEL_NAME,config=config)\n        \n    out = backbone(tokens, attention_mask=attention,token_type_ids=token_type_id)[0]\n    \n    out = tf.keras.layers.Dropout(0.2)(out)\n    out = tf.keras.layers.Dense(1, activation='sigmoid')(out)\n    \n    model = tf.keras.Model([tokens,attention,token_type_id],out)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:35.628285Z","iopub.execute_input":"2022-03-18T03:28:35.628615Z","iopub.status.idle":"2022-03-18T03:28:35.639011Z","shell.execute_reply.started":"2022-03-18T03:28:35.628574Z","shell.execute_reply":"2022-03-18T03:28:35.63787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\nepochs = 50\ninit_lr = 1e-5","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:35.640365Z","iopub.execute_input":"2022-03-18T03:28:35.641187Z","iopub.status.idle":"2022-03-18T03:28:35.65307Z","shell.execute_reply.started":"2022-03-18T03:28:35.641149Z","shell.execute_reply":"2022-03-18T03:28:35.652382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    val_key = \"val_f1_m\"\n    scores = []\n    with strategy.scope():\n        i = 0\n        for (train_ds,steps_per_epoch),(test_ds,steps) in data_splits:\n            model = build_model()\n            callback = tf.keras.callbacks.EarlyStopping(monitor=val_key,mode='max', patience=20)\n            ckp_callback = tf.keras.callbacks.ModelCheckpoint(\n                                                    filepath=f'model{i}.h5',\n                                                    save_weights_only=True,\n                                                    monitor=val_key,\n                                                    mode='max',\n                                                    options=tf.train.CheckpointOptions(experimental_io_device='/job:localhost'),\n                                                    save_best_only=True)\n            reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=val_key,mode='max',factor=0.2,patience=2, min_lr=1e-6)\n            callbacks=[callback,ckp_callback,reduce_lr]\n            # Compile the model\n            model.compile(optimizer=tf.keras.optimizers.Adam(init_lr),\n                          loss=tf.keras.losses.BinaryCrossentropy(),\n                          metrics=['acc',f1_m])\n\n            history = model.fit(train_ds,\n                                steps_per_epoch=steps_per_epoch,\n                                validation_data=test_ds,\n                                validation_steps=steps,\n                                epochs=epochs,\n                                callbacks=callbacks)\n            scores.append(max(history.history[val_key]))\n            i += 1\nif TRAIN:\n    print(scores)\n    print(f\"CV Score : {np.mean(scores)}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:35.654188Z","iopub.execute_input":"2022-03-18T03:28:35.654786Z","iopub.status.idle":"2022-03-18T03:28:35.669432Z","shell.execute_reply.started":"2022-03-18T03:28:35.654758Z","shell.execute_reply":"2022-03-18T03:28:35.668595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_test():\n    input_ids, attention_masks, token_type_ids,offsets,row_ids = [],[],[],[],[]\n    for g1 in tqdm(test.groupby('pn_num')):\n        gdf = g1[1]\n        for index, row in gdf.iterrows():\n            pn_history = row.pn_history\n            question = row.feature_text\n            row_id = row.id\n            tokens = tokenizer.encode_plus(pn_history, question, max_length=SEQUENCE_LENGTH, padding='max_length',\n                                           truncation=True, return_offsets_mapping=True)\n            input_id = np.array(tokens['input_ids'], dtype=np.int32)\n            attention_mask = np.array(tokens['attention_mask'], dtype=np.uint8)\n            token_type_id = np.array(tokens['token_type_ids'], dtype=np.uint8)\n            offset = tokens['offset_mapping']\n\n            input_ids.append(input_id)\n            attention_masks.append(attention_mask)\n            token_type_ids.append(token_type_id)\n            offsets.append(offset)\n            row_ids.append(row_id)\n            del question,row_id,tokens,input_id,attention_mask,token_type_id,offset\n\n    attention_masks = np.array(attention_masks, dtype=np.uint8)\n    token_type_ids = np.array(token_type_ids, dtype=np.uint8)\n    input_ids = np.array(input_ids, dtype=np.int32)\n    return row_ids,offsets,input_ids,attention_masks,token_type_ids\nrow_ids,offsets,input_ids,attention_masks,token_type_ids = build_test()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:35.670749Z","iopub.execute_input":"2022-03-18T03:28:35.671018Z","iopub.status.idle":"2022-03-18T03:28:35.733131Z","shell.execute_reply.started":"2022-03-18T03:28:35.67099Z","shell.execute_reply":"2022-03-18T03:28:35.732253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model()\npath =  DATA_PATH if not TRAIN else \".\"\npreds = None \nfor i in range(n_splits):\n    print(f\"SPLIT {i}\")\n    model.load_weights(path+f\"/model{i}.h5\")\n    pred = model.predict((input_ids,attention_masks,token_type_ids),batch_size=16)\n    if preds is None:\n        preds = pred\n    else: \n        preds += pred\n    del pred\n    gc.collect()\npreds = preds/(i+1)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:28:35.73466Z","iopub.execute_input":"2022-03-18T03:28:35.735231Z","iopub.status.idle":"2022-03-18T03:29:19.177276Z","shell.execute_reply.started":"2022-03-18T03:28:35.73519Z","shell.execute_reply":"2022-03-18T03:29:19.176428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_special_ids = set(tokenizer.all_special_ids)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:29:19.178597Z","iopub.execute_input":"2022-03-18T03:29:19.178824Z","iopub.status.idle":"2022-03-18T03:29:19.184404Z","shell.execute_reply.started":"2022-03-18T03:29:19.178799Z","shell.execute_reply":"2022-03-18T03:29:19.18343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_position(pos):\n    return \";\".join([\" \".join(np.array(p).astype(str)) for p in pos])\n\n\ndef translate(preds,row_ids,input_ids,offsets,token_type_ids):\n    all_ids = []\n    all_pos = []\n\n    for k in range(len(preds)):\n        offset = offsets[k]\n        pred = preds[k]\n        row_id = row_ids[k]\n        input_id = input_ids[k]\n        token_type_id = token_type_ids[k]\n        prediction = []\n        pred = (pred>0.5).astype(np.uint8)\n        \n        i = 0\n        while i<SEQUENCE_LENGTH:\n            \n            if token_type_id[i] != 0:\n                break\n            if int(input_id[i]) in all_special_ids:\n                i += 1\n                continue\n            if pred[i] == 0:\n                i += 1\n                continue\n            if pred[i] == 1:\n                start = min(offset[i])\n                while i<SEQUENCE_LENGTH:\n                    if pred[i] != 1:\n                        break\n                    elif token_type_id[i] != 0:\n                        break\n                    elif int(input_id[i]) in all_special_ids:\n                        break\n                    else:\n                        end = max(offset[i])\n                    i += 1\n                if  end == 0:\n                    break\n                prediction.append((start,end))\n            else:\n                i+=1\n        all_ids.append(row_id)\n        all_pos.append(decode_position(prediction))\n            \n    df = pd.DataFrame({\n        \"id\":all_ids,\n        \"location\": all_pos\n    })\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:29:19.185941Z","iopub.execute_input":"2022-03-18T03:29:19.18641Z","iopub.status.idle":"2022-03-18T03:29:19.202457Z","shell.execute_reply.started":"2022-03-18T03:29:19.186368Z","shell.execute_reply":"2022-03-18T03:29:19.201459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**submission**","metadata":{}},{"cell_type":"code","source":"sub = translate(preds,row_ids,input_ids,offsets,token_type_ids)\nsub.to_csv('submission.csv',index=False)\nsub.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:29:19.203816Z","iopub.execute_input":"2022-03-18T03:29:19.204095Z","iopub.status.idle":"2022-03-18T03:29:19.240287Z","shell.execute_reply.started":"2022-03-18T03:29:19.20407Z","shell.execute_reply":"2022-03-18T03:29:19.239312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.sort_values(by=\"id\").reset_index()[[\"id\",\"location\"]].head(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:29:19.241427Z","iopub.execute_input":"2022-03-18T03:29:19.24168Z","iopub.status.idle":"2022-03-18T03:29:19.280689Z","shell.execute_reply.started":"2022-03-18T03:29:19.241654Z","shell.execute_reply":"2022-03-18T03:29:19.280034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission= pd.read_csv(\"../input/nbme-score-clinical-patient-notes/sample_submission.csv\")\ndef build_test():\n    input_ids, attention_masks, token_type_ids,offsets,row_ids = [],[],[],[],[]\n    for g1 in tqdm(test.groupby('pn_num')):\n        gdf = g1[1]\n        for index, row in gdf.iterrows():\n            pn_history = row.pn_history\n            question = row.feature_text\n            row_id = row.id\n            tokens = tokenizer.encode_plus(pn_history, question, max_length=SEQUENCE_LENGTH, padding='max_length',\n                                           truncation=True, return_offsets_mapping=True)\n            input_id = np.array(tokens['input_ids'], dtype=np.int32)\n            attention_mask = np.array(tokens['attention_mask'], dtype=np.uint8)\n            token_type_id = np.array(tokens['token_type_ids'], dtype=np.uint8)\n            offset = tokens['offset_mapping']\n\n            input_ids.append(input_id)\n            attention_masks.append(attention_mask)\n            token_type_ids.append(token_type_id)\n            offsets.append(offset)\n            row_ids.append(row_id)\n            del question,row_id,tokens,input_id,attention_mask,token_type_id,offset\n\n    attention_masks = np.array(attention_masks, dtype=np.uint8)\n    token_type_ids = np.array(token_type_ids, dtype=np.uint8)\n    input_ids = np.array(input_ids, dtype=np.int32)\n    return row_ids,offsets,input_ids,attention_masks,token_type_ids\nrow_ids,offsets,input_ids,attention_masks,token_type_ids = build_test()\nmodel = build_model()\npath =  DATA_PATH if not TRAIN else \".\"\npreds = None \nfor i in range(n_splits):\n    print(f\"SPLIT {i}\")\n    model.load_weights(path+f\"/model{i}.h5\")\n    pred = model.predict((input_ids,attention_masks,token_type_ids),batch_size=16)\n    if preds is None:\n        preds = pred\n    else: \n        preds += pred\n    del pred\n    gc.collect()\npreds = preds/(i+1)\nall_special_ids = set(tokenizer.all_special_ids)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:29:19.28182Z","iopub.execute_input":"2022-03-18T03:29:19.282548Z","iopub.status.idle":"2022-03-18T03:29:53.488328Z","shell.execute_reply.started":"2022-03-18T03:29:19.282519Z","shell.execute_reply":"2022-03-18T03:29:53.48733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = translate(preds,row_ids,input_ids,offsets,token_type_ids)\nsub.to_csv('submission.csv',index=False)\nsub.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T03:29:53.489425Z","iopub.execute_input":"2022-03-18T03:29:53.489647Z","iopub.status.idle":"2022-03-18T03:29:53.512713Z","shell.execute_reply.started":"2022-03-18T03:29:53.489623Z","shell.execute_reply":"2022-03-18T03:29:53.511974Z"},"trusted":true},"execution_count":null,"outputs":[]}]}