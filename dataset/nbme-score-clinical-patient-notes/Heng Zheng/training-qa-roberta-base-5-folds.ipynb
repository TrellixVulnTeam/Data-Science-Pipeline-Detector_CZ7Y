{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"base on: https://www.kaggle.com/tomohiroh/nbme-bert-for-beginners","metadata":{}},{"cell_type":"markdown","source":"- v2: data cleaning from https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\nimport os\nimport ast \nfrom itertools import chain\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm, trange\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_recall_fscore_support\n\nimport torch\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-09T03:36:21.540665Z","iopub.execute_input":"2022-02-09T03:36:21.54102Z","iopub.status.idle":"2022-02-09T03:36:30.075159Z","shell.execute_reply.started":"2022-02-09T03:36:21.540931Z","shell.execute_reply":"2022-02-09T03:36:30.074231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# data preprocessing","metadata":{}},{"cell_type":"code","source":"ROOT = '../input/nbme-score-clinical-patient-notes'\nN_FOLDS = 5","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:36:30.077447Z","iopub.execute_input":"2022-02-09T03:36:30.077882Z","iopub.status.idle":"2022-02-09T03:36:30.087064Z","shell.execute_reply.started":"2022-02-09T03:36:30.077837Z","shell.execute_reply":"2022-02-09T03:36:30.085175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_train_df(debug=False):\n    feats = pd.read_csv(f\"{ROOT}/features.csv\")\n    feats.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    \n    notes = pd.read_csv(f\"{ROOT}/patient_notes.csv\")\n    train = pd.read_csv(f\"{ROOT}/train.csv\")\n    \n    train['annotation'] = train['annotation'].apply(ast.literal_eval)\n    train['location'] = train['location'].apply(ast.literal_eval)\n    \n    train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n    train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n\n    train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n    train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n\n    train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n    train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n\n    train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n    train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n\n    train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n    train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n\n    train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n    train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n\n    train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n    train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n\n    train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n    train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n\n    train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n    train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n\n    train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n    train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n\n    train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n    train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n\n    train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n    train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n\n    train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n    train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n\n    train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n    train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n\n    train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n    train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n\n    train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n    train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n\n    train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n    train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n\n    train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n    train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n\n    train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n    train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n\n    train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n    train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n\n    train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n    train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n\n    train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n    train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n\n    train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n    train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n\n    train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n    train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n\n    train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n    train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n\n    train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n    train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n\n    train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n    train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n\n    train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n    train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n\n    train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n    train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n\n    train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n    train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n\n    train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n    train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n\n    train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n    train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n\n    train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n    train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n\n    train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n    train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n\n    train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n    train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n\n    train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n    train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n\n    train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n    train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n\n    train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n    train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n\n    train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n    train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n\n    train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n    train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n\n    train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n    train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n\n    train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n    train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n\n    merged = train.merge(notes, how = \"left\")\n    merged = merged.merge(feats, how = \"left\")\n    \n    def process_feature_text(text):\n        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    \n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n    \n    if debug:\n        merged = merged.sample(frac=0.5).reset_index(drop=True)\n        \n    skf = StratifiedKFold(n_splits=N_FOLDS)\n    merged[\"stratify_on\"] = merged[\"case_num\"].astype(str) + merged[\"feature_num\"].astype(str)\n    merged[\"fold\"] = -1\n    for fold, (_, valid_idx) in enumerate(skf.split(merged[\"id\"], y=merged[\"stratify_on\"])):\n        merged.loc[valid_idx, \"fold\"] = fold\n    \n    return merged\n\ntrain_df = create_train_df()\ndisplay(train_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:36:30.091311Z","iopub.execute_input":"2022-02-09T03:36:30.092354Z","iopub.status.idle":"2022-02-09T03:36:31.671155Z","shell.execute_reply.started":"2022-02-09T03:36:30.092272Z","shell.execute_reply":"2022-02-09T03:36:31.670079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:36:31.672803Z","iopub.execute_input":"2022-02-09T03:36:31.673952Z","iopub.status.idle":"2022-02-09T03:36:31.681151Z","shell.execute_reply.started":"2022-02-09T03:36:31.673905Z","shell.execute_reply":"2022-02-09T03:36:31.680155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tokenizer","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = 'roberta-base'\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:36:31.684661Z","iopub.execute_input":"2022-02-09T03:36:31.685296Z","iopub.status.idle":"2022-02-09T03:36:39.889805Z","shell.execute_reply.started":"2022-02-09T03:36:31.685249Z","shell.execute_reply":"2022-02-09T03:36:39.8886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_add_labels(tokenizer, example):\n    tokenized_inputs = tokenizer(\n        example[\"feature_text\"],      # question\n        example[\"pn_history\"],        # content\n        truncation=\"only_second\",\n        max_length=480,\n        padding=\"max_length\",\n        return_offsets_mapping=True\n    )\n    labels = [0.0] * len(tokenized_inputs[\"input_ids\"])\n    tokenized_inputs[\"location_int\"] = loc_list_to_ints(example[\"location\"])\n    tokenized_inputs[\"sequence_ids\"] = tokenized_inputs.sequence_ids()\n    \n    for idx, (seq_id, offsets) in enumerate(zip(tokenized_inputs[\"sequence_ids\"], tokenized_inputs[\"offset_mapping\"])):\n        if seq_id is None or seq_id == 0:     # seq_id == None: special tokens | seq_id == 0: question\n            labels[idx] = -100\n            continue\n        exit = False\n        token_start, token_end = offsets\n        for feature_start, feature_end in tokenized_inputs[\"location_int\"]:\n            if exit:\n                break\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                exit = True\n    tokenized_inputs[\"labels\"] = labels\n    \n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:36:39.892016Z","iopub.execute_input":"2022-02-09T03:36:39.892939Z","iopub.status.idle":"2022-02-09T03:36:39.909833Z","shell.execute_reply.started":"2022-02-09T03:36:39.892863Z","shell.execute_reply":"2022-02-09T03:36:39.908506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first = train_df.loc[0]\nexample = {\n    \"feature_text\": first.feature_text,\n    \"pn_history\": first.pn_history,\n    \"location\": first.location,\n    \"annotation\": first.annotation\n}\nfor key in example.keys():\n    print(key)\n    print(example[key])\n    print(\"=\" * 100)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:36:39.916417Z","iopub.execute_input":"2022-02-09T03:36:39.920005Z","iopub.status.idle":"2022-02-09T03:36:39.945461Z","shell.execute_reply.started":"2022-02-09T03:36:39.919939Z","shell.execute_reply":"2022-02-09T03:36:39.944025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_inputs = tokenize_and_add_labels(tokenizer, example)\nfor key in tokenized_inputs.keys():\n    print(key)\n    print(tokenized_inputs[key])\n    print(\"=\" * 100)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:36:39.951681Z","iopub.execute_input":"2022-02-09T03:36:39.952509Z","iopub.status.idle":"2022-02-09T03:36:39.987438Z","shell.execute_reply.started":"2022-02-09T03:36:39.952238Z","shell.execute_reply":"2022-02-09T03:36:39.986164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# dataset","metadata":{}},{"cell_type":"code","source":"class NBMEData(torch.utils.data.Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = tokenize_and_add_labels(self.tokenizer, example)\n        \n        input_ids = np.array(tokenized[\"input_ids\"])\n        attention_mask = np.array(tokenized[\"attention_mask\"])\n        labels = np.array(tokenized[\"labels\"])\n        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\")\n        \n        return input_ids, attention_mask, labels, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:36:39.993766Z","iopub.execute_input":"2022-02-09T03:36:39.994425Z","iopub.status.idle":"2022-02-09T03:36:40.01635Z","shell.execute_reply.started":"2022-02-09T03:36:39.994378Z","shell.execute_reply":"2022-02-09T03:36:40.014824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model","metadata":{}},{"cell_type":"code","source":"class NBMEModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = AutoModel.from_pretrained(MODEL_NAME)\n        self.config = AutoConfig.from_pretrained(MODEL_NAME)\n        self.dropout = torch.nn.Dropout(p=0.2)\n        self.classifier = torch.nn.Linear(self.config.hidden_size, 1)\n        \n    def forward(self, input_ids, attention_mask):\n        pooler_outputs = self.backbone(input_ids=input_ids, \n                                       attention_mask=attention_mask)[0]\n        logits = self.classifier(self.dropout(pooler_outputs)).squeeze(-1)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:36:40.023602Z","iopub.execute_input":"2022-02-09T03:36:40.02899Z","iopub.status.idle":"2022-02-09T03:36:40.045981Z","shell.execute_reply.started":"2022-02-09T03:36:40.028945Z","shell.execute_reply":"2022-02-09T03:36:40.043569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# training","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDEVICE","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:36:40.054826Z","iopub.execute_input":"2022-02-09T03:36:40.055098Z","iopub.status.idle":"2022-02-09T03:36:40.124571Z","shell.execute_reply.started":"2022-02-09T03:36:40.055067Z","shell.execute_reply":"2022-02-09T03:36:40.123206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nACCUMULATION_STEPS = 2\nEPOCHS = 5","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:36:40.12972Z","iopub.execute_input":"2022-02-09T03:36:40.130008Z","iopub.status.idle":"2022-02-09T03:36:40.136828Z","shell.execute_reply.started":"2022-02-09T03:36:40.129972Z","shell.execute_reply":"2022-02-09T03:36:40.135188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n = 1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n\ndef get_location_predictions(preds, offset_mapping, sequence_ids, test = False):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        pred = sigmoid(pred)\n        start_idx = None\n        current_preds = []\n        for p, o, s_id in zip(pred, offsets, seq_ids):\n            if s_id is None or s_id == 0:\n                continue\n            if p > 0.5:\n                if start_idx is None:\n                    start_idx = o[0]\n                end_idx = o[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n    return all_predictions\n\n\ndef calculate_char_CV(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros((num_chars))\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n        char_preds = np.zeros((num_chars))\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n    results = precision_recall_fscore_support(all_labels, all_preds, average = \"binary\")\n    return {\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }\n\n\ndef compute_metrics(p):\n    predictions, y_true = p\n    y_true = y_true.astype(int)\n    y_pred = [\n        [int(p > 0.5) for (p, l) in zip(pred, label) if l != -100]\n        for pred, label in zip(predictions, y_true)\n    ]\n    y_true = [\n        [l for l in label if l != -100] for label in y_true\n    ]\n    results = precision_recall_fscore_support(list(chain(*y_true)), list(chain(*y_pred)), average = \"binary\")\n    return {\n        \"token_precision\": results[0],\n        \"token_recall\": results[1],\n        \"token_f1\": results[2]\n    }","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:36:40.13916Z","iopub.execute_input":"2022-02-09T03:36:40.140475Z","iopub.status.idle":"2022-02-09T03:36:40.167638Z","shell.execute_reply.started":"2022-02-09T03:36:40.140426Z","shell.execute_reply":"2022-02-09T03:36:40.166394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(fold):\n    \n    # get dataloader\n    train = train_df[train_df[\"fold\"] != fold].reset_index(drop=True)\n    valid = train_df[train_df[\"fold\"] == fold].reset_index(drop=True)\n    train_ds = NBMEData(train, tokenizer)\n    valid_ds = NBMEData(valid, tokenizer)\n    train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, \n                                           pin_memory=True, shuffle=True, drop_last=True)\n    valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=BATCH_SIZE * 2, \n                                           pin_memory=True, shuffle=False, drop_last=False)\n    \n    # get model\n    model = NBMEModel().to(DEVICE)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n    \n    history = {\"train\": [], \"valid\": []}\n    best_loss = np.inf\n    \n    # training\n    for epoch in range(EPOCHS):\n        model.train()\n        train_loss = AverageMeter()\n        pbar = tqdm(train_dl)\n        for i, batch in enumerate(pbar):\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            labels = batch[2].to(DEVICE)\n            logits = model(input_ids, attention_mask)\n            loss = loss_fn(logits, labels)\n            loss = torch.masked_select(loss, labels > -1).mean()\n            loss /= ACCUMULATION_STEPS\n            loss.backward()\n            if (i+1) % ACCUMULATION_STEPS == 0: \n                optimizer.step()\n                optimizer.zero_grad()\n            train_loss.update(val=loss.item(), n=len(input_ids))\n            pbar.set_postfix(Loss=train_loss.avg)\n        print(f\"EPOCH: {epoch} train loss: {train_loss.avg}\")\n        history[\"train\"].append(train_loss.avg)\n        \n        # evaluation\n        model.eval()\n        valid_loss = AverageMeter()\n        pbar = tqdm(valid_dl)\n        with torch.no_grad():\n            for i, batch in enumerate(pbar):\n                input_ids = batch[0].to(DEVICE)\n                attention_mask = batch[1].to(DEVICE)\n                labels = batch[2].to(DEVICE)\n                logits = model(input_ids, attention_mask)\n                loss = loss_fn(logits, labels)\n                loss = torch.masked_select(loss, labels > -1).mean()\n                valid_loss.update(val=loss.item(), n=len(input_ids))\n                pbar.set_postfix(Loss=valid_loss.avg)\n        print(f\"EPOCH: {epoch} valid loss: {valid_loss.avg}\")\n        history[\"valid\"].append(valid_loss.avg)\n\n        # save model\n        if valid_loss.avg < best_loss:\n            best_loss = valid_loss.avg\n            torch.save(model.state_dict(), f\"nbme_{fold}.pth\")\n        \n    # evaluation summary\n    model.load_state_dict(torch.load(f\"nbme_{fold}.pth\", map_location = DEVICE))\n    model.eval()\n    preds = []\n    offsets = []\n    seq_ids = []\n    lbls = []\n    with torch.no_grad():\n        for batch in tqdm(valid_dl):\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            labels = batch[2].to(DEVICE)\n            offset_mapping = batch[3]\n            sequence_ids = batch[4]\n            logits = model(input_ids, attention_mask)\n            preds.append(logits.cpu().numpy())\n            offsets.append(offset_mapping.numpy())\n            seq_ids.append(sequence_ids.numpy())\n            lbls.append(labels.cpu().numpy())\n    preds = np.concatenate(preds, axis=0)\n    offsets = np.concatenate(offsets, axis=0)\n    seq_ids = np.concatenate(seq_ids, axis=0)\n    lbls = np.concatenate(lbls, axis=0)\n    location_preds = get_location_predictions(preds, offsets, seq_ids, test=False)\n    score = calculate_char_CV(location_preds, offsets, seq_ids, lbls)\n    print(f\"Fold: {fold} CV score: {score}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:36:40.172807Z","iopub.execute_input":"2022-02-09T03:36:40.173562Z","iopub.status.idle":"2022-02-09T03:36:40.20102Z","shell.execute_reply.started":"2022-02-09T03:36:40.173497Z","shell.execute_reply":"2022-02-09T03:36:40.199942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(N_FOLDS):\n    train_fn(fold)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:36:40.202427Z","iopub.execute_input":"2022-02-09T03:36:40.203172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}