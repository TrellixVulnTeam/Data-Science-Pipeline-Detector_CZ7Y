{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"All credit goes to @nbroad and @tomohiroh.\n\nPlease upvote their wonderful notebooks:\n\nhttps://www.kaggle.com/nbroad/qa-ner-hybrid-infer-nbme\n\nhttps://www.kaggle.com/tomohiroh/nbme-bert-for-beginners\n\nAll I did in this notebook was comment out one line\n\n```\n#     merged = merged.loc[merged[\"annotation\"] != \"[]\"].copy().reset_index(drop = True)\n```\n\nto use all samples. :)\n\nThe result is a huge boost from 0.695 to 0.803.","metadata":{}},{"cell_type":"markdown","source":"This notebook is for beginners.\n\nThanks for [Nicholas's work](https://www.kaggle.com/nbroad/qa-ner-hybrid-train-nbme).","metadata":{}},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom ast import literal_eval\nfrom itertools import chain\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom tqdm.notebook import tqdm, trange\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nfrom transformers import AutoModel, AutoTokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-07T02:44:33.158596Z","iopub.execute_input":"2022-02-07T02:44:33.159569Z","iopub.status.idle":"2022-02-07T02:44:40.467095Z","shell.execute_reply.started":"2022-02-07T02:44:33.159456Z","shell.execute_reply":"2022-02-07T02:44:40.466332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create df","metadata":{}},{"cell_type":"code","source":"ROOT = \"../input/nbme-score-clinical-patient-notes\"","metadata":{"execution":{"iopub.status.busy":"2022-02-07T02:44:40.468707Z","iopub.execute_input":"2022-02-07T02:44:40.468963Z","iopub.status.idle":"2022-02-07T02:44:40.477013Z","shell.execute_reply.started":"2022-02-07T02:44:40.46893Z","shell.execute_reply":"2022-02-07T02:44:40.475727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_train_df(debug = False):\n    feats = pd.read_csv(f\"{ROOT}/features.csv\")\n    notes = pd.read_csv(f\"{ROOT}/patient_notes.csv\")\n    train = pd.read_csv(f\"{ROOT}/train.csv\")\n\n    train[\"annotation_list\"] = [literal_eval(x) for x in train[\"annotation\"]]\n    train[\"location_list\"] = [literal_eval(x) for x in train[\"location\"]]\n    merged = train.merge(notes, how = \"left\")\n    merged = merged.merge(feats, how = \"left\")\n#     merged = merged.loc[merged[\"annotation\"] != \"[]\"].copy().reset_index(drop = True)\n    \n    def process_feature_text(text):\n        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    \n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n\n    if debug:\n        merged = merged.sample(frac = 0.5).reset_index(drop = True)\n\n    skf = StratifiedKFold(n_splits = 5)\n    merged[\"stratify_on\"] = merged[\"case_num\"].astype(str) + merged[\"feature_num\"].astype(str)\n    merged[\"fold\"] = -1\n    for fold, (_, valid_idx) in enumerate(skf.split(merged[\"id\"], y = merged[\"stratify_on\"])):\n        merged.loc[valid_idx, \"fold\"] = fold\n    \n    print(merged.shape)\n    return merged\n\ndf = create_train_df()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T02:44:40.479806Z","iopub.execute_input":"2022-02-07T02:44:40.480193Z","iopub.status.idle":"2022-02-07T02:44:41.837946Z","shell.execute_reply.started":"2022-02-07T02:44:40.480155Z","shell.execute_reply":"2022-02-07T02:44:41.837169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T02:44:41.840144Z","iopub.execute_input":"2022-02-07T02:44:41.840575Z","iopub.status.idle":"2022-02-07T02:44:41.862473Z","shell.execute_reply.started":"2022-02-07T02:44:41.840533Z","shell.execute_reply":"2022-02-07T02:44:41.861689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first = df.loc[0]\nexample = {\n    \"feature_text\": first.feature_text,\n    \"pn_history\": first.pn_history,\n    \"location_list\": first.location_list,\n    \"annotation_list\": first.annotation_list\n}\nfor key in example.keys():\n    print(key)\n    print(example[key])\n    print(\"=\" * 100)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T02:44:41.863883Z","iopub.execute_input":"2022-02-07T02:44:41.86415Z","iopub.status.idle":"2022-02-07T02:44:41.875369Z","shell.execute_reply.started":"2022-02-07T02:44:41.864114Z","shell.execute_reply":"2022-02-07T02:44:41.874367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\nprint(example[\"location_list\"])\nexample_loc_ints = loc_list_to_ints(example[\"location_list\"])[0]\nprint(example_loc_ints)\nprint(example[\"pn_history\"][example_loc_ints[0] : example_loc_ints[1]])","metadata":{"execution":{"iopub.status.busy":"2022-02-07T02:44:41.876757Z","iopub.execute_input":"2022-02-07T02:44:41.877042Z","iopub.status.idle":"2022-02-07T02:44:41.888247Z","shell.execute_reply.started":"2022-02-07T02:44:41.877002Z","shell.execute_reply":"2022-02-07T02:44:41.887462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"nice.","metadata":{}},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"../input/huggingface-bert/bert-base-uncased\" # we cant connect internet for submission\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T02:44:41.889657Z","iopub.execute_input":"2022-02-07T02:44:41.890045Z","iopub.status.idle":"2022-02-07T02:44:41.958775Z","shell.execute_reply.started":"2022-02-07T02:44:41.890006Z","shell.execute_reply":"2022-02-07T02:44:41.958026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_add_labels(tokenizer, example):\n    tokenized_inputs = tokenizer(\n        example[\"feature_text\"],\n        example[\"pn_history\"],\n        truncation = \"only_second\",\n        max_length = 416, # max length is 406\n        padding = \"max_length\",\n        return_offsets_mapping = True\n    )\n    labels = [0.0] * len(tokenized_inputs[\"input_ids\"])\n    tokenized_inputs[\"location_int\"] = loc_list_to_ints(example[\"location_list\"])\n    tokenized_inputs[\"sequence_ids\"] = tokenized_inputs.sequence_ids()\n\n    for idx, (seq_id, offsets) in enumerate(zip(tokenized_inputs[\"sequence_ids\"], tokenized_inputs[\"offset_mapping\"])):\n        if seq_id is None or seq_id == 0:\n            labels[idx] = -100\n            continue\n        exit = False\n        token_start, token_end = offsets\n        for feature_start, feature_end in tokenized_inputs[\"location_int\"]:\n            if exit:\n                break\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                exit = True\n    tokenized_inputs[\"labels\"] = labels\n    \n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2022-02-07T02:44:41.960192Z","iopub.execute_input":"2022-02-07T02:44:41.960546Z","iopub.status.idle":"2022-02-07T02:44:41.969643Z","shell.execute_reply.started":"2022-02-07T02:44:41.960505Z","shell.execute_reply":"2022-02-07T02:44:41.968716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_inputs = tokenize_and_add_labels(tokenizer, example)\nfor key in tokenized_inputs.keys():\n    print(key)\n    print(tokenized_inputs[key])\n    print(\"=\" * 100)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T02:44:41.971051Z","iopub.execute_input":"2022-02-07T02:44:41.971527Z","iopub.status.idle":"2022-02-07T02:44:41.985647Z","shell.execute_reply.started":"2022-02-07T02:44:41.971489Z","shell.execute_reply":"2022-02-07T02:44:41.984703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we need \"input_ids\" and \"attention_mask\" for BERT.\n\nlabels are 1.0 at annotation.\n\nso we can train as binary classification; is this word(token) represent the feature? -> 1 or 0","metadata":{}},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class NBMEData(torch.utils.data.Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = tokenize_and_add_labels(self.tokenizer, example)\n\n        input_ids = np.array(tokenized[\"input_ids\"]) # for input BERT\n        attention_mask = np.array(tokenized[\"attention_mask\"]) # for input BERT\n        labels = np.array(tokenized[\"labels\"]) # for calculate loss and cv score\n\n        offset_mapping = np.array(tokenized[\"offset_mapping\"]) # for calculate cv score\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\") # for calculate cv score\n        \n        return input_ids, attention_mask, labels, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-02-07T02:44:41.988376Z","iopub.execute_input":"2022-02-07T02:44:41.98863Z","iopub.status.idle":"2022-02-07T02:44:41.996454Z","shell.execute_reply.started":"2022-02-07T02:44:41.988595Z","shell.execute_reply":"2022-02-07T02:44:41.995466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class NBMEModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = AutoModel.from_pretrained(MODEL_NAME) # BERT model\n        self.dropout = torch.nn.Dropout(p = 0.2)\n        self.classifier = torch.nn.Linear(768, 1) # BERT has pooler_output(size: 768)\n    \n    def forward(self, input_ids, attention_mask):\n        pooler_outputs = self.backbone(input_ids = input_ids, attention_mask = attention_mask)[0] # idx 0 is pooler_output\n        logits = self.classifier(self.dropout(pooler_outputs)).squeeze(-1)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-02-07T02:44:41.998054Z","iopub.execute_input":"2022-02-07T02:44:41.998435Z","iopub.status.idle":"2022-02-07T02:44:42.006454Z","shell.execute_reply.started":"2022-02-07T02:44:41.9984Z","shell.execute_reply":"2022-02-07T02:44:42.005466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"fold = 0\nBATCH_SIZE = 16\nEPOCHS = 3\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = NBMEModel().to(DEVICE)\ncriterion = torch.nn.BCEWithLogitsLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr = 1e-5)\n\ntrain = df.loc[df[\"fold\"] != fold].reset_index(drop = True)\nvalid = df.loc[df[\"fold\"] == fold].reset_index(drop = True)\ntrain_ds = NBMEData(train, tokenizer)\nvalid_ds = NBMEData(valid, tokenizer)\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size = BATCH_SIZE, pin_memory = True, shuffle = True, drop_last = True)\nvalid_dl = torch.utils.data.DataLoader(valid_ds, batch_size = BATCH_SIZE * 2, pin_memory = True, shuffle = False, drop_last = False)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T02:44:42.008019Z","iopub.execute_input":"2022-02-07T02:44:42.008333Z","iopub.status.idle":"2022-02-07T02:44:52.636431Z","shell.execute_reply.started":"2022-02-07T02:44:42.008295Z","shell.execute_reply":"2022-02-07T02:44:52.635617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n = 1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef get_location_predictions(preds, offset_mapping, sequence_ids, test = False):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        pred = sigmoid(pred)\n        start_idx = None\n        current_preds = []\n        for p, o, s_id in zip(pred, offsets, seq_ids):\n            if s_id is None or s_id == 0:\n                continue\n            if p > 0.5:\n                if start_idx is None:\n                    start_idx = o[0]\n                end_idx = o[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n    return all_predictions\n\ndef calculate_char_CV(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros((num_chars))\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n        char_preds = np.zeros((num_chars))\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n    results = precision_recall_fscore_support(all_labels, all_preds, average = \"binary\")\n    return {\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }\n\ndef compute_metrics(p):\n    predictions, y_true = p\n    y_true = y_true.astype(int)\n    y_pred = [\n        [int(p > 0.5) for (p, l) in zip(pred, label) if l != -100]\n        for pred, label in zip(predictions, y_true)\n    ]\n    y_true = [\n        [l for l in label if l != -100] for label in y_true\n    ]\n    results = precision_recall_fscore_support(list(chain(*y_true)), list(chain(*y_pred)), average = \"binary\")\n    return {\n        \"token_precision\": results[0],\n        \"token_recall\": results[1],\n        \"token_f1\": results[2]\n    }","metadata":{"execution":{"iopub.status.busy":"2022-02-07T02:44:52.637664Z","iopub.execute_input":"2022-02-07T02:44:52.637936Z","iopub.status.idle":"2022-02-07T02:44:52.659724Z","shell.execute_reply.started":"2022-02-07T02:44:52.6379Z","shell.execute_reply":"2022-02-07T02:44:52.657156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = {\"train\": [], \"valid\": []}\nbest_loss = np.inf\n\nfor epoch in range(EPOCHS):\n    #training\n    model.train()\n    train_loss = AverageMeter()\n    pbar = tqdm(train_dl)\n    for batch in pbar:\n        optimizer.zero_grad()\n        input_ids = batch[0].to(DEVICE)\n        attention_mask = batch[1].to(DEVICE)\n        labels = batch[2].to(DEVICE)\n        offset_mapping = batch[3]\n        sequence_ids = batch[4]\n        logits = model(input_ids, attention_mask)\n        loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n        loss = loss_fct(logits, labels)\n        loss = torch.masked_select(loss, labels > -1).mean() # we should calculate at \"pn_history\"; labels at \"feature_text\" are -100 < -1\n        loss.backward()\n        optimizer.step()\n        train_loss.update(val = loss.item(), n = len(input_ids))\n        pbar.set_postfix(Loss = train_loss.avg)\n    print(epoch, train_loss.avg)\n    history[\"train\"].append(train_loss.avg)\n\n    #evaluation\n    model.eval()\n    valid_loss = AverageMeter()\n    with torch.no_grad():\n        for batch in tqdm(valid_dl):\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            labels = batch[2].to(DEVICE)\n            offset_mapping = batch[3]\n            sequence_ids = batch[4]\n            logits = model(input_ids, attention_mask)\n            loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n            loss = loss_fct(logits, labels)\n            loss = torch.masked_select(loss, labels > -1).mean()\n            valid_loss.update(val = loss.item(), n = len(input_ids))\n            pbar.set_postfix(Loss = valid_loss.avg)\n    print(epoch, valid_loss.avg)\n    history[\"valid\"].append(valid_loss.avg)\n\n    # save model\n    if valid_loss.avg < best_loss:\n        best_loss = valid_loss.avg\n        torch.save(model.state_dict(), \"nbme.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T02:44:52.660926Z","iopub.execute_input":"2022-02-07T02:44:52.6615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"nbme.pth\", map_location = DEVICE))\n\nmodel.eval()\npreds = []\noffsets = []\nseq_ids = []\nlbls = []\nwith torch.no_grad():\n    for batch in tqdm(valid_dl):\n        input_ids = batch[0].to(DEVICE)\n        attention_mask = batch[1].to(DEVICE)\n        labels = batch[2].to(DEVICE)\n        offset_mapping = batch[3]\n        sequence_ids = batch[4]\n        logits = model(input_ids, attention_mask)\n        preds.append(logits.cpu().numpy())\n        offsets.append(offset_mapping.numpy())\n        seq_ids.append(sequence_ids.numpy())\n        lbls.append(labels.cpu().numpy())\npreds = np.concatenate(preds, axis = 0)\noffsets = np.concatenate(offsets, axis = 0)\nseq_ids = np.concatenate(seq_ids, axis = 0)\nlbls = np.concatenate(lbls, axis = 0)\nlocation_preds = get_location_predictions(preds, offsets, seq_ids, test = False)\nscore = calculate_char_CV(location_preds, offsets, seq_ids, lbls)\nprint(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inrefence","metadata":{}},{"cell_type":"code","source":"def create_test_df():\n    feats = pd.read_csv(f\"{ROOT}/features.csv\")\n    notes = pd.read_csv(f\"{ROOT}/patient_notes.csv\")\n    test = pd.read_csv(f\"{ROOT}/test.csv\")\n\n    merged = test.merge(notes, how = \"left\")\n    merged = merged.merge(feats, how = \"left\")\n\n    def process_feature_text(text):\n        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    \n    print(merged.shape)\n    return merged","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NBMETestData(torch.utils.data.Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = self.tokenizer(\n            example[\"feature_text\"],\n            example[\"pn_history\"],\n            truncation = \"only_second\",\n            max_length = 416,\n            padding = \"max_length\",\n            return_offsets_mapping = True\n        )\n        tokenized[\"sequence_ids\"] = tokenized.sequence_ids()\n\n        input_ids = np.array(tokenized[\"input_ids\"])\n        attention_mask = np.array(tokenized[\"attention_mask\"])\n        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\")\n\n        return input_ids, attention_mask, offset_mapping, sequence_ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = create_test_df()\ntest_ds = NBMETestData(test, tokenizer)\ntest_dl = torch.utils.data.DataLoader(test_ds, batch_size = BATCH_SIZE * 2, pin_memory = True, shuffle = False, drop_last = False)\n\nmodel.eval()\npreds = []\noffsets = []\nseq_ids = []\nwith torch.no_grad():\n    for batch in tqdm(test_dl):\n        input_ids = batch[0].to(DEVICE)\n        attention_mask = batch[1].to(DEVICE)\n        offset_mapping = batch[2]\n        sequence_ids = batch[3]\n        logits = model(input_ids, attention_mask)\n        preds.append(logits.cpu().numpy())\n        offsets.append(offset_mapping.numpy())\n        seq_ids.append(sequence_ids.numpy())\n\npreds = np.concatenate(preds, axis = 0)\noffsets = np.concatenate(offsets, axis = 0)\nseq_ids = np.concatenate(seq_ids, axis = 0)\n\nlocation_preds = get_location_predictions(preds, offsets, seq_ids, test = True)\ntest[\"location\"] = location_preds\ntest[[\"id\", \"location\"]].to_csv(\"submission.csv\", index = False)\npd.read_csv(\"submission.csv\").head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}