{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py', \"deberta__init__.py\"]:\n    if str(filename).startswith(\"deberta\"):\n        filepath = deberta_v2_path/str(filename).replace(\"deberta\", \"\")\n    else:\n        filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:28:39.804737Z","iopub.execute_input":"2022-05-10T05:28:39.805149Z","iopub.status.idle":"2022-05-10T05:28:39.860738Z","shell.execute_reply.started":"2022-05-10T05:28:39.805048Z","shell.execute_reply":"2022-05-10T05:28:39.860074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:28:39.862469Z","iopub.execute_input":"2022-05-10T05:28:39.862944Z","iopub.status.idle":"2022-05-10T05:28:39.867317Z","shell.execute_reply.started":"2022-05-10T05:28:39.862893Z","shell.execute_reply":"2022-05-10T05:28:39.866211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{"papermill":{"duration":0.015756,"end_time":"2021-11-16T19:32:29.999526","exception":false,"start_time":"2021-11-16T19:32:29.98377","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CFG:\n    model=\"microsoft/deberta-v3-large\"\n    epochs=3\n    encoder_lr=2e-5\n    decoder_lr=2e-5\n#     epochs=4\n#     encoder_lr=3e-5\n#     decoder_lr=3e-5\n    batch_size=4\n    valid_batch_size=32\n    max_len=512\n    trn_fold=[4]\n\n    apex=True\n    print_freq=100\n    num_workers=2\n    scheduler='cosine' # ['linear', 'cosine']\n    batch_scheduler=True\n    num_cycles=0.5\n    num_warmup_steps=0\n    eps=1e-8\n    betas=(0.9, 0.999)\n    fc_dropout=0.2\n    weight_decay=0.0001\n    gradient_accumulation_steps=1\n    max_grad_norm=1000\n    seed=1119\n    n_fold=5","metadata":{"papermill":{"duration":0.02543,"end_time":"2021-11-16T19:32:30.040766","exception":false,"start_time":"2021-11-16T19:32:30.015336","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-10T05:28:39.868748Z","iopub.execute_input":"2022-05-10T05:28:39.86908Z","iopub.status.idle":"2022-05-10T05:28:39.877868Z","shell.execute_reply.started":"2022-05-10T05:28:39.869046Z","shell.execute_reply":"2022-05-10T05:28:39.877086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\n# warnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"papermill":{"duration":30.77583,"end_time":"2021-11-16T19:33:11.013554","exception":false,"start_time":"2021-11-16T19:32:40.237724","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-10T05:28:39.879176Z","iopub.execute_input":"2022-05-10T05:28:39.879512Z","iopub.status.idle":"2022-05-10T05:28:48.138876Z","shell.execute_reply.started":"2022-05-10T05:28:39.879478Z","shell.execute_reply":"2022-05-10T05:28:48.138112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n    EX: print(spans_to_binary([[0, 5], [10, 15]], length=30))\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n    return binary\n\ndef micro_f1(preds, truths):\n    \"\"\"\n    EX:\n    preds = [[0, 0, 1], [0, 0, 0]]\n    truths = [[0, 0, 1], [1, 0, 0]]\n    micro_f1(preds, truths)\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    return f1_score(truths, preds)\n\n\n# 以整体为单位，内部用for一个句子一个句子提取\ndef span_micro_f1(preds, truths):\n    \"\"\"\n    EX: \n    pred = [[[1, 2]], [[3, 4]]]\n    truth = [[[1, 2]], [[3, 6]]]\n    span_micro_f1(pred, truth)  # 每个句子是二维\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    count_diff = 0\n    for i, (pred, truth) in enumerate(zip(preds, truths)):\n        if not len(pred) and not len(truth):\n            continue\n\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        \n#         print(f\"pred: {pred}\")\n#         print(f\"truth: {truth}\")\n#         print('_' * 50)\n#         time.sleep(1)\n        \n        bin_pred = spans_to_binary(pred, length)\n        bin_truth = spans_to_binary(truth, length)\n        if (bin_pred != bin_truth).any():\n            count_diff += 1\n#             print(i)\n#             print(f\"pred: {pred}\")\n#             print(f\"truth: {truth}\")\n#             print('_' * 50)\n#             time.sleep(2)\n        bin_preds.append(bin_pred)\n        bin_truths.append(bin_truth)\n    print(f\"count_diff: {count_diff}\")\n    return micro_f1(bin_preds, bin_truths)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:28:48.141911Z","iopub.execute_input":"2022-05-10T05:28:48.142113Z","iopub.status.idle":"2022-05-10T05:28:48.153311Z","shell.execute_reply.started":"2022-05-10T05:28:48.142087Z","shell.execute_reply":"2022-05-10T05:28:48.1524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_char_probs(texts, predictions, tokenizer):\n    \"\"\"\n    将预测的token的标签还原成字符，因为原数据给的是字符级别的\n    \"\"\"\n    char_probs = [np.zeros(len(text)) for text in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        inputs = tokenizer(text, return_offsets_mapping=True)\n        for k, (offset_mapping, pred) in enumerate(zip(inputs['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            char_probs[i][start: end] = pred  # 将token对应的所有字符都打上和token一样的标签\n        \n        # 如果当前位置是空格，且上一个字符和下一个字符的预测概率都大于阈值，则连起来\n        for j, char_prob in enumerate(char_probs[i]):\n            if j > 0 and j < len(char_probs[i]) - 1:\n                if texts[i][j] == ' ':\n                    if char_probs[i][j-1] >= 0.5 and char_probs[i][j+1] >= 0.5:\n                        char_probs[i][j] = 1.0\n                    elif char_probs[i][j-1] < 0.5 or char_probs[i][j+1] < 0.5:\n                        char_probs[i][j] = 0.0\n    return char_probs\n\n\ndef get_results(char_probs, th=0.5):\n    \"\"\"\n    将char_prob通过threshold转换成 location 的形式（即span, 例：'70 91;176 183' 字符串的形式）\n    分组：\n    [list(g) for _, g in itertools.groupby([0, 2, 3, 8, 9], key=lambda n, c=itertools.count(): n - next(c))]    \n    输出：[[0], [2, 3], [8, 9]]\n    \"\"\"\n    results = []\n    for char_prob in char_probs: \n        label_idx = np.where(char_prob >= th)[0]  # 是1的索引\n        span_list = [list(g) for _, g in itertools.groupby(label_idx, key=lambda n, c=itertools.count(): n - next(c))] \n        result = [f\"{span[0]} {span[-1] + 1}\" for span in span_list]  # 因为原location是左闭右开，所以右边界要 + 1\n        result = \";\".join(result)\n        results.append(result)\n    return results\n\n\ndef get_predictions(results):\n    \"\"\"\n    将rusults转换成 [[70, 91], [176, 183]] 列表的形式\n    \"\"\"\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n    return predictions\n\ndef create_labels_for_scoring(df):\n    \"\"\"\n    将ground_truth的location转换成 [[70, 91], [176, 183]] 列表的形式\n    \"\"\"\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n#         if len(location_list) > 0:\n        if len(location_list[0]) > 0:\n            location = location_list[0]\n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n    return truths","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:28:48.154843Z","iopub.execute_input":"2022-05-10T05:28:48.155521Z","iopub.status.idle":"2022-05-10T05:28:48.180055Z","shell.execute_reply.started":"2022-05-10T05:28:48.155415Z","shell.execute_reply":"2022-05-10T05:28:48.179267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def get_score(y_true, y_pred):\n    score = span_micro_f1(y_true, y_pred)\n    return score\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CFG.seed)","metadata":{"papermill":{"duration":0.034649,"end_time":"2021-11-16T19:33:11.105766","exception":false,"start_time":"2021-11-16T19:33:11.071117","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-10T05:28:48.183004Z","iopub.execute_input":"2022-05-10T05:28:48.183186Z","iopub.status.idle":"2022-05-10T05:28:48.196265Z","shell.execute_reply.started":"2022-05-10T05:28:48.183164Z","shell.execute_reply":"2022-05-10T05:28:48.195589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{"papermill":{"duration":0.018406,"end_time":"2021-11-16T19:33:11.150174","exception":false,"start_time":"2021-11-16T19:33:11.131768","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# train = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')\ntrain = pd.read_csv('../input/nbme-train-new/train_new.csv', usecols=['id', 'case_num', 'pn_num', 'feature_num', 'annotation', 'location'])\nfeatures= pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\npatient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n\ntrain['annotation'] = train['annotation'].apply(ast.literal_eval)\ntrain['location'] = train['location'].apply(ast.literal_eval)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:28:48.197607Z","iopub.execute_input":"2022-05-10T05:28:48.197999Z","iopub.status.idle":"2022-05-10T05:28:49.297855Z","shell.execute_reply.started":"2022-05-10T05:28:48.197881Z","shell.execute_reply":"2022-05-10T05:28:49.297142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n\ntrain = train.merge(features, on=['feature_num', 'case_num'], how='left')\ntrain = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:28:49.299205Z","iopub.execute_input":"2022-05-10T05:28:49.299465Z","iopub.status.idle":"2022-05-10T05:28:49.332571Z","shell.execute_reply.started":"2022-05-10T05:28:49.299431Z","shell.execute_reply":"2022-05-10T05:28:49.331933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# incorrect annotation\ntrain.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\ntrain.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n\ntrain.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\ntrain.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n\ntrain.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\ntrain.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n\ntrain.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\ntrain.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n\ntrain.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\ntrain.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n\ntrain.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\ntrain.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n\ntrain.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\ntrain.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n\ntrain.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\ntrain.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n\ntrain.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\ntrain.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n\ntrain.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\ntrain.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n\ntrain.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\ntrain.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n\ntrain.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\ntrain.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n\ntrain.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\ntrain.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n\ntrain.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\ntrain.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n\ntrain.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\ntrain.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n\ntrain.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\ntrain.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n\ntrain.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\ntrain.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n\ntrain.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\ntrain.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n\ntrain.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\ntrain.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n\ntrain.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\ntrain.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n\ntrain.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\ntrain.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n\ntrain.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\ntrain.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n\ntrain.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\ntrain.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n\ntrain.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\ntrain.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n\ntrain.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\ntrain.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n\ntrain.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\ntrain.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n\ntrain.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\ntrain.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n\ntrain.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\ntrain.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n\ntrain.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\ntrain.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n\ntrain.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\ntrain.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n\ntrain.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\ntrain.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n\ntrain.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\ntrain.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n\ntrain.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\ntrain.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n\ntrain.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\ntrain.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n\ntrain.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\ntrain.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n\ntrain.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\ntrain.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n\ntrain.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\ntrain.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n\ntrain.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\ntrain.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n\ntrain.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\ntrain.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n\ntrain.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\ntrain.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n\ntrain.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\ntrain.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n\ntrain.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\ntrain.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:28:49.334091Z","iopub.execute_input":"2022-05-10T05:28:49.334362Z","iopub.status.idle":"2022-05-10T05:28:49.43167Z","shell.execute_reply.started":"2022-05-10T05:28:49.334317Z","shell.execute_reply":"2022-05-10T05:28:49.431025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['annotation_length'] = train['annotation'].apply(len)\ndisplay(train['annotation_length'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:28:49.432617Z","iopub.execute_input":"2022-05-10T05:28:49.432802Z","iopub.status.idle":"2022-05-10T05:28:49.449103Z","shell.execute_reply.started":"2022-05-10T05:28:49.43278Z","shell.execute_reply":"2022-05-10T05:28:49.448408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train[train['annotation_length'] != 0].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:28:49.450201Z","iopub.execute_input":"2022-05-10T05:28:49.450878Z","iopub.status.idle":"2022-05-10T05:28:49.460147Z","shell.execute_reply.started":"2022-05-10T05:28:49.450815Z","shell.execute_reply":"2022-05-10T05:28:49.459138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CV split","metadata":{"papermill":{"duration":0.02099,"end_time":"2021-11-16T19:33:11.849001","exception":false,"start_time":"2021-11-16T19:33:11.828011","status":"completed"},"tags":[]}},{"cell_type":"code","source":"Fold = GroupKFold(n_splits=CFG.n_fold)\ngroups = train['pn_num'].values\nfor n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n    train.loc[val_index, 'fold'] = int(n)\ntrain['fold'] = train['fold'].astype(int)\ndisplay(train.groupby('fold').size())","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:28:49.461696Z","iopub.execute_input":"2022-05-10T05:28:49.462032Z","iopub.status.idle":"2022-05-10T05:28:49.48523Z","shell.execute_reply.started":"2022-05-10T05:28:49.461997Z","shell.execute_reply":"2022-05-10T05:28:49.484636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tokenizer","metadata":{"papermill":{"duration":0.02017,"end_time":"2021-11-16T19:33:11.963889","exception":false,"start_time":"2021-11-16T19:33:11.943719","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from transformers.models.deberta_v2 import DebertaV2TokenizerFast\ntokenizer = DebertaV2TokenizerFast.from_pretrained(CFG.model)\ntokenizer.save_pretrained(OUTPUT_DIR + 'tokenizer/')\nCFG.tokenizer = tokenizer","metadata":{"papermill":{"duration":8.833709,"end_time":"2021-11-16T19:33:20.817889","exception":false,"start_time":"2021-11-16T19:33:11.98418","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-10T05:28:49.488978Z","iopub.execute_input":"2022-05-10T05:28:49.48916Z","iopub.status.idle":"2022-05-10T05:28:55.577506Z","shell.execute_reply.started":"2022-05-10T05:28:49.489138Z","shell.execute_reply":"2022-05-10T05:28:55.576736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"# for text_col in ['pn_history']:\n#     pn_history_lengths = []\n#     tk0 = tqdm(patient_notes[text_col].fillna(\"\").values, total=len(patient_notes))\n#     for text in tk0:\n#         length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n#         pn_history_lengths.append(length)\n#     print(f'{text_col} max(lengths): {max(pn_history_lengths)}')\n\n# for text_col in ['feature_text']:\n#     features_lengths = []\n#     tk0 = tqdm(features[text_col].fillna(\"\").values, total=len(features))\n#     for text in tk0:\n#         length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n#         features_lengths.append(length)\n#     print(f'{text_col} max(lengths): {max(features_lengths)}')\n\n# CFG.max_len = max(pn_history_lengths) + max(features_lengths) + 3 # cls & sep & sep\n# print(f\"max_len: {CFG.max_len}\")\nCFG.max_len = 354","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:28:55.578871Z","iopub.execute_input":"2022-05-10T05:28:55.579578Z","iopub.status.idle":"2022-05-10T05:28:55.584732Z","shell.execute_reply.started":"2022-05-10T05:28:55.579539Z","shell.execute_reply":"2022-05-10T05:28:55.583693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text, feature_text):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           max_length=CFG.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False,\n                           truncation='only_first')\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\ndef create_label(cfg, text, annotation_length, location_list):\n    inputs = tokenizer(text, max_length=CFG.max_len, padding=\"max_length\", return_offsets_mapping=True, truncation='only_first')\n    \n    offset_mapping = inputs['offset_mapping']\n    label = np.zeros(len(offset_mapping))\n    not_text = np.where(np.array(inputs.sequence_ids()) != 0)[0]\n    is_text = np.where(np.array(inputs.sequence_ids()) == 0)[0]\n    label[not_text] = -1\n    \n#     if not location_list:\n    if not location_list or not location_list[0]:\n        return torch.tensor(label, dtype=torch.float)\n    \n    location_new = []\n    for loc in location_list:\n        if len(loc.split(';')) > 1:  # 对于同一个特征不是连续的词，用;隔开的情况，如['1 2;5 6']\n            location_new.extend([lo.split() for lo in loc.split(';')])\n        else:\n            location_new.append(loc.split())\n            \n    for i in is_text:\n        span = offset_mapping[i]\n        for loc in location_new:\n            loc_left = int(loc[0])\n            loc_right = int(loc[1])\n            # 分词后没有包含空格时：如 bert:\n            if span[0] >= loc_left and span[1] <= loc_right:  # 1. span再location里时\n                label[i] = 1\n                break\n            elif span[0] <= loc_left < span[1] or span[0] < loc_right <= span[1]:  # 2. span在location外时\n                label[i] = 1\n                break\n            \n    return torch.tensor(label, dtype=torch.float)","metadata":{"papermill":{"duration":0.040128,"end_time":"2021-11-16T19:33:20.931029","exception":false,"start_time":"2021-11-16T19:33:20.890901","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-10T05:28:55.586204Z","iopub.execute_input":"2022-05-10T05:28:55.586526Z","iopub.status.idle":"2022-05-10T05:28:55.602228Z","shell.execute_reply.started":"2022-05-10T05:28:55.586488Z","shell.execute_reply":"2022-05-10T05:28:55.601549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n        self.annotation_lengths = df['annotation_length'].values\n        self.locations = df['location'].values\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item])\n        label = create_label(self.cfg, \n                             self.pn_historys[item], \n                             self.annotation_lengths[item], \n                             self.locations[item])\n        return inputs, label","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:28:55.605083Z","iopub.execute_input":"2022-05-10T05:28:55.605761Z","iopub.status.idle":"2022-05-10T05:28:55.613491Z","shell.execute_reply.started":"2022-05-10T05:28:55.605727Z","shell.execute_reply":"2022-05-10T05:28:55.612703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.02209,"end_time":"2021-11-16T19:33:20.978793","exception":false,"start_time":"2021-11-16T19:33:20.956703","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n#         self.model.resize_token_embeddings(len(tokenizer))\n        \n#         self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, inputs):\n        last_hidden_states = self.model(**inputs)[0]\n#         out = self.layer_norm(last_hidden_states)\n        out = self.fc(self.fc_dropout(last_hidden_states))\n        return out","metadata":{"papermill":{"duration":0.032939,"end_time":"2021-11-16T19:33:21.034275","exception":false,"start_time":"2021-11-16T19:33:21.001336","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-10T05:28:55.616453Z","iopub.execute_input":"2022-05-10T05:28:55.618166Z","iopub.status.idle":"2022-05-10T05:28:55.63039Z","shell.execute_reply.started":"2022-05-10T05:28:55.618138Z","shell.execute_reply":"2022-05-10T05:28:55.629592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helpler functions","metadata":{"papermill":{"duration":0.022058,"end_time":"2021-11-16T19:33:21.081885","exception":false,"start_time":"2021-11-16T19:33:21.059827","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:28:55.632549Z","iopub.execute_input":"2022-05-10T05:28:55.632804Z","iopub.status.idle":"2022-05-10T05:28:55.642027Z","shell.execute_reply.started":"2022-05-10T05:28:55.632771Z","shell.execute_reply":"2022-05-10T05:28:55.641263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    model.train()\n    losses = AverageMeter()\n    start = end = time.time()\n    global_step = 0\n    for step, (inputs, labels) in enumerate(train_loader):\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        y_preds = model(inputs)\n        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n        \n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        loss.backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            global_step += 1\n            if CFG.batch_scheduler:\n                scheduler.step()\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print(f\"Epoch:[{epoch+1}/{CFG.epochs}], batch:[{step}/{len(train_loader)}], maxlen:{CFG.max_len}, Elapsed:{timeSince(start, float(step+1) / len(train_loader)):s}, Loss: {losses.avg:.5f}, Grad: {grad_norm:.4f}, LR: {optimizer.param_groups[0]['lr']:.8f}\")\n    return losses.avg\n\n\ndef valid_fn(valid_loader, model, criterion, device):\n    losses = AverageMeter()\n    model.eval()\n    preds = []\n    start = end = time.time()\n    for step, (inputs, labels) in enumerate(valid_loader):\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print(f\"EVAL: [{step}/{len(valid_loader)}], Elapsed {timeSince(start, float(step+1)/len(valid_loader)):s}, Loss: {losses.avg:.5f}\")\n \n    predictions = np.concatenate(preds)\n    return losses.avg, predictions\n\n\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"papermill":{"duration":0.044153,"end_time":"2021-11-16T19:33:21.148373","exception":false,"start_time":"2021-11-16T19:33:21.10422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-10T05:28:55.643381Z","iopub.execute_input":"2022-05-10T05:28:55.64391Z","iopub.status.idle":"2022-05-10T05:28:55.663881Z","shell.execute_reply.started":"2022-05-10T05:28:55.643875Z","shell.execute_reply":"2022-05-10T05:28:55.663209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(folds, fold):\n    \n    print(f\"========== fold: {fold} training ==========\")\n\n    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n    valid_texts = valid_folds['pn_history'].values\n    valid_labels = create_labels_for_scoring(valid_folds)\n    \n    train_dataset = TrainDataset(CFG, train_folds)\n    valid_dataset = TrainDataset(CFG, valid_folds)\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=True,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)  # false\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=CFG.valid_batch_size,\n                              shuffle=False,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n\n    model = CustomModel(CFG, config_path=None, pretrained=True)    \n    torch.save(model.config, OUTPUT_DIR+'config.pth')  # 从model保存 model_config\n    model.to(device)\n    \n    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': weight_decay},\n            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': 0.0},\n            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n             'lr': decoder_lr, 'weight_decay': 0.0}\n        ]\n        return optimizer_parameters\n\n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=CFG.encoder_lr, \n                                                decoder_lr=CFG.decoder_lr,\n                                                weight_decay=CFG.weight_decay)\n    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n    \n    def get_scheduler(cfg, optimizer, num_train_steps):\n        if cfg.scheduler=='linear':\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n            )\n        elif cfg.scheduler=='cosine':\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n            )\n        return scheduler\n    \n    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n\n    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n    \n    best_score = 0.\n\n    for epoch in range(CFG.epochs):\n\n        start_time = time.time()\n\n        # train\n        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n        \n        # eval\n        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n        predictions = predictions.reshape((len(valid_folds), CFG.max_len))\n        \n        # scoring from valid set\n        char_probs = get_char_probs(valid_texts, predictions, CFG.tokenizer)  \n        results = get_results(char_probs, th=0.5)\n        preds = get_predictions(results)\n        score = get_score(valid_labels, preds)\n\n        elapsed = time.time() - start_time\n\n        print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.5f}  avg_val_loss: {avg_val_loss:.5f}  time: {elapsed:.0f}s')\n        print(f'Epoch {epoch+1} - Score: {score:.5f}')\n        \n        if best_score < score:\n            best_score = score\n            print(f'Epoch {epoch+1} - Save Best Score: {best_score:.5f} Model')\n            torch.save({'model': model.state_dict(),\n                        'predictions': predictions},\n                        OUTPUT_DIR + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n\n    predictions = torch.load(OUTPUT_DIR + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", map_location=torch.device('cpu'))['predictions']\n    valid_folds[[i for i in range(CFG.max_len)]] = predictions\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return valid_folds","metadata":{"papermill":{"duration":0.051076,"end_time":"2021-11-16T19:33:21.225819","exception":false,"start_time":"2021-11-16T19:33:21.174743","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-10T05:28:55.666708Z","iopub.execute_input":"2022-05-10T05:28:55.667105Z","iopub.status.idle":"2022-05-10T05:28:55.688919Z","shell.execute_reply.started":"2022-05-10T05:28:55.667066Z","shell.execute_reply":"2022-05-10T05:28:55.68823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    \n    def get_result(oof_df):\n        labels = create_labels_for_scoring(oof_df)\n        predictions = oof_df[[i for i in range(CFG.max_len)]].values\n        char_probs = get_char_probs(oof_df['pn_history'].values, predictions, CFG.tokenizer)\n        results = get_results(char_probs, th=0.5)\n        preds = get_predictions(results)\n        score = get_score(labels, preds)\n        print(f'Score: {score:<.5f}')\n    \n    oof_df = pd.DataFrame()\n    for fold in range(CFG.n_fold):\n        if fold in CFG.trn_fold:\n            _oof_df = train_loop(train, fold)\n            oof_df = pd.concat([oof_df, _oof_df])\n            print(f\"========== fold: {fold} result ==========\")\n            get_result(_oof_df)\n    oof_df = oof_df.reset_index(drop=True)\n    print(f\"========== CV ==========\")\n    get_result(oof_df)\n    oof_df.to_pickle(OUTPUT_DIR + 'oof_df.pkl')","metadata":{"papermill":{"duration":4899.627687,"end_time":"2021-11-16T20:55:00.880155","exception":false,"start_time":"2021-11-16T19:33:21.252468","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-10T05:28:55.690422Z","iopub.execute_input":"2022-05-10T05:28:55.690849Z","iopub.status.idle":"2022-05-10T07:28:06.684634Z","shell.execute_reply.started":"2022-05-10T05:28:55.690816Z","shell.execute_reply":"2022-05-10T07:28:06.683794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink \nFileLink(r'./microsoft-deberta-v3-large_fold4_best.pth')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T07:28:48.286453Z","iopub.execute_input":"2022-05-10T07:28:48.287182Z","iopub.status.idle":"2022-05-10T07:28:48.29254Z","shell.execute_reply.started":"2022-05-10T07:28:48.287144Z","shell.execute_reply":"2022-05-10T07:28:48.291618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n2e-5, epoch 3, fold 0:\ncount_diff: 670\nEpoch 3 - avg_train_loss: 0.00624  avg_val_loss: 0.01063  time: 2410s\nEpoch 3 - Score: 0.88539\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-10T07:28:06.68621Z","iopub.execute_input":"2022-05-10T07:28:06.686657Z","iopub.status.idle":"2022-05-10T07:28:06.693737Z","shell.execute_reply.started":"2022-05-10T07:28:06.686618Z","shell.execute_reply":"2022-05-10T07:28:06.69301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n3e-5, epoch 4\nfold 0:\nEpoch 4 - avg_train_loss: 0.00447  avg_val_loss: 0.01256\nEpoch 4 - Score: 0.88294\n\n\nfold 1:\ncount_diff: 634\nEpoch 3 - avg_train_loss: 0.00701  avg_val_loss: 0.01207\nEpoch 3 - Score: 0.88259\n\nfold 2:\ncount_diff: 696\nEpoch 4 - avg_train_loss: 0.00491  avg_val_loss: 0.01241\nEpoch 4 - Score: 0.88047\nEpoch 4 - Save Best Score: 0.88047 Model\n\nfold 3:\ncount_diff: 664\nEpoch 4 - avg_train_loss: 0.00424  avg_val_loss: 0.01338\nEpoch 4 - Score: 0.87419\n\n\nfold4: \nweight_decay: 0.01\ncount_diff: 679\nEpoch 4 - avg_train_loss: 0.00406  avg_val_loss: 0.01271\nEpoch 4 - Score: 0.88178\n\nweight_decay: 0.0001\nEpoch 4 - avg_train_loss: 0.00408  avg_val_loss: 0.01286\nEpoch 4 - Score: 0.88392\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-10T07:28:06.694874Z","iopub.execute_input":"2022-05-10T07:28:06.695194Z","iopub.status.idle":"2022-05-10T07:28:06.705273Z","shell.execute_reply.started":"2022-05-10T07:28:06.695157Z","shell.execute_reply":"2022-05-10T07:28:06.704563Z"},"trusted":true},"execution_count":null,"outputs":[]}]}