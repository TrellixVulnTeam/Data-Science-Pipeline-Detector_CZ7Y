{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-14T13:51:50.76767Z","iopub.execute_input":"2022-02-14T13:51:50.768492Z","iopub.status.idle":"2022-02-14T13:51:50.800529Z","shell.execute_reply.started":"2022-02-14T13:51:50.768386Z","shell.execute_reply":"2022-02-14T13:51:50.799428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" free semantically annotated corpus that anyone can edit!\n\nThe Groningen Meaning Bank (GMB), developed at the University of Groningen, comprises thousands of texts in raw and tokenised format, tags for part of speech, named entities and lexical categories, and discourse representation structures compatible with first-order logic. The current (development) version of the GMB is accessible via the GMB Explorer. The multi-lingual successor of the GMB is the Parallel Meaning Bank.","metadata":{}},{"cell_type":"markdown","source":"CoNLL format\nhttps://www.clips.uantwerpen.be/conll2002/ner/\nSoftware and Data\n\nThe data consists of two columns separated by a single space. Each word has been put on a separate line and there is an empty line after each sentence. The first item on each line is a word and the second the named entity tag. The tags have the same format as in the chunking task: a B denotes the first item of a phrase and an I any non-initial word. There are four types of phrases: person names (PER), organizations (ORG), locations (LOC) and miscellaneous names (MISC). Here is an example:\n\n        Wolff B-PER\n            , O\n    currently O\n            a O\n   journalist O\n           in O\n    Argentina B-LOC\n            , O\n       played O\n         with O\n          Del B-PER\n       Bosque I-PER\n           in O\n          the O\n        final O\n        years O\n           of O\n          the O\n    seventies O\n           in O\n         Real B-ORG\n       Madrid I-ORG\n            . O\n\nThe data consists of three files per language: one training file and two test files testa and testb. The first test file will be used in the development phase for finding good parameters for the learning system. The second test file will be used for the final evaluation.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport json\nimport glob\nimport re\nfrom tqdm import tqdm\nimport ast","metadata":{"execution":{"iopub.status.busy":"2022-02-14T13:51:50.8026Z","iopub.execute_input":"2022-02-14T13:51:50.803484Z","iopub.status.idle":"2022-02-14T13:51:50.808949Z","shell.execute_reply.started":"2022-02-14T13:51:50.80344Z","shell.execute_reply":"2022-02-14T13:51:50.807941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Data","metadata":{}},{"cell_type":"code","source":"features = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\nfeatures.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T13:51:50.810575Z","iopub.execute_input":"2022-02-14T13:51:50.811086Z","iopub.status.idle":"2022-02-14T13:51:50.844571Z","shell.execute_reply.started":"2022-02-14T13:51:50.811042Z","shell.execute_reply":"2022-02-14T13:51:50.84401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\ntrain = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')\ntrain['annotation'] = train['annotation'].apply(ast.literal_eval)\ntrain['location'] = train['location'].apply(ast.literal_eval)\nfeatures = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"#for testing do something\n    return features\nfeatures = preprocess_features(features)\npatient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n\nprint(f\"train.shape: {train.shape}\")\ndisplay(train.head())\nprint(f\"features.shape: {features.shape}\")\ndisplay(features.head())\nprint(f\"patient_notes.shape: {patient_notes.shape}\")\ndisplay(patient_notes.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-14T13:51:50.846124Z","iopub.execute_input":"2022-02-14T13:51:50.846456Z","iopub.status.idle":"2022-02-14T13:51:51.865418Z","shell.execute_reply.started":"2022-02-14T13:51:50.846428Z","shell.execute_reply":"2022-02-14T13:51:51.864813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T13:51:51.866541Z","iopub.execute_input":"2022-02-14T13:51:51.867247Z","iopub.status.idle":"2022-02-14T13:51:51.8826Z","shell.execute_reply.started":"2022-02-14T13:51:51.867204Z","shell.execute_reply":"2022-02-14T13:51:51.881425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Extract positions from dataframe location field. \nStackoverflow:\nhttps://stackoverflow.com/questions/4289331/how-to-extract-numbers-from-a-string-in-python\n","metadata":{}},{"cell_type":"code","source":"import re\ndef extract_numbers(x):\n        sentence = x\n        s = [float(s) for s in re.findall(r'-?\\d+\\.?\\d*', sentence)]\n        return s","metadata":{"execution":{"iopub.status.busy":"2022-02-14T13:51:51.884642Z","iopub.execute_input":"2022-02-14T13:51:51.884973Z","iopub.status.idle":"2022-02-14T13:51:51.896402Z","shell.execute_reply.started":"2022-02-14T13:51:51.884929Z","shell.execute_reply":"2022-02-14T13:51:51.895599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Transformation of annotations spans from char form to token/word level.\nThat is, if in the competition the annotations are presented in the form of the location-character-start location-character-stop  After the transformation, the format will be: annotation to token position x (where the position is word or character space in the present implementation)\n#### Spacy training data format for Named Entity Recognition (NER):\n\nThe format is a list of tuple: text and dictionary of spans.\n\nTRAIN_DATA = [\n\n('Who is Shaka Khan?', {'entities': [(7, 17, 'PERSON')]}),\n\n('I like London and Berlin.', {'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]})\n\n]\n\nDue to the fact that spans are in char level there are some limitation: \n* tokenisation is limited to none :) every character including white space is tokenized. \n* it is not wise to split the sentences befeore geting the span value at token level.\n\n#### Two set of labels : features and cases. \n* Features were coded using the feature number. Feature 0 is coded for exemple : NBME0\n* Cases were coded using the case number. Case 0 is coded for exemple : NBME-C-0 . the file with case numbers is used to test the model.(fewer labels -more cases- a better validation split) Considerig thata therea are some features with very few cases, simple validation without CV is challenging. \n\n#### Why loops instead of pandas ... For clarity. \nIt easier for me to see ..\n\nThere are zillions better ways to do the conversion includig spacy conll convertor. The Python motto \"batteries included\" is meant to convey the idea that Python comes with everything you need. But...\nA single line break make the diference between a **F1** score of **0.1** and **0.69**\n\n#### Whatâ€™s the Point?\n\nNewbie perspective: same sentence repeated hundred times with different labels: the most extreme case: one label per text has catrastofic results.\n\nhttps://stackoverflow.com/questions/67055339/how-to-handle-repeating-text-data-but-with-different-labels-or-classes\n\nWhat the script do: tag each individual pacient note with spans and labels coreponding to each feature. \n\nAt the end the text look like this:\n\n[['17 Y/O M CAME TO THE CLINIC C/O HEART POUNDING. STARTED 2-3 MO AGO. IT STARTED SUDDENLY. DOES NOT RECALL ANY TRIGGERING EVENTS. IT COMES AND GOES, IT HAPPENED 5-6 TIMES SINCE IT STARTED. IT LASTS 3-4 MIN, AFTER THAN  JUST GOES AWAY. HE HAS ALSO EXPERIENCING SOB, PRESSURE ON HER CHEST WHEN HE HAS THIS ATTACK. HE IS A COLLEGE STUDENT, EXPERIENCING SOME STRESS RECENTLY. \\r\\nDENIES COUGH, CHEST PAIN.\\r\\nROS NEG EXCEPT AS ABOVE.\\r\\nPMH NONE. MEDS ATEROL, FOR HIS STUDIES, SHARING W HIS ROOMMATE. NKDA.\\r\\nPSH/ HOSP/ TRAVEL/ TRAUMA NONE.\\r\\nFH MOM HAS THYROID PROBLEMS.\\r\\nSH SEX ACTIVE W GIRLFRIEND, NO STDS, USING CONDOMS. SMOKE NONE. ETOH ONLY WEEKENDS. DRUG ONLY ONCE, 1 MO AGO.',\n\n  {'entities': [[532, 556, 'NBME1'],\n  \n    [263, 284, 'NBME2'],\n    \n    [131, 145, 'NBME3'],\n    \n    [150, 168, 'NBME3'],\n    \n    [258, 261, 'NBME7'],\n    [32, 46, 'NBME9'],\n    \n    [48, 66, 'NBME10'],\n    \n    [0, 6, 'NBME11'],\n    \n    [7, 8, 'NBME12']]}],\n    \n#### The tuple are saved in a spacy like format. \nTheoretically the results can be imported in spacy and used for NER labeleing from within spacy. But... there is a lot to clean ... :)","metadata":{}},{"cell_type":"code","source":"alpha=0\n\nTRAIN_DATA3 = []\nTRAIN_DATA4 = []\ndumb_data=0\n#for i in range(150):\nfor i in range(len(patient_notes)):\n    #print(i)\n    pn_num=patient_notes.pn_num[i]\n    text=str(patient_notes.pn_history[i])\n    temp_df=pd.DataFrame()\n    temp_df=train[train[\"pn_num\"]==pn_num]\n    temp_df=temp_df.reset_index()\n    temp_df['location_string']=temp_df['location'].apply(lambda x: str(x))\n    temp_df['spans']=temp_df['location_string'].apply(lambda x: extract_numbers(x))\n    temp_df['span_length'] = temp_df['spans'].apply(lambda x: len(x))\n    if len(temp_df)!=0:\n        #contor_lista=working_corpus.annotation_length[i]\n            ent_dic = {}\n            ent_dic2 = {}\n            lista1=[]\n            lista2=[]\n            for j in range(len(temp_df)):\n                #print(\"sunt j\",j)\n                temp_df.feature_num[j]\n                code_is=\"NBME\"+str(temp_df[\"feature_num\"][j])\n                code_is2=\"NBME\"+\"-C-\"+str(temp_df[\"case_num\"][j])\n                #extract spans\n                spans=[]\n                alpha=int((temp_df.span_length[j])/2)\n                k=0\n\n                for m in range(alpha):\n                        #print('sunt m',m)\n\n                        spans=temp_df[\"spans\"][j]\n\n                        #print('spansx',spans[k])\n                        #print('spansy',spans[k+1])\n                        #print(\"lista este\",lista1)\n                        #stringul=[]\n                                #stringul=[]\n                        #print(train_df[\"feature_num\"][i])\n\n                        #dict_list.append([spans[k],spans[k+1],code_is])\n                        #lista1=[spans[k],spans[k+1],code_is]\n                        lista1.append([int(spans[k]),int(spans[k+1]),code_is])\n                        lista2.append([int(spans[k]),int(spans[k+1]),code_is2])\n                        \n                        #print(lista1)\n#                         ent_dic = {}\n#                         ent_dic2 = {}\n                    #     #ent_dic['entities'] = dict_list[-1]\n                        ent_dic['entities'] = lista1#single value lista2 multiple values\n                        ent_dic2['entities'] = lista2 #single value lista2 multiple values \n                    #     #tuple_list.append(tupla)\n#                         TRAIN_DATA3.append([text,ent_dic])\n#                         TRAIN_DATA4.append([text,ent_dic2])\n                        #print(k)\n                #         ent_dic2={}\n                #         ent_dic2[\"entities\"] = lista1\n                #         #tupla=(text,{\"entities\"+\":\", location_is[k],location_is[k+1]})\n                #         TRAIN_DATA.append([text,ent_dic2])\n                        k=k+2\n            TRAIN_DATA3.append([text,ent_dic])\n            TRAIN_DATA4.append([text,ent_dic2])\n    else:\n        dumb_data=dumb_data+1\n        #print(\"An Ai without data is like a flower without water\")\nprint(\"useless pn:\",dumb_data)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T13:51:52.000938Z","iopub.execute_input":"2022-02-14T13:51:52.001618Z","iopub.status.idle":"2022-02-14T13:53:40.478505Z","shell.execute_reply.started":"2022-02-14T13:51:52.001564Z","shell.execute_reply":"2022-02-14T13:53:40.477677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(TRAIN_DATA3)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T13:53:40.479901Z","iopub.execute_input":"2022-02-14T13:53:40.480388Z","iopub.status.idle":"2022-02-14T13:53:40.48594Z","shell.execute_reply.started":"2022-02-14T13:53:40.480347Z","shell.execute_reply":"2022-02-14T13:53:40.485264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_DATA3[1:4]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T13:53:40.487172Z","iopub.execute_input":"2022-02-14T13:53:40.487391Z","iopub.status.idle":"2022-02-14T13:53:40.501657Z","shell.execute_reply.started":"2022-02-14T13:53:40.48736Z","shell.execute_reply":"2022-02-14T13:53:40.500785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ents","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:14:49.139176Z","iopub.execute_input":"2022-02-14T15:14:49.140311Z","iopub.status.idle":"2022-02-14T15:14:49.145042Z","shell.execute_reply.started":"2022-02-14T15:14:49.140268Z","shell.execute_reply":"2022-02-14T15:14:49.144173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Export to conll2003 format:\n\n Here is an example:\n\n        Wolff 0\n            , O\n    currently O\n    HEART     'NBME5\n    POUNDING  'NBME5\n\n            a O\n   ====New Line ====\n   The elxt is labbles token by token each token on a row with a label associated.\n   Sentences are separate by new line. \n   \n The result is the file:conll_corpus_train_lab_patient_notes.conll\n \n     Which is the only one important for NBME.\n     \n     The rest of the notebook represent trial and errors for augumenting the text. \n           ","metadata":{}},{"cell_type":"code","source":"import spacy\n\ndata = TRAIN_DATA3\n#data=TRAIN_DATA3[1:5]\nnlp = spacy.blank(\"en\")\n\n# Construction from class\n# from spacy.pipeline import EntityRuler\n# ruler = EntityRuler(nlp, overwrite_ents=True)\n#myfile = open('xyz.txt', 'w')\ni=0\nwith open('conll_corpus_train_lab_patient_notes.conll', 'w') as f:\n   \n    for text, labels in data:\n        doc=[]\n        #print(text)\n        doc = nlp(text)\n    \n        ents = []\n        labels[\"entities\"]=sorted(labels[\"entities\"])\n        for start, end, label in labels[\"entities\"]:\n            #print(start,end,label)\n           \n            ents.append(doc.char_span(start, end, label))\n        \n        #print(ents)\n        try:\n            doc.ents = ents\n            next_step=0\n            for tok in doc:\n                label = tok.ent_iob_\n                if tok.ent_iob_ != \"O\":\n                    label += '-' + tok.ent_type_\n                    #comment for adding new line after sentences\n                    #print(label)\n                    if (label.__contains__(\"B-\")):\n                                    f.write('\\n')\n                                    #print(\"new line __-\")\n                                    f.write(f'{tok} {label}\\n')\n                                    #print(tok,label)\n                                    next_step=0\n                    else:\n                                    f.write(f'{tok} {label}\\n')\n                                    #print(tok,label)\n                                    next_step=1\n#                     f.write('\\n')\n#                     print(\"new line end of sentence1\",\"   \")\n                else:\n                    humpty_dumpty=1\n            f.write('\\n')\n            #print(\"new line end of sentence1\",\"   \")\n                \n            if next_step==1:\n                    for tok in doc:\n                        label = tok.ent_iob_\n                        if tok.ent_iob_ != \"O\":\n                            label += '-' + tok.ent_type_\n                            #comment for adding new line after sentences\n                            f.write(f'{tok} {label}\\n')\n                            print(tok,label)\n                        else:\n                            f.write(f'{tok} {\"O\"}\\n')\n                            #print(tok,\"O\")\n                            jeton=str(tok)\n                            if (jeton.__contains__(\".\")):\n                                f.write('\\n')\n                                #print(\"separator propozitie\")\n                            else:\n                                humpty_dumpty=2\n                            next_step=0\n                            #print(\"new line end of sentence2\",\"   \")\n\n            else:\n                 next_step=0\n            #doc=[]\n            \n                \n        except:\n            x=1\n            #print(\"An exception occurred\")\n#         f.write('\\n')\n#         print(\"new line end of sentence3\",\"   \")\n                #print(tok, label, sep=\"\\t\")\n# Add new line aftre sencences                \n#                 if str(tok) == \".\":\n#                     f.write('\\n')\n#                 else:\n#                     #f.write\n#                     f.write(f'{tok} {label}\\n')\n#                     #print(i)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:14:56.262566Z","iopub.execute_input":"2022-02-14T15:14:56.263443Z","iopub.status.idle":"2022-02-14T15:15:11.219736Z","shell.execute_reply.started":"2022-02-14T15:14:56.263389Z","shell.execute_reply":"2022-02-14T15:15:11.218836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Same ideea like above but for case labels and spans. \n\n    file :conll_corpus_train_lab_patient_case.conll'","metadata":{}},{"cell_type":"code","source":"data = TRAIN_DATA4\n\nnlp = spacy.blank(\"en\")\n\n# Construction from class\n# from spacy.pipeline import EntityRuler\n# ruler = EntityRuler(nlp, overwrite_ents=True)\n#myfile = open('xyz.txt', 'w')\ni=0\nwith open('conll_corpus_train_lab_patient_case.conll', 'w') as f:\n   \n    for text, labels in data:\n        doc=[]\n        #print(text)\n        doc = nlp(text)\n    \n        ents = []\n        labels[\"entities\"]=sorted(labels[\"entities\"])\n        for start, end, label in labels[\"entities\"]:\n            print(start,end,label)\n           \n            ents.append(doc.char_span(start, end, label))\n        \n        print(ents)\n        try:\n            doc.ents = ents\n            next_step=0\n            for tok in doc:\n                label = tok.ent_iob_\n                if tok.ent_iob_ != \"O\":\n                    label += '-' + tok.ent_type_\n                    #comment for adding new line after sentences\n                    f.write(f'{tok} {label}\\n')\n                    next_step=1\n                    f.write('\\n')\n                else:\n                    humpty_dumpty=1\n                if next_step==1:\n                    for tok in doc:\n                        label = tok.ent_iob_\n                        if tok.ent_iob_ != \"O\":\n                            label += '-' + tok.ent_type_\n                            #comment for adding new line after sentences\n                            f.write(f'{tok} {label}\\n')\n                        else:\n                            f.write(f'{tok} {\"O\"}\\n')\n                    next_ste==0\n                else:\n                    next_step=0\n            doc=[]\n            f.write('\\n')      \n                #print(tok, label, sep=\"\\t\")\n# Add new line aftre sencences                \n#                 if str(tok) == \".\":\n#                     f.write('\\n')\n#                 else:\n#                     #f.write\n#                     f.write(f'{tok} {label}\\n')\n#                     #print(i)\n        \n        except:\n            x=1","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:18:32.780763Z","iopub.execute_input":"2022-02-14T15:18:32.781086Z","iopub.status.idle":"2022-02-14T15:18:39.052036Z","shell.execute_reply.started":"2022-02-14T15:18:32.781052Z","shell.execute_reply":"2022-02-14T15:18:39.050917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TRAIN_DATA4","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:32.786616Z","iopub.execute_input":"2022-02-14T15:19:32.787219Z","iopub.status.idle":"2022-02-14T15:19:32.791339Z","shell.execute_reply.started":"2022-02-14T15:19:32.787174Z","shell.execute_reply":"2022-02-14T15:19:32.790416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Consolidate Train corpus","metadata":{}},{"cell_type":"code","source":"train = train.merge(features, on=['feature_num', 'case_num'], how='left')\ntrain = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\ndisplay(train.head(10))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:33.992749Z","iopub.execute_input":"2022-02-14T15:19:33.993067Z","iopub.status.idle":"2022-02-14T15:19:34.050477Z","shell.execute_reply.started":"2022-02-14T15:19:33.99303Z","shell.execute_reply":"2022-02-14T15:19:34.049918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.case_num.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:35.1029Z","iopub.execute_input":"2022-02-14T15:19:35.103431Z","iopub.status.idle":"2022-02-14T15:19:35.112276Z","shell.execute_reply.started":"2022-02-14T15:19:35.103397Z","shell.execute_reply":"2022-02-14T15:19:35.111525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['pn_history'][10]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:36.20414Z","iopub.execute_input":"2022-02-14T15:19:36.204664Z","iopub.status.idle":"2022-02-14T15:19:36.210402Z","shell.execute_reply.started":"2022-02-14T15:19:36.20463Z","shell.execute_reply":"2022-02-14T15:19:36.209822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# incorrect annotation\ntrain.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\ntrain.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n\ntrain.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\ntrain.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n\ntrain.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\ntrain.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n\ntrain.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\ntrain.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n\ntrain.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\ntrain.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n\ntrain.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\ntrain.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n\ntrain.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\ntrain.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n\ntrain.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\ntrain.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n\ntrain.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\ntrain.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n\ntrain.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\ntrain.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n\ntrain.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\ntrain.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n\ntrain.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\ntrain.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n\ntrain.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\ntrain.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n\ntrain.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\ntrain.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n\ntrain.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\ntrain.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n\ntrain.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\ntrain.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n\ntrain.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\ntrain.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n\ntrain.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\ntrain.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n\ntrain.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\ntrain.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n\ntrain.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\ntrain.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n\ntrain.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\ntrain.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n\ntrain.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\ntrain.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n\ntrain.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\ntrain.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n\ntrain.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\ntrain.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n\ntrain.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\ntrain.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n\ntrain.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\ntrain.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n\ntrain.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\ntrain.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n\ntrain.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\ntrain.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n\ntrain.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\ntrain.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n\ntrain.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\ntrain.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n\ntrain.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\ntrain.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n\ntrain.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\ntrain.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n\ntrain.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\ntrain.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n\ntrain.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\ntrain.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n\ntrain.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\ntrain.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n\ntrain.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\ntrain.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n\ntrain.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\ntrain.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n\ntrain.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\ntrain.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n\ntrain.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\ntrain.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n\ntrain.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\ntrain.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n\ntrain.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\ntrain.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n\ntrain.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\ntrain.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:36.737387Z","iopub.execute_input":"2022-02-14T15:19:36.73769Z","iopub.status.idle":"2022-02-14T15:19:36.833847Z","shell.execute_reply.started":"2022-02-14T15:19:36.737656Z","shell.execute_reply":"2022-02-14T15:19:36.832872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(100)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:37.789907Z","iopub.execute_input":"2022-02-14T15:19:37.790231Z","iopub.status.idle":"2022-02-14T15:19:37.812064Z","shell.execute_reply.started":"2022-02-14T15:19:37.790194Z","shell.execute_reply":"2022-02-14T15:19:37.811272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['annotation_length'] = train['annotation'].apply(len)\ndisplay(train['annotation_length'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:38.688749Z","iopub.execute_input":"2022-02-14T15:19:38.68915Z","iopub.status.idle":"2022-02-14T15:19:38.701754Z","shell.execute_reply.started":"2022-02-14T15:19:38.68911Z","shell.execute_reply":"2022-02-14T15:19:38.700956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=train[train.annotation_length !=0]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:39.425307Z","iopub.execute_input":"2022-02-14T15:19:39.426283Z","iopub.status.idle":"2022-02-14T15:19:39.434672Z","shell.execute_reply.started":"2022-02-14T15:19:39.426233Z","shell.execute_reply":"2022-02-14T15:19:39.433927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=train.reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:40.234825Z","iopub.execute_input":"2022-02-14T15:19:40.235149Z","iopub.status.idle":"2022-02-14T15:19:40.241801Z","shell.execute_reply.started":"2022-02-14T15:19:40.235113Z","shell.execute_reply":"2022-02-14T15:19:40.241023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef extract_numbers(x):\n        sentence = x\n        s = [float(s) for s in re.findall(r'-?\\d+\\.?\\d*', sentence)]\n        return s","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:40.954016Z","iopub.execute_input":"2022-02-14T15:19:40.954614Z","iopub.status.idle":"2022-02-14T15:19:40.959304Z","shell.execute_reply.started":"2022-02-14T15:19:40.954573Z","shell.execute_reply":"2022-02-14T15:19:40.958286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['location_string']=train['location'].apply(lambda x: str(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:43.9082Z","iopub.execute_input":"2022-02-14T15:19:43.908703Z","iopub.status.idle":"2022-02-14T15:19:43.919333Z","shell.execute_reply.started":"2022-02-14T15:19:43.908668Z","shell.execute_reply":"2022-02-14T15:19:43.918651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" train.location_string[6]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:45.454923Z","iopub.execute_input":"2022-02-14T15:19:45.455444Z","iopub.status.idle":"2022-02-14T15:19:45.460886Z","shell.execute_reply.started":"2022-02-14T15:19:45.455411Z","shell.execute_reply":"2022-02-14T15:19:45.46025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nsentence = train.location_string[5]\ns = [float(s) for s in re.findall(r'-?\\d+\\.?\\d*', sentence)]\nprint(s)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:46.077057Z","iopub.execute_input":"2022-02-14T15:19:46.0776Z","iopub.status.idle":"2022-02-14T15:19:46.082926Z","shell.execute_reply.started":"2022-02-14T15:19:46.077564Z","shell.execute_reply":"2022-02-14T15:19:46.082132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['spans']=train['location_string'].apply(lambda x: extract_numbers(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:46.823955Z","iopub.execute_input":"2022-02-14T15:19:46.824594Z","iopub.status.idle":"2022-02-14T15:19:46.864287Z","shell.execute_reply.started":"2022-02-14T15:19:46.824556Z","shell.execute_reply":"2022-02-14T15:19:46.863388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train['annotation_length'].value_counts()) ","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:47.763826Z","iopub.execute_input":"2022-02-14T15:19:47.76412Z","iopub.status.idle":"2022-02-14T15:19:47.771217Z","shell.execute_reply.started":"2022-02-14T15:19:47.764085Z","shell.execute_reply":"2022-02-14T15:19:47.770452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['span_length'] = train['spans'].apply(lambda x: len(x))\ndisplay(train['span_length'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:48.502259Z","iopub.execute_input":"2022-02-14T15:19:48.502566Z","iopub.status.idle":"2022-02-14T15:19:48.515431Z","shell.execute_reply.started":"2022-02-14T15:19:48.502533Z","shell.execute_reply":"2022-02-14T15:19:48.514442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:49.333782Z","iopub.execute_input":"2022-02-14T15:19:49.3341Z","iopub.status.idle":"2022-02-14T15:19:49.341198Z","shell.execute_reply.started":"2022-02-14T15:19:49.334067Z","shell.execute_reply":"2022-02-14T15:19:49.340364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['spans']=train['spans'].apply(lambda x: np.array(x, int))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:50.170747Z","iopub.execute_input":"2022-02-14T15:19:50.171179Z","iopub.status.idle":"2022-02-14T15:19:50.191383Z","shell.execute_reply.started":"2022-02-14T15:19:50.171145Z","shell.execute_reply":"2022-02-14T15:19:50.190449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef loc_coordinates(location):\n    sentence = str(location)\n    s = [int(s) for s in re.findall(r'-?\\d+\\.?\\d*', sentence)]\n    return s","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:50.819719Z","iopub.execute_input":"2022-02-14T15:19:50.820184Z","iopub.status.idle":"2022-02-14T15:19:50.825063Z","shell.execute_reply.started":"2022-02-14T15:19:50.820145Z","shell.execute_reply":"2022-02-14T15:19:50.823917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['span_length'] = train['spans'].apply(len)\ndisplay(train['span_length'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:51.869517Z","iopub.execute_input":"2022-02-14T15:19:51.869783Z","iopub.status.idle":"2022-02-14T15:19:51.880894Z","shell.execute_reply.started":"2022-02-14T15:19:51.869755Z","shell.execute_reply":"2022-02-14T15:19:51.879719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set labels","metadata":{}},{"cell_type":"code","source":"train.head(100)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:58.571636Z","iopub.execute_input":"2022-02-14T15:19:58.571914Z","iopub.status.idle":"2022-02-14T15:19:58.60416Z","shell.execute_reply.started":"2022-02-14T15:19:58.571885Z","shell.execute_reply":"2022-02-14T15:19:58.603194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"labels\"]=train[\"feature_num\"].apply(lambda x:\"NBME\"+str(x))\nLABEL=train[\"labels\"].unique().tolist()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:59.053796Z","iopub.execute_input":"2022-02-14T15:19:59.054335Z","iopub.status.idle":"2022-02-14T15:19:59.065237Z","shell.execute_reply.started":"2022-02-14T15:19:59.054296Z","shell.execute_reply":"2022-02-14T15:19:59.064345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_2=train","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:19:59.903646Z","iopub.execute_input":"2022-02-14T15:19:59.904379Z","iopub.status.idle":"2022-02-14T15:19:59.908236Z","shell.execute_reply.started":"2022-02-14T15:19:59.904332Z","shell.execute_reply":"2022-02-14T15:19:59.907398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_2.feature_text=train_2.feature_text.str.replace('-',' ')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:00.401254Z","iopub.execute_input":"2022-02-14T15:20:00.401669Z","iopub.status.idle":"2022-02-14T15:20:00.410419Z","shell.execute_reply.started":"2022-02-14T15:20:00.401637Z","shell.execute_reply":"2022-02-14T15:20:00.409532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_2['feature_text_lenght']=train_2['feature_text'].apply(lambda x: len(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:00.784148Z","iopub.execute_input":"2022-02-14T15:20:00.784689Z","iopub.status.idle":"2022-02-14T15:20:00.79507Z","shell.execute_reply.started":"2022-02-14T15:20:00.78464Z","shell.execute_reply":"2022-02-14T15:20:00.794205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_2['feature_text_span']=train_2['feature_text_lenght'].apply(lambda x: [0,x])","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:01.132255Z","iopub.execute_input":"2022-02-14T15:20:01.132558Z","iopub.status.idle":"2022-02-14T15:20:01.140894Z","shell.execute_reply.started":"2022-02-14T15:20:01.132516Z","shell.execute_reply":"2022-02-14T15:20:01.140192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_2.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:01.505592Z","iopub.execute_input":"2022-02-14T15:20:01.506059Z","iopub.status.idle":"2022-02-14T15:20:01.527837Z","shell.execute_reply.started":"2022-02-14T15:20:01.506015Z","shell.execute_reply":"2022-02-14T15:20:01.527136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Spacy incorrect format","metadata":{}},{"cell_type":"code","source":"# test=test.reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:03.765373Z","iopub.execute_input":"2022-02-14T15:20:03.765797Z","iopub.status.idle":"2022-02-14T15:20:03.76985Z","shell.execute_reply.started":"2022-02-14T15:20:03.765763Z","shell.execute_reply":"2022-02-14T15:20:03.768671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"new tagger","metadata":{}},{"cell_type":"code","source":"#temp_df=train[train.pn_num ==16]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:04.350552Z","iopub.execute_input":"2022-02-14T15:20:04.351158Z","iopub.status.idle":"2022-02-14T15:20:04.355487Z","shell.execute_reply.started":"2022-02-14T15:20:04.351098Z","shell.execute_reply":"2022-02-14T15:20:04.354852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#len(temp_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:04.568616Z","iopub.execute_input":"2022-02-14T15:20:04.569221Z","iopub.status.idle":"2022-02-14T15:20:04.57395Z","shell.execute_reply.started":"2022-02-14T15:20:04.56916Z","shell.execute_reply":"2022-02-14T15:20:04.573096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def getOverlap(a, b):\n#     return max(0, min(a[1], b[1]) - max(a[0], b[0]))\n# span_list=[]\n# for i in range(len(temp_df)):\n#         k=0\n#         l=0\n#         if temp_df.span_length[i]==0 :\n#                 print(1)\n#                 span_list.append([0,0,i,k])\n#         else:\n#             rangex=(temp_df.span_length[i])/2\n#             for j in range(int(rangex)):\n#                 print(\"j:\",j)\n#                 if np.isnan(temp_df.span_length[i]) :\n#                     print(2)\n#                     span_list.append([0,0,i,k])\n#                     print(k)\n#                     #k=k+2\n#                 else:\n#                     print(\"j:\",j)\n                \n#                     a=temp_df.spans[j]\n#                     print(type(a))\n#                     #b=temp_df.spans[k+1]\n#                     #print(temp_df.span_length[i])\n#                     print(a)\n#                     print(a[k])\n#                     print(a[k+1])\n#                     #b=temp_df.spans[k+1]\n#                     span_list.append([a,b,i,k])\n#                     print(k)\n#             k=k+2","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:04.817918Z","iopub.execute_input":"2022-02-14T15:20:04.818487Z","iopub.status.idle":"2022-02-14T15:20:04.822397Z","shell.execute_reply.started":"2022-02-14T15:20:04.818449Z","shell.execute_reply":"2022-02-14T15:20:04.821571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=train","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:05.489053Z","iopub.execute_input":"2022-02-14T15:20:05.489348Z","iopub.status.idle":"2022-02-14T15:20:05.493279Z","shell.execute_reply.started":"2022-02-14T15:20:05.489316Z","shell.execute_reply":"2022-02-14T15:20:05.492615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_DATA = []\ntuple_list=[]\ndict_list=[]\nalpha=0\n#for i in range(20):\nfor i in range(len(train_df)):\n    \n    #contor_lista=working_corpus.annotation_length[i]\n    \n    k=0\n    #print(working_corpus.annotation_length[i])\n    #print(location_is)\n    dict_list=[]\n    lista1=[]\n    lista2=[]\n    lista3=[]\n    lista4=[]\n    #print(train_df.span_length[i])\n    alpha=int((train_df.span_length[i])/2)\n    #print(alpha)\n    #location_is=loc_coordinates(train_df[\"spans\"][i])\n    for j in range(alpha):\n        text=train_df.pn_history[i]\n#         if np.isnan(train_df[\"spans\"][i])[1]:\n#             print('no good')\n#         else:\n#             print('we are good')\n        #print(location_is)\n        #print(train_df[\"spans\"][i])\n        #print(location_is[k])\n        spans=train_df[\"spans\"][i]\n        \n        #print('spansx',spans[k])\n        #print('spansy',spans[k+1])\n        #print(\"lista este\",lista1)\n        #stringul=[]\n                #stringul=[]\n        #print(train_df[\"feature_num\"][i])\n        code_is=\"NBME\"+str(train_df[\"feature_num\"][i])\n        dict_list.append([spans[k],spans[k+1],code_is])\n        lista1=[spans[k],spans[k+1],code_is]\n        lista2.append([spans[k],spans[k+1],code_is])\n        #print(lista1)\n        ent_dic = {}\n    #     #ent_dic['entities'] = dict_list[-1]\n        ent_dic['entities'] = [lista1] #single value lista2 multiple values \n    #     #tuple_list.append(tupla)\n        TRAIN_DATA.append([text,ent_dic])\n        #print(k)\n#         ent_dic2={}\n#         ent_dic2[\"entities\"] = lista1\n#         #tupla=(text,{\"entities\"+\":\", location_is[k],location_is[k+1]})\n#         TRAIN_DATA.append([text,ent_dic2])\n        k=k+2\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-14T15:20:05.918383Z","iopub.execute_input":"2022-02-14T15:20:05.91895Z","iopub.status.idle":"2022-02-14T15:20:06.572921Z","shell.execute_reply.started":"2022-02-14T15:20:05.918912Z","shell.execute_reply":"2022-02-14T15:20:06.572052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(TRAIN_DATA)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:08.155413Z","iopub.execute_input":"2022-02-14T15:20:08.155706Z","iopub.status.idle":"2022-02-14T15:20:08.160926Z","shell.execute_reply.started":"2022-02-14T15:20:08.155671Z","shell.execute_reply":"2022-02-14T15:20:08.160208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#TRAIN_DATA","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:21:16.061518Z","iopub.execute_input":"2022-02-12T18:21:16.06172Z","iopub.status.idle":"2022-02-12T18:21:16.071127Z","shell.execute_reply.started":"2022-02-12T18:21:16.061695Z","shell.execute_reply":"2022-02-12T18:21:16.07044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CONLL format converter","metadata":{}},{"cell_type":"code","source":"import sys","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:18.842407Z","iopub.execute_input":"2022-02-14T15:20:18.842984Z","iopub.status.idle":"2022-02-14T15:20:18.84732Z","shell.execute_reply.started":"2022-02-14T15:20:18.842933Z","shell.execute_reply":"2022-02-14T15:20:18.846408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text=('control text .de . control. . si mai amabil')\n# doc = nlp(text)\n# doc[0]\n# for tok in doc:\n#     print(tok)\n#     if str(tok) == \".\":\n#         print()\n#         write('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:19.301285Z","iopub.execute_input":"2022-02-14T15:20:19.301531Z","iopub.status.idle":"2022-02-14T15:20:19.304784Z","shell.execute_reply.started":"2022-02-14T15:20:19.301506Z","shell.execute_reply":"2022-02-14T15:20:19.303979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(TRAIN_DATA)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:19.82291Z","iopub.execute_input":"2022-02-14T15:20:19.823445Z","iopub.status.idle":"2022-02-14T15:20:19.828719Z","shell.execute_reply.started":"2022-02-14T15:20:19.823411Z","shell.execute_reply":"2022-02-14T15:20:19.8281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import spacy\n\ndata = TRAIN_DATA\n\nnlp = spacy.blank(\"en\")\n#myfile = open('xyz.txt', 'w')\ni=0\nwith open('conll_corpus_train_lab1x.conll', 'w') as f:\n    doc=[]\n    for text, labels in data:\n        doc = nlp(text)\n        ents = []\n        for start, end, label in labels[\"entities\"]:\n            ents.append(doc.char_span(start, end, label))\n        \n        try:\n            doc.ents = ents\n            next_step=0\n            for tok in doc:\n                label = tok.ent_iob_\n                if tok.ent_iob_ != \"O\":\n                    label += '-' + tok.ent_type_\n                    #comment for adding new line after sentences\n                    f.write(f'{tok} {label}\\n')\n                    next_step=1\n                    f.write('\\n')\n                else:\n                    humpty_dumpty=1\n                if next_step==1:\n                    for tok in doc:\n                        label = tok.ent_iob_\n                        if tok.ent_iob_ != \"O\":\n                            label += '-' + tok.ent_type_\n                            #comment for adding new line after sentences\n                            f.write(f'{tok} {label}\\n')\n                        else:\n                            f.write(f'{tok} {\"O\"}\\n')\n                    next_ste==0\n                else:\n                    next_step=0\n            f.write('\\n')      \n                #print(tok, label, sep=\"\\t\")\n# Add new line aftre sencences                \n#                 if str(tok) == \".\":\n#                     f.write('\\n')\n#                 else:\n#                     #f.write\n#                     f.write(f'{tok} {label}\\n')\n#                     #print(i)\n        \n        except:\n            x=1\n            #print(\"An exception occurred\")\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:43.926354Z","iopub.execute_input":"2022-02-14T15:20:43.926912Z","iopub.status.idle":"2022-02-14T15:20:51.137729Z","shell.execute_reply.started":"2022-02-14T15:20:43.926874Z","shell.execute_reply":"2022-02-14T15:20:51.136867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Labels by cases","metadata":{}},{"cell_type":"code","source":"train[\"labels\"]=train[\"case_num\"].apply(lambda x:\"NBME-Cno-\"+str(x))\nLABEL=train[\"labels\"].unique().tolist()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:57.128029Z","iopub.execute_input":"2022-02-14T15:20:57.128306Z","iopub.status.idle":"2022-02-14T15:20:57.138231Z","shell.execute_reply.started":"2022-02-14T15:20:57.128277Z","shell.execute_reply":"2022-02-14T15:20:57.137453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_DATA = []\ntuple_list=[]\ndict_list=[]\nalpha=0\n#for i in range(20):\nfor i in range(len(train_df)):\n    \n    #contor_lista=working_corpus.annotation_length[i]\n    \n    k=0\n    #print(working_corpus.annotation_length[i])\n    #print(location_is)\n    dict_list=[]\n    lista1=[]\n    lista2=[]\n    lista3=[]\n    lista4=[]\n    alpha=int(train_df.span_length[i]/2)\n    #print(alpha)\n    for j in range(alpha):\n        text=train_df.pn_history[i]\n        location_is=loc_coordinates(train_df[\"spans\"][i])\n        \n        \n        #print(\"lista este\",lista1)\n        #stringul=[]\n                #stringul=[]\n        code_is=\"NBME-Cno-\"+str(train_df[\"case_num\"][i])\n        dict_list.append([location_is[k],location_is[k+1],code_is])\n        lista1=[location_is[k],location_is[k+1],code_is]\n        lista2.append([location_is[k],location_is[k+1],code_is])\n        ent_dic = {}\n    #     #ent_dic['entities'] = dict_list[-1]\n        ent_dic['entities'] = [lista1] #single value lista2 multiple values \n    #     #tuple_list.append(tupla)\n        TRAIN_DATA.append([text,ent_dic])\n        #print(k)\n#         ent_dic2={}\n#         ent_dic2[\"entities\"] = lista1\n#         #tupla=(text,{\"entities\"+\":\", location_is[k],location_is[k+1]})\n#         TRAIN_DATA.append([text,ent_dic2])\n        k=k+2\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:20:59.162815Z","iopub.execute_input":"2022-02-14T15:20:59.163095Z","iopub.status.idle":"2022-02-14T15:21:00.37305Z","shell.execute_reply.started":"2022-02-14T15:20:59.163065Z","shell.execute_reply":"2022-02-14T15:21:00.372255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(TRAIN_DATA)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:21:03.563938Z","iopub.execute_input":"2022-02-14T15:21:03.564246Z","iopub.status.idle":"2022-02-14T15:21:03.571154Z","shell.execute_reply.started":"2022-02-14T15:21:03.564217Z","shell.execute_reply":"2022-02-14T15:21:03.570337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_DATA2 = []\ntuple_list=[]\ndict_list=[]\nalpha=0\n#for i in range(20):\nfor i in range(len(train_2)):\n    \n    #contor_lista=working_corpus.annotation_length[i]\n    \n    k=0\n    #print(working_corpus.annotation_length[i])\n    #print(location_is)\n    dict_list=[]\n    lista1=[]\n    lista2=[]\n    lista3=[]\n    lista4=[]\n    #print(train_df.span_length[i])\n    #alpha=int((train_2.span_length[i])/2)\n    #print(alpha)\n    #location_is=loc_coordinates(train_df[\"spans\"][i])\n\n    text=train_2.feature_text[i]\n#         if np.isnan(train_df[\"spans\"][i])[1]:\n#             print('no good')\n#         else:\n#             print('we are good')\n    #print(location_is)\n    #print(train_2[\"spans\"][i])\n    #print(location_is[k])\n    spans=train_2[\"feature_text_span\"][i]\n\n    #print('spansx',spans[k])\n    #print('spansy',spans[k+1])\n    #print(\"lista este\",lista1)\n    #stringul=[]\n            #stringul=[]\n    #print(train_df[\"feature_num\"][i])\n    code_is=\"NBME\"+str(train_2[\"feature_num\"][i])\n    dict_list.append([spans[0],spans[1],code_is])\n    lista1=[spans[0],spans[1],code_is]\n    lista2.append([spans[0],spans[1],code_is])\n    #print(lista1)\n    ent_dic = {}\n#     #ent_dic['entities'] = dict_list[-1]\n    ent_dic['entities'] = [lista1] #single value lista2 multiple values \n#     #tuple_list.append(tupla)\n    TRAIN_DATA2.append([text,ent_dic])\n    #print(k)\n#         ent_dic2={}\n#         ent_dic2[\"entities\"] = lista1\n#         #tupla=(text,{\"entities\"+\":\", location_is[k],location_is[k+1]})\n#         TRAIN_DATA.append([text,ent_dic2])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:21:06.165587Z","iopub.execute_input":"2022-02-14T15:21:06.166316Z","iopub.status.idle":"2022-02-14T15:21:06.404726Z","shell.execute_reply.started":"2022-02-14T15:21:06.166264Z","shell.execute_reply":"2022-02-14T15:21:06.40374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = TRAIN_DATA2\n\nnlp = spacy.blank(\"en\")\n#myfile = open('xyz.txt', 'w')\n#f=open(\"guru99.txt\", \"a+\")\ni=0\nwith open('conll_corpus_train_lab1y.conll', 'a+') as f:\n    doc=[]\n    for text, labels in data:\n        doc = nlp(text)\n        ents = []\n        for start, end, label in labels[\"entities\"]:\n            ents.append(doc.char_span(start, end, label))\n        \n        try:\n            doc.ents = ents\n            next_step=0\n            for tok in doc:\n                label = tok.ent_iob_\n                if tok.ent_iob_ != \"O\":\n                    label += '-' + tok.ent_type_\n                    #comment for adding new line after sentences\n                    f.write(f'{tok} {label}\\n')\n                    next_step=1\n                    f.write('\\n')\n                if next_step==1:\n                \n                    for tok in doc:\n                        label = tok.ent_iob_\n                        if tok.ent_iob_ != \"O\":\n                            label += '-' + tok.ent_type_\n                            #comment for adding new line after sentences\n                            f.write(f'{tok} {label}\\n')\n                        else:\n                            f.write(f'{tok} {\"O\"}\\n')\n                else:\n                    next_step=0\n            f.write('\\n')      \n# Add new line aftre sencences                \n#                 if str(tok) == \".\":\n#                     f.write('\\n')\n#                 else:\n#                     #f.write\n#                     f.write(f'{tok} {label}\\n')\n#                     #print(i)\n        \n        except:\n            x=1\n            #print(\"An exception occurred\")\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:21:14.637084Z","iopub.execute_input":"2022-02-14T15:21:14.637357Z","iopub.status.idle":"2022-02-14T15:21:15.665654Z","shell.execute_reply.started":"2022-02-14T15:21:14.637326Z","shell.execute_reply":"2022-02-14T15:21:15.664867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = TRAIN_DATA\n\nnlp = spacy.blank(\"en\")\n#myfile = open('xyz.txt', 'w')\ni=0\nwith open('conll_corpus_train_lab2', 'w') as f:\n\n    for text, labels in data:\n        doc = nlp(text)\n        ents = []\n        for start, end, label in labels[\"entities\"]:\n            ents.append(doc.char_span(start, end, label))\n        \n        try:\n            doc.ents = ents\n            for tok in doc:\n                label = tok.ent_iob_\n                if tok.ent_iob_ != \"O\":\n                    label += '-' + tok.ent_type_\n                    #comment for adding new line after sentences\n                    f.write(f'{tok} {label}\\n')\n                #print(tok, label, sep=\"\\t\")\n# Add new line aftre sencences                \n#                 if str(tok) == \".\":\n#                     f.write('\\n')\n#                 else:\n#                     #f.write\n#                     f.write(f'{tok} {label}\\n')\n#                     #print(i)\n        except:\n            x=1\n            #print(\"An exception occurred\")\n    f.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:21:55.136125Z","iopub.execute_input":"2022-02-14T15:21:55.136796Z","iopub.status.idle":"2022-02-14T15:21:59.697168Z","shell.execute_reply.started":"2022-02-14T15:21:55.136748Z","shell.execute_reply":"2022-02-14T15:21:59.696352Z"},"trusted":true},"execution_count":null,"outputs":[]}]}