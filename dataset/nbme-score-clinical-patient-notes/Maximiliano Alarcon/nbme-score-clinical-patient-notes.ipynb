{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center><b>NBME (National Board of Medical Examiners)</b></center></h1>","metadata":{}},{"cell_type":"markdown","source":"NBME is a mission-driven organization that specializes in the creation of high-quality assessments and learning tools.\n\nMission: Protecting the Health of the Public through State of the Art Assessment\n\nVision: Improving Health Care around the World through Assessment\n\nIn addition to offering assessment tools for every stage of the medical school journey, NBME aims to build meaningful collaborations and make lasting contributions to the medical education community. ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport pandas as pd, numpy as np\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nroot = \"../input/nbme-score-clinical-patient-notes/\"","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:42.364323Z","iopub.execute_input":"2022-05-03T22:15:42.364825Z","iopub.status.idle":"2022-05-03T22:15:42.397604Z","shell.execute_reply.started":"2022-05-03T22:15:42.364703Z","shell.execute_reply":"2022-05-03T22:15:42.396669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Abstract**","metadata":{}},{"cell_type":"markdown","source":"Medical diagnoses require extracting symptoms and characteristics of the patient from the history notes.\n\nLearning and evaluating the ability to write patient notes requires feedback from other doctors, a time-consuming process that could be improved with the addition of machine learning.\n\nUntil recently, a clinical skills exam was taken from the United States Medical Licensing Examination.\n\nThe exam required examinees to interact with standardized patients (people trained to act out specific clinical cases) and write a note for the patient.\n\nTrained medical raters then scored the patients' notes with rubrics that described the important concepts in each case (known as features).","metadata":{}},{"cell_type":"markdown","source":"**Goal**\n\nIn this competition, the goal is to identify specific clinical concepts in patient notes.\n\nSpecifically, the solution will be an automated method for mapping clinical concepts from an exam rubric (eg, \"decreased appetite\") to the various ways these concepts are expressed in patient clinical notes written by medical students ( eg, “eat less”, “clothes are looser”).","metadata":{}},{"cell_type":"markdown","source":"# **Libraries**","metadata":{}},{"cell_type":"code","source":"#These are the necessary libraries for data analysis and solution development\nimport pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt,json,re,nltk","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:42.54109Z","iopub.execute_input":"2022-05-03T22:15:42.542184Z","iopub.status.idle":"2022-05-03T22:15:43.233274Z","shell.execute_reply.started":"2022-05-03T22:15:42.542124Z","shell.execute_reply":"2022-05-03T22:15:43.232392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Dataset descriptions**","metadata":{}},{"cell_type":"markdown","source":"The training dataset stores the **feature** records extracted from clinical notes.\n\nThe pn_num column identifies a clinical note.\n\nThe feature_num column identifies the extracted **concept/feature.**\n\nThe annotation column describes the text(s) associated with **feature_num**.\n\nThe location column represents the indices of the annotations, for example: the index 0 4 represents the first 5 letters of the clinical note.","metadata":{}},{"cell_type":"code","source":"import os\nprint(\"List dir\")\nprint(os.listdir())\nprint(\"First directory\")\nfirstdir = os.getcwd()\nprint(os.getcwd())","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:57:49.5318Z","iopub.execute_input":"2022-05-03T22:57:49.532082Z","iopub.status.idle":"2022-05-03T22:57:49.538811Z","shell.execute_reply.started":"2022-05-03T22:57:49.53205Z","shell.execute_reply":"2022-05-03T22:57:49.537746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(root+\"train.csv\")\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:57:07.149158Z","iopub.execute_input":"2022-05-03T22:57:07.149436Z","iopub.status.idle":"2022-05-03T22:57:07.186423Z","shell.execute_reply.started":"2022-05-03T22:57:07.149407Z","shell.execute_reply":"2022-05-03T22:57:07.185586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features dataset contains all the **features** that can be extracted from a note, each one has its own description and identifier (**feature_num**).\n\nIn total, there are 143 features","metadata":{}},{"cell_type":"code","source":"df_features = pd.read_csv(root+\"features.csv\")\ndf_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:43.277078Z","iopub.execute_input":"2022-05-03T22:15:43.27738Z","iopub.status.idle":"2022-05-03T22:15:43.290576Z","shell.execute_reply.started":"2022-05-03T22:15:43.277342Z","shell.execute_reply":"2022-05-03T22:15:43.289607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_features.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:43.292621Z","iopub.execute_input":"2022-05-03T22:15:43.292896Z","iopub.status.idle":"2022-05-03T22:15:43.298516Z","shell.execute_reply.started":"2022-05-03T22:15:43.292856Z","shell.execute_reply":"2022-05-03T22:15:43.297612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The patient notes dataset stores the clinical notes identified by **pn_num**","metadata":{}},{"cell_type":"code","source":"df_patient_notes = pd.read_csv(root+\"patient_notes.csv\")\ndf_patient_notes.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:43.30026Z","iopub.execute_input":"2022-05-03T22:15:43.300534Z","iopub.status.idle":"2022-05-03T22:15:43.622314Z","shell.execute_reply.started":"2022-05-03T22:15:43.300498Z","shell.execute_reply":"2022-05-03T22:15:43.621564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Annotations example**","metadata":{}},{"cell_type":"markdown","source":"The process of extracting a feature is divided into these steps:\n\n1) Take note of the patient\n\n2) Identify the feature to extract by feature_num\n\n3) Search and find the annotation(s) corresponding to the feature","metadata":{}},{"cell_type":"code","source":"df_train[\"pn_num\"].unique()[0:100]","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:43.624082Z","iopub.execute_input":"2022-05-03T22:15:43.624432Z","iopub.status.idle":"2022-05-03T22:15:43.633642Z","shell.execute_reply.started":"2022-05-03T22:15:43.624392Z","shell.execute_reply":"2022-05-03T22:15:43.632722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pn_num = 39449\n\nmask = df_patient_notes[\"pn_num\"] == pn_num\ndf_masked = df_patient_notes[mask].copy()\nprint(\"Patient note \"+str(pn_num))\nprint(\"=\"*64)\nprint(df_masked.iloc[0][\"pn_history\"])\nprint(\"\")\nprint(\"\")\n\ndf_train_masked = df_train[df_train[\"pn_num\"] == pn_num].copy()\ndf_train_masked = df_train_masked.merge(df_features, on=[\"feature_num\",\"case_num\"])\ndf_train_masked.head(100)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:43.63524Z","iopub.execute_input":"2022-05-03T22:15:43.635844Z","iopub.status.idle":"2022-05-03T22:15:43.66309Z","shell.execute_reply.started":"2022-05-03T22:15:43.635802Z","shell.execute_reply":"2022-05-03T22:15:43.662411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_masked = df_train_masked.merge(df_features, on=[\"feature_num\",\"case_num\"])\ndf_train_masked = df_train[df_train[\"feature_num\"] == 308].copy()\ndf_train_masked[\"pn_num\"].unique()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:43.744734Z","iopub.execute_input":"2022-05-03T22:15:43.74494Z","iopub.status.idle":"2022-05-03T22:15:43.758629Z","shell.execute_reply.started":"2022-05-03T22:15:43.744915Z","shell.execute_reply":"2022-05-03T22:15:43.757205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Tags extraction**","metadata":{}},{"cell_type":"markdown","source":"The proposed solution is to register keywords or tags to identify features in a text, and thus extract annotations.\n\nThe development is divided into these steps:\n\n1) Loop through the feature dataset\n\n2) For each feature, search for all associated annotations in the training dataset.\n\n3) Join all the tags for each feature in a list and save them in a dictionary","metadata":{}},{"cell_type":"code","source":"dictionary_features = {}","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:43.901012Z","iopub.execute_input":"2022-05-03T22:15:43.901882Z","iopub.status.idle":"2022-05-03T22:15:43.905935Z","shell.execute_reply.started":"2022-05-03T22:15:43.901845Z","shell.execute_reply":"2022-05-03T22:15:43.904866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extraer_tags_de_anotaciones(df_train_masked):\n  lista = []\n  for index, row in df_train_masked.iterrows():\n    if row[\"annotation\"] != \"[]\":\n      tags = row[\"annotation\"].lower()\n      tags = tags.replace(\"'\", '\"')\n      try:\n        tags = json.loads(tags)\n        for tag in tags:\n          if tag not in lista:\n            lista.append(tag)\n      except ValueError:\n        print(\"Error al convertir: \"+str(tags)+\" en lista\")\n  return lista\n\n\ndef obtnener_coordenadas_tag(tag,text,contraccion_izq=0,contraccion_der=0):\n  lentag = len(tag)\n  index = 0\n  locations = []\n  while index < len(text):\n      index = text.find(tag, index)\n      if index == -1:\n          break\n      locations.append(str(index+contraccion_izq)+\" \"+str(index+lentag+contraccion_der))\n      index += lentag\n  if len(locations) == 0:\n      locations = None\n  else:\n      locations = ';'.join(locations)\n  return locations","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:44.06477Z","iopub.execute_input":"2022-05-03T22:15:44.065175Z","iopub.status.idle":"2022-05-03T22:15:44.074437Z","shell.execute_reply.started":"2022-05-03T22:15:44.06514Z","shell.execute_reply":"2022-05-03T22:15:44.073598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in df_features.iterrows():\n    #print(\"Feature num: \"+str(row['feature_num'])+\" Feature text: \"+str(row[\"feature_text\"]))\n    #print(\"=\"*64)\n\n    df_train_masked = df_train[df_train[\"feature_num\"] == row['feature_num']].copy()\n    df_train_masked = df_train_masked[[\"annotation\"]].copy()\n    tags = extraer_tags_de_anotaciones(df_train_masked)\n\n    #print(\"Count tags: \"+str(len(tags)))\n    dictionary_features[row['feature_num']] = tags\n    #print(\"\")","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:44.217658Z","iopub.execute_input":"2022-05-03T22:15:44.217968Z","iopub.status.idle":"2022-05-03T22:15:45.151078Z","shell.execute_reply.started":"2022-05-03T22:15:44.21794Z","shell.execute_reply":"2022-05-03T22:15:45.150271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Problems in text processing**","metadata":{}},{"cell_type":"markdown","source":"There were some errors during annotation processing.\n\nThey will be corrected manually and representative tags will also be added for each set. The intention is that, at the very least, the solution can merely detect the location(s) of the concepts within the notes.","metadata":{}},{"cell_type":"code","source":"n=0\ndictionary_features[n].append('dad with \"heart attack\"')\n\nn=1\ndictionary_features[n].append('mom with \"thyroid disease\"')\ndictionary_features[n].append('mom has \"thyroid disorder\"')\ndictionary_features[n].append('mother with \"thyroid\"')\ndictionary_features[n].append('mom with \"thyroid problem\"')\ndictionary_features[n].append('mother-\"thyroid problem\"')\ndictionary_features[n].append('mother thyroid \"problem\"')\ndictionary_features[n].append('mother has \"thyroid issues\"')\ndictionary_features[n].append('mom has \"thyroid problems\"')\ndictionary_features[n].append('mother has \"thyroid issue\"')\ndictionary_features[n].append('mother - \"thyroid problem\"')\ndictionary_features[n].append('mother with \"thyroid problem\"')\ndictionary_features[n].append('mom- \"thyroid problem\"')\ndictionary_features[n].append('mother with \"thyroid\" condition\"')\n\nn=4\ndictionary_features[n].append('felt ike he was going to \"pass out\"')\n\nn=7\ndictionary_features[n].append(\"couldn't catch his breath\")\n\nn=9\ndictionary_features[n].append('heart pounding')\ndictionary_features[n].append('heart \"jumping out of his chest\"')\ndictionary_features[n].append('heart is \"pounding\"')\ndictionary_features[n].append('heart racing')\n\nn=101\ndictionary_features[n].append('clothes \"fit looser\"')\n\nn=102\ndictionary_features[n].append(\"hasn't been sexually active in 9 months\")\ndictionary_features[n].append('not sexually active in 9 months')\n\n\nn=109\ndictionary_features[n].append('low appetite')\ndictionary_features[n].append(\"didn't eat anything since last dinner\")\ndictionary_features[n].append(\"hasn't felt hungry\")\n\n\nn=205\ndictionary_features[n].append('irregular periods')\ndictionary_features[n].append('happen every 3 weeks to 4 months')\ndictionary_features[n].append('periods are \"unpredictable\"')\ndictionary_features[n].append('have been \"unpredictable,\"')\ndictionary_features[n].append('\"unpredictable\" periods')\ndictionary_features[n].append('periods have been \"unpredictable\"')\ndictionary_features[n].append('now \"unpredictable\"')\n\nn=206\ndictionary_features[n].append('last week, nausea')\ndictionary_features[n].append(\"last week flu-like sx's\")\n\nn=210\ndictionary_features[n].append(\"lmp was 2 mo's ago\")\n\nn=212\ndictionary_features[n].append('range from 3 weeks to 4 months and are \"consistently inconsistent\" in timing')\n\nn=300\ndictionary_features[n].append('uncle with \"bleeding ulcer\"')\ndictionary_features[n].append('paternal uncle with \"bleeding ulcer\"')\n\nn=301\ndictionary_features[n].append('sensation in epigastric (\"mid-chest\") area')\n\nn=304\ndictionary_features[n].append('\"burning\" and \"gnawing\"')\n\nn=313\ndictionary_features[n].append(\"tums doesn't work any more\")\ndictionary_features[n].append(\"at first tums helped him  but no it isn't\")\ndictionary_features[n].append('tums helped at first')\ndictionary_features[n].append('tums now don\"t help')\ndictionary_features[n].append(\"tum didn't improve the problem\")\n\nn=401\ndictionary_features[n].append('nervousness')\ndictionary_features[n].append('feels as if she is \"overwhelmed\"')\ndictionary_features[n].append('feels as if \"losing her mind\"')\ndictionary_features[n].append('feels \"overwhelmed\"')\ndictionary_features[n].append('anxiousness')\ndictionary_features[n].append(\"i feel like i'm losing my mind a bit\")\n\nn=405\ndictionary_features[n].append(\"hasn't lost any weight\")\n\nn=408\ndictionary_features[n].append(\"doesn't have an appetite\")\n\nn=503\ndictionary_features[n].append('associated with dyspnea')\ndictionary_features[n].append(\"associated with she] can't catch breath\")\n\nn=504\ndictionary_features[n].append('feeling that \"my heart is going to beat out of my chest\"')\ndictionary_features[n].append('palpitations')\ndictionary_features[n].append('presenting for a follow up from an er visit 2 weeks ago from \"heart racing\"')\n\n\nn=509\ndictionary_features[n].append('five years ago occuring once every 2-3 months occuring \"every few days for the last two weeks\"')\ndictionary_features[n].append('worse in frequency for the last two weeks')\n\nn=510\ndictionary_features[n].append('felt her heart \"pounding\" and \"racing\", she feels a sense of doom')\ndictionary_features[n].append('sensation that she \"was going to die\"')\ndictionary_features[n].append(\"feels like i'm going to die\")\ndictionary_features[n].append('associated feeling \"something bad is going to happen.\"')\ndictionary_features[n].append(\"feels like she's going to die\")\n\nn=513\ndictionary_features[n].append('clammy')\ndictionary_features[n].append('going from \"hot\"')\ndictionary_features[n].append('feeling \"hot\"')\n\nn=600\ndictionary_features[n].append('feeling \"hot\"')\ndictionary_features[n].append('feeling \"warm\"')\n\nn=603\ndictionary_features[n].append('stuffy nose for past several days')\ndictionary_features[n].append('stuffy nose')\ndictionary_features[n].append('\"stuffy nose\"')\ndictionary_features[n].append('\"stuffy nose\" for past several days')\n\nn=610\ndictionary_features[n].append(\"albuterol didn't help\")\ndictionary_features[n].append(\"albuterol didn't alleviate the pain\")\ndictionary_features[n].append(\"inhaler didn't help.\")\n\nn=702\ndictionary_features[n].append('menstrual irregularity')\ndictionary_features[n].append('menorrhagia')\ndictionary_features[n].append('tampon changes every \"couple of hours\"')\ndictionary_features[n].append('excessive bleeding')\ndictionary_features[n].append('irrgualr uterine bleeding')\ndictionary_features[n].append('heavyier periods')\ndictionary_features[n].append('irregulites in her period')\ndictionary_features[n].append('change her tampons \"every few hours\"')\n\nn=704\ndictionary_features[n].append(\"don't use condoms\")\n\nn=706\ndictionary_features[n].append(\"couldn't conceive\")\ndictionary_features[n].append(\"i can't get pregnant\")\ndictionary_features[n].append(\"couldn't get pregnant\")\n\nn=801\ndictionary_features[n].append(\"son's death 3 weeks ago\")\ndictionary_features[n].append('son was killed 3 weeks ago')\ndictionary_features[n].append(\"son's death\")\ndictionary_features[n].append(\"3 weeks son's death\")\ndictionary_features[n].append(\"3 weeks ago time of her son's death\")\n\nn=803\ndictionary_features[n].append(\"auditory hallucination of neighbor's playing music last night\")\ndictionary_features[n].append(\"thought she heard voices from a neighbors' party despite that a party never occurred\")\ndictionary_features[n].append(\"heard loud 'party like noises' from neighbor\\'s house 2 nights ago, but neighbor did not have a party\")\ndictionary_features[n].append(\"heard a party at her neighbor's that wasn't there\")\ndictionary_features[n].append(\"episodes of hearing a neighbor's party\")\ndictionary_features[n].append(\"heard loud music/party where there wasn't one\")\ndictionary_features[n].append(\"heard noises from neighbor's yard that were not present\")\ndictionary_features[n].append(\"heard a party going on next door which didn't really happen\")\n\nn=809\ndictionary_features[n].append(\"can't take a nap because can't fall to slee\")\n\nn=810\ndictionary_features[n].append(\"don't improves even when she took a ambien\")\ndictionary_features[n].append(\"tried sleeping pills but didn't help\")\ndictionary_features[n].append(\"tried ambien to sleep but it didn't help\")\ndictionary_features[n].append(\"ambien hasn't helped\")\n\nn=811\ndictionary_features[n].append('feels \"tired\"')\ndictionary_features[n].append('low energy')\ndictionary_features[n].append('feeling \"drained\"')\ndictionary_features[n].append('feels \"drained\"')\n\nn=813\ndictionary_features[n].append('saw her deceased son in the kitchen')\ndictionary_features[n].append('\"saw her son\" in the living room, although she knew he was not there')\ndictionary_features[n].append('saw her son at the kitchen table 4 days ago, but knows he wasn\"t there')\ndictionary_features[n].append('episodes of seeing \"visions\" of her son')\ndictionary_features[n].append(\"seeing her son at times but she knows he isn't there\")\ndictionary_features[n].append('\"saw leonard\" 4 days ago')\ndictionary_features[n].append(\"she saw and heard her son in the kitchen even though she knew it wasn't really him\")\n\nn=817\ndictionary_features[n].append('poor sleeps')\ndictionary_features[n].append(\"hasn't been able to get good sleep\")\ndictionary_features[n].append('4-5 hours of sleep every night')\n\nn=900\ndictionary_features[n].append(\"ibuprofen didn't help\")\ndictionary_features[n].append(\"tylenol didn't help\")\n\nn=904\ndictionary_features[n].append('\"head hurting\" all over')\n\nn=916\ndictionary_features[n].append('feeling \"warm\"')","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:45.153184Z","iopub.execute_input":"2022-05-03T22:15:45.153459Z","iopub.status.idle":"2022-05-03T22:15:45.196217Z","shell.execute_reply.started":"2022-05-03T22:15:45.153421Z","shell.execute_reply":"2022-05-03T22:15:45.195527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Ambiguous tags**","metadata":{}},{"cell_type":"markdown","source":"There are some tags with a length less than or equal to 2, and they can be interpreted as subtexts of words with another meaning.","metadata":{}},{"cell_type":"code","source":"ambiguous_tags = []\nfor index, row in df_features.iterrows():\n    lesslength = 10000000\n    for tag in dictionary_features[row['feature_num']]:\n      if len(tag) < lesslength:\n        lesslength = len(tag)\n    if lesslength <= 2:\n      print(\"Feature num: \"+str(row['feature_num'])+\" Feature text: \"+str(row[\"feature_text\"]))\n      print(\"=\"*64)\n      print(dictionary_features[row['feature_num']])\n      print(\" \")\n      ambiguous_tags.append({\"feature_num\":row['feature_num'],\"feature_text\":row[\"feature_text\"]})\n\ndf_ambiguous_tags = pd.DataFrame(ambiguous_tags)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:45.198892Z","iopub.execute_input":"2022-05-03T22:15:45.199163Z","iopub.status.idle":"2022-05-03T22:15:45.237262Z","shell.execute_reply.started":"2022-05-03T22:15:45.199134Z","shell.execute_reply":"2022-05-03T22:15:45.236518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Definition of feature categories**","metadata":{}},{"cell_type":"markdown","source":"Duplicate features exist within the feature dataset.\n\nSince some features represent the same concepts as others, it is convenient to create feature categories to unite the tags and thus be able to carry out a more comprehensive search.\n\nFor example, in the column \"feature_text\" the words \"Female\" and \"Male\" are repeated","metadata":{}},{"cell_type":"code","source":"df_ambiguous_tags.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:45.241133Z","iopub.execute_input":"2022-05-03T22:15:45.24134Z","iopub.status.idle":"2022-05-03T22:15:45.252805Z","shell.execute_reply.started":"2022-05-03T22:15:45.241308Z","shell.execute_reply":"2022-05-03T22:15:45.252071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Categories:\n\n* 0: It is a feature that does not need any special treatment\n* 1: females, needs a global tag list and special treatment\n* 2: males, needs a global tag list and special treatment\n* 3: \"num\" - year, needs a function that generates a list of tags for each \"num\"","metadata":{}},{"cell_type":"code","source":"df_features.head(200)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:45.254506Z","iopub.execute_input":"2022-05-03T22:15:45.255022Z","iopub.status.idle":"2022-05-03T22:15:45.270267Z","shell.execute_reply.started":"2022-05-03T22:15:45.254958Z","shell.execute_reply":"2022-05-03T22:15:45.269464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_categories = [0,0,0,0,0,0,0,0,0,0,0,3,2,0,0,0,0,3,0,0,0,0,0,0,0,1,0,0,0,0,\n 0,0,0,0,1,0,0,0,0,0,0,0,3,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,3,0,0,0,\n 0,0,0,0,1,0,3,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0,\n 2,3,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,3,0,0,0,1,0,0,\n 3,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0]\n\ndf_features[\"feature_cat\"] = pd.Series(feature_categories)\n\ndf_features.head(200)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:45.271959Z","iopub.execute_input":"2022-05-03T22:15:45.272249Z","iopub.status.idle":"2022-05-03T22:15:45.294028Z","shell.execute_reply.started":"2022-05-03T22:15:45.272215Z","shell.execute_reply":"2022-05-03T22:15:45.292948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Addition of representative tags**","metadata":{}},{"cell_type":"markdown","source":"The tags encompass common concepts, in case of not detecting text segments, the software will at least be able to identify the main tags.\n\nTherefore, artificial tags will be added and registered in a dataset","metadata":{}},{"cell_type":"code","source":"history_artificial_tags = []\n\n\n\nfeature_num = 0\nadditional_tags = [\"heart attack\",\"cardiac issues\",\"cardiac issue\",\"heart problem\",\"heart disease\",\"heart-attack\"\n,\"chest-pains\",\"chest pains\",\"chest-pain\",\"chest pain\",\"coronary-infarction\",\"coronary infarction\",\"infarction\",\"coronary\",\"heart-failure\",\n\"heart failure\",\"congestive-heart-failure\",\"myocardial-infarction\",\"myocardial infarction\",\"cardiac-arrest\",\"cardiac arrest\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 1\nadditional_tags = [\"thyroid problems\",\"thyroid-problems\",\"thyroid-problem\",\"thyroid issues\",\"thyroid issue\",\n\"thyroid-issues\",\"thyroid-issue\",\"hypothyroidism\",\"thyoid disorder\",\"thyoid-disorder\",\"thyroid disease\",\"thyroidal\",\n\"pituitary\",\"thyroid-gland\",\"thyroid gland\",\"thyroid\",\"parathyroid\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 2\nadditional_tags = [\"chest-pressure\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 4\nadditional_tags = [\"empty-headed\",\"empty headed\",\"delirious\",\"pass out\",\"passed out\",\"dizzy\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 5\nadditional_tags = [\"hair changes\",\"hair chages\",\"nail changes\",\"nail chages\",\"heat intolerance\",\"temperature intolerance\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 10\nadditional_tags = [\"few months\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 100\nadditional_tags = [\"no vaginal discharge\",\"no discharge\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 102\nadditional_tags = [\"no sexual activity\",\"no seuxual actvity\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 103\nadditional_tags = [\"diarrhoea\",\"constipation\",\"collywobbles\",\"lientery\",\"indigestion\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 204\nadditional_tags = [\"vaginal parchedness\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 205\nadditional_tags = [\"irregular-menses\",\"Irregular menses\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 206\nadditional_tags = [\"recent nausea\",\"recent vomiting\",\"recent flulike\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 207\nadditional_tags = [\"no premenstrual\",\"denies premenstrual\",\"no premenstrual symptoms\",\"denies premenstrual symptoms\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 209\nadditional_tags = [\"stress\",\"stressed\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 212\nadditional_tags = [\"irregular-flow\",\"irregular-frequency\",\"irregular frequency\",\"irregular-intervals\",\"Irregular intervals\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 214\nadditional_tags = [\"heavy-sweating\",\"heavy sweating\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 215\nadditional_tags = [\"sleep-disturbance\",\"sleep disturbance\",\"early-awakenings\",\"early awakenings\",\"awakes at\",\"insomnia\",\n                   \"inability-to-sleep\",\"insomnolence\",\"hypersomnia\",\"sleeplessness\",\"restlessness\",\"early waking\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 301\nadditional_tags = [\"epigastric-discomfort\",\"epigastric discomfort\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 313\nadditional_tags = [\"tums does not work\",\"tums do not work\",\"tums don't work\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 403\nadditional_tags = [\"excessive caffeine\",\"lot of coffie\",\"heavy caffeine\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 505\nadditional_tags = [\"2 weeks ago workup normal\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 512\nadditional_tags = [\"throat-tightness\",\"throat tightness\",\"throat tightening\",\"throat-tightening\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 600\nadditional_tags = [\"feeling 'hot'\",\"feeling 'warm'\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 606\ndictionary_features[feature_num].remove(\"cp\")\n\n\nfeature_num = 610\nadditional_tags = [\"albuterol no\",\"albuterol did not\",\"albuterol didn't\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 801\nadditional_tags = [\"son passed away\",\"son died\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 803\nadditional_tags = [\"hallucination\",\"auditory hallucination\",\"hearing noises\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 804\nadditional_tags = [\"tossing\",\"turning\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 807\nadditional_tags = [\"hallucinations after\",\"hallucination after\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 809\nadditional_tags = [\"unsuccessful napping\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 810\nadditional_tags = [\"ambient didn't\",\"ambient hasn't\",\"ambien didn't\",\"ambien hasn't\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 815\nadditional_tags = [\"wakes up at\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 904\nadditional_tags = [\"headache\"]\ndictionary_features[feature_num] += additional_tags\nhistory_artificial_tags += additional_tags\n\n\nfeature_num = 906\ndictionary_features[feature_num].remove(\"v\")\n\n\nfeature_num = 908\ndictionary_features[feature_num].remove(\"n\")\n\n\nfeature_num = 909\ndictionary_features[feature_num].remove(\"st\")","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:45.295742Z","iopub.execute_input":"2022-05-03T22:15:45.29624Z","iopub.status.idle":"2022-05-03T22:15:45.324257Z","shell.execute_reply.started":"2022-05-03T22:15:45.296199Z","shell.execute_reply":"2022-05-03T22:15:45.323465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Tags for female features**","metadata":{}},{"cell_type":"markdown","source":"This are all the \"female\" features.\n\nNow i am going to join all the tags of this section of the dataset to form a general list","metadata":{}},{"cell_type":"code","source":"mask_1 = df_features[\"feature_cat\"] == 1\ndf_features_1 = df_features[mask_1].copy()\ndf_features_1.head(100)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:45.326532Z","iopub.execute_input":"2022-05-03T22:15:45.327107Z","iopub.status.idle":"2022-05-03T22:15:45.344362Z","shell.execute_reply.started":"2022-05-03T22:15:45.327069Z","shell.execute_reply":"2022-05-03T22:15:45.343647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"general_female_tags = []\nfor index,row in df_features_1.iterrows():\n  for tag in dictionary_features[row[\"feature_num\"]]:\n    if tag not in general_female_tags and len(tag) > 1:\n      general_female_tags.append(tag)\ngeneral_female_tags.append(\" f \")\ngeneral_female_tags = list(dict.fromkeys(general_female_tags))\ngeneral_female_tags","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:45.486435Z","iopub.execute_input":"2022-05-03T22:15:45.486759Z","iopub.status.idle":"2022-05-03T22:15:45.497808Z","shell.execute_reply.started":"2022-05-03T22:15:45.486728Z","shell.execute_reply":"2022-05-03T22:15:45.496918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Tags for male features**","metadata":{}},{"cell_type":"markdown","source":"This are all the \"male\" features.\n\nNow i am going to join all the tags of this section of the dataset to form a general list","metadata":{}},{"cell_type":"code","source":"mask_2 = df_features[\"feature_cat\"] == 2\ndf_features_2 = df_features[mask_2].copy()\ndf_features_2.head(100)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:45.639691Z","iopub.execute_input":"2022-05-03T22:15:45.639972Z","iopub.status.idle":"2022-05-03T22:15:45.650184Z","shell.execute_reply.started":"2022-05-03T22:15:45.639943Z","shell.execute_reply":"2022-05-03T22:15:45.649383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"general_male_tags = []\nfor index,row in df_features_2.iterrows():\n  for tag in dictionary_features[row[\"feature_num\"]]:\n    if tag not in general_male_tags and len(tag) > 1:\n      general_male_tags.append(tag)\ngeneral_male_tags.append(\" m \")\ngeneral_male_tags = list(dict.fromkeys(general_male_tags))\ngeneral_male_tags","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:45.80715Z","iopub.execute_input":"2022-05-03T22:15:45.807551Z","iopub.status.idle":"2022-05-03T22:15:45.815613Z","shell.execute_reply.started":"2022-05-03T22:15:45.807518Z","shell.execute_reply":"2022-05-03T22:15:45.814727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Tags for num-year features**","metadata":{}},{"cell_type":"markdown","source":"This are all the \"num\"-year features.\n\nNow i am going to join all the tags of this section of the dataset to form a general list","metadata":{}},{"cell_type":"code","source":"mask_3 = df_features[\"feature_cat\"] == 3\ndf_features_3 = df_features[mask_3].copy()\ndf_features_3.head(100)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:45.973932Z","iopub.execute_input":"2022-05-03T22:15:45.974185Z","iopub.status.idle":"2022-05-03T22:15:45.985696Z","shell.execute_reply.started":"2022-05-03T22:15:45.974157Z","shell.execute_reply":"2022-05-03T22:15:45.984773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"general_num_year_tags = []\nfor index,row in df_features_3.iterrows():\n  for tag in dictionary_features[row[\"feature_num\"]]:\n    if tag not in general_num_year_tags and len(tag) > 1:\n      general_num_year_tags.append(tag)\ngeneral_num_year_tags = list(dict.fromkeys(general_num_year_tags))\n#general_num_year_tags","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:46.12895Z","iopub.execute_input":"2022-05-03T22:15:46.129559Z","iopub.status.idle":"2022-05-03T22:15:46.13559Z","shell.execute_reply.started":"2022-05-03T22:15:46.129527Z","shell.execute_reply":"2022-05-03T22:15:46.134553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"general_num_year_tags = ['xyo',\n 'x y/o','x yo','x year old','x-year-old','x y.o.','x','x yr old','x y.o','xy','x years old','x y/o','x yo','x year',\n 'x y','xy/o','xyo','x-year','x yr','x year old','x y o','x','x-year-old','x-year old','x y.o','x ye','x yo','x','x y.o','xyo',\n 'x yo f',\n 'x year old','x year','xf','x-year-old','x y/o','x years old','x y-o','x yr old','xyear','x year olf','x year old','xyo','x yo','x','x-year-old','x y.o',\n '5 year old',\n 'x y',\n 'x y.o.','x y/o','x yr old','x year old','x year','x-year','xyo','x yo','x yr','x y/o','x-year-old','x y o','x yo','x y.o',\n 'x',\n 'x yo',\n 'xy',\n 'x year old',\n 'x','x y/o','xyo','x-year-old','x','x y','x year','x yeo','x f','x years old','xy/o','xyr','x year','x  yo','xyr old',\n 'x y o',\n 'x yo old','x year-old','x-yo','x yo','x-year old','x f','x y o','x  yo','x y.o','xyo','x yo','x y old','x year old','x y/o','x yr',\n 'x',\n 'x-year-old',\n 'x y','x','x y.o.','x yr old','x y.o.','xyear old','x yo','x year-old','x yr old','x year olf','x y old','x yo','xyr old',\n 'x year odl']\n\ngeneral_num_year_tags = list(dict.fromkeys(general_num_year_tags))\ngeneral_num_year_tags","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-03T22:15:46.295409Z","iopub.execute_input":"2022-05-03T22:15:46.29562Z","iopub.status.idle":"2022-05-03T22:15:46.307477Z","shell.execute_reply.started":"2022-05-03T22:15:46.295595Z","shell.execute_reply":"2022-05-03T22:15:46.306595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_num_year_tags(num):\n  x = str(num)\n  return [x+'yo',x+' y/o',x+' yo',x+' year old',x+'-year-old',x+' y.o.',x+' yr old',x+' y.o',x+'y',x+' years old',x+' year',x+' y',x+'y/o',x+'-year',\n  x+' yr',x+' y o',x+'-year old',x+' ye',x+' yo f',x+'f',x+' y-o',x+'year',x+' year olf',x+' year old',x+' yeo',x+' f',x+'yr',x+'  yo',x+'yr old',\n  x+' yo old',x+' year-old',x+'-yo',x+' y old',x+'year old',x+' year odl']","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:46.450652Z","iopub.execute_input":"2022-05-03T22:15:46.450984Z","iopub.status.idle":"2022-05-03T22:15:46.457065Z","shell.execute_reply.started":"2022-05-03T22:15:46.450954Z","shell.execute_reply":"2022-05-03T22:15:46.456222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ALBERT model**","metadata":{}},{"cell_type":"markdown","source":"Obviously, category 0 features will require a model to extract the sentences in case the tag lists are not enough for the cognitive search.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tqdm.notebook import tqdm #Useful to monitor progress in loop\ntqdm.pandas()\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom transformers import PreTrainedTokenizerFast, TFAlbertModel, AlbertConfig\n\nimport re,os,random,math,pickle","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:46.616829Z","iopub.execute_input":"2022-05-03T22:15:46.617169Z","iopub.status.idle":"2022-05-03T22:15:49.064179Z","shell.execute_reply.started":"2022-05-03T22:15:46.617138Z","shell.execute_reply":"2022-05-03T22:15:49.063384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input length for AlBERT model\ninput_length = 512\n# Random seed\nrandom_seed = 42\n# List of models\nMODELS = []\n\nfeatures = pd.read_csv(root+\"features.csv\")\n\n# Additional feature_num\nfeatures['feature_num_ordinal'] = features['feature_num'].astype('category').cat.codes\n\n#This will be the dimension of the output in the model\nn_features = len(features)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:49.066068Z","iopub.execute_input":"2022-05-03T22:15:49.066354Z","iopub.status.idle":"2022-05-03T22:15:49.078176Z","shell.execute_reply.started":"2022-05-03T22:15:49.066308Z","shell.execute_reply":"2022-05-03T22:15:49.077247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predefined configuration\nalbert_xxlarge_config = AlbertConfig(\n  hidden_size = 4096,\n  intermediate_size = 16384,\n  max_position_embeddings = 512,\n  model_type = 'albert',\n  num_attention_heads = 64,\n)\n\nalbert_base_config = AlbertConfig(\n  hidden_size = 768,\n  intermediate_size = 3072,\n  max_position_embeddings = 512,\n  model_type = 'albert',\n  num_attention_heads = 12,\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:49.080516Z","iopub.execute_input":"2022-05-03T22:15:49.082135Z","iopub.status.idle":"2022-05-03T22:15:49.087619Z","shell.execute_reply.started":"2022-05-03T22:15:49.082095Z","shell.execute_reply":"2022-05-03T22:15:49.086661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_model(file_name, config):\n    # Input Layer\n    input_ids = tf.keras.layers.Input(shape = (input_length), dtype=tf.int32, name='input_ids')\n    attention_mask = tf.keras.layers.Input(shape=input_length, dtype=tf.int32, name='attention_mask')\n\n    # AlBERT Model\n    albert = TFAlbertModel(config)\n    last_hidden_state = albert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n    do = tf.keras.layers.Dropout(0.00, name='dropout')(last_hidden_state)\n\n    # Output layer gives probabilities of each token to belong to each feature\n    output = tf.keras.layers.Dense(n_features, activation='sigmoid', name='head/classifier')(do)\n\n    # Final model\n    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=[output])\n    \n    # Weights\n    if file_name is None:\n        model.load_weights('/kaggle/input/nbme-albert-large-training-tpu-dataset/model.h5')\n    else:\n        model.load_weights(f'/{file_name}.h5')\n    \n    # Append to Models List\n    MODELS.append(model)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:49.09012Z","iopub.execute_input":"2022-05-03T22:15:49.090647Z","iopub.status.idle":"2022-05-03T22:15:49.099716Z","shell.execute_reply.started":"2022-05-03T22:15:49.090607Z","shell.execute_reply":"2022-05-03T22:15:49.098908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clear backend\ntf.keras.backend.clear_session()\n# Enable XLA optmizations, it is a compiler that speeds up model training in tensorflow\ntf.config.optimizer.set_jit(True)\nalbert_xxlarge_v18_model = generate_model(None, albert_base_config)\n# Models\nfor m in MODELS:\n    print(m.summary())\n    print('\\n' * 3)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:49.10138Z","iopub.execute_input":"2022-05-03T22:15:49.101926Z","iopub.status.idle":"2022-05-03T22:15:57.689724Z","shell.execute_reply.started":"2022-05-03T22:15:49.101887Z","shell.execute_reply":"2022-05-03T22:15:57.688843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_notes = pd.read_csv(root+'patient_notes.csv')\npatient_notes = patient_notes.set_index(['case_num', 'pn_num'])\npatient_notes['pn_history_lower'] = patient_notes['pn_history'].str.lower()\npatient_notes.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:57.692203Z","iopub.execute_input":"2022-05-03T22:15:57.692482Z","iopub.status.idle":"2022-05-03T22:15:58.126398Z","shell.execute_reply.started":"2022-05-03T22:15:57.692445Z","shell.execute_reply":"2022-05-03T22:15:58.118873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading tokenizer\ntokenizer = PreTrainedTokenizerFast.from_pretrained('../input/nbme-preprocessing-albert-public/tokenizer')\n\n# This function tokenize the text according to a AlBERT model tokenizer\ndef tokenize(note):\n    return tokenizer(\n        note,\n        padding = 'max_length',\n        truncation = True,\n        max_length = input_length,\n        return_offsets_mapping = True,\n    )\n\ndef correct_probability_in_prediction(t_dec_prev, t_dec, t_dec_next, row_feature_num, prob):    \n    if row_feature_num == 70:\n        if t_dec in ['ms']:\n            return 1\n    elif row_feature_num == 107:\n        if re.fullmatch('\\d{1}', t_dec) or t_dec in ['months', 'mot', 'nh', 's']:\n            return 1\n    elif row_feature_num == 103:\n        if t_dec in ['unprotected', 'sex']:\n            return 1\n    elif row_feature_num == 92:\n        if t_dec == 'as' and t_dec_next == 'tham':\n            return 1\n        elif t_dec_prev == 'as' and t_dec == 'tham':\n            return 1\n    elif row_feature_num == 93:\n        if prob > 0.01 and t_dec in ['chest', 'pain']:\n            return 1\n    return prob\n\n\ndef find_all(a, b, offset=0):\n    res = []\n    # Find Ignoring Case\n    start_idx = a.lower().find(b.lower())\n    if start_idx != -1:\n        return [offset + start_idx] + find_all(a[start_idx + len(b):], b, offset=offset + start_idx + len(b))\n    else:\n        return []\n    \n# Returns the character position in the patient note\ndef get_char_pos(row, return_str):\n    annotation = row['t_dec']\n    start_ann, end_ann = row['om']\n    if start_ann == -1:\n        if return_str:\n             return chr(0)\n        else:\n            return (-1, -1)\n    \n    patient_note = patient_notes.loc[row['group_idx'], 'pn_history']\n    \n    starts_in_patient_note = find_all(patient_note, annotation)\n    \n    for start in starts_in_patient_note:\n        end = start + len(annotation)\n        if start >= start_ann - 1 and end <= end_ann + 1 and end <= len(patient_note):\n            if return_str:\n                 return patient_note[start:end]\n            else:\n                return start, end","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:58.127875Z","iopub.execute_input":"2022-05-03T22:15:58.12818Z","iopub.status.idle":"2022-05-03T22:15:58.238532Z","shell.execute_reply.started":"2022-05-03T22:15:58.128142Z","shell.execute_reply":"2022-05-03T22:15:58.237574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = 0.50\ntest = pd.read_csv(root+'test.csv')\ntest.head(100)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:58.239974Z","iopub.execute_input":"2022-05-03T22:15:58.240242Z","iopub.status.idle":"2022-05-03T22:15:58.260486Z","shell.execute_reply.started":"2022-05-03T22:15:58.240206Z","shell.execute_reply":"2022-05-03T22:15:58.259639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\ndef process_data(dataset):\n    dataset['feature_num_ordinal'] = features.set_index('feature_num').loc[dataset['feature_num'], 'feature_num_ordinal'].values\n    dataset = dataset.set_index(['case_num','pn_num'])\n    list_processed_data = []\n    for group_idx, group in tqdm(dataset.groupby(['case_num','pn_num'])):\n        # Patient Note\n        pn_history_clean = patient_notes.loc[group_idx,'pn_history_lower']\n        # Tokenize Patient Note\n        tokens = tokenize(pn_history_clean)\n    \n        # Token Properties\n        input_ids = tokens['input_ids']\n        attention_mask = tokens['attention_mask']\n        offset_mapping = tokens['offset_mapping']\n    \n        # Probabilities of each token belonging to each feature\n        y_pred = np.zeros(shape=[n_features, input_length], dtype=np.float32)\n    \n        for m in MODELS:\n            y_pred += m.predict_on_batch({\n                'input_ids': np.array([input_ids]),\n                'attention_mask': np.array([attention_mask]),\n            }).squeeze().T / len(MODELS)\n    \n        # Iterate over all features\n        for row_id, row_feature_num in group[['id','feature_num_ordinal']].itertuples(index=False, name=None):\n            annotation_found = False\n        \n            # Prediction per Feature Number\n            y_pred_row = y_pred[row_feature_num]\n        \n            om_pred = []\n            # Iterate over all offset mappings, input tokens and prediction probabilities\n            for idx, (om, t, prob) in enumerate(zip(offset_mapping, input_ids, y_pred_row)):\n                # Decode Token\n                t_dec = tokenizer.decode(t)\n            \n                t_dec_prev = tokenizer.decode(input_ids[idx - 1]) if idx > 0 else None\n                t_dec_next = tokenizer.decode(input_ids[idx + 1]) if idx < len(input_ids) - 1 else None\n                prob = correct_probability_in_prediction(t_dec_prev, t_dec, t_dec_next, row_feature_num, prob)\n            \n                # Minimum prediction threshold and token should not be utlity token (START, END, PAD etc.)\n                if prob > threshold and len(t_dec) > 0 and t > 4 :\n                    annotation_found = True\n                    list_processed_data.append({\n                        'row_id': row_id,\n                        'om': om,\n                        't': t,\n                        't_dec': t_dec,\n                        'group_idx': group_idx,\n                    })\n                \n            # Add Empty Annotation if no annotation is found\n            if not annotation_found:\n                list_processed_data.append({\n                    'row_id': row_id,\n                    'om': (-1,-1),\n                    't': -1,\n                    't_dec': chr(0),\n                    'group_idx': group_idx,\n                })\n                \n    df_data = pd.DataFrame.from_dict(list_processed_data)\n    df_data['om_original'] = df_data.progress_apply(get_char_pos, axis=1, return_str=False)\n    df_data['om_original_str'] = df_data.progress_apply(get_char_pos, axis=1, return_str=True)\n    return df_data\n\ndata_preprocessed = process_data(test)\ndata_preprocessed.head(100)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:15:58.264438Z","iopub.execute_input":"2022-05-03T22:15:58.266714Z","iopub.status.idle":"2022-05-03T22:16:05.08641Z","shell.execute_reply.started":"2022-05-03T22:15:58.266671Z","shell.execute_reply":"2022-05-03T22:16:05.085643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def albert_predict(processed_data):\n    submission_rows = []\n    for group_idx, group in tqdm(processed_data.groupby('row_id')):\n        start_prev = np.NINF\n        end_prev = np.NINF\n        location = ''\n        for start, end in group['om_original']:\n            # Previous token also belongs to location, increase end location\n            # i.e. 3:5 followed by 6:10 will have start 3 and end will be increased from 5 -> 10\n            if end_prev + 1 >= start and start <= end_prev + 2:\n                end_prev = end\n            else:\n                # Previous token does not belong to location, add current location\n                if end_prev != np.NINF:\n                    # After first location span is added, following location spans are delimited with a \";\"\n                    if len(location) > 0:\n                        location += f';{start_prev} {end_prev}'\n                    # First location span has no \";\"\n                    else:\n                        location += f'{start_prev} {end_prev}'\n                # First location span, set start and end\n                if start > -1 and end > -1:\n                    start_prev = start\n                    end_prev = end         \n        # Add last location\n        if end_prev > -1:\n            if len(location) > 0:\n                location += f';{start_prev} {end_prev}'\n            else:\n                location += f'{start_prev} {end_prev}'   \n        submission_rows.append({ 'id': group_idx, 'location': location })\n    return submission_rows\n\npredictions_test = pd.DataFrame.from_dict(albert_predict(data_preprocessed))\npredictions_test.head(100)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:05.089736Z","iopub.execute_input":"2022-05-03T22:16:05.09018Z","iopub.status.idle":"2022-05-03T22:16:05.148151Z","shell.execute_reply.started":"2022-05-03T22:16:05.09014Z","shell.execute_reply":"2022-05-03T22:16:05.147169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ROBERTA Model**","metadata":{}},{"cell_type":"code","source":"import os,re,ast,json,glob,numpy as np,pandas as pd\nfrom tqdm.notebook import tqdm\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nDATA_PATH = \"../input/nbme-score-clinical-patient-notes/\"\nOUT_PATH = \"../input/nbme-roberta-large/\"\nWEIGHTS_FOLDER = \"../input/nbme-roberta-large/\"\n\nNUM_WORKERS = 2\n\ndef process_feature_text(text):\n    text = re.sub('I-year', '1-year', text)\n    text = re.sub('-OR-', \" or \", text)\n    text = re.sub('-', ' ', text)\n    return text\n\n\ndef clean_spaces(txt):\n    txt = re.sub('\\n', ' ', txt)\n    txt = re.sub('\\t', ' ', txt)\n    txt = re.sub('\\r', ' ', txt)\n#     txt = re.sub(r'\\s+', ' ', txt)\n    return txt\n\n\ndef load_and_prepare_test(root=\"\"):\n    patient_notes = pd.read_csv(root + \"patient_notes.csv\")\n    features = pd.read_csv(root + \"features.csv\")\n    df = pd.read_csv(root + \"test.csv\")\n\n    df = df.merge(features, how=\"left\", on=[\"case_num\", \"feature_num\"])\n    df = df.merge(patient_notes, how=\"left\", on=['case_num', 'pn_num'])\n\n    df['pn_history'] = df['pn_history'].apply(lambda x: x.strip())\n    df['feature_text'] = df['feature_text'].apply(process_feature_text)\n\n    df['feature_text'] = df['feature_text'].apply(clean_spaces)\n    df['clean_text'] = df['pn_history'].apply(clean_spaces)\n\n    df['target'] = \"\"\n    return df\n\n\nimport itertools\n\n\ndef token_pred_to_char_pred(token_pred, offsets):\n    char_pred = np.zeros((np.max(offsets), token_pred.shape[1]))\n    for i in range(len(token_pred)):\n        s, e = int(offsets[i][0]), int(offsets[i][1])  # start, end\n        char_pred[s:e] = token_pred[i]\n\n        if token_pred.shape[1] == 3:  # following characters cannot be tagged as start\n            s += 1\n            char_pred[s: e, 1], char_pred[s: e, 2] = (\n                np.max(char_pred[s: e, 1:], 1),\n                np.min(char_pred[s: e, 1:], 1),\n            )\n\n    return char_pred\n\n\ndef labels_to_sub(labels):\n    all_spans = []\n    for label in labels:\n        indices = np.where(label > 0)[0]\n        indices_grouped = [\n            list(g) for _, g in itertools.groupby(\n                indices, key=lambda n, c=itertools.count(): n - next(c)\n            )\n        ]\n\n        spans = [f\"{min(r)} {max(r) + 1}\" for r in indices_grouped]\n        all_spans.append(\";\".join(spans))\n    return all_spans\n\n\ndef char_target_to_span(char_target):\n    spans = []\n    start, end = 0, 0\n    for i in range(len(char_target)):\n        if char_target[i] == 1 and char_target[i - 1] == 0:\n            if end:\n                spans.append([start, end])\n            start = i\n            end = i + 1\n        elif char_target[i] == 1:\n            end = i + 1\n        else:\n            if end:\n                spans.append([start, end])\n            start, end = 0, 0\n    return spans\n\n\nimport numpy as np\nfrom transformers import AutoTokenizer\n\n\ndef get_tokenizer(name, precompute=False, df=None, folder=None):\n    if folder is None:\n        tokenizer = AutoTokenizer.from_pretrained(name)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(folder)\n\n    tokenizer.name = name\n    tokenizer.special_tokens = {\n        \"sep\": tokenizer.sep_token_id,\n        \"cls\": tokenizer.cls_token_id,\n        \"pad\": tokenizer.pad_token_id,\n    }\n\n    if precompute:\n        tokenizer.precomputed = precompute_tokens(df, tokenizer)\n    else:\n        tokenizer.precomputed = None\n\n    return tokenizer\n\n\ndef precompute_tokens(df, tokenizer):\n    feature_texts = df[\"feature_text\"].unique()\n\n    ids = {}\n    offsets = {}\n\n    for feature_text in feature_texts:\n        encoding = tokenizer(\n            feature_text,\n            return_token_type_ids=True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        ids[feature_text] = encoding[\"input_ids\"]\n        offsets[feature_text] = encoding[\"offset_mapping\"]\n\n    texts = df[\"clean_text\"].unique()\n\n    for text in texts:\n        encoding = tokenizer(\n            text,\n            return_token_type_ids=True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        ids[text] = encoding[\"input_ids\"]\n        offsets[text] = encoding[\"offset_mapping\"]\n\n    return {\"ids\": ids, \"offsets\": offsets}\n\n\ndef encodings_from_precomputed(feature_text, text, precomputed, tokenizer, max_len=300):\n    tokens = tokenizer.special_tokens\n\n    # Input ids\n    if \"roberta\" in tokenizer.name:\n        qa_sep = [tokens[\"sep\"], tokens[\"sep\"]]\n    else:\n        qa_sep = [tokens[\"sep\"]]\n\n    input_ids = [tokens[\"cls\"]] + precomputed[\"ids\"][feature_text] + qa_sep\n    n_question_tokens = len(input_ids)\n\n    input_ids += precomputed[\"ids\"][text]\n    input_ids = input_ids[: max_len - 1] + [tokens[\"sep\"]]\n\n    # Token type ids\n    if \"roberta\" not in tokenizer.name:\n        token_type_ids = np.ones(len(input_ids))\n        token_type_ids[:n_question_tokens] = 0\n        token_type_ids = token_type_ids.tolist()\n    else:\n        token_type_ids = [0] * len(input_ids)\n\n    # Offsets\n    offsets = [(0, 0)] * n_question_tokens + precomputed[\"offsets\"][text]\n    offsets = offsets[: max_len - 1] + [(0, 0)]\n\n    # Padding\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([tokens[\"pad\"]] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        offsets = offsets + ([(0, 0)] * padding_length)\n\n    encoding = {\n        \"input_ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"offset_mapping\": offsets,\n    }\n\n    return encoding\n\n\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\n\n\nclass PatientNoteDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n\n        self.texts = df['clean_text'].values\n        self.feature_text = df['feature_text'].values\n        self.char_targets = df['target'].values.tolist()\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        feature_text = self.feature_text[idx]\n        char_target = self.char_targets[idx]\n\n        # Tokenize\n        if self.tokenizer.precomputed is None:\n            encoding = self.tokenizer(\n                feature_text,\n                text,\n                return_token_type_ids=True,\n                return_offsets_mapping=True,\n                return_attention_mask=False,\n                truncation=\"only_second\",\n                max_length=self.max_len,\n                padding='max_length',\n            )\n            raise NotImplementedError(\"fix issues with question offsets\")\n        else:\n            encoding = encodings_from_precomputed(\n                feature_text,\n                text,\n                self.tokenizer.precomputed,\n                self.tokenizer,\n                max_len=self.max_len\n            )\n\n        return {\n            \"ids\": torch.tensor(encoding[\"input_ids\"], dtype=torch.long),\n            \"token_type_ids\": torch.tensor(encoding[\"token_type_ids\"], dtype=torch.long),\n            \"target\": torch.tensor([0], dtype=torch.float),\n            \"offsets\": np.array(encoding[\"offset_mapping\"]),\n            \"text\": text,\n        }\n\n    def __len__(self):\n        return len(self.texts)\n\n    \nimport spacy\nimport numpy as np\n\ndef plot_annotation(df, pn_num):\n    options = {\"colors\": {}}\n\n    df_text = df[df[\"pn_num\"] == pn_num].reset_index(drop=True)\n\n    text = df_text[\"pn_history\"][0]\n    ents = []\n\n    for spans, feature_text, feature_num in df_text[[\"span\", \"feature_text\", \"feature_num\"]].values:\n        for s in spans:\n            ents.append({\"start\": int(s[0]), \"end\": int(s[1]), \"label\": feature_text})\n\n        options[\"colors\"][feature_text] =  f\"rgb{tuple(np.random.randint(100, 255, size=3))}\"\n\n    doc = {\"text\": text, \"ents\": sorted(ents, key=lambda i: i[\"start\"])}\n\n    spacy.displacy.render(doc, style=\"ent\", options=options, manual=True, jupyter=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:05.150013Z","iopub.execute_input":"2022-05-03T22:16:05.150324Z","iopub.status.idle":"2022-05-03T22:16:07.388086Z","shell.execute_reply.started":"2022-05-03T22:16:05.150282Z","shell.execute_reply":"2022-05-03T22:16:07.387317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport transformers\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModel\n\n\nclass NERTransformer(nn.Module):\n    def __init__(\n        self,\n        model,\n        num_classes=1,\n        config_file=None,\n        pretrained=True,\n    ):\n        super().__init__()\n        self.name = model\n        self.pad_idx = 1 if \"roberta\" in self.name else 0\n\n        transformers.logging.set_verbosity_error()\n\n        if config_file is None:\n            config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n        else:\n            config = torch.load(config_file)\n\n        if pretrained:\n            self.transformer = AutoModel.from_pretrained(model, config=config)\n        else:\n            self.transformer = AutoModel.from_config(config)\n\n        self.nb_features = config.hidden_size\n\n#         self.cnn = nn.Identity()\n        self.logits = nn.Linear(self.nb_features, num_classes)\n\n    def forward(self, tokens, token_type_ids):\n        \"\"\"\n        Usual torch forward function\n\n        Arguments:\n            tokens {torch tensor} -- Sentence tokens\n            token_type_ids {torch tensor} -- Sentence tokens ids\n        \"\"\"\n        hidden_states = self.transformer(\n            tokens,\n            attention_mask=(tokens != self.pad_idx).long(),\n            token_type_ids=token_type_ids,\n        )[-1]\n\n        features = hidden_states[-1]\n\n        logits = self.logits(features)\n\n        return logits\n    \n    \nimport torch\n\ndef load_model_weights(model, filename, verbose=1, cp_folder=\"\", strict=True):\n    \"\"\"\n    Loads the weights of a PyTorch model. The exception handles cpu/gpu incompatibilities.\n\n    Args:\n        model (torch model): Model to load the weights to.\n        filename (str): Name of the checkpoint.\n        verbose (int, optional): Whether to display infos. Defaults to 1.\n        cp_folder (str, optional): Folder to load from. Defaults to \"\".\n        strict (bool, optional): Whether to allow missing/additional keys. Defaults to False.\n\n    Returns:\n        torch model: Model with loaded weights.\n    \"\"\"\n    if verbose:\n        print(f\"\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n\")\n\n    try:\n        model.load_state_dict(\n            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n            strict=strict,\n        )\n    except RuntimeError:\n        model.encoder.fc = torch.nn.Linear(model.nb_ft, 1)\n        model.load_state_dict(\n            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n            strict=strict,\n        )\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:07.38953Z","iopub.execute_input":"2022-05-03T22:16:07.389805Z","iopub.status.idle":"2022-05-03T22:16:07.409852Z","shell.execute_reply.started":"2022-05-03T22:16:07.389769Z","shell.execute_reply":"2022-05-03T22:16:07.408956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\n\n\ndef predict(model, dataset, data_config, activation=\"softmax\"):\n    \"\"\"\n    Usual predict torch function\n    \"\"\"\n    model.eval()\n\n    loader = DataLoader(\n        dataset,\n        batch_size=data_config['val_bs'],\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n\n    preds = []\n    with torch.no_grad():\n        for data in tqdm(loader):\n            ids, token_type_ids = data[\"ids\"], data[\"token_type_ids\"]\n\n            y_pred = model(ids.cuda(), token_type_ids.cuda())\n\n            if activation == \"sigmoid\":\n                y_pred = y_pred.sigmoid()\n            elif activation == \"softmax\":\n                y_pred = y_pred.softmax(-1)\n\n            preds += [\n                token_pred_to_char_pred(y, offsets) for y, offsets\n                in zip(y_pred.detach().cpu().numpy(), data[\"offsets\"].numpy())\n            ]\n\n    return preds\n\n\ndef inference_test(df, exp_folder, config, cfg_folder=None):\n    preds = []\n\n    if cfg_folder is not None:\n        model_config_file = cfg_folder + config.name.split('/')[-1] + \"/config.pth\"\n        tokenizer_folder = cfg_folder + config.name.split('/')[-1] + \"/tokenizers/\"\n    else:\n        model_config_file, tokenizer_folder = None, None\n\n    tokenizer = get_tokenizer(\n        config.name, precompute=config.precompute_tokens, df=df, folder=tokenizer_folder\n    )\n\n    dataset = PatientNoteDataset(\n        df,\n        tokenizer,\n        max_len=config.max_len,\n    )\n\n    model = NERTransformer(\n        config.name,\n        num_classes=config.num_classes,\n        config_file=model_config_file,\n        pretrained=False\n    ).cuda()\n    model.zero_grad()\n\n    weights = sorted(glob.glob(exp_folder + \"*.pt\"))\n    for weight in weights:\n        model = load_model_weights(model, weight)\n\n        pred = predict(\n            model,\n            dataset,\n            data_config=config.data_config,\n            activation=config.loss_config[\"activation\"]\n        )\n        preds.append(pred)\n\n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:07.411239Z","iopub.execute_input":"2022-05-03T22:16:07.411707Z","iopub.status.idle":"2022-05-03T22:16:07.42701Z","shell.execute_reply.started":"2022-05-03T22:16:07.411669Z","shell.execute_reply":"2022-05-03T22:16:07.426238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    # Architecture\n    name = \"roberta-large\"\n    num_classes = 1\n\n    # Texts\n    max_len = 310\n    precompute_tokens = True\n\n    # Training    \n    loss_config = {\n        \"activation\": \"sigmoid\",\n    }\n\n    data_config = {\n        \"val_bs\": 16 if \"large\" in name else 32,\n        \"pad_token\": 1 if \"roberta\" in name else 0,\n    }\n\n    verbose = 1","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:07.428856Z","iopub.execute_input":"2022-05-03T22:16:07.429877Z","iopub.status.idle":"2022-05-03T22:16:07.439914Z","shell.execute_reply.started":"2022-05-03T22:16:07.429824Z","shell.execute_reply":"2022-05-03T22:16:07.439147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def post_process_spaces(target, text):\n    target = np.copy(target)\n\n    if len(text) > len(target):\n        padding = np.zeros(len(text) - len(target))\n        target = np.concatenate([target, padding])\n    else:\n        target = target[:len(text)]\n\n    if text[0] == \" \":\n        target[0] = 0\n    if text[-1] == \" \":\n        target[-1] = 0\n\n    for i in range(1, len(text) - 1):\n        if text[i] == \" \":\n            if target[i] and not target[i - 1]:  # space before\n                target[i] = 0\n\n            if target[i] and not target[i + 1]:  # space after\n                target[i] = 0\n\n            if target[i - 1] and target[i + 1]:\n                target[i] = 1\n\n    return target","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:07.441944Z","iopub.execute_input":"2022-05-03T22:16:07.44314Z","iopub.status.idle":"2022-05-03T22:16:07.453769Z","shell.execute_reply.started":"2022-05-03T22:16:07.443095Z","shell.execute_reply":"2022-05-03T22:16:07.452973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\"\n!pip install GPUtil\n\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache()   \n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:07.455123Z","iopub.execute_input":"2022-05-03T22:16:07.455952Z","iopub.status.idle":"2022-05-03T22:16:07.465039Z","shell.execute_reply.started":"2022-05-03T22:16:07.455912Z","shell.execute_reply":"2022-05-03T22:16:07.46417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\"\"\"\nimport os\nprint(os.chdir(\"/kaggle/input/gputil/GPUtil\"))\nprint(os.getcwd())\nprint(os.listdir(\"/kaggle/input/gputil/GPUtil\"))\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:51:49.202999Z","iopub.execute_input":"2022-05-03T22:51:49.203756Z","iopub.status.idle":"2022-05-03T22:51:49.209508Z","shell.execute_reply.started":"2022-05-03T22:51:49.203719Z","shell.execute_reply":"2022-05-03T22:51:49.208676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install GPUtil\nos.chdir(\"/kaggle/input/gputil/GPUtil\")\nimport GPUtil\n\nimport torch\ntorch.cuda.empty_cache()\n\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache()\n\nos.chdir(firstdir)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:07.466377Z","iopub.execute_input":"2022-05-03T22:16:07.46785Z","iopub.status.idle":"2022-05-03T22:16:08.665831Z","shell.execute_reply.started":"2022-05-03T22:16:07.467821Z","shell.execute_reply":"2022-05-03T22:16:08.664742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = load_and_prepare_test(root=DATA_PATH)\ndisplay(df_test.head())\n\npreds = inference_test(\n    df_test,\n    WEIGHTS_FOLDER,\n    Config,\n    cfg_folder=OUT_PATH\n)[0]\n\ndf_test['preds'] = preds\ndf_test['preds'] = df_test.apply(lambda x: x['preds'][:len(x['clean_text'])], 1)\n\ndf_test['preds'] = df_test['preds'].apply(lambda x: (x > 0.5).flatten())\n\ndf_test['preds_pp'] = df_test.apply(lambda x: post_process_spaces(x['preds'], x['clean_text']), 1)\n\ndf_test['location'] = labels_to_sub(df_test['preds_pp'].values)\n\ndf_test[[\"id\",\"location\"]].head(20)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:08.667886Z","iopub.execute_input":"2022-05-03T22:16:08.668252Z","iopub.status.idle":"2022-05-03T22:16:20.988178Z","shell.execute_reply.started":"2022-05-03T22:16:08.668208Z","shell.execute_reply":"2022-05-03T22:16:20.987354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def roberta_prediction(x):\n    df = x.copy()\n    patient_notes = pd.read_csv(root + \"patient_notes.csv\")\n    features = pd.read_csv(root + \"features.csv\")\n    df = df.merge(features, how=\"left\", on=[\"case_num\", \"feature_num\"])\n    df = df.merge(patient_notes, how=\"left\", on=['case_num', 'pn_num'])\n    df['pn_history'] = df['pn_history'].apply(lambda x: x.strip())\n    df['feature_text'] = df['feature_text'].apply(process_feature_text)\n    df['feature_text'] = df['feature_text'].apply(clean_spaces)\n    df['clean_text'] = df['pn_history'].apply(clean_spaces)\n    df['target'] = \"\"\n    preds = inference_test(\n        df,\n        WEIGHTS_FOLDER,\n        Config,\n        cfg_folder=OUT_PATH\n    )[0]\n    df['preds'] = preds\n    df['preds'] = df.apply(lambda x: x['preds'][:len(x['clean_text'])], 1)\n    df['preds'] = df['preds'].apply(lambda x: (x > 0.5).flatten())\n    df['preds_pp'] = df.apply(lambda x: post_process_spaces(x['preds'], x['clean_text']), 1)\n    df['location'] = labels_to_sub(df['preds_pp'].values)\n    return df['location'].to_list()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:20.989776Z","iopub.execute_input":"2022-05-03T22:16:20.990382Z","iopub.status.idle":"2022-05-03T22:16:21.000451Z","shell.execute_reply.started":"2022-05-03T22:16:20.990339Z","shell.execute_reply":"2022-05-03T22:16:20.999395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndftest = []\ndftest.append({\"id\":\"00016_000\",\"case_num\":0,\"pn_num\":16,\"feature_num\":0})\ndftest = pd.DataFrame(dftest)\nlocation = roberta_prediction(dftest)[0]\nlocation\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:21.002069Z","iopub.execute_input":"2022-05-03T22:16:21.002792Z","iopub.status.idle":"2022-05-03T22:16:21.018131Z","shell.execute_reply.started":"2022-05-03T22:16:21.002752Z","shell.execute_reply":"2022-05-03T22:16:21.017057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install GPUtil --target=/kaggle/working/GPUtil","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:21.019497Z","iopub.execute_input":"2022-05-03T22:16:21.020303Z","iopub.status.idle":"2022-05-03T22:16:21.027726Z","shell.execute_reply.started":"2022-05-03T22:16:21.020246Z","shell.execute_reply":"2022-05-03T22:16:21.026844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Final solution**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Next, the final solution proposed by your server will be developed.\n\nIt is divided into the following steps:\n\n1) Convert the entire clinical note to lowercase\n\n2) Identify the category of each feature\n\n3) If it is category 0, perform the search by tags\n\n3) In case it is category 1, perform the search with the female list\n\n4) In case it is category 2, perform the search with the male list\n\n5) If it is category 3, generate the list based on age. Then perform the search with the previously made list","metadata":{}},{"cell_type":"markdown","source":"The dataset below is used for an evaluation of the software","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(root+\"test.csv\")\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:21.029279Z","iopub.execute_input":"2022-05-03T22:16:21.02983Z","iopub.status.idle":"2022-05-03T22:16:21.046674Z","shell.execute_reply.started":"2022-05-03T22:16:21.02979Z","shell.execute_reply":"2022-05-03T22:16:21.046025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_0 = df_features[\"feature_cat\"] == 0\ndf_features_0 = df_features[mask_0].copy()\nlist_0 = df_features_0[\"feature_num\"].tolist()\n\nmask_1 = df_features[\"feature_cat\"] == 1\ndf_features_1 = df_features[mask_1].copy()\nlist_1 = df_features_1[\"feature_num\"].tolist()\n\nmask_2 = df_features[\"feature_cat\"] == 2\ndf_features_2 = df_features[mask_2].copy()\nlist_2 = df_features_2[\"feature_num\"].tolist()\n\nmask_3 = df_features[\"feature_cat\"] == 3\ndf_features_3 = df_features[mask_3].copy()\nlist_3 = df_features_3[\"feature_num\"].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:21.047949Z","iopub.execute_input":"2022-05-03T22:16:21.04842Z","iopub.status.idle":"2022-05-03T22:16:21.058905Z","shell.execute_reply.started":"2022-05-03T22:16:21.048383Z","shell.execute_reply":"2022-05-03T22:16:21.058113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def corregir_locaciones_irrelevantes(locations):\n    locations_numbers = []\n    split_locs = locations.split(\";\")\n    for loc in split_locs:\n        numbers = loc.split(\" \")\n        left = int(numbers[0])\n        right = int(numbers[1])\n        locations_numbers += np.arange(left,right+1).tolist()\n    locations_numbers = list(dict.fromkeys(locations_numbers))\n    locations_numbers.sort()\n    result = \"\"\n    for n in locations_numbers:\n        if (result == \"\") or (result[-1] == \";\"):\n            result += str(n)+\"\"\n        if (n+1) in locations_numbers:\n            continue;\n        else:\n            result += \" \"+str(n)+\";\"\n    if result[-1] == \";\":\n        result = result[:-1]\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:21.059965Z","iopub.execute_input":"2022-05-03T22:16:21.061831Z","iopub.status.idle":"2022-05-03T22:16:21.072267Z","shell.execute_reply.started":"2022-05-03T22:16:21.061799Z","shell.execute_reply":"2022-05-03T22:16:21.071416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib widget","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:21.073935Z","iopub.execute_input":"2022-05-03T22:16:21.074249Z","iopub.status.idle":"2022-05-03T22:16:21.098739Z","shell.execute_reply.started":"2022-05-03T22:16:21.074209Z","shell.execute_reply":"2022-05-03T22:16:21.097852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def delete_dupicated_spaces(text):\n    return re.sub(' +', ' ',text)\n\ndef count_split(text,delimiter=\" \"):\n    return len(text.split(delimiter))","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:21.100933Z","iopub.execute_input":"2022-05-03T22:16:21.101224Z","iopub.status.idle":"2022-05-03T22:16:21.108021Z","shell.execute_reply.started":"2022-05-03T22:16:21.101184Z","shell.execute_reply":"2022-05-03T22:16:21.107293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate random integer values\nfrom random import seed\nfrom random import randint\n# seed random number generator\n# generate some integers\nvalue = randint(0, 10)\nprint(value)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:19:35.532481Z","iopub.execute_input":"2022-05-03T23:19:35.532739Z","iopub.status.idle":"2022-05-03T23:19:35.537588Z","shell.execute_reply.started":"2022-05-03T23:19:35.53271Z","shell.execute_reply":"2022-05-03T23:19:35.536559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def busqueda_caracteristicas(df):\n  count_rows = df.shape[0]\n  i = 0\n  predictions = []\n  for index, row in df.iterrows():\n    if i%100 == 0:\n        print(\"Progress: \"+str(round((i/count_rows)*100,2)))\n    mask = df_patient_notes[\"pn_num\"] == row[\"pn_num\"]\n    df_masked = df_patient_notes[mask].copy()\n    history = df_masked.iloc[0][\"pn_history\"]\n    history = history.lower()\n    locations = [] \n    if row[\"feature_num\"] in list_0:                              #CATEGORY 0#\n      for tag in dictionary_features[row[\"feature_num\"]]:         \n        locs = obtnener_coordenadas_tag(tag,history)\n        if (tag in history_artificial_tags) and (locs != None):\n            tag_cleaned = delete_dupicated_spaces(tag)\n            if (count_split(tag_cleaned,\" \") <= 2) or (count_split(tag_cleaned,\"-\") <= 5):\n                tokens = nltk.sent_tokenize(history)\n                for t in tokens:\n                    tokens2 = t.splitlines()\n                    for t2 in tokens2:\n                        if tag in t2:\n                            locs_ = obtnener_coordenadas_tag(t2,history)\n                            if locs_ != None:\n                                locations.append(locs_)\n                        \n        if locs != None:\n          locations.append(locs)\n    elif row[\"feature_num\"] in list_1:                           #CATEGORY 1#\n      for tag in general_female_tags:\n        if tag == \" f \":\n          locs = obtnener_coordenadas_tag(tag,history,1,-1)\n        else:\n          locs = obtnener_coordenadas_tag(tag,history)\n        if locs != None:\n          locations.append(locs)\n\n    elif row[\"feature_num\"] in list_2:                          #CATEGORY 2#\n      for tag in general_male_tags:\n        if tag == \" m \":\n          locs = obtnener_coordenadas_tag(tag,history,1,-1)\n        else:\n          locs = obtnener_coordenadas_tag(tag,history)\n        if locs != None:\n          locations.append(locs)\n\n    elif row[\"feature_num\"] in list_3:                           #CATEGORY 3#\n      num = int(re.findall(r'\\b\\d+\\b', df_features[df_features[\"feature_num\"] == row[\"feature_num\"]].iloc[0,2])[0])\n      list_tags = generate_num_year_tags(num)\n      for tag in list_tags:\n        locs = obtnener_coordenadas_tag(tag,history)\n        if locs != None:\n          locations.append(locs)\n\n    if (len(locations) == 0) and (row[\"feature_num\"] in list_0):\n      if randint(0, 10) % 2 == 0:\n          row_1 = pd.DataFrame([{\"id\":row[\"id\"],\"case_num\":row[\"case_num\"],\"pn_num\":row[\"pn_num\"],\"feature_num\":row[\"feature_num\"]}])\n          listlocation = roberta_prediction(row_1)\n          locations = \";\".join(listlocation)\n          #row_1_processed = process_data(row_1)\n          #row_1_prediction = albert_predict(row_1_processed)\n          #locations = row_1_prediction[0][\"location\"]\n          if locations == \"\":\n            locations = np.nan\n    elif len(locations) == 0:\n      locations = np.nan\n    else:\n      locations = \";\".join(locations)\n      locations = corregir_locaciones_irrelevantes(locations)\n    predictions.append({\"id\":row[\"id\"],\"location\":locations})\n    i += 1\n  return pd.DataFrame(predictions)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:21.111479Z","iopub.execute_input":"2022-05-03T22:16:21.111976Z","iopub.status.idle":"2022-05-03T22:16:21.131834Z","shell.execute_reply.started":"2022-05-03T22:16:21.111944Z","shell.execute_reply":"2022-05-03T22:16:21.131115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_preds = busqueda_caracteristicas(df_test)\ndf_preds.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:16:21.136658Z","iopub.execute_input":"2022-05-03T22:16:21.137083Z","iopub.status.idle":"2022-05-03T22:16:21.166832Z","shell.execute_reply.started":"2022-05-03T22:16:21.137048Z","shell.execute_reply":"2022-05-03T22:16:21.166145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"/kaggle/input/\")\ndf_preds.to_csv(\"../working/submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:52:22.928066Z","iopub.execute_input":"2022-05-03T22:52:22.92869Z","iopub.status.idle":"2022-05-03T22:52:22.935711Z","shell.execute_reply.started":"2022-05-03T22:52:22.92865Z","shell.execute_reply":"2022-05-03T22:52:22.934664Z"},"trusted":true},"execution_count":null,"outputs":[]}]}