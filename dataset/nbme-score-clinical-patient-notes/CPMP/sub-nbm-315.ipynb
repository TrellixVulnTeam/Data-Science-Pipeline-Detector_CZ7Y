{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import argparse\nimport os\nimport sys\nfrom pathlib import Path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_coeff = {\n    'nbme_182': 0.55 * 0.5,\n    'nbme_182_full': 0.55 * 0.5,\n    'nbme_254': 0.55 * 0.5,\n    'nbme_254_full': 0.55 * 0.5,\n    'nbme_187': 0.45 * 0.60,\n    'nbme_187_full': 0.45 * 0.60,\n    'nbme_256': 0.45 * 0.40,\n    'nbme_256_full': 0.45 * 0.40,\n}\n\npred_thr = -0.20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_coeff = {\n    'nbme_315': 1,\n}\n\npred_thr = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport copy\nimport time\nimport random\nimport string\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Utils\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport itertools\n\n# Sklearn Imports\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW, AutoModelForQuestionAnswering\nfrom transformers import RobertaTokenizerFast\nfrom tokenizers import AddedToken\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nos.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n\ndevice = torch.device('cuda')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEBUG = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    test = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')[:2000]\nelse:\n    test = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\ntest","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\nfeature_year = features.loc[[ft.endswith('year') for ft in features.feature_text]]\nfeature_year = feature_year.feature_num.values\nfeature_female = features.loc[[ft == 'Female' for ft in features.feature_text]]\nfeature_female = feature_female.feature_num.values\nfeature_male = features.loc[[ft == 'Male' for ft in features.feature_text]]\nfeature_male = feature_male.feature_num.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_notes = notes[notes.pn_num.isin(set(test.pn_num.unique()))].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_notes['text_length'] = [len(pn_history) for pn_history in test_notes.pn_history]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.merge(test_notes[['pn_num', 'text_length']], how='left', on='pn_num')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.sort_values(by='text_length').reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_lf(text, tokenizer):\n    res_l = [ tokenizer(t, \n                        return_offsets_mapping=True, \n                        return_attention_mask=False,\n                        return_token_type_ids=False,\n                        add_special_tokens=False, \n                        max_length=CONFIG['max_length'], \n                        truncation=True,\n                        padding=False,\n                       ) for t in text.split('\\n') ]\n    r = res_l[0]\n    res = {'input_ids':r['input_ids'], 'offset_mapping':r['offset_mapping']}\n    cum_len = res['offset_mapping'][-1][1]\n    for r in res_l[1:]:\n        res['input_ids'].append(tokenizer.lf_token_id)\n        res['offset_mapping'].append((cum_len, cum_len+1))\n        cum_len = cum_len + 1\n        res['input_ids'].extend(r['input_ids'])\n        res['offset_mapping'].extend([(start+cum_len, end+cum_len) for (start, end) in r['offset_mapping']])\n        cum_len = res['offset_mapping'][-1][1]\n    return res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NBMEDataset(Dataset):\n    def __init__(self, annotations, CONFIG, features, notes,):\n        super(NBMEDataset, self).__init__()\n        self.id = annotations.id.values\n        self.pn_num = annotations.pn_num.values\n        self.feature_num = annotations.feature_num.values\n        try:\n            self.location = annotations.location.values\n        except:\n            self.location = None\n        self.feature_token = self.tokenize(features, 'feature_num', 'feature_text', CONFIG)\n        \n        self.pn_history_token = self.tokenize(notes[notes.pn_num.isin(set(annotations.pn_num.unique()))], \n                                              'pn_num', 'pn_history', CONFIG)\n        self.max_length = CONFIG['max_length']\n        tokenizer = CONFIG['tokenizer']\n        self.special_tokens = {\n            \"sep\": tokenizer.sep_token_id,\n            \"cls\": tokenizer.cls_token_id,\n            \"pad\": tokenizer.pad_token_id,            \n        }\n        self.config = CONFIG\n        \n    def __len__(self):\n        return len(self.pn_num)\n    \n    def __getitem__(self, idx):        \n        pn_num = self.pn_num[idx]\n        pn_history_token = self.pn_history_token[pn_num]\n        feature_num = self.feature_num[idx]\n        feature_token = self.feature_token[feature_num]\n        location = None\n        if self.location is not None:\n            location = self.location[idx]\n        data = self.get_data(pn_history_token, feature_token, location, feature_num)\n        data.update({\n            'id':self.id[idx],\n        })\n        return data\n    \n    def get_data(self, pn_history_token, feature_token, location, feature_num, ):\n        max_length = self.max_length\n        text = pn_history_token['text']\n        pn_history_token = pn_history_token['tokens']\n        feature_token = feature_token['tokens']\n        \n        sep = self.special_tokens[\"sep\"]\n        cls = self.special_tokens[\"cls\"]\n        pad = self.special_tokens[\"pad\"]\n        q_input_ids = [cls] + feature_token['input_ids'] + [sep]\n        if \"roberta\" in self.config['model_name']:\n            q_input_ids = q_input_ids + [sep]       \n        input_ids = q_input_ids + pn_history_token['input_ids']\n        input_ids = input_ids[: max_length - 1] + [self.special_tokens[\"sep\"]]\n        len_token = len(input_ids)\n        \n        offset_mapping = [(0,0)] * len(q_input_ids) + pn_history_token['offset_mapping']\n        offset_mapping = offset_mapping[: max_length - 1] + [(0,0)]\n        max_token = len(text)\n        assert(len_token == len(offset_mapping))\n        \n        len_padding = max_length - len_token\n        if len_padding > 0:\n            input_ids = input_ids + [self.special_tokens[\"pad\"]] * len_padding\n            \n        attention_mask = np.zeros(max_length, dtype='int')\n        attention_mask[:len_token] = 1\n        \n        if \"roberta\" in self.config['model_name']:\n            token_type_ids = [0]\n        else:\n            token_type_ids = np.ones(max_length)\n            token_type_ids[:len(q_input_ids)] = 0\n            \n        out_dict = {\n            'input_ids' : torch.tensor(input_ids, dtype=torch.long),\n            'token_type_ids' :  torch.tensor(token_type_ids, dtype=torch.long),\n            'offset_mapping' : offset_mapping,\n            'attention_mask' : torch.tensor(attention_mask, dtype=torch.long),\n            'len_token' : len_token,\n            'max_token' : max_token,\n            'text' : text,\n            'feature_num' : feature_num,\n        }  \n        \n        if self.location is not None:\n            len_text = len(text)\n            char_type = np.zeros((len_text, ))\n            char_start = np.zeros((len_text, ))\n            char_end = np.zeros((len_text, ))\n            annots = eval(location)\n            annots = [a for ans in annots for a in ans.split(';')]\n            for annot in annots:\n                annot = annot.split()            \n                start = int(annot[0])\n                end = int(annot[1])\n                #print(feature, row['feature_num'], start, end)\n                char_type[start:end] = 1\n                char_start[start] = 1\n                char_end[end - 1] = 1\n            token_type = np.zeros((max_length, ))\n            token_start = np.zeros((max_length, ))\n            token_end = np.zeros((max_length, ))\n            for i, (start, end) in enumerate(offset_mapping):\n                if start == end:\n                    continue\n                token_type[i] = char_type[start:end].max(0)\n                token_start[i] = char_start[start:end].max(0)\n                token_end[i] = char_end[start:end].max(0)\n            out_dict.update({\n                'token_type':torch.tensor(token_type, dtype=torch.float32).unsqueeze(-1),\n                'token_start':torch.tensor(token_start, dtype=torch.float32).unsqueeze(-1),\n                'token_end':torch.tensor(token_end, dtype=torch.float32).unsqueeze(-1),\n            })\n        \n        return out_dict\n    \n    def tokenize(self, data, key, text, CONFIG):\n        tokenizer = CONFIG['tokenizer']\n        res = {k:{'tokens':tokenize_lf(text, tokenizer), 'text':text,}\n               for k,text in zip(data[key], data[text])}\n        return res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_collate_keys = ['id', 'offset_mapping', 'len_token', 'max_token', 'text', 'feature_num']\ncollate_keys = ['input_ids', 'token_type_ids', 'attention_mask', \n                #'token_type', 'token_start', 'token_end',\n               ]\n\ndef feedback_collate(batch):\n    batch_dict = {}\n    len_token_max = np.max([sample['len_token'] for sample in batch])\n    for key in collate_keys:\n        try:\n            batch_dict[key] = torch.stack([b[key][:len_token_max] for b in batch])\n        except:\n            print('key not found:', key)\n    for key in not_collate_keys:\n        if key == 'offset_mapping':\n            batch_dict[key] = [b[key][:len_token_max] for b in batch]\n        else:\n            batch_dict[key] = [b[key] for b in batch]\n    return batch_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data_loader(data, shuffle, CONFIG, features=features, notes=notes,):\n    if shuffle:\n        batch_size = CONFIG['train_batch_size']\n    else:\n        batch_size = CONFIG['valid_batch_size']\n    dataset = NBMEDataset(data, CONFIG, features, notes,)\n    data_loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        num_workers=CONFIG['workers'],\n        shuffle=shuffle,\n        pin_memory=True,\n        #worker_init_fn=worker_init_fn,\n        collate_fn=feedback_collate,\n    )\n    return data_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def criterion(pred, target):\n    return nn.BCEWithLogitsLoss(reduction='none')(pred, target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def glorot_uniform(parameter):\n    nn.init.xavier_uniform_(parameter.data, gain=1.0)\n    \nclass NBMEHead(nn.Module):\n    def __init__(self, input_dim, output_dim, loss, criterion):\n        super(NBMEHead, self).__init__()\n        self.loss = loss\n        self.criterion = criterion\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        self.classifier = nn.Linear(input_dim, output_dim)\n        glorot_uniform(self.classifier.weight)\n        \n    def forward(self, x, attention_mask, target=None):\n        # x is B x S x C\n        logits1 = self.classifier(self.dropout1(x))\n        logits2 = self.classifier(self.dropout2(x))\n        logits3 = self.classifier(self.dropout3(x))\n        logits4 = self.classifier(self.dropout4(x))\n        logits5 = self.classifier(self.dropout5(x))\n                              \n        logits = ((logits1 + logits2 + logits3 + logits4 + logits5) / 5)\n        logits = logits * attention_mask\n        \n        if self.loss:\n            loss1 = self.criterion(logits1, target)\n            loss2 = self.criterion(logits2, target)\n            loss3 = self.criterion(logits3, target)\n            loss4 = self.criterion(logits4, target)\n            loss5 = self.criterion(logits5, target)\n            loss = (loss1 + loss2 + loss3  + loss4 + loss5) / 5\n            \n            #print(loss.shape, attention_mask.shape)\n            loss = loss * attention_mask\n            loss = loss.sum(1) / (1e-6 + attention_mask.sum(1))\n            loss = loss.mean()\n        else:\n            loss = 0\n        return logits, loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_keys = ['token_type_logits',]\nloss_keys = ['loss', 'tp_count', 'all_count', ]\nnot_collate_keys, collate_keys","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keep_collate_keys = []\ntest_collate_keys =  ['input_ids', 'attention_mask', 'token_type_ids']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spaces = ' \\n\\r'\n\ndef post_process_spaces(pred, text):\n    text = text[:len(pred)]\n    pred = pred[:len(text)]\n    if text[0] in spaces:\n        pred[0] = 0\n    if text[-1] in spaces:\n        pred[-1] = 0\n\n    for i in range(1, len(text) - 1):\n        if text[i] in spaces:\n            if pred[i] and not pred[i - 1]:  # space before\n                pred[i] = 0\n\n            if pred[i] and not pred[i + 1]:  # space after\n                pred[i] = 0\n\n            if pred[i - 1] and pred[i + 1]:\n                pred[i] = 1\n \n    return pred\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred_to_chars(token_type_logits, len_token, max_token, offset_mapping, text, feature_num):\n    token_type_logits = token_type_logits[:len_token]\n    offset_mapping = offset_mapping[:len_token]\n    char_preds = np.ones(len(text)) * -1e10\n    for i, (start,end) in enumerate(offset_mapping):\n        char_preds[start:end] = token_type_logits[i]\n    return (char_preds, text)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred_to_chars(token_type_logits, len_token, max_token, offset_mapping, text, feature_num):\n    token_type_logits = token_type_logits[:len_token]\n    offset_mapping = offset_mapping[:len_token]\n    char_preds = np.ones(len(text)) * -1e10\n    for i, (start,end) in enumerate(offset_mapping):\n        if text[start:end] == 'of' and start > 0 and text[start-1:end] == 'yof':\n            if feature_num in feature_female:\n                char_preds[end-1:end] = 1\n            elif feature_num in feature_year:\n                char_preds[start:start+1] = token_type_logits[i-1]\n            else:\n                char_preds[start:end] = token_type_logits[i]\n        elif text[start:end] == 'om' and start > 0 and text[start-1:end] == 'yom':\n            if feature_num in feature_male:\n                char_preds[end-1:end] = 1\n            elif feature_num in feature_year:\n                char_preds[start:start+1] = token_type_logits[i-1]\n            else:\n                char_preds[start:end] = token_type_logits[i]\n        else:\n            char_preds[start:end] = token_type_logits[i]\n    return (char_preds, text)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def char_preds_to_string(char_preds, text, pred_thr):\n    char_preds = (char_preds > pred_thr) * 1\n    post_process_spaces(char_preds, text)\n    indices = np.where(char_preds == 1)[0]\n    indices_grouped = [\n        list(g) for _, g in itertools.groupby(\n            indices, key=lambda n, c=itertools.count(): n - next(c)\n        )\n    ]\n    spans = [f\"{min(r)} {max(r) + 1}\" for r in indices_grouped]\n    spans = ';'.join(spans)\n    return spans\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_epoch(loader, models, device):\n\n    for model in models:\n        model.eval()\n    char_preds = []\n    with torch.no_grad():\n        if CONFIG['verbose']:\n            bar = tqdm(range(len(loader)))\n        else:\n            bar = range(len(loader))\n        load_iter = iter(loader)\n\n        for i in bar:\n            batch = load_iter.next()\n            input_dict = {k:batch[k].to(device, non_blocking=True) for k in collate_keys}\n\n            batch_out_dict = {}\n            for key in prediction_keys :\n                batch_out_dict[key] = 0             \n            for model in models:\n                out_dict = model(input_dict)\n                for key in prediction_keys :\n                    batch_out_dict[key] = batch_out_dict[key] + out_dict[key].detach() / len(models)             \n            token_type_logits = (batch_out_dict['token_type_logits']).detach()\n            token_type_logits = token_type_logits.cpu().numpy()\n\n            char_preds.extend([\n                pred_to_chars(*p) for p in zip(token_type_logits, \n                                                batch['len_token'], \n                                                batch['max_token'], \n                                                batch['offset_mapping'],\n                                                batch['text'],\n                                                batch['feature_num'],\n                                               )\n            ])\n\n    return char_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model_checkpoint(dirname, fname, fold):\n    model = NBMEModel(CONFIG['model_name'], loss=False, pretrained=False).to(device)\n    checkpoint = torch.load('../input/%s/%s_%d.pt' % (dirname, fname, fold))\n    print(dirname, fname, fold, checkpoint['epoch'])\n    model.load_state_dict(checkpoint['model'])\n    model.eval()\n    return model\n\ndef get_char_preds(test, CONFIG, notes):\n    device = torch.device('cuda')\n    \n    models = [load_model_checkpoint(CONFIG['dirname'],CONFIG['fname'], fold) for fold in CONFIG[\"folds\"]]\n\n    test_data_loader = get_data_loader(test, shuffle=False, CONFIG=CONFIG, notes=notes)\n    char_preds = test_epoch(test_data_loader, models, device)   \n    del test_data_loader, models\n    gc.collect()\n    return char_preds\n\ndef get_preds(char_preds, texts, pred_thr):\n    preds = [char_preds_to_string(p, text, pred_thr) for p,text in zip(char_preds, texts)]\n    df = pd.DataFrame({'id':test['id'], 'location':preds,})\n    return df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_preds_all = []\nmodel_coeff_all = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NBME 254","metadata":{}},{"cell_type":"code","source":"class NBMEModel(nn.Module):\n    def __init__(self, model_name, loss=False, pretrained=True):\n        super(NBMEModel, self).__init__()\n        config = CONFIG['config']\n        self.config = config\n        if pretrained:\n            self.backbone = AutoModel.from_pretrained(model_name)\n        else:\n            self.backbone = AutoModel.from_config(config)\n        self.backbone.resize_token_embeddings(len(CONFIG['tokenizer']))\n        self.loss = loss\n        #self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout0 = nn.Dropout(p=CONFIG['dropout'])\n        self.rnn0 = nn.LSTM(CONFIG['config'].hidden_size,\n                           CONFIG['config'].hidden_size//2,\n                           num_layers=1,\n                           batch_first=True,\n                           bidirectional=True,\n                          )\n        self.token_type_head = NBMEHead(config.hidden_size, 1,\n                                           loss, criterion)\n        self.token_start_head = NBMEHead(config.hidden_size, 1,\n                                           loss, criterion)\n        self.token_end_head = NBMEHead(config.hidden_size, 1,\n                                           loss, criterion)\n        self.model_name = model_name\n        weight_data = torch.linspace(-5, 5, 1+self.config.num_hidden_layers)\n        weight_data = weight_data.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n        self.weights = nn.Parameter(weight_data, requires_grad=True)\n       \n    def forward(self, input_dict):     \n        input_ids = input_dict['input_ids']\n        attention_mask = input_dict['attention_mask']\n        if 'roberta' in self.model_name:\n            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n                                output_hidden_states=True)\n        else:\n            token_type_ids = input_dict['token_type_ids']\n            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n                                token_type_ids=token_type_ids, output_hidden_states=True)\n        x = torch.stack(out.hidden_states)\n        w = F.softmax(self.weights, 0)\n        w = self.dropout0(w)\n        x = (w * x).sum(0)\n        x = self.rnn0(x)[0]\n        #x = out.hidden_states[-1]\n        if self.loss:\n            token_type = input_dict['token_type']\n            token_start = input_dict['token_start']\n            token_end = input_dict['token_end']\n            target_mask = (attention_mask * token_type_ids).unsqueeze(-1)\n        else:\n            token_type = None\n            token_start = None\n            token_end = None\n            target_mask = 1\n        token_type_logits, loss_type = self.token_type_head(x, target_mask, token_type) \n        token_start_logits, loss_start = self.token_start_head(x, target_mask, token_start) \n        token_end_logits, loss_end = self.token_end_head(x, target_mask, token_end)\n        out_dict = {\n            'token_type_logits' : token_type_logits,\n            'token_start_logits' : token_start_logits,\n            'token_end_logits' : token_end_logits,\n        }\n        \n        if self.loss:\n            loss = loss_type + loss_start + loss_end\n            token_type_pred = ((token_type_logits >= 0) * target_mask).detach()\n            tp_count = (token_type_pred * token_type).sum().detach().item()\n            all_count = (token_type_pred.sum() + token_type.sum()).detach().item()\n            #print(tp_count, token_type_pred.sum().item(), token_type.sum().item())\n            out_dict.update({\n                'loss' : loss,\n                'tp_count' : tp_count,\n                'all_count' : all_count,\n            })\n            \n        return out_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_abbrev(text):\n    text = text.replace('FHx', 'FH ')\n    text = text.replace('FHX', 'FH ')\n    text = text.replace('PMHx', 'PMH ')\n    text = text.replace('PMHX', 'PMH ')\n    text = text.replace('SHx', 'SH ')\n    text = text.replace('SHX', 'SH ')\n    text = text.lower()\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\"fname\" : 'nbme_254',\n          'dirname' : 'nbme-254',\n          \"seed\": 2021,\n          \"epochs\": 10,\n          \"model_name\": \"../input/deberta-v3-large-squadv2\",\n          \"train_batch_size\": 8,\n          \"valid_batch_size\": 32,\n          \"max_length\": 512,\n          \"learning_rate\": 1e-4,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-8,\n          \"folds\": [0, 1, 2, 3],\n          \"n_accumulate\": 4,\n          \"num_classes\": 2,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          'opt_wd_non_norm_bias' : 0.01,\n          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n          'opt_beta1' : 0.9,\n          'opt_beta2' : 0.99,\n          'opt_eps' : 1e-5, # same as Adam in Fastai\n          'verbose': DEBUG,\n          'valid_check' : 8,\n          'workers':8,\n          'dropout' : 0.2,\n          'checkpointing':False,\n          }\n\nif CONFIG['fname'] in model_coeff:\n    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n\n    tokenizer = DebertaV2TokenizerFast.from_pretrained(CONFIG['model_name'])\n    CONFIG[\"tokenizer\"] = tokenizer\n\n    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n    tokenizer.add_tokens([lf_token])\n    \n    test_clean_notes = test_notes.copy()\n    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n    char_preds_all.append(char_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\"fname\" : 'nbme_254_full',\n          'dirname' : 'nbme-254-full',\n          \"seed\": 2021,\n          \"epochs\": 10,\n          \"model_name\": \"../input/deberta-v3-large-squadv2\",\n          \"train_batch_size\": 8,\n          \"valid_batch_size\": 32,\n          \"max_length\": 512,\n          \"learning_rate\": 1e-4,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-8,\n          \"folds\": [0, 1, 2, 3],\n          \"n_accumulate\": 4,\n          \"num_classes\": 2,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          'opt_wd_non_norm_bias' : 0.01,\n          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n          'opt_beta1' : 0.9,\n          'opt_beta2' : 0.99,\n          'opt_eps' : 1e-5, # same as Adam in Fastai\n          'verbose': DEBUG,\n          'valid_check' : 8,\n          'workers':8,\n          'dropout' : 0.2,\n          'checkpointing':False,\n          }\n\nif CONFIG['fname'] in model_coeff:\n    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n\n    tokenizer = DebertaV2TokenizerFast.from_pretrained(CONFIG['model_name'])\n    CONFIG[\"tokenizer\"] = tokenizer\n\n    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n    tokenizer.add_tokens([lf_token])\n    \n    test_clean_notes = test_notes.copy()\n    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n    char_preds_all.append(char_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NBME 182","metadata":{}},{"cell_type":"code","source":"class NBMEModel(nn.Module):\n    def __init__(self, model_name, loss=False, pretrained=True):\n        super(NBMEModel, self).__init__()\n        config = CONFIG['config']\n        self.config = config\n        if pretrained:\n            self.backbone = AutoModel.from_pretrained(model_name)\n        else:\n            self.backbone = AutoModel.from_config(config)\n        self.backbone.resize_token_embeddings(len(CONFIG['tokenizer']))\n        self.loss = loss\n        #self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout0 = nn.Dropout(p=CONFIG['dropout'])\n        self.rnn0 = nn.GRU(CONFIG['config'].hidden_size,\n                           CONFIG['config'].hidden_size//2,\n                           num_layers=1,\n                           batch_first=True,\n                           bidirectional=True,\n                          )\n        self.token_type_head = NBMEHead(config.hidden_size, 1,\n                                           loss, criterion)\n        self.token_start_head = NBMEHead(config.hidden_size, 1,\n                                           loss, criterion)\n        self.token_end_head = NBMEHead(config.hidden_size, 1,\n                                           loss, criterion)\n        self.model_name = model_name\n        weight_data = torch.linspace(-5, 5, 1+self.config.num_hidden_layers)\n        weight_data = weight_data.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n        self.weights = nn.Parameter(weight_data, requires_grad=True)\n       \n    def forward(self, input_dict):     \n        input_ids = input_dict['input_ids']\n        attention_mask = input_dict['attention_mask']\n        if 'roberta' in self.model_name:\n            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n                                output_hidden_states=True)\n        else:\n            token_type_ids = input_dict['token_type_ids']\n            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n                                token_type_ids=token_type_ids, output_hidden_states=True)\n        x = torch.stack(out.hidden_states)\n        w = F.softmax(self.weights, 0)\n        w = self.dropout0(w)\n        x = (w * x).sum(0)\n        x = self.rnn0(x)[0]\n        #x = out.hidden_states[-1]\n        if self.loss:\n            token_type = input_dict['token_type']\n            token_start = input_dict['token_start']\n            token_end = input_dict['token_end']\n            target_mask = (attention_mask * token_type_ids).unsqueeze(-1)\n        else:\n            token_type = None\n            token_start = None\n            token_end = None\n            target_mask = 1\n        token_type_logits, loss_type = self.token_type_head(x, target_mask, token_type) \n        token_start_logits, loss_start = self.token_start_head(x, target_mask, token_start) \n        token_end_logits, loss_end = self.token_end_head(x, target_mask, token_end)\n        out_dict = {\n            'token_type_logits' : token_type_logits,\n            'token_start_logits' : token_start_logits,\n            'token_end_logits' : token_end_logits,\n        }\n        \n        if self.loss:\n            loss = loss_type + loss_start + loss_end\n            token_type_pred = ((token_type_logits >= 0) * target_mask).detach()\n            tp_count = (token_type_pred * token_type).sum().detach().item()\n            all_count = (token_type_pred.sum() + token_type.sum()).detach().item()\n            #print(tp_count, token_type_pred.sum().item(), token_type.sum().item())\n            out_dict.update({\n                'loss' : loss,\n                'tp_count' : tp_count,\n                'all_count' : all_count,\n            })\n            \n        return out_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_abbrev(text):\n    text = text.replace('FHx', 'FH ')\n    text = text.replace('FHX', 'FH ')\n    text = text.replace('PMHx', 'PMH ')\n    text = text.replace('PMHX', 'PMH ')\n    text = text.replace('SHx', 'SH ')\n    text = text.replace('SHX', 'SH ')\n    text = text.lower()\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\"fname\" : 'nbme_182',\n          'dirname' : 'nbme-182',\n          \"seed\": 2021,\n          \"epochs\": 10,\n          \"model_name\": \"../input/deberta-v3-large-squadv2\",\n          \"train_batch_size\": 8,\n          \"valid_batch_size\": 32,\n          \"max_length\": 512,\n          \"learning_rate\": 1e-4,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-8,\n          \"folds\": [0, 1, 2, 3],\n          \"n_accumulate\": 4,\n          \"num_classes\": 2,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          'opt_wd_non_norm_bias' : 0.01,\n          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n          'opt_beta1' : 0.9,\n          'opt_beta2' : 0.99,\n          'opt_eps' : 1e-5, # same as Adam in Fastai\n          'verbose': DEBUG,\n          'valid_check' : 8,\n          'workers':8,\n          'dropout' : 0.2,\n          'checkpointing':False,\n          }\n\nif CONFIG['fname'] in model_coeff:\n    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n\n    tokenizer = DebertaV2TokenizerFast.from_pretrained(CONFIG['model_name'])\n    CONFIG[\"tokenizer\"] = tokenizer\n\n    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n    tokenizer.add_tokens([lf_token])\n    \n    test_clean_notes = test_notes.copy()\n    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n    char_preds_all.append(char_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\"fname\" : 'nbme_182_full',\n          'dirname' : 'nbme-182-full',\n          \"seed\": 2021,\n          \"epochs\": 10,\n          \"model_name\": \"../input/deberta-v3-large-squadv2\",\n          \"train_batch_size\": 8,\n          \"valid_batch_size\": 32,\n          \"max_length\": 512,\n          \"learning_rate\": 1e-4,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-8,\n          \"folds\": [0, 1, 2, 3],\n          \"n_accumulate\": 4,\n          \"num_classes\": 2,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          'opt_wd_non_norm_bias' : 0.01,\n          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n          'opt_beta1' : 0.9,\n          'opt_beta2' : 0.99,\n          'opt_eps' : 1e-5, # same as Adam in Fastai\n          'verbose': DEBUG,\n          'valid_check' : 8,\n          'workers':8,\n          'dropout' : 0.2,\n          'checkpointing':False,\n          }\n\nif CONFIG['fname'] in model_coeff:\n    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n\n    tokenizer = DebertaV2TokenizerFast.from_pretrained(CONFIG['model_name'])\n    CONFIG[\"tokenizer\"] = tokenizer\n\n    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n    tokenizer.add_tokens([lf_token])\n    \n    test_clean_notes = test_notes.copy()\n    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n    char_preds_all.append(char_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NBME 315","metadata":{}},{"cell_type":"code","source":"CONFIG = {\"fname\" : 'nbme_315',\n          'dirname' : 'nbme-315',\n          \"seed\": 2021,\n          \"epochs\": 10,\n          \"model_name\": \"../input/deberta-v3-large-squadv2\",\n          \"train_batch_size\": 8,\n          \"valid_batch_size\": 32,\n          \"max_length\": 512,\n          \"learning_rate\": 1e-4,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-8,\n          \"folds\": [0, 1, 2, 3],\n          \"n_accumulate\": 4,\n          \"num_classes\": 2,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          'opt_wd_non_norm_bias' : 0.01,\n          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n          'opt_beta1' : 0.9,\n          'opt_beta2' : 0.99,\n          'opt_eps' : 1e-5, # same as Adam in Fastai\n          'verbose': DEBUG,\n          'valid_check' : 8,\n          'workers':8,\n          'dropout' : 0.2,\n          'checkpointing':False,\n          }\n\nif CONFIG['fname'] in model_coeff:\n    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n\n    tokenizer = DebertaV2TokenizerFast.from_pretrained(CONFIG['model_name'])\n    CONFIG[\"tokenizer\"] = tokenizer\n\n    lf_token = AddedToken(\"\\n\", lstrip=True, rstrip=True)\n    tokenizer.add_tokens([lf_token])\n    tokenizer.lf_token_id = tokenizer.get_added_vocab()['\\n']\n    tokenizer.lf_token_id  \n    \n    test_clean_notes = test_notes.copy()\n    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n    char_preds_all.append(char_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NBME 256","metadata":{}},{"cell_type":"code","source":"class NBMEModel(nn.Module):\n    def __init__(self, model_name, loss=False, pretrained=True):\n        super(NBMEModel, self).__init__()\n        config = CONFIG['config']\n        self.config = config\n        if pretrained:\n            self.backbone = AutoModel.from_pretrained(model_name)\n        else:\n            self.backbone = AutoModel.from_config(config)\n        self.backbone.resize_token_embeddings(len(CONFIG['tokenizer']))\n        self.loss = loss\n        #self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout0 = nn.Dropout(p=CONFIG['dropout'])\n        self.rnn0 = nn.LSTM(CONFIG['config'].hidden_size,\n                           CONFIG['config'].hidden_size//2,\n                           num_layers=1,\n                           batch_first=True,\n                           bidirectional=True,\n                          )\n        self.token_type_head = NBMEHead(config.hidden_size, 1,\n                                           loss, criterion)\n        self.token_start_head = NBMEHead(config.hidden_size, 1,\n                                           loss, criterion)\n        self.token_end_head = NBMEHead(config.hidden_size, 1,\n                                           loss, criterion)\n        self.model_name = model_name\n        weight_data = torch.linspace(-5, 5, 1+self.config.num_hidden_layers)\n        weight_data = weight_data.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n        self.weights = nn.Parameter(weight_data, requires_grad=True)\n       \n    def forward(self, input_dict):     \n        input_ids = input_dict['input_ids']\n        attention_mask = input_dict['attention_mask']\n        if 'roberta' in self.model_name:\n            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n                                output_hidden_states=True)\n        else:\n            token_type_ids = input_dict['token_type_ids']\n            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n                                token_type_ids=token_type_ids, output_hidden_states=True)\n        x = torch.stack(out.hidden_states)\n        w = F.softmax(self.weights, 0)\n        w = self.dropout0(w)\n        x = (w * x).sum(0)\n        x = self.rnn0(x)[0]\n        #x = out.hidden_states[-1]\n        if self.loss:\n            token_type = input_dict['token_type']\n            token_start = input_dict['token_start']\n            token_end = input_dict['token_end']\n            target_mask = (attention_mask * token_type_ids).unsqueeze(-1)\n        else:\n            token_type = None\n            token_start = None\n            token_end = None\n            target_mask = 1\n        token_type_logits, loss_type = self.token_type_head(x, target_mask, token_type) \n        token_start_logits, loss_start = self.token_start_head(x, target_mask, token_start) \n        token_end_logits, loss_end = self.token_end_head(x, target_mask, token_end)\n        out_dict = {\n            'token_type_logits' : token_type_logits,\n            'token_start_logits' : token_start_logits,\n            'token_end_logits' : token_end_logits,\n        }\n        \n        if self.loss:\n            loss = loss_type + loss_start + loss_end\n            token_type_pred = ((token_type_logits >= 0) * target_mask).detach()\n            tp_count = (token_type_pred * token_type).sum().detach().item()\n            all_count = (token_type_pred.sum() + token_type.sum()).detach().item()\n            #print(tp_count, token_type_pred.sum().item(), token_type.sum().item())\n            out_dict.update({\n                'loss' : loss,\n                'tp_count' : tp_count,\n                'all_count' : all_count,\n            })\n            \n        return out_dict\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\"fname\" : 'nbme_256',\n          'dirname' : 'nbme-256',\n          \"seed\": 2021,\n          \"epochs\": 10,\n          \"model_name\": \"../input/deberta-v1-large-squadv2-ep2\",\n          \"train_batch_size\": 8,\n          \"valid_batch_size\": 32,\n          \"max_length\": 512,\n          \"learning_rate\": 1e-4,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-8,\n          \"folds\": [0, 1, 2, 3],\n          \"n_accumulate\": 4,\n          \"num_classes\": 2,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          'opt_wd_non_norm_bias' : 0.01,\n          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n          'opt_beta1' : 0.9,\n          'opt_beta2' : 0.99,\n          'opt_eps' : 1e-5, # same as Adam in Fastai\n          'verbose': DEBUG,\n          'valid_check' : 8,\n          'workers':8,\n          'dropout' : 0.2,\n          'checkpointing':False,\n          }\n\nif CONFIG['fname'] in model_coeff:\n    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n    \n    tokenizer = tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n    CONFIG[\"tokenizer\"] = tokenizer\n\n    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n    tokenizer.add_tokens([lf_token])\n    \n    test_clean_notes = test_notes.copy()\n    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n    char_preds_all.append(char_preds)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\"fname\" : 'nbme_256_full',\n          'dirname' : 'nbme-256-full',\n          \"seed\": 2021,\n          \"epochs\": 10,\n          \"model_name\": \"../input/deberta-v1-large-squadv2-ep2\",\n          \"train_batch_size\": 8,\n          \"valid_batch_size\": 32,\n          \"max_length\": 512,\n          \"learning_rate\": 1e-4,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-8,\n          \"folds\": [0, 1, 2, 3],\n          \"n_accumulate\": 4,\n          \"num_classes\": 2,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          'opt_wd_non_norm_bias' : 0.01,\n          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n          'opt_beta1' : 0.9,\n          'opt_beta2' : 0.99,\n          'opt_eps' : 1e-5, # same as Adam in Fastai\n          'verbose': DEBUG,\n          'valid_check' : 8,\n          'workers':8,\n          'dropout' : 0.2,\n          'checkpointing':False,\n          }\n\nif CONFIG['fname'] in model_coeff:\n    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n    \n    tokenizer = tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n    CONFIG[\"tokenizer\"] = tokenizer\n\n    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n    tokenizer.add_tokens([lf_token])\n    \n    test_clean_notes = test_notes.copy()\n    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n    char_preds_all.append(char_preds)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NBME 187/ 187_full","metadata":{}},{"cell_type":"code","source":"class NBMEModel(nn.Module):\n    def __init__(self, model_name, loss=False, pretrained=True):\n        super(NBMEModel, self).__init__()\n        config = CONFIG['config']\n        self.config = config\n        if pretrained:\n            self.backbone = AutoModel.from_pretrained(model_name)\n        else:\n            self.backbone = AutoModel.from_config(config)\n        self.backbone.resize_token_embeddings(len(CONFIG['tokenizer']))\n        self.loss = loss\n        #self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout0 = nn.Dropout(p=CONFIG['dropout'])\n        self.rnn0 = nn.GRU(CONFIG['config'].hidden_size,\n                           CONFIG['config'].hidden_size//2,\n                           num_layers=1,\n                           batch_first=True,\n                           bidirectional=True,\n                          )\n        self.token_type_head = NBMEHead(config.hidden_size, 1,\n                                           loss, criterion)\n        self.token_start_head = NBMEHead(config.hidden_size, 1,\n                                           loss, criterion)\n        self.token_end_head = NBMEHead(config.hidden_size, 1,\n                                           loss, criterion)\n        self.model_name = model_name\n        weight_data = torch.linspace(-5, 5, 1+self.config.num_hidden_layers)\n        weight_data = weight_data.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n        self.weights = nn.Parameter(weight_data, requires_grad=True)\n       \n    def forward(self, input_dict):     \n        input_ids = input_dict['input_ids']\n        attention_mask = input_dict['attention_mask']\n        if 'roberta' in self.model_name:\n            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n                                output_hidden_states=True)\n        else:\n            token_type_ids = input_dict['token_type_ids']\n            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n                                token_type_ids=token_type_ids, output_hidden_states=True)\n        x = torch.stack(out.hidden_states)\n        w = F.softmax(self.weights, 0)\n        w = self.dropout0(w)\n        x = (w * x).sum(0)\n        x = self.rnn0(x)[0]\n        #x = out.hidden_states[-1]\n        if self.loss:\n            token_type = input_dict['token_type']\n            token_start = input_dict['token_start']\n            token_end = input_dict['token_end']\n            target_mask = (attention_mask * token_type_ids).unsqueeze(-1)\n        else:\n            token_type = None\n            token_start = None\n            token_end = None\n            target_mask = 1\n        token_type_logits, loss_type = self.token_type_head(x, target_mask, token_type) \n        token_start_logits, loss_start = self.token_start_head(x, target_mask, token_start) \n        token_end_logits, loss_end = self.token_end_head(x, target_mask, token_end)\n        out_dict = {\n            'token_type_logits' : token_type_logits,\n            'token_start_logits' : token_start_logits,\n            'token_end_logits' : token_end_logits,\n        }\n        \n        if self.loss:\n            loss = loss_type + loss_start + loss_end\n            token_type_pred = ((token_type_logits >= 0) * target_mask).detach()\n            tp_count = (token_type_pred * token_type).sum().detach().item()\n            all_count = (token_type_pred.sum() + token_type.sum()).detach().item()\n            #print(tp_count, token_type_pred.sum().item(), token_type.sum().item())\n            out_dict.update({\n                'loss' : loss,\n                'tp_count' : tp_count,\n                'all_count' : all_count,\n            })\n            \n        return out_dict\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\"fname\" : 'nbme_187',\n          'dirname' : 'nbme-187',\n          \"seed\": 2021,\n          \"epochs\": 10,\n          \"model_name\": \"../input/deberta-v1-large-squadv2-ep2\",\n          \"train_batch_size\": 8,\n          \"valid_batch_size\": 32,\n          \"max_length\": 512,\n          \"learning_rate\": 1e-4,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-8,\n          \"folds\": [0, 1, 2, 3],\n          \"n_accumulate\": 4,\n          \"num_classes\": 2,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          'opt_wd_non_norm_bias' : 0.01,\n          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n          'opt_beta1' : 0.9,\n          'opt_beta2' : 0.99,\n          'opt_eps' : 1e-5, # same as Adam in Fastai\n          'verbose': DEBUG,\n          'valid_check' : 8,\n          'workers':8,\n          'dropout' : 0.2,\n          'checkpointing':False,\n          }\n\nif CONFIG['fname'] in model_coeff:\n    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n    \n    tokenizer = tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n    CONFIG[\"tokenizer\"] = tokenizer\n\n    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n    tokenizer.add_tokens([lf_token])\n    \n    test_clean_notes = test_notes.copy()\n    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n    char_preds_all.append(char_preds)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\"fname\" : 'nbme_187_full',\n          'dirname' : 'nbme-187-full',\n          \"seed\": 2021,\n          \"epochs\": 10,\n          \"model_name\": \"../input/deberta-v1-large-squadv2-ep2\",\n          \"train_batch_size\": 8,\n          \"valid_batch_size\": 32,\n          \"max_length\": 512,\n          \"learning_rate\": 1e-4,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-8,\n          \"folds\": [0, 1, 2, 3],\n          \"n_accumulate\": 4,\n          \"num_classes\": 2,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          'opt_wd_non_norm_bias' : 0.01,\n          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n          'opt_beta1' : 0.9,\n          'opt_beta2' : 0.99,\n          'opt_eps' : 1e-5, # same as Adam in Fastai\n          'verbose': DEBUG,\n          'valid_check' : 8,\n          'workers':8,\n          'dropout' : 0.2,\n          'checkpointing':False,\n          }\n\nif CONFIG['fname'] in model_coeff:\n    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n    \n    tokenizer = tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n    CONFIG[\"tokenizer\"] = tokenizer\n\n    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n    tokenizer.add_tokens([lf_token])\n    \n    test_clean_notes = test_notes.copy()\n    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n    char_preds_all.append(char_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_coeff_all","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_coeff_all = np.array(model_coeff_all) / np.sum(model_coeff_all)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_coeff_all","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.special import expit, logit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_all = [chars_preds[1] for chars_preds in char_preds_all[0]]\ntext_all[:2]\n\nchars_all = [[chars_preds[0] for chars_preds in char_preds_all_i] \\\n             for char_preds_all_i in char_preds_all]\n#chars_all[0][:2]\nlen(chars_all), len(chars_all[0])\n\n\nchars_all_final = [expit(p) * model_coeff_all[0] for p in chars_all[0]]\nfor chars_all_i, model_coeff_all_i in zip(chars_all[1:], model_coeff_all[1:]):\n    chars_all_final = [p + (expit(p1) * model_coeff_all_i) for p,p1 in zip(chars_all_final, chars_all_i)]\n\nsub = get_preds(chars_all_final, text_all, expit(pred_thr))\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}