{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## NBME Competition Notebook\n\n### Notebook Features\n- HuggingFace API, PyTorch\n- Sequence Classification\n- Binary Token Classification for character multi-span","metadata":{}},{"cell_type":"markdown","source":"## 0. Load Dependencies\nLoad `datasets` library offline. The method refer to:\n- https://www.kaggle.com/code/samuelepino/pip-downloading-packages-to-your-local-machine/notebook?scriptVersionId=29576961","metadata":{}},{"cell_type":"code","source":"!ls ../input/nbme-pre-trained-models/datasets","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:34:57.070464Z","iopub.execute_input":"2022-05-11T09:34:57.070749Z","iopub.status.idle":"2022-05-11T09:34:57.746532Z","shell.execute_reply.started":"2022-05-11T09:34:57.070699Z","shell.execute_reply":"2022-05-11T09:34:57.745718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets --no-index --find-links=file:///kaggle/input/nbme-pre-trained-models/datasets","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:34:57.751579Z","iopub.execute_input":"2022-05-11T09:34:57.75203Z","iopub.status.idle":"2022-05-11T09:35:07.441243Z","shell.execute_reply.started":"2022-05-11T09:34:57.75199Z","shell.execute_reply":"2022-05-11T09:35:07.440369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport numpy as np \nimport pandas as pd\nimport tensorflow as tf\n\nimport os\nimport torch\nfrom torch import nn\n\nfrom datasets import load_dataset, Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification\nfrom transformers import DataCollatorWithPadding, DataCollatorForTokenClassification\nfrom transformers import TrainingArguments, Trainer\n\npath = '../input/nbme-score-clinical-patient-notes'\n\ntrain = pd.read_csv(path + '/train.csv')\nfeatures = pd.read_csv(path + '/features.csv')\npns = pd.read_csv(path + '/patient_notes.csv')\n\ntest = pd.read_csv(path + '/test.csv')\nsubmission = pd.read_csv(path + '/sample_submission.csv')\n\nprint(len(train), train.columns)\nprint(len(features), features.columns)\nprint(len(pns), pns.columns)\n\nprint(len(test), test.columns)\nprint(len(submission), submission.columns)\n\nprint()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Current Device: \", device)\nif torch.cuda.is_available():\n    print(\"Number of CUDA device: \", torch.cuda.device_count())\n    print(\"Device name: \", torch.cuda.get_device_name(0))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:35:07.444695Z","iopub.execute_input":"2022-05-11T09:35:07.444915Z","iopub.status.idle":"2022-05-11T09:35:10.374551Z","shell.execute_reply.started":"2022-05-11T09:35:07.444886Z","shell.execute_reply":"2022-05-11T09:35:10.373765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_feature_text = features[\"feature_text\"].apply(lambda x: len(x.strip()))\nprint(f\"Character Length of feature text: MAX - {max(len_feature_text)}, MIN - {min(len_feature_text)}\")\n\nlen_pn_history = pns[\"pn_history\"].apply(lambda x: len(x))\nprint(f\"Character Length of patient note: MAX - {max(len_pn_history)}, MIN - {min(len_pn_history)}\")\n\nmax_len_annotation = train[\"annotation\"].apply(lambda x: max(list(map(len, x.split(';')))))\nmin_len_annotation = train[\"annotation\"].apply(lambda x: min(list(map(len, x.split(';')))))\nprint(f\"Character Length of annotation: MAX - {max(max_len_annotation)}, MIN - {min(min_len_annotation)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Prepare Datasets","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"../input/nbme-pre-trained-models/tokenizer\")\n\ndef tokenize(text):\n    \"\"\"Tokenize a sequence.\"\"\"\n    return tokenizer.tokenize(text, add_special_tokens=False)\n\nexample_text = pns.iloc[0].at['pn_history']\nprint(\"Example Tokens: \\n\", tokenize(example_text))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:35:14.016055Z","iopub.execute_input":"2022-05-11T09:35:14.01696Z","iopub.status.idle":"2022-05-11T09:35:14.051762Z","shell.execute_reply.started":"2022-05-11T09:35:14.01691Z","shell.execute_reply":"2022-05-11T09:35:14.051029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1-1. Prepare sequence training dataset.","metadata":{}},{"cell_type":"code","source":"seq_train = pns[['pn_history', 'case_num']]\nseq_train.rename(columns = {'pn_history':'sequence', 'case_num':'labels'}, inplace = True)\nseq_train['sequence'] = seq_train['sequence'].apply(lambda text: tokenize(text))\n\ntoken_len = seq_train['sequence'].apply(lambda x: len(x))\nprint(f'Token length: MAX - {max(token_len)}, MIN - {min(token_len)}')\n\nnum_labels = pns[\"case_num\"].nunique()\nprint(f\"Number of Cases: {num_labels}\")\n\nseq_train_shuffled = seq_train.sample(frac=1).reset_index(drop=True)\nseq_valid = seq_train_shuffled.loc[: len(seq_train) * 0.01]\nseq_train = seq_train_shuffled.loc[len(seq_train) * 0.01:]\n\nprint(f'Length of Train data: {len(seq_train)}')\nprint(f'Length of Validation data: {len(seq_valid)}')\n\ndisplay(seq_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:35:16.396526Z","iopub.execute_input":"2022-05-11T09:35:16.397256Z","iopub.status.idle":"2022-05-11T09:35:46.10449Z","shell.execute_reply.started":"2022-05-11T09:35:16.397215Z","shell.execute_reply":"2022-05-11T09:35:46.103761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1-2. Prepare token classification training dataset.\nFirst, define functions to encode from character indices to token labels and decode the other way round.","metadata":{}},{"cell_type":"code","source":"def idx2token_label(text, tokens, location):\n    \"\"\"Converts character indices('location') to token labels.\"\"\"\n    \n    token_len = len(tokens)\n    token_label = [0] * (token_len)\n    \n    pat = re.compile('\\d+ \\d+')\n    indices = pat.findall(location)\n    if not indices:\n        return token_label\n    indices = list(map(lambda s: s.split(), indices))\n    indices.append(['0', '0'])\n\n    s, e = list(map(int, indices.pop(0)))\n    char_idx, token_idx = 0, 0\n    text_len = len(text)\n    while char_idx < text_len and token_idx < token_len and indices:\n        if s <= char_idx < e:\n            while token_idx < token_len and s <= char_idx < e:\n#                 print(tokens[token_idx], char_idx)\n                token_label[token_idx] = 1\n                char_idx += len(re.sub('#', '', tokens[token_idx], flags=re.MULTILINE))\n                while char_idx < text_len and text[char_idx] in \" \\t\\n\\r\\f\\v\":\n                    char_idx += 1\n                token_idx += 1\n            s, e = list(map(int, indices.pop(0)))\n        else:\n            char_idx += len(re.sub('#', '', tokens[token_idx], flags=re.MULTILINE))\n            while char_idx < text_len and text[char_idx] in \" \\t\\n\\r\\f\\v\":\n                char_idx += 1\n            token_idx += 1\n    return token_label\n\n\ndef token_label2idx(text, tokens, token_label):\n    \"\"\"Converts token labels back to character indices.\"\"\"\n    \n    char_indices = []\n\n    token_len = len(tokens)\n    text_len = len(text)\n    char_idx, token_idx = 0, 0\n    \n    while char_idx < text_len and token_idx < token_len:\n        if token_label[token_idx] == 1:\n            s = char_idx\n            while token_idx < token_len and token_label[token_idx] == 1:\n                flag = False\n                char_idx += len(re.sub('#', '', tokens[token_idx], flags=re.MULTILINE))\n                while char_idx < text_len and text[char_idx] in \" \\t\\n\\r\\f\\v\":\n                    char_idx += 1\n                    flag = True\n                token_idx += 1\n            e = char_idx - 1 if flag else char_idx\n            char_indices.append(' '.join((str(s), str(e))))\n        else:\n            char_idx += len(re.sub('#', '', tokens[token_idx], flags=re.MULTILINE))\n            while char_idx < text_len and text[char_idx] in \" \\t\\n\\r\\f\\v\":\n                char_idx += 1\n            token_idx += 1\n    \n    return ';'.join(char_indices)\n\n\ntmp = train.merge(pns, on='pn_num')\nfor idx in range(50, 70):\n    print(tmp.iloc[idx].at['annotation'])\n    example_text = tmp.iloc[idx].at['pn_history']\n    example_idx = tmp.iloc[idx].at['location']\n    print(f'Example Location: {example_idx}')\n    ex_token_label = idx2token_label(example_text, tokenize(example_text), example_idx)\n    print(f'Example Token Label: {ex_token_label}')\n    print(f'Example Decoding: {token_label2idx(example_text, tokenize(example_text), ex_token_label)}\\n')\ndel tmp","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:35:46.106357Z","iopub.execute_input":"2022-05-11T09:35:46.10682Z","iopub.status.idle":"2022-05-11T09:35:46.200728Z","shell.execute_reply.started":"2022-05-11T09:35:46.106783Z","shell.execute_reply":"2022-05-11T09:35:46.199974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prepare train dataset in DataFrame form.","metadata":{}},{"cell_type":"code","source":"def get_tokens_and_labels(df):\n    \"\"\"Get input and output of token classification model.\"\"\"\n    df = df.merge(pns, on='pn_num')\n    df = df.merge(features, on='feature_num')\n    df.rename(columns = {'feature_text':'features', 'pn_history':'text'}, inplace = True)\n    df['tokens'] = df['features'].apply(lambda s: [s]) + df['text'].apply(lambda s: tokenize(s))\n    if 'location' in df:\n        inputs = df[['text', 'tokens', 'location']]\n        df['tags'] = inputs.apply(lambda sr: [0] + idx2token_label(sr[0], sr[1], sr[2]), axis=1)\n        return df[['id', 'features', 'text', 'tokens', 'tags', 'location']]\n    return df[['id', 'features', 'text', 'tokens']]\n\ntag_train = get_tokens_and_labels(train)\ntag_test = get_tokens_and_labels(test)\n\ntag_train_shuffled = tag_train.sample(frac=1).reset_index(drop=True)\ntag_valid = tag_train_shuffled.loc[: len(tag_train) * 0.03]\ntag_train = tag_train_shuffled.loc[len(tag_train) * 0.03:]\n\nprint(f'Length of Train data: {len(tag_train)}')\nprint(f'Length of Validation data: {len(tag_valid)}')\nprint(f'Length of Test data: {len(tag_test)}')\ndisplay(tag_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:35:46.202201Z","iopub.execute_input":"2022-05-11T09:35:46.20249Z","iopub.status.idle":"2022-05-11T09:35:58.567816Z","shell.execute_reply.started":"2022-05-11T09:35:46.202435Z","shell.execute_reply":"2022-05-11T09:35:58.56712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Task 1 - Sequence Classification","metadata":{}},{"cell_type":"code","source":"# Encode pandas DataFrame into HuggingFace Dataset object.\ntrain_seq_dataset = Dataset.from_pandas(seq_train)\nvalid_seq_dataset = Dataset.from_pandas(seq_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Observe an example from train data\nexample_input = train_seq_dataset['sequence'][0]\nexample_tokenized = tokenizer(example_input, is_split_into_words=True)\n\nprint(\"Example input: \\n\", example_input)\nprint(\"Example tokenized : \\n\", example_tokenized.input_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize Dataset for training\ndef prepare_class_features(examples):\n    tokenized_data = tokenizer(examples[\"sequence\"],\n                               truncation=True,\n                               is_split_into_words=True)\n    tokenized_data['labels'] = examples['labels']\n    return tokenized_data\n\ntokenized_seq_train = train_seq_dataset.map(prepare_class_features, \n                                            batched=True, \n                                            remove_columns=train_seq_dataset.column_names)\ntokenized_seq_valid = valid_seq_dataset.map(prepare_class_features, \n                                            batched=True, \n                                            remove_columns=valid_seq_dataset.column_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Pretrained Model to current device\nseq_model = AutoModelForSequenceClassification.from_pretrained(\"../input/nbme-pre-trained-models/seq_model\", \n                                                              num_labels=num_labels).to(device)\n# Print model structure\n# seq_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\nepochs = 5\n\nargs = TrainingArguments(\n    \"./train/nbme-case\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=epochs,\n    weight_decay=0.01,\n    report_to=\"none\",\n    fp16=True, # half precision\n)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer = Trainer(\n    seq_model,\n    args,\n    train_dataset=tokenized_seq_train,\n    eval_dataset=tokenized_seq_valid,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_summary(result):\n    print(f\"Total Time: {result.metrics['train_runtime']:.2f}s\")\n    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}s\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = trainer.train()\nprint_summary(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Task 2 - Token Tagging ","metadata":{}},{"cell_type":"code","source":"# Encode pandas DataFrame into HuggingFace Dataset object.\ntrain_tag_dataset = Dataset.from_pandas(tag_train)\nvalid_tag_dataset = Dataset.from_pandas(tag_valid)\ntest_dataset = Dataset.from_pandas(tag_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:36:01.761625Z","iopub.execute_input":"2022-05-11T09:36:01.761922Z","iopub.status.idle":"2022-05-11T09:36:02.204487Z","shell.execute_reply.started":"2022-05-11T09:36:01.761892Z","shell.execute_reply":"2022-05-11T09:36:02.203771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Observe an example from train dataset.\nexample_input = train_tag_dataset['tokens'][0]\nexample_tokenized = tokenizer(example_input, is_split_into_words=True)\n\nprint(\"Example text: \\n\", train_tag_dataset['text'][0])\nprint(\"\\nExample input: \\n\", example_input)\nprint(\"\\nExample tokenized : \\n\", example_tokenized.input_ids)\nprint(\"\\nExample label: \\n\", train_tag_dataset['tags'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare dataset for training.\n# For labels, convert it to a tokenized label form using word_idx of BatchEncode object.\ndef prepare_tag_features(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"], \n        truncation=True, \n        padding=\"max_length\",\n        is_split_into_words=True)\n\n    labels = []\n    for i, label in enumerate(examples[\"tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx: \n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\ntokenized_tag_train = train_tag_dataset.map(prepare_tag_features, \n                                            batched=True, \n                                            remove_columns=train_tag_dataset.column_names)\ntokenized_tag_valid = valid_tag_dataset.map(prepare_tag_features, \n                                            batched=True, \n                                            remove_columns=valid_tag_dataset.column_names)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:36:52.616847Z","iopub.execute_input":"2022-05-11T09:36:52.617543Z","iopub.status.idle":"2022-05-11T09:37:21.344526Z","shell.execute_reply.started":"2022-05-11T09:36:52.6175Z","shell.execute_reply":"2022-05-11T09:37:21.343771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load PreTrained Model Structure.\ntoken_model = AutoModelForTokenClassification.from_pretrained(\"../input/nbme-pre-trained-models/token_model\", \n                                                              num_labels=2).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:36:33.587577Z","iopub.execute_input":"2022-05-11T09:36:33.588057Z","iopub.status.idle":"2022-05-11T09:36:36.644448Z","shell.execute_reply.started":"2022-05-11T09:36:33.588018Z","shell.execute_reply":"2022-05-11T09:36:36.643657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load previous sequence model weight into token model.\nstate_dict = seq_model.state_dict()\nstate_dict.pop('classifier.weight')\nstate_dict.pop('classifier.bias')\n\ntoken_model.load_state_dict(state_dict, strict=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NbmeTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(\"labels\").to(device)\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\").to(device)\n        loss_fct = nn.CrossEntropyLoss().to(device)\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:37:57.452288Z","iopub.execute_input":"2022-05-11T09:37:57.452801Z","iopub.status.idle":"2022-05-11T09:37:57.458763Z","shell.execute_reply.started":"2022-05-11T09:37:57.452761Z","shell.execute_reply":"2022-05-11T09:37:57.458066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\nepochs = 7\n\nargs = TrainingArguments(\n    \"./nbme-tag\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=epochs,\n    weight_decay=0.01,\n    report_to=\"none\",\n    fp16=True # half precision\n)\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\ntrainer = NbmeTrainer(\n    token_model,\n    args,\n    train_dataset=tokenized_tag_train,\n    eval_dataset=tokenized_tag_valid,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:37:58.741842Z","iopub.execute_input":"2022-05-11T09:37:58.742134Z","iopub.status.idle":"2022-05-11T09:37:58.759565Z","shell.execute_reply.started":"2022-05-11T09:37:58.742099Z","shell.execute_reply":"2022-05-11T09:37:58.758896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = trainer.train()\nprint_summary(result)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:37:59.863613Z","iopub.execute_input":"2022-05-11T09:37:59.864112Z","iopub.status.idle":"2022-05-11T09:38:02.887724Z","shell.execute_reply.started":"2022-05-11T09:37:59.864057Z","shell.execute_reply":"2022-05-11T09:38:02.885151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Evaluate Samples","metadata":{}},{"cell_type":"code","source":"# Get prediction values from the last model.\neval_outputs = trainer.predict(tokenized_tag_valid)\neval_pred = eval_outputs.predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print index many examples from the evaluation data.\nindex = 50\n\nfor idx in range(index):\n    eval_text = valid_tag_dataset['text'][idx]\n    eval_tokens = valid_tag_dataset['tokens'][idx]\n    eval_labels = np.argmax(eval_pred[idx], axis=-1)\n\n    eval_indices = token_label2idx(eval_text, eval_tokens, eval_labels)\n    result = eval_indices if eval_indices else \"0 0\"\n    eval_true = valid_tag_dataset['location'][idx]\n\n    print(f\"Prediction: {result} / True value: {eval_true}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Predict on Test Dataset","metadata":{}},{"cell_type":"code","source":"# Prepare dataset for prediction.\ndef prepare_test_features(examples):\n    return tokenizer(\n        examples[\"tokens\"], \n        truncation=True, \n        padding=\"max_length\",\n        is_split_into_words=True)\n\ntokenized_test_dataset = test_dataset.map(prepare_test_features, \n                                          batched=True, \n                                          remove_columns=test_dataset.column_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_output = trainer.predict(tokenized_test_dataset)\ntest_pred = test_output.predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\n\nfor idx in range(len(test_pred)):\n    test_text = test_dataset['text'][idx]\n    test_tokens = test_dataset['tokens'][idx]\n    test_labels = np.argmax(test_pred[idx], axis=-1)\n    \n    test_indices = token_label2idx(test_text, test_tokens, test_labels)\n    predictions.append(test_indices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['location'] = predictions\nsubmission.to_csv('submission.csv', index=False)\ndisplay(submission)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}