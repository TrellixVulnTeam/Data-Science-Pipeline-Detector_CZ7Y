{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NBME competition notebook\n\nAbout this notebook :\n- This notebook performs token binary classification on labeled datasets.\n- Transfer learning weights are from another notebook (which performs sequence classification on all interview notes, https://www.kaggle.com/code/joanyeo/nbme-hf-distilbert-2train).\n- Utilize DistilBERT instead of BERT to speed up training.\n- Manage configurations with yaml files as used in Dockerfiles.\n- Automatically changes learning rate which can be monitored by Weights & Biases.\n- Performs 5 fold cross validation to adapt on general datasets.\n- Simplify training code with `train` and `run` functions.\n- Automatically save best models and load the best one from each fold in inference.\n- Epoch is adjusted to prevent overfitting : at first training, validation losses were generally higher than training losses.\n\n- This notebook got some great ideas from 2 notebooks that are:\n    - https://www.kaggle.com/code/tanyadayanand/nbme-bert-base-uncased-using-pytorch\n    - https://prgms.tistory.com/73\n    \n\n## Contents\n- 0. Getting Ready\n    - Import Lib\n    - ConfigManager\n    - Fix SEED\n- 1. Data & Model\n    - Load Data\n    - Dataset\n    - Model\n    - AverageMeter\n    - Loss\n- 2. Training\n- 3. Inference","metadata":{}},{"cell_type":"markdown","source":"## 0. Getting Ready\n### Import Lib","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport math\nimport time\nimport tqdm\nimport yaml\nimport torch\nimport random\nimport warnings\nimport tokenizers\nimport numpy as np\nimport pandas as pd\nimport transformers\n\nfrom tqdm.auto import tqdm\nfrom ast import literal_eval\nfrom easydict import EasyDict\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom sklearn.model_selection import KFold, train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:14:22.127859Z","iopub.execute_input":"2022-06-08T02:14:22.128438Z","iopub.status.idle":"2022-06-08T02:14:22.511664Z","shell.execute_reply.started":"2022-06-08T02:14:22.128393Z","shell.execute_reply":"2022-06-08T02:14:22.510648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:14:22.946476Z","iopub.execute_input":"2022-06-08T02:14:22.946983Z","iopub.status.idle":"2022-06-08T02:14:22.953081Z","shell.execute_reply.started":"2022-06-08T02:14:22.946948Z","shell.execute_reply":"2022-06-08T02:14:22.952029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set True to use Wandb\n# It is only avaiable with Internet connection.\nWANDB = False\n\nif WANDB:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value = user_secrets.get_secret(\"wandb\")\n    os.environ[\"WANDB_API_KEY\"] = secret_value\n\n    !pip -q install wandb\n    !wandb login \n\n    import wandb\n    wandb.init('nbme-study')\nelse:\n    os.environ['WANDB_CONSOLE'] = 'off'","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:31:37.994234Z","iopub.execute_input":"2022-06-08T02:31:37.994668Z","iopub.status.idle":"2022-06-08T02:31:57.100909Z","shell.execute_reply.started":"2022-06-08T02:31:37.994622Z","shell.execute_reply":"2022-06-08T02:31:57.100078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ConfigManager\n- Use .yaml file to manage configurations.\n- We can test on different configs.","metadata":{}},{"cell_type":"code","source":"class YamlConfigManager:\n    def __init__(self, config_file_path='../input/config/config.yaml', config_name='base'):\n        super().__init__()\n        self.values = EasyDict()\n        if config_file_path:\n            self.config_file_path = config_file_path\n            self.config_name = config_name\n            self.reload()\n            \n    def reload(self):\n        self.clear()\n        if self.config_file_path:\n            with open(self.config_file_path, 'r') as f:\n                self.values.update(yaml.safe_load(f)[self.config_name])\n                \n    def clear(self):\n        self.values.clear()\n        \n    def update(self, yaml_dict):\n        for k1, v1 in yaml_dict.items():\n            if isinstance(v1, dict):\n                for k2, v2 in v1.items():\n                    if isinstance(v2, dict):\n                        for k3, v3 in v2.items():\n                            self.values[k1][k2][k3] = v3\n                    else:\n                        self.values[k1][k2] = v2\n            else:\n                self.values[k1] = v1\n                \n    def export(self, save_file_path):\n        if save_file_path:\n            with open(save_file_path, 'w') as f:\n                yaml.dump(dict(self.values), f)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:14:26.950637Z","iopub.execute_input":"2022-06-08T02:14:26.951218Z","iopub.status.idle":"2022-06-08T02:14:26.962747Z","shell.execute_reply.started":"2022-06-08T02:14:26.95118Z","shell.execute_reply":"2022-06-08T02:14:26.961591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# config.yaml\n\n# base:\n#     seed: 1004\n#     model_arc: 'distilbert'\n#     num_classes: 2\n#     input_dir: '../input/nbme-score-clinical-patient-notes'\n#     output_dir: './results/'\n#     train_only: False\n#     max_len: 512\n#     ckp_path: '../input/nbme-hf-distilbert-2train/train/nbme-case/checkpoint-10432/'\n#     train_args:\n#         num_epochs: 7\n#         train_batch_size: 32\n#         val_batch_size: 32\n#         model_path: 'pytorch_model.bin'\n#         dropout_rate: 0.2 # 0.1~0.3\n#         max_grad_norm: 1.0\n#         max_lr: 0.0001\n#         min_lr: 0.00001\n#         cycle: 3\n#         gamma: 0.5\n#         weight_decay: 0.000001\n#         log_intervals: 10\n#         eval_metric: 'accuracy'\n#         n_splits: 5","metadata":{"execution":{"iopub.status.busy":"2022-05-26T10:01:19.894298Z","iopub.execute_input":"2022-05-26T10:01:19.895028Z","iopub.status.idle":"2022-05-26T10:01:19.903849Z","shell.execute_reply.started":"2022-05-26T10:01:19.894989Z","shell.execute_reply":"2022-05-26T10:01:19.903108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = YamlConfigManager()\nSEED = cfg.values.seed\nMODEL_ARC = cfg.values.model_arc\nINPUT_DIR = cfg.values.input_dir\nOUTPUT_DIR = cfg.values.output_dir\nTRAIN_ONLY = cfg.values.train_only\nMAX_LEN = cfg.values.max_len\nTOKENIZER = tokenizers.BertWordPieceTokenizer(f\"{cfg.values.ckp_path}/vocab.txt\", lowercase = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:14:28.842692Z","iopub.execute_input":"2022-06-08T02:14:28.843091Z","iopub.status.idle":"2022-06-08T02:14:28.903431Z","shell.execute_reply.started":"2022-06-08T02:14:28.843057Z","shell.execute_reply":"2022-06-08T02:14:28.902559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yaml_dict = dict(cfg.values)\nyaml_dict['train_args']['num_epochs'] = 4\ncfg.update(yaml_dict)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:14:32.459252Z","iopub.execute_input":"2022-06-08T02:14:32.459632Z","iopub.status.idle":"2022-06-08T02:14:32.464678Z","shell.execute_reply.started":"2022-06-08T02:14:32.459581Z","shell.execute_reply":"2022-06-08T02:14:32.463834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fix SEED","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed=1004):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:14:34.89811Z","iopub.execute_input":"2022-06-08T02:14:34.898988Z","iopub.status.idle":"2022-06-08T02:14:34.904303Z","shell.execute_reply.started":"2022-06-08T02:14:34.898949Z","shell.execute_reply":"2022-06-08T02:14:34.903581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:14:35.793393Z","iopub.execute_input":"2022-06-08T02:14:35.794013Z","iopub.status.idle":"2022-06-08T02:14:35.801514Z","shell.execute_reply.started":"2022-06-08T02:14:35.793973Z","shell.execute_reply":"2022-06-08T02:14:35.800495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data & Model\n### Load Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\nfeature_df = pd.read_csv(os.path.join(INPUT_DIR, 'features.csv'))\npn_df = pd.read_csv(os.path.join(INPUT_DIR, 'patient_notes.csv'))\ntest_df = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\nsubmission_df = pd.read_csv(os.path.join(INPUT_DIR, 'sample_submission.csv'))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:14:37.958347Z","iopub.execute_input":"2022-06-08T02:14:37.959087Z","iopub.status.idle":"2022-06-08T02:14:38.658035Z","shell.execute_reply.started":"2022-06-08T02:14:37.95905Z","shell.execute_reply":"2022-06-08T02:14:38.657076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.merge(train_df, feature_df, on=['feature_num','case_num'], how='inner')\ndf = pd.merge(df, pn_df, on=['pn_num','case_num'], how='inner')\ndf.sample(5, random_state=SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:14:40.595385Z","iopub.execute_input":"2022-06-08T02:14:40.596491Z","iopub.status.idle":"2022-06-08T02:14:40.656392Z","shell.execute_reply.started":"2022-06-08T02:14:40.596433Z","shell.execute_reply":"2022-06-08T02:14:40.655541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['feature_text'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:14:42.480024Z","iopub.execute_input":"2022-06-08T02:14:42.480404Z","iopub.status.idle":"2022-06-08T02:14:42.493248Z","shell.execute_reply.started":"2022-06-08T02:14:42.480371Z","shell.execute_reply":"2022-06-08T02:14:42.491997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"annotation\"] = [literal_eval(x) for x in df[\"annotation\"]]\ndf[\"location\"] = [literal_eval(x) for x in df[\"location\"]]\ndf.sample(5, random_state=SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:14:44.027121Z","iopub.execute_input":"2022-06-08T02:14:44.02749Z","iopub.status.idle":"2022-06-08T02:14:44.272265Z","shell.execute_reply.started":"2022-06-08T02:14:44.02746Z","shell.execute_reply":"2022-06-08T02:14:44.271436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset\n","metadata":{}},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:14:49.255062Z","iopub.execute_input":"2022-06-08T02:14:49.255536Z","iopub.status.idle":"2022-06-08T02:14:49.264707Z","shell.execute_reply.started":"2022-06-08T02:14:49.255493Z","shell.execute_reply":"2022-06-08T02:14:49.263813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(pn_history, feature_text, annotation, location):\n    \n    location_list = loc_list_to_ints(location)        \n    char_targets = [0] * len(pn_history) \n    \n    for loc, anno in zip(location_list, annotation): \n        len_st = loc[1] - loc[0]\n        idx0 = None\n        idx1 = None\n        for ind in (i for i, e in enumerate(pn_history) if (e == anno[0] and i == loc[0])):\n            if pn_history[ind: ind + len_st] == anno:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                if idx0 != None and idx1 != None:\n                    for ct in range(idx0, idx1 + 1):\n                        char_targets[ct] = 1  \n                break\n      \n    tokenized_input = TOKENIZER.encode(feature_text, pn_history)\n    \n    input_ids = tokenized_input.ids\n    mask = tokenized_input.attention_mask\n    token_type_ids = tokenized_input.type_ids\n    offsets = tokenized_input.offsets\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n            \n    #padding\n    padding_length = MAX_LEN - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        offsets = offsets + ([(0, 0)] * padding_length)\n       \n    #creating label\n    ignore_idxes = np.where(np.array(token_type_ids) != 1)[0]\n\n    label = np.zeros(len(offsets))\n    label[ignore_idxes] = -1\n    label[target_idx] = 1\n\n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'labels': label,\n        'offsets': offsets\n    }","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:20:28.640069Z","iopub.execute_input":"2022-06-08T02:20:28.640847Z","iopub.status.idle":"2022-06-08T02:20:28.655046Z","shell.execute_reply.started":"2022-06-08T02:20:28.640811Z","shell.execute_reply":"2022-06-08T02:20:28.653888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NBMEDataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.df = df.reset_index()\n        self.pn_history = df.pn_history.values\n        self.feature_text = df.feature_text.values\n        self.annotation = df.annotation.values\n        self.location = df.location.values\n        \n    def __len__(self):\n        return len(self.pn_history)\n    \n    def __getitem__(self, item):\n        data = preprocess(\n            self.pn_history[item],\n            self.feature_text[item],\n            self.annotation[item],\n            self.location[item]\n        )\n        \n        return {\n            'ids': torch.tensor(data['ids'], dtype=torch.long),\n            'mask': torch.tensor(data['mask'], dtype=torch.long),\n            'token_type_ids': torch.tensor(data['token_type_ids'], dtype=torch.long),\n            'labels': torch.tensor(data['labels'], dtype=torch.long),\n            'offsets': torch.tensor(data['offsets'], dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:20:31.836825Z","iopub.execute_input":"2022-06-08T02:20:31.837303Z","iopub.status.idle":"2022-06-08T02:20:31.85392Z","shell.execute_reply.started":"2022-06-08T02:20:31.837263Z","shell.execute_reply":"2022-06-08T02:20:31.853043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataloader(df, batch_size, shuffle):\n    dataset = NBMEDataset(df=df)\n    \n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        num_workers=4\n    )\n    \n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:20:34.554942Z","iopub.execute_input":"2022-06-08T02:20:34.555312Z","iopub.status.idle":"2022-06-08T02:20:34.561953Z","shell.execute_reply.started":"2022-06-08T02:20:34.555281Z","shell.execute_reply":"2022-06-08T02:20:34.560248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model\n","metadata":{}},{"cell_type":"code","source":"class NBMEModel(transformers.DistilBertModel):\n    def __init__(self, conf):\n        super(NBMEModel, self).__init__(conf)\n        self.pretrained_model = transformers.DistilBertModel.from_pretrained(cfg.values.ckp_path, config=conf)\n        self.dropout = torch.nn.Dropout(cfg.values.train_args.dropout_rate)\n        self.classifier = torch.nn.Linear(768, 1)\n        torch.nn.init.normal_(self.classifier.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        sequence_output = self.pretrained_model(\n            input_ids=ids, \n            attention_mask=mask,\n            # DistilBert does not take in token_type_ids\n        )[0]\n        \n        sequence_output = self.dropout(sequence_output)\n        \n        logits = self.classifier(sequence_output)\n        logits = logits.squeeze(-1)\n        \n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:20:36.342234Z","iopub.execute_input":"2022-06-08T02:20:36.342599Z","iopub.status.idle":"2022-06-08T02:20:36.561713Z","shell.execute_reply.started":"2022-06-08T02:20:36.342567Z","shell.execute_reply":"2022-06-08T02:20:36.5609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AverageMeter","metadata":{}},{"cell_type":"code","source":"class AverageMeter():\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:20:46.653336Z","iopub.execute_input":"2022-06-08T02:20:46.653757Z","iopub.status.idle":"2022-06-08T02:20:46.660556Z","shell.execute_reply.started":"2022-06-08T02:20:46.653723Z","shell.execute_reply":"2022-06-08T02:20:46.659678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss\n- Use Binary Cross Entoropy loss","metadata":{}},{"cell_type":"code","source":"class ComputeMetric(object):\n    def __init__(self, metric='bce') -> None:\n        super().__init__()\n        self.metric = metric\n\n    def compute_loss(self, logits, labels):\n        if self.metric == 'bce':\n            loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n            loss = loss_fct(logits, labels)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:20:48.173175Z","iopub.execute_input":"2022-06-08T02:20:48.173789Z","iopub.status.idle":"2022-06-08T02:20:48.180024Z","shell.execute_reply.started":"2022-06-08T02:20:48.173751Z","shell.execute_reply":"2022-06-08T02:20:48.1789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Training\n\n- Do 5 Fold cross validation to be ready for general dataset.\n- Train each model with different learning rate.\n- Use CosineAnnealingWarmRestart Scheduler for learning rate annealing.\n- When using Wandb, we log epoch, learning rate, loss and logits.","metadata":{}},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:22:47.239812Z","iopub.execute_input":"2022-06-08T02:22:47.240225Z","iopub.status.idle":"2022-06-08T02:22:47.311001Z","shell.execute_reply.started":"2022-06-08T02:22:47.240191Z","shell.execute_reply":"2022-06-08T02:22:47.310002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(cfg, fold, train_loader, valid_loader):\n        \n    # Set train arguments\n    num_epochs = cfg.values.train_args.num_epochs\n    log_intervals = cfg.values.train_args.log_intervals\n    max_lr = cfg.values.train_args.max_lr\n    min_lr = cfg.values.train_args.min_lr\n    cycle = cfg.values.train_args.cycle\n    gamma = cfg.values.train_args.gamma\n    weight_decay = cfg.values.train_args.weight_decay\n    ckp_path = cfg.values.ckp_path\n    max_grad_norm = cfg.values.train_args.max_grad_norm\n    train_batch_size = cfg.values.train_args.train_batch_size\n    val_batch_size = cfg.values.train_args.val_batch_size\n    \n    # Load model\n    model_config = transformers.DistilBertConfig.from_pretrained(os.path.join(ckp_path, 'config.json'))\n    model_config.output_hidden_states = True\n    model = NBMEModel(conf=model_config)\n    model.to(DEVICE)\n    \n    num_train_steps = int(len(train_loader) / train_batch_size * num_epochs)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    \n    # Set optimizer and scheduler\n    optimizer = AdamW(model.parameters(), \n                      lr=max_lr, \n                      weight_decay=weight_decay)\n    first_cycle_steps = len(train_loader) * num_epochs // cycle\n    scheduler = CosineAnnealingWarmRestarts(\n        optimizer,\n        T_0=first_cycle_steps,\n        eta_min=min_lr,\n    )\n    \n    eval_metric = ComputeMetric(metric='bce')\n    best_loss = np.inf\n    \n    os.makedirs(os.path.join(OUTPUT_DIR, MODEL_ARC), exist_ok=True)\n    \n    # Train num_epochs times\n    for epoch in range(num_epochs):\n \n        model.train()\n        since = time.time()\n        loss_values = AverageMeter()\n        \n        for idx, train_batch in enumerate(tqdm(train_loader, desc=f'Train')):\n            \n            ids = train_batch['ids'].to(DEVICE, dtype=torch.long)\n            mask = train_batch['mask'].to(DEVICE, dtype=torch.long)\n            token_type_ids = train_batch['token_type_ids'].to(DEVICE, dtype=torch.long)\n            offsets = train_batch['offsets'].to(DEVICE, dtype=torch.long)\n            labels = train_batch['labels'].to(DEVICE, dtype=torch.float64)\n            \n            model.zero_grad()\n            logits = model(ids=ids, \n                           mask=mask,\n                           token_type_ids=token_type_ids) #last_hidden_state\n            \n            # measure evaluation metric and record loss\n            loss = eval_metric.compute_loss(logits, labels)\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            loss_values.update(loss.item(), ids.size(0))\n            loss.requires_grad_(True)\n            loss.backward()\n\n            # compute gradient and do optimizer step\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            optimizer.step()\n            scheduler.step()\n\n            if WANDB:\n                wandb.log({\n                    \"epoch\": epoch,\n                    \"lr\": scheduler.get_lr()[0],\n                    \"loss\": loss, \n                    \"logits\": wandb.Histogram(logits.cpu().detach().numpy()),\n                })\n            \n            if idx % log_intervals == 0:\n                current_lr = scheduler.get_lr()[0]\n                time_elapsed = time.time() - since\n                tqdm.write(f\"Epoch : [{epoch + 1} / {num_epochs}][{idx}/{len(train_loader)}] || \"\n                           f\"LR : {current_lr:.5f} || \"\n                           f\"Train Loss : {loss_values.val:.4f} ({loss_values.avg:.4f}) || \"\n                           f\"Training completed in {time_elapsed % 60:.0f}s\"\n                          )\n    \n        if not TRAIN_ONLY:\n            \n            since = time.time()\n            \n            with torch.no_grad():\n                model.eval()\n                loss_values = AverageMeter()\n\n                for idx, val_batch in enumerate(tqdm(valid_loader, desc=f\"Validation\")):\n\n                    ids = val_batch['ids'].to(DEVICE, dtype=torch.long)\n                    mask = val_batch['mask'].to(DEVICE, dtype=torch.long)\n                    token_type_ids = val_batch['token_type_ids'].to(DEVICE, dtype=torch.long)\n                    offsets = val_batch['offsets'].to(DEVICE, dtype=torch.long)\n                    labels = val_batch['labels'].to(DEVICE, dtype=torch.float64)\n\n                    model.zero_grad()\n                    logits = model(ids=ids, \n                                   mask=mask, \n                                   token_type_ids=token_type_ids) #last_hidden_state\n\n                    # measure evaluation metric and record loss\n                    loss = eval_metric.compute_loss(logits, labels)\n                    loss = torch.masked_select(loss, labels > -1.0).mean()\n                    loss_values.update(loss.item(), ids.size(0))\n\n            time_elapsed = time.time() - since\n            tqdm.write(f\"Epoch : [{epoch + 1} / {num_epochs}] || \"\n                       f\"Val Loss : {loss_values.avg:.4f} || \"\n                       f\"Validation completed in {time_elapsed % 60:.0f}s\"\n                      )\n\n            is_best = loss_values.avg < best_loss\n            best_loss = min(loss_values.avg, best_loss)\n\n            if is_best:\n                os.makedirs(os.path.join(OUTPUT_DIR, MODEL_ARC, f\"{fold+1}_fold\"), exist_ok=True)\n                torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, MODEL_ARC, f\"{fold+1}_fold\", f\"{epoch+1}_epoch_{best_loss:.4f}%_with_val.pth\"))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T03:52:31.073164Z","iopub.execute_input":"2022-06-08T03:52:31.073618Z","iopub.status.idle":"2022-06-08T03:52:31.135273Z","shell.execute_reply.started":"2022-06-08T03:52:31.073558Z","shell.execute_reply":"2022-06-08T03:52:31.134446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(cfg, df): \n    since = time.time()\n\n    # Set train arguments\n    n_splits = cfg.values.train_args.n_splits\n    train_batch_size = cfg.values.train_args.train_batch_size\n    val_batch_size = cfg.values.train_args.val_batch_size\n    \n    # Train on K-fold\n    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    for fold, (train_idx, val_idx) in enumerate(kfold.split(df)):\n        print('\\n')\n        print('*' * 15 + f\" {fold + 1}-Fold Cross Validation \" + '*' * 15)\n\n        train_df = df.iloc[train_idx]\n        val_df = df.iloc[val_idx]\n\n        train_loader = get_dataloader(\n            df=train_df, \n            batch_size=train_batch_size, \n            shuffle=True\n        )\n        \n        val_loader = get_dataloader(\n            df=val_df,\n            batch_size=val_batch_size,\n            shuffle=False\n        )\n    \n        train(cfg, fold, train_loader, val_loader)\n        \n    time_elapsed = time.time() - since\n    print('*' * 50)\n    print(f\"Total Time {time_elapsed // 3600}h {(time_elapsed // 60) % 60}m {time_elapsed % 60:.0f}s Elapsed.\")","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:22:51.330445Z","iopub.execute_input":"2022-06-08T02:22:51.331327Z","iopub.status.idle":"2022-06-08T02:22:51.339418Z","shell.execute_reply.started":"2022-06-08T02:22:51.331287Z","shell.execute_reply":"2022-06-08T02:22:51.338284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\nrun(cfg, df)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T03:52:31.164245Z","iopub.execute_input":"2022-06-08T03:52:31.164605Z","iopub.status.idle":"2022-06-08T03:52:36.341647Z","shell.execute_reply.started":"2022-06-08T03:52:31.164575Z","shell.execute_reply":"2022-06-08T03:52:36.338094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Inference\n- Make prediction, convert data types and make submission.csv file.","metadata":{}},{"cell_type":"code","source":"df_tst = pd.merge(test_df, feature_df, on=['feature_num','case_num'], how='inner')\ndf_tst = pd.merge(df_tst, pn_df, on=['pn_num','case_num'], how='inner')\ndf_tst.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:29:09.072274Z","iopub.execute_input":"2022-06-08T02:29:09.07272Z","iopub.status.idle":"2022-06-08T02:29:09.111101Z","shell.execute_reply.started":"2022-06-08T02:29:09.072673Z","shell.execute_reply":"2022-06-08T02:29:09.110089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_preprocess(pn_history, feature_text):\n      \n    tokenized_input = TOKENIZER.encode(feature_text, pn_history)\n    \n    input_ids = tokenized_input.ids\n    mask = tokenized_input.attention_mask\n    token_type_ids = tokenized_input.type_ids\n    offsets = tokenized_input.offsets\n            \n    #padding\n    padding_length = MAX_LEN - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        offsets = offsets + ([(0, 0)] * padding_length)\n\n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'offsets': offsets\n    }","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:29:19.394887Z","iopub.execute_input":"2022-06-08T02:29:19.39526Z","iopub.status.idle":"2022-06-08T02:29:19.402046Z","shell.execute_reply.started":"2022-06-08T02:29:19.39523Z","shell.execute_reply":"2022-06-08T02:29:19.401085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.df = df.reset_index()\n        self.pn_history = df.pn_history\n        self.feature_text = df.feature_text\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        data = test_preprocess(\n            self.pn_history[idx],\n            self.feature_text[idx],\n        )\n        \n        return {\n            'ids': torch.tensor(data['ids'], dtype=torch.long),\n            'mask': torch.tensor(data['mask'], dtype=torch.long),\n            'token_type_ids': torch.tensor(data['token_type_ids'], dtype=torch.long),\n            'offsets': torch.tensor(data['offsets'], dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:29:25.948859Z","iopub.execute_input":"2022-06-08T02:29:25.94933Z","iopub.status.idle":"2022-06-08T02:29:25.961161Z","shell.execute_reply.started":"2022-06-08T02:29:25.949284Z","shell.execute_reply":"2022-06-08T02:29:25.960238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the last checkpoint, which scored best in validation, from each folds\nmodel_ckps = []\nn_splits = cfg.values.train_args.n_splits\n\nfor fold in range(n_splits):\n    path = os.listdir(os.path.join(OUTPUT_DIR, MODEL_ARC, f\"{fold+1}_fold\"))[-1]\n    model_ckps.append(os.path.join(OUTPUT_DIR, MODEL_ARC,  f\"{fold+1}_fold\", path))\n    \nmodel_config = transformers.DistilBertConfig.from_pretrained(cfg.values.ckp_path)\nmodel_config.output_hidden_states = True\nmodel = NBMEModel(conf=model_config)\n\n\n# Prepare Test DataLoader\ntest_dataset = TestDataset(df_tst)\n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=cfg.values.train_args.train_batch_size,\n    num_workers=1\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on Test DataLoader\navg_logits_list = []\nresults_ = []\n\nwith torch.no_grad():\n    tk = tqdm(test_dataloader, total=len(test_dataloader)) \n    \n    test_logits = []\n    for idx, test_batch in enumerate(tk):\n        ids = test_batch['ids'].to(DEVICE, dtype=torch.long)\n        mask = test_batch[\"mask\"].to(DEVICE, dtype=torch.long)\n        token_type_ids = test_batch[\"token_type_ids\"].to(DEVICE, dtype=torch.long)\n        \n        for model_ckp in model_ckps:\n            model.load_state_dict(torch.load(model_ckp))\n            model.to(DEVICE)\n            model.eval()\n            \n            logits = model(ids=ids, \n                           mask=mask, \n                           token_type_ids=token_type_ids\n                           ) #last_hidden_state\n            \n            test_logits.append(logits.cpu().detach().numpy())\n        \n        avg_logits = np.mean(test_logits, axis=0)\n        results_.append(avg_logits)\n\n    results_ = np.concatenate(results_)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T02:29:37.44622Z","iopub.execute_input":"2022-06-08T02:29:37.446643Z","iopub.status.idle":"2022-06-08T02:29:39.869101Z","shell.execute_reply.started":"2022-06-08T02:29:37.446584Z","shell.execute_reply":"2022-06-08T02:29:39.867496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def token_label2idx(text, tokens, token_label):\n    \"\"\"Converts token labels back to character indices.\"\"\"\n    \n    char_indices = []\n\n    token_len = len(tokens)\n    text_len = len(text)\n    char_idx, token_idx = 0, 0\n    \n    while char_idx < text_len and token_idx < token_len:\n        if token_label[token_idx] == 1:\n            s = char_idx\n            while token_idx < token_len and token_label[token_idx] == 1:\n                flag = False\n                char_idx += len(re.sub('#', '', tokens[token_idx], flags=re.MULTILINE))\n                while char_idx < text_len and text[char_idx] in \" \\t\\n\\r\\f\\v\":\n                    char_idx += 1\n                    flag = True\n                token_idx += 1\n            e = char_idx - 1 if flag else char_idx\n            char_indices.append(' '.join((str(s), str(e))))\n        else:\n            char_idx += len(re.sub('#', '', tokens[token_idx], flags=re.MULTILINE))\n            while char_idx < text_len and text[char_idx] in \" \\t\\n\\r\\f\\v\":\n                char_idx += 1\n            token_idx += 1\n    \n    return ';'.join(char_indices)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make prediction\nfor idx, ret in enumerate(results_):\n    results_[idx] = list(map(lambda x: 1 if x > 0 else 0, ret))\n    \nassert len(df_tst) == len(results_), \"Prediction length does not match input size.\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert results to character indices.\nchar_target = []\nfor text, label in zip(df_tst.pn_history, results_):\n    token = TOKENIZER.encode(text).tokens\n    char_target.append(token_label2idx(text, token, label))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['location'] = char_target\nsubmission_df.to_csv(f'submission.csv', index=False)\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2022-05-26T10:07:38.281649Z","iopub.status.idle":"2022-05-26T10:07:38.282578Z","shell.execute_reply.started":"2022-05-26T10:07:38.282308Z","shell.execute_reply":"2022-05-26T10:07:38.282335Z"},"trusted":true},"execution_count":null,"outputs":[]}]}