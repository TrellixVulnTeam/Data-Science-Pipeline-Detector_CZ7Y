{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NBME Augmentations 🎨\n\nThis notebook is an attempt to use nlpaug to augment pn_history to produce more data for NBME model training . This is based on augmenting good labelled data for now .","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install nlpaug -q","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:24:01.726968Z","iopub.execute_input":"2022-04-22T21:24:01.727278Z","iopub.status.idle":"2022-04-22T21:24:09.735643Z","shell.execute_reply.started":"2022-04-22T21:24:01.727246Z","shell.execute_reply":"2022-04-22T21:24:09.734733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)\n#pd.set_option('max_colwidth', -1)\npd.set_option('expand_frame_repr', True)\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:24:09.73768Z","iopub.execute_input":"2022-04-22T21:24:09.737966Z","iopub.status.idle":"2022-04-22T21:24:09.746606Z","shell.execute_reply.started":"2022-04-22T21:24:09.737935Z","shell.execute_reply":"2022-04-22T21:24:09.745541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nimport numpy as np\n\ndef plot_annotation(df, pn_num,plot_pred = False):\n    options = {\"colors\": {}}\n\n    df_text = df[df[\"pn_num\"] == pn_num].reset_index(drop=True)\n\n    text = df_text[\"pn_history\"][0]\n    ents = []\n\n    for spans, location , feature_text, feature_num in df_text[[\"span\",\"location\", \"feature_text\", \"feature_num\"]].values:\n        span=  location[0].split(\";\")\n        if len(span) >0:\n            for sp in span:\n                s = sp.split(\" \")\n                if len(s) >1:\n                    ents.append({\"start\": int(s[0]), \"end\": int(s[1]), \"label\": feature_text})\n\n        options[\"colors\"][feature_text] =  f\"rgb{tuple(np.random.randint(100, 255, size=3))}\"\n\n    doc = {\"text\": text, \"ents\": sorted(ents, key=lambda i: i[\"start\"])}\n\n    spacy.displacy.render(doc, style=\"ent\", options=options, manual=True, jupyter=True)\n    \n    \ndef plot_annotation_aug(df, pn_num,text_var = \"pn_history\",location_var = \"location\",plot_aug = False):\n    if len(df) <1:\n        return\n        \n    options = {\"colors\": {}}\n    df_text = df[df[\"pn_num\"] == pn_num].reset_index(drop=True)\n    text = df_text[text_var][0] \n    print(text)\n    ents = []\n\n    for location , feature_text, feature_num in df_text[[ location_var, \"feature_text\", \"feature_num\"]].values:\n        span=  location\n        if len(span) >0:\n            for l in span:\n                for st in [ st for st in l.split(\";\") ]:\n                    s = st.split()\n                    if len(s) >1:\n                        ents.append({\"start\": int(s[0]), \"end\": int(s[1]), \"label\": feature_text})\n\n        options[\"colors\"][feature_text] =  f\"rgb{tuple(np.random.randint(100, 255, size=3))}\"\n\n    doc = {\"text\": text, \"ents\": sorted(ents, key=lambda i: i[\"start\"])}\n\n    spacy.displacy.render(doc, style=\"ent\", options=options, manual=True, jupyter=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:24:09.748495Z","iopub.execute_input":"2022-04-22T21:24:09.748909Z","iopub.status.idle":"2022-04-22T21:24:09.765443Z","shell.execute_reply.started":"2022-04-22T21:24:09.748869Z","shell.execute_reply":"2022-04-22T21:24:09.764726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentations 🛎️","metadata":{}},{"cell_type":"code","source":"fold0 = pd.read_pickle(\"../input/helpers-for-the-ride-folds/train_folds_5.pickle\")\nfold0['span'] = fold0.location","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:24:09.767604Z","iopub.execute_input":"2022-04-22T21:24:09.76821Z","iopub.status.idle":"2022-04-22T21:24:09.824412Z","shell.execute_reply.started":"2022-04-22T21:24:09.768158Z","shell.execute_reply":"2022-04-22T21:24:09.823409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data augmentation\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.char as nac\nfrom sklearn.utils import shuffle\nimport nltk\nimport re \nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('punkt')\naug = naw.ContextualWordEmbsAug(model_path=\"../input/deberta-v3-large/deberta-v3-large\",\n                                device=\"cpu\", action=\"substitute\")\nkeyAug = nac.KeyboardAug(name='Keyboard_Aug', aug_char_min=1, aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3, \n                      aug_word_min=1, aug_word_max=10, stopwords=None, tokenizer=None, reverse_tokenizer=None, \n                      include_special_char=True, include_numeric=True, include_upper_case=True, lang='en', verbose=1, \n                      stopwords_regex=None, model_path=None, min_char=4)\nantonymAug = naw.AntonymAug(name='Antonym_Aug', aug_min=1, aug_max=10, aug_p=0.3, lang='eng', stopwords=None, tokenizer=None, \n                     reverse_tokenizer=None, stopwords_regex=None, verbose=1)\nspelLAug = naw.SpellingAug(dict_path=None, name='Spelling_Aug', aug_min=1, aug_max=10, aug_p=0.3, stopwords=None, \n                      tokenizer=None, reverse_tokenizer=None, include_reverse=True, stopwords_regex=None, verbose=1)\nsynAug = naw.SynonymAug(aug_src='wordnet', model_path=None, name='Synonym_Aug', aug_min=1, aug_max=10, aug_p=0.3, lang='eng', \n                     stopwords=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, force_reload=False, \n                     verbose=1)\n\nrandomeDelaug =naw.RandomWordAug(action='delete', name='RandomWord_Aug', aug_min=1, aug_max=10, aug_p=0.3,\n                                 stopwords=None, target_words=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, verbose=1)\n\nrandomeSwapaug =naw.RandomWordAug(action='swap', name='RandomWord_Aug', aug_min=1, aug_max=10, aug_p=0.3,\n                                 stopwords=None, target_words=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, verbose=1)\n\nrandomeCropaug =naw.RandomWordAug(action='crop', name='RandomWord_Aug', aug_min=1, aug_max=10, aug_p=0.3,\n                                 stopwords=None, target_words=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, verbose=1)\n\nbacktranslation= naw.BackTranslationAug(\n    from_model_name='facebook/wmt19-en-de', \n    to_model_name='facebook/wmt19-de-en'\n)\naugment_dict = {\n    \"word\":aug,\n    \"key\":keyAug,\n    \"antonymAug\": antonymAug,\n    \"spelLaug\" : spelLAug,\n    \"synAug\": synAug,\n    \"backtra\":backtranslation,\n    \"randomeCropaug\":randomeCropaug,\n    \"randomeDelaug\":randomeDelaug,\n    \"randomeSwapaug\":randomeSwapaug\n}\n\nimport re\n# The _tokenizer is not good enough as punctuation will be removed in return.\ndef _tokenizer(text, token_pattern=r\"(?u)\\b\\w\\s\\w+\\b\"):\n            token_pattern = re.compile(token_pattern)\n            return token_pattern.findall(text)\n\n\ndef agument_text_partial(df, flavor=\"synAug\" ,pr=0.4):\n    \"\"\"Augments text and then adds back new NER positions\n    \"\"\"\n    \n    def labels(x):\n        # Add stopwords\n        stopwords = []\n        if x.annotation_length > 0:\n            for anna in x.location: \n                for text in [e.split() for e in anna.split(\";\")]:\n                    stopwords.append(x.pn_history[int(text[0]):int(text[1])])\n        return stopwords\n        \n    def augs(x): \n        #aug.tokenizer = CFG.tokenizer\n        print(f\"Got stopwords {x.labels}\")\n        aug = augment_dict[flavor]    \n        aug.aug_p=pr \n        aug.stopwords = x.labels\n        return aug.augment(x.pn_history)\n    \n    def add_aug_location(x):\n        an = []\n        lables = x.labels\n        idx = 0\n        if x.annotation_length > 0:\n            for anna in x.location:\n                a_i = \"\"\n                for orig_locn in [e for e in anna.split(\";\")]:\n                    text = lables[idx]\n                    start = x.aug_history.find(text)\n                    #print(f\"Got {text} {start}  orig {orig_locn,a_i}\")    \n                    if start>=0:    \n                        a_i += f\"{start} {start+len(text)};\"\n                        an.append(f\"{a_i[:-1]}\")\n                    idx+=1    \n                #print(f\"{idx}[{a_i}]\")    \n            return an\n        else:\n            return []\n    # Augment    \n    df['labels'] = df.apply(lambda x : labels(x),axis=1) \n    df['aug_history'] = df.apply(lambda x : augs(x),axis=1) \n    df['aug_location'] = df.apply(lambda x : add_aug_location(x),axis=1) \n    df['aug_location_len'] = df.aug_location.apply(len)\n    # Only keep if NER labels count matches\n    return df[df.aug_location_len == df.annotation_length]\n\nprint(\"\\nSynonym Augmentation\")\nadf = agument_text_partial(fold0[1:3])\ndisplay(adf[[\"annotation\",\"aug_location\",\"location\",\"aug_location_len\",\"annotation_length\",\"pn_num\",\"labels\",\"pn_history\",\"aug_history\"]])\nif len(adf) >0:\n    plot_annotation_aug(adf , adf['pn_num'][2])\n    print(f\"\\nAugmented {adf['pn_num'][2]}\")\n    plot_annotation_aug(adf , adf['pn_num'][2],\"aug_history\",\"aug_location\", True)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:24:09.826408Z","iopub.execute_input":"2022-04-22T21:24:09.826746Z","iopub.status.idle":"2022-04-22T21:24:10.018105Z","shell.execute_reply.started":"2022-04-22T21:24:09.826694Z","shell.execute_reply":"2022-04-22T21:24:10.017361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fold0.iloc[1:4]","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:24:10.019117Z","iopub.execute_input":"2022-04-22T21:24:10.019351Z","iopub.status.idle":"2022-04-22T21:24:10.024666Z","shell.execute_reply.started":"2022-04-22T21:24:10.019322Z","shell.execute_reply":"2022-04-22T21:24:10.023234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n Spelling Augmentation\")\nsadf = agument_text_partial(fold0[1:4],\"spelLaug\") \ndisplay(sadf)\nplot_annotation_aug(sadf , sadf['pn_num'][3])\nplot_annotation_aug(sadf , sadf['pn_num'][3],\"aug_history\",\"aug_location\", True) ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:24:10.02568Z","iopub.execute_input":"2022-04-22T21:24:10.025919Z","iopub.status.idle":"2022-04-22T21:24:10.102586Z","shell.execute_reply.started":"2022-04-22T21:24:10.02589Z","shell.execute_reply":"2022-04-22T21:24:10.101512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"randomeSwapaug Augmentation\")\nadf_par = agument_text_partial(fold0.iloc[1:3],\"randomeSwapaug\")\ndisplay(adf_par)\nplot_annotation_aug(adf_par , adf_par['pn_num'][2])\nplot_annotation_aug(adf_par , adf_par['pn_num'][2],\"aug_history\",\"aug_location\", True) ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:24:10.103897Z","iopub.execute_input":"2022-04-22T21:24:10.104215Z","iopub.status.idle":"2022-04-22T21:24:10.151934Z","shell.execute_reply.started":"2022-04-22T21:24:10.104181Z","shell.execute_reply":"2022-04-22T21:24:10.150502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n key Augmentation\")\nkeyadf = agument_text_partial(fold0[1:3],\"key\")\ndisplay(keyadf.head())\nplot_annotation_aug(keyadf , keyadf['pn_num'][2])\nplot_annotation_aug(keyadf , keyadf['pn_num'][2],\"aug_history\",\"aug_location\", True) ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:24:10.153952Z","iopub.execute_input":"2022-04-22T21:24:10.154312Z","iopub.status.idle":"2022-04-22T21:24:10.203369Z","shell.execute_reply.started":"2022-04-22T21:24:10.154275Z","shell.execute_reply":"2022-04-22T21:24:10.202249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"print(\"RandomeDelaug Augmentation\\n\")\nadfr = agument_text_partial(fold0.iloc[1:3],\"randomeDelaug\")\ndisplay(adfr.head())\nplot_annotation_aug(adfr , adfr['pn_num'][2])\nplot_annotation_aug(adfr , adfr['pn_num'][2],\"aug_history\",\"aug_location\", True) ","metadata":{}},{"cell_type":"code","source":"print(\"RandomeCropaug Augmentation\\n\")\nadfc = agument_text_partial(fold0.iloc[1:3],\"randomeCropaug\")\ndisplay(adfc.head())\nplot_annotation_aug(adfc , adfc['pn_num'][2])\nplot_annotation_aug(adfc, adfc['pn_num'][2],\"aug_history\",\"aug_location\", True) ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:24:10.205546Z","iopub.execute_input":"2022-04-22T21:24:10.205797Z","iopub.status.idle":"2022-04-22T21:24:10.25003Z","shell.execute_reply.started":"2022-04-22T21:24:10.205768Z","shell.execute_reply":"2022-04-22T21:24:10.248999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thats all folks 😃\n\nHope it helps , I have not shared the code on how to include this into the pipeline though that should be pretty straightforward . There might be some mistakes in stopword impn in **nlpaug** so for now I am just excluding those and not adding them to model training .\n\n⚠️ Also disclaimer there might be mistakes here as this is just a reference code so use our own judgement and testing to implement it . ","metadata":{}}]}