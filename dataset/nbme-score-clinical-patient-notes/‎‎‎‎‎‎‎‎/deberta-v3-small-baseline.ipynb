{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"log\n\nlocal cv: \n \n*     f1 micro : 0.853646\n     \n*     all_time :  1 min 01 sec\n     \n*     estimated for hidden test 2k text files :  0 hr 10 min\n    \n    \nactual public lb:\n\n*     f1 micro : 0.848\n     \n*     all_time :  10 min ","metadata":{}},{"cell_type":"code","source":"if 1:\n    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer\n    import shutil\n    from pathlib import Path\n    \n    transformers_path = Path('/opt/conda/lib/python3.7/site-packages/transformers')\n    input_dir = Path('../input/nbme-submit-00/deberta_v2_convert_tokenizer')\n    \n    convert_file = input_dir / 'convert_slow_tokenizer.py'\n    conversion_path = transformers_path/convert_file.name\n    if conversion_path.exists():\n        conversion_path.unlink()\n    shutil.copy(convert_file, transformers_path)\n    \n    deberta_v2_path = transformers_path / 'models' / 'deberta_v2'\n    for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n        filepath = deberta_v2_path/filename\n        if filepath.exists():\n            filepath.unlink()\n        shutil.copy(input_dir/filename, filepath)\n\n        \nimport sys\nsys.path.append('../input/nbme-submit-00')\n\nimport os\nimport numpy as np\nimport glob\nimport pandas as pd\nimport itertools\nimport ast\nfrom sklearn import metrics\n\nfrom timeit import default_timer as timer\nimport psutil\nimport gc\n\nis_amp   = True  #True #False\nis_cuda  = True\nis_debug = False\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-30T02:49:29.29628Z","iopub.execute_input":"2022-03-30T02:49:29.296725Z","iopub.status.idle":"2022-03-30T02:49:29.31319Z","shell.execute_reply.started":"2022-03-30T02:49:29.296687Z","shell.execute_reply":"2022-03-30T02:49:29.312342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper---------------------------------------------------------------------------------\ndef time_to_str(t, mode='min'):\n    if mode=='min':\n        t  = int(t)/60\n        hr = t//60\n        min = t%60\n        return '%2d hr %02d min'%(hr,min)\n    elif mode=='sec':\n        t   = int(t)\n        min = t//60\n        sec = t%60\n        return '%2d min %02d sec'%(min,sec)\n    else:\n        raise NotImplementedError\n\n#https://stackoverflow.com/questions/61366458/how-to-find-memory-usage-of-kaggle-notebook\ndef memory_used_to_str():\n    pid = os.getpid()\n    processs = psutil.Process(pid)\n    memory_use = processs.memory_info()[0] / 2. ** 30\n    return 'use ram memory gb ' + str(np.round(memory_use, 2))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:49:29.354664Z","iopub.execute_input":"2022-03-30T02:49:29.354867Z","iopub.status.idle":"2022-03-30T02:49:29.361461Z","shell.execute_reply.started":"2022-03-30T02:49:29.354841Z","shell.execute_reply":"2022-03-30T02:49:29.360782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 512\nthreshold_score = 0.5","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:49:29.406172Z","iopub.execute_input":"2022-03-30T02:49:29.406476Z","iopub.status.idle":"2022-03-30T02:49:29.409847Z","shell.execute_reply.started":"2022-03-30T02:49:29.406448Z","shell.execute_reply":"2022-03-30T02:49:29.408987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset ######################################################################################\nsubmit_dir = ''\nfeature_csv_file      = '../input/nbme-score-clinical-patient-notes/features.csv'\npatient_note_csv_file = '../input/nbme-score-clinical-patient-notes/patient_notes.csv'\n\nif is_debug: \n    valid_csv_file = '../input/nbme-score-clinical-patient-notes/train.csv'\n    valid_df = pd.read_csv(valid_csv_file)\n    \n    fold_df = pd.read_csv('../input/nbme-submit-00/valid_fold0.id.csv')\n    valid_df = valid_df[valid_df.id.isin(fold_df.id)].reset_index(drop=True)\n\nelse:\n    valid_csv_file = '../input/nbme-score-clinical-patient-notes/test.csv' \n    valid_df = pd.read_csv(valid_csv_file)\n    \n    \n#---\nfeature_df = pd.read_csv(feature_csv_file)\nfeature_df.loc[27, 'feature_text'] = 'Last-Pap-smear-1-year-ago'\n\npatient_note_df = pd.read_csv(patient_note_csv_file)\nvalid_df = valid_df.merge(feature_df, on=['feature_num', 'case_num'], how='left')\nvalid_df = valid_df.merge(patient_note_df, on=['pn_num', 'case_num'], how='left')\n\nprint('CHECK DF FILES ####################################')\nprint('len(valid_df)',len(valid_df))\nprint('')\nfor t in range(3): print(valid_df.iloc[t],'\\n')\nprint('ok!')\n\n#<todo> sort text for speed?\n\n\n\ndef get_loader(tokenizer, batch_size):\n    from torch.utils.data.dataset import Dataset\n    from torch.utils.data import DataLoader\n    from torch.utils.data.sampler import SequentialSampler\n    import torch\n    \n    def null_collate_fn(batch):\n        d = {}\n        key = batch[0].keys()\n        for k in key:\n            v = [b[k] for b in batch]\n            if k in ['token_in_mask', 'token_out_mask', 'token_type_id', 'token_id',]:\n                v = torch.stack(v)\n            d[k] = v\n        return d\n\n    class NBMEDataset(Dataset):\n        def __init__(self, df, tokenizer, max_length):\n            self.df = df\n            self.max_length = max_length\n            self.tokenizer  = tokenizer\n            self.length     = len(self.df)\n        \n        def __len__(self):\n            return self.length\n        \n        def __getitem__(self, index):\n            d    = self.df.iloc[index]\n            id   = d['id']\n            pn_history   = d.pn_history\n            feature_text = d.feature_text\n            \n            #text to token\n            e = tokenizer(\n                pn_history,\n                feature_text,\n                add_special_tokens=True,\n                max_length=max_length,\n                padding='max_length',\n                return_offsets_mapping=True\n            )\n            token_in_mask  = e['attention_mask']\n            token_type_id  = e['token_type_ids']\n            token_id       = e['input_ids']\n            token_offset   = e['offset_mapping']\n            token_out_mask = (np.array(token_in_mask)==1)&(np.array(token_type_id)==0).astype(np.int8).tolist()\n            \n \n            #-------------------------------------\n            r = {}\n            r['index'] = index\n            r['id'   ] = id\n            r['pn_history'    ] = pn_history\n            r['feature_text'  ] = feature_text\n            r['token_offset'  ] = token_offset\n            r['token_out_mask'] = torch.tensor(token_out_mask, dtype=torch.long)\n            r['token_in_mask' ] = torch.tensor(token_in_mask , dtype=torch.long)\n            r['token_type_id' ] = torch.tensor(token_type_id , dtype=torch.long)\n            r['token_id'      ] = torch.tensor(token_id      , dtype=torch.long)\n            return r\n        \n    valid_dataset = NBMEDataset(valid_df, tokenizer, max_length)\n    valid_loader  = DataLoader(\n        valid_dataset,\n        sampler = SequentialSampler(valid_dataset),\n        batch_size  = batch_size, #4, #\n        drop_last   = False,\n        num_workers = 2, #0, #\n        pin_memory  = False,\n        collate_fn = null_collate_fn,\n    )\n    return valid_loader\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:49:29.459698Z","iopub.execute_input":"2022-03-30T02:49:29.459998Z","iopub.status.idle":"2022-03-30T02:49:29.846472Z","shell.execute_reply.started":"2022-03-30T02:49:29.45997Z","shell.execute_reply":"2022-03-30T02:49:29.845723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## post processing #######################################################\n\ndef array_to_span(array):\n    span = [list(g) for _, g in itertools.groupby(np.where(array==1)[0], key=lambda n, c=itertools.count(): n - next(c))]\n    span = ['%d %d'%(min(r), max(r)+1) for r in span]\n    #location = ';'.join(span)\n    return span\n\n\ndef location_to_array(location, pn_history, format='truth'):\n    if format=='truth':\n        location = location.replace('\"', \"'\").replace(';', \"','\")\n        location = ast.literal_eval(location)\n    if format=='predict':\n        if location is not '':\n            location = location.replace(';', ',')\n            location = location.split(',')\n            #print(location)\n    \n    array = np.zeros(len(pn_history))\n    for loc in location:\n        #print(loc)\n        start, end = loc.split()\n        start, end = int(start), int(end)\n        array[start:end]=1\n    return array\n\n\ndef compute_lb_f1_score(predict_df, truth_df):\n    assert(predict_df['id']==truth_df['id']).all()\n    L = len(truth_df)\n\n    truth    = []\n    predict  = []\n    for i in range(L):\n        dt = truth_df.iloc[i]\n        dp = predict_df.iloc[i]\n        t = location_to_array(dt.location, dt.pn_history, format='truth')\n        p = location_to_array(dp.location, dt.pn_history, format='predict')\n        truth.append(t)\n        predict.append(p)\n        \n    \n    predict = np.concatenate(predict)\n    truth = np.concatenate(truth)\n    lb = metrics.f1_score(truth, predict)\n\n    return lb\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:49:29.848101Z","iopub.execute_input":"2022-03-30T02:49:29.849352Z","iopub.status.idle":"2022-03-30T02:49:29.861868Z","shell.execute_reply.started":"2022-03-30T02:49:29.849309Z","shell.execute_reply":"2022-03-30T02:49:29.860823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## post processing #######################################################\n\ndef array_to_span(array):\n    span = [list(g) for _, g in itertools.groupby(np.where(array==1)[0], key=lambda n, c=itertools.count(): n - next(c))]\n    span = ['%d %d'%(min(r), max(r)+1) for r in span]\n    #location = ';'.join(span)\n    return span\n\n\ndef location_to_array(location, pn_history, format='truth'):\n    if format=='truth':\n        location = location.replace('\"', \"'\").replace(';', \"','\")\n        location = ast.literal_eval(location)\n    if format=='predict':\n        if location is not '':\n            location = location.replace(';', ',')\n            location = location.split(',')\n            #print(location)\n    \n    array = np.zeros(len(pn_history))\n    for loc in location:\n        #print(loc)\n        start, end = loc.split()\n        start, end = int(start), int(end)\n        array[start:end]=1\n    return array\n\n\ndef compute_lb_f1_score(predict_df, truth_df):\n    assert(predict_df['id']==truth_df['id']).all()\n    L = len(truth_df)\n\n    truth    = []\n    predict  = []\n    for i in range(L):\n        dt = truth_df.iloc[i]\n        dp = predict_df.iloc[i]\n        t = location_to_array(dt.location, dt.pn_history, format='truth')\n        p = location_to_array(dp.location, dt.pn_history, format='predict')\n        truth.append(t)\n        predict.append(p)\n        \n    \n    predict = np.concatenate(predict)\n    truth = np.concatenate(truth)\n    lb = metrics.f1_score(truth, predict)\n\n    return lb\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:49:29.863414Z","iopub.execute_input":"2022-03-30T02:49:29.86379Z","iopub.status.idle":"2022-03-30T02:49:29.877902Z","shell.execute_reply.started":"2022-03-30T02:49:29.863755Z","shell.execute_reply":"2022-03-30T02:49:29.877251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model ##################################################################\n\nensemble =(\n    {\n        'arch' : '../input/nbme-submit-00/microsoft-deberta-v3-small',\n        'checkpoint' : \t[\n            '../input/nbme-submit-00/00004000.model.pth',\n            #'/root/share1/kaggle/2022/NBME-patient-notes/result/run02/debert-v3-small-xx1/fold-1/checkpoint/00006000.model.pth',\n        ],\n        'batch_size' : 64,\n    },\n    \n \n\n)\n\ndef load_model_and_run(config):\n    \n    if config['arch']=='../input/nbme-submit-00/microsoft-deberta-v3-small':\n        from deberta_v3_small_model import Net, get_tokenizer\n    # elif config['arch']=='./microsoft-deberta-v3-large':\n    # \tfrom deberta_v3_large_model import Net, get_tokenizer\n    else:\n        raise NotImplementedError\n  \n    import torch\n    from torch.nn.parallel.data_parallel import data_parallel\n    import torch.cuda.amp as amp\n    \n    #from torch.profiler import profile, record_function, ProfilerActivity\n    #with profile(activities=[ProfilerActivity.CPU],\n    #           profile_memory=True, record_shapes=True) as prof:\n    #--------------------------------------------------------------------------\n    \n    num_fold = len(config['checkpoint'])\n    net = Net(config['arch'])\n    net.eval()\n    if is_cuda:\n        net.cuda()\n \n    tokenizer = get_tokenizer(config['arch'])\n    valid_loader = get_loader(tokenizer, config['batch_size'])\n    \n    result = None\n    for n in range(num_fold) :\n        net.load_state_dict(torch.load(config['checkpoint'][n], map_location=lambda storage, loc: storage)['state_dict'],strict=False)\n        print('load \"%s\" ok'%config['checkpoint'][n])\n        \n        r = {\n            'token_out_mask':[],\n            'token_offset':[],\n            'probability' :[],\n            'correction'  :[],\n        }\n        \n        num_valid=0\n        start_timer = timer()\n        for t, batch in enumerate(valid_loader):\n            batch_size = len(batch['index'])\n            if is_cuda:\n                batch['token_in_mask'] = batch['token_in_mask'].cuda()\n                batch['token_type_id'] = batch['token_type_id'].cuda()\n                batch['token_id'     ] = batch['token_id'     ].cuda()\n                \n            with torch.no_grad():\n                with amp.autocast(enabled=is_amp):\n                    output = data_parallel(net,batch)\n                    \n                    r['token_offset'  ] += batch['token_offset']\n                    r['token_out_mask'].append( batch['token_out_mask'].data.cpu().numpy() )\n                    r['probability'   ].append( output['token_label'].data.cpu().numpy() )\n                    r['correction'    ].append( output['token_correction'].data.cpu().numpy() )\n                    num_valid += batch_size\n                    \n            print('\\r\\t%d/%d  %s'%(num_valid, len(valid_loader.dataset), time_to_str(timer() - start_timer,'sec')),end='',flush=True)\n        print('')\n        if result is None:\n            r['token_offset'   ] = np.array(r['token_offset'], object)\n            r['token_out_mask' ] = np.concatenate(r['token_out_mask'])\n            r['probability'    ] = np.concatenate(r['probability'])\n            r['correction'     ] = np.concatenate(r['correction'])\n            result = r\n        else:\n            result['probability'] += np.concatenate(r['probability'])\n            result['correction' ] += np.concatenate(r['correction'])\n    #-----------------------------------------------------------------------\n    #average\n    result['probability'] /= num_fold\n    result['correction' ] /= num_fold\n    result['correction' ] = result['correction'].astype(np.uint8)\n    #print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n    \n    del net, tokenizer\n    del valid_loader\n    gc.collect()\n    if is_cuda: torch.cuda.empty_cache()\n    \n    return result\n\n\ndef run_submit():\n    if is_debug: print(\"THIS IS DEBUG ####################################\")\n    all_time = 0\n    print('start', memory_used_to_str())\n    \n    ensemble_result = []\n    for config in ensemble:\n        start_timer = timer()\n        result = load_model_and_run(config)\n        ensemble_result.append(result)\n        all_time += timer() - start_timer\n        print('')\n        print('end', memory_used_to_str())\n    num_model = len(ensemble_result)\n    \n    \n    #-----------------------------------\n    submit_df = []\n    for i,d in valid_df.iterrows():\n     \n        #ensemble -----\n        prob_pn_history = np.full((len(d.pn_history)),0, np.float32)\n        for m in range(num_model):\n            #<todo> verify id\n            \n            p = ensemble_result[m]['probability'][i]\n            c = ensemble_result[m]['correction' ][i]\n            token_offset = ensemble_result[m]['token_offset'][i]\n            \n            T = ensemble_result[m]['token_out_mask'][i].sum()\n            for t in range(T):\n               start,end = token_offset[t]\n               s = start + c[t,0]\n               e = end   - c[t,1]\n               prob_pn_history[s:e] += p[t]\n        prob_pn_history = prob_pn_history/num_model\n        #ensemble -----\n        \n        predict = prob_pn_history>threshold_score\n        span = array_to_span(predict)#[:-1]\n        location = ';'.join(span)\n        submit_df.append({'id':d.id, 'location':location})\n        \n        #print('\\r preparing submit_df :', i, d.id, len(d.pn_history), end ='', flush=True)\n    print('\\n')\n\n    #----------------------------------------\n    submit_df = pd.DataFrame(data = submit_df)\n    print(submit_df)\n    submit_df.to_csv('submission.csv', index=False)\n \n    \n\n    print('------------------')\n    for t in range(3): print(submit_df.iloc[t],'\\n')\n    print('submission ok!----')\n\n    if is_debug:\n        print(\"THIS IS DEBUG ####################################\")\n        f1_score = compute_lb_f1_score(submit_df, valid_df)\n        print('f1 micro : %f\\n' % f1_score)\n  \n        #2000 patient notes in the test set.\n        print('all_time : %s'%(time_to_str(all_time,'sec')))\n        print('estimated for 10k text files : %s'%(time_to_str(all_time/len(valid_df)*30_000,'min')))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:49:29.880162Z","iopub.execute_input":"2022-03-30T02:49:29.88043Z","iopub.status.idle":"2022-03-30T02:49:29.910583Z","shell.execute_reply.started":"2022-03-30T02:49:29.880396Z","shell.execute_reply":"2022-03-30T02:49:29.90971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_submit()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:49:29.911895Z","iopub.execute_input":"2022-03-30T02:49:29.912187Z","iopub.status.idle":"2022-03-30T02:50:47.535397Z","shell.execute_reply.started":"2022-03-30T02:49:29.912153Z","shell.execute_reply":"2022-03-30T02:50:47.533497Z"},"trusted":true},"execution_count":null,"outputs":[]}]}