{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"paper\n\"Meta Pseudo Labels\" - Hieu Pham\nhttps://arxiv.org/abs/2003.10580\n\nreference\nhttps://github.com/kekmodel/MPL-pytorch\nhttps://zhuanlan.zhihu.com/p/396127336\n\nfor discussion see: https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/discussion/315707","metadata":{}},{"cell_type":"code","source":"import numpy as np \nfrom sklearn import datasets\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n#dummy data ##########################\n\nnum_unlabel = 500\nnum_train = 3\nnum_valid = 100\n\nx_unlabel,y_unlabel = datasets.make_moons(n_samples=num_unlabel*2, shuffle=False, noise=0.2, random_state=123)\nx_train, y_train = datasets.make_moons(n_samples=num_train*2, shuffle=False, noise=0.2, random_state=456)\nx_valid, y_valid = datasets.make_moons(n_samples=num_valid*2, shuffle=False, noise=0.2, random_state=789)\n#print(x_unlabel.max(),x_unlabel.min())\n\ngrid = torch.meshgrid(torch.linspace(-3, 3, steps=100), torch.linspace(-3, 3, steps=100))#, indexing='ij')\ngrid = torch.stack(grid,-1)\ngrid = grid.reshape(-1,2).cuda()\n\n\n\ndef show_data():\n    plt.scatter(x_unlabel[:num_unlabel,0],x_unlabel[:num_unlabel,1], s=2, alpha=1, facecolors='r',)\n    plt.scatter(x_unlabel[num_unlabel:,0],x_unlabel[num_unlabel:,1], s=2, alpha=1, facecolors='b',)\n    plt.scatter(x_valid[:num_valid,0],x_valid[:num_valid,1], s=100, alpha=0.15, c='r',)\n    plt.scatter(x_valid[num_valid:,0],x_valid[num_valid:,1], s=100, alpha=0.15, c='b',)\n    plt.scatter(x_train[:num_train,0],x_train[:num_train,1], s=100, alpha=1, edgecolors='r', facecolors='w',)\n    plt.scatter(x_train[num_train:,0],x_train[num_train:,1], s=100, alpha=1, edgecolors='b', facecolors='w',)\n    plt.ylim([-3, 3])\n    plt.xlim([-3, 3])\n\nshow_data()\nplt.show()\n\n\n\n \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-07T05:46:55.418469Z","iopub.execute_input":"2022-04-07T05:46:55.418785Z","iopub.status.idle":"2022-04-07T05:46:55.757231Z","shell.execute_reply.started":"2022-04-07T05:46:55.418754Z","shell.execute_reply":"2022-04-07T05:46:55.756558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# network and display function ##########################\n\n\ndef show_predict_space(net):\n    net = net.eval()\n    predict = torch.sigmoid(net(grid))\n    predict = predict.data.cpu().numpy()\n    g = grid.data.cpu().numpy()\n    plt.scatter(g[:,0],g[:,1], c=predict)\n    show_data()\n\ndef print_validate(net):\n    \n    x = torch.from_numpy(x_valid).float().cuda()\n    y = torch.from_numpy(y_valid).float().cuda()\n    \n    net = net.eval()\n    logit = net(x)\n    loss = F.binary_cross_entropy_with_logits(logit,y)\n    \n    predict = (torch.sigmoid(logit)>0.5).long()\n    predict = predict.data.cpu().numpy()\n    correct = (predict==y_valid).mean()\n    \n    print('valid BCE loss:', loss.item())\n    print('valid accuracy:', correct)\n    \n    \n##########################################################################################\n\nclass Residual1d(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        \n        self.linear1 = nn.Linear(dim,dim)\n        self.bn1  = nn.BatchNorm1d(dim)\n        self.linear2 = nn.Linear(dim,dim)\n        self.bn2  = nn.BatchNorm1d(dim)\n        self.act = nn.SiLU(inplace=True)\n     \n    def forward(self, x ):\n        residual = x\n        x = self.bn1(self.linear1(x))\n        x = self.act(x)\n        x = self.bn2(self.linear2(x))\n        x = residual+x\n        x = self.act(x)\n        return x\n        \nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.stem = nn.Sequential(\n            nn.Linear(2,128),\n            nn.BatchNorm1d(128),\n            nn.SiLU(inplace=True),\n        )\n        self.feature = nn.ModuleList([\n            Residual1d(128) for i in range(3)\n        ])\n        self.logit = nn.Linear(128, 1)\n    \n     \n    def forward(self, x ):\n        x = self.stem(x)\n        x = self.feature[0](x)\n        x = self.feature[1](x)\n        x = self.feature[2](x)\n        logit = self.logit(x).reshape(-1)\n        return logit\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-07T05:46:55.761257Z","iopub.execute_input":"2022-04-07T05:46:55.763293Z","iopub.status.idle":"2022-04-07T05:46:55.785973Z","shell.execute_reply.started":"2022-04-07T05:46:55.763254Z","shell.execute_reply":"2022-04-07T05:46:55.785035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#experiment 1.\nif 1:\n    #upper bound : train using ground truth of all label+unlabel\n    \n    l_x = torch.from_numpy(x_train).float().cuda()\n    l_y = torch.from_numpy(y_train).float().cuda()\n    \n    net = Net().cuda()\n    optimizer = optim.SGD(net.parameters(),lr=0.001, momentum=0.9)\n    \n    for iteration in range(200):\n        random_sample = np.random.choice(num_unlabel*2,16).tolist()\n        u_x = torch.from_numpy(x_unlabel[random_sample]).float().cuda()\n        u_y = torch.from_numpy(y_unlabel[random_sample]).float().cuda()\n        \n        \n        l_logit = net(l_x)\n        u_logit = net(u_x)\n        loss = (F.binary_cross_entropy_with_logits(l_logit,l_y)+ F.binary_cross_entropy_with_logits(u_logit,u_y))/2\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        #print('\\r', iteration, loss.item(), end='', flush=True)\n    print('')\n    print('upper bound')\n    print_validate(net)\n    show_predict_space(net)\n    plt.title('upper bound')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T05:46:55.790324Z","iopub.execute_input":"2022-04-07T05:46:55.792782Z","iopub.status.idle":"2022-04-07T05:46:57.68311Z","shell.execute_reply.started":"2022-04-07T05:46:55.792748Z","shell.execute_reply":"2022-04-07T05:46:57.682461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#experiment.2 : using labelled data only\nif 1:\n    \n    l_x = torch.from_numpy(x_train).float().cuda()\n    l_y = torch.from_numpy(y_train).float().cuda()\n    \n    net = Net().cuda()\n    optimizer = optim.SGD(net.parameters(),lr=0.001, momentum=0.9)\n    \n    for iteration in range(200):\n        logit = net(l_x)\n        loss = F.binary_cross_entropy_with_logits(logit,l_y)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        \n        #print('\\r', iteration, loss.item(), end='', flush=True)\n    print('')\n    print('labelled only')\n    print_validate(net)\n    show_predict_space(net)\n    plt.title('labelled only')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T05:46:57.685018Z","iopub.execute_input":"2022-04-07T05:46:57.685257Z","iopub.status.idle":"2022-04-07T05:46:58.833022Z","shell.execute_reply.started":"2022-04-07T05:46:57.685225Z","shell.execute_reply":"2022-04-07T05:46:58.832377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# experiment3: metal pseudo label\n# https://github.com/kekmodel/MPL-pytorch/blob/main/main.py\nif 1:\n    l_x = torch.from_numpy(x_train).float().cuda()\n    l_y = torch.from_numpy(y_train).float().cuda()\n    \n    \n    teacher = Net().cuda()\n    student = Net().cuda()\n    t_optimizer = optim.SGD(teacher.parameters(),lr=0.0005, momentum=0.9)\n    s_optimizer = optim.SGD(student.parameters(),lr=0.0005, momentum=0.9)\n\n    for iteration in range(200):\n        # subscript: t,s : teacher,student\n        # subscript: l,u : label,unlabel\n        \n        teacher.train()\n        student.train()\n        t_optimizer.zero_grad()\n        s_optimizer.zero_grad()\n        \n        random_sample = np.random.choice(num_unlabel*2,16).tolist()\n        u_x = torch.from_numpy(x_unlabel[random_sample]).float().cuda()\n     \n        \n        #prepare other input\n        #note these is detached, i.e. not used for backprop\n        s_l_logit = student(l_x)\n        s_l_loss = F.binary_cross_entropy_with_logits(s_l_logit.detach(), l_y)\n        \n        t_u_logit = teacher(u_x)\n        pseudo_y =  (t_u_logit.detach()>0).float()\n        #------\n        \n        \n        \n        #train student : update student using pesudo label data only\n        s_u_logit = student(u_x)\n        s_u_loss  = F.binary_cross_entropy_with_logits(s_u_logit, pseudo_y)\n        s_u_loss.backward()\n        s_optimizer.step()\n        \n        \n        #train teacher : update teacher using pesudo label data student change in loss\n        s_l_logit_new = student(l_x)\n        s_l_loss_new  = F.binary_cross_entropy_with_logits(s_l_logit_new.detach(), l_y)\n        #change = s_l_loss - s_l_loss_new #s_l_loss_new - s_l_loss ??\n        change = s_l_loss_new - s_l_loss\n        #https://github.com/kekmodel/MPL-pytorch/issues/6\n        \n        t_l_logit = teacher(l_x)\n        t_l_loss = F.binary_cross_entropy_with_logits(t_l_logit, l_y)\n        t_mpl_loss  = change * F.binary_cross_entropy_with_logits(t_u_logit, pseudo_y)\n        \n        (t_l_loss+t_mpl_loss).backward()\n        t_optimizer.step()\n    \n        #print(iteration, s_u_loss.item(), t_l_loss.item(), t_mpl_loss.item())\n    #-------------------------------\n    print('metal pseudo label : student')\n    print_validate(student)\n    show_predict_space(student)\n    plt.title('metal pseudo label : student')\n    plt.show()\n    \n    print('metal pseudo label : teacher')\n    print_validate(teacher)\n    show_predict_space(teacher)\n    plt.title('metal pseudo label : teacher')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-07T05:46:58.834096Z","iopub.execute_input":"2022-04-07T05:46:58.834455Z","iopub.status.idle":"2022-04-07T05:47:02.458159Z","shell.execute_reply.started":"2022-04-07T05:46:58.834412Z","shell.execute_reply":"2022-04-07T05:47:02.457526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# experiment4: meta-pesudo-label ( exact, no approximation)\n# https://zhuanlan.zhihu.com/p/396127336\n\nif 1:\n    l_x = torch.from_numpy(x_train).float().cuda()\n    l_y = torch.from_numpy(y_train).float().cuda()\n    \n    \n    teacher = Net().cuda()\n    student = Net().cuda()\n    t_optimizer = optim.SGD(teacher.parameters(),lr=0.001, momentum=0.9)\n    s_optimizer = optim.SGD(student.parameters(),lr=0.001, momentum=0.9)\n    \n    for iteration in range(200):\n        # subscript: t,s : teacher,student\n        # subscript: l,u : label,unlabel\n        \n        teacher.train()\n        student.train()\n        t_optimizer.zero_grad()\n        s_optimizer.zero_grad()\n        \n        random_sample = np.random.choice(num_unlabel*2,16).tolist()\n        u_x = torch.from_numpy(x_unlabel[random_sample]).float().cuda()\n        \n        \n        #prepare other input\n        #note these is detached, i.e. not used for backprop\n        #s_l_logit = student(l_x)\n        #s_l_loss = F.binary_cross_entropy_with_logits(s_l_logit.detach(), l_y)\n        \n        t_u_logit = teacher(u_x)\n        pseudo_y =  torch.sigmoid(t_u_logit) #t_u_logit>0).float()\n        #------\n        \n        \n        \n        #train student : update student using pesudo label data only\n        s_u_logit = student(u_x)\n        s_u_loss  = F.binary_cross_entropy_with_logits(s_u_logit, pseudo_y,reduction='none')\n        s_u_loss  = s_u_loss[torch.abs(pseudo_y-0.5)>0.45].mean()\n        s_u_loss.backward()\n        s_optimizer.step()\n        \n        \n        #train teacher : update teacher using pesudo label data student change in loss\n        s_l_logit_new = student(l_x)\n        s_l_loss_new  = F.binary_cross_entropy_with_logits(s_l_logit_new, l_y)\n        t_mpl_loss    = s_l_loss_new\n        \n        t_l_logit = teacher(l_x)\n        t_l_loss = F.binary_cross_entropy_with_logits(t_l_logit, l_y)\n        \n        \n        (t_l_loss+t_mpl_loss).backward()\n        #(t_mpl_loss).backward()\n        t_optimizer.step()\n        \n       \n        #print(iteration, s_u_loss.item(), t_l_loss.item(), t_mpl_loss.item())\n    #-------------------------------\n    print('metal pseudo label (exact) : student')\n    print_validate(student)\n    show_predict_space(student)\n    plt.title('metal pseudo label (exact): student')\n    plt.show()\n    \n    print('metal pseudo label (exact): teacher')\n    print_validate(teacher)\n    show_predict_space(teacher)\n    plt.title('metal pseudo label (exact): teacher')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-07T05:47:02.459715Z","iopub.execute_input":"2022-04-07T05:47:02.459962Z","iopub.status.idle":"2022-04-07T05:47:06.047376Z","shell.execute_reply.started":"2022-04-07T05:47:02.459927Z","shell.execute_reply":"2022-04-07T05:47:06.046579Z"},"trusted":true},"execution_count":null,"outputs":[]}]}