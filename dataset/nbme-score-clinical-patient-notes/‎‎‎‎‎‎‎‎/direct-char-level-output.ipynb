{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook has two parts:\n1. construct model\n2. construct dataset\n\nfor details please refer to \nhttps://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/discussion/315707\n\n\nimplementation could be buggy. please report any problems. thanks!","metadata":{}},{"cell_type":"code","source":"# set up fast tokenizer from \n# https://www.kaggle.com/code/thanhns/deberta-v3-large-0-883-lb\n# https://www.kaggle.com/datasets/thanhns/deberta-tokenizer\n\n# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:00:15.044756Z","iopub.execute_input":"2022-04-02T13:00:15.045164Z","iopub.status.idle":"2022-04-02T13:00:15.061261Z","shell.execute_reply.started":"2022-04-02T13:00:15.045116Z","shell.execute_reply":"2022-04-02T13:00:15.059714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport ast\nimport itertools\n\n\n\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SequentialSampler\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.cuda.amp as amp\n    \nfrom transformers import AutoTokenizer, AutoConfig, AutoModel, AutoModelForTokenClassification\nfrom transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast as DebertaV3TokenizerFast\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import DebertaV2Model as DebertaV3Model\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:00:15.089536Z","iopub.execute_input":"2022-04-02T13:00:15.089976Z","iopub.status.idle":"2022-04-02T13:00:15.096624Z","shell.execute_reply.started":"2022-04-02T13:00:15.089943Z","shell.execute_reply":"2022-04-02T13:00:15.096017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer\n\narch = 'microsoft/deberta-v3-small'\nlen_tokenizer =128001  \n\ndef get_tokenizer():\n    #tokenizer = AutoTokenizer.from_pretrained(arch, use_fast=True)#, add_prefix_space=True)\n    tokenizer = DebertaV3TokenizerFast.from_pretrained(arch)\n    print('len(tokenizer)', len(tokenizer))  \n    assert(len_tokenizer==len(tokenizer))\n    return tokenizer\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:00:15.13399Z","iopub.execute_input":"2022-04-02T13:00:15.134427Z","iopub.status.idle":"2022-04-02T13:00:15.140464Z","shell.execute_reply.started":"2022-04-02T13:00:15.134395Z","shell.execute_reply":"2022-04-02T13:00:15.139841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model\n\nclass RNNCharHead(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.lstm = nn.LSTM(in_dim, hidden_size=512, num_layers=2, batch_first=True, bidirectional=True)\n        self.out  = nn.Linear(512*2,out_dim)\n        \n    def forward(self, x, x_length):\n        batch_size,max_length,dim=x.shape\n        x_pack = torch.nn.utils.rnn.pack_padded_sequence(x, x_length, batch_first=True, enforce_sorted = False)\n        x_pack, hidden = self.lstm(x_pack)\n        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x_pack, batch_first=True)\n    \n        x = self.out(x)\n        x = F.pad(x,(0,0,0,max_length-max(x_length)),mode='constant',value=0)\n        return x\n    \n    \ndef token_to_char(x, token_to_char_index):\n    batch_size, L, dim = x.shape\n    x = x.reshape(-1,dim)\n    \n    i = token_to_char_index + (torch.arange(batch_size)*L).reshape(-1,1).to(x.device)\n    i = i.reshape(-1)\n \n    c = x[i]\n    c[i==0] = 0\n    c = c.reshape(batch_size,-1,dim)\n    return c\n    \n    \nclass Net(nn.Module):\n    def __init__(self, ):\n        super().__init__()\n        self.output_type = ['probability', 'loss']\n        \n        config = AutoConfig.from_pretrained(arch)\n        config.update(\n            {\n                'output_hidden_states': True,\n                'hidden_dropout_prob': 0.1,\n                'layer_norm_eps':  1e-7,\n                'add_pooling_layer': False,\n                'num_labels': 1,\n            }\n        )\n        self.transformer = AutoModel.from_pretrained(arch, config=config)\n        self.transformer.resize_token_embeddings(len_tokenizer) #len(tokenizer))\n \n        #token\n        self.token_label = nn.Linear(config.hidden_size, 1)\n        self.rnn = RNNCharHead(config.hidden_size, 1)\n     \n        \n \n\n    def forward(self, batch ):\n    \n        tx = self.transformer(\n            input_ids      = batch['token_id'],\n            attention_mask = batch['token_in_mask'],\n            token_type_ids = batch['token_type_id'],\n        )\n        last = tx.last_hidden_state\n        token_label = self.token_label(last)\n        \n        last_char  = token_to_char(last, batch['token_to_char_index'])\n        char_label = self.rnn(last_char, batch['char_length'])\n        \n        \n        output = {}\n        if 'loss' in self.output_type:\n            \n            if self.training==True:\n                # Multi-Sample Dropout https://github.com/abhishekkrthakur/long-text-token-classification/issues/3\n                token_loss = 0\n                for dropout_rate in [0.1, 0.2, 0.3, 0.4, 0.5]:\n                    t = self.token_label( F.dropout(last, dropout_rate, training=self.training) )\n                    token_loss += compute_token_label_loss(t, batch['token_label'], batch['token_out_mask'])/5\n                output['token_loss'] = token_loss\n          \n            if self.training==False:\n                output['token_loss']  = compute_token_label_loss(token_label, batch['token_label'], batch['token_out_mask'])\n            #---\n            output['char_loss']  = compute_char_label_loss(char_label, batch['char_label'], batch['char_mask'])\n         \n        if 'probability' in self.output_type:\n            output['token_label'] = torch.sigmoid(token_label).squeeze(-1)\n            output['char_label' ] = torch.sigmoid(char_label).squeeze(-1)\n            \n        return output\n\n#loss function\ndef compute_token_label_loss(logit, target, mask):\n    batch_size, max_len, _ = logit.shape\n\n    keep   = (mask.reshape(-1) == 1) & (target.reshape(-1) >= 0)\n    logit  = logit.reshape(-1)\n    target = target.reshape(-1)\n    logit  = logit[keep]\n    target = target[keep]\n    \n    loss = F.binary_cross_entropy_with_logits(logit, target)\n    return loss\n\n\ndef compute_char_label_loss(logit, target, mask):\n    \n    keep   = (mask.reshape(-1) == 1)\n    logit  = logit.reshape(-1)\n    target = target.reshape(-1)\n    logit  = logit[keep]\n    target = target[keep]\n    \n    loss = F.binary_cross_entropy_with_logits(logit, target)\n    return loss\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-02T13:00:15.179583Z","iopub.execute_input":"2022-04-02T13:00:15.180058Z","iopub.status.idle":"2022-04-02T13:00:15.208225Z","shell.execute_reply.started":"2022-04-02T13:00:15.180022Z","shell.execute_reply":"2022-04-02T13:00:15.207452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset\n\ndef d_to_token(d, tokenizer, max_length, max_char_length):\n   \n    e = tokenizer.encode_plus(\n        d.pn_history,\n        d.feature_text,\n        add_special_tokens=True,\n        max_length=max_length,\n        padding='max_length',\n        truncation=True,\n        return_offsets_mapping=True,\n        return_token_type_ids=True,\n    )\n    \n    token_in_mask  = e['attention_mask']\n    token_type_id  = e['token_type_ids']\n    token_id       = e['input_ids']\n    token_offset   = e['offset_mapping']\n    token_out_mask = (np.array(token_in_mask)==1)&(np.array(token_type_id)==0).astype(np.int8).tolist()\n    token_label    = np.zeros(max_length, np.int8)\n\n\n    token_to_char_index = np.zeros(max_char_length, np.int32)\n    char_label  = np.zeros(max_char_length, np.int8)\n    char_length = len(d.pn_history)\n    char_mask   = np.zeros(max_char_length, np.int8)\n    char_mask[:char_length]=1\n    \n    for (start, end) in d.span:\n        char_label[start:end]=1\n        \n    for i,(start,end) in enumerate(token_offset):\n        if start!=end:\n            token_label[i] =  max(char_label[start:end]) \n            if token_type_id[i]==0 : token_to_char_index[start:end]=i\n         \n    ignore = np.where(np.array(e.sequence_ids())!= 0)[0]\n    token_label[ignore] = -1\n    \n    #--- \n    if 0: # debug : print encoding\n        print('pn_history:\\n',d.pn_history)\n        print('feature_text:\\n',d.feature_text)\n        print('')\n        print('sum(token_out_mask):',sum(token_out_mask))\n        token = tokenizer.convert_ids_to_tokens(token_id)\n        for i,(start,end) in enumerate(token_offset):\n            print(\n                '%4d'%i,\n                '%12s'%str(token_offset[i]),\n                '%d'%token_in_mask[i],\n                '%d'%token_out_mask[i],\n                '%d'%token_type_id[i],\n                '%6d'%token_id[i],\n                '%2d'%token_label[i],\n                '%20s'%token[i],\n                '%20s'%repr(d.pn_history[start:end]) if token_type_id[i]==0 else '%20s'%repr(d.feature_text[start:end]),\n                '%+40s'%str(char_label[start:end]) if token_type_id[i]==0 else '',\n                '%+40s'%str(token_to_char_index[start:end]) if token_type_id[i]==0 else '',\n            )\n        input('wait for key press')\n    \n    r = {}\n    r['pn_history'    ] = d.pn_history\n    r['feature_text'  ] = d.feature_text\n    r['token_offset'  ] = token_offset\n    r['token_out_mask'] = torch.tensor(token_out_mask, dtype=torch.long)\n    r['token_in_mask' ] = torch.tensor(token_in_mask , dtype=torch.long)\n    r['token_type_id' ] = torch.tensor(token_type_id , dtype=torch.long)\n    r['token_id'      ] = torch.tensor(token_id      , dtype=torch.long)\n    r['token_label'   ] = torch.tensor(token_label   , dtype=torch.float)\n\n    r['char_length'   ] = char_length\n    r['char_label'    ] = torch.tensor(char_label   , dtype=torch.float)\n    r['char_mask'     ] = torch.tensor(char_mask    , dtype=torch.long)\n    r['token_to_char_index' ] = torch.tensor(token_to_char_index   , dtype=torch.long)\n    return r\n\n\ntensor_list = [\n    'token_in_mask', 'token_out_mask', 'token_type_id', 'token_id',\n    'token_label',\n    'char_label', 'char_mask', 'token_to_char_index',\n]\n\n\nclass NBMEDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length, max_char_length):\n        \n        self.df = df\n        self.max_length = max_length\n        self.max_char_length = max_char_length\n        self.tokenizer = tokenizer\n        self.length = len(self.df)\n    \n    def __str__(self):\n        string = ''\n        string += '\\tlen = %d\\n' % len(self)\n        return string\n    \n    def __len__(self):\n        return self.length\n    \n    def __getitem__(self, index):\n        d = self.df.iloc[index]\n        r = d_to_token(d, self.tokenizer, self.max_length, self.max_char_length)\n        \n        r['index']= index\n        r['id'] = d.id\n        return r\n\n\ndef null_collate_fn(batch):\n    d = {}\n    key = batch[0].keys()\n    for k in key:\n        v = [b[k] for b in batch]\n        if k in tensor_list:\n            v = torch.stack(v)\n        d[k] = v\n    return d","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:01:21.064852Z","iopub.execute_input":"2022-04-02T13:01:21.065301Z","iopub.status.idle":"2022-04-02T13:01:21.097299Z","shell.execute_reply.started":"2022-04-02T13:01:21.065257Z","shell.execute_reply":"2022-04-02T13:01:21.096209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def location_to_array(location, pn_history, format='truth'):\n    \n    if format=='truth':\n        location = location.replace('\"', \"'\").replace(';', \"','\")\n        location = ast.literal_eval(location)\n    if format=='predict':\n        if location is not '':\n            location = location.replace(';', ',')\n            location = location.split(',')\n            #print(location)\n        \n    array = np.zeros(len(pn_history))\n    for loc in location:\n        start, end = loc.split()\n        start, end = int(start), int(end)\n        array[start:end]=1\n    return array\n\ndef array_to_span(array, format='string'):\n    span = [list(g) for _, g in itertools.groupby(np.where(array==1)[0], key=lambda n, c=itertools.count(): n - next(c))]\n    \n    if format=='string':\n        span = ['%d %d'%(min(r), max(r)+1) for r in span]\n    if format=='list':\n        span = [[min(r), max(r)+1] for r in span]\n    \n    #location = ';'.join(span)\n    return span\n\ndef location_to_span(location, pn_history):\n    array = location_to_array(location, pn_history, format='truth')\n    span = array_to_span(array, format='list')\n    return span","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:00:15.253224Z","iopub.execute_input":"2022-04-02T13:00:15.25371Z","iopub.status.idle":"2022-04-02T13:00:15.269915Z","shell.execute_reply.started":"2022-04-02T13:00:15.253666Z","shell.execute_reply":"2022-04-02T13:00:15.269241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#example\n\ntrain_df   = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')\nfeature_df = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\npatient_note_df = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n\nall_df = train_df.merge(feature_df, on=['feature_num', 'case_num'], how='left')\nall_df = all_df.merge(patient_note_df, on=['pn_num', 'case_num'], how='left')\nall_df['span'] = all_df.apply(lambda d: location_to_span(d.location, d.pn_history), axis=1)\n \ntokenizer = get_tokenizer()\n\ndataset = NBMEDataset(all_df, tokenizer, max_length=256, max_char_length=1024)\nloader = DataLoader(\n            dataset,\n            sampler = SequentialSampler(dataset),\n            batch_size  = 8,\n            drop_last   = True,\n            num_workers = 2,\n            pin_memory  = False,\n            worker_init_fn = lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n            collate_fn = null_collate_fn,\n        )\nprint(dataset)\nbatch = next(iter(loader)) \n\n#---\n\nnet = Net()\n#print(net)\nnet.train()\n\nwith torch.no_grad(): \n    output = net(batch)\n\nprint('batch')\nfor k,v in batch.items():\n    if k in tensor_list: \n        print('%32s :'%k, v.shape)\n\nprint('output')\nfor k,v in output.items():\n    if 'loss' not in k:\n        print('%32s :'%k, v.shape)\nfor k,v in output.items():\n    if 'loss' in k:\n        print('%32s :'%k, v.item())\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:02:38.457045Z","iopub.execute_input":"2022-04-02T13:02:38.457456Z","iopub.status.idle":"2022-04-02T13:02:54.079581Z","shell.execute_reply.started":"2022-04-02T13:02:38.457417Z","shell.execute_reply":"2022-04-02T13:02:54.078571Z"},"trusted":true},"execution_count":null,"outputs":[]}]}