{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## This is the partner notebook to [my training notebook](https://www.kaggle.com/nbroad/qa-ner-hybrid-train-nbme)\n\nThis is using a base model so the score will likely go up when using a large model. Please feel free to leave any comments or questions!","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\n\nfrom datasets import Dataset\nfrom transformers import (\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n    RobertaPreTrainedModel,\n    RobertaModel,\n    AutoTokenizer,\n    AutoConfig,\n    logging,\n)\nfrom transformers.modeling_outputs import TokenClassifierOutput\n\n\nlogging.set_verbosity(logging.WARNING)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:01:09.219812Z","iopub.execute_input":"2022-02-28T10:01:09.22016Z","iopub.status.idle":"2022-02-28T10:01:17.031297Z","shell.execute_reply.started":"2022-02-28T10:01:09.220075Z","shell.execute_reply":"2022-02-28T10:01:17.030605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    \n    n_folds = 5\n    model_path = \"../input/5f-rob-b-nbme/fold{fold}\"\n    args = TrainingArguments(\n        output_dir=\".\",\n        per_device_eval_batch_size=64,\n        dataloader_num_workers=2,\n    )","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:01:17.032788Z","iopub.execute_input":"2022-02-28T10:01:17.034585Z","iopub.status.idle":"2022-02-28T10:01:17.091747Z","shell.execute_reply.started":"2022-02-28T10:01:17.034554Z","shell.execute_reply":"2022-02-28T10:01:17.091049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/test.csv\")\nnotes_df = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/patient_notes.csv\") \nfeats_df = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/features.csv\")\nmerged = test_df.merge(notes_df, how=\"left\")   \nmerged = merged.merge(feats_df, how=\"left\")\nmerged.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:01:17.093087Z","iopub.execute_input":"2022-02-28T10:01:17.093359Z","iopub.status.idle":"2022-02-28T10:01:17.795731Z","shell.execute_reply.started":"2022-02-28T10:01:17.093308Z","shell.execute_reply":"2022-02-28T10:01:17.794787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_feature_text(text):\n    return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n\ndef tokenize(examples):\n    tokenized_inputs =  tokenizer(\n        examples[\"feature_text\"],\n        examples[\"pn_history\"],\n        padding=True,\n        max_length=416,\n        truncation=\"only_second\",\n        return_offsets_mapping=True\n    )\n    tokenized_inputs[\"sequence_ids\"] = [tokenized_inputs.sequence_ids(i) for i in range(len(tokenized_inputs[\"input_ids\"]))]\n    return tokenized_inputs\n\nds = Dataset.from_pandas(merged)\n\nds = ds.map(lambda x: {\"feature_text\": process_feature_text(x[\"feature_text\"])})\n\ntokenizer = AutoTokenizer.from_pretrained(CFG.model_path.format(fold=0))\ntokenized_ds = ds.map(tokenize, batched=True)\n\ntokenized_ds","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:01:17.797575Z","iopub.execute_input":"2022-02-28T10:01:17.798311Z","iopub.status.idle":"2022-02-28T10:01:18.146669Z","shell.execute_reply.started":"2022-02-28T10:01:17.798272Z","shell.execute_reply":"2022-02-28T10:01:18.145989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mostly copied from: https://github.com/huggingface/transformers/blob/master/src/transformers/models/roberta/modeling_roberta.py\nclass HybridRoberta(RobertaPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = torch.nn.Dropout(classifier_dropout)\n        self.classifier = torch.nn.Linear(config.hidden_size, 1)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n            loss = loss_fct(logits.view(-1, 1), labels.view(-1, 1))\n            \n            loss = torch.masked_select(loss, labels.view(-1, 1) > -1).mean()\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:01:18.148023Z","iopub.execute_input":"2022-02-28T10:01:18.148576Z","iopub.status.idle":"2022-02-28T10:01:18.161081Z","shell.execute_reply.started":"2022-02-28T10:01:18.148537Z","shell.execute_reply":"2022-02-28T10:01:18.160201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sigmoid(z):\n    return 1/(1 + np.exp(-z))\n\ndef get_location_predictions(dataset, preds, test=False):\n    \"\"\"\n    It's easier to run CV if we don't convert predictions into\n    the format expected at test time.\n    \n    If `test=True` then it will turn the predictions into the format\n    expected for submission\n    \"\"\"\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, dataset[\"offset_mapping\"], dataset[\"sequence_ids\"]):\n        pred = sigmoid(pred)\n        start_idx = None\n        current_preds = []\n        for p, o, s_id in zip(pred, offsets, seq_ids):\n            if s_id is None or s_id == 0:\n                continue\n                \n            if p > 0.5:\n                if start_idx is None:\n                    start_idx = o[0]\n                end_idx = o[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        \n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n    \n    return all_predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:01:18.163767Z","iopub.execute_input":"2022-02-28T10:01:18.163982Z","iopub.status.idle":"2022-02-28T10:01:18.173727Z","shell.execute_reply.started":"2022-02-28T10:01:18.163959Z","shell.execute_reply":"2022-02-28T10:01:18.173078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n\n%env TOKENIZERS_PARALLELISM=true\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nconfig = AutoConfig.from_pretrained(CFG.model_path.format(fold=0))\n\nall_preds = None\nfor fold in range(CFG.n_folds):\n\n\n    model = HybridRoberta.from_pretrained(CFG.model_path.format(fold=fold), config=config)\n\n    trainer = Trainer(\n        model=model,\n        args=CFG.args,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n    )\n\n    predictions = trainer.predict(tokenized_ds)\n\n    if all_preds is None:\n        all_preds = predictions.predictions.astype(np.float32)\n    else:\n        all_preds += predictions.predictions.astype(np.float32)\n\n\n    torch.cuda.empty_cache()\n    \nall_preds /= CFG.n_folds","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:01:18.17488Z","iopub.execute_input":"2022-02-28T10:01:18.175192Z","iopub.status.idle":"2022-02-28T10:02:11.603059Z","shell.execute_reply.started":"2022-02-28T10:01:18.175159Z","shell.execute_reply":"2022-02-28T10:02:11.602281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_predictions = get_location_predictions(tokenized_ds, all_preds.squeeze(), test=True)\n\nsubmission_df = pd.DataFrame(data={\n            \"id\": tokenized_ds[\"id\"], \n            \"location\": location_predictions\n        })\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\ndisplay(submission_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:02:11.607427Z","iopub.execute_input":"2022-02-28T10:02:11.612227Z","iopub.status.idle":"2022-02-28T10:02:11.651127Z","shell.execute_reply.started":"2022-02-28T10:02:11.612186Z","shell.execute_reply":"2022-02-28T10:02:11.650375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# If you vist this notebook please upvote it.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}