{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from ast import literal_eval\nfrom itertools import chain\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport time\n\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModel, AutoTokenizer\n#display options\npd.set_option('display.max_colwidth', None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-01T05:28:41.472793Z","iopub.execute_input":"2022-05-01T05:28:41.473345Z","iopub.status.idle":"2022-05-01T05:28:41.480705Z","shell.execute_reply.started":"2022-05-01T05:28:41.473305Z","shell.execute_reply":"2022-05-01T05:28:41.479993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WordCloud","metadata":{}},{"cell_type":"code","source":"features = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\nfeatures.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:41.484548Z","iopub.execute_input":"2022-05-01T05:28:41.484962Z","iopub.status.idle":"2022-05-01T05:28:41.503561Z","shell.execute_reply.started":"2022-05-01T05:28:41.484917Z","shell.execute_reply":"2022-05-01T05:28:41.50288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"body = '-'.join(features['feature_text'].apply(lambda x: x.lower()).tolist())","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:41.505089Z","iopub.execute_input":"2022-05-01T05:28:41.505306Z","iopub.status.idle":"2022-05-01T05:28:41.509939Z","shell.execute_reply.started":"2022-05-01T05:28:41.505275Z","shell.execute_reply":"2022-05-01T05:28:41.509038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import package\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Generate word cloud\nwordcloud = WordCloud(width=3000,\n                      height=2000,\n                      random_state=1, \n                      background_color='white',\n                      colormap='Pastel1',\n                      collocations=False,\n                      stopwords=['-', 'or', 'of', 'with', 'ago'])\\\n                      .generate(body)\n# Plot\nplt.rcParams[\"figure.figsize\"] = (15, 10)\nplt.imshow(wordcloud)\nplt.title('Wordcloud of features', fontsize=24)\nplt.axis('off')\nplt.savefig('./wordcloud.png')","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:41.511197Z","iopub.execute_input":"2022-05-01T05:28:41.5116Z","iopub.status.idle":"2022-05-01T05:28:57.526332Z","shell.execute_reply.started":"2022-05-01T05:28:41.511564Z","shell.execute_reply":"2022-05-01T05:28:57.524994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data_preprocess Functions","metadata":{}},{"cell_type":"code","source":"BASE_URL = \"../input/nbme-score-clinical-patient-notes\"\n\ndef process_feature_text(text):\n    return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \").replace(\"I-year\", \"1-year\")\n\ndef prepare_datasets():\n    features = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    df = pd.read_csv(f\"{BASE_URL}/train.csv\")\n    \n    df[\"annotation_list\"] = [literal_eval(x) for x in df[\"annotation\"]]\n    df[\"location_list\"] = [literal_eval(x) for x in df[\"location\"]]\n\n    df.loc[338, 'annotation_list'] = literal_eval('[[\"father heart attack\"]]')\n    df.loc[338, 'location_list'] = literal_eval('[[\"764 783\"]]')\n    \n    df.loc[621, 'annotation_list'] = literal_eval('[[\"for the last 2-3 months\"]]')\n    df.loc[621, 'location_list'] = literal_eval('[[\"77 100\"]]')\n    \n    df.loc[655, 'annotation_list'] = literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n    df.loc[655, 'location_list'] = literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n    \n    df.loc[1262, 'annotation_list'] = literal_eval('[[\"mother thyroid problem\"]]')\n    df.loc[1262, 'location_list'] = literal_eval('[[\"551 557;565 580\"]]')\n    \n    df.loc[1265, 'annotation_list'] = literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n    df.loc[1265, 'location_list'] = literal_eval('[[\"131 135;181 212\"]]')\n    \n    df.loc[1396, 'annotation_list'] = literal_eval('[[\"stool , with no blood\"]]')\n    df.loc[1396, 'location_list'] = literal_eval('[[\"259 280\"]]')\n    \n    df.loc[1591, 'annotation_list'] = literal_eval('[[\"diarrhoe non blooody\"]]')\n    df.loc[1591, 'location_list'] = literal_eval('[[\"176 184;201 212\"]]')\n    \n    df.loc[1615, 'annotation_list'] = literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n    df.loc[1615, 'location_list'] = literal_eval('[[\"249 257;271 288\"]]')\n    \n    df.loc[1664, 'annotation_list'] = literal_eval('[[\"no vaginal discharge\"]]')\n    df.loc[1664, 'location_list'] = literal_eval('[[\"822 824;907 924\"]]')\n    \n    df.loc[1714, 'annotation_list'] = literal_eval('[[\"started about 8-10 hours ago\"]]')\n    df.loc[1714, 'location_list'] = literal_eval('[[\"101 129\"]]')\n    \n    df.loc[1929, 'annotation_list'] = literal_eval('[[\"no blood in the stool\"]]')\n    df.loc[1929, 'location_list'] = literal_eval('[[\"531 539;549 561\"]]')\n    \n    df.loc[2134, 'annotation_list'] = literal_eval('[[\"last sexually active 9 months ago\"]]')\n    df.loc[2134, 'location_list'] = literal_eval('[[\"540 560;581 593\"]]')\n    \n    df.loc[2191, 'annotation_list'] = literal_eval('[[\"right lower quadrant pain\"]]')\n    df.loc[2191, 'location_list'] = literal_eval('[[\"32 57\"]]')\n    \n    df.loc[2553, 'annotation_list'] = literal_eval('[[\"diarrhoea no blood\"]]')\n    df.loc[2553, 'location_list'] = literal_eval('[[\"308 317;376 384\"]]')\n    \n    df.loc[3124, 'annotation_list'] = literal_eval('[[\"sweating\"]]')\n    df.loc[3124, 'location_list'] = literal_eval('[[\"549 557\"]]')\n    \n    df.loc[3858, 'annotation_list'] = literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n    df.loc[3858, 'location_list'] = literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n    \n    df.loc[4373, 'annotation_list'] = literal_eval('[[\"for 2 months\"]]')\n    df.loc[4373, 'location_list'] = literal_eval('[[\"33 45\"]]')\n    \n    df.loc[4763, 'annotation_list'] = literal_eval('[[\"35 year old\"]]')\n    df.loc[4763, 'location_list'] = literal_eval('[[\"5 16\"]]')\n    \n    df.loc[4782, 'annotation_list'] = literal_eval('[[\"darker brown stools\"]]')\n    df.loc[4782, 'location_list'] = literal_eval('[[\"175 194\"]]')\n    \n    df.loc[4908, 'annotation_list'] = literal_eval('[[\"uncle with peptic ulcer\"]]')\n    df.loc[4908, 'location_list'] = literal_eval('[[\"700 723\"]]')\n    \n    df.loc[6016, 'annotation_list'] = literal_eval('[[\"difficulty falling asleep\"]]')\n    df.loc[6016, 'location_list'] = literal_eval('[[\"225 250\"]]')\n    \n    df.loc[6192, 'annotation_list'] = literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n    df.loc[6192, 'location_list'] = literal_eval('[[\"197 218;236 260\"]]')\n    \n    df.loc[6380, 'annotation_list'] = literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n    df.loc[6380, 'location_list'] = literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n    \n    df.loc[6562, 'annotation_list'] = literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n    df.loc[6562, 'location_list'] = literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n    \n    df.loc[6862, 'annotation_list'] = literal_eval('[[\"stressor taking care of many sick family members\"]]')\n    df.loc[6862, 'location_list'] = literal_eval('[[\"288 296;324 363\"]]')\n    \n    df.loc[7022, 'annotation_list'] = literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n    df.loc[7022, 'location_list'] = literal_eval('[[\"108 182\"]]')\n    \n    df.loc[7422, 'annotation_list'] = literal_eval('[[\"first started 5 yrs\"]]')\n    df.loc[7422, 'location_list'] = literal_eval('[[\"102 121\"]]')\n    \n    df.loc[8876, 'annotation_list'] = literal_eval('[[\"No shortness of breath\"]]')\n    df.loc[8876, 'location_list'] = literal_eval('[[\"481 483;533 552\"]]')\n    \n    df.loc[9027, 'annotation_list'] = literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n    df.loc[9027, 'location_list'] = literal_eval('[[\"92 102\"], [\"123 164\"]]')\n    \n    df.loc[9938, 'annotation_list'] = literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n    df.loc[9938, 'location_list'] = literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n   \n    df.loc[9973, 'annotation_list'] = literal_eval('[[\"gaining 10-15 lbs\"]]')\n    df.loc[9973, 'location_list'] = literal_eval('[[\"344 361\"]]')\n    \n    df.loc[10513, 'annotation_list'] = literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n    df.loc[10513, 'location_list'] = literal_eval('[[\"600 611\"], [\"607 623\"]]')\n    \n    df.loc[11551, 'annotation_list'] = literal_eval('[[\"seeing her son knows are not real\"]]')\n    df.loc[11551, 'location_list'] = literal_eval('[[\"386 400;443 461\"]]')\n    \n    df.loc[11677, 'annotation_list'] = literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n    df.loc[11677, 'location_list'] = literal_eval('[[\"160 201\"]]')\n    \n    df.loc[12124, 'annotation_list'] = literal_eval('[[\"tried Ambien but it didnt work\"]]')\n    df.loc[12124, 'location_list'] = literal_eval('[[\"325 337;349 366\"]]')\n    \n    df.loc[12279, 'annotation_list'] = literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n    df.loc[12279, 'location_list'] = literal_eval('[[\"405 459;488 524\"]]')\n    \n    df.loc[12289, 'annotation_list'] = literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n    df.loc[12289, 'location_list'] = literal_eval('[[\"353 400;488 524\"]]')\n    \n    df.loc[13238, 'annotation_list'] = literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n    df.loc[13238, 'location_list'] = literal_eval('[[\"293 307\"], [\"321 331\"]]')\n    \n    df.loc[13297, 'annotation_list'] = literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n    df.loc[13297, 'location_list'] = literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n    \n    df.loc[13299, 'annotation_list'] = literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n    df.loc[13299, 'location_list'] = literal_eval('[[\"79 88\"], [\"409 418\"]]')\n    \n    df.loc[13845, 'annotation_list'] = literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n    df.loc[13845, 'location_list'] = literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n    \n    df.loc[14083, 'annotation_list'] = literal_eval('[[\"headache generalized in her head\"]]')\n    df.loc[14083, 'location_list'] = literal_eval('[[\"56 64;156 179\"]]')\n    \n    merged = df.merge(notes, how=\"left\")\n    merged = merged.merge(features, how=\"left\")\n\n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n    \n    if hyperparameters['debug']:\n        merged = merged.sample(n=1000,random_state=0).reset_index(drop=True)\n    \n    return merged","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:57.528571Z","iopub.execute_input":"2022-05-01T05:28:57.529078Z","iopub.status.idle":"2022-05-01T05:28:57.565964Z","shell.execute_reply.started":"2022-05-01T05:28:57.529039Z","shell.execute_reply":"2022-05-01T05:28:57.565141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer Helper Functions","metadata":{}},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\n\ndef tokenize_and_add_labels(tokenizer, data, config):\n    out = tokenizer(\n        data[\"feature_text\"],  # question\n        data[\"pn_history\"],  # context\n        truncation=config['truncation'],\n        max_length=config['max_length'],\n        padding=config['padding'],\n        return_offsets_mapping=config['return_offsets_mapping']\n    )\n    labels = [0.0] * len(out[\"input_ids\"])\n    out[\"location_int\"] = loc_list_to_ints(data[\"location_list\"])\n    out[\"sequence_ids\"] = out.sequence_ids()\n\n    for idx, (seq_id, offsets) in enumerate(zip(out[\"sequence_ids\"],\n                                                out[\"offset_mapping\"])):\n        if not seq_id or seq_id == 0:    # seq_id == None: special tokens | seq_id == 0: question\n            labels[idx] = -1\n            continue\n\n        token_start, token_end = offsets\n        for feature_start, feature_end in out[\"location_int\"]:\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                break\n\n    out[\"labels\"] = labels\n\n    return out","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:57.567199Z","iopub.execute_input":"2022-05-01T05:28:57.567494Z","iopub.status.idle":"2022-05-01T05:28:57.580362Z","shell.execute_reply.started":"2022-05-01T05:28:57.567453Z","shell.execute_reply":"2022-05-01T05:28:57.57959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction and Score Helper Function","metadata":{}},{"cell_type":"code","source":"def get_location_predictions(preds, offset_mapping, sequence_ids, test=False):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        pred = 1 / (1 + np.exp(-pred))\n        start_idx = None\n        end_idx = None\n        current_preds = []\n        for pred, offset, seq_id in zip(pred, offsets, seq_ids):\n            if seq_id is None or seq_id == 0:\n                continue\n\n            if pred > 0.5:\n                if start_idx is None:\n                    start_idx = offset[0]\n                end_idx = offset[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n            \n    return all_predictions\n\n\ndef calculate_char_cv(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n                                               \n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros(num_chars)\n\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n\n        char_preds = np.zeros(num_chars)\n\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n            \n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n\n    results = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", labels=np.unique(all_preds))\n                                        \n    accuracy = accuracy_score(all_labels, all_preds)\n\n    return {\n        \"Accuracy\": accuracy,\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:57.581742Z","iopub.execute_input":"2022-05-01T05:28:57.582219Z","iopub.status.idle":"2022-05-01T05:28:57.59659Z","shell.execute_reply.started":"2022-05-01T05:28:57.582183Z","shell.execute_reply":"2022-05-01T05:28:57.595824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data_Loader","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        data = self.data.iloc[idx]\n        tokens = tokenize_and_add_labels(self.tokenizer, data, self.config)\n\n        input_ids = np.array(tokens[\"input_ids\"])\n        attention_mask = np.array(tokens[\"attention_mask\"])\n#         token_type_ids = np.array(tokens[\"token_type_ids\"])\n\n        labels = np.array(tokens[\"labels\"])\n        offset_mapping = np.array(tokens['offset_mapping'])\n        sequence_ids = np.array(tokens['sequence_ids']).astype(\"float16\")\n        \n#         return input_ids, attention_mask, token_type_ids, labels, offset_mapping, sequence_ids\n        return input_ids, attention_mask, labels, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:57.597906Z","iopub.execute_input":"2022-05-01T05:28:57.598373Z","iopub.status.idle":"2022-05-01T05:28:57.608868Z","shell.execute_reply.started":"2022-05-01T05:28:57.598338Z","shell.execute_reply":"2022-05-01T05:28:57.608205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hyperparameters = {\n    \"max_length\": 416,\n    \"padding\": \"max_length\",\n    \"return_offsets_mapping\": True,\n    \"truncation\": \"only_second\",\n    \"debug\": False,\n\n    \"model_name\": (\"../input/roberta-transformers-pytorch/roberta-base\"),\n    \"dropout\": 0.5,\n    \"encoder_lr\": 2e-5,\n    \"decoder_lr\": 2e-5,\n    \"weight_decay\": 0.01,\n    \"betas\": (0.9, 0.999),\n    \"hidden_size\": 768,\n    \n    \"seed\": 1268,\n    \"batch_size\": 8,\n    \"epochs\": 3,\n    \n    \"apex\": True,\n    \"eps\": 1e-6,\n    \n    \"n_fold\": 5,\n    \"trn_fold\": [1, 2, 3, 4, 5]\n}\nif hyperparameters['debug']:\n    hyperparameters['epochs'] = 2\n    hyperparameters['trn_fold'] = [1,2]","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:57.611736Z","iopub.execute_input":"2022-05-01T05:28:57.612028Z","iopub.status.idle":"2022-05-01T05:28:57.620868Z","shell.execute_reply.started":"2022-05-01T05:28:57.611993Z","shell.execute_reply":"2022-05-01T05:28:57.620218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass CustomModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.model = AutoModel.from_pretrained(config['model_name']) \n        self.config = config\n        self.dropout = nn.Dropout(p=config['dropout'])\n#         self.fc1 = nn.Linear(768, 512)\n#         self.fc2 = nn.Linear(512, 512)\n#         self.fc3 = nn.Linear(512, 1)\n \n#         self.lstm = nn.LSTM(input_size=768, hidden_size=384,\n#                             num_layers=2, bidirectional=True, batch_first=True)\n#         self.norm = nn.BatchNorm1d(416, affine=False)\n        self.fc = nn.Linear(768, 1)\n        \n    def forward(self, input_ids, attention_mask ):\n        outputs = self.model(input_ids=input_ids,\n                            attention_mask=attention_mask)\n#                             token_type_ids=token_type_ids)\n#         logits = F.relu(self.fc1(outputs[0]))\n#         logits = F.relu(self.fc2(self.dropout(logits)))\n#         logits = self.fc3(self.dropout(logits)).squeeze(-1)\n\n#         enc, _ = self.lstm(outputs[0])\n#         logits = self.fc(self.dropout(enc)).squeeze(-1)\n\n#         logits = self.norm(self.dropout(outputs[0]))\n\n        logits = F.relu(self.dropout(outputs[0]))\n        logits = self.fc(logits).squeeze(-1)\n        \n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:57.621785Z","iopub.execute_input":"2022-05-01T05:28:57.621988Z","iopub.status.idle":"2022-05-01T05:28:57.63157Z","shell.execute_reply.started":"2022-05-01T05:28:57.621965Z","shell.execute_reply":"2022-05-01T05:28:57.630709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 实例化K折\nfrom sklearn.model_selection import KFold\nfold = KFold(n_splits=5, shuffle=True, random_state=1268)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:57.632817Z","iopub.execute_input":"2022-05-01T05:28:57.633287Z","iopub.status.idle":"2022-05-01T05:28:57.642241Z","shell.execute_reply.started":"2022-05-01T05:28:57.63325Z","shell.execute_reply":"2022-05-01T05:28:57.641544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(hyperparameters['model_name'])\n# tokenizer.save_pretrained('./tokenizer/')","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:57.64384Z","iopub.execute_input":"2022-05-01T05:28:57.644204Z","iopub.status.idle":"2022-05-01T05:28:57.815692Z","shell.execute_reply.started":"2022-05-01T05:28:57.644164Z","shell.execute_reply":"2022-05-01T05:28:57.814971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"# Optimizer\ndef get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay):\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n         'lr': encoder_lr, 'weight_decay': weight_decay},\n        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n         'lr': encoder_lr, 'weight_decay': 0.0},\n        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n         'lr': decoder_lr, 'weight_decay': 0.0}\n    ]\n    return optimizer_parameters","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:57.817215Z","iopub.execute_input":"2022-05-01T05:28:57.817484Z","iopub.status.idle":"2022-05-01T05:28:57.824799Z","shell.execute_reply.started":"2022-05-01T05:28:57.817447Z","shell.execute_reply":"2022-05-01T05:28:57.823887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, criterion):\n    #,scheduler\n    model.train()\n    train_loss = []\n    #scaler = torch.cuda.amp.GradScaler(enabled=hyperparameters['apex'])\n    global_step = 0\n    for step,batch in enumerate(tqdm(dataloader)):\n        optimizer.zero_grad()\n        #(8,416),context和question的词表索引+padding（0）\n        input_ids = batch[0].to(DEVICE)\n        #(8,416) valid ,0或1,1表示需要attention计算（包括context和queation）\n        attention_mask = batch[1].to(DEVICE)\n        #(8,416)，0或1，区分(context,queation)和其他序列(包括padding,[cls][seq])\n#         token_type_ids = batch[2].to(DEVICE)\n        #（8，416），context和question为0，其中答案为1，其他为0（包括padding,[cls][seq]）\n        labels = batch[2].to(DEVICE)\n\n        #(8,416)\n        logits = model(input_ids, attention_mask)\n                       \n        #(8,416),因为实例化时添加reduction=None\n        loss = criterion(logits, labels)\n        #计算平均loss忽略labels=-1的位置\n        #标量\n        loss = torch.masked_select(loss, labels > -1.0).mean()\n        train_loss.append(loss.item() * input_ids.size(0))\n        loss.backward()\n        #scaler.scale(loss).backward()\n        # clip the the gradients to 1.0.\n        # It helps in preventing the exploding gradient problem\n        # it's also improve f1 accuracy slightly\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n#         if (step+1) % hyperparameters['gradient_accumulation_steps'] ==0:\n#             scaler.step(optimizer)\n#             scaler.update()\n#             optimizer.zero_grad()\n#             global_step+=1\n#             if hyperparameters['batch_scheduler']:\n#                 scheduler.step()\n\n    return sum(train_loss) / len(train_loss)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:57.826237Z","iopub.execute_input":"2022-05-01T05:28:57.826541Z","iopub.status.idle":"2022-05-01T05:28:57.838777Z","shell.execute_reply.started":"2022-05-01T05:28:57.826504Z","shell.execute_reply":"2022-05-01T05:28:57.837882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, dataloader, criterion):\n        model.eval()\n        valid_loss = []\n        preds = []\n        offsets = []\n        seq_ids = []\n        valid_labels = []\n\n        for batch in tqdm(dataloader):\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n#             token_type_ids = batch[2].to(DEVICE)\n            labels = batch[2].to(DEVICE)\n            offset_mapping = batch[3]\n            sequence_ids = batch[4]\n\n            logits = model(input_ids, attention_mask)\n            loss = criterion(logits, labels)\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            valid_loss.append(loss.item() * input_ids.size(0))\n\n            preds.append(logits.detach().cpu().numpy())\n            offsets.append(offset_mapping.numpy())\n            seq_ids.append(sequence_ids.numpy())\n            valid_labels.append(labels.detach().cpu().numpy())\n\n        preds = np.concatenate(preds, axis=0)\n        offsets = np.concatenate(offsets, axis=0)\n        seq_ids = np.concatenate(seq_ids, axis=0)\n        valid_labels = np.concatenate(valid_labels, axis=0)\n        \n        location_preds = get_location_predictions(preds,offsets,seq_ids, test=False)                                                           \n        score = calculate_char_cv(location_preds, offsets, seq_ids, valid_labels)\n                  \n        return sum(valid_loss)/len(valid_loss), score","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:57.840209Z","iopub.execute_input":"2022-05-01T05:28:57.840694Z","iopub.status.idle":"2022-05-01T05:28:57.852363Z","shell.execute_reply.started":"2022-05-01T05:28:57.840657Z","shell.execute_reply":"2022-05-01T05:28:57.851627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = prepare_datasets()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:57.854478Z","iopub.execute_input":"2022-05-01T05:28:57.854998Z","iopub.status.idle":"2022-05-01T05:28:58.480651Z","shell.execute_reply.started":"2022-05-01T05:28:57.854962Z","shell.execute_reply":"2022-05-01T05:28:58.479939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time, gc\nsince = time.time()\n\nfrom torch.utils.data import random_split,SubsetRandomSampler\nfrom matplotlib import pyplot as plt\n\ntrain_loss_data_avg, valid_loss_data_avg, score_data_avg_list= [], [], []\ndf_data = CustomDataset(train_df,tokenizer,hyperparameters)\n\nfold_num = 0\nfor train_index,test_index in fold.split(train_df):\n    if (fold_num+1) in hyperparameters['trn_fold']:\n        train_sampler = SubsetRandomSampler(train_index)\n        test_sampler = SubsetRandomSampler(test_index)\n        print(f'第{fold_num+1}折模型')\n        print(f'训练集大小：{len(train_index)}\\n',f'测试集大小:{len(test_index)}')\n        \n        train_dataloader = DataLoader(df_data,\n                                     batch_size=hyperparameters['batch_size'],\n                                     shuffle=False,pin_memory=True,sampler=train_sampler)\n        test_dataloader = DataLoader(df_data,\n                                     batch_size=hyperparameters['batch_size'],\n                                     shuffle=False,pin_memory=True,sampler=test_sampler)\n        \n        train_loss_data, valid_loss_data, score_data_list = [], [], []\n        valid_loss_min = np.Inf\n        best_loss = np.inf\n        \n        DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        #初始化模型、loss、优化器\n        model = CustomModel(hyperparameters).to(DEVICE)\n        criterion = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n        optimizer_parameters = get_optimizer_params(model,\n                                            encoder_lr=hyperparameters['encoder_lr'],\n                                            decoder_lr=hyperparameters['decoder_lr'],\n                                            weight_decay=hyperparameters['weight_decay'])\n        optimizer = optim.AdamW(optimizer_parameters, lr=hyperparameters['encoder_lr'],eps=hyperparameters['eps'],betas=hyperparameters['betas'])\n\n        for i in range(hyperparameters['epochs']):\n            print(\"Epoch: {}/{}\".format(i + 1, hyperparameters['epochs']))\n            # first train model\n            train_loss = train_model(model, train_dataloader, optimizer, criterion)\n#             train_loss = train_loss/len(train_dataloader.sampler)\n            train_loss_data.append(train_loss)\n            print(f\"Train loss: {train_loss}\")\n\n            #evaluate model\n            valid_loss, score = eval_model(model, test_dataloader, criterion)\n#             valid_loss = valid_loss/len(test_dataloader.sampler)\n            valid_loss_data.append(valid_loss)\n            score_data_list.append(score['f1'])\n            print(f\"Valid loss: {valid_loss}\")\n            print(f\"Valid score: {score}\")\n\n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                torch.save(model.state_dict(), f\"nbme_pubmed_bert_fold{fold_num+1}.pth\")\n            \n            train_loss_data_avg.append(np.mean(train_loss_data))\n            valid_loss_data_avg.append(np.mean(valid_loss_data))\n            score_data_avg_list.append(np.mean(score_data_list)) \n        \n        plt.plot(train_loss_data, label=\"Training loss\")\n        plt.plot(valid_loss_data, label=\"validation loss\")\n        plt.legend(frameon=False)\n        plt.show()\n        plt.clf()  \n        \n        torch.cuda.empty_cache()\n        gc.collect()\n        fold_num+=1\n    else:\n        fold_num+=1\n        train_loss_data, valid_loss_data, score_data_list = [], [], []\n        valid_loss_min = np.Inf\n        best_loss = np.inf\n \nprint('Performance of {} fold cross validation'.format(hyperparameters['n_fold']))\nprint(\"Average Training Loss: {:.3f} \\t Average Valid Loss: {:.3f} \\t Average Score: {:.2f}\"\n      .format(np.mean(train_loss_data_avg),np.mean(valid_loss_data_avg),np.mean(score_data_avg_list)))  \n    \ntime_elapsed = time.time() - since\nprint('Training completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:28:58.482843Z","iopub.execute_input":"2022-05-01T05:28:58.483263Z","iopub.status.idle":"2022-05-01T05:29:31.700304Z","shell.execute_reply.started":"2022-05-01T05:28:58.483226Z","shell.execute_reply":"2022-05-01T05:29:31.699101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from matplotlib import pyplot as plt\n\n# plt.plot(train_loss_data, label=\"Training loss\")\n# plt.plot(valid_loss_data, label=\"validation loss\")\n# plt.legend(frameon=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:29:31.701539Z","iopub.status.idle":"2022-05-01T05:29:31.702183Z","shell.execute_reply.started":"2022-05-01T05:29:31.701948Z","shell.execute_reply":"2022-05-01T05:29:31.701973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nscore_df = pd.DataFrame.from_dict(score_data_list)\nscore_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:29:31.703443Z","iopub.status.idle":"2022-05-01T05:29:31.704085Z","shell.execute_reply.started":"2022-05-01T05:29:31.703818Z","shell.execute_reply":"2022-05-01T05:29:31.703842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"# def create_test_df():\n#     feats = pd.read_csv(f\"{BASE_URL}/features.csv\")\n#     notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n#     test = pd.read_csv(f\"{BASE_URL}/test.csv\")\n\n#     merged = test.merge(notes, how=\"left\")\n#     merged = merged.merge(feats, how=\"left\")\n\n#     def process_feature_text(text):\n#         return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \").replace(\"I-year\", \"1-year\")\n    \n#     merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n#     return merged\n\n\n# class SubmissionDataset(Dataset):\n#     def __init__(self, data, tokenizer, config):\n#         self.data = data\n#         self.tokenizer = tokenizer\n#         self.config = config\n    \n#     def __len__(self):\n#         return len(self.data)\n    \n#     def __getitem__(self, idx):\n#         example = self.data.loc[idx]\n#         tokenized = self.tokenizer(\n#             example[\"feature_text\"],\n#             example[\"pn_history\"],\n#             truncation = self.config['truncation'],\n#             max_length = self.config['max_length'],\n#             padding = self.config['padding'],\n#             return_offsets_mapping = self.config['return_offsets_mapping']\n#         )\n#         tokenized[\"sequence_ids\"] = tokenized.sequence_ids()\n\n#         input_ids = np.array(tokenized[\"input_ids\"])\n#         attention_mask = np.array(tokenized[\"attention_mask\"])\n#         token_type_ids = np.array(tokenized[\"token_type_ids\"])\n#         offset_mapping = np.array(tokenized[\"offset_mapping\"])\n#         sequence_ids = np.array(tokenized[\"sequence_ids\"])\\\n#                         .astype(\"float16\")\n\n#         return input_ids, attention_mask, token_type_ids, offset_mapping, sequence_ids\n\n\n# test_df = create_test_df()\n\n# submission_data = SubmissionDataset(test_df, tokenizer, hyperparameters)\n# submission_dataloader = DataLoader(submission_data,\n#                                    batch_size=hyperparameters['batch_size'],\n#                                    pin_memory=True,\n#                                    shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:29:31.705324Z","iopub.status.idle":"2022-05-01T05:29:31.705954Z","shell.execute_reply.started":"2022-05-01T05:29:31.705697Z","shell.execute_reply":"2022-05-01T05:29:31.705721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = []\n# offsets = []\n# seq_ids = []\n# logits_container = []\n# for batch in tqdm(submission_dataloader):\n#     input_ids = batch[0].to(DEVICE)\n#     attention_mask = batch[1].to(DEVICE)\n#     token_type_ids = batch[2].to(DEVICE)\n#     offset_mapping = batch[3]\n#     sequence_ids = batch[4]\n\n#     for fold in hyperparameters['trn_fold']:\n#         model.load_state_dict(torch.load(f\"nbme_pubmed_bert_fold{fold}.pth\"))\n#         model.eval()\n#         logits = model(input_ids, attention_mask, token_type_ids).detach().cpu().numpy()\n#         logits_container.append(logits)\n    \n#     print(logits_container)\n#     preds.append(np.mean(logits_container,axis=0))\n#     offsets.append(offset_mapping.numpy())\n#     seq_ids.append(sequence_ids.numpy())\n\n# preds = np.concatenate(preds, axis=0)\n# offsets = np.concatenate(offsets, axis=0)\n# seq_ids = np.concatenate(seq_ids, axis=0)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:29:31.70719Z","iopub.status.idle":"2022-05-01T05:29:31.707862Z","shell.execute_reply.started":"2022-05-01T05:29:31.707604Z","shell.execute_reply":"2022-05-01T05:29:31.70763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# location_preds = get_location_predictions(preds, offsets, seq_ids, test=True)\n# test_df[\"location\"] = location_preds\n# test_df[[\"id\", \"location\"]].to_csv(\"submission.csv\", index = False)\n# pd.read_csv(f\"{BASE_URL}/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-01T05:29:31.709118Z","iopub.status.idle":"2022-05-01T05:29:31.709723Z","shell.execute_reply.started":"2022-05-01T05:29:31.709496Z","shell.execute_reply":"2022-05-01T05:29:31.709519Z"},"trusted":true},"execution_count":null,"outputs":[]}]}