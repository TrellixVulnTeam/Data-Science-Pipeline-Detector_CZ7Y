{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Ensemble of the three public Deberta notebooks. Scores 0.884 on the leaderboard (scoring takes 5+ hours).\n\nPlease upvote the original notebooks:\n\n**Deberta v3 large**\n\n[inference](https://www.kaggle.com/code/lunapandachan/nbme-thanh-s-infer-add-test)\n\n[original inference](https://www.kaggle.com/code/thanhns/deberta-v3-large-0-883-lb)\n\n[model](https://www.kaggle.com/datasets/thanhns/deberta-v3-large-5-folds-public)\n\n**Deberta v1 large**\n\n[inference](https://www.kaggle.com/code/manojprabhaakr/nbme-deberta-large-baseline-inference)\n\n[model](https://www.kaggle.com/datasets/manojprabhaakr/debertalarge)\n\n**Deberta v1 base**\n\n[train](https://www.kaggle.com/code/yasufuminakama/nbme-deberta-base-baseline-train)\n\n[inference](https://www.kaggle.com/code/yasufuminakama/nbme-deberta-base-baseline-inference)","metadata":{}},{"cell_type":"markdown","source":"**What is going on:**\nFirst I use the publicly available models to calculate the predictions_v3_l, predictions_v1_l and predictions_v3_b. These are lists of np.arrays. Each np.array corresponds to one patient note / feature number combination and represents probabilities that n-th letter in the patient note should be selected as belonging to the feature.\n\nThen in the very end I take these probabilities and for each patient note+feature number combine them in a simple linear combination:\n```\npredictions = []\nfor p1, p2, p3 in zip(predictions_v3_l, predictions_v1_l, predictions_v1_b):\n    predictions.append(w1*p1 + w2*p2 + w3*p3)\n```\nThe weights `w1,w2,w3` I got from playing with the out-of-fold results that comes with each trained model (this is in a separate notebook, private at the moment).\n\nWith the final \"probabilities\" (they can now actually go above one, so not really probabilities any more), I just get the results\n\n```\nresults = get_results(predictions)\n```\n\nand save them.\n\n**Why it takes five hours:**\nThe notebook evaluates all three models on the full test dataset (\\~2000 patient notes) and then combines the results. The large Debertas take about 2 hours each, the base one takes about 1 hour. Reason why this happens is because the models are huge, Kaggle GPU does not have much RAM and you need to use small batchsizes. It may be possible to increase the batchsizes here by a bit (~ factor two), but I did not look into it. That would speed things up a little. When you run the notebook, the models are only evaluated for the five notes in \"test.csv\" so it runs way quicker.","metadata":{}},{"cell_type":"markdown","source":"# Weights","metadata":{"execution":{"iopub.status.busy":"2022-04-01T01:01:55.256298Z","iopub.execute_input":"2022-04-01T01:01:55.256585Z","iopub.status.idle":"2022-04-01T01:01:55.275393Z","shell.execute_reply.started":"2022-04-01T01:01:55.256514Z","shell.execute_reply":"2022-04-01T01:01:55.274772Z"}}},{"cell_type":"code","source":"w1 = 0.5     # Deberta v3 large  # 0.5\nw2 = 0.4      # Deberta v3 large - 5fold\nw3 = 0.2     # Deberta v1 (large or base)  # 0.4\n# w4 = 0.1     # Deberta v1 base   # 0.18","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    \n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport ast\nimport sys\nimport copy\nimport json\nimport math\nimport string\nimport pickle\nimport random\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Seed","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed=42):\n    '''\n    Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.\n    '''\n    random.seed(seed)\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        # When running on the CuDNN backend, two further options must be set\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nseed_everything(seed=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions for scoring","metadata":{}},{"cell_type":"code","source":"def micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    \n    return f1_score(truths, preds)\n\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n        \n    return binary\n\n\ndef span_micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n        \n    return micro_f1(bin_preds, bin_truths)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_labels_for_scoring(df):\n    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n        if len(location_list) > 0:\n            location = location_list[0]\n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n        \n    return truths\n\n\ndef get_char_probs(texts, predictions, tokenizer):\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n            \n    return results\n\n\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n        \n    return results\n\n\ndef get_predictions(results):\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n        \n    return predictions\n\n\ndef get_score(y_true, y_pred):\n    return span_micro_f1(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# post-processing and ensemble tools","metadata":{}},{"cell_type":"code","source":"def _trim_space_start_or_end(location_string):\n    trim_location_string = location_string\n    if not location_string != location_string:\n        if location_string[0] == ' ' and location_string[-1] == ' ':\n            trim_location_string = location_string[1:]\n            trim_location_string = trim_location_string[:-1]\n        elif location_string[0] == ' ':\n            trim_location_string = location_string[1:]\n        elif location_string[-1] == ' ':\n            trim_location_string = location_string[:-1]\n    return trim_location_string\n\n\ndef _get_start_end_from_location_string(base_location, other_location):\n    base_location = _trim_space_start_or_end(base_location)\n    other_location = _trim_space_start_or_end(other_location)\n    base_start = int(base_location.split(' ')[0])\n    base_end = int(base_location.split(' ')[1])\n    other_start = int(other_location.split(' ')[0])\n    other_end = int(other_location.split(' ')[1])\n    return base_start, base_end, other_start, other_end\n\n\ndef _get_location_difference(base_location, other_location):\n    base_start, base_end, other_start, other_end = _get_start_end_from_location_string(base_location, other_location)\n    start_difference = abs(base_start - other_start)\n    end_difference = abs(base_end - other_end)\n    mid_difference_base = abs(base_end - other_start)\n    mid_difference_other = abs(other_end - base_start)\n    return start_difference, end_difference, mid_difference_base, mid_difference_other","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Weighted Box Fusions","metadata":{}},{"cell_type":"code","source":"def _get_box_fusion(base_location, other_location):\n    base_start, base_end, other_start, other_end = _get_start_end_from_location_string(base_location, other_location)\n    mean_start = int((base_start + other_start) / 2)\n    mean_end = int((base_end + other_end) / 2)\n    box_fusion = f'{mean_start} {mean_end}'\n    return box_fusion\n\n\ndef _get_matrix_cross_difference(from_location_list):\n    len_location_split = len(from_location_list)\n    matrix_cross_location = np.ones((len_location_split, len_location_split), dtype='int') * 999\n    for index_i in range(len_location_split):\n        for index_j in range(index_i+1, len_location_split):\n            location_difference = _get_location_difference(from_location_list[index_i], from_location_list[index_j])\n            matrix_cross_location[index_i, index_j] = min(location_difference[2], location_difference[3])\n    return matrix_cross_location\n\n\ndef _get_matrix_start_position_difference(from_location_list):\n    len_location_split = len(from_location_list)\n    matrix_start_position_location = np.ones((len_location_split, len_location_split), dtype='int') * 999\n    for index_i in range(len_location_split):\n        for index_j in range(index_i+1, len_location_split):\n            location_difference = _get_location_difference(from_location_list[index_i], from_location_list[index_j])\n            matrix_start_position_location[index_i, index_j] = min(location_difference[0], location_difference[1])\n    return matrix_start_position_location\n\ndef _order_by_start_position(location_list):\n    tuple_list = [(int(element.split(' ')[0]), int(element.split(' ')[1])) for element in location_list]\n    sorted_tuple_list = sorted(tuple_list, key=lambda t: t[0])\n    ordered_location_list = [f\"{element[0]} {element[1]}\" for element in sorted_tuple_list]\n    return ordered_location_list\n\n\ndef _matrix_postprocess_location(location_string, matrix_usage='cross'):\n    postprocess_location = location_string\n    if location_string == '':\n        postprocess_location = float('NaN')\n    elif not location_string != location_string:\n        location_split = location_string.split(';')\n        location_split = [_trim_space_start_or_end(element) for element in location_split]\n        location_split = list(set(location_split))\n        location_split = _order_by_start_position(location_split)\n        if len(location_split) > 1:\n            if matrix_usage == 'cross':\n                matrix_location = _get_matrix_cross_difference(location_split)\n                where_ones = np.where(matrix_location <= 2)\n            elif matrix_usage == 'start':\n                matrix_location = _get_matrix_start_position_difference(location_split)\n                where_ones = np.where(matrix_location <= 5)\n\n            array_ones_size = where_ones[0].size\n            if array_ones_size > 0:\n                sub_location_split = []\n                for index in range(array_ones_size):\n                    index_i = where_ones[0][index]\n                    index_j = where_ones[1][index]\n                    start_end_location = _get_start_end_from_location_string(location_split[index_i], \n                                                                             location_split[index_j])\n                    min_start = min(start_end_location[0], start_end_location[2])  \n                    max_end = max(start_end_location[1], start_end_location[3])\n                    sub_location_split.append(f'{min_start} {max_end}')\n                \n                processed_index = list(set(list(np.unique(where_ones[0])) + list(np.unique(where_ones[1]))))\n                not_processed_index = [index_raw for index_raw in range(len(location_split)) if index_raw not in processed_index]\n                for index_raw in not_processed_index:\n                    sub_location_split.append(location_split[index_raw])\n                sub_location = ';'.join(sub_location_split)\n                postprocess_location = _matrix_postprocess_location(sub_location, matrix_usage=matrix_usage)\n            else:\n                postprocess_location = ';'.join(location_split)\n    return postprocess_location","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _postprocess_location(location_string):\n    cross_matrix_postprocess = _matrix_postprocess_location(location_string)\n    postprocess_location = _matrix_postprocess_location(cross_matrix_postprocess, matrix_usage='start')\n    return postprocess_location","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _custom_weighted_box_fusion(base_location, other_location, treshold:int = 5):\n    # Check if NaN (works with string and float('NaN') values)\n    isnan_base = base_location != base_location\n    isnan_other = other_location != other_location\n    # If base is NaN and other is not, fill base value with other's\n    if isnan_base and not isnan_other:\n        wbf_location = other_location\n    elif not isnan_other and not isnan_base:\n        if base_location != other_location:\n            wbf_location = ''\n            base_location_split = base_location.split(\";\")\n            other_location_split = other_location.split(';')\n            set_location_split = list(set(base_location_split + other_location_split))\n            first_set_location = set_location_split.pop(0)\n            box_fusion_used = False\n            for set_location_string in set_location_split:\n                start_difference, end_difference, mid_difference_base, mid_difference_other = _get_location_difference(first_set_location, \n                                                                                                                       set_location_string)\n                if start_difference <= treshold and end_difference <= treshold:\n                    if wbf_location != '' and wbf_location[-1] != ';':\n                        wbf_location += ';'\n                    wbf_location += _get_box_fusion(first_set_location, set_location_string)\n                    wbf_location += ';'\n                    box_fusion_used = True\n                else:\n                    if wbf_location != '' and wbf_location[-1] != ';':\n                        wbf_location += ';'\n                    wbf_location += f'{first_set_location};{set_location_string}'\n\n            if box_fusion_used:\n                if wbf_location[-1] == ';':\n                    wbf_location = wbf_location[:-1]\n        else:\n            wbf_location = base_location\n    else:\n        wbf_location = base_location\n    \n    return_wbf = _postprocess_location(wbf_location)     \n    return return_wbf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _check_duplications(location_string):\n    no_duplicate = location_string\n    if not location_string != location_string:\n        location_split = location_string.split(';')\n        location_split = [_trim_space_start_or_end(element) for element in location_split]\n        location_split = list(set(location_split))\n        location_split = _order_by_start_position(location_split)\n        no_duplicate = ';'.join(location_split)\n    return no_duplicate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"main_dir=\"../input/nbme-score-clinical-patient-notes/\"\n\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\n\n\ntest = pd.read_csv(main_dir+'test.csv')\n# test = pd.read_csv(main_dir+'train.csv')\nsubmission = pd.read_csv(main_dir+'sample_submission.csv')\nfeatures = pd.read_csv(main_dir+'features.csv')\npatient_notes = pd.read_csv(main_dir+'patient_notes.csv')\n\nfeatures = preprocess_features(features)\n\nprint(f\"test.shape: {test.shape}\")\ndisplay(test.head())\nprint(f\"features.shape: {features.shape}\")\ndisplay(features.head())\nprint(f\"patient_notes.shape: {patient_notes.shape}\")\ndisplay(patient_notes.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.merge(features, on=['feature_num', 'case_num'], how='left')\ntest = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\ndisplay(test.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deberta v3 large","metadata":{}},{"cell_type":"code","source":"# class CFG:\n#     num_workers=4\n#     path=\"../input/deberta-v3-large-5-folds-public/\"\n#     config_path=path+'config.pth'\n#     model=\"microsoft/deberta-v3-large\"\n#     batch_size=32\n#     fc_dropout=0.2\n#     max_len=354\n#     seed=42\n#     n_fold=5\n#     trn_fold=[0, 1, 2, 3, 4]\n\n\n# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    path=\"../input/nbmedebertav3largefold10/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"  # [\"microsoft/deberta-base\", \"kamalkraj/bioelectra-base-discriminator-pubmed\"]\n    batch_size=32\n    fc_dropout=0.2\n    max_len=354\n    seed=42\n    n_fold=10\n    trn_fold=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] # [0, 1, 2, 3, 4]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n\ntokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\nCFG.tokenizer = tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text, feature_text):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           max_length=CFG.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item])\n        \n        return inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[link to the notebook for the fast inference](https://www.kaggle.com/code/anyai28/fast-inference-by-padding-optimization)","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input_fast(cfg, text, feature_text, batch_max_len):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           max_length=batch_max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDatasetFast(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n        self.batch_max_len = df['batch_max_length'].values\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input_fast(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item],\n                               self.batch_max_len[item],\n                              )\n        return inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ScoringModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        \n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        \n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        \n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    \n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    \n    return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn_fast(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n    # for inputs in test_loader:\n        bs = len(inputs['input_ids'])\n        pred_w_pad = np.zeros((bs, CFG.max_len, 1))\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        y_preds = y_preds.sigmoid().to('cpu').numpy()\n        pred_w_pad[:, :y_preds.shape[1]] = y_preds\n        preds.append(pred_w_pad)\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Reduce Padding Inference ######\n\n# sort by token num\n# input_lengths = []\n# tk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\n# for text, feature_text in tk0:\n#     length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n#     input_lengths.append(length)\n# test['input_lengths'] = input_lengths\n# length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n\n# # sort dataframe\n# sort_df = test.iloc[length_sorted_idx]\n\n# # calc max_len per batch\n# sorted_input_length = sort_df['input_lengths'].values\n# batch_max_length = np.zeros_like(sorted_input_length)\n# bs = CFG.batch_size\n# for i in range((len(sorted_input_length)//bs)+1):\n#     batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \n# sort_df['batch_max_length'] = batch_max_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Original version\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions_v3_l = []\nfor fold in CFG.trn_fold:\n    model = ScoringModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"debertav3-itpt-10ep_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    \n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions_v3_l.append(char_probs)\n    del model, state, prediction, char_probs\n    gc.collect()\n    torch.cuda.empty_cache()\npredictions_v3_l = np.mean(predictions_v3_l, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fast version\n\n# test_dataset = TestDatasetFast(CFG, sort_df)\n# test_loader = DataLoader(test_dataset,\n#                       batch_size=CFG.batch_size,\n#                       shuffle=False,\n#                       num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = ScoringModel(CFG, config_path=CFG.config_path, pretrained=False)\n#     state = torch.load(CFG.path+f\"debertav3-itpt-10ep_fold{fold}_best.pth\",\n#                        map_location=torch.device('cpu'))\n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn_fast(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n#     ## data re-sort ## \n#     prediction = prediction[np.argsort(length_sorted_idx)]\n\n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs; gc.collect()\n#     torch.cuda.empty_cache()\n# predictions_v3_l = np.mean(predictions, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deberta v3 large (5 fold)","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=\"../input/nbme-deverta-v3-large-new-model-ep5-fold5/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    model_name='deberta-v3-large'\n    batch_size=32\n    fc_dropout=0.2\n    max_len=354\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\nCFG.tokenizer = tokenizer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Original\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = ScoringModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"microsoft-deberta-v3-large_fold{fold}_best.pth\",\n                           map_location=torch.device('cpu'))\n       \n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs\n    gc.collect()\n    torch.cuda.empty_cache()\n    \npredictions_v3_l_5f = np.mean(predictions, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fast version\n\n# test_dataset = TestDatasetFast(CFG, sort_df)\n# test_loader = DataLoader(test_dataset,\n#                       batch_size=CFG.batch_size,\n#                       shuffle=False,\n#                       num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = ScoringModel(CFG, config_path=CFG.config_path, pretrained=False)\n#     state = torch.load(CFG.path+f\"{CFG.model_name.replace('/', '-')}_fold{fold}_best.pth\",\n#                        map_location=torch.device('cpu'))\n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn_fast(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n#     ## data re-sort ## \n#     prediction = prediction[np.argsort(length_sorted_idx)]\n\n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs; gc.collect()\n#     torch.cuda.empty_cache()\n# predictions_v3_l_5f = np.mean(predictions, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deberta large","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    path=\"../input/debertalarge/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-large\"\n    batch_size=32\n    fc_dropout=0.2\n    max_len=466\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn_fast(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n    # for inputs in test_loader:\n        bs = len(inputs['input_ids'])\n        pred_w_pad = np.zeros((bs, CFG.max_len, 1))\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        y_preds = y_preds.sigmoid().to('cpu').numpy()\n        pred_w_pad[:, :y_preds.shape[1]] = y_preds\n        preds.append(pred_w_pad)\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Original version\n\n# test_dataset = TestDataset(CFG, test)\n# test_loader = DataLoader(test_dataset,\n#                          batch_size=CFG.batch_size,\n#                          shuffle=False,\n#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n#     state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n#                        map_location=torch.device('cpu'))\n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs; gc.collect()\n#     torch.cuda.empty_cache()\n# predictions_v1_l = np.mean(predictions, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fast version\n\n# test_dataset = TestDataset(CFG, test)\n# test_loader = DataLoader(test_dataset,\n#                          batch_size=CFG.batch_size,\n#                          shuffle=False,\n#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n#     state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n#                        map_location=torch.device('cpu'))\n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn_fast(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n#     ## data re-sort ## \n#     prediction = prediction[np.argsort(length_sorted_idx)]\n\n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs; gc.collect()\n#     torch.cuda.empty_cache()\n# predictions_v1_l = np.mean(predictions, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deberta base","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\n# class CFG:\n#     num_workers=4\n#     path=\"../input/nbme-deberta-base-baseline-train/\"\n#     config_path=path+'config.pth'\n#     model=\"microsoft/deberta-base\"\n#     batch_size=24\n#     fc_dropout=0.2\n#     max_len=466\n#     seed=42\n#     n_fold=5\n#     trn_fold=[0, 1, 2, 3, 4]\n\n\nclass CFG:\n    num_workers=4\n    path=\"../input/deberta-v3-pseudo-labeling/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    model_name='deberta-v3-large'\n    batch_size=32\n    fc_dropout=0.2\n    max_len=354\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\n# CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ntokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\nCFG.tokenizer = tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Original version\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n#     state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n#                        map_location=torch.device('cpu'))\n    state = torch.load(CFG.path+f\"microsoft-deberta-v3-large_fold{fold}_best.pth\",\n                           map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_v1_b = np.mean(predictions, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fast version\n\n# test_dataset = TestDatasetFast(CFG, sort_df)\n# test_loader = DataLoader(test_dataset,\n#                       batch_size=CFG.batch_size,\n#                       shuffle=False,\n#                       num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n#     state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n#                        map_location=torch.device('cpu'))\n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn_fast(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n#     ## data re-sort ## \n#     prediction = prediction[np.argsort(length_sorted_idx)]\n\n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs; gc.collect()\n#     torch.cuda.empty_cache()\n# predictions_v1_b = np.mean(predictions, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combine","metadata":{}},{"cell_type":"code","source":"predictions = []\n# predictions_v3_l, predictions_v3_l_5f, predictions_v1_l, predictions_v1_b\nfor p1, p2, p3 in zip(predictions_v3_l, predictions_v3_l_5f, predictions_v1_b):\n    predictions.append(w1*p1 + w2*p2 + w3*p3)\n# for p1, p2, p3, p4 in zip(predictions_v3_l, predictions_v3_l_5f, predictions_v1_l, predictions_v1_b):\n#     predictions.append(w1*p1 + w2*p2 + w3*p3 + w4*p4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions = []\n# my_w1 = 0.5\n# my_w2 = 0.5\n# for p1, p2 in zip(predictions_v3_l, predictions_v3_l_5f):\n#     predictions.append(my_w1 * p1 + my_w2 * p2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # WBF example notebook: <https://www.kaggle.com/code/maximegatineau/nbme-post-processing-and-ensemble-tools/notebook>\n# results1 = get_results(predictions_v3_l)\n# results2 = get_results(predictions)\n# postprocess_base_value = list(map(_postprocess_location, results1))\n# postprocess_other_value = list(map(_postprocess_location, results2))\n# list_wbf = list(map(_custom_weighted_box_fusion, postprocess_base_value, postprocess_other_value))\n\n# # submission_df = pd.DataFrame({'id': test['id'].values, 'final_location': list_wbf})\n# submission_df = pd.DataFrame({'id': submission['id'].values, 'location': list_wbf})\n# submission_df.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = get_results(predictions)\nsubmission['location'] = results\ndisplay(submission.head())\nsubmission[['id', 'location']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}