{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport re\nimport copy\nimport pickle\nimport random\nimport torch \nimport torch.nn as nn\nimport tokenizers\nimport transformers\nimport gc\nfrom sklearn import model_selection\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, get_linear_schedule_with_warmup, AutoModelForSequenceClassification\nfrom torch.optim import AdamW\nfrom torch.nn import Sigmoid\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom IPython.display import display","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class args:\n    MAX_LEN = 300\n    MODEL_PATH = \"../input/bertbaseuncased/\"\n    MODEL_SAVE_PATH = \"../input/nbms-dataset/model.bin\"\n    tokenizer = tokenizers.BertWordPieceTokenizer(\"../input/bertbaseuncased/vocab.txt\",lowercase=True)\n    seed = 42\n    device=\"cuda\"\n    TEST_BATCH_SIZE = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=args.seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_pn_history(text):\n    text = re.sub(r\"\\r\\n\",\" \",text)\n    return text\n\n\ndef process_feature_text(text):\n    text = re.sub(r\"-\",\" \",text)\n    return text\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NBMEDataset:\n    def __init__(self,df,tokenizer):\n        self.df = df\n        self.pn_history = df.pn_history.values\n        self.feature_text = df.feature_text.values\n        self.max_len = args.MAX_LEN\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self,item):\n        text = self.pn_history[item]\n        feature_text = self.feature_text[item]\n        \n        \n        tok_text = self.tokenizer.encode(text,feature_text)\n        tok_text_tokens = tok_text.tokens\n        tok_text_ids = tok_text.ids\n        tok_text_type_ids = tok_text.type_ids\n        tok_text_offsets = tok_text.offsets[1:-1]\n        \n        \n        mask = [1] * len(tok_text_ids)\n        token_type_ids = tok_text_type_ids\n        padding_len = self.max_len - len(tok_text_ids)\n        \n        ids = tok_text_ids + [0] * padding_len\n        mask = mask + [0] * padding_len\n        token_type_ids = token_type_ids + [0] * padding_len\n        offsets = tok_text.offsets + [(0,0)] * padding_len\n        \n        \n        return {\n            \"ids\":torch.tensor(ids,dtype=torch.long),\n            \"mask\":torch.tensor(mask,dtype=torch.long),\n            \"token_type_ids\":torch.tensor(token_type_ids,dtype=torch.long),\n            \"offsets\":str(offsets)\n            \n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(args.MODEL_PATH)\n        self.bert_drop = nn.Dropout(0.3)\n        self.l0 = nn.Linear(768,1)\n        \n    def forward(self, ids, mask, token_type_ids):\n        output = self.bert(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids,\n        )\n        \n        \n        logits = self.l0(output[0])\n        logits = logits.squeeze(-1)\n        \n        \n        return logits","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_fn(dataloader, model, device):\n    model.eval()\n    tk0 = tqdm(dataloader, total=len(dataloader))\n    fin_output = []\n    fin_offsets = []\n    fin_output_location = []\n    \n    for bi, d in enumerate(tk0):\n        #print(len(d))\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        offsets = d[\"offsets\"]\n        \n        \n        \n        for i in range(len(offsets)):\n            offsets[i] = eval(offsets[i])\n        fin_offsets.extend(offsets)\n        \n        \n        ids = ids.to(device)\n        token_type_ids = token_type_ids.to(device)\n        mask = mask.to(device)\n        \n        \n        o1 = model(\n            ids=ids,\n            token_type_ids=token_type_ids,\n            mask=mask\n        )\n        \n        \n        \n        ids = ids.cpu().detach().numpy()\n        token_type_ids = token_type_ids.cpu().detach().numpy()\n        mask = mask.cpu().detach().numpy()\n        del ids\n        del token_type_ids\n        del mask\n        \n        threshold = 0.5\n        \n        fin_output.append(torch.sigmoid(o1).cpu().detach().numpy())\n        del o1\n        \n        \n        \n    fin_offsets = np.array(fin_offsets)\n    \n    \n    fin_output = np.vstack(fin_output)\n    \n    \n    for i in range(len(fin_output)):\n        output = [1 if i>=threshold else 0 for i in fin_output[i]]\n        offset = fin_offsets[i]\n        output_location = []\n        start = -1\n        for j in range(len(output)):\n            if output[j]==1 and start==-1:\n                start = offset[j][0]\n            if output[j]==0 and start!=-1:\n                end = offset[j-1][-1]\n                output_location.extend([str(start) + \" \" + str(end)])\n                start=-1\n            \n        fin_output_location.append(output_location)\n    \n    \n    new_location = [i[0] if len(i)==1 else \";\".join(i) for i in fin_output_location]   \n    return np.array(new_location)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/test.csv\")\nfeature = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/features.csv\")\npn_notes = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/patient_notes.csv\")\nsample = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/sample_submission.csv\")\n\nfeature[\"feature_text\"] = feature[\"feature_text\"].apply(process_feature_text)\npn_notes[\"pn_history\"] = pn_notes[\"pn_history\"].apply(process_pn_history)\ntest_df = test_df.merge(feature,how=\"left\",on=[\"feature_num\",\"case_num\"])\ntest_df = test_df.merge(pn_notes,how=\"left\",on=[\"pn_num\",\"case_num\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BERTBaseUncased()\nmodel.to(args.device)\nmodel.load_state_dict(torch.load(args.MODEL_SAVE_PATH))\n\ntest_dataset = NBMEDataset(\n    df=test_df,\n    tokenizer=args.tokenizer\n)\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=args.TEST_BATCH_SIZE)\n\npred_location = eval_fn(test_dataloader, model, args.device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub = pd.DataFrame({\n    \"id\":sample.id.values,\n    \"location\":pred_location\n})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub.to_csv(\"submission.csv\",index=False)\ndisplay(sample_sub)","metadata":{},"execution_count":null,"outputs":[]}]}