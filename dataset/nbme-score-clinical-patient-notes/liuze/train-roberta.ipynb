{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I trained roberta-large model, got local f1 Score: 0.8680, but I use this model to submit got 0.807, what's your roberta local vs lb gap? is it that big? \n\n\nThanks\n\n- https://www.kaggle.com/code/yasufuminakama/nbme-deberta-base-baseline-train\n- https://www.kaggle.com/code/theoviel/roberta-strikes-back","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport re\nimport sys\nimport random\nimport ast\nimport numpy as np\nimport torch\nimport transformers\nimport torch.nn as nn\nimport transformers\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score\nimport itertools\nimport math\nfrom datetime import datetime\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\n%env TOKENIZERS_PARALLELISM=true","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-26T09:52:23.857958Z","iopub.execute_input":"2022-03-26T09:52:23.858215Z","iopub.status.idle":"2022-03-26T09:52:23.867891Z","shell.execute_reply.started":"2022-03-26T09:52:23.858186Z","shell.execute_reply":"2022-03-26T09:52:23.867144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    wandb = False\n    competition = 'NBME'\n    _wandb_kernel = 'nakama'\n    debug = False\n    apex = True\n    print_freq = 100\n    num_workers = 2\n    model = \"roberta-large\"\n    scheduler = 'cosine'  # ['linear', 'cosine']\n    batch_scheduler = True\n    num_cycles = 0.5\n    num_warmup_steps = 0\n    epochs = 5\n    encoder_lr = 2e-5\n    decoder_lr = 2e-5\n    min_lr = 1e-6\n    eps = 1e-6\n    betas = (0.9, 0.999)\n    batch_size = 4\n    fc_dropout = 0.2\n    max_len = 466\n    weight_decay = 0.01\n    gradient_accumulation_steps = 1\n    max_grad_norm = 1000\n    precompute_tokens = True\n    seed = 42\n    n_fold = 5\n    trn_fold = [0]\n    data_config = {\n        \"val_bs\": 16 if \"large\" in model else 32,\n        \"pad_token\": 1 if \"roberta\" in model else 0,\n    }\n    device = \"cuda\"\n    DATA_PATH = \"../input/nbme-score-clinical-patient-notes/\"\n    OUTPUT_DIR = './'\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n        \n        \nmodel_path = {\n    'roberta-base': '../input/roberta-base',\n    'roberta-large': '../input/robertalarge'\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-26T09:52:23.869441Z","iopub.execute_input":"2022-03-26T09:52:23.870097Z","iopub.status.idle":"2022-03-26T09:52:23.881801Z","shell.execute_reply.started":"2022-03-26T09:52:23.870056Z","shell.execute_reply":"2022-03-26T09:52:23.880886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef get_logger(filename):\n    import logging\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        handlers=[logging.StreamHandler(sys.stdout)],\n    )\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    #     handler1 = StreamHandler()\n    #     handler1.setFormatter(Formatter(\"%(message)s\"))\n    exp = datetime.now().strftime('%Y%m%d-%H%M%S')\n    handler2 = FileHandler(filename=f\"{filename}_{exp}.log\")\n    print('log file', f\"{filename}_{exp}.log\")\n    #     handler2.setFormatter(Formatter(\"%(message)s\"))\n    #     logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\n\n\ndef clean_data(train):\n    # incorrect annotation\n    train.loc[338,\n              'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n    train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n\n    train.loc[621,\n              'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n    train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n\n    train.loc[655, 'annotation'] = ast.literal_eval(\n        '[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n    train.loc[655, 'location'] = ast.literal_eval(\n        '[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n\n    train.loc[1262,\n              'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n    train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n\n    train.loc[1265, 'annotation'] = ast.literal_eval(\n        '[[\\'felt like he was going to \"pass out\"\\']]')\n    train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n\n    train.loc[1396,\n              'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n    train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n\n    train.loc[1591,\n              'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n    train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n\n    train.loc[1615, 'annotation'] = ast.literal_eval(\n        '[[\"diarrhea for last 2-3 days\"]]')\n    train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n\n    train.loc[1664,\n              'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n    train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n\n    train.loc[1714, 'annotation'] = ast.literal_eval(\n        '[[\"started about 8-10 hours ago\"]]')\n    train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n\n    train.loc[1929,\n              'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n    train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n\n    train.loc[2134, 'annotation'] = ast.literal_eval(\n        '[[\"last sexually active 9 months ago\"]]')\n    train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n\n    train.loc[2191, 'annotation'] = ast.literal_eval(\n        '[[\"right lower quadrant pain\"]]')\n    train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n\n    train.loc[2553,\n              'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n    train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n\n    train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n    train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n\n    train.loc[3858, 'annotation'] = ast.literal_eval(\n        '[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]'\n    )\n    train.loc[3858, 'location'] = ast.literal_eval(\n        '[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]'\n    )\n\n    train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n    train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n\n    train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n    train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n\n    train.loc[4782,\n              'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n    train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n\n    train.loc[4908,\n              'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n    train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n\n    train.loc[6016, 'annotation'] = ast.literal_eval(\n        '[[\"difficulty falling asleep\"]]')\n    train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n\n    train.loc[6192, 'annotation'] = ast.literal_eval(\n        '[[\"helps to take care of aging mother and in-laws\"]]')\n    train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n\n    train.loc[6380, 'annotation'] = ast.literal_eval(\n        '[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]'\n    )\n    train.loc[6380, 'location'] = ast.literal_eval(\n        '[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]'\n    )\n\n    train.loc[6562, 'annotation'] = ast.literal_eval(\n        '[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]'\n    )\n    train.loc[6562, 'location'] = ast.literal_eval(\n        '[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n\n    train.loc[6862, 'annotation'] = ast.literal_eval(\n        '[[\"stressor taking care of many sick family members\"]]')\n    train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n\n    train.loc[7022, 'annotation'] = ast.literal_eval(\n        '[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]'\n    )\n    train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n\n    train.loc[7422,\n              'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n    train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n\n    train.loc[8876,\n              'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n    train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n\n    train.loc[9027, 'annotation'] = ast.literal_eval(\n        '[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n    train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n\n    train.loc[9938, 'annotation'] = ast.literal_eval(\n        '[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]'\n    )\n    train.loc[9938, 'location'] = ast.literal_eval(\n        '[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n\n    train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n    train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n\n    train.loc[10513, 'annotation'] = ast.literal_eval(\n        '[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n    train.loc[10513,\n              'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n\n    train.loc[11551, 'annotation'] = ast.literal_eval(\n        '[[\"seeing her son knows are not real\"]]')\n    train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n\n    train.loc[11677, 'annotation'] = ast.literal_eval(\n        '[[\"saw him once in the kitchen after he died\"]]')\n    train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n\n    train.loc[12124, 'annotation'] = ast.literal_eval(\n        '[[\"tried Ambien but it didnt work\"]]')\n    train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n\n    train.loc[12279, 'annotation'] = ast.literal_eval(\n        '[[\"heard what she described as a party later than evening these things did not actually happen\"]]'\n    )\n    train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n\n    train.loc[12289, 'annotation'] = ast.literal_eval(\n        '[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]'\n    )\n    train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n\n    train.loc[13238, 'annotation'] = ast.literal_eval(\n        '[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n    train.loc[13238,\n              'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n\n    train.loc[13297, 'annotation'] = ast.literal_eval(\n        '[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]'\n    )\n    train.loc[13297, 'location'] = ast.literal_eval(\n        '[[\"182 221\"], [\"182 213;225 234\"]]')\n\n    train.loc[13299, 'annotation'] = ast.literal_eval(\n        '[[\"yesterday\"], [\"yesterday\"]]')\n    train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n\n    train.loc[13845, 'annotation'] = ast.literal_eval(\n        '[[\"headache global\"], [\"headache throughout her head\"]]')\n    train.loc[13845, 'location'] = ast.literal_eval(\n        '[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n\n    train.loc[14083, 'annotation'] = ast.literal_eval(\n        '[[\"headache generalized in her head\"]]')\n    train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n    train['annotation_length'] = train['annotation'].apply(len)\n    return train\n\n\ndef clean_feature(df):\n    df['pn_history'] = df['pn_history'].apply(lambda x: x.strip())\n    df['feature_text'] = df['feature_text'].apply(process_feature_text)\n\n    df['feature_text'] = df['feature_text'].apply(clean_spaces)\n    df['clean_text'] = df['pn_history'].apply(clean_spaces)\n    return df\n\ndef process_feature_text(text):\n    text = re.sub('I-year', '1-year', text)\n    text = re.sub('-OR-', \" or \", text)\n    text = re.sub('-', ' ', text)\n    return text\n\ndef clean_spaces(txt):\n    txt = re.sub('\\n', ' ', txt)\n    txt = re.sub('\\t', ' ', txt)\n    txt = re.sub('\\r', ' ', txt)\n    #     txt = re.sub(r'\\s+', ' ', txt)\n    return txt\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\ndef token_pred_to_char_pred(token_pred, offsets):\n    char_pred = np.zeros((np.max(offsets), 1))\n    for i in range(len(token_pred)):\n        s, e = int(offsets[i][0]), int(offsets[i][1])  # start, end\n        char_pred[s:e] = token_pred[i]\n    return char_pred\n\ndef post_process_spaces(target, text):\n    target = np.copy(target)\n\n    if len(text) > len(target):\n        padding = np.zeros(len(text) - len(target))\n        target = np.concatenate([target, padding])\n    else:\n        target = target[:len(text)]\n\n    if text[0] == \" \":\n        target[0] = 0\n    if text[-1] == \" \":\n        target[-1] = 0\n\n    for i in range(1, len(text) - 1):\n        if text[i] == \" \":\n            if target[i] and not target[i - 1]:  # space before\n                target[i] = 0\n\n            if target[i] and not target[i + 1]:  # space after\n                target[i] = 0\n\n            if target[i - 1] and target[i + 1]:\n                target[i] = 1\n\n    return target\n\n\ndef micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    return f1_score(truths, preds)\n\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n    return binary\n\n\ndef span_micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(\n            np.max(pred) if len(pred) else 0,\n            np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n    return micro_f1(bin_preds, bin_truths)\n\ndef get_score(y_true, y_pred):\n    score = span_micro_f1(y_true, y_pred)\n    return score\n\ndef char_target_to_span(char_target):\n    spans = []\n    start, end = 0, 0\n    for i in range(len(char_target)):\n        if char_target[i] == 1 and char_target[i - 1] == 0:\n            if end:\n                spans.append([start, end])\n            start = i\n            end = i + 1\n        elif char_target[i] == 1:\n            end = i + 1\n        else:\n            if end:\n                spans.append([start, end])\n            start, end = 0, 0\n    return spans","metadata":{"execution":{"iopub.status.busy":"2022-03-26T09:52:24.147851Z","iopub.execute_input":"2022-03-26T09:52:24.148159Z","iopub.status.idle":"2022-03-26T09:52:24.209692Z","shell.execute_reply.started":"2022-03-26T09:52:24.148125Z","shell.execute_reply":"2022-03-26T09:52:24.208847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## dataset","metadata":{}},{"cell_type":"code","source":"\ndef get_tokenizer(name, precompute=False, df=None, folder=None):\n    if folder is None:\n        tokenizer = AutoTokenizer.from_pretrained(model_path[name])\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(folder)\n\n    tokenizer.name = name\n    tokenizer.special_tokens = {\n        \"sep\": tokenizer.sep_token_id,\n        \"cls\": tokenizer.cls_token_id,\n        \"pad\": tokenizer.pad_token_id,\n    }\n\n    if precompute:\n        tokenizer.precomputed = precompute_tokens(df, tokenizer)\n    else:\n        tokenizer.precomputed = None\n\n    return tokenizer\n\n\ndef precompute_tokens(df, tokenizer):\n    feature_texts = df[\"feature_text\"].unique()\n\n    ids = {}\n    offsets = {}\n\n    for feature_text in feature_texts:\n        encoding = tokenizer(\n            feature_text,\n            return_token_type_ids=True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        ids[feature_text] = encoding[\"input_ids\"]\n        offsets[feature_text] = encoding[\"offset_mapping\"]\n\n    texts = df[\"clean_text\"].unique()\n\n    for text in texts:\n        encoding = tokenizer(\n            text,\n            return_token_type_ids=True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        ids[text] = encoding[\"input_ids\"]\n        offsets[text] = encoding[\"offset_mapping\"]\n\n    return {\"ids\": ids, \"offsets\": offsets}\n\n\ndef encodings_from_precomputed(feature_text, text, char_target, precomputed, tokenizer, max_len=300):\n    tokens = tokenizer.special_tokens\n\n    # Input ids\n    if \"roberta\" in tokenizer.name:\n        qa_sep = [tokens[\"sep\"], tokens[\"sep\"]]\n    else:\n        qa_sep = [tokens[\"sep\"]]\n\n    input_ids = [tokens[\"cls\"]] + precomputed[\"ids\"][feature_text] + qa_sep\n    n_question_tokens = len(input_ids)\n\n    input_ids += precomputed[\"ids\"][text]\n    input_ids = input_ids[: max_len - 1] + [tokens[\"sep\"]]\n\n    # Token type ids\n    if \"roberta\" not in tokenizer.name:\n        token_type_ids = np.ones(len(input_ids))\n        token_type_ids[:n_question_tokens] = 0\n        token_type_ids = token_type_ids.tolist()\n    else:\n        token_type_ids = [0] * len(input_ids)\n\n    # Offsets\n    offsets = [(0, 0)] * n_question_tokens + precomputed[\"offsets\"][text]\n    offsets = offsets[: max_len - 1] + [(0, 0)]\n\n    label = np.zeros(len(offsets))\n    label[:n_question_tokens] = -1\n    label[-1] = -1\n#         location_list = ['696 724']\n# [s.split() for s in location.split(';')]\n#     for location in char_target:\n    for loc in char_target:\n        start_idx = -1\n        end_idx = -1\n        start, end = int(loc[0]), int(loc[1])\n        for idx in range(len(offsets)):\n            if (start_idx == -1) & (start < offsets[idx][0]):\n                start_idx = idx - 1\n            if (end_idx == -1) & (end <= offsets[idx][1]):\n                end_idx = idx + 1\n        if start_idx == -1:\n            start_idx = end_idx\n        if (start_idx != -1) & (end_idx != -1):\n            label[start_idx:end_idx] = 1\n    # Padding\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([tokens[\"pad\"]] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        offsets = offsets + ([(0, 0)] * padding_length)\n        label = np.pad(label, (0, padding_length), constant_values=-1)\n\n    encoding = {\n        \"input_ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"offset_mapping\": offsets,\n        \"label\": label,\n    }\n\n    return encoding\n\nclass PatientNoteDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n\n        self.texts = df['clean_text'].values\n        self.feature_text = df['feature_text'].values\n        self.char_targets = df['target'].values.tolist()\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        feature_text = self.feature_text[idx]\n        char_target = self.char_targets[idx]\n\n        # Tokenize\n        if self.tokenizer.precomputed is None:\n            encoding = self.tokenizer(\n                feature_text,\n                text,\n                return_token_type_ids=True,\n                return_offsets_mapping=True,\n                return_attention_mask=False,\n                truncation=\"only_second\",\n                max_length=self.max_len,\n                padding='max_length',\n            )\n            raise NotImplementedError(\"fix issues with question offsets\")\n        else:\n            encoding = encodings_from_precomputed(\n                feature_text,\n                text,\n                char_target,\n                self.tokenizer.precomputed,\n                self.tokenizer,\n                max_len=self.max_len\n            )\n        return {\n            \"ids\": torch.tensor(encoding[\"input_ids\"], dtype=torch.long),\n            \"token_type_ids\": torch.tensor(encoding[\"token_type_ids\"], dtype=torch.long),\n            \"target\": torch.tensor(encoding['label'], dtype=torch.float),\n            \"offsets\": np.array(encoding[\"offset_mapping\"]),\n            \"text\": text,\n        }\n\n    def __len__(self):\n        return len(self.texts)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T09:52:24.456461Z","iopub.execute_input":"2022-03-26T09:52:24.456729Z","iopub.status.idle":"2022-03-26T09:52:24.483925Z","shell.execute_reply.started":"2022-03-26T09:52:24.456698Z","shell.execute_reply":"2022-03-26T09:52:24.483085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.optim import Adam, SGD, AdamW\nimport gc\nimport time\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T09:52:24.487098Z","iopub.execute_input":"2022-03-26T09:52:24.487289Z","iopub.status.idle":"2022-03-26T09:52:24.497867Z","shell.execute_reply.started":"2022-03-26T09:52:24.487266Z","shell.execute_reply":"2022-03-26T09:52:24.496923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler,\n             device, CFG):\n    model.train()\n    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n    losses = AverageMeter()\n    start = end = time.time()\n    global_step = 0\n    #     dict_keys(['ids', 'token_type_ids', 'target', 'offsets', 'text'])\n\n    for step, data in enumerate(train_loader):\n        labels = data['target'].to(device)\n        batch_size = labels.size(0)\n        with torch.cuda.amp.autocast(enabled=CFG.apex):\n            outputs = model(data['ids'].to(device),\n                            data['token_type_ids'].to(device))\n            # loss = outputs['loss']\n            y_preds = outputs['logits']\n        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        scaler.scale(loss).backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),\n                                                   CFG.max_grad_norm)\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            global_step += 1\n            if CFG.batch_scheduler:\n                scheduler.step()\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f}  '\n                  'LR: {lr:.8f}  '.format(\n                      epoch + 1,\n                      step,\n                      len(train_loader),\n                      remain=timeSince(start,\n                                       float(step + 1) / len(train_loader)),\n                      loss=losses,\n                      grad_norm=grad_norm,\n                      lr=scheduler.get_lr()[0]))\n        if CFG.wandb:\n            wandb.log({\n                f\"[fold{fold}] loss\": losses.val,\n                f\"[fold{fold}] lr\": scheduler.get_lr()[0]\n            })\n    return losses.avg\n\n\ndef valid_fn(valid_loader, model, criterion, device, CFG):\n    losses = AverageMeter()\n    model.eval()\n    preds = []\n    start = end = time.time()\n    for step, data in enumerate(valid_loader):\n        labels = data['target'].to(device)\n        batch_size = labels.size(0)\n        with torch.no_grad():\n            outputs = model(data['ids'].to(device),\n                            data['token_type_ids'].to(device))\n            # loss = outputs['loss']\n            y_preds = outputs['logits']\n        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n            print('EVAL: [{0}/{1}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '.format(\n                      step,\n                      len(valid_loader),\n                      loss=losses,\n                      remain=timeSince(start,\n                                       float(step + 1) / len(valid_loader))))\n    predictions = np.concatenate(preds)\n    return losses.avg, predictions\n\n\ndef train_loop(folds, fold, tokenizer, CFG, LOGGER):\n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n    if CFG.debug:\n        LOGGER.info('debug mode')\n        train_folds = train_folds.head(100)\n        valid_folds = valid_folds.head(100)\n#     valid_texts = valid_folds['pn_history'].values\n#     valid_labels = create_labels_for_scoring(valid_folds)\n\n    #     train_dataset = TrainDataset(CFG, train_folds)\n    #     valid_dataset = TrainDataset(CFG, valid_folds)\n    train_dataset = PatientNoteDataset(\n        train_folds,\n        tokenizer,\n        max_len=CFG.max_len,\n    )\n    valid_dataset = PatientNoteDataset(\n        valid_folds,\n        tokenizer,\n        max_len=CFG.max_len,\n    )\n    train_loader = DataLoader(train_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=True,\n                              num_workers=CFG.num_workers,\n                              pin_memory=True,\n                              drop_last=True)\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=False,\n                              num_workers=CFG.num_workers,\n                              pin_memory=True,\n                              drop_last=False)\n    offsets = [x['offsets'] for x in valid_dataset]\n    offsets = np.array(offsets)\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    model = NERTransformer(CFG.model)\n    #     torch.save(model.config, OUTPUT_DIR+'config.pth')\n    model.to(CFG.device)\n\n    # # logits  transformer\n    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [{\n            'params': [\n                p for n, p in model.transformer.named_parameters()\n                if not any(nd in n for nd in no_decay)\n            ],\n            'lr':\n            encoder_lr,\n            'weight_decay':\n            weight_decay\n        }, {\n            'params': [\n                p for n, p in model.transformer.named_parameters()\n                if any(nd in n for nd in no_decay)\n            ],\n            'lr':\n            encoder_lr,\n            'weight_decay':\n            0.0\n        }, {\n            'params':\n            [p for n, p in model.named_parameters() if \"transformer\" not in n],\n            'lr':\n            decoder_lr,\n            'weight_decay':\n            0.0\n        }]\n        return optimizer_parameters\n\n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=CFG.encoder_lr,\n                                                decoder_lr=CFG.decoder_lr,\n                                                weight_decay=CFG.weight_decay)\n    optimizer = AdamW(optimizer_parameters,\n                      lr=CFG.encoder_lr,\n                      eps=CFG.eps,\n                      betas=CFG.betas)\n\n    # ====================================================\n    # scheduler\n    # ====================================================\n    def get_scheduler(cfg, optimizer, num_train_steps):\n        if cfg.scheduler == 'linear':\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer,\n                num_warmup_steps=cfg.num_warmup_steps,\n                num_training_steps=num_train_steps)\n        elif cfg.scheduler == 'cosine':\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer,\n                num_warmup_steps=cfg.num_warmup_steps,\n                num_training_steps=num_train_steps,\n                num_cycles=cfg.num_cycles)\n        return scheduler\n\n    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n\n    best_score = -1\n    df_test = valid_folds.copy()\n    for epoch in range(CFG.epochs):\n\n        start_time = time.time()\n\n        # train\n        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer,\n                            epoch, scheduler, CFG.device, CFG)\n#         torch.save(model.state_dict(), CFG.OUTPUT_DIR + f'model_{epoch}.pth')\n#         LOGGER.info(f'save model model_{epoch}.pth')\n        # eval\n        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion,\n                                             CFG.device, CFG)\n        predictions = predictions.reshape((len(valid_folds), CFG.max_len))\n\n        preds = [\n            token_pred_to_char_pred(y, ofs)\n            for y, ofs in zip(predictions, offsets)\n        ]\n\n        df_test['preds'] = preds\n        df_test['preds'] = df_test.apply(\n            lambda x: x['preds'][:len(x['clean_text'])], 1)\n        df_test['preds'] = df_test['preds'].apply(lambda x:\n                                                  (x > 0.5).flatten())\n        # df_test['span'] = df_test['preds'].apply(char_target_to_span)\n        df_test['preds_pp'] = df_test.apply(\n            lambda x: post_process_spaces(x['preds'], x['clean_text']), 1)\n        df_test['span'] = df_test['preds_pp'].apply(char_target_to_span)\n        score = get_score(df_test['target'], df_test['span'])\n        \n        elapsed = time.time() - start_time\n\n        LOGGER.info(\n            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s'\n        )\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n\n        if best_score < score:\n            best_score = score\n            LOGGER.info(\n                f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save(\n                {\n                    'model': model.state_dict(),\n                    'predictions': predictions\n                }, CFG.OUTPUT_DIR +\n                f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n\n    predictions = torch.load(\n        CFG.OUTPUT_DIR + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n        map_location=torch.device('cpu'))['predictions']\n    valid_folds[[i for i in range(CFG.max_len)]] = predictions\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    return valid_folds\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T09:52:24.592904Z","iopub.execute_input":"2022-03-26T09:52:24.595283Z","iopub.status.idle":"2022-03-26T09:52:24.664211Z","shell.execute_reply.started":"2022-03-26T09:52:24.59517Z","shell.execute_reply":"2022-03-26T09:52:24.663393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model","metadata":{}},{"cell_type":"code","source":"class NERTransformer(nn.Module):\n    def __init__(\n        self,\n        model,\n        num_classes=1,\n        config_file=None,\n        pretrained=True,\n    ):\n        super().__init__()\n        self.name = model\n        self.pad_idx = 1 if \"roberta\" in self.name else 0\n\n        transformers.logging.set_verbosity_error()\n\n        if config_file is None:\n            config = AutoConfig.from_pretrained(model_path[model], output_hidden_states=True)\n        else:\n            config = torch.load(config_file)\n\n        if pretrained:\n            self.transformer = AutoModel.from_pretrained(model_path[model], config=config)\n        else:\n            self.transformer = AutoModel.from_config(config)\n\n        self.nb_features = config.hidden_size\n\n#         self.cnn = nn.Identity()\n        self.dropout = nn.Dropout(0.1)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        self.output = nn.Linear(self.nb_features, num_classes)\n\n    def loss(self, outputs, targets, attention_mask):\n        criterion = nn.BCEWithLogitsLoss(reduction='none')\n        loss = criterion(outputs.view(-1, 1), targets.view(-1, 1))\n        loss = torch.masked_select(loss, targets.view(-1, 1) != -1).mean()\n\n#         active_loss = attention_mask.view(-1) == 1\n#         active_logits = outputs.view(-1, self.num_labels)\n#         true_labels = targets.view(-1)\n#         outputs = active_logits.argmax(dim=-1)\n#         idxs = np.where(active_loss.cpu().numpy() == 1)[0]\n#         active_logits = active_logits[idxs]\n#         true_labels = true_labels[idxs].to(torch.long)\n#         loss = loss_fct(active_logits, true_labels)\n        return loss\n\n    def forward(self, tokens, token_type_ids, targets=None):\n        \"\"\"\n        Usual torch forward function\n\n        Arguments:\n            tokens {torch tensor} -- Sentence tokens\n            token_type_ids {torch tensor} -- Sentence tokens ids\n        \"\"\"\n        mask = (tokens != self.pad_idx).long()\n        sequence_output = self.transformer(\n            tokens,\n            attention_mask=mask,\n            token_type_ids=token_type_ids,\n        ).last_hidden_state\n\n#         features = hidden_states[-1]\n#         logits = self.logits(self.dropout(features))\n        logits = self.output(self.dropout(sequence_output))\n        # logits1 = self.output(self.dropout1(sequence_output))\n        # logits2 = self.output(self.dropout2(sequence_output))\n        # logits3 = self.output(self.dropout3(sequence_output))\n        # logits4 = self.output(self.dropout4(sequence_output))\n        # logits5 = self.output(self.dropout5(sequence_output))\n\n        # logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n#         logits = torch.sigmoid(logits)\n        loss = 0\n        output = {\"logits\": logits}\n        if targets is not None:\n            loss = self.loss(logits, targets, mask)\n            # loss1 = self.loss(logits1, targets, attention_mask=mask)\n            # loss2 = self.loss(logits2, targets, attention_mask=mask)\n            # loss3 = self.loss(logits3, targets, attention_mask=mask)\n            # loss4 = self.loss(logits4, targets, attention_mask=mask)\n            # loss5 = self.loss(logits5, targets, attention_mask=mask)\n            # loss = (loss1 + loss2 + loss3 + loss4 + loss5) / 5\n            output[\"loss\"] = loss\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-26T09:52:25.056994Z","iopub.execute_input":"2022-03-26T09:52:25.057835Z","iopub.status.idle":"2022-03-26T09:52:25.081218Z","shell.execute_reply.started":"2022-03-26T09:52:25.057772Z","shell.execute_reply":"2022-03-26T09:52:25.080266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOGGER = get_logger(CFG.OUTPUT_DIR+'train')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T09:52:25.126976Z","iopub.execute_input":"2022-03-26T09:52:25.127867Z","iopub.status.idle":"2022-03-26T09:52:25.136295Z","shell.execute_reply.started":"2022-03-26T09:52:25.127829Z","shell.execute_reply":"2022-03-26T09:52:25.135479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(42)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T09:52:25.141002Z","iopub.execute_input":"2022-03-26T09:52:25.141768Z","iopub.status.idle":"2022-03-26T09:52:25.150972Z","shell.execute_reply.started":"2022-03-26T09:52:25.141731Z","shell.execute_reply":"2022-03-26T09:52:25.149737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(CFG.DATA_PATH + 'train.csv')\ntrain['annotation'] = train['annotation'].apply(ast.literal_eval)\ntrain['location'] = train['location'].apply(ast.literal_eval)\nfeatures = pd.read_csv(CFG.DATA_PATH + 'features.csv')\nfeatures = preprocess_features(features)\npatient_notes = pd.read_csv(CFG.DATA_PATH + 'patient_notes.csv')\n\nprint(f\"train.shape: {train.shape}\")\nprint(f\"features.shape: {features.shape}\")\nprint(f\"patient_notes.shape: {patient_notes.shape}\")\n\ntrain = train.merge(features, on=['feature_num', 'case_num'], how='left')\ntrain = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n\ntrain = clean_data(train)\ntrain = clean_feature(train)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T09:52:25.152702Z","iopub.execute_input":"2022-03-26T09:52:25.153579Z","iopub.status.idle":"2022-03-26T09:52:26.141385Z","shell.execute_reply.started":"2022-03-26T09:52:25.153542Z","shell.execute_reply":"2022-03-26T09:52:26.140608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T09:52:26.142508Z","iopub.execute_input":"2022-03-26T09:52:26.143382Z","iopub.status.idle":"2022-03-26T09:52:26.163657Z","shell.execute_reply.started":"2022-03-26T09:52:26.143341Z","shell.execute_reply":"2022-03-26T09:52:26.163033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ret = []\nfor x in train['location']:\n    if x:\n        ret.append([(int(s.split()[0]), int(s.split()[1]))\n                    for s in x[0].split(';')])\n    else:\n        ret.append([])\ntrain['target'] = ret","metadata":{"execution":{"iopub.status.busy":"2022-03-26T09:52:26.165716Z","iopub.execute_input":"2022-03-26T09:52:26.165992Z","iopub.status.idle":"2022-03-26T09:52:26.201831Z","shell.execute_reply.started":"2022-03-26T09:52:26.165957Z","shell.execute_reply":"2022-03-26T09:52:26.201045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = get_tokenizer(CFG.model,\n                          precompute=CFG.precompute_tokens,\n                          df=train)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T09:52:26.203958Z","iopub.execute_input":"2022-03-26T09:52:26.20457Z","iopub.status.idle":"2022-03-26T09:52:43.372115Z","shell.execute_reply.started":"2022-03-26T09:52:26.204532Z","shell.execute_reply":"2022-03-26T09:52:43.371277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Fold = GroupKFold(n_splits=5)\ngroups = train['pn_num'].values\nfor n, (train_index, val_index) in enumerate(\n        Fold.split(train, train['location'], groups)):\n    train.loc[val_index, 'fold'] = int(n)\ntrain['fold'] = train['fold'].astype(int)\ntrain.groupby('fold').size()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T09:52:43.373482Z","iopub.execute_input":"2022-03-26T09:52:43.374344Z","iopub.status.idle":"2022-03-26T09:52:43.398398Z","shell.execute_reply.started":"2022-03-26T09:52:43.3743Z","shell.execute_reply":"2022-03-26T09:52:43.397697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_oof_df = train_loop(train, 0, tokenizer, CFG, LOGGER)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T09:52:43.399518Z","iopub.execute_input":"2022-03-26T09:52:43.400229Z","iopub.status.idle":"2022-03-26T09:53:28.453302Z","shell.execute_reply.started":"2022-03-26T09:52:43.400189Z","shell.execute_reply":"2022-03-26T09:53:28.451857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}