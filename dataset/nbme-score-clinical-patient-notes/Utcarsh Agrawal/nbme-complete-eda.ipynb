{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install stylecloud","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-15T05:39:19.093047Z","iopub.execute_input":"2022-03-15T05:39:19.093685Z","iopub.status.idle":"2022-03-15T05:39:38.13659Z","shell.execute_reply.started":"2022-03-15T05:39:19.093534Z","shell.execute_reply":"2022-03-15T05:39:38.135365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:#73d2de;font-family:newtimeroman;color:#00509d;font-size:150%;text-align:center;border-radius:40px 40px;\">NBME SCORE CLINICAL PATIENT NOTES</p>","metadata":{}},{"cell_type":"markdown","source":"<h1 align='center'>Introduction üìù</h1>\nThe goal of this competition is to identify the relevant features within each patient note, with a special focus on the patient history portions of the notes. I will deep dive into the dataset to understand and get all the insights from the data.","metadata":{}},{"cell_type":"markdown","source":"<h1 align='center'>Dataset Info üìà</h1>\n<h2>Training Data</h2>\n<b>patient_notes.csv - A collection of about 40,000 Patient Note history portions.</b><br>\n\n* ```pn_num``` - A unique identifier for each patient note.\n* ```case_num``` - A unique identifier for the clinical case a patient note represents.\n* ```pn_history``` - The text of the encounter as recorded by the test taker.\n\n<b>features.csv - The rubric of features (or key concepts) for each clinical case.</b><br>\n* ```feature_num``` - A unique identifier for each feature.\n* ```case_num``` - A unique identifier for each case.\n* ```feature_text``` - A description of the feature.\n\n<b>train.csv - Feature annotations for 1000 of the patient notes, 100 for each of ten cases.</b><br>\n* ```id``` - Unique identifier for each patient note / feature pair.\n* ```pn_num``` - The patient note annotated in this row.\n* ```feature_num``` - The feature annotated in this row.\n* ```case_num``` - The case to which this patient note belongs.\n* ```annotation``` - The text(s) within a patient note indicating a feature. A feature may be indicated multiple times within a single note.\n* ```location``` - Character spans indicating the location of each annotation within the note. Multiple spans may be needed to represent an annotation, in which case the spans are delimited by a semicolon ;.","metadata":{}},{"cell_type":"markdown","source":"<h1 align='center'>Evaluation Metric üìê</h1>\nThe competition is evaluated by a micro-averaged F1 score.\n\n<img src=\"https://user-images.githubusercontent.com/55939250/153265944-04388967-90b3-4fb8-84ce-04c538bcd550.png\" width=700px height=400px>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">\n    <h2 align='center'>Please do an upvote if you found the kernel useful.</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:#73d2de;font-family:newtimeroman;color:#00509d;font-size:150%;text-align:center;border-radius:40px 40px;\">TABLE OF CONTENTS</p>\n<ul style=\"list-style-type:square\">\n    <li><a href=\"#1\">Importing Libraries</a></li>\n    <li><a href=\"#2\">Reading the data</a></li>\n    <li><a href=\"#3\">Explore</a></li>\n    <ul style=\"list-style-type:disc\">\n        <li><a href=\"#3.1\">Train Data</a></li>\n        <li><a href=\"#3.2\">Features Data</a></li>\n        <li><a href=\"#3.3\">Patient Notes Data</a></li>\n    </ul>\n</ul>\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id='1'></a>\n# <p style=\"background-color:#73d2de;font-family:newtimeroman;color:#00509d;font-size:150%;text-align:center;border-radius:40px 40px;\">IMPORTING LIBRARIES</p>","metadata":{}},{"cell_type":"code","source":"import gc\nimport ast\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport spacy\nfrom spacy import displacy\nimport stylecloud\nfrom IPython.display import Image\nfrom nltk.corpus import stopwords\nfrom collections import Counter, defaultdict\n\nimport warnings\nwarnings.simplefilter('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:39:38.140295Z","iopub.execute_input":"2022-03-15T05:39:38.140748Z","iopub.status.idle":"2022-03-15T05:39:51.942782Z","shell.execute_reply.started":"2022-03-15T05:39:38.14069Z","shell.execute_reply":"2022-03-15T05:39:51.941761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2'></a>\n# <p style=\"background-color:#73d2de;font-family:newtimeroman;color:#00509d;font-size:150%;text-align:center;border-radius:40px 40px;\">READING THE DATA</p>","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:39:51.944549Z","iopub.execute_input":"2022-03-15T05:39:51.944865Z","iopub.status.idle":"2022-03-15T05:39:52.020381Z","shell.execute_reply.started":"2022-03-15T05:39:51.944828Z","shell.execute_reply":"2022-03-15T05:39:52.01879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:39:52.022599Z","iopub.execute_input":"2022-03-15T05:39:52.022968Z","iopub.status.idle":"2022-03-15T05:39:52.068056Z","shell.execute_reply.started":"2022-03-15T05:39:52.02292Z","shell.execute_reply":"2022-03-15T05:39:52.066734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\nfeature.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:39:52.071537Z","iopub.execute_input":"2022-03-15T05:39:52.072661Z","iopub.status.idle":"2022-03-15T05:39:52.095828Z","shell.execute_reply.started":"2022-03-15T05:39:52.07259Z","shell.execute_reply":"2022-03-15T05:39:52.09492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:39:52.097201Z","iopub.execute_input":"2022-03-15T05:39:52.098213Z","iopub.status.idle":"2022-03-15T05:39:52.111799Z","shell.execute_reply.started":"2022-03-15T05:39:52.098159Z","shell.execute_reply":"2022-03-15T05:39:52.110824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_note = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\npatient_note.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:39:52.11296Z","iopub.execute_input":"2022-03-15T05:39:52.113769Z","iopub.status.idle":"2022-03-15T05:39:52.872726Z","shell.execute_reply.started":"2022-03-15T05:39:52.113728Z","shell.execute_reply":"2022-03-15T05:39:52.871598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_note.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:39:52.873906Z","iopub.execute_input":"2022-03-15T05:39:52.874108Z","iopub.status.idle":"2022-03-15T05:39:52.896911Z","shell.execute_reply.started":"2022-03-15T05:39:52.874082Z","shell.execute_reply":"2022-03-15T05:39:52.896082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3'></a>\n# <p style=\"background-color:#73d2de;font-family:newtimeroman;color:#00509d;font-size:150%;text-align:center;border-radius:40px 40px;\">EXPLORE</p>","metadata":{}},{"cell_type":"markdown","source":"<a id='3.1'></a>\n# Train Data","metadata":{}},{"cell_type":"markdown","source":"### We will start by looking the distribution of case_num.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 9))\n\nsns.countplot(x='case_num', data=train, palette = 'flare')\nplt.title('Distribution of Case_Num in Training Data', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:39:52.898941Z","iopub.execute_input":"2022-03-15T05:39:52.899199Z","iopub.status.idle":"2022-03-15T05:39:53.226951Z","shell.execute_reply.started":"2022-03-15T05:39:52.899162Z","shell.execute_reply":"2022-03-15T05:39:53.225731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Then let's look the distribution of pn_num.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 6))\n\nsns.histplot(x='pn_num', data=train, hue='case_num', bins=50, palette='rainbow')\nplt.title('Distribution of Pn_Num in Training Data', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:39:53.228566Z","iopub.execute_input":"2022-03-15T05:39:53.228924Z","iopub.status.idle":"2022-03-15T05:39:55.31075Z","shell.execute_reply.started":"2022-03-15T05:39:53.228877Z","shell.execute_reply":"2022-03-15T05:39:55.309466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### After that let's analyse the annotations. For this, first of all we will focus at the number of features in the annotations and then we will consider the most common words in the annotations.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(20, 9))\nfig.suptitle('Distribution of Number of Annotations', size=15)\ntrain['annot_features'] = train['annotation'].apply(lambda x : len(ast.literal_eval(x))) \nsns.countplot(x=train['annot_features'], palette='crest', ax=ax[0])\n\nsizes = []\nno_annotations = len(train[train['annot_features']==0])\nsizes.append(no_annotations)\nannotated = len(train) - len(train[train['annot_features']==0])\nsizes.append(annotated)\n\nprint('Number of Rows with no Annotations -', no_annotations)\nprint('Number of Rows with Annotations -', annotated)\n\nlabels = ['Annotation', 'No Annotation']\ncolors = ['#72CC50', '#54C2CC']\nax[1].pie(sizes, colors=colors, startangle=90, labels=labels,\n        autopct='%1.0f%%', pctdistance=0.7,textprops={'fontsize':12}, counterclock=False)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:39:55.312582Z","iopub.execute_input":"2022-03-15T05:39:55.313052Z","iopub.status.idle":"2022-03-15T05:39:55.832524Z","shell.execute_reply.started":"2022-03-15T05:39:55.31301Z","shell.execute_reply":"2022-03-15T05:39:55.831405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def join_fea(annotation):\n    text = [word for words in ast.literal_eval(annotation) for word in words.split()]\n    return text\n\ntrain['text'] = train['annotation'].apply(lambda x : join_fea(x))\n\ntop = Counter([word for words in train['text'] for word in words])\ndf_temp = pd.DataFrame(top.most_common(25))\ndf_temp.columns = ['Common_words','count']\n\nfig = px.bar(df_temp, x='count', y='Common_words', title='Most Common Words(including stopwords) in Annotations', orientation='h', width=900,height=700, color='Common_words')\nfig.show()\n\ndef join_fea(annotation):\n    text = [word for words in ast.literal_eval(annotation) for word in words.split() if word not in set(stopwords.words('english'))]\n    return text\n\ntrain['text'] = train['annotation'].apply(lambda x : join_fea(x))\n\ntop = Counter([word for words in train['text'] for word in words])\ndf_temp = pd.DataFrame(top.most_common(25))\ndf_temp.columns = ['Common_words','count']\n\nfig = px.bar(df_temp, x='count', y='Common_words', title='Most Common Words(excluding stopwords) in Annotations', orientation='h', width=900,height=700, color='Common_words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:39:55.834257Z","iopub.execute_input":"2022-03-15T05:39:55.834536Z","iopub.status.idle":"2022-03-15T05:40:02.631248Z","shell.execute_reply.started":"2022-03-15T05:39:55.834502Z","shell.execute_reply":"2022-03-15T05:40:02.630418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features Data","metadata":{}},{"cell_type":"markdown","source":"### Now we will analyse the features data. We will start by looking the distribution of case_num.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 9))\n\nsns.countplot(x='case_num', data=feature, palette = 'Purples_r')\nplt.title('Distribution of Case_Num in Features Data', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:40:02.63302Z","iopub.execute_input":"2022-03-15T05:40:02.633511Z","iopub.status.idle":"2022-03-15T05:40:02.935265Z","shell.execute_reply.started":"2022-03-15T05:40:02.633462Z","shell.execute_reply":"2022-03-15T05:40:02.934021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Then we will inspect some feature_text properties like number of words in it and the average word length distributions.","metadata":{}},{"cell_type":"code","source":"text_len = feature['feature_text'].str.split('-').map(lambda x : len(x))\nfig = ff.create_distplot([text_len], ['feature'], colors=['#2ca02c'])\nfig.update_layout(title_text='Word Count Distribution')\nfig.show()\n\navg_word_len = feature['feature_text'].str.split('-').apply(lambda x : [len(i) for i in x]).map(lambda x : np.mean(x))\nfig = ff.create_distplot([avg_word_len], ['feature'], colors=['#ffa408'])\nfig.update_layout(title_text='Average Word Length Distribution')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:40:02.938823Z","iopub.execute_input":"2022-03-15T05:40:02.939244Z","iopub.status.idle":"2022-03-15T05:40:03.038245Z","shell.execute_reply.started":"2022-03-15T05:40:02.93921Z","shell.execute_reply":"2022-03-15T05:40:03.037653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now let's perform N-gram analysis on feature_text.","metadata":{}},{"cell_type":"code","source":"def generate_n_grams(text,ngram=1):\n    words=[word for word in text.split('-')]\n    temp=zip(*[words[i:] for i in range(0,ngram)])\n    ans=[' '.join(ngram) for ngram in temp]\n    return ans\n\n# UNIGRAM\ncounts=defaultdict(int)\nfor text in feature['feature_text']:\n    for word in generate_n_grams(text):\n        counts[word]+=1\n    \ndf=pd.DataFrame(sorted(counts.items(),key=lambda x:x[1],reverse=True))\npd1=df[0][:50]\npd2=df[1][:50]\n\nplt.figure(1,figsize=(30,30))\nsns.barplot(pd2,pd1, color='#FF4040')\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Words in dataframe\", fontsize=20)\nplt.title(\"Top 50 words in UNIGRAM ANALYSIS\", fontsize=30)\nplt.tick_params(axis='both', labelsize=20)\nplt.show()\n\n# BIGRAM\ncounts=defaultdict(int)\nfor text in feature['feature_text']:\n    for word in generate_n_grams(text, ngram=2):\n        counts[word]+=1\n    \ndf=pd.DataFrame(sorted(counts.items(),key=lambda x:x[1],reverse=True))\npd1=df[0][:50]\npd2=df[1][:50]\n\nplt.figure(1,figsize=(30,30))\nsns.barplot(pd2,pd1, color='#00BFFF')\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Words in dataframe\", fontsize=20)\nplt.title(\"Top 50 words in BIGRAM ANALYSIS\", fontsize=30)\nplt.tick_params(axis='both', labelsize=20)\nplt.show()\n\n# TRIGRAM\ncounts=defaultdict(int)\nfor text in feature['feature_text']:\n    for word in generate_n_grams(text, ngram=3):\n        counts[word]+=1\n    \ndf=pd.DataFrame(sorted(counts.items(),key=lambda x:x[1],reverse=True))\npd1=df[0][:50]\npd2=df[1][:50]\n\nplt.figure(1,figsize=(30,30))\nsns.barplot(pd2,pd1, color='#BF3EFF')\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Words in dataframe\", fontsize=20)\nplt.title(\"Top 50 words in TRIGRAM ANALYSIS\", fontsize=30)\nplt.tick_params(axis='both', labelsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:40:03.039465Z","iopub.execute_input":"2022-03-15T05:40:03.039952Z","iopub.status.idle":"2022-03-15T05:40:06.640359Z","shell.execute_reply.started":"2022-03-15T05:40:03.039915Z","shell.execute_reply":"2022-03-15T05:40:06.639419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wordcloud of feature_text","metadata":{}},{"cell_type":"code","source":"# Reference - https://www.kaggle.com/kapakudaibergenov/stylecloud/notebook\nconcat_data = ' '.join([i for x in feature['feature_text'].str.split('-') for i in x])\nstylecloud.gen_stylecloud(text=concat_data,\n                          icon_name='fas fa-eye',\n                          palette='cmocean.sequential.Matter_10',\n                          background_color='black',\n                          gradient='horizontal',\n                          size=1024)\n\n\nImage(filename=\"./stylecloud.png\", width=1024, height=768)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:40:06.641754Z","iopub.execute_input":"2022-03-15T05:40:06.642013Z","iopub.status.idle":"2022-03-15T05:40:12.527278Z","shell.execute_reply.started":"2022-03-15T05:40:06.64198Z","shell.execute_reply":"2022-03-15T05:40:12.526574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3.3'></a>\n# Patient Notes Data\n### Lastly, we will analyse the patient_notes data. We will start by looking the distribution of case_num.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 9))\n\nsns.countplot(x='case_num', data=patient_note, palette = 'winter')\nplt.title('Distribution of Case_Num in Patient Notes Data', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:40:12.528459Z","iopub.execute_input":"2022-03-15T05:40:12.529387Z","iopub.status.idle":"2022-03-15T05:40:12.813047Z","shell.execute_reply.started":"2022-03-15T05:40:12.529344Z","shell.execute_reply":"2022-03-15T05:40:12.812005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Then similarly we will inspect some patient history notes properties like number of words in it and the average word length distributions.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 1, figsize=(20, 12))\n\ntext_len = patient_note['pn_history'].str.split().map(lambda x : len(x))\nsns.histplot(text_len, element=\"step\", kde=True, color='#2ca02c', ax=ax[0])\nax[0].set_title('Word Count Distribution', size=20)\n\navg_word_len = patient_note['pn_history'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x : np.mean(x))\nsns.histplot(avg_word_len, element=\"step\", kde=True, color='#ffa408', ax=ax[1])\nax[1].set_title('Average Word Length Distribution', size=20)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:40:12.814466Z","iopub.execute_input":"2022-03-15T05:40:12.814744Z","iopub.status.idle":"2022-03-15T05:40:18.311868Z","shell.execute_reply.started":"2022-03-15T05:40:12.814713Z","shell.execute_reply":"2022-03-15T05:40:18.310902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### After that let's perform N-gram analysis on patient history notes.","metadata":{}},{"cell_type":"code","source":"def generate_n_grams(text,ngram=1):\n    words=[word for word in text.split()]\n    temp=zip(*[words[i:] for i in range(0,ngram)])\n    ans=[' '.join(ngram) for ngram in temp]\n    return ans\n\n# UNIGRAM\ncounts=defaultdict(int)\nfor text in patient_note['pn_history']:\n    for word in generate_n_grams(text):\n        counts[word]+=1\n    \ndf=pd.DataFrame(sorted(counts.items(),key=lambda x:x[1],reverse=True))\npd1=df[0][:50]\npd2=df[1][:50]\n\nplt.figure(1,figsize=(30,30))\nsns.barplot(pd2,pd1, color='#FF4040')\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Words in dataframe\", fontsize=20)\nplt.title(\"Top 50 words in UNIGRAM ANALYSIS\", fontsize=30)\nplt.tick_params(axis='both', labelsize=20)\nplt.show()\n\n# BIGRAM\ncounts=defaultdict(int)\nfor text in patient_note['pn_history']:\n    for word in generate_n_grams(text, ngram=2):\n        counts[word]+=1\n    \ndf=pd.DataFrame(sorted(counts.items(),key=lambda x:x[1],reverse=True))\npd1=df[0][:50]\npd2=df[1][:50]\n\nplt.figure(1,figsize=(30,30))\nsns.barplot(pd2,pd1, color='#00BFFF')\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Words in dataframe\", fontsize=20)\nplt.title(\"Top 50 words in BIGRAM ANALYSIS\", fontsize=30)\nplt.tick_params(axis='both', labelsize=20)\nplt.show()\n\n# TRIGRAM\ncounts=defaultdict(int)\nfor text in patient_note['pn_history']:\n    for word in generate_n_grams(text, ngram=3):\n        counts[word]+=1\n    \ndf=pd.DataFrame(sorted(counts.items(),key=lambda x:x[1],reverse=True))\npd1=df[0][:50]\npd2=df[1][:50]\n\nplt.figure(1,figsize=(30,30))\nsns.barplot(pd2,pd1, color='#BF3EFF')\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Words in dataframe\", fontsize=20)\nplt.title(\"Top 50 words in TRIGRAM ANALYSIS\", fontsize=30)\nplt.tick_params(axis='both', labelsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:40:18.313461Z","iopub.execute_input":"2022-03-15T05:40:18.314504Z","iopub.status.idle":"2022-03-15T05:40:40.964653Z","shell.execute_reply.started":"2022-03-15T05:40:18.314446Z","shell.execute_reply":"2022-03-15T05:40:40.963398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Annotations Visualization","metadata":{}},{"cell_type":"code","source":"# Reference - https://www.kaggle.com/vanguarde/nbme-eda\nnlp = spacy.blank('en')\nloc = list(train.loc[(train.pn_num==224) & (train.location!='[]'), 'location'].str.replace(\"['\", \"\", regex=False).str.replace(\"']\", \"\", regex=False))\ntext = patient_note[patient_note.pn_num==224].pn_history.values[0]\ndoc = nlp.make_doc(text)\nents = []\nfor l in loc:\n    start, end = l.split(' ')\n    ent = doc.char_span(int(start), int(end), label='annotation')\n    ents.append(ent)\ndoc.ents = ents\ncolor = {\"Annotation\": '#A32EFF'}\ndisplacy.render(doc, style=\"ent\", jupyter=True, options={'colors': color})","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:40:40.96643Z","iopub.execute_input":"2022-03-15T05:40:40.966754Z","iopub.status.idle":"2022-03-15T05:40:41.308723Z","shell.execute_reply.started":"2022-03-15T05:40:40.966715Z","shell.execute_reply":"2022-03-15T05:40:41.307651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wordcloud of patient history notes","metadata":{}},{"cell_type":"code","source":"# Reference - https://www.kaggle.com/kapakudaibergenov/stylecloud/notebook\nconcat_data = ' '.join([i for i in patient_note.pn_history.astype(str)])\nstylecloud.gen_stylecloud(text=concat_data,\n                          icon_name='fas fa-tree',\n                          palette='cartocolors.qualitative.Bold_6',\n                          background_color='black',\n                          gradient='horizontal',\n                          size=1024)\n\n\nImage(filename=\"./stylecloud.png\", width=1024, height=1024)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:40:41.310181Z","iopub.execute_input":"2022-03-15T05:40:41.311013Z","iopub.status.idle":"2022-03-15T05:41:18.197031Z","shell.execute_reply.started":"2022-03-15T05:40:41.310969Z","shell.execute_reply":"2022-03-15T05:41:18.196128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\nIf you are a beginner to NLP then I would refer my another notebook and it will definitely help you to start in NLP:-\n</div>\n<div class=\"row\" align=\"center\">\n    <div class = \"card\">\n      <div class = \"card-body\" style = \"width: 20rem; \">\n        <h5 class = \"card-title\" style = \"font-size: 1.2em;\"align=\"center\">Natural Language Processing</h5>\n          <img src=\"https://www.asksid.ai/wp-content/uploads/2021/02/an-introduction-to-natural-language-processing-with-python-for-seos-5f3519eeb8368.png\" class = \"card_img-top\" style = \"padding: 2% 0;width:19rem;height:10rem;border-radius:30%\">\n        <p class=\"card-text\" style = \"font-size: 1.0em;text-align: center \"><b>(Most) NLP Techniquesüìö</b></p>\n        <a href = \"https://www.kaggle.com/utcarshagrawal/commonlit-eda-most-nlp-techniques\" class = \"btn btn-info btn-lg active\"  role = \"button\" style = \"color: white; margin: 0 15% 0 25%\" data-toggle = \"popover\" title = \"Click\">Click here</a>\n      </div>\n    </div>\n  </div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n    <h2 align='center'>üîéTHANK YOUüîé</h2>\n    <h2 align='center'>Please consider upvoting the kernel if you found it useful.</h2>\n</div>","metadata":{}}]}