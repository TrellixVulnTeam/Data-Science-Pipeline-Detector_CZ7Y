{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport re\nfrom ast import literal_eval\nfrom itertools import chain\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-09T12:39:41.862773Z","iopub.execute_input":"2022-05-09T12:39:41.863343Z","iopub.status.idle":"2022-05-09T12:39:50.576985Z","shell.execute_reply.started":"2022-05-09T12:39:41.863213Z","shell.execute_reply":"2022-05-09T12:39:50.575922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Understanding\ndata_dir = \"/kaggle/input/nbme-score-clinical-patient-notes\"\n# Training data files\ntrain=pd.read_csv(data_dir+\"/train.csv\")\npatient_notes=pd.read_csv(data_dir+\"/patient_notes.csv\")\nfeatures=pd.read_csv(data_dir+\"/features.csv\")\n\n# Test data file/s\ntest=pd.read_csv(data_dir+\"/test.csv\")\n\n# submission sample \nsubmission=pd.read_csv(data_dir+\"/sample_submission.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:50.579442Z","iopub.execute_input":"2022-05-09T12:39:50.579987Z","iopub.status.idle":"2022-05-09T12:39:51.380328Z","shell.execute_reply.started":"2022-05-09T12:39:50.57993Z","shell.execute_reply":"2022-05-09T12:39:51.379292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train \n****Column Description :****\n\n* id - Unique identifier for each patient note / feature pair.\n* case_num - The case to which this patient note belongs.\n* pn_num - The patient note annotated in this row.\n* feature_num - The feature annotated in this row.\n* annotation - The text(s) within a patient note indicating a feature. A feature may be indicated multiple times within a single note.\n* location - Character spans indicating the location of each annotation within the note. Multiple spans may be needed to represent an annotation, in which case the spans are delimited by a semicolon ;.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:51.381645Z","iopub.execute_input":"2022-05-09T12:39:51.382051Z","iopub.status.idle":"2022-05-09T12:39:51.407785Z","shell.execute_reply.started":"2022-05-09T12:39:51.382018Z","shell.execute_reply":"2022-05-09T12:39:51.40679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of rows in train data: {}'.format(train.shape[0]))\nprint('Number of columns in train data: {}'.format(train.shape[1]))\nprint('Number of unique cases: {}'.format(train.case_num.nunique()))\nprint('Number of unique patients: {}'.format(train.pn_num.nunique()))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:51.410064Z","iopub.execute_input":"2022-05-09T12:39:51.410293Z","iopub.status.idle":"2022-05-09T12:39:51.425199Z","shell.execute_reply.started":"2022-05-09T12:39:51.410265Z","shell.execute_reply":"2022-05-09T12:39:51.423866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features\n****Column Description :****\n\n* feature_num - A unique identifier for each feature.\n* case_num - The case to which this patient note belongs.\n* feature_text - A description of the feature.","metadata":{}},{"cell_type":"code","source":"features.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:51.426822Z","iopub.execute_input":"2022-05-09T12:39:51.4271Z","iopub.status.idle":"2022-05-09T12:39:51.441185Z","shell.execute_reply.started":"2022-05-09T12:39:51.427067Z","shell.execute_reply":"2022-05-09T12:39:51.439862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample Feature Text\nfeatures[\"feature_text\"].iloc[4], features[\"feature_text\"].iloc[40], features[\"feature_text\"].iloc[41]","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:51.443146Z","iopub.execute_input":"2022-05-09T12:39:51.443401Z","iopub.status.idle":"2022-05-09T12:39:51.457386Z","shell.execute_reply.started":"2022-05-09T12:39:51.443373Z","shell.execute_reply":"2022-05-09T12:39:51.456661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Patient Notes\n**Column Description :**\n* pn_num - A unique identifier for each patient note.\n* case_num - A unique identifier for the clinical case a patient note represents.\n* pn_history - The text of the encounter as recorded by the test taker.","metadata":{}},{"cell_type":"code","source":"patient_notes.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:51.458566Z","iopub.execute_input":"2022-05-09T12:39:51.459558Z","iopub.status.idle":"2022-05-09T12:39:51.475306Z","shell.execute_reply.started":"2022-05-09T12:39:51.459519Z","shell.execute_reply":"2022-05-09T12:39:51.474161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample Patient Note\nprint(patient_notes[\"pn_history\"].iloc[8])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:51.476379Z","iopub.execute_input":"2022-05-09T12:39:51.47714Z","iopub.status.idle":"2022-05-09T12:39:51.491418Z","shell.execute_reply.started":"2022-05-09T12:39:51.477102Z","shell.execute_reply":"2022-05-09T12:39:51.490364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\ndef create_labels_for_scoring(df):\n    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n        if len(location_list) > 0:\n            location = location_list[0]\n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n    return truths","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:51.492951Z","iopub.execute_input":"2022-05-09T12:39:51.493973Z","iopub.status.idle":"2022-05-09T12:39:51.505101Z","shell.execute_reply.started":"2022-05-09T12:39:51.49393Z","shell.execute_reply":"2022-05-09T12:39:51.504014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Preprocess\ndef process_feature_text(text):\n    return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \").replace(\"I-year\", \"1-year\")\n\n\ndef clean_spaces(txt):\n    txt = re.sub('\\n', ' ', txt)\n    txt = re.sub('\\t', ' ', txt)\n    txt = re.sub('\\r', ' ', txt)\n    return txt","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:51.507979Z","iopub.execute_input":"2022-05-09T12:39:51.508569Z","iopub.status.idle":"2022-05-09T12:39:51.526586Z","shell.execute_reply.started":"2022-05-09T12:39:51.508525Z","shell.execute_reply":"2022-05-09T12:39:51.525526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nbme-score-clinical-patient-notes/train.csv\")\n\n# Merge Datasets to Prepare Training Data\nmerged_df = train.merge(features, how=\"left\", on=[\"case_num\", \"feature_num\"])\nmerged_df = merged_df.merge(patient_notes, how=\"left\", on=['case_num', 'pn_num'])\n\n# Preprocess\nmerged_df['pn_history'] = merged_df['pn_history'].apply(lambda x: x.strip())\nmerged_df['pn_history'] = merged_df['pn_history'].apply(clean_spaces)\nmerged_df['pn_history'] = merged_df['pn_history'].apply(lambda x: x.lower())\nmerged_df['feature_text'] = merged_df['feature_text'].apply(process_feature_text)\nmerged_df['feature_text'] = merged_df['feature_text'].apply(clean_spaces)\nmerged_df['feature_text'] = merged_df['feature_text'].apply(lambda x: x.lower())\n\n\nmerged_df['annotation_len'] = train['annotation'].apply(len)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:51.527761Z","iopub.execute_input":"2022-05-09T12:39:51.528051Z","iopub.status.idle":"2022-05-09T12:39:51.822893Z","shell.execute_reply.started":"2022-05-09T12:39:51.528019Z","shell.execute_reply":"2022-05-09T12:39:51.82178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df['annotation'] = merged_df['annotation'].apply(ast.literal_eval)\nmerged_df['location'] = merged_df['location'].apply(ast.literal_eval)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:51.824167Z","iopub.execute_input":"2022-05-09T12:39:51.824491Z","iopub.status.idle":"2022-05-09T12:39:52.078023Z","shell.execute_reply.started":"2022-05-09T12:39:51.824455Z","shell.execute_reply":"2022-05-09T12:39:52.076956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntruths = create_labels_for_scoring(merged_df)\nmerged_df\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:52.079408Z","iopub.execute_input":"2022-05-09T12:39:52.08038Z","iopub.status.idle":"2022-05-09T12:39:57.95964Z","shell.execute_reply.started":"2022-05-09T12:39:52.080325Z","shell.execute_reply":"2022-05-09T12:39:57.958434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data as train and test\ntest_size = int(len(merged_df)* (0.2))\ntrain_df, test_df = train_test_split(merged_df, test_size=test_size, random_state=500)\nprint(len(train_df), len(test_df))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:57.961245Z","iopub.execute_input":"2022-05-09T12:39:57.961628Z","iopub.status.idle":"2022-05-09T12:39:57.978015Z","shell.execute_reply.started":"2022-05-09T12:39:57.961579Z","shell.execute_reply":"2022-05-09T12:39:57.976941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#   k fold ekle///\n\n# Fold = GroupKFold(n_splits=CFG.n_fold)\n# groups = train['pn_num'].values\n# for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n#     train.loc[val_index, 'fold'] = int(n)\n# train['fold'] = train['fold'].astype(int)\n# display(train.groupby('fold').size())","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:57.979629Z","iopub.execute_input":"2022-05-09T12:39:57.980049Z","iopub.status.idle":"2022-05-09T12:39:57.983593Z","shell.execute_reply.started":"2022-05-09T12:39:57.980013Z","shell.execute_reply":"2022-05-09T12:39:57.982705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data, param, tokenizer):\n        self.data = data\n        self.param = param\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        inputs, label, offset_mapping, sequence_ids = prepare_dataset(self.data, self.param, self.tokenizer, item)\n        return inputs, label, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:57.984823Z","iopub.execute_input":"2022-05-09T12:39:57.985238Z","iopub.status.idle":"2022-05-09T12:39:57.997752Z","shell.execute_reply.started":"2022-05-09T12:39:57.985196Z","shell.execute_reply":"2022-05-09T12:39:57.997102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset(data, param, tokenizer, item):\n    pn_history = data['pn_history'].values[item]\n    feature_text = data['feature_text'].values[item]\n    locations = data['location'].values[item]\n    annotation_length = data['annotation_len'].values[item]\n    \n    inputs = tokenizer(pn_history, feature_text, max_length = param['max_len'], padding = param['padding'], return_offsets_mapping = False)\n      \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    \n    encodings = tokenizer(pn_history, max_length = param['max_len'], padding = param['padding'], return_offsets_mapping=True)     \n    \n    offset_mapping = encodings['offset_mapping']\n    sequence_ids = encodings.sequence_ids()\n    offset_mapping = np.array(offset_mapping)\n    sequence_ids = np.array(sequence_ids).astype(\"float16\")\n    ignore_idxes = np.where(np.array(sequence_ids) != 0)[0]\n    \n    label = np.zeros(len(offset_mapping))    \n    label[ignore_idxes] = -1\n    \n    if annotation_length != 0:\n        for location in locations:\n            for loc in [s.split() for s in location.split(';')]:\n                start_idx = -1\n                end_idx = -1\n                start, end = int(loc[0]), int(loc[1])\n                for idx in range(len(offset_mapping)):\n                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n                        start_idx = idx - 1\n                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n                        end_idx = idx + 1\n                if start_idx == -1:\n                    start_idx = end_idx\n                if (start_idx != -1) & (end_idx != -1):\n                    label[start_idx:end_idx] = 1\n    label = torch.tensor(label, dtype=torch.float)\n    return inputs, label, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:57.998689Z","iopub.execute_input":"2022-05-09T12:39:57.999213Z","iopub.status.idle":"2022-05-09T12:39:58.015733Z","shell.execute_reply.started":"2022-05-09T12:39:57.999183Z","shell.execute_reply":"2022-05-09T12:39:58.014621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self,param):\n        super().__init__()\n        self.param = param       \n        self.bert = AutoModel.from_pretrained(param['model_name'])  # BERT model\n        self.config = AutoConfig.from_pretrained(param['model_name'], output_hidden_states=True)\n        self.dropout = nn.Dropout(p=param['dropout'])\n        self.fc1 = nn.Linear(768, 128)\n        self.fc2 = nn.Linear(128, 1)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        logits = self.fc1(outputs[0])\n        logits = self.fc2(self.dropout(logits)).squeeze(-1)\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:58.017019Z","iopub.execute_input":"2022-05-09T12:39:58.017379Z","iopub.status.idle":"2022-05-09T12:39:58.034706Z","shell.execute_reply.started":"2022-05-09T12:39:58.017351Z","shell.execute_reply":"2022-05-09T12:39:58.033902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    \"max_len\": 416,\n    \"padding\": \"max_length\",\n    \"return_offsets_mapping\": True,\n    \"truncation\": \"only_second\",\n    \"model_name\": \"bert-base-uncased\",\n    \"dropout\": 0.2,\n    \"lr\": 1e-5,\n    \"test_size\": 0.2,\n    \"seed\": 1268,\n    \"batch_size\": 8\n}\n\ntokenizer = AutoTokenizer.from_pretrained(params['model_name'])\n\ntraining_data = CustomDataset(train_df, params, tokenizer)\ntrain_dataloader = DataLoader(training_data, batch_size=params['batch_size'], shuffle=True)\n\ntest_data = CustomDataset(test_df, params, tokenizer)\ntest_dataloader = DataLoader(test_data, batch_size=params['batch_size'], shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:39:58.03631Z","iopub.execute_input":"2022-05-09T12:39:58.036587Z","iopub.status.idle":"2022-05-09T12:40:11.979272Z","shell.execute_reply.started":"2022-05-09T12:39:58.036552Z","shell.execute_reply":"2022-05-09T12:40:11.978272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = CustomModel(params).to(DEVICE)\n\ncriterion = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\noptimizer = optim.AdamW(model.parameters(), lr=params['lr'])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:40:11.980508Z","iopub.execute_input":"2022-05-09T12:40:11.980777Z","iopub.status.idle":"2022-05-09T12:40:54.917545Z","shell.execute_reply.started":"2022-05-09T12:40:11.980745Z","shell.execute_reply":"2022-05-09T12:40:54.916579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef get_location_predictions(preds, offset_mapping, sequence_ids, test=False):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        pred = 1 / (1 + np.exp(-pred))\n        start_idx = None\n        end_idx = None\n        current_preds = []\n        for pred, offset, seq_id in zip(pred, offsets, seq_ids):\n            if seq_id is None or seq_id == 0:\n                continue\n\n            if pred > 0.5:\n                if start_idx is None:\n                    start_idx = offset[0]\n                end_idx = offset[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n            \n    return all_predictions\n\n\ndef calculate_char_cv(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros(num_chars)\n\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n\n        char_preds = np.zeros(num_chars)\n\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n\n    results = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", labels=np.unique(all_preds))\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n\n    return {\n        \"Accuracy\": accuracy,\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:40:54.918766Z","iopub.execute_input":"2022-05-09T12:40:54.919045Z","iopub.status.idle":"2022-05-09T12:40:54.934702Z","shell.execute_reply.started":"2022-05-09T12:40:54.919009Z","shell.execute_reply":"2022-05-09T12:40:54.933993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, criterion):\n    model.train()\n    train_loss = []\n    for step, (inputs, labels, a,b) in enumerate(dataloader):             \n        for k, v in inputs.items():\n            inputs[k] = v.to(DEVICE)     \n            \n        labels = labels.to(DEVICE)            \n        logits = model(inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids'])\n        loss = criterion(logits, labels)\n        size = inputs['input_ids'].size(0)\n        loss = torch.masked_select(loss, labels > -1.0).mean()\n        train_loss.append(loss.item() * size)\n        loss.backward()\n\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n    return sum(train_loss)/len(train_loss)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:40:54.935786Z","iopub.execute_input":"2022-05-09T12:40:54.936722Z","iopub.status.idle":"2022-05-09T12:40:54.953572Z","shell.execute_reply.started":"2022-05-09T12:40:54.93668Z","shell.execute_reply":"2022-05-09T12:40:54.952838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, dataloader, criterion):\n    model.eval()\n    valid_loss = []\n    preds = []\n    offsets = []\n    seq_ids = []\n    valid_labels = []\n    for step, (inputs, labels, offset_mapping, sequence_ids) in enumerate(dataloader):             \n        for k, v in inputs.items():\n            inputs[k] = v.to(DEVICE)     \n\n        labels = labels.to(DEVICE)            \n        logits = model(inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids'])\n        loss = criterion(logits, labels)\n        loss = torch.masked_select(loss, labels > -1.0).mean()\n        size = inputs['input_ids'].size(0)\n        valid_loss.append(loss.item() * size)\n\n        preds.append(logits.detach().cpu().numpy())\n        offsets.append(offset_mapping.numpy())\n        seq_ids.append(sequence_ids.numpy())\n        valid_labels.append(labels.detach().cpu().numpy())\n        \n\n    preds = np.concatenate(preds, axis=0)\n    offsets = np.concatenate(offsets, axis=0)\n    seq_ids = np.concatenate(seq_ids, axis=0)\n    valid_labels = np.concatenate(valid_labels, axis=0)\n    location_preds = get_location_predictions(preds, offsets, seq_ids, test=False)\n    score = calculate_char_cv(location_preds, offsets, seq_ids, valid_labels)\n\n    return sum(valid_loss)/len(valid_loss), score","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:40:54.97222Z","iopub.execute_input":"2022-05-09T12:40:54.972674Z","iopub.status.idle":"2022-05-09T12:40:54.987688Z","shell.execute_reply.started":"2022-05-09T12:40:54.972632Z","shell.execute_reply":"2022-05-09T12:40:54.986922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\ntrain_loss_data, valid_loss_data = [], []\nscore_data_list = []\nvalid_loss_min = np.Inf\nsince = time.time()\nepochs = 3\nbest_loss = np.inf\n\nfor i in range(epochs):\n    print(\"Epoch: {}/{}\".format(i + 1, epochs))\n    # first train model\n    train_loss = train_model(model, train_dataloader, optimizer, criterion)\n    train_loss_data.append(train_loss)\n    print(f\"Train loss: {train_loss}\")\n    # evaluate model\n    valid_loss, score = eval_model(model, test_dataloader, criterion)\n    valid_loss_data.append(valid_loss)\n    score_data_list.append(score)\n    print(f\"Valid loss: {valid_loss}\")\n    print(f\"Valid score: {score}\")\n    \n    if valid_loss < best_loss:\n        best_loss = valid_loss\n        torch.save(model.state_dict(), \"nbme_bert_v2.pth\")\n\n    \ntime_elapsed = time.time() - since\nprint('Training completed in {:.0f}m {:.0f}s'.format(\n    time_elapsed // 60, time_elapsed % 60))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:40:54.989267Z","iopub.execute_input":"2022-05-09T12:40:54.989718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}