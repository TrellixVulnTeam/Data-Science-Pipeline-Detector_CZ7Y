{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello Fellow Kagglers,\n\nThis notebook demonstrates the preprocessing of texts for the NBME - Score Clinical Patient Notes competition.\n\nFirst some properties of the training data are visualised, followed by the tokenization of the training texts and lastly the target labels are generated.\n\n[TPU training notebook](https://www.kaggle.com/markwijkhuizen/nbme-albert-large-training-tpu)\n\nInference notebook coming soon!\n\n**V3**\n* Labels shape [Number of Tokens, Number of Features] to predict the features a token belongs to\n* Saving labels as sparse tensors to reduce memory usage\n* Added Train patient note corrections","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tqdm.notebook import tqdm\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom transformers import TFAlbertModel, PreTrainedTokenizerFast\nfrom textblob import TextBlob\n\nimport re\nimport ast\n\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:57:28.306472Z","iopub.execute_input":"2022-02-18T17:57:28.306803Z","iopub.status.idle":"2022-02-18T17:57:37.376363Z","shell.execute_reply.started":"2022-02-18T17:57:28.306715Z","shell.execute_reply":"2022-02-18T17:57:37.375527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features\n\nfeatures.csv - The rubric of features (or key concepts) for each clinical case.\n\nThese are currently not used","metadata":{}},{"cell_type":"code","source":"features = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/features.csv')\n\n# Sort For Reproducible Ordinal Encoding\nfeatures = features.sort_values('feature_num')\n\n# Add Ordinal Encoding\nfeatures['feature_num_ordinal'] = features['feature_num'].astype('category').cat.codes\n\ndisplay(features.head())\n\ndisplay(features.info())","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:57:37.37794Z","iopub.execute_input":"2022-02-18T17:57:37.378172Z","iopub.status.idle":"2022-02-18T17:57:37.431607Z","shell.execute_reply.started":"2022-02-18T17:57:37.378143Z","shell.execute_reply":"2022-02-18T17:57:37.430797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_CLASSES = len(features)\nprint(f'N_CLASSES: {N_CLASSES}')","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:03:37.513555Z","iopub.execute_input":"2022-02-18T18:03:37.513832Z","iopub.status.idle":"2022-02-18T18:03:37.523591Z","shell.execute_reply.started":"2022-02-18T18:03:37.513801Z","shell.execute_reply":"2022-02-18T18:03:37.522595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Patient Notes\n\nA collection of about 40,000 Patient Note history portions. Only a subset of these have features annotated. You may wish to apply unsupervised learning techniques on the notes without annotations. The patient notes in the test set are not included in the public version of this file.","metadata":{}},{"cell_type":"code","source":"patient_notes = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv')\n\n# Set Case Number and Patient Number as Index for Convenient Access\npatient_notes = patient_notes.set_index(['case_num', 'pn_num'])\n\n# Clean Patient History\ndef clean_text(s, ret_n_char_window):\n    s = str(s).lower()\n    n_char_window = 0\n    \n    replacements = [\n        ('20yof c/o abd pain', '20yo f c/o abd pain', 1),\n        ('does endorse a historyof taking', 'does endorse a history of taking',  1),\n        ('mr. dillon is a 17yo mm', 'mr. dillon is a 17yo m', 1),\n        ('ms. montgomery is a 44 yof', 'ms. montgomery is a 44 yo f', 1),\n        ('dolores montgomery is a 44yof', 'dolores montgomery is a 44yo f', 1),\n        ('44 yof with history', '44 yo f with history', 1),\n        ('s. montgomery is a 44yof', 's. montgomery is a 44yo f', 1),\n        ('ms. moore is a 45yof', 'ms. moore is a 45yo f', 1),\n        ('pt is a 45 y/o wf', 'pt is a 45 y/o w f', 1),\n        ('45 yof who', '45 yo f who', 1),\n        ('karin moore is a 45yof with', 'karin moore is a 45yo f with', 1),\n        ('ms. moore is a 45 y/o wf', 'ms. moore is a 45 y/o w f', 1),\n        ('mrs. moore is a 45 year old wf', 'mrs. moore is a 45 year old w f', 1),\n        ('edie whelan is a 26 year old aaf', 'edie whelan is a 26 year old aa f', 1),\n        ('ms tompkin is  35 yof with', 'ms tompkin is  35 yo f with', 1),\n        ('35 yof', '35 yo f', 1),\n        ('loraine wicks, a 67yof', 'loraine wicks, a 67yo f', 1),\n        ('ms. madden 20yof w/ ha', 'ms. madden 20yo f w/ ha', 1),\n        ('20yof with no', '20yo f with no', 1),\n        ('ms. madden is a 20 yof female', 'ms. madden is a 20 yo f female', 1),\n    ]\n    \n    for a, b, c in replacements:\n        if a in s:\n            n_char_window += s.count(a) * c\n            s = s.replace(a, b)\n    \n    if ret_n_char_window:\n        return n_char_window\n    else:\n        return s\n\npatient_notes['pn_history_clean'] = patient_notes['pn_history'].transform(clean_text, ret_n_char_window=False)\npatient_notes['n_char_window'] = patient_notes['pn_history'].transform(clean_text, ret_n_char_window=True)\n\ndisplay(patient_notes.head())\n\ndisplay(patient_notes.info())","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:57:37.433423Z","iopub.execute_input":"2022-02-18T17:57:37.433983Z","iopub.status.idle":"2022-02-18T17:57:39.614795Z","shell.execute_reply.started":"2022-02-18T17:57:37.433939Z","shell.execute_reply":"2022-02-18T17:57:39.613994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note Word Count\npatient_notes['n_words'] = patient_notes['pn_history'].progress_apply(word_tokenize).apply(len)\n\n# Note Sentence Count\npatient_notes['n_sentences'] = patient_notes['pn_history'].progress_apply(sent_tokenize).apply(len)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:57:39.617027Z","iopub.execute_input":"2022-02-18T17:57:39.617436Z","iopub.status.idle":"2022-02-18T17:59:37.815635Z","shell.execute_reply.started":"2022-02-18T17:57:39.617388Z","shell.execute_reply":"2022-02-18T17:59:37.814964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Note Word Count Distribution\n\nInput texts are around 160 words with a maximum of 225, that's good news as AlBERT has a maximum input lenght of 512 tokens!","metadata":{}},{"cell_type":"code","source":"display(patient_notes['n_words'].describe().to_frame())","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:59:37.816745Z","iopub.execute_input":"2022-02-18T17:59:37.817228Z","iopub.status.idle":"2022-02-18T17:59:37.831614Z","shell.execute_reply.started":"2022-02-18T17:59:37.817184Z","shell.execute_reply":"2022-02-18T17:59:37.831059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\npatient_notes['n_words'].plot(kind='hist', bins=32)\nplt.title('Number of Words per Patient Note', size=24)\nplt.xticks(size=16)\nplt.yticks(size=16)\nplt.xlabel('Note Word Count', size=18)\nplt.ylabel('Frequency', size=18)\nplt.grid()\npass","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:59:37.832746Z","iopub.execute_input":"2022-02-18T17:59:37.832961Z","iopub.status.idle":"2022-02-18T17:59:38.124367Z","shell.execute_reply.started":"2022-02-18T17:59:37.832933Z","shell.execute_reply":"2022-02-18T17:59:38.123506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Note Sentence Count Distribution","metadata":{}},{"cell_type":"code","source":"display(patient_notes['n_sentences'].describe().to_frame())","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:59:38.126028Z","iopub.execute_input":"2022-02-18T17:59:38.12634Z","iopub.status.idle":"2022-02-18T17:59:38.14082Z","shell.execute_reply.started":"2022-02-18T17:59:38.126296Z","shell.execute_reply":"2022-02-18T17:59:38.140238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\npatient_notes['n_sentences'].plot(kind='hist', bins=16)\nplt.title('Number of Sentences per Patient Note', size=24)\nplt.xticks(size=16)\nplt.yticks(size=16)\nplt.xlabel('Note Sentence Count', size=18)\nplt.ylabel('Frequency', size=18)\nplt.grid()\npass","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:59:38.141966Z","iopub.execute_input":"2022-02-18T17:59:38.142171Z","iopub.status.idle":"2022-02-18T17:59:38.382731Z","shell.execute_reply.started":"2022-02-18T17:59:38.142146Z","shell.execute_reply":"2022-02-18T17:59:38.382111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:59:38.383951Z","iopub.execute_input":"2022-02-18T17:59:38.384704Z","iopub.status.idle":"2022-02-18T17:59:38.424673Z","shell.execute_reply.started":"2022-02-18T17:59:38.384657Z","shell.execute_reply":"2022-02-18T17:59:38.423962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Annotation Correction\n\n!!! This is not my work but that of [Y.NAKAMA](https://www.kaggle.com/yasufuminakama), a big thanks for these corrections !!!","metadata":{}},{"cell_type":"code","source":"# incorrect annotation source: https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train\ntrain.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\ntrain.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n\ntrain.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\ntrain.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n\ntrain.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\ntrain.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n\ntrain.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\ntrain.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n\ntrain.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\ntrain.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n\ntrain.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\ntrain.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n\ntrain.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\ntrain.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n\ntrain.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\ntrain.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n\ntrain.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\ntrain.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n\ntrain.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\ntrain.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n\ntrain.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\ntrain.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n\ntrain.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\ntrain.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n\ntrain.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\ntrain.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n\ntrain.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\ntrain.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n\ntrain.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\ntrain.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n\ntrain.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\ntrain.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n\ntrain.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\ntrain.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n\ntrain.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\ntrain.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n\ntrain.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\ntrain.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n\ntrain.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\ntrain.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n\ntrain.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\ntrain.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n\ntrain.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\ntrain.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n\ntrain.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\ntrain.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n\ntrain.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\ntrain.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n\ntrain.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\ntrain.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n\ntrain.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\ntrain.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n\ntrain.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\ntrain.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n\ntrain.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\ntrain.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n\ntrain.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\ntrain.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n\ntrain.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\ntrain.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n\ntrain.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\ntrain.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n\ntrain.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\ntrain.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n\ntrain.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\ntrain.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n\ntrain.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\ntrain.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n\ntrain.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\ntrain.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n\ntrain.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\ntrain.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n\ntrain.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\ntrain.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n\ntrain.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\ntrain.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n\ntrain.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\ntrain.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n\ntrain.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\ntrain.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n\ntrain.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\ntrain.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n\ntrain.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\ntrain.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:59:38.427496Z","iopub.execute_input":"2022-02-18T17:59:38.427739Z","iopub.status.idle":"2022-02-18T17:59:38.513225Z","shell.execute_reply.started":"2022-02-18T17:59:38.427705Z","shell.execute_reply":"2022-02-18T17:59:38.512614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_annotation_list(s):\n    s = str(s).lower()\n    \n    for r in ['[\\'', '\\']', '[\"', '\"]']:\n        s = s.replace(r, '')\n        \n    for r in ['\\', \\'', '\", \\'', '\\', \"', '\", \"']:\n        s = s.replace(r, '<split>')\n        \n    # Custom\n    s = s.replace('\\\\\\'', '\\'')\n        \n    ss = s.split('<split>')\n    \n    return ss\n    \ntrain['annotation_list'] = train['annotation'].apply(get_annotation_list)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:59:38.514131Z","iopub.execute_input":"2022-02-18T17:59:38.514933Z","iopub.status.idle":"2022-02-18T17:59:38.55834Z","shell.execute_reply.started":"2022-02-18T17:59:38.514901Z","shell.execute_reply":"2022-02-18T17:59:38.557554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop Empty Annotations\ntrain.drop(train.loc[train['annotation'] == '[]'].index, inplace=True)\n\n# Cast annotation Column to String\ntrain['annotation'] = train['annotation'].astype(str)\ntrain['location'] = train['location'].astype(str)\n\n# Set Case Number and Patient Number as Index\ntrain = train.set_index(['case_num', 'pn_num'])\n\n# Add Feature Num Ordinal Encoded\ntrain['feature_num_ordinal'] = features.set_index('feature_num').loc[train['feature_num'], 'feature_num_ordinal'].values\n\ndisplay(train.head(25))\n\ndisplay(train.info())","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:59:38.559184Z","iopub.execute_input":"2022-02-18T17:59:38.55943Z","iopub.status.idle":"2022-02-18T17:59:38.617049Z","shell.execute_reply.started":"2022-02-18T17:59:38.559402Z","shell.execute_reply":"2022-02-18T17:59:38.616241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AlBERT Tokenizer","metadata":{}},{"cell_type":"code","source":"# Maximum Token Input Size\nSEQ_LENGTH = 512\n\n# AlBERT Base is used here, but the tokenization process is the same for all AlBERT Sizes\ntokenizer = PreTrainedTokenizerFast.from_pretrained('albert-base-v2')\ntokenizer.add_special_tokens({'pad_token': '<pad>'})\n\n# Save Tokenizer for Offline Inference Process\ntokenizer.save_pretrained('./tokenizer/')","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:59:38.618374Z","iopub.execute_input":"2022-02-18T17:59:38.619154Z","iopub.status.idle":"2022-02-18T17:59:41.936862Z","shell.execute_reply.started":"2022-02-18T17:59:38.619109Z","shell.execute_reply":"2022-02-18T17:59:41.936074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function tokenize the text according to a AlBERT model tokenizer\ndef tokenize(note):\n    return tokenizer(\n            note,  # The input text\n            padding = 'max_length', # Add Pad Tokens to Maximum Length\n            truncation = True, # Truncate input texts if they are too long\n            max_length = SEQ_LENGTH, # Maximum token length \n            return_offsets_mapping = True, # Return character offset to token mapping\n        )","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:59:41.93804Z","iopub.execute_input":"2022-02-18T17:59:41.938604Z","iopub.status.idle":"2022-02-18T17:59:41.943601Z","shell.execute_reply.started":"2022-02-18T17:59:41.938574Z","shell.execute_reply":"2022-02-18T17:59:41.942851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Example Model","metadata":{}},{"cell_type":"code","source":"albert = TFAlbertModel.from_pretrained(\n        'albert-large-v2',\n        output_hidden_states = True,\n        return_dict = True,\n    )","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:01.011553Z","iopub.execute_input":"2022-02-18T18:04:01.011832Z","iopub.status.idle":"2022-02-18T18:04:03.155654Z","shell.execute_reply.started":"2022-02-18T18:04:01.011804Z","shell.execute_reply":"2022-02-18T18:04:03.154652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = tf.keras.layers.Input(shape = (SEQ_LENGTH), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=SEQ_LENGTH, dtype=tf.int32, name='attention_mask')\n\n# Get the last hidden state\nlast_hidden_state = albert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n\n# Output Layer of Size [Tokens, Number of Features]\noutput = tf.keras.layers.Dense(N_CLASSES, activation='sigmoid')(last_hidden_state)\n\nmodel = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:03.157242Z","iopub.execute_input":"2022-02-18T18:04:03.157437Z","iopub.status.idle":"2022-02-18T18:04:06.185395Z","shell.execute_reply.started":"2022-02-18T18:04:03.157413Z","shell.execute_reply":"2022-02-18T18:04:06.184596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:06.186922Z","iopub.execute_input":"2022-02-18T18:04:06.187137Z","iopub.status.idle":"2022-02-18T18:04:06.196636Z","shell.execute_reply.started":"2022-02-18T18:04:06.18711Z","shell.execute_reply":"2022-02-18T18:04:06.194991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:06.19912Z","iopub.execute_input":"2022-02-18T18:04:06.199358Z","iopub.status.idle":"2022-02-18T18:04:07.237264Z","shell.execute_reply.started":"2022-02-18T18:04:06.199331Z","shell.execute_reply":"2022-02-18T18:04:07.236371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_text = patient_notes.loc[(0, 0), 'pn_history_clean']\nalbert_tokens = tokenize(example_text)\n\n# Show all tokenization outputs\nfor k, v in albert_tokens.items():\n    print(f'k: {k}, v shape: {np.array(v).shape}')\n    \nmodel_output = model.predict_on_batch({\n        'input_ids': np.array([albert_tokens['input_ids']]),\n        'attention_mask': np.array([albert_tokens['attention_mask']])\n    })\n\n# Check Output Shape\nprint(f'model_output shape: {model_output.shape}')\n\n# Check Model Output\nplt.figure(figsize=(8, 4))\nplt.xlim(0, 1)\nplt.xticks(np.arange(0, 1.1, 0.1))\npd.Series(model_output.flatten()).plot(kind='hist')\nplt.plot()\npass","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:07.240007Z","iopub.execute_input":"2022-02-18T18:04:07.240431Z","iopub.status.idle":"2022-02-18T18:04:15.479759Z","shell.execute_reply.started":"2022-02-18T18:04:07.240392Z","shell.execute_reply":"2022-02-18T18:04:15.478851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Offset Mapping","metadata":{}},{"cell_type":"code","source":"tokens = albert_tokens['input_ids'][:20]\noffset_mapping = albert_tokens['offset_mapping'][:20]\n\nprint(f'example_text: {example_text}\\n')\n\nfor idx, (start_char_index, end_char_index) in enumerate(offset_mapping):\n    # The token tokenizes a part of the original text, this does not have to be a single word!!!\n    # For example \"-\", \",\" and \".\" are also encoded as a single token\n    substr = example_text[start_char_index:end_char_index]\n    print(\n          f'start_char_index: {start_char_index:02d}, \\tend_char_index: {end_char_index:02d},'\n          f'\\ttoken: {tokens[idx]}, \\tsubstr: {substr}'\n        )","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:15.48117Z","iopub.execute_input":"2022-02-18T18:04:15.481412Z","iopub.status.idle":"2022-02-18T18:04:15.48902Z","shell.execute_reply.started":"2022-02-18T18:04:15.481383Z","shell.execute_reply":"2022-02-18T18:04:15.48841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Utility Features","metadata":{}},{"cell_type":"code","source":"# Returns the start location and string length of annotations\ndef get_location_start_offset(s):\n    s = re.sub(r\"(\\['\\s*|'\\]|\\s*')\", '', s)\n    s = s.split(',')\n    s = [e.split(';') for e in s]\n    \n    res = []\n    for eee in s:\n        res_annotation = []    \n        for ee in eee:\n            \n            ee = ee.split(';')\n            for e in ee:\n                start_idx, end_idx = e.split(' ')\n                res_annotation.append([int(start_idx), int(end_idx) - int(start_idx)])\n        res.append(res_annotation)\n    \n    return res\n\ntrain['location_start_offset'] = train['location'].apply(get_location_start_offset)\n\n# Note location is a string, wheras location_start_offset is a nested list\ndisplay(train.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:15.490152Z","iopub.execute_input":"2022-02-18T18:04:15.490562Z","iopub.status.idle":"2022-02-18T18:04:16.096137Z","shell.execute_reply.started":"2022-02-18T18:04:15.490527Z","shell.execute_reply":"2022-02-18T18:04:16.095362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Returns the start index of the annotation location, used for sorting annotations\ndef get_location_start(location_start_offset):\n    res = np.PINF\n    for l in location_start_offset:\n        for start, _ in l:\n            res = min(res, start)\n        \n    return res\n\ntrain['location_start'] = train['location_start_offset'].apply(get_location_start)\n\n# location_start is simply the minimum start location off annotations\ndisplay(train.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:16.097399Z","iopub.execute_input":"2022-02-18T18:04:16.097607Z","iopub.status.idle":"2022-02-18T18:04:16.135563Z","shell.execute_reply.started":"2022-02-18T18:04:16.09758Z","shell.execute_reply":"2022-02-18T18:04:16.134776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Labels\n\nLabels are saved as sparse tensor indices to reduce memory usage by only saving the indices of the non-zero elements, have a look at the [Tensorflow SparseTensor documentation](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor).\n","metadata":{}},{"cell_type":"code","source":"# Maximum Annotations Per Patient Note\nMAX_ANNOTATIONS = 256","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:16.13684Z","iopub.execute_input":"2022-02-18T18:04:16.137041Z","iopub.status.idle":"2022-02-18T18:04:16.148407Z","shell.execute_reply.started":"2022-02-18T18:04:16.137015Z","shell.execute_reply":"2022-02-18T18:04:16.147595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find all function for multiple string occurances\ndef find_all(a, b, offset=0):\n    if len(a) == 0:\n        return []\n    \n    res = []\n    start_idx = a.find(b)\n    if start_idx != -1:\n        return [offset + start_idx] + find_all(a[start_idx + len(b):], b, offset=offset + start_idx + len(b))\n    else:\n        return []\n    \nprint(find_all('I like cats and cats and cats', 'cats'))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:16.150807Z","iopub.execute_input":"2022-02-18T18:04:16.15103Z","iopub.status.idle":"2022-02-18T18:04:16.160876Z","shell.execute_reply.started":"2022-02-18T18:04:16.151003Z","shell.execute_reply":"2022-02-18T18:04:16.160048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    This function is the beating heart of the target label generation.\n    The challenge finding the indices of the tokens that belong to the annotation\n    Especially as the texts are cleaned, thus the annotations are almost off by a few characters\n\"\"\"\ndef get_token_indices(patient_note, ann, om, start_offset, n_char_window, recursion=False):\n    # Result, the token indices that belong to the annotation\n    token_indices = []\n    #\n    starts_ann = find_all(patient_note, ann)\n\n    # Check if annotation contains multiple parts\n    # for example the patient \"heart racing and pounding\" has annoation \"heart pounding\" consisting of 2 character ranges\n    if len(start_offset) > 1:\n        offset = 0\n        for start_idx, str_len in start_offset:\n            ann_partial = ann[offset:offset+str_len]\n            offset += str_len + 1\n            # Go in recursion with partial annotation to get tokens of that annotation part\n            token_indices += get_token_indices(patient_note, ann_partial, om, [[start_idx, str_len]], n_char_window)\n        return token_indices\n    else:\n        # Annotation string start index and offset in ORIGINAL text, thus not the clean text!\n        start_str, str_len = start_offset[0]\n        # Minimum start is the original srtart position minus the removed character by cleaning\n        start_min = start_str - n_char_window\n        # Maximum string position if the start position plus the string length, a token can consist of a single character!\n        start_max = start_str + str_len + n_char_window\n    \n    # Find the tokens belonging to the annoation, loop over all found instances of the annotation in the patient note\n    for ann_start_idx in starts_ann:\n        # End character position of current token\n        ann_end_idx = ann_start_idx + len(ann)\n        # Iterate over all token character positions\n        for idx, (start, end) in enumerate(om):\n            # Token text must fall within the annotation min and max possible position\n            if not(start == 0 and end == 0) and (start >= start_min and start <= start_max):\n                # Exact Match\n                if start >= ann_start_idx and end <= ann_end_idx:\n                    token_indices.append(idx)\n                # Label is Subset of Token\n                elif ann_start_idx >= start and ann_end_idx <= end:\n                    token_indices.append(idx)\n            \n    return token_indices","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:16.162376Z","iopub.execute_input":"2022-02-18T18:04:16.16259Z","iopub.status.idle":"2022-02-18T18:04:16.173966Z","shell.execute_reply.started":"2022-02-18T18:04:16.162563Z","shell.execute_reply":"2022-02-18T18:04:16.173141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to get Training texts X and target labels y\n# Use the debug flag to see what this function is doing!\ndef get_x_y(debug=False):\n    N_TRAIN_SAMPLES = train.index.unique().size\n    error_c = 0\n    # Training texts and labels numpy arrays\n    X = np.zeros(shape=[N_TRAIN_SAMPLES, SEQ_LENGTH], dtype=np.int32)\n    y = np.full(shape=[N_TRAIN_SAMPLES, MAX_ANNOTATIONS, 2], fill_value=-1, dtype=np.int16)\n    for idx, train_idx in enumerate(tqdm(train.index.unique())):\n        # Define your training patient note index herer\n        if debug:\n            train_idx = (0, 82)\n        \n        # Patient Note\n        patient_note = patient_notes.loc[train_idx, 'pn_history_clean']\n        # Character Window of Character Removed/Added during patient note correction\n        n_char_window = patient_notes.loc[train_idx, 'n_char_window']\n        if debug:\n            print(f'n_chars_removed: {n_char_window}')\n            print(f'patient_note: {patient_note}')\n        \n        # Tokenize Patient Note\n        tokens = tokenize(patient_note)\n        \n        input_ids = np.array(tokens['input_ids'], dtype=np.int32)\n        offset_mapping = tokens['offset_mapping']\n        \n        X[idx] = input_ids\n        \n        # Get Annotation Mask\n        annotation_labels = []\n        for row in train.loc[train_idx].itertuples(index=False, name='Pandas'):\n            annotations = row.annotation_list\n            location_start_offsets = row.location_start_offset\n            feature_num = row.feature_num_ordinal\n            \n            if debug:\n                print('\\n', f'annotations: {annotations}, location_start_offsets: {location_start_offsets}')\n                \n            for ann_idx, (ann, start_offset) in enumerate(zip(annotations, location_start_offsets)):\n                if debug:\n                    print(f'ann: {ann}, start_offset: {start_offset}')\n                \n                token_indices = get_token_indices(patient_note, ann, offset_mapping, start_offset, n_char_window)\n                if debug:\n                    print(f'token_indices: {token_indices}, decoded: {tokenizer.decode(input_ids[token_indices])}')\n                    \n                for t in token_indices:\n                    annotation_labels.append((t, feature_num))\n                \n                if debug:\n                    decoded = tokenizer.decode(input_ids[token_indices])\n                    if ann != decoded and ann not in decoded:\n                        print(error_c, train_idx, ann, '|', decoded, f' | same: {ann==decoded}', input_ids[token_indices])\n                        error_c += 1\n                        if '<pad>' in decoded:\n                            return patient_note, ann, offset_mapping, start_offset\n                        \n        # Add Sorted Annotations\n        unique_sorted_annotations = np.unique(np.array(sorted(annotation_labels, key=lambda tup: tup[0])), axis=0)\n        y[idx, :len(unique_sorted_annotations)] = unique_sorted_annotations\n        \n        if debug:\n            return input_ids, offset_mapping\n        \n    return X, y\n\nX, y = get_x_y(debug=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:16.175307Z","iopub.execute_input":"2022-02-18T18:04:16.175515Z","iopub.status.idle":"2022-02-18T18:04:24.642302Z","shell.execute_reply.started":"2022-02-18T18:04:16.17549Z","shell.execute_reply":"2022-02-18T18:04:24.641437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save X and y\nnp.save('./X.npy', X)\nnp.save('./y.npy', y)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:24.643993Z","iopub.execute_input":"2022-02-18T18:04:24.644255Z","iopub.status.idle":"2022-02-18T18:04:24.652035Z","shell.execute_reply.started":"2022-02-18T18:04:24.644223Z","shell.execute_reply":"2022-02-18T18:04:24.651127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target Label Validation\n\nThis next function visualizes the assigned labels to check whether this whole notebook produces the desired output","metadata":{}},{"cell_type":"code","source":"class bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:24.653237Z","iopub.execute_input":"2022-02-18T18:04:24.653639Z","iopub.status.idle":"2022-02-18T18:04:24.660755Z","shell.execute_reply.started":"2022-02-18T18:04:24.653609Z","shell.execute_reply":"2022-02-18T18:04:24.659772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for label_idx, train_idx in enumerate(train.index.unique()[:10]):\n    # Print patient Note Clean\n    print(f'===== PATIENT NOTE ===== | {train_idx}')\n    patient_note = patient_notes.loc[train_idx, 'pn_history_clean']\n    print(patient_note)\n    \n    # Print Train\n    print('===== ANNOTATION LABELS =====')\n    display(train.loc[train_idx].sort_values('location_start')[['annotation_list', 'location']])\n    \n    # Print Train\n    print('===== TARGET ANNOTATION LABELS =====')\n    X_train_idx = X[label_idx]\n    y_train_idx = y[label_idx]\n    \n    # Convert Indices to Dense Tensor\n    y_train_idx = tf.constant(y_train_idx, dtype=tf.int64)\n    idxs = tf.math.reduce_any(y_train_idx > -1, axis=1)\n    idxs = tf.gather_nd(y_train_idx, tf.where(idxs))\n\n    sp = tf.SparseTensor(indices=idxs, values=tf.ones(shape=len(idxs)), dense_shape=[SEQ_LENGTH, 143])\n    y_train_idx = tf.sparse.to_dense(sp).numpy().sum(axis=1)\n    \n    y_train_annotations = []\n    c = 0\n    annotation = []\n    patient_note_colored = ''\n    # Loop over all tokens and labels\n    for x_token, y_label in zip(X_train_idx, y_train_idx):\n        # Token 0,1 and 2 are start/end/pad tokens, ignore them\n        if x_token > 2:\n            # Annoated token, print it green!\n            if y_label == 1:\n                patient_note_colored += f'{bcolors.FAIL}{tokenizer.decode(x_token)}{bcolors.ENDC} '\n            # Unannotated, simply print\n            else:\n                patient_note_colored += tokenizer.decode(x_token) + ' '\n            \n        # Add token to annoated list\n        if y_label == 1:\n            annotation.append(x_token)\n        elif y_label == 0 and len(annotation) > 0:\n            print(tokenizer.decode(annotation))\n            annotation.clear()\n            \n    print('\\n')\n    print(patient_note_colored)\n    print('\\n')\n            \n    print('\\n', '=' * 50, '\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:24.662072Z","iopub.execute_input":"2022-02-18T18:04:24.662297Z","iopub.status.idle":"2022-02-18T18:04:24.956161Z","shell.execute_reply.started":"2022-02-18T18:04:24.662269Z","shell.execute_reply":"2022-02-18T18:04:24.955362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Train with Features","metadata":{}},{"cell_type":"code","source":"# FEATURES\nfeatures.to_pickle('features.pkl')\n\n# PATIENT NOTES\npatient_notes.to_pickle('patient_notes.pkl')\n\n# TRAIN\ntrain.to_pickle('train.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:04:24.957706Z","iopub.execute_input":"2022-02-18T18:04:24.957935Z","iopub.status.idle":"2022-02-18T18:04:25.110612Z","shell.execute_reply.started":"2022-02-18T18:04:24.957909Z","shell.execute_reply":"2022-02-18T18:04:25.109686Z"},"trusted":true},"execution_count":null,"outputs":[]}]}