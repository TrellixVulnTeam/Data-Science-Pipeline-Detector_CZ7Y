{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello Fellow Kagglers,\n\nThis notebook demonstrates the inference process by predicting the features each token belongs to.\n\nThe inference process is structured as follows:\n\n1) For each token, predict the features it belongs to by thresholding the probability, resulting in pairs of tokens and features: i.e. token 42 belongs to feature 10,20 and 25\n\n2) For each pair, find the character location in the patient note: i.e. token 42 decodes to \"doctor\" and refers to patient note character span 100:106\n\n3) Group the tokens that belong to a single feature and produce the location spans by grouping consecutive tokens: i.e. character location 1:5 and 6:10 will result in location span 1:10\n\n[Training Notebook](https://www.kaggle.com/markwijkhuizen/nbme-albert-large-training-tpu)\n\n[Preprocessing Notebook](https://www.kaggle.com/markwijkhuizen/nbme-preprocessing-albert)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn import metrics\n\nfrom tqdm.notebook import tqdm\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom transformers import PreTrainedTokenizerFast, TFAlbertModel, AlbertConfig\nfrom sklearn.model_selection import train_test_split\n\nimport re\nimport os\nimport random\nimport math\nimport pickle\n\ntqdm.pandas()\n\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:58:53.126736Z","iopub.execute_input":"2022-02-27T14:58:53.127079Z","iopub.status.idle":"2022-02-27T14:59:02.148422Z","shell.execute_reply.started":"2022-02-27T14:58:53.126965Z","shell.execute_reply":"2022-02-27T14:59:02.147474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Token Input Length of AlBERT Model\nSEQ_LENGTH = 512\n# Global Random Seed\nSEED = 42\n# Models List\nMODELS = []","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:59:02.150637Z","iopub.execute_input":"2022-02-27T14:59:02.151247Z","iopub.status.idle":"2022-02-27T14:59:02.162696Z","shell.execute_reply.started":"2022-02-27T14:59:02.151205Z","shell.execute_reply":"2022-02-27T14:59:02.161792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/features.csv')\n\n# Add Ordinal Encoding\nfeatures['feature_num_ordinal'] = features['feature_num'].astype('category').cat.codes\n\nN_LABELS = len(features)\nprint(f'N_LABELS: {N_LABELS}')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:59:02.166974Z","iopub.execute_input":"2022-02-27T14:59:02.169094Z","iopub.status.idle":"2022-02-27T14:59:02.208699Z","shell.execute_reply.started":"2022-02-27T14:59:02.16905Z","shell.execute_reply":"2022-02-27T14:59:02.207752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# Inference does not allow internet connection, define AlBERT model manually\nalbert_xxlarge_config = AlbertConfig(\n  hidden_size = 4096,\n  intermediate_size = 16384,\n  max_position_embeddings = 512,\n  model_type = 'albert',\n  num_attention_heads = 64,\n)\n\n# Inference does not allow internet connection, define AlBERT model manually\nalbert_base_config = AlbertConfig(\n  hidden_size = 768,\n  intermediate_size = 3072,\n  max_position_embeddings = 512,\n  model_type = 'albert',\n  num_attention_heads = 12,\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:59:02.213109Z","iopub.execute_input":"2022-02-27T14:59:02.215869Z","iopub.status.idle":"2022-02-27T14:59:02.224414Z","shell.execute_reply.started":"2022-02-27T14:59:02.215825Z","shell.execute_reply":"2022-02-27T14:59:02.223532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_albert_model(file_name, config):\n    # Input Layer\n    input_ids = tf.keras.layers.Input(shape = (SEQ_LENGTH), dtype=tf.int32, name='input_ids')\n    attention_mask = tf.keras.layers.Input(shape=SEQ_LENGTH, dtype=tf.int32, name='attention_mask')\n\n    # AlBERT Model\n    albert = TFAlbertModel(config)\n\n    # Get the last hidden state\n    last_hidden_state = albert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n\n    # Dropout Layer\n    do = tf.keras.layers.Dropout(0.00, name='dropout')(last_hidden_state)\n\n    # Output Layer gives probabilities of each token to belong to each feature\n    output = tf.keras.layers.Dense(N_LABELS, activation='sigmoid', name='head/classifier')(do)\n\n    # Define Model\n    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=[output])\n    \n    # Load Weights\n    if file_name is None:\n        model.load_weights('/kaggle/input/nbme-albert-large-training-tpu-dataset/model.h5')\n    else:\n        model.load_weights(f'/kaggle/input/nbme-albert-model-assemble/{file_name}.h5')\n    \n    # Append to Models List\n    MODELS.append(model)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:59:47.015363Z","iopub.execute_input":"2022-02-27T14:59:47.015704Z","iopub.status.idle":"2022-02-27T14:59:47.024986Z","shell.execute_reply.started":"2022-02-27T14:59:47.015674Z","shell.execute_reply":"2022-02-27T14:59:47.023717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clear Backend\ntf.keras.backend.clear_session()\n\n# enable XLA optmizations\ntf.config.optimizer.set_jit(True)\n\nalbert_xxlarge_v18_model = get_albert_model(None, albert_base_config)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:59:47.297904Z","iopub.execute_input":"2022-02-27T14:59:47.299768Z","iopub.status.idle":"2022-02-27T15:00:00.315348Z","shell.execute_reply.started":"2022-02-27T14:59:47.299723Z","shell.execute_reply":"2022-02-27T15:00:00.314344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show Models\nfor m in MODELS:\n    print(m.summary())\n    print('\\n' * 3)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:00.319181Z","iopub.execute_input":"2022-02-27T15:00:00.319395Z","iopub.status.idle":"2022-02-27T15:00:00.331316Z","shell.execute_reply.started":"2022-02-27T15:00:00.319368Z","shell.execute_reply":"2022-02-27T15:00:00.330329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean Text","metadata":{}},{"cell_type":"code","source":"patient_notes = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv')\n\n# Set Case Number and Patient Number as Index for Convenient Access\npatient_notes = patient_notes.set_index(['case_num', 'pn_num'])\n\npatient_notes['pn_history_clean'] = patient_notes['pn_history'].str.lower()\n\ndisplay(patient_notes.head())\n\ndisplay(patient_notes.info())","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:05.350295Z","iopub.execute_input":"2022-02-27T15:00:05.350586Z","iopub.status.idle":"2022-02-27T15:00:06.214257Z","shell.execute_reply.started":"2022-02-27T15:00:05.350557Z","shell.execute_reply":"2022-02-27T15:00:06.213139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize","metadata":{}},{"cell_type":"code","source":"# Load saved Tokenizer from preprocessing notebook\ntokenizer = PreTrainedTokenizerFast.from_pretrained('../input/nbme-preprocessing-albert-public/tokenizer')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:06.216343Z","iopub.execute_input":"2022-02-27T15:00:06.217188Z","iopub.status.idle":"2022-02-27T15:00:06.388921Z","shell.execute_reply.started":"2022-02-27T15:00:06.217155Z","shell.execute_reply":"2022-02-27T15:00:06.387896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function tokenize the text according to a AlBERT model tokenizer\ndef tokenize(note):\n    return tokenizer(\n            note,\n            padding = 'max_length',\n            truncation = True,\n            max_length = SEQ_LENGTH,\n            return_offsets_mapping = True,\n        )","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:06.755655Z","iopub.execute_input":"2022-02-27T15:00:06.755908Z","iopub.status.idle":"2022-02-27T15:00:06.760919Z","shell.execute_reply.started":"2022-02-27T15:00:06.75588Z","shell.execute_reply":"2022-02-27T15:00:06.759937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sample Submission","metadata":{}},{"cell_type":"code","source":"# Let's take a look at the sample submission\nsample_submission = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/sample_submission.csv')\n\ndisplay(sample_submission)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:07.867939Z","iopub.execute_input":"2022-02-27T15:00:07.868514Z","iopub.status.idle":"2022-02-27T15:00:07.891691Z","shell.execute_reply.started":"2022-02-27T15:00:07.868466Z","shell.execute_reply":"2022-02-27T15:00:07.890914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference Helpers","metadata":{}},{"cell_type":"code","source":"def correct_prediction(t_dec_prev, t_dec, t_dec_next, row_feature_num, prob):    \n    if row_feature_num == 70:\n        if t_dec in ['ms']:\n            return 1\n    elif row_feature_num == 107:\n        if re.fullmatch('\\d{1}', t_dec) or t_dec in ['months', 'mot', 'nh', 's']:\n            return 1\n    elif row_feature_num == 103:\n        if t_dec in ['unprotected', 'sex']:\n            return 1\n    elif row_feature_num == 92:\n        if t_dec == 'as' and t_dec_next == 'tham':\n            return 1\n        elif t_dec_prev == 'as' and t_dec == 'tham':\n            return 1\n    elif row_feature_num == 93:\n        if prob > 0.01 and t_dec in ['chest', 'pain']:\n            return 1\n        \n    # Default to return original pred\n    return prob","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:08.448773Z","iopub.execute_input":"2022-02-27T15:00:08.449104Z","iopub.status.idle":"2022-02-27T15:00:08.459798Z","shell.execute_reply.started":"2022-02-27T15:00:08.449039Z","shell.execute_reply":"2022-02-27T15:00:08.456479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# This is the threshold that gave the best F1 score, determined in the training notebook\nTHRESHOLD = 0.50\nprint(f'THRESHOLD: {THRESHOLD:.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:08.746242Z","iopub.execute_input":"2022-02-27T15:00:08.74645Z","iopub.status.idle":"2022-02-27T15:00:08.752823Z","shell.execute_reply.started":"2022-02-27T15:00:08.746425Z","shell.execute_reply":"2022-02-27T15:00:08.751717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/test.csv')\n\ntest['feature_num_ordinal'] = features.set_index('feature_num').loc[test['feature_num'], 'feature_num_ordinal'].values\n\ntest = test.set_index(['case_num', 'pn_num'])\n\ndisplay(test.head(10))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:08.877856Z","iopub.execute_input":"2022-02-27T15:00:08.878238Z","iopub.status.idle":"2022-02-27T15:00:08.902612Z","shell.execute_reply.started":"2022-02-27T15:00:08.878193Z","shell.execute_reply":"2022-02-27T15:00:08.901613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Token Feature Probabilities\n\nIdentify tokens which belong to a feature","metadata":{}},{"cell_type":"code","source":"pred_df = []\n\n# Loop over all test rows\nfor group_idx, group in tqdm(test.groupby(['case_num', 'pn_num'])):\n    # Patient Note\n    pn_history_clean = patient_notes.loc[group_idx, 'pn_history_clean']\n    \n    # Tokenize Patient Note\n    tokens = tokenize(pn_history_clean)\n    \n    # Token Properties\n    input_ids = tokens['input_ids']\n    attention_mask = tokens['attention_mask']\n    offset_mapping = tokens['offset_mapping']\n    \n    # Probabilities of each token belonging to each feature\n    y_pred = np.zeros(shape=[N_LABELS, SEQ_LENGTH], dtype=np.float32)\n    \n    for m in MODELS:\n        y_pred += m.predict_on_batch({\n                'input_ids': np.array([input_ids]),\n                'attention_mask': np.array([attention_mask]),\n            }).squeeze().T / len(MODELS)\n    \n    # Iterate over all features\n    for row_id, row_feature_num in group[['id', 'feature_num_ordinal']].itertuples(index=False, name=None):\n        annotation_found = False\n        \n        # Prediction per Feature Number\n        y_pred_row = y_pred[row_feature_num]\n        \n        om_pred = []\n        # Iterate over all offset mappings, input tokens and prediction probabilities\n        for idx, (om, t, prob) in enumerate(zip(offset_mapping, input_ids, y_pred_row)):\n            # Decode Token\n            t_dec = tokenizer.decode(t)\n            \n            t_dec_prev = tokenizer.decode(input_ids[idx - 1]) if idx > 0 else None\n            t_dec_next = tokenizer.decode(input_ids[idx + 1]) if idx < len(input_ids) - 1 else None\n            prob = correct_prediction(t_dec_prev, t_dec, t_dec_next, row_feature_num, prob)\n            \n            # Minimum prediction threshold and token should not be utlity token (START, END, PAD etc.)\n            if prob > THRESHOLD and len(t_dec) > 0 and t > 4 :\n                annotation_found = True\n                pred_df.append({\n                        'row_id': row_id,\n                        'om': om,\n                        't': t,\n                        't_dec': t_dec,\n                        'group_idx': group_idx,\n                    })\n                \n        # Add Empty Annotation if no annotation is found\n        if not annotation_found:\n            pred_df.append({\n                'row_id': row_id,\n                'om': (-1,-1),\n                't': -1,\n                't_dec': chr(0),\n                'group_idx': group_idx,\n            })","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:09.202242Z","iopub.execute_input":"2022-02-27T15:00:09.202518Z","iopub.status.idle":"2022-02-27T15:00:16.312953Z","shell.execute_reply.started":"2022-02-27T15:00:09.202491Z","shell.execute_reply":"2022-02-27T15:00:16.311838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show Prediction DataFrame\npred_df = pd.DataFrame.from_dict(pred_df)\ndisplay(pred_df.head(10))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:16.315281Z","iopub.execute_input":"2022-02-27T15:00:16.316446Z","iopub.status.idle":"2022-02-27T15:00:16.33984Z","shell.execute_reply.started":"2022-02-27T15:00:16.316371Z","shell.execute_reply":"2022-02-27T15:00:16.338563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decode Predictions\n\nAssigns the patient note location of all predicted tokens","metadata":{}},{"cell_type":"code","source":"def find_all(a, b, offset=0):\n    res = []\n    # Find Ignoring Case\n    start_idx = a.lower().find(b.lower())\n    if start_idx != -1:\n        return [offset + start_idx] + find_all(a[start_idx + len(b):], b, offset=offset + start_idx + len(b))\n    else:\n        return []","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:16.341208Z","iopub.execute_input":"2022-02-27T15:00:16.342396Z","iopub.status.idle":"2022-02-27T15:00:16.3538Z","shell.execute_reply.started":"2022-02-27T15:00:16.342352Z","shell.execute_reply":"2022-02-27T15:00:16.35265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Returns the character position in the patient note\ndef get_char_pos(row, return_str):\n    annotation = row['t_dec']\n    start_ann, end_ann = row['om']\n    if start_ann == -1:\n        if return_str:\n             return chr(0)\n        else:\n            return (-1, -1)\n    \n    patient_note = patient_notes.loc[row['group_idx'], 'pn_history']\n    \n    starts_in_patient_note = find_all(patient_note, annotation)\n    \n    for start in starts_in_patient_note:\n        end = start + len(annotation)\n        if start >= start_ann - 1 and end <= end_ann + 1 and end <= len(patient_note):\n            if return_str:\n                 return patient_note[start:end]\n            else:\n                return start, end\n    \npred_df['om_original'] = pred_df.progress_apply(get_char_pos, axis=1, return_str=False)\npred_df['om_original_str'] = pred_df.progress_apply(get_char_pos, axis=1, return_str=True)\n\ndisplay(pred_df.head(10))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:16.356962Z","iopub.execute_input":"2022-02-27T15:00:16.357304Z","iopub.status.idle":"2022-02-27T15:00:16.491973Z","shell.execute_reply.started":"2022-02-27T15:00:16.357262Z","shell.execute_reply":"2022-02-27T15:00:16.490848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission Location\n\nGroups token locations together","metadata":{}},{"cell_type":"code","source":"submission_rows = []\n\nfor group_idx, group in tqdm(pred_df.groupby('row_id')):\n    start_prev = np.NINF\n    end_prev = np.NINF\n    location = ''\n    \n    for start, end in group['om_original']:\n        # Previous token also belongs to location, increase end location\n        # i.e. 3:5 followed by 6:10 will have start 3 and end will be increased from 5 -> 10\n        if end_prev + 1 >= start and start <= end_prev + 2:\n            end_prev = end\n        else:\n            # Previous token does not belong to location, add current location\n            if end_prev != np.NINF:\n                # After first location span is added, following location spans are delimited with a \";\"\n                if len(location) > 0:\n                    location += f';{start_prev} {end_prev}'\n                # First location span has no \";\"\n                else:\n                    location += f'{start_prev} {end_prev}'\n            # First location span, set start and end\n            if start > -1 and end > -1:\n                start_prev = start\n                end_prev = end\n            \n    # Add last location\n    if end_prev > -1:\n        if len(location) > 0:\n            location += f';{start_prev} {end_prev}'\n        else:\n            location += f'{start_prev} {end_prev}'\n        \n    submission_rows.append({ 'id': group_idx, 'location': location })","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:16.493724Z","iopub.execute_input":"2022-02-27T15:00:16.494158Z","iopub.status.idle":"2022-02-27T15:00:16.551491Z","shell.execute_reply.started":"2022-02-27T15:00:16.494113Z","shell.execute_reply":"2022-02-27T15:00:16.550494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Create submission DataFrame\nsubmission = pd.DataFrame.from_dict(submission_rows)\n\ndisplay(submission.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:16.55295Z","iopub.execute_input":"2022-02-27T15:00:16.553884Z","iopub.status.idle":"2022-02-27T15:00:16.567747Z","shell.execute_reply.started":"2022-02-27T15:00:16.553837Z","shell.execute_reply":"2022-02-27T15:00:16.56662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save submission as CSV\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:00:16.569701Z","iopub.execute_input":"2022-02-27T15:00:16.570278Z","iopub.status.idle":"2022-02-27T15:00:16.578514Z","shell.execute_reply.started":"2022-02-27T15:00:16.570237Z","shell.execute_reply":"2022-02-27T15:00:16.577078Z"},"trusted":true},"execution_count":null,"outputs":[]}]}