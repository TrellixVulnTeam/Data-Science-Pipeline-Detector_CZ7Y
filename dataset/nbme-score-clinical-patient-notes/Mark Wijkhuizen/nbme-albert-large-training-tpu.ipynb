{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello Fellow Kagglers,\n \nThis notebook demonstrates the training process for the NBME - Score Clinical Patient Notes competition using the parameter efficient AlBERT Model. The model is trained on a 420TFLOPS TPU making the training process a matter of minutes.\n\nAlBERT Paper: [ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS](https://arxiv.org/pdf/1909.11942.pdf)\n\nAlBERT GitHub: [google-research/albert](https://github.com/google-research/albert)\n\nAlBERT Hugging Face: [ALBERT - Hugging Face](https://huggingface.co/docs/transformers/model_doc/albert)\n\nThe model is trained as a token classiffier. For each token a probability is predicted for that token to be an annotation.\n\nA two step training process is used where first only the classifier fully connected layer is trained, as this layer is initialized with random weights. Secondly the whole model, including the pretrained AlBERT model, is fine tuned.\n\nThis notebook first defines the model, followed by the data pipeline. Next the actual training process is done followed by a thorough validation performance analysis.\n\n[Preprocessing Notebook](https://www.kaggle.com/markwijkhuizen/nbme-preprocessing-albert)\n\n[Inference Notebook](https://www.kaggle.com/markwijkhuizen/nbme-albert-inference-public)\n\n[Extra Training Data](https://www.kaggle.com/markwijkhuizen/nbme-albert-extra-training-data-public)\n\n**V2**\n* Per token feature prediction having a [Number of Tokens, Number of Features] output, predicting the probability of each token to belong to each feature\n* [SigmoidFocalCrossEntropy](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/SigmoidFocalCrossEntropy), special loss for high class inbalance. Of the label just 0.061% is positive (1). The loss of each prediction is scaled by $(1-p)^\\gamma$ for positive labels(1) and $p^\\gamma$ for  negative labels(0). Easy negative examples will quickly have a near 0 prediction, making the loss practically zero and thereby taking the easy negative example not into account when updating the loss. With the $\\gamma$ parameter the scaling can be further adapted. For more details see the [Paper](https://arxiv.org/pdf/1708.02002.pdf)\n\n**V8**\n* Added clipnorm to get consistent optimization steps, solves loss spikes and slightly improves validation F1 score","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom sklearn import metrics\n\nfrom tqdm.notebook import tqdm\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom transformers import TFAlbertModel\nfrom sklearn.model_selection import train_test_split\n\nimport re\nimport os\nimport random\nimport math\nimport time\nimport sys\n\ntqdm.pandas()\n\nAUTO = tf.data.experimental.AUTOTUNE\n\nprint(f'Python Version: {sys.version}')\nprint(f'Tensorflow Version: {tf.__version__}')\nprint(f'Tensorflow Keras Version: {tf.keras.__version__}')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:41.838614Z","iopub.execute_input":"2022-02-27T12:28:41.839339Z","iopub.status.idle":"2022-02-27T12:28:41.850497Z","shell.execute_reply.started":"2022-02-27T12:28:41.839283Z","shell.execute_reply":"2022-02-27T12:28:41.849648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seed everything for deterministic behaviour, however TPU's are generally undeterministic by definition\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nSEED = 42\nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:41.851932Z","iopub.execute_input":"2022-02-27T12:28:41.852796Z","iopub.status.idle":"2022-02-27T12:28:41.867385Z","shell.execute_reply.started":"2022-02-27T12:28:41.852711Z","shell.execute_reply":"2022-02-27T12:28:41.866563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The Token Input length for the AlBERT Model\nSEQ_LENGTH = 512\n# A TPU V3-8 has 8 computing cores, the global batch size will be 1/16 x 8 = 8/128\nBATCH_SIZE_BASE = 4\nBATCH_SIZE_BASE_EXTRA = 16\n\n# Different Training Dataset\n# 'train': use the annotated 1000 training samples\n# 'train_extra': use both annotated and soft labelled samples\nTRAIN_MODE = 'train_extra'\n\n# AlBERT Version\nALBERT_VERSION = 'base'\n\n# Number of Test Samples to Use (0 or 100)\nN_TEST_SAMPLES = 0\n\n# Epsilon Value\nEPSILON = tf.keras.backend.epsilon()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:41.868467Z","iopub.execute_input":"2022-02-27T12:28:41.868922Z","iopub.status.idle":"2022-02-27T12:28:41.879903Z","shell.execute_reply.started":"2022-02-27T12:28:41.868886Z","shell.execute_reply":"2022-02-27T12:28:41.879151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Number of Label","metadata":{}},{"cell_type":"code","source":"features = pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/features.csv')\n\nN_LABELS = len(features)\nprint(f'N_LABELS: {N_LABELS}')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:41.881121Z","iopub.execute_input":"2022-02-27T12:28:41.881989Z","iopub.status.idle":"2022-02-27T12:28:41.899766Z","shell.execute_reply.started":"2022-02-27T12:28:41.881941Z","shell.execute_reply":"2022-02-27T12:28:41.898783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hardware Configuration","metadata":{}},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', TPU.master())\nexcept ValueError:\n    print('Running on GPU')\n    TPU = None\n\nif TPU:\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nN_REPLICAS = strategy.num_replicas_in_sync\n# Number of computing cores, is 8 for a TPU V3-8\nprint(f'N_REPLICAS: {N_REPLICAS}')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:41.901865Z","iopub.execute_input":"2022-02-27T12:28:41.902175Z","iopub.status.idle":"2022-02-27T12:28:47.433142Z","shell.execute_reply.started":"2022-02-27T12:28:41.902145Z","shell.execute_reply":"2022-02-27T12:28:47.432479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# F1 Score","metadata":{}},{"cell_type":"code","source":"def f1(y_true, y_pred):\n    y_true = tf.reshape(y_true, [-1, N_LABELS])\n    y_pred = tf.reshape(y_pred, [-1, N_LABELS])\n    \n    return f1_score(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:47.434259Z","iopub.execute_input":"2022-02-27T12:28:47.434773Z","iopub.status.idle":"2022-02-27T12:28:47.440258Z","shell.execute_reply.started":"2022-02-27T12:28:47.434738Z","shell.execute_reply":"2022-02-27T12:28:47.439249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class F1Score(tf.keras.metrics.Metric):\n    def __init__(self, name='f1', **kwargs):\n        super(F1Score, self).__init__(name=name, **kwargs)\n        with strategy.scope():\n            self.f1 = tfa.metrics.F1Score(num_classes=N_LABELS, average='micro', threshold=EPSILON)\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.reshape(y_true, [-1, N_LABELS])\n        y_pred = tf.reshape(y_pred, [-1, N_LABELS])\n        self.f1.update_state(y_true, y_pred)\n        \n    def reset_state(self):\n        self.f1.reset_state()\n    \n    def result(self):\n        return self.f1.result()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:47.441535Z","iopub.execute_input":"2022-02-27T12:28:47.441771Z","iopub.status.idle":"2022-02-27T12:28:47.453515Z","shell.execute_reply.started":"2022-02-27T12:28:47.441734Z","shell.execute_reply":"2022-02-27T12:28:47.452496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    # METRICS\n    model_metrics = [\n        tf.keras.metrics.Precision(name='precision', thresholds=EPSILON),\n        tf.keras.metrics.Recall(name='recall', thresholds=EPSILON),\n        F1Score(),\n    ]","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:47.456973Z","iopub.execute_input":"2022-02-27T12:28:47.457378Z","iopub.status.idle":"2022-02-27T12:28:47.552661Z","shell.execute_reply.started":"2022-02-27T12:28:47.457342Z","shell.execute_reply":"2022-02-27T12:28:47.551568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Returns the dropout rate of the model\ndef get_dropout_rate():\n    for layer in model.layers:\n        if layer.name == 'dropout':\n            return layer.rate","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:47.55386Z","iopub.execute_input":"2022-02-27T12:28:47.554077Z","iopub.status.idle":"2022-02-27T12:28:47.559322Z","shell.execute_reply.started":"2022-02-27T12:28:47.554052Z","shell.execute_reply":"2022-02-27T12:28:47.558401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_layers_trainable(train_strategy, alpha=0, gamma=1):\n    with strategy.scope():\n        for layer in model.layers:\n            if train_strategy == 'classifier':\n                layer.trainable = 'head/' in layer.name\n                # OPTIMIZER\n                model_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-3, clipnorm=1.0)\n                # LOSS\n                model_loss = tfa.losses.SigmoidFocalCrossEntropy(from_logits=True, alpha=alpha, gamma=gamma)\n                # No Dropout\n                if layer.name == 'dropout':\n                    layer.rate = 0.00\n            elif train_strategy == 'train_extra':\n                layer.trainable = True\n                # OPTIMIZER\n                model_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, clipnorm=1.0)\n                # LOSS\n                model_loss = tfa.losses.SigmoidFocalCrossEntropy(from_logits=True, alpha=alpha, gamma=gamma)\n                # 30% Dropout\n                if layer.name == 'dropout':\n                    layer.rate = 0.20\n            elif train_strategy == 'train':\n                layer.trainable = True\n                # OPTIMIZER\n                model_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, clipnorm=1.0)\n                # LOSS\n                model_loss = tfa.losses.SigmoidFocalCrossEntropy(from_logits=True, alpha=alpha, gamma=gamma)\n                # 30% Dropout\n                if layer.name == 'dropout':\n                    layer.rate = 0.20\n\n        model.compile(optimizer=model_optimizer, loss=model_loss, metrics=model_metrics)        ","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:47.562768Z","iopub.execute_input":"2022-02-27T12:28:47.563498Z","iopub.status.idle":"2022-02-27T12:28:47.57703Z","shell.execute_reply.started":"2022-02-27T12:28:47.563449Z","shell.execute_reply":"2022-02-27T12:28:47.575691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    # Clear Backend\n    tf.keras.backend.clear_session()\n\n    # enable XLA optmizations\n    tf.config.optimizer.set_jit(True)\n    \n    with strategy.scope():\n        # Input Layers\n        input_ids = tf.keras.layers.Input(shape = (SEQ_LENGTH), dtype=tf.int32, name='input_ids')\n        attention_mask = tf.keras.layers.Input(shape=SEQ_LENGTH, dtype=tf.int32, name='attention_mask')\n        \n        # AlBERT Model\n        albert = TFAlbertModel.from_pretrained(\n            f'albert-{ALBERT_VERSION}-v2',\n            output_hidden_states = True,\n            return_dict = True,\n        )\n\n        # Get the last hidden state\n        last_hidden_state = albert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n        \n        do = tf.keras.layers.Dropout(0.00, name='dropout')(last_hidden_state)\n        \n        # Custom Kernel Initializer\n        initializer = tf.keras.initializers.GlorotNormal(seed=SEED)\n        # Can be used to scale the random initialzied weights\n        initializer.scale = 1.0\n\n        # Classification layer\n        output = tf.keras.layers.Dense(N_LABELS, kernel_initializer=initializer, activation=None, name='head/classifier')(do)\n    \n        # Define Model\n        model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=[output])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:47.578824Z","iopub.execute_input":"2022-02-27T12:28:47.579127Z","iopub.status.idle":"2022-02-27T12:28:47.593844Z","shell.execute_reply.started":"2022-02-27T12:28:47.579088Z","shell.execute_reply":"2022-02-27T12:28:47.593115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:47.595566Z","iopub.execute_input":"2022-02-27T12:28:47.596223Z","iopub.status.idle":"2022-02-27T12:28:52.24527Z","shell.execute_reply.started":"2022-02-27T12:28:47.596158Z","shell.execute_reply":"2022-02-27T12:28:52.244154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:52.24672Z","iopub.execute_input":"2022-02-27T12:28:52.247166Z","iopub.status.idle":"2022-02-27T12:28:52.259852Z","shell.execute_reply.started":"2022-02-27T12:28:52.247134Z","shell.execute_reply":"2022-02-27T12:28:52.258674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:52.261193Z","iopub.execute_input":"2022-02-27T12:28:52.261479Z","iopub.status.idle":"2022-02-27T12:28:52.630396Z","shell.execute_reply.started":"2022-02-27T12:28:52.261448Z","shell.execute_reply":"2022-02-27T12:28:52.629305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Configuration","metadata":{}},{"cell_type":"code","source":"# Training configuration\nBATCH_SIZE = BATCH_SIZE_BASE * N_REPLICAS\nBATCH_SIZE_EXTRA = BATCH_SIZE_BASE_EXTRA * N_REPLICAS\n\nprint(f'BATCH_SIZE_BASE: {BATCH_SIZE_BASE}, BATCH SIZE: {BATCH_SIZE}, BATCH_SIZE_EXTRA: {BATCH_SIZE_EXTRA}')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:52.631885Z","iopub.execute_input":"2022-02-27T12:28:52.632727Z","iopub.status.idle":"2022-02-27T12:28:52.63853Z","shell.execute_reply.started":"2022-02-27T12:28:52.632675Z","shell.execute_reply":"2022-02-27T12:28:52.637429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.load('/kaggle/input/nbme-preprocessing-albert-public/X.npy')\ny = np.load('/kaggle/input/nbme-preprocessing-albert-public/y.npy')\n\nif N_TEST_SAMPLES == 0:\n    X_extra = np.load('/kaggle/input/nbme-albert-extra-training-data-public-dataset/X_extra.npy')\n    y_extra_indices = np.load('/kaggle/input/nbme-albert-extra-training-data-public-dataset/y_extra_indices.npy')\n    y_extra_values = np.load('/kaggle/input/nbme-albert-extra-training-data-public-dataset/y_extra_values.npy')\nelse:\n    X_extra = np.load('/kaggle/input/nbme-albert-extra-training-data-public-dataset/X_extra_no_val.npy')\n    y_extra_indices = np.load('/kaggle/input/nbme-albert-extra-training-data-public-dataset/y_extra_indices_no_val.npy')\n    y_extra_values = np.load('/kaggle/input/nbme-albert-extra-training-data-public-dataset/y_extra_values_no_val.npy')\n\nprint(f'X shape: {X.shape}, y shape: {y.shape}')\nprint(f'X_extra shape: {X_extra.shape}, y_extra_indices shape: {y_extra_indices.shape}, y_extra_indices shape: {y_extra_indices.shape}')\nprint(f'X dtype: {X.dtype}, y dtype: {y.dtype}')\nprint(f'X_extra dtype: {X_extra.dtype}, y_extra_indices dtype: {y_extra_indices.dtype}, y_extra_indices dtype: {y_extra_indices.dtype}')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:52.640247Z","iopub.execute_input":"2022-02-27T12:28:52.640631Z","iopub.status.idle":"2022-02-27T12:28:52.834802Z","shell.execute_reply.started":"2022-02-27T12:28:52.640593Z","shell.execute_reply":"2022-02-27T12:28:52.833944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label Distribution","metadata":{}},{"cell_type":"code","source":"y_1s = (y[:,:,0].flatten() > -1).sum()\ny_0s = len(y) * SEQ_LENGTH * N_LABELS - y_1s\ny_value_counts_df = pd.DataFrame({\n        'label': ['0', '1'],\n        'count': [y_0s, y_1s],\n    })\n\ndisplay(y_value_counts_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:52.836127Z","iopub.execute_input":"2022-02-27T12:28:52.836574Z","iopub.status.idle":"2022-02-27T12:28:52.851385Z","shell.execute_reply.started":"2022-02-27T12:28:52.836544Z","shell.execute_reply":"2022-02-27T12:28:52.850334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Distribution\nplt.figure(figsize=(8,8))\nplt.title('Label Distribution', size=24)\npd.DataFrame(y_value_counts_df)['count'].plot(kind='pie', autopct='%1.3f%%', textprops={'fontsize': 16})\nplt.ylabel('Label Distribution', size=18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:52.853123Z","iopub.execute_input":"2022-02-27T12:28:52.853426Z","iopub.status.idle":"2022-02-27T12:28:52.979672Z","shell.execute_reply.started":"2022-02-27T12:28:52.853395Z","shell.execute_reply":"2022-02-27T12:28:52.978871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if N_TEST_SAMPLES > 0:\n    # Train Test Split\n    test_size = N_TEST_SAMPLES / len(X)\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=SEED)\n    print(f'X_train shape: {X_train.shape}, X_val shape: {X_val.shape}')\n    print(f'y_train shape: {y_train.shape}, y_val shape: {y_val.shape}')\nelse:\n    X_train = X\n    y_train = y\n    print(f'X_train shape: {X_train.shape}, y_train shape: {y_train.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:52.981032Z","iopub.execute_input":"2022-02-27T12:28:52.981318Z","iopub.status.idle":"2022-02-27T12:28:52.988101Z","shell.execute_reply.started":"2022-02-27T12:28:52.981288Z","shell.execute_reply":"2022-02-27T12:28:52.987239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Utility Functions","metadata":{}},{"cell_type":"code","source":"# Simple function to benchmark the dataset\ndef benchmark_dataset(dataset, num_epochs=3, n_steps_per_epoch=100):\n    start_time = time.perf_counter()\n    bs = None\n    for epoch_num in range(num_epochs):\n        for idx, (inputs, labels) in enumerate(dataset.take(n_steps_per_epoch + 1)):\n            if bs is None:\n                bs = len(labels)\n            # Epoch Start Time\n            if idx == 0:\n                epoch_start = time.perf_counter()\n            else:\n                pass\n        epoch_t = time.perf_counter() - epoch_start\n        mean_step_t = round(epoch_t / n_steps_per_epoch * 1000, 1)\n        n_texts_per_s = int(1 / (mean_step_t / 1000) * bs)\n        print(f'epoch {epoch_num} took: {round(epoch_t, 2)} sec, mean step duration: {mean_step_t}ms, texts/s: {n_texts_per_s} (bs={bs})')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:52.98951Z","iopub.execute_input":"2022-02-27T12:28:52.989785Z","iopub.status.idle":"2022-02-27T12:28:53.007084Z","shell.execute_reply.started":"2022-02-27T12:28:52.989755Z","shell.execute_reply":"2022-02-27T12:28:53.006054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function()\ndef add_attention_mask(X_sample, y_sample):\n    input_ids_sample = X_sample['input_ids']\n    # Attention Mask\n    attention_mask_sample = tf.where(tf.math.equal(input_ids_sample, 0), x=0, y=1)\n    # Cast y_sample to float\n    y_sample_i = y_sample['indices']\n    y_sample_v = y_sample['values']\n    \n    y_sparse_idxs_int64 = tf.cast(y_sample_i, dtype=tf.int64)\n    y_idxs = tf.math.reduce_any(y_sparse_idxs_int64 > -1, axis=1)\n    y_idxs = tf.where(y_idxs)\n    y_indices = tf.gather_nd(y_sparse_idxs_int64, y_idxs)\n    y_values = tf.gather_nd(y_sample_v, y_idxs)\n\n\n    y_dense = tf.SparseTensor(indices=y_indices, values=y_values, dense_shape=[SEQ_LENGTH, N_LABELS])\n    y_dense = tf.sparse.to_dense(y_dense)\n    y_dense = tf.cast(y_dense, tf.float32)\n        \n    return { 'input_ids': input_ids_sample, 'attention_mask': attention_mask_sample }, y_dense","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:53.008738Z","iopub.execute_input":"2022-02-27T12:28:53.009125Z","iopub.status.idle":"2022-02-27T12:28:53.021849Z","shell.execute_reply.started":"2022-02-27T12:28:53.009088Z","shell.execute_reply":"2022-02-27T12:28:53.020739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataset(X, y_indices, shuffle_repeat, y_values=None, bs=BATCH_SIZE, dr=True):\n    if y_values is None:\n        y_values = np.ones(y_indices.shape[:2], dtype=np.int8)\n    \n    # Create dataset from numpy arrays\n    dataset = tf.data.Dataset.from_tensor_slices((\n        # Input\n        { 'input_ids': X },\n        # Label\n        { 'indices': y_indices, 'values': y_values },\n    ))\n    \n    if shuffle_repeat:\n        dataset = dataset.shuffle(len(X))\n        dataset = dataset.repeat()\n    \n    # Add Attention Mask\n    dataset = dataset.map(add_attention_mask, num_parallel_calls=AUTO, deterministic=False)\n    \n    # Prefetech to not map the whole dataset\n    dataset = dataset.prefetch(AUTO)\n    \n    # Batch Samples\n    dataset = dataset.batch(bs, drop_remainder=dr)\n    \n    # Always have a batch ready\n    dataset = dataset.prefetch(1)\n    \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:53.023351Z","iopub.execute_input":"2022-02-27T12:28:53.023584Z","iopub.status.idle":"2022-02-27T12:28:53.039998Z","shell.execute_reply.started":"2022-02-27T12:28:53.023558Z","shell.execute_reply":"2022-02-27T12:28:53.039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Dataset","metadata":{}},{"cell_type":"code","source":"if TRAIN_MODE == 'train':\n    print(f'Using Train Dataset With {len(X_train)} Samples')\n    # TRAIN DATASET\n    train_dataset = get_dataset(X_train, y_train, shuffle_repeat=True)\nelif TRAIN_MODE == 'train_extra':\n    print(f'Using Train Extra Dataset With {len(X_extra)} Samples')\n    # TRAIN EXTRA DATASET\n    train_dataset = get_dataset(X_extra, y_extra_indices, y_values=y_extra_values, shuffle_repeat=True, bs=BATCH_SIZE_EXTRA)\nelse:\n    raise Exception('Training Mode Invalid!!!')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:53.041521Z","iopub.execute_input":"2022-02-27T12:28:53.041763Z","iopub.status.idle":"2022-02-27T12:28:54.264178Z","shell.execute_reply.started":"2022-02-27T12:28:53.041736Z","shell.execute_reply":"2022-02-27T12:28:54.262951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example of a batch\ntrain_x, train_y = next(iter(train_dataset))\nprint(f'train_x keys: {list(train_x.keys())}')\nprint(f'train_x input ids shape: {train_x[\"input_ids\"].shape}')\nprint(f'train_x input ids dtype: {train_x[\"input_ids\"].dtype}')\nprint(f'train_y shape: {train_y.shape}, train_y dtype: {train_y.dtype}')\n\nprint('\\n===== Benchmarking Dataset =====')\nbenchmark_dataset(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:54.277653Z","iopub.execute_input":"2022-02-27T12:28:54.277944Z","iopub.status.idle":"2022-02-27T12:28:58.413342Z","shell.execute_reply.started":"2022-02-27T12:28:54.277915Z","shell.execute_reply":"2022-02-27T12:28:58.412302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation Dataset","metadata":{}},{"cell_type":"code","source":"if N_TEST_SAMPLES > 0:\n    # TRAIN DATASET\n    val_dataset = get_dataset(X_val, y_val, shuffle_repeat=False, bs=TEST_SIZE)\n\n    # Example of a batch\n    val_x, val_y = next(iter(val_dataset))\n    print(f'val_x keys: {list(val_x.keys())}')\n    print(f'val_x input ids shape: {val_x[\"input_ids\"].shape}')\n    print(f'val_x input ids dtype: {val_x[\"input_ids\"].dtype}')\n    print(f'val_y shape: {val_y.shape}, val_y dtype: {val_y.dtype}')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:58.414542Z","iopub.execute_input":"2022-02-27T12:28:58.414812Z","iopub.status.idle":"2022-02-27T12:28:58.422127Z","shell.execute_reply.started":"2022-02-27T12:28:58.414781Z","shell.execute_reply":"2022-02-27T12:28:58.420998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Weight Initialization Test\n\nThis is a sanity check for the model architecture, the output should be between 0 and 1. Several seeds are tried to get an initial bias towards 0, which is in line with the label distribution. This should speed up training. The output should also be roughly evenly distributed between 0-1, if all output is 0 or 1 this is a red flag for the model architecture/weight initialization.","metadata":{}},{"cell_type":"code","source":"if TRAIN_MODE == 'train':\n    TRAIN_STEPS_PER_EPOCH = len(X_train) // BATCH_SIZE\nelse:\n        TRAIN_STEPS_PER_EPOCH = len(X_extra) // BATCH_SIZE_EXTRA\n\nprint(f'TRAIN_STEPS_PER_EPOCH: {TRAIN_STEPS_PER_EPOCH}')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:58.423971Z","iopub.execute_input":"2022-02-27T12:28:58.424452Z","iopub.status.idle":"2022-02-27T12:28:58.4364Z","shell.execute_reply.started":"2022-02-27T12:28:58.424408Z","shell.execute_reply":"2022-02-27T12:28:58.435614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sanity check for output layer\noutputs = []\ntrain_dataset_iter = iter(get_dataset(X_train, y_train, False, bs=100))\nfor X_batch, _ in tqdm(train_dataset_iter, total=len(X_train) // 100):\n    output_batch = model.predict_on_batch(X_batch)\n    outputs += output_batch.flatten().tolist()\n\ndisplay(pd.Series(outputs, dtype=np.float32).describe(percentiles=[0.05, 0.10, 0.90, 0.95]).to_frame())\n\nplt.figure(figsize=(15,6))\nplt.title('Output Distribution', size=24)\npd.Series(outputs).plot(kind='hist', bins=16)\nplt.grid()\nplt.yticks(size=16)\nplt.xticks(size=16)\nplt.xlabel('Sigmoid Output', size=18)\nplt.ylabel('Count', size=18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:28:58.437498Z","iopub.execute_input":"2022-02-27T12:28:58.438217Z","iopub.status.idle":"2022-02-27T12:29:57.359289Z","shell.execute_reply.started":"2022-02-27T12:28:58.438167Z","shell.execute_reply":"2022-02-27T12:29:57.358321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Callbacks","metadata":{}},{"cell_type":"code","source":"if N_TEST_SAMPLES > 0:\n    # Checkpoint Callback, only save the best model weights with respect to the validation loss\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(f'model.h5', monitor='val_f1', save_best_only=True, save_weights_only=True, verbose=1, mode='max')\nelse:\n    # Monitor Training F1 when only using training data\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(f'model.h5', monitor='f1', save_best_only=True, save_weights_only=True, verbose=1, mode='max')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:29:57.360986Z","iopub.execute_input":"2022-02-27T12:29:57.361564Z","iopub.status.idle":"2022-02-27T12:29:57.368352Z","shell.execute_reply.started":"2022-02-27T12:29:57.361522Z","shell.execute_reply":"2022-02-27T12:29:57.367434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"N_EPOCHS_CLASSIFIER = 10 if TRAIN_MODE == 'train' else 1\nN_EPOCHS_ALL = 10 if TRAIN_MODE == 'train' else 8\nN_TOTAL_EPOCHS = N_EPOCHS_CLASSIFIER + N_EPOCHS_ALL\nprint(f'N_EPOCHS_CLASSIFIER: {N_EPOCHS_CLASSIFIER}, N_EPOCHS_ALL: {N_EPOCHS_ALL}')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:29:57.369469Z","iopub.execute_input":"2022-02-27T12:29:57.369972Z","iopub.status.idle":"2022-02-27T12:29:57.383424Z","shell.execute_reply.started":"2022-02-27T12:29:57.369937Z","shell.execute_reply":"2022-02-27T12:29:57.38258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classifier Only","metadata":{}},{"cell_type":"code","source":"# Set Only The Classification Layer Trainable\nset_layers_trainable('classifier')\n\n# Verify Learning Rate\nprint(f'Optimzier Learning Rate: {model.optimizer.learning_rate.numpy():.1e}')\nprint(f'Optimzier Epsilon: {model.optimizer.epsilon:.1e}')\nprint(f'Dropout Rate: {get_dropout_rate():.2f} \\n')\n\n# Verify we only train the head/classifier layer!\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:29:57.384987Z","iopub.execute_input":"2022-02-27T12:29:57.385512Z","iopub.status.idle":"2022-02-27T12:29:57.470385Z","shell.execute_reply.started":"2022-02-27T12:29:57.385468Z","shell.execute_reply":"2022-02-27T12:29:57.469395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Classifier Only, a decent performance can be achieved with just training 1025 neurons!\nhistory_classifier = model.fit(\n    train_dataset,\n    steps_per_epoch = TRAIN_STEPS_PER_EPOCH,\n    validation_data = val_dataset if N_TEST_SAMPLES > 0 else None,\n    epochs = N_EPOCHS_CLASSIFIER,\n    verbose = 1,\n    callbacks = [\n        checkpoint_callback,\n    ],\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:29:57.471829Z","iopub.execute_input":"2022-02-27T12:29:57.472123Z","iopub.status.idle":"2022-02-27T12:30:41.016216Z","shell.execute_reply.started":"2022-02-27T12:29:57.472088Z","shell.execute_reply":"2022-02-27T12:30:41.015261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tuning Whole Model","metadata":{}},{"cell_type":"code","source":"# Set Only The Classification Layer Trainable\nset_layers_trainable(TRAIN_MODE)\n\n# Verify Learning Rate\nprint(f'Optimzier Learning Rate: {model.optimizer.learning_rate.numpy():.1e}')\nprint(f'Optimzier Epsilon: {model.optimizer.epsilon:.1e}')\nprint(f'Dropout Rate: {get_dropout_rate():.2f} \\n')\n\n# Verify we only train the head/classifier layer!\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:30:41.017849Z","iopub.execute_input":"2022-02-27T12:30:41.018179Z","iopub.status.idle":"2022-02-27T12:30:41.102872Z","shell.execute_reply.started":"2022-02-27T12:30:41.018135Z","shell.execute_reply":"2022-02-27T12:30:41.101848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the whole model, thus also the AlBERT model\nhistory_whole_model = model.fit(\n    train_dataset,\n    steps_per_epoch = TRAIN_STEPS_PER_EPOCH,\n    validation_data = val_dataset if N_TEST_SAMPLES > 0 else None,\n    epochs = N_EPOCHS_ALL,\n    verbose = 1,\n    callbacks = [\n        checkpoint_callback,\n    ],\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:30:41.104104Z","iopub.execute_input":"2022-02-27T12:30:41.104392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Best Weights\nmodel.load_weights('model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training History","metadata":{}},{"cell_type":"code","source":"# Merge History\nHISTORY = {}\nh1 = history_classifier.history.items()\nh2 = history_whole_model.history.items()\nfor (k, v1), (_, v2) in zip(h1, h2):\n    HISTORY[k] = v1 + v2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_history_metric(metric, f_best=np.argmax, yscale='linear'):\n    x = np.arange(1, len(HISTORY[metric]) + 1)\n    y_train = HISTORY[metric]\n    plt.figure(figsize=(20, 8))\n    # TRAIN\n    plt.plot(x, y_train, color='tab:blue', lw=3, label='train')\n    plt.title(f'Training {metric}', fontsize=24, pad=10)\n    plt.ylabel(metric, fontsize=20, labelpad=10)\n    plt.xlabel('epoch', fontsize=20, labelpad=10)\n    plt.xticks([1] + np.arange(5, N_TOTAL_EPOCHS + 1, 5).tolist(), fontsize=16) # set tick step to 1 and let x axis start at 1\n    plt.yticks(fontsize=16)\n    plt.yscale(yscale)\n    \n    # Train Best Marker\n    x_best = f_best(y_train)\n    y_best = y_train[x_best]\n    plt.scatter(x_best + 1, y_best, color='purple', s=100, marker='o', label=f'train best: {y_best:.4f}')\n    \n    if N_TEST_SAMPLES > 0:\n        # VALIDATION\n        y_val = HISTORY[f'val_{metric}']\n        plt.plot(x, y_val, color='tab:orange', lw=3, label='validation')\n\n        # Validation Best Marker\n        x_best = f_best(y_val)\n        y_best = y_val[x_best]\n        plt.scatter(x_best + 1, y_best, color='red', s=100, marker='o', label=f'validation best: {y_best:.4f}')\n    \n    # Calssifier Part\n    plt.vlines(N_EPOCHS_CLASSIFIER + 1, *plt.ylim(), color='black', lw=3, linestyles='dashed', alpha=0.50, label='Training Classifier/Whole Model')\n\n    plt.grid()\n    plt.legend(prop={'size': 18})\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history_metric('loss', f_best=np.argmin, yscale='log')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history_metric('precision', f_best=np.argmax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history_metric('recall', f_best=np.argmax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history_metric('f1', f_best=np.argmax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Per Validation Sample Metrics\n\nThe validation metrics are means, you should always be careful with means!!! This next function plots the validation metrics for each individual sample to check for variability in performance. As can be observed the performance is quite consistent, giving confidence in the model. A red flag would be if the performance is split in two groups, one with near 0 metrics and one group with near perfect performance. This would indicate there are errors in for example the data processing.\n\nThese metrics exclude the start/end/pad token, resulting in worse performance as it is trivial for a model to learn that these tokens should result in 0.","metadata":{}},{"cell_type":"code","source":"def get_y_true_and_y_pred():\n    y_true = []\n    y_pred = []\n    val_metrics = {\n        'focal_crossentropy': [],\n        'precision': [],\n        'recall': [],\n        'f1': [],\n    }\n\n    for X_batch, y_true_batch in tqdm(get_dataset(X_val, y_val, False, bs=1)):\n        \n        y_pred_batch = model.predict_on_batch(X_batch)\n        y_pred_batch_sigmoid = tf.nn.sigmoid(y_pred_batch).numpy()\n        y_pred += y_pred_batch_sigmoid.flatten().tolist()\n        y_true += y_true_batch.numpy().flatten().tolist()\n\n        val_metrics['focal_crossentropy'].append(tfa.losses.SigmoidFocalCrossEntropy()(y_true_batch, y_pred_batch_sigmoid).numpy().mean())\n        val_metrics['precision'].append(tf.keras.metrics.Precision()(y_true_batch, y_pred_batch_sigmoid).numpy())\n        val_metrics['recall'].append(tf.keras.metrics.Recall()(y_true_batch, y_pred_batch_sigmoid).numpy())\n        val_metrics['f1'].append(F1Score()(y_true_batch, y_pred_batch).numpy())\n        \n    # Convert y to uint8 and y_pred to float32 to reduce memory usage\n    y_true = np.array(y_true, dtype=np.uint8)\n    y_pred = np.array(y_pred, dtype=np.float32)\n        \n    return y_true, y_pred, val_metrics\n\nif N_TEST_SAMPLES > 0:\n    y_true, y_pred, val_metrics = get_y_true_and_y_pred()\n    print(f'y_true shape: {y_true.shape}, y_pred shape: {y_pred.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Unflattened Predictions","metadata":{}},{"cell_type":"code","source":"if N_TEST_SAMPLES > 0:\n    y_true_tensor = y_true.reshape([len(X_val), SEQ_LENGTH, N_LABELS])\n    y_pred_tensor = y_pred.reshape([len(X_val), SEQ_LENGTH, N_LABELS])\n\n    np.save('y_true_tensor.npy', y_true_tensor)\n    np.save('y_pred_tensor.npy', y_pred_tensor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Per Sample Validation Analysis","metadata":{}},{"cell_type":"code","source":"if N_TEST_SAMPLES > 0:\n    for metric, values in val_metrics.items():\n        plt.figure(figsize=(15, 8))\n        plt.title(f'Validation {metric}', size=24)\n        pd.Series(values).plot(kind='hist', bins=16, color='tab:orange')\n\n        if metric != 'focal_crossentropy':\n            plt.xlim(0, 1)\n\n        plt.xticks(size=16)\n        plt.yticks(size=16)\n        plt.xlabel(metric, size=18)\n        plt.ylabel('Count', size=18)\n        plt.grid()\n        plt.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Precision/Recall Curve\n\n**Precision:** this can be thought of as number of hits when shooting. In a binary classification shooting means assigning a 1 to the token and hits is the ratio of the token actually being labelled as 1.\n\n**Recall:** this can be though of as the number of found targets. In a binary classification this is the ratio of tokens labelled as 1 which are actually predicted as 1.\n\nThe trade-off here is that generally whenever a high precision is achieved the model is oversecure, resulting in a low recall. Whenever a high recall is achieved the model is generally overconfident, resulting in a low precision.\n\nThe art is to find a threshold, the sigmoid output above which a token is classified as 1, which has both a high precision and a high recall. For this model this seems to be 0.60.","metadata":{}},{"cell_type":"code","source":"if N_TEST_SAMPLES > 0:\n    precision, recall, thresholds = metrics.precision_recall_curve(y_true, y_pred)\n    thresholds = np.concatenate(([0], thresholds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if N_TEST_SAMPLES > 0:\n    plt.figure(figsize=(15,8))\n    plt.plot(precision, recall, color='darkorange', label='Precision/Recall', linewidth=3)\n    plt.title('Precision/Recall Curve', size=24)\n    plt.xlabel('Precision', size=18)\n    plt.ylabel('Recall', size=18)\n    plt.xticks(size=16)\n    plt.yticks(size=16)\n    plt.grid()\n    plt.legend(prop={'size': 16})\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if N_TEST_SAMPLES > 0:\n    plt.figure(figsize=(15,8))\n    plt.plot(recall, thresholds,  color='darkorange', label='Recall/Threshold', linewidth=3)\n    plt.title('Threshold/Recall Curve', size=24)\n    plt.xlabel('Threshold', size=18)\n    plt.ylabel('Recall', size=18)\n    plt.xticks(size=16)\n    plt.yticks(size=16)\n    plt.grid()\n    plt.legend(prop={'size': 16})\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if N_TEST_SAMPLES > 0:\n    plt.figure(figsize=(15,8))\n    plt.plot(thresholds, precision,  color='darkorange', label='Precision/Threshold', linewidth=3)\n    plt.title('Threshold/Precision Curve', size=24)\n    plt.xlabel('Threshold', size=18)\n    plt.ylabel('Precision', size=18)\n    plt.xticks(size=16)\n    plt.yticks(size=16)\n    plt.grid()\n    plt.legend(prop={'size': 16})\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if N_TEST_SAMPLES > 0:\n    f1 = 2 * (precision * recall) / (precision + recall)\n    f1_best_threshold = thresholds[np.argmax(f1)]\n    f1_best_value = f1.max()\n    print(f'Threshold Best F1({f1_best_value:.3f}): {f1_best_threshold:.3f}')\n\n    plt.figure(figsize=(15,8))\n    plt.plot(thresholds, f1,  color='darkorange', label='Threshold/F1', linewidth=3)\n    plt.scatter(f1_best_threshold, f1_best_value, color='red', s=100, marker='o', label=f'F1 best: {f1.max():.3f}')\n    plt.title('Threshold/F1', size=24)\n    plt.xlabel('Threshold', size=18)\n    plt.ylabel('F1', size=18)\n    plt.xticks(size=16)\n    plt.yticks(size=16)\n    plt.grid()\n    plt.legend(prop={'size': 16})\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}