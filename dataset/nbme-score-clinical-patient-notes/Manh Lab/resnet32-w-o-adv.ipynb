{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/kuangliu/pytorch-cifar.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-28T10:32:35.134942Z","iopub.execute_input":"2022-03-28T10:32:35.135508Z","iopub.status.idle":"2022-03-28T10:32:36.65727Z","shell.execute_reply.started":"2022-03-28T10:32:35.135416Z","shell.execute_reply":"2022-03-28T10:32:36.656477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd pytorch-cifar","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:32:38.116449Z","iopub.execute_input":"2022-03-28T10:32:38.116724Z","iopub.status.idle":"2022-03-28T10:32:38.123292Z","shell.execute_reply.started":"2022-03-28T10:32:38.116694Z","shell.execute_reply":"2022-03-28T10:32:38.122607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile main.py\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport os\nimport argparse\n\nfrom models import *\nfrom utils import progress_bar\n\n\nparser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\nparser.add_argument('--lr', default=0.1, type=float, help='learning rate')\nparser.add_argument('--resume', '-r', action='store_true',\n                    help='resume from checkpoint')\nargs = parser.parse_args()\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbest_acc = 0  # best test accuracy\nstart_epoch = 0  # start from epoch 0 or last checkpoint epoch\n\n# Data\nprint('==> Preparing data..')\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntrainset = torchvision.datasets.CIFAR10(\n    root='./data', train=True, download=True, transform=transform_train)\ntrainloader = torch.utils.data.DataLoader(\n    trainset, batch_size=128, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(\n    root='./data', train=False, download=True, transform=transform_test)\ntestloader = torch.utils.data.DataLoader(\n    testset, batch_size=100, shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer',\n           'dog', 'frog', 'horse', 'ship', 'truck')\n\n# Model\nprint('==> Building model..')\n# net = VGG('VGG19')\nnet = ResNet18()\n# net = PreActResNet18()\n# net = GoogLeNet()\n# net = DenseNet121()\n# net = ResNeXt29_2x64d()\n# net = MobileNet()\n# net = MobileNetV2()\n# net = DPN92()\n# net = ShuffleNetG2()\n# net = SENet18()\n# net = ShuffleNetV2(1)\n# net = EfficientNetB0()\n# net = RegNetX_200MF()\n# net = SimpleDLA()\nnet = net.to(device)\nif device == 'cuda':\n    net = torch.nn.DataParallel(net)\n    cudnn.benchmark = True\n\nif args.resume:\n    # Load checkpoint.\n    print('==> Resuming from checkpoint..')\n    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n    checkpoint = torch.load('./checkpoint/ckpt.pth')\n    net.load_state_dict(checkpoint['net'])\n    best_acc = checkpoint['acc']\n    start_epoch = checkpoint['epoch']\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=args.lr,\n                      momentum=0.9, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n\n\n# Training\ndef train(epoch):\n    print('\\nEpoch: %d' % epoch)\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n\ndef test(epoch):\n    global best_acc\n    net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n    # Save checkpoint.\n    acc = 100.*correct/total\n    if acc > best_acc:\n        print('Saving..')\n        state = {\n            'net': net.state_dict(),\n            'acc': acc,\n            'epoch': epoch,\n        }\n        if not os.path.isdir('checkpoint'):\n            os.mkdir('checkpoint')\n        torch.save(state, './checkpoint/ckpt.pth')\n        best_acc = acc\n\n\nfor epoch in range(start_epoch, start_epoch+200):\n    train(epoch)\n    test(epoch)\n    scheduler.step()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:32:39.335761Z","iopub.execute_input":"2022-03-28T10:32:39.337596Z","iopub.status.idle":"2022-03-28T10:32:39.34563Z","shell.execute_reply.started":"2022-03-28T10:32:39.337561Z","shell.execute_reply":"2022-03-28T10:32:39.344771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python main.py","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:32:41.121722Z","iopub.execute_input":"2022-03-28T10:32:41.122272Z","iopub.status.idle":"2022-03-28T10:40:26.228538Z","shell.execute_reply.started":"2022-03-28T10:32:41.12223Z","shell.execute_reply":"2022-03-28T10:40:26.227704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/MadryLab/robustness.git\n%cd robustness","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:40:28.31977Z","iopub.execute_input":"2022-03-28T10:40:28.320485Z","iopub.status.idle":"2022-03-28T10:40:30.079786Z","shell.execute_reply.started":"2022-03-28T10:40:28.320447Z","shell.execute_reply":"2022-03-28T10:40:30.078932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:40:31.162188Z","iopub.execute_input":"2022-03-28T10:40:31.162612Z","iopub.status.idle":"2022-03-28T10:40:31.826417Z","shell.execute_reply.started":"2022-03-28T10:40:31.162579Z","shell.execute_reply":"2022-03-28T10:40:31.825567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile robustness/attacker.py\n\"\"\"\n**For most use cases, this can just be considered an internal class and\nignored.**\n\nThis module houses the :class:`robustness.attacker.Attacker` and\n:class:`robustness.attacker.AttackerModel` classes. \n\n:class:`~robustness.attacker.Attacker` is an internal class that should not be\nimported/called from outside the library.\n:class:`~robustness.attacker.AttackerModel` is a \"wrapper\" class which is fed a\nmodel and adds to it adversarial attack functionalities as well as other useful\noptions. See :meth:`robustness.attacker.AttackerModel.forward` for documentation\non which arguments AttackerModel supports, and see\n:meth:`robustness.attacker.Attacker.forward` for the arguments pertaining to\nadversarial examples specifically.\n\nFor a demonstration of this module in action, see the walkthrough\n\":doc:`../example_usage/input_space_manipulation`\"\n\n**Note 1**: :samp:`.forward()` should never be called directly but instead the\nAttackerModel object itself should be called, just like with any\n:samp:`nn.Module` subclass.\n\n**Note 2**: Even though the adversarial example arguments are documented in\n:meth:`robustness.attacker.Attacker.forward`, this function should never be\ncalled directly---instead, these arguments are passed along from\n:meth:`robustness.attacker.AttackerModel.forward`.\n\"\"\"\n\n\nimport torch as ch\nimport dill\nimport os\nif int(os.environ.get(\"NOTEBOOK_MODE\", 0)) == 1:\n    from tqdm import tqdm_notebook as tqdm\nelse:\n    from tqdm import tqdm\n\nfrom .tools import helpers\nfrom . import attack_steps\n\nSTEPS = {\n    'inf': attack_steps.LinfStep,\n    '2': attack_steps.L2Step,\n    'unconstrained': attack_steps.UnconstrainedStep,\n    'fourier': attack_steps.FourierStep,\n    'random_smooth': attack_steps.RandomStep\n}\n\nclass Attacker(ch.nn.Module):\n    \"\"\"\n    Attacker class, used to make adversarial examples.\n\n    This is primarily an internal class, you probably want to be looking at\n    :class:`robustness.attacker.AttackerModel`, which is how models are actually\n    served (AttackerModel uses this Attacker class).\n\n    However, the :meth:`robustness.Attacker.forward` function below\n    documents the arguments supported for adversarial attacks specifically.\n    \"\"\"\n    def __init__(self, model, dataset):\n        \"\"\"\n        Initialize the Attacker\n\n        Args:\n            nn.Module model : the PyTorch model to attack\n            Dataset dataset : dataset the model is trained on, only used to get mean and std for normalization\n        \"\"\"\n        super(Attacker, self).__init__()\n        self.normalize = helpers.InputNormalize(dataset.mean, dataset.std)\n        self.model = model\n\n    def forward(self, x, target, *_, constraint, eps, step_size, iterations, with_latent=False,\n                random_start=False, random_restarts=False, do_tqdm=False,\n                targeted=False, custom_loss=None, should_normalize=True,\n                orig_input=None, use_best=True, return_image=True,\n                est_grad=None, mixed_precision=False):\n        \"\"\"\n        Implementation of forward (finds adversarial examples). Note that\n        this does **not** perform inference and should not be called\n        directly; refer to :meth:`robustness.attacker.AttackerModel.forward`\n        for the function you should actually be calling.\n\n        Args:\n            x, target (ch.tensor) : see :meth:`robustness.attacker.AttackerModel.forward`\n            constraint\n                (\"2\"|\"inf\"|\"unconstrained\"|\"fourier\"|:class:`~robustness.attack_steps.AttackerStep`)\n                : threat model for adversarial attacks (:math:`\\ell_2` ball,\n                :math:`\\ell_\\infty` ball, :math:`[0, 1]^n`, Fourier basis, or\n                custom AttackerStep subclass).\n            eps (float) : radius for threat model.\n            step_size (float) : step size for adversarial attacks.\n            iterations (int): number of steps for adversarial attacks.\n            random_start (bool) : if True, start the attack with a random step.\n            random_restarts (bool) : if True, do many random restarts and\n                take the worst attack (in terms of loss) per input.\n            do_tqdm (bool) : if True, show a tqdm progress bar for the attack.\n            targeted (bool) : if True (False), minimize (maximize) the loss.\n            custom_loss (function|None) : if provided, used instead of the\n                criterion as the loss to maximize/minimize during\n                adversarial attack. The function should take in\n                :samp:`model, x, target` and return a tuple of the form\n                :samp:`loss, None`, where loss is a tensor of size N\n                (per-element loss).\n            should_normalize (bool) : If False, don't normalize the input\n                (not recommended unless normalization is done in the\n                custom_loss instead).\n            orig_input (ch.tensor|None) : If not None, use this as the\n                center of the perturbation set, rather than :samp:`x`.\n            use_best (bool) : If True, use the best (in terms of loss)\n                iterate of the attack process instead of just the last one.\n            return_image (bool) : If True (default), then return the adversarial\n                example as an image, otherwise return it in its parameterization\n                (for example, the Fourier coefficients if 'constraint' is\n                'fourier')\n            est_grad (tuple|None) : If not None (default), then these are\n                :samp:`(query_radius [R], num_queries [N])` to use for estimating the\n                gradient instead of autograd. We use the spherical gradient\n                estimator, shown below, along with antithetic sampling [#f1]_\n                to reduce variance:\n                :math:`\\\\nabla_x f(x) \\\\approx \\\\sum_{i=0}^N f(x + R\\\\cdot\n                \\\\vec{\\\\delta_i})\\\\cdot \\\\vec{\\\\delta_i}`, where\n                :math:`\\delta_i` are randomly sampled from the unit ball.\n            mixed_precision (bool) : if True, use mixed-precision calculations\n                to compute the adversarial examples / do the inference.\n        Returns:\n            An adversarial example for x (i.e. within a feasible set\n            determined by `eps` and `constraint`, but classified as:\n\n            * `target` (if `targeted == True`)\n            *  not `target` (if `targeted == False`)\n\n        .. [#f1] This means that we actually draw :math:`N/2` random vectors\n            from the unit ball, and then use :math:`\\delta_{N/2+i} =\n            -\\delta_{i}`.\n        \"\"\"\n        # Can provide a different input to make the feasible set around\n        # instead of the initial point\n        if orig_input is None: orig_input = x.detach()\n        orig_input = orig_input.cuda()\n\n        # Multiplier for gradient ascent [untargeted] or descent [targeted]\n        m = -1 if targeted else 1\n\n        # Initialize step class and attacker criterion\n        criterion = ch.nn.CrossEntropyLoss(reduction='none')\n        step_class = STEPS[constraint] if isinstance(constraint, str) else constraint\n        step = step_class(eps=eps, orig_input=orig_input, step_size=step_size) \n\n        def calc_loss(inp, target):\n            '''\n            Calculates the loss of an input with respect to target labels\n            Uses custom loss (if provided) otherwise the criterion\n            '''\n            if should_normalize:\n                inp = self.normalize(inp)\n            output = self.model(inp)\n            if custom_loss:\n                return custom_loss(self.model, inp, target)\n\n            return criterion(output, target), output\n\n        # Main function for making adversarial examples\n        def get_adv_examples(x):\n            # Random start (to escape certain types of gradient masking)\n            if random_start:\n                x = step.random_perturb(x)\n\n            iterator = range(iterations)\n            if do_tqdm: iterator = tqdm(iterator)\n\n            # Keep track of the \"best\" (worst-case) loss and its\n            # corresponding input\n            best_loss = None\n            best_x = None\n\n            # A function that updates the best loss and best input\n            def replace_best(loss, bloss, x, bx):\n                if bloss is None:\n                    bx = x.clone().detach()\n                    bloss = loss.clone().detach()\n                else:\n                    replace = m * bloss < m * loss\n                    bx[replace] = x[replace].clone().detach()\n                    bloss[replace] = loss[replace]\n\n                return bloss, bx\n\n            # PGD iterates\n            for _ in iterator:\n                x = x.clone().detach().requires_grad_(True)\n                losses, out = calc_loss(step.to_image(x), target)\n                assert losses.shape[0] == x.shape[0], \\\n                        'Shape of losses must match input!'\n\n                loss = ch.mean(losses)\n\n                if step.use_grad:\n                    if (est_grad is None) and mixed_precision:\n                        with amp.scale_loss(loss, []) as sl:\n                            sl.backward()\n                        grad = x.grad.detach()\n                        x.grad.zero_()\n                    elif (est_grad is None):\n                        grad, = ch.autograd.grad(m * loss, [x])\n                    else:\n                        f = lambda _x, _y: m * calc_loss(step.to_image(_x), _y)[0]\n                        grad = helpers.calc_est_grad(f, x, target, *est_grad)\n                else:\n                    grad = None\n\n                with ch.no_grad():\n                    args = [losses, best_loss, x, best_x]\n                    best_loss, best_x = replace_best(*args) if use_best else (losses, x)\n\n                    x = step.step(x, grad)\n                    x = step.project(x)\n                    if do_tqdm: iterator.set_description(\"Current loss: {l}\".format(l=loss))\n\n            # Save computation (don't compute last loss) if not use_best\n            if not use_best: \n                ret = x.clone().detach()\n                return step.to_image(ret) if return_image else ret\n\n            losses, _ = calc_loss(step.to_image(x), target)\n            args = [losses, best_loss, x, best_x]\n            best_loss, best_x = replace_best(*args)\n            return step.to_image(best_x) if return_image else best_x\n\n        # Random restarts: repeat the attack and find the worst-case\n        # example for each input in the batch\n        if random_restarts:\n            to_ret = None\n\n            orig_cpy = x.clone().detach()\n            for _ in range(random_restarts):\n                adv = get_adv_examples(orig_cpy)\n\n                if to_ret is None:\n                    to_ret = adv.detach()\n\n                _, output = calc_loss(adv, target)\n                corr, = helpers.accuracy(output, target, topk=(1,), exact=True)\n                corr = corr.byte()\n                misclass = ~corr\n                to_ret[misclass] = adv[misclass]\n\n            adv_ret = to_ret\n        else:\n            adv_ret = get_adv_examples(x)\n\n        return adv_ret\n\nclass AttackerModel(ch.nn.Module):\n    \"\"\"\n    Wrapper class for adversarial attacks on models. Given any normal\n    model (a ``ch.nn.Module`` instance), wrapping it in AttackerModel allows\n    for convenient access to adversarial attacks and other applications.::\n\n        model = ResNet50()\n        model = AttackerModel(model)\n        x = ch.rand(10, 3, 32, 32) # random images\n        y = ch.zeros(10) # label 0\n        out, new_im = model(x, y, make_adv=True) # adversarial attack\n        out, new_im = model(x, y, make_adv=True, targeted=True) # targeted attack\n        out = model(x) # normal inference (no label needed)\n\n    More code examples available in the documentation for `forward`.\n    For a more comprehensive overview of this class, see \n    :doc:`our detailed walkthrough <../example_usage/input_space_manipulation>`.\n    \"\"\"\n    def __init__(self, model, dataset):\n        super(AttackerModel, self).__init__()\n        self.normalizer = helpers.InputNormalize(dataset.mean, dataset.std)\n        self.model = model\n        self.attacker = Attacker(model, dataset)\n\n    def forward(self, inp, target=None, make_adv=False,\n                fake_relu=False, no_relu=False, with_image=True, **attacker_kwargs):\n        \"\"\"\n        Main function for running inference and generating adversarial\n        examples for a model.\n\n        Parameters:\n            inp (ch.tensor) : input to do inference on [N x input_shape] (e.g. NCHW)\n            target (ch.tensor) : ignored if `make_adv == False`. Otherwise,\n                labels for adversarial attack.\n            make_adv (bool) : whether to make an adversarial example for\n                the model. If true, returns a tuple of the form\n                :samp:`(model_prediction, adv_input)` where\n                :samp:`model_prediction` is a tensor with the *logits* from\n                the network.\n            with_latent (bool) : also return the second-last layer along\n                with the logits. Output becomes of the form\n                :samp:`((model_logits, model_layer), adv_input)` if\n                :samp:`make_adv==True`, otherwise :samp:`(model_logits, model_layer)`.\n            fake_relu (bool) : useful for activation maximization. If\n                :samp:`True`, replace the ReLUs in the last layer with\n                \"fake ReLUs,\" which are ReLUs in the forwards pass but\n                identity in the backwards pass (otherwise, maximizing a\n                ReLU which is dead is impossible as there is no gradient).\n            no_relu (bool) : If :samp:`True`, return the latent output with\n                the (pre-ReLU) output of the second-last layer, instead of the\n                post-ReLU output. Requires :samp:`fake_relu=False`, and has no\n                visible effect without :samp:`with_latent=True`.\n            with_image (bool) : if :samp:`False`, only return the model output\n                (even if :samp:`make_adv == True`).\n\n        \"\"\"\n        if make_adv:\n            assert target is not None\n            prev_training = bool(self.training)\n            self.eval()\n            adv = self.attacker(inp, target, **attacker_kwargs)\n            if prev_training:\n                self.train()\n\n            inp = adv\n\n        normalized_inp = self.normalizer(inp)\n\n        if no_relu and (not with_latent):\n            print(\"WARNING: 'no_relu' has no visible effect if 'with_latent is False.\")\n        if no_relu and fake_relu:\n            raise ValueError(\"Options 'no_relu' and 'fake_relu' are exclusive\")\n\n        output = self.model(normalized_inp, \n                                )\n        if with_image:\n            return (output, inp)\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:40:51.071114Z","iopub.execute_input":"2022-03-28T10:40:51.071422Z","iopub.status.idle":"2022-03-28T10:40:51.08777Z","shell.execute_reply.started":"2022-03-28T10:40:51.071391Z","shell.execute_reply":"2022-03-28T10:40:51.086773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install cox","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:41:01.625486Z","iopub.execute_input":"2022-03-28T10:41:01.625755Z","iopub.status.idle":"2022-03-28T10:41:11.376714Z","shell.execute_reply.started":"2022-03-28T10:41:01.625712Z","shell.execute_reply":"2022-03-28T10:41:11.375873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/pytorch-cifar","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:41:11.380215Z","iopub.execute_input":"2022-03-28T10:41:11.380445Z","iopub.status.idle":"2022-03-28T10:41:11.388644Z","shell.execute_reply.started":"2022-03-28T10:41:11.380419Z","shell.execute_reply":"2022-03-28T10:41:11.387819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfrom models import *\ndevice = \"cuda:0\"\nnet = ResNet18().to(device)\nnet = torch.nn.DataParallel(net)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:41:13.339337Z","iopub.execute_input":"2022-03-28T10:41:13.340062Z","iopub.status.idle":"2022-03-28T10:41:15.685263Z","shell.execute_reply.started":"2022-03-28T10:41:13.340024Z","shell.execute_reply":"2022-03-28T10:41:15.684509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/pytorch-cifar/robustness/\n# %cd ..","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:41:17.755282Z","iopub.execute_input":"2022-03-28T10:41:17.757678Z","iopub.status.idle":"2022-03-28T10:41:17.767379Z","shell.execute_reply.started":"2022-03-28T10:41:17.757634Z","shell.execute_reply":"2022-03-28T10:41:17.76673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom robustness.datasets import CIFAR\nfrom robustness.model_utils import make_and_restore_model\nimport sys \n# sys.path.append('..')\n# from models import *\nimport torch\n\nimport gc\ngc.collect()\ndevice = \"cuda:0\"\n\n\nfor seed in [2019, 2020, 2021]:\n#     net = ResNet18().to(device)\n    checkpoint = torch.load('../checkpoint/ckpt.pth')\n    net.load_state_dict(checkpoint['net'])\n#     net.load_state_dict(torch.load(\"../checkpoint/ckpt.pth\")['net'])\n    ds = CIFAR('../data')\n    net.eval()\n    m, _ = make_and_restore_model(arch=net, dataset=ds, add_custom_forward=False)\n    m.eval()\n    _, test_loader = ds.make_loaders(workers=4, batch_size=28)\n    _, (im, label) = next(enumerate(test_loader))\n\n    def test_model_adv_accuracy(m, model, dataloader, kwargs):\n        correct = 0\n        total = 0\n        for data in dataloader:\n            x, y = data\n            _, x_adv_pgd_hard = m(x.cuda(), y.cuda(), make_adv=True, **kwargs)\n\n            outputs = net(x_adv_pgd_hard)\n            _, predicted = torch.max(outputs.data, 1)\n            total += y.size(0)\n            correct += (predicted == y.cuda()).sum().item()\n\n        print(\"adv accuracy: \", correct / total)\n\n        return correct / total\n\n    for adv_epsilon in [0.1, 0.5, 1, 2, 4]:\n        kwargs = {\n        'constraint':'2', # use L2-PGD\n        'eps': adv_epsilon, # L2 radius around original image\n        'step_size': 1,\n        'iterations': 100,\n        'do_tqdm': False,\n    }\n        temp_adv_acc = test_model_adv_accuracy(m, net, test_loader, kwargs)\n        print(\"adv_epsilon\", adv_epsilon, \"seed\", seed)\n#             robustness_dict[adv_epsilon] = robustness_dict.setdefault(adv_epsilon, default=[]).append(adv_epsilon)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:41:19.905379Z","iopub.execute_input":"2022-03-28T10:41:19.905842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}