{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ../input/pytorchlightning160/pytorch_lightning-1.6.0-py3-none-any.whl\n# !pip install pytorch-crf","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:09.509969Z","iopub.execute_input":"2022-04-21T09:28:09.510379Z","iopub.status.idle":"2022-04-21T09:28:22.64205Z","shell.execute_reply.started":"2022-04-21T09:28:09.510295Z","shell.execute_reply":"2022-04-21T09:28:22.640899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 因为 transformers 里面的 Deberta 不支持 FastTokenizer，\n# 而我们需要用到offset_mappping，这个只有 FastTokenizer 才有，所以需要导入 FastTokenizer\n# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py', \"deberta__init__.py\"]:\n    if str(filename).startswith(\"deberta\"):\n        filepath = deberta_v2_path/str(filename).replace(\"deberta\", \"\")\n    else:\n        filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-21T09:28:22.644673Z","iopub.execute_input":"2022-04-21T09:28:22.645344Z","iopub.status.idle":"2022-04-21T09:28:22.679848Z","shell.execute_reply.started":"2022-04-21T09:28:22.645282Z","shell.execute_reply":"2022-04-21T09:28:22.67882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Library","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom pathlib import Path\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n# from torchcrf import CRF\nimport pytorch_lightning as pl\nfrom pathlib import Path\n\n# os.system('pip uninstall -y transformers')\n# os.system('python -m pip install --no-index --find-links=../input/nbme-pip-wheels transformers')\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-04-21T09:28:22.682539Z","iopub.execute_input":"2022-04-21T09:28:22.683575Z","iopub.status.idle":"2022-04-21T09:28:32.699244Z","shell.execute_reply.started":"2022-04-21T09:28:22.683529Z","shell.execute_reply":"2022-04-21T09:28:32.698157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**数据解析**\n\n本次比赛提供了5份数据分别是 train, test, features, patient_notes, submission， 其中test, submission为提交答案时用\n\n重点是如下3个文件\n\ntrain 文件标记了每个病例中，不同症状的相关描述\n\nfeatures 中给出了所有病症的名称和id\n\npatient_notes 中给出了每份病例的详细描述\n\n总体来说我们希望，通过对病例和症状的分析和挖掘，自动的找出不同病症在病例中的相关描述","metadata":{}},{"cell_type":"markdown","source":"## CFG","metadata":{}},{"cell_type":"code","source":"class CFG:\n    ## 常规设置\n#     data_dir = '/home/tzj/data/nbme-score-clinical-patient-notes'\n#     output_dir = '../output'\n    data_dir = '../input/nbme-score-clinical-patient-notes'\n    output_dir = './'\n    debug = False\n    debug_size = 0\n    train = True\n#     seed = 42\n    seed = 6001\n    n_fold = 5\n    trn_fold = [0]\n    print_freq = 100\n#     print_freq = 1\n\n    ## 数据设置\n    num_workers = 4\n    batch_size = 4\n    max_len = 512\n    pin_memory = True\n\n    ## 模型设置\n#     model = \"/home/tzj/pretrained_models/en-deberta-v3-large\"\n    model = \"../input/deberta-v3-large/deberta-v3-large\"\n    fc_dropout = 0.2\n    fgm = False\n    label_smooth = False\n    smoothing = 0.05\n\n\n    ## 优化器设置\n    scheduler = 'cosine'  # ['linear', 'cosine']\n#     batch_scheduler = True\n    num_cycles = 0.5\n    warmup_steps = 0.1\n    encoder_lr = 2e-5\n    decoder_lr = 2e-5\n#     min_lr = 1e-6\n    eps = 1e-6\n    betas = (0.9, 0.999)\n    weight_decay = 0.01\n\n    ## Trainer设置\n    apex = False\n    apex_level = 'O1'\n    max_epochs = 5\n    gradient_accumulation_steps = 4\n    precision = 32\n    max_grad_norm = 1\n    fast_dev_run = 0 # 快速检验，取 n 个train, val, test batches\n    num_sanity_val_steps = 0 # 在开始前取 n 个val batches\n    val_check_interval = 0.5\n\n\nif CFG.debug:\n    CFG.debug_size = 100\n    CFG.epochs = 2\n    CFG.trn_fold = [0]\n    CFG.fast_dev_run = 2\n    CFG.num_sanity_val_steps = 0\n    CFG.val_check_interval = 0.5\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:32.702431Z","iopub.execute_input":"2022-04-21T09:28:32.7034Z","iopub.status.idle":"2022-04-21T09:28:32.716029Z","shell.execute_reply.started":"2022-04-21T09:28:32.703353Z","shell.execute_reply":"2022-04-21T09:28:32.714992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper functions for scoring","metadata":{}},{"cell_type":"code","source":"# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n\ndef micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    return f1_score(truths, preds)\n\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n    return binary\n\n\ndef span_micro_f1(truths, preds):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n        \n    将 preds 和 truths 转换为 0，1 编码， 1 表示是annotation\n    然后进行 f1_score(binary)\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n    return micro_f1(bin_preds, bin_truths)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:32.718279Z","iopub.execute_input":"2022-04-21T09:28:32.719047Z","iopub.status.idle":"2022-04-21T09:28:32.747864Z","shell.execute_reply.started":"2022-04-21T09:28:32.719003Z","shell.execute_reply":"2022-04-21T09:28:32.746576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label Smoothing","metadata":{}},{"cell_type":"code","source":"class LabelSmoothLoss(nn.Module):\n    def __init__(self, smoothing=0.0, loss_func=nn.BCEWithLogitsLoss(reduction='sum')):\n        super(LabelSmoothLoss, self).__init__()\n        self.smoothing = smoothing\n        self.loss_func = loss_func\n\n    def forward(self, inputs, target):\n        # inputs为未经过激活的logits\n        #target为数值时才使用scatter_， 此处target为one-hot\n        '''\n        log_prob = F.log_softmax(inputs, dim=-1)\n        weight = inputs.new_ones(inputs.size()) * self.smoothing / (inputs.size(-1) - 1.)\n        weight.scatter_(-1, target.unsqueeze(-1), (1. - self.smoothing))\n        loss = (-weight * log_prob).sum(dim=-1).mean()'''\n        '''\n        log_prob = F.log_softmax(inputs, dim=-1)\n        # 由于将多标签看为多个二分类，因此不用除以类别数\n        weight = inputs.new_ones(inputs.size()) * self.smoothing\n        weight[target==1] = 1. - self.smoothing\n        loss = (-weight * log_prob).sum(dim=-1).mean()'''\n\n        weight = inputs.new_ones(inputs.size()) * self.smoothing\n        weight[target == 1] = 1. - self.smoothing\n        loss = self.loss_func(inputs, weight)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:32.750515Z","iopub.execute_input":"2022-04-21T09:28:32.751183Z","iopub.status.idle":"2022-04-21T09:28:32.763389Z","shell.execute_reply.started":"2022-04-21T09:28:32.751135Z","shell.execute_reply":"2022-04-21T09:28:32.762248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_labels_for_scoring(df):\n    # 整理原数据集中的 location ，作为打分的标签\n    truths = []\n    for location_list in df['location']:\n        # 有些标注中带有 \";\"\n        location_list = [loc for location in location_list for loc in location.split(';')]\n        truth = []\n        if len(location_list) > 0:\n            for loc in location_list:\n                start, end = loc.split()\n                truth.append([int(start), int(end)])\n        truths.append(truth)\n        '''\n        输入形式如下：\n        [[[696, 724]],\n         [[668, 693]],\n         [[203, 217]],\n         [[70, 91], [176, 183]],\n         [[222, 258]],\n         [],\n         [[321, 329], [404, 413], [652, 661]]]\n        '''\n    return truths\n\ndef get_char_probs(features, texts, predictions, tokenizer):  \n    # 获取每个字符所属类别的概率\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (feature, text, prediction) in enumerate(zip(features, texts, predictions)):\n        encoded = tokenizer(feature, text, add_special_tokens=True, return_offsets_mapping=True)\n        offset_mapping = encoded['offset_mapping']\n        sequence_ids = encoded.sequence_ids()\n        # 这里 offset_mapping 和 prediction 的长度可能不一致，因为 predictions 带有填充，但是 zip 自动丢弃了多余的部分\n        for j, (offset, pred) in enumerate(zip(offset_mapping, prediction)):\n            if sequence_ids[j] != 1:\n                continue\n            start = offset[0]\n            end = offset[1]\n            # 属于同一个 token 的 char 的概率统一为该 token 的概率\n            results[i][start:end] = pred\n    return results\n\ndef get_results(char_probs, th=0.5):\n    # 获得预测结果大于 th 的 char index， 并获得其起止位置， 用 “；”隔开每一对\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n#         result = np.where(char_prob >= th)[0]\n        # itertools.count()： 计数器，默认从 0 开始\n        # itertools.groupby(res, key)，将 res 中所有 key 相同的元素进行分组\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n#         result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n        '''\n        返回形式如下：\n        ['2 3;11 14;30 32;44 46']\n        '''\n    return results\n\n\ndef get_predictions(results):\n    # 将 str 类型的预测的 char index 转为 int 型\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n    '''\n    返回形式如下：\n    [[2,3], [11, 14], [30, 32], [44, 46]]\n    '''\n    return predictions\n\n\ndef get_score(y_true, y_pred):\n    score = span_micro_f1(y_true, y_pred)\n    return score","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:32.765656Z","iopub.execute_input":"2022-04-21T09:28:32.766132Z","iopub.status.idle":"2022-04-21T09:28:32.790302Z","shell.execute_reply.started":"2022-04-21T09:28:32.766052Z","shell.execute_reply":"2022-04-21T09:28:32.789081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def create_labels_for_scoring_n(df):\n#     # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n#     df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n#     for i in range(len(df)):\n#         lst = df.loc[i, 'location']\n#         if lst:\n#             new_lst = ';'.join(lst)\n#             df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n#     # create labels\n#     truths = []\n#     for location_list in df['location_for_create_labels'].values:\n#         truth = []\n#         if len(location_list) > 0:\n#             location = location_list[0]\n#             for loc in [s.split() for s in location.split(';')]:\n#                 start, end = int(loc[0]), int(loc[1])\n#                 truth.append([start, end])\n#         truths.append(truth)\n#     return truths\n\n\n\n# def get_char_probs_n(texts, predictions, tokenizer):\n#     # 获取每个字符所属类别的概率\n#     results = [np.zeros(len(t)) for t in texts]\n\n#     for i, (text, prediction) in enumerate(zip(texts, predictions)):\n#         encoded = tokenizer(text,\n#                             add_special_tokens=True,\n#                             return_offsets_mapping=True)\n#         # 这里 offset_mapping 和 prediction 的长度可能不一致，因为 predictions 带有填充，但是 zip 自动丢弃了多余的部分\n#         for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n#             start = offset_mapping[0]\n#             end = offset_mapping[1]\n#             # 属于同一个 token 的 char 的概率统一为该 token 的概率\n#             results[i][start:end] = pred\n#     return results\n\n# def get_results(char_probs, th=0.5):\n#     # 获得预测结果大于 th 的 char index， 并获得其起止位置， 用 “；”隔开每一对\n#     results = []\n#     for char_prob in char_probs:\n#         result = np.where(char_prob >= th)[0] + 1\n# #         result = np.where(char_prob >= th)[0]\n#         # itertools.count()： 计数器，默认从 0 开始\n#         # itertools.groupby(res, key)，将 res 中所有 key 相同的元素进行分组\n#         result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n#         result = [f\"{min(r)} {max(r)}\" for r in result]\n# #         result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n#         result = \";\".join(result)\n#         results.append(result)\n#         '''\n#         返回形式如下：\n#         ['2 3;11 14;30 32;44 46']\n#         '''\n#     return results\n\n\n# def get_predictions(results):\n#     # 将 str 类型的预测的 char index 转为 int 型\n#     predictions = []\n#     for result in results:\n#         prediction = []\n#         if result != \"\":\n#             for loc in [s.split() for s in result.split(';')]:\n#                 start, end = int(loc[0]), int(loc[1])\n#                 prediction.append([start, end])\n#         predictions.append(prediction)\n#     '''\n#     返回形式如下：\n#     [[2,3], [11, 14], [30, 32], [44, 46]]\n#     '''\n#     return predictions\n\n\n# def get_score(y_true, y_pred):\n#     score = span_micro_f1(y_true, y_pred)\n#     return score","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-04-21T09:28:32.793345Z","iopub.execute_input":"2022-04-21T09:28:32.793992Z","iopub.status.idle":"2022-04-21T09:28:32.804197Z","shell.execute_reply.started":"2022-04-21T09:28:32.793946Z","shell.execute_reply":"2022-04-21T09:28:32.802892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"markdown","source":"### incorrect annotation","metadata":{}},{"cell_type":"code","source":"def process_feature_text(text):\n    text = re.sub('I-year', '1-year', text)\n    text = re.sub('-OR-', \" or \", text)\n    text = re.sub('-', ' ', text)\n    return text\n\n# incorrect annotation\ndef correcting(train, features, patient_notes):\n    train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n    train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n\n    train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n    train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n\n    train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n    train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n\n    train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n    train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n\n    train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n    train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n\n    train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n    train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n\n    train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n    train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n\n    train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n    train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n\n    train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n    train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n\n    train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n    train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n\n    train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n    train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n\n    train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n    train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n\n    train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n    train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n\n    train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n    train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n\n    train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n    train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n\n    train.loc[3858, 'annotation'] = ast.literal_eval(\n        '[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n    train.loc[3858, 'location'] = ast.literal_eval(\n        '[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n\n    train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n    train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n\n    train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n    train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n\n    train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n    train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n\n    train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n    train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n\n    train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n    train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n\n    train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n    train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n\n    train.loc[6380, 'annotation'] = ast.literal_eval(\n        '[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n    train.loc[6380, 'location'] = ast.literal_eval(\n        '[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n\n    train.loc[6562, 'annotation'] = ast.literal_eval(\n        '[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n    train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n\n    train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n    train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n\n    train.loc[7022, 'annotation'] = ast.literal_eval(\n        '[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n    train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n\n    train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n    train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n\n    train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n    train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n\n    train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n    train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n\n    train.loc[9938, 'annotation'] = ast.literal_eval(\n        '[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n    train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n\n    train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n    train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n\n    train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n    train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n\n    train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n    train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n\n    train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n    train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n\n    train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n    train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n\n    train.loc[12279, 'annotation'] = ast.literal_eval(\n        '[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n    train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n\n    train.loc[12289, 'annotation'] = ast.literal_eval(\n        '[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n    train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n\n    train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n    train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n\n    train.loc[13297, 'annotation'] = ast.literal_eval(\n        '[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n    train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n\n    train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n    train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n\n    train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n    train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n\n    train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n    train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n\n    patient_notes['pn_history'] = patient_notes['pn_history'].apply(\n        lambda x: x.replace('dad with recent heart attcak', 'dad with recent heart attack'))\n\n    features['feature_text'] = features['feature_text'].apply(process_feature_text)\n\n    return train, features, patient_notes","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-04-21T09:28:32.806632Z","iopub.execute_input":"2022-04-21T09:28:32.807045Z","iopub.status.idle":"2022-04-21T09:28:32.850683Z","shell.execute_reply.started":"2022-04-21T09:28:32.807002Z","shell.execute_reply":"2022-04-21T09:28:32.849526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CV split","metadata":{}},{"cell_type":"code","source":"def CV_group_split(dataset, n_splits=5, debug=False, debug_size=1000):\n# 使用 GroupKFold 是因为训练数据中，每一条 patient_note 中有多个标记，因此需要从 patient_note 层面对数据进行切分\n# 即将每条 patient_note 的不同标记划分到训练集和验证集中\n    Fold = GroupKFold(n_splits=n_splits)\n    groups = dataset['pn_num'].values\n    for n, (train_index, val_index) in enumerate(Fold.split(dataset, dataset['location'], groups)):\n        dataset.loc[val_index, 'fold'] = int(n)\n    dataset['fold'] = dataset['fold'].astype(int)\n    if debug:\n        dataset = dataset.sample(n=debug_size, random_state=0).reset_index(drop=True)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:32.856244Z","iopub.execute_input":"2022-04-21T09:28:32.856658Z","iopub.status.idle":"2022-04-21T09:28:32.866529Z","shell.execute_reply.started":"2022-04-21T09:28:32.856616Z","shell.execute_reply":"2022-04-21T09:28:32.865253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenizer","metadata":{}},{"cell_type":"code","source":"def get_tokenizer(tokenizer_path):\n    if 'deberta' in tokenizer_path:\n        from transformers.models.deberta_v2 import DebertaV2TokenizerFast\n        tokenizer = DebertaV2TokenizerFast.from_pretrained(tokenizer_path)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n    return tokenizer\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:32.870033Z","iopub.execute_input":"2022-04-21T09:28:32.870324Z","iopub.status.idle":"2022-04-21T09:28:32.888793Z","shell.execute_reply.started":"2022-04-21T09:28:32.870282Z","shell.execute_reply":"2022-04-21T09:28:32.887649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DataModule","metadata":{}},{"cell_type":"code","source":"class NBMEDataModule(pl.LightningDataModule):\n    def __init__(self, config, prepare_train=True, prepare_test=True):\n        super().__init__()\n        self.prepare_data_per_node = False\n        self.debug = config.debug\n        self.debug_size = 0 if self.debug == False else config.debug_size\n        self.shuffle = (self.debug == False)\n        self.batch_size = config.batch_size\n        self.pin_memory = config.pin_memory\n        self.num_workers = config.num_workers\n        self.max_len = config.max_len\n        self.data_dir = config.data_dir\n        self.n_fold = config.n_fold\n\n        self.tokenizer = get_tokenizer(config.model)\n        self.prepare_train = prepare_train\n        self.prepare_test = prepare_test\n\n    def set_trn_fold(self, trn_fold):\n        self.trn_fold = trn_fold\n\n    def load_train(self):\n        train = pd.read_csv(Path(self.data_dir) / 'train.csv')\n        features = pd.read_csv(Path(self.data_dir) / 'features.csv')\n        patient_notes = pd.read_csv(Path(self.data_dir) / 'patient_notes.csv')\n        train['annotation'] = train['annotation'].apply(ast.literal_eval)\n        train['location'] = train['location'].apply(ast.literal_eval)\n\n        train, features, patient_notes = correcting(train, features, patient_notes)\n        train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n        train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n        train['annotation_length'] = train['annotation'].apply(len)\n        return train\n\n    def load_test(self):\n        test = pd.read_csv(Path(self.data_dir) / 'test.csv')\n        features = pd.read_csv(Path(self.data_dir) / 'features.csv')\n        patient_notes = pd.read_csv(Path(self.data_dir) / 'patient_notes.csv')\n        submission = pd.read_csv(Path(self.data_dir) / 'sample_submission.csv')\n\n        features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n\n        test = test.merge(features, on=['feature_num', 'case_num'], how='left')\n        test = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n        return test, submission\n\n    def calculate_max_len(self, dataset):\n        dataset['pn_history'].fillna('')\n        dataset['feature_text'].fillna('')\n        tqdm.pandas(desc=\"pn_history_lens\")\n        pn_history_lens = dataset['pn_history'].progress_apply(\n            lambda x: len(self.tokenizer(x, add_special_tokens=False)['input_ids']))\n        tqdm.pandas(desc=\"pn_history_lens\")\n        feature_text_lens = dataset['feature_text'].progress_apply(\n            lambda x: len(self.tokenizer(x, add_special_tokens=False)['input_ids']))\n        max_len_feat = feature_text_lens.max()\n        max_len_pn = pn_history_lens.max()\n        return (feature_text_lens, pn_history_lens, max_len_feat + max_len_pn + 3)  # cls & sep & sep\n\n    def prepare_data(self):\n        if self.prepare_train == True:\n            train = self.load_train()\n            # 将数据切分成 n 折\n            train = CV_group_split(train, self.n_fold, self.debug, self.debug_size)\n            self.train_max_len = self.calculate_max_len(train)[2]\n            self.train = train\n            self.prepare_train = False\n            print('Train data prepared!')\n\n        if self.prepare_test == True:\n            self.test, self.submission = self.load_test()\n            self.prepare_test = False\n            print('Test data prepared!')\n\n    def setup(self, stage='fit'):\n        if stage == 'fit':\n            self.build_fit_dataset(trn_fold=self.trn_fold)\n\n        elif stage == 'test':\n            self.build_test_dataset()\n\n        elif stage == 'predict':\n            self.build_predict_dataset()\n\n    def build_fit_dataset(self, trn_fold=None):\n        df = self.train\n        if trn_fold != None:\n            self.train_df = df[df['fold'] != trn_fold].reset_index(drop=True)\n            self.val_df = df[df['fold'] == trn_fold].reset_index(drop=True)\n            self.train_dataset = NBMEDataset(self.train_df, self.tokenizer, self.train_max_len)\n            self.val_dataset = NBMEDataset(self.val_df, self.tokenizer, self.train_max_len)\n\n    def build_test_dataset(self):\n        self.test_dataset = NBMEInferDataset(self.test, self.tokenizer, self.max_len)\n\n    def build_predict_dataset(self):\n        self.predict_dataset = NBMEInferDataset(self.test, self.tokenizer, self.max_len)\n\n    def train_dataloader(self):\n        loader = DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers,\n                            pin_memory=self.pin_memory, shuffle=self.shuffle)\n        return loader\n\n    def val_dataloader(self):\n        loader = DataLoader(self.val_dataset, batch_size=self.batch_size * 4, num_workers=self.num_workers,\n                            shuffle=False)\n        return loader\n\n    def test_dataloader(self):\n        loader = DataLoader(self.test_dataset, batch_size=self.batch_size * 4, num_workers=self.num_workers,\n                            shuffle=False)\n        return loader\n\n    def predict_dataloader(self):\n        loader = DataLoader(self.predict_dataset, batch_size=self.batch_size * 4, num_workers=self.num_workers,\n                            shuffle=False)\n        return loader\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:32.892457Z","iopub.execute_input":"2022-04-21T09:28:32.893106Z","iopub.status.idle":"2022-04-21T09:28:32.926001Z","shell.execute_reply.started":"2022-04-21T09:28:32.893029Z","shell.execute_reply":"2022-04-21T09:28:32.924895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"def tokenize(tokenizer, singel_data, max_len, return_offsets_mapping=False):\n    features, text = singel_data[['feature_text', 'pn_history']]\n    inputs = tokenizer(\n        features,  # question\n        text,  # paragraph\n        add_special_tokens=True,  # cls, sep\n        max_length=max_len,\n        padding='max_length',\n        return_offsets_mapping=return_offsets_mapping)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\ndef add_labels(tokenizer, singel_data, max_len, return_offsets_mapping=False):\n    features, text, location_list, annotation_list = singel_data[\n        ['feature_text', 'pn_history', 'location', 'annotation']]\n    inputs = tokenizer(\n        features,  # question\n        text,  # paragraph\n        add_special_tokens=True,  # cls, sep\n        max_length=max_len,\n        padding='max_length',\n        return_offsets_mapping=True,\n        return_tensors='pt')\n    for k, v in inputs.items():\n        inputs[k].squeeze_()\n\n    offset_mapping = inputs.pop('offset_mapping')\n    sequence_ids = np.array(inputs.sequence_ids())\n\n    label = np.where(sequence_ids != 1, -1, 0)\n    # 找到 patient note 的开始和结束下标\n    token_start_idx = np.where(label == 0)[0][0]\n    token_end_idx = np.where(label == 0)[0][-1]\n\n    # 每个 feature 可能在同一条 patient note 出现多次\n    for i, location in enumerate(location_list):\n        # 注意，可能有些location 存在\";\"，如第 8478 条，表示对应的 annotation 存在跳跃\n        # ['79 94;100 116']  -> [['79 94'], ['100' '116']]\n        location = [s for s in location.split(';')]\n        for loc in location:\n            char_start, char_end = map(int, loc.split())\n            cur_token_start, cur_token_end = token_start_idx, token_end_idx\n\n            ######## offset_mapping 里面， 每一组(start, end)可能会包含前置的空格\n            # token_start_index 不能超过界限， 并且其对应单词的首个 char 的位置不能大于 answer 的 start_char\n            while cur_token_start <= cur_token_end and offset_mapping[cur_token_start][0] <= char_start:\n                cur_token_start += 1\n            cur_token_start -= 1\n\n            while cur_token_start <= cur_token_end and offset_mapping[cur_token_end][1] >= char_end:\n                cur_token_end -= 1\n            cur_token_end += 1\n\n            label[cur_token_start: cur_token_end + 1] = 1.0\n    label = torch.tensor(label, dtype=torch.float32)\n    return label","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:32.928182Z","iopub.execute_input":"2022-04-21T09:28:32.928549Z","iopub.status.idle":"2022-04-21T09:28:32.946951Z","shell.execute_reply.started":"2022-04-21T09:28:32.928487Z","shell.execute_reply":"2022-04-21T09:28:32.945915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NBMEDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        super().__init__()\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        single_data = self.df.iloc[index]\n        inputs = tokenize(self.tokenizer, single_data, self.max_len)\n        label = add_labels(self.tokenizer, single_data, self.max_len, return_offsets_mapping=False)\n        return inputs, label\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:32.950012Z","iopub.execute_input":"2022-04-21T09:28:32.950668Z","iopub.status.idle":"2022-04-21T09:28:32.961632Z","shell.execute_reply.started":"2022-04-21T09:28:32.950406Z","shell.execute_reply":"2022-04-21T09:28:32.960518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### FGM","metadata":{}},{"cell_type":"code","source":"class FGM():\n    \"\"\"\n    定义对抗训练方法FGM,对模型embedding参数进行扰动\n    \"\"\"\n    def __init__(self, model, epsilon=0.25):\n        self.model = model\n        self.epsilon = epsilon\n        self.backup = {}\n\n    def attack(self, embed_name='word_embeddings'):\n        \"\"\"\n        得到对抗样本\n        :param emb_name:模型中embedding的参数名\n        :return:\n        \"\"\"\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and embed_name in name:\n                self.backup[name] = param.data.clone()\n                norm = torch.norm(param.grad)\n\n                if norm != 0 and not torch.isnan(norm):\n                    r_at = self.epsilon * param.grad / norm\n                    param.data.add_(r_at)\n\n    def restore(self, embed_name='word_embeddings'):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and embed_name in name:\n                assert name in self.backup\n                param.data = self.backup[name]\n        self.backup = {}\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:32.963467Z","iopub.execute_input":"2022-04-21T09:28:32.965025Z","iopub.status.idle":"2022-04-21T09:28:32.977417Z","shell.execute_reply.started":"2022-04-21T09:28:32.964965Z","shell.execute_reply":"2022-04-21T09:28:32.976412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:32.979219Z","iopub.execute_input":"2022-04-21T09:28:32.98027Z","iopub.status.idle":"2022-04-21T09:28:32.989533Z","shell.execute_reply.started":"2022-04-21T09:28:32.980167Z","shell.execute_reply":"2022-04-21T09:28:32.98816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NBMEModel(pl.LightningModule):\n    def __init__(self, config, model_config_path=None, pretrained=False, weight_path=None):\n        super().__init__()\n        self.save_hyperparameters('config')\n\n        if model_config_path:\n            self.model_config = torch.load(model_config_path)\n        else:\n            self.model_config = AutoConfig.from_pretrained(config.model, output_hidden_states=True)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(config.model, config=self.model_config)\n        else:\n            self.model = AutoModel.from_config(self.model_config)\n\n        self.fc = nn.Linear(self.model_config.hidden_size, 1)\n\n        # TODO multi_dropout / layer norm\n        self.dropout_0 = nn.Dropout(config.fc_dropout / 2.)\n        self.dropout_1 = nn.Dropout(config.fc_dropout / 1.5)\n        self.dropout_2 = nn.Dropout(config.fc_dropout)\n        self.dropout_3 = nn.Dropout(config.fc_dropout * 1.5)\n        self.dropout_4 = nn.Dropout(config.fc_dropout * 2.)     \n        self.__init_weight(self.fc)\n        self.__set_metrics()\n \n        if config.label_smooth:\n            self.criterion = LabelSmoothLoss(smoothing=config.smoothing, loss_func=nn.BCEWithLogitsLoss(reduction=\"none\"))\n        else:\n            self.criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n\n        if hasattr(self.hparams.config, 'fgm') and self.hparams.config.fgm:\n            self.automatic_optimization = False\n            self.fgm = FGM(self)\n            \n        if weight_path != None:\n            weight = torch.load(weight_path, map_location='cpu')\n            if 'state_dict' in weight.keys():\n                weight = weight['state_dict']\n            self.load_state_dict(weight)\n\n\n    def __set_metrics(self):\n        self.train_losses = AverageMeter()\n        self.val_losses = AverageMeter()\n        self.val_acc = AverageMeter()\n\n        self.train_losses.reset()\n        self.val_losses.reset()\n        self.val_acc.reset()\n\n    def __init_weight(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states, pooler_output = outputs[0], outputs[1]\n        output_0 = self.fc(self.dropout_0(last_hidden_states))\n        output_1 = self.fc(self.dropout_1(last_hidden_states))\n        output_2 = self.fc(self.dropout_2(last_hidden_states))\n        output_3 = self.fc(self.dropout_3(last_hidden_states))\n        output_4 = self.fc(self.dropout_4(last_hidden_states))\n        return (output_0 + output_1 + output_2 + output_3 + output_4) / 5\n#         return output_2\n\n    def training_step(self, batch, batch_idx):\n        inputs, labels = batch\n        y_preds = self.forward(inputs)\n        loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n        self.train_losses.update(loss.item(), len(labels))\n        self.log('train/avg_loss', self.train_losses.avg)\n        # 因为 optimizer 有 3 组参数，所有 get_last_lr() 会返回含有 3 个元素的列表\n        en_lr = self.trainer.lr_scheduler_configs[0].scheduler.get_last_lr()[0]\n        de_lr = self.trainer.lr_scheduler_configs[0].scheduler.get_last_lr()[-1]\n        self.log('train/en_lr', en_lr, prog_bar=True)\n        self.log('train/de_lr', de_lr, prog_bar=True)\n\n        if (self.trainer.global_step) % self.hparams.config.print_freq == 0:\n            # if (self.trainer.global_step + 1) % self.hparams.config.print_freq == 0:\n            self.print('Global step:{global_step}.'\n                       'Train Loss: {loss.val:.4f}(avg: {loss.avg:.4f}) '\n                       'Encoder LR: {en_lr:.8f}, Decoder LR: {de_lr:.8f}'\n                       .format(global_step=self.trainer.global_step,\n                           loss=self.train_losses,\n                               en_lr=en_lr,\n                               de_lr=de_lr))\n        # 如果没有FGM，在这里就可以返回loss\n        # 为了使用FGM，这里要手动进行求导和优化器更新\n        if self.hparams.config.fgm:\n            # loss regularization， 但是不加效果要更好一些\n            # if self.hparams.config.gradient_accumulation_steps > 1:\n            #     loss = loss / self.hparams.config.gradient_accumulation_steps\n            self.manual_backward(loss)\n            torch.nn.utils.clip_grad_norm(self.parameters(), self.hparams.config.max_grad_norm)\n            # 这里不能用 global_step ，否则因为关闭了自动优化，global_step 只能在 step 之后才会更新，会陷入死循环\n            if (batch_idx + 1) % self.hparams.config.gradient_accumulation_steps == 0:\n            # if (self.trainer.global_step + 1) % self.hparams.config.gradient_accumulation_steps == 0:\n                self.fgm.attack()\n                y_preds_adv = self.forward(inputs)\n                loss_adv = self.criterion(y_preds_adv.view(-1, 1), labels.view(-1, 1))\n                loss_adv = torch.masked_select(loss_adv, labels.view(-1, 1) != -1).mean()\n                self.manual_backward(loss_adv)\n                self.fgm.restore()\n\n                opt = self.optimizers()\n                opt.step()\n                opt.zero_grad()\n                sch = self.lr_schedulers()\n                sch.step()\n\n        return loss\n\n    def training_epoch_end(self, outs):\n        torch.cuda.empty_cache()\n\n    def validation_step(self, batch, batch_idx):\n        inputs, labels = batch\n        y_preds = self.forward(inputs)\n        loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n        self.val_losses.update(loss.item(), len(labels))\n        self.log('val/avg_loss', self.val_losses.avg)\n        return loss, y_preds.sigmoid().cpu().numpy()\n\n    def validation_epoch_end(self, outs):\n        val_df = self.trainer.datamodule.val_df\n        val_labels = create_labels_for_scoring(val_df)\n        valid_features, valid_texts = val_df['feature_text'], val_df['pn_history']\n        preds = np.concatenate([item[1] for item in outs])\n        val_loss_avg = self.val_losses.avg\n        #  ======================== scoring ============================\n        char_probs = get_char_probs(valid_features, valid_texts, preds, self.trainer.datamodule.tokenizer)\n#         char_probs = get_char_probs_n(valid_texts, preds, self.trainer.datamodule.tokenizer)\n        results = get_results(char_probs)\n        predictions = get_predictions(results)\n        score = get_score(val_labels, predictions)\n        self.log(f'val/loss_avg', val_loss_avg)\n        self.log(f'val/score', score)\n        self.print(f'Global step:{self.trainer.global_step}.\\n Val loss avg: {val_loss_avg}, score: {score}')\n\n        self.val_losses.reset()\n        self.val_acc.reset()\n\n    def predict_step(self, batch, batch_idx, dataloader_idx= None):\n        inputs = batch\n        y_preds = self.forward(inputs)\n        return y_preds.sigmoid().cpu().numpy()\n\n    def configure_optimizers(self):\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        encoder_lr = self.hparams.config.encoder_lr\n        decoder_lr = self.hparams.config.decoder_lr\n        num_cycles = self.hparams.config.num_cycles\n        # end_lr = self.hparams.config.min_lr\n        weight_decay = self.hparams.config.weight_decay\n        eps = self.hparams.config.eps\n        betas = self.hparams.config.betas\n        optimizer_parameters = [\n            {'params': [p for n, p in self.model.named_parameters()\n                        if not any(nd in n for nd in no_decay)],\n             'lr':encoder_lr , 'weight_decay': weight_decay,\n             },\n            {'params': [p for n, p in self.model.named_parameters()\n                        if any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': 0.0,\n             },\n            {'params': [p for n, p in self.named_parameters() if \"model\" not in n],\n             'lr': decoder_lr, 'weight_decay': 0.0,\n             }\n        ]\n        optimizer = AdamW(optimizer_parameters,\n                          lr=encoder_lr, eps=eps, betas=betas)\n\n        if self.trainer.max_steps == None or self.trainer.max_epochs != None:\n            # 注意，因为使用FGM需要关闭自动优化，传入 trainer 的 accumulate_grad_batches 是None\n            # 因此这里计算不能使用 trainer 的参数，要使用 config 里的参数\n            # max_steps = (\n            #         len(self.trainer.datamodule.train_dataloader()) * self.trainer.max_epochs\n            #         // self.trainer.accumulate_grad_batches\n            # )\n            max_steps = (\n                    len(self.trainer.datamodule.train_dataloader()) * self.trainer.max_epochs\n                    // self.hparams.config.gradient_accumulation_steps\n            )\n        else:\n            max_steps = self.trainer.max_steps\n\n        warmup_steps = self.hparams.config.warmup_steps\n        if isinstance(warmup_steps, float):\n            warmup_steps = int(warmup_steps * max_steps)\n\n        print(f'====== Max steps: {max_steps},\\t Warm up steps: {warmup_steps} =========')\n\n        if self.hparams.config.scheduler == 'linear':\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=warmup_steps, num_training_steps=max_steps,\n            )\n        elif self.hparams.config.scheduler == 'cosine':\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer, num_warmup_steps=warmup_steps, num_training_steps=max_steps,\n                                num_cycles=num_cycles\n            )\n        else:\n            scheduler = None\n        sched = {\n            'scheduler': scheduler, 'interval': 'step'\n        }\n        return ([optimizer], [sched])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:32.991472Z","iopub.execute_input":"2022-04-21T09:28:32.992353Z","iopub.status.idle":"2022-04-21T09:28:33.05092Z","shell.execute_reply.started":"2022-04-21T09:28:32.992234Z","shell.execute_reply":"2022-04-21T09:28:33.049904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run","metadata":{}},{"cell_type":"code","source":"pl.seed_everything(CFG.seed)\ndm = NBMEDataModule(CFG, prepare_test=False)\ndm.prepare_data()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:33.053403Z","iopub.execute_input":"2022-04-21T09:28:33.054129Z","iopub.status.idle":"2022-04-21T09:28:47.950555Z","shell.execute_reply.started":"2022-04-21T09:28:33.054062Z","shell.execute_reply":"2022-04-21T09:28:47.948539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.trn_fold = [4]\nfgm_p = 'fgm_' if CFG.fgm else ''\nls = 'ls_' if CFG.label_smooth else ''\nprint(f\"FGM:{CFG.fgm}, \\t label_smooth:{CFG.label_smooth}_{CFG.smoothing}\")\nprint(f\"decoder_lr:{CFG.decoder_lr}, \\t batch_size:{CFG.batch_size} * {CFG.gradient_accumulation_steps}\")\nprint(f\"precision:{CFG.precision}, grad_norm:{CFG.max_grad_norm}, \\t apex:{CFG.apex}_{CFG.apex_level}\")  \nfor train_fold in CFG.trn_fold:\n    prefix = f'{fgm_p}{ls}fold{train_fold}'\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n            filename=prefix+'step{step}-val_loss{val/loss_avg:.4f}-val_score{val/score:.4f}',\n            auto_insert_metric_name=False,\n            save_top_k=1, monitor='val/score', mode='max', save_last=False, verbose=True, save_weights_only=True,\n        )\n    callbacks = [checkpoint_callback]\n    dm.set_trn_fold(train_fold)\n\n    model = NBMEModel(CFG, model_config_path=None, pretrained=True)\n    trainer = pl.Trainer(\n        gpus=[0],\n        default_root_dir=f\"seed{CFG.seed}_fold{train_fold}_from_{CFG.model.split('/')[-1]}\",\n        log_every_n_steps=10,\n        amp_backend=\"apex\" if CFG.apex else \"native\",\n        amp_level=CFG.apex_level if CFG.apex else None,\n        precision=16 if CFG.apex else CFG.precision,\n        max_epochs=CFG.max_epochs,\n        callbacks=callbacks,\n        gradient_clip_val=None if CFG.fgm else CFG.max_grad_norm,\n        accumulate_grad_batches=None if CFG.fgm else CFG.gradient_accumulation_steps,\n        fast_dev_run=CFG.fast_dev_run,\n        num_sanity_val_steps=CFG.num_sanity_val_steps,\n        val_check_interval=CFG.val_check_interval,\n    )\n    trainer.fit(model, datamodule=dm)\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:28:47.952476Z","iopub.execute_input":"2022-04-21T09:28:47.953094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}