{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ../input/pytorchlightning160/pytorch_lightning-1.6.0-py3-none-any.whl\n# !pip install pytorch-crf","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:36:20.913312Z","iopub.execute_input":"2022-04-23T16:36:20.913707Z","iopub.status.idle":"2022-04-23T16:36:51.437079Z","shell.execute_reply.started":"2022-04-23T16:36:20.913624Z","shell.execute_reply":"2022-04-23T16:36:51.436259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py', \"deberta__init__.py\"]:\n    if str(filename).startswith(\"deberta\"):\n        filepath = deberta_v2_path / str(filename).replace(\"deberta\", \"\")\n    else:\n        filepath = deberta_v2_path / filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-23T16:36:51.44266Z","iopub.execute_input":"2022-04-23T16:36:51.442878Z","iopub.status.idle":"2022-04-23T16:36:51.470936Z","shell.execute_reply.started":"2022-04-23T16:36:51.44285Z","shell.execute_reply":"2022-04-23T16:36:51.470226Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n# from torchcrf import CRF\nimport pytorch_lightning as pl\nfrom pathlib import Path\n\n# os.system('pip uninstall -y transformers')\n# os.system('python -m pip install --no-index --find-links=../input/nbme-pip-wheels transformers')\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:36:51.47234Z","iopub.execute_input":"2022-04-23T16:36:51.472615Z","iopub.status.idle":"2022-04-23T16:36:59.570919Z","shell.execute_reply.started":"2022-04-23T16:36:51.472581Z","shell.execute_reply":"2022-04-23T16:36:59.570065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    ## 常规设置\n#     data_dir = '/home/tzj/data/nbme-score-clinical-patient-notes'\n    data_dir = '../input/nbme-score-clinical-patient-notes'\n    output_dir = './'\n#     weight_path = '../input/multi-drop'\n    weight_path = [\n        '../input/nbme-pos-weight',\n#         '../input/esemble-lb01',\n#         '../input/fgm-fold4',\n    ]\n\n    debug = False\n\n    seed = 6001\n    n_fold = 5\n    trn_fold = [0, 1, 2, 3, 4]\n\n    ## 数据设置\n    num_workers = 4\n    batch_size = 4\n    max_len = 512\n    pin_memory = True\n\n    ## 模型设置\n    model = \"../input/deberta-v3-large/deberta-v3-large\"\n    fgm = False\n    label_smooth = False\n    smoothing = 0.1\n\n    fc_dropout = 0.2","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:36:59.573188Z","iopub.execute_input":"2022-04-23T16:36:59.573396Z","iopub.status.idle":"2022-04-23T16:36:59.580005Z","shell.execute_reply.started":"2022-04-23T16:36:59.573364Z","shell.execute_reply":"2022-04-23T16:36:59.57925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n\ndef micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    return f1_score(truths, preds)\n\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n    return binary\n\n\ndef span_micro_f1(truths, preds):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n        \n    将 preds 和 truths 转换为 0，1 编码， 1 表示是annotation\n    然后进行 f1_score(binary)\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n    return micro_f1(bin_preds, bin_truths)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:36:59.581523Z","iopub.execute_input":"2022-04-23T16:36:59.582172Z","iopub.status.idle":"2022-04-23T16:36:59.59415Z","shell.execute_reply.started":"2022-04-23T16:36:59.582114Z","shell.execute_reply":"2022-04-23T16:36:59.593387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_labels_for_scoring_n(df):\n    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n        if len(location_list) > 0:\n            location = location_list[0]\n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n    return truths\n\ndef create_labels_for_scoring(df):\n    # 整理原数据集中的 location ，作为打分的标签\n    truths = []\n    for location_list in df['location']:\n        # 有些标注中带有 \";\"\n        location_list = [loc for location in location_list for loc in location.split(';')]\n        truth = []\n        if len(location_list) > 0:\n            for loc in location_list:\n                start, end = loc.split()\n                truth.append([int(start), int(end)])\n        truths.append(truth)\n        '''\n        输入形式如下：\n        [[[696, 724]],\n         [[668, 693]],\n         [[203, 217]],\n         [[70, 91], [176, 183]],\n         [[222, 258]],\n         [],\n         [[321, 329], [404, 413], [652, 661]]]\n        '''\n    return truths\n\ndef get_char_probs(features, texts, predictions, tokenizer):  \n    # 获取每个字符所属类别的概率\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (feature, text, prediction) in enumerate(zip(features, texts, predictions)):\n        encoded = tokenizer(feature, text, add_special_tokens=True, return_offsets_mapping=True)\n        offset_mapping = encoded['offset_mapping']\n        sequence_ids = encoded.sequence_ids()\n        # 这里 offset_mapping 和 prediction 的长度可能不一致，因为 predictions 带有填充，但是 zip 自动丢弃了多余的部分\n        for j, (offset, pred) in enumerate(zip(offset_mapping, prediction)):\n            if sequence_ids[j] != 1:\n                continue\n            start = offset[0]\n            end = offset[1]\n            # 属于同一个 token 的 char 的概率统一为该 token 的概率\n            results[i][start:end] = pred\n    return results\n\ndef get_results(char_probs, th=0.5):\n    # TODO 确认字符位置的 +1 -1 \n    # 获得预测结果大于 th 的 char index， 并获得其起止位置， 用 “；”隔开每一对\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1 \n#         result = np.where(char_prob >= th)[0]\n        # itertools.count()： 计数器，默认从 0 开始\n        # itertools.groupby(res, key)，将 res 中所有 key 相同的元素进行分组\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n#         result = [f\"{min(r)} {max(r) + 1}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n        '''\n        返回形式如下：\n        ['2 3;11 14;30 32;44 46']\n        '''\n    return results\n\ndef get_predictions(results):\n    # 将 str 类型的预测的 char index 转为 int 型\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n    '''\n    返回形式如下：\n    [[2,3], [11, 14], [30, 32], [44, 46]]\n    '''\n    return predictions\n\ndef get_score(y_true, y_pred):\n    score = span_micro_f1(y_true, y_pred)\n    return score","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:36:59.595588Z","iopub.execute_input":"2022-04-23T16:36:59.595914Z","iopub.status.idle":"2022-04-23T16:36:59.620687Z","shell.execute_reply.started":"2022-04-23T16:36:59.595879Z","shell.execute_reply":"2022-04-23T16:36:59.619958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def get_tokenizer(tokenizer_path):\n    if 'deberta' in tokenizer_path:\n        from transformers.models.deberta_v2 import DebertaV2TokenizerFast\n        tokenizer = DebertaV2TokenizerFast.from_pretrained(tokenizer_path)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n    return tokenizer\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:36:59.622257Z","iopub.execute_input":"2022-04-23T16:36:59.622681Z","iopub.status.idle":"2022-04-23T16:36:59.62939Z","shell.execute_reply.started":"2022-04-23T16:36:59.622644Z","shell.execute_reply":"2022-04-23T16:36:59.628722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NBMEDataModule(pl.LightningDataModule):\n    def __init__(self, config, prepare_train=True, prepare_test=True):\n        super().__init__()\n        self.prepare_data_per_node = False\n        self.debug = config.debug\n        self.debug_size = 0 if self.debug == False else config.debug_size\n        self.shuffle = (self.debug == False)\n        self.batch_size = config.batch_size\n        self.pin_memory = config.pin_memory\n        self.num_workers = config.num_workers\n        self.max_len = config.max_len\n        self.data_dir = config.data_dir\n        self.n_fold = config.n_fold\n\n        self.tokenizer = get_tokenizer(config.model)\n        self.prepare_train = prepare_train\n        self.prepare_test = prepare_test\n\n    def set_trn_fold(self, trn_fold):\n        self.trn_fold = trn_fold\n\n    def load_train(self):\n        train = pd.read_csv(Path(self.data_dir) / 'train.csv')\n        features = pd.read_csv(Path(self.data_dir) / 'features.csv')\n        patient_notes = pd.read_csv(Path(self.data_dir) / 'patient_notes.csv')\n        train['annotation'] = train['annotation'].apply(ast.literal_eval)\n        train['location'] = train['location'].apply(ast.literal_eval)\n\n        train, features, patient_notes = correcting(train, features, patient_notes)\n        train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n        train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n        train['annotation_length'] = train['annotation'].apply(len)\n        return train\n\n    def load_test(self):\n        test = pd.read_csv(Path(self.data_dir) / 'test.csv')\n        features = pd.read_csv(Path(self.data_dir) / 'features.csv')\n        patient_notes = pd.read_csv(Path(self.data_dir) / 'patient_notes.csv')\n        submission = pd.read_csv(Path(self.data_dir) / 'sample_submission.csv')\n\n        features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n\n        test = test.merge(features, on=['feature_num', 'case_num'], how='left')\n        test = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n        return test, submission\n\n    def calculate_max_len(self, dataset):\n        dataset['pn_history'].fillna('')\n        dataset['feature_text'].fillna('')\n        tqdm.pandas(desc=\"pn_history_lens\")\n        pn_history_lens = dataset['pn_history'].progress_apply(\n            lambda x: len(self.tokenizer(x, add_special_tokens=False)['input_ids']))\n        tqdm.pandas(desc=\"pn_history_lens\")\n        feature_text_lens = dataset['feature_text'].progress_apply(\n            lambda x: len(self.tokenizer(x, add_special_tokens=False)['input_ids']))\n        max_len_feat = feature_text_lens.max()\n        max_len_pn = pn_history_lens.max()\n        return (feature_text_lens, pn_history_lens, max_len_feat + max_len_pn + 3)  # cls & sep & sep\n\n    def prepare_data(self):\n        if self.prepare_train == True:\n            train = self.load_train()\n            # 将数据切分成 n 折\n            train = CV_group_split(train, self.n_fold, self.debug, self.debug_size)\n            self.train_max_len = self.calculate_max_len(train)[2]\n            self.train = train\n            self.prepare_train = False\n            print('Train data prepared!')\n\n        if self.prepare_test == True:\n            self.test, self.submission = self.load_test()\n            self.prepare_test = False\n            print('Test data prepared!')\n\n    def setup(self, stage='fit'):\n        if stage == 'fit':\n            self.build_fit_dataset(trn_fold=self.trn_fold)\n\n        elif stage == 'test':\n            self.build_test_dataset()\n\n        elif stage == 'predict':\n            self.build_predict_dataset()\n\n    def build_fit_dataset(self, trn_fold=None):\n        df = self.train\n        if trn_fold != None:\n            self.train_df = df[df['fold'] != trn_fold].reset_index(drop=True)\n            self.val_df = df[df['fold'] == trn_fold].reset_index(drop=True)\n            self.train_dataset = NBMEDataset(self.train_df, self.tokenizer, self.train_max_len, split='train')\n            self.val_dataset = NBMEDataset(self.val_df, self.tokenizer, self.train_max_len, split='val')\n\n    def build_test_dataset(self):\n        self.test_dataset = NBMEInferDataset(self.test, self.tokenizer, self.max_len)\n\n    def build_predict_dataset(self):\n        self.predict_dataset = NBMEInferDataset(self.test, self.tokenizer, self.max_len)\n\n    def train_dataloader(self):\n        loader = DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers,\n                            pin_memory=self.pin_memory, shuffle=self.shuffle)\n        return loader\n\n    def val_dataloader(self):\n        loader = DataLoader(self.val_dataset, batch_size=self.batch_size * 4, num_workers=self.num_workers,\n                            shuffle=False)\n        return loader\n\n    def test_dataloader(self):\n        loader = DataLoader(self.test_dataset, batch_size=self.batch_size * 4, num_workers=self.num_workers,\n                            shuffle=False)\n        return loader\n\n    def predict_dataloader(self):\n        loader = DataLoader(self.predict_dataset, batch_size=self.batch_size * 4, num_workers=self.num_workers,\n                            shuffle=False)\n        return loader\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:36:59.631604Z","iopub.execute_input":"2022-04-23T16:36:59.631788Z","iopub.status.idle":"2022-04-23T16:36:59.662596Z","shell.execute_reply.started":"2022-04-23T16:36:59.631765Z","shell.execute_reply":"2022-04-23T16:36:59.661846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(tokenizer, singel_data, max_len, return_offsets_mapping=True):\n    features, text = singel_data[['feature_text', 'pn_history']]\n    inputs = tokenizer(\n        features,  # question\n        text,  # paragraph\n        add_special_tokens=True,  # cls, sep\n        max_length=max_len,\n        padding='max_length',\n        return_offsets_mapping=return_offsets_mapping)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\ndef add_labels(tokenizer, singel_data, max_len, return_offsets_mapping=True):\n    text, location_list, annotation_length = singel_data[\n        ['pn_history', 'location', 'annotation_length']]\n    encoded = tokenizer(\n        text,\n        add_special_tokens=True,\n        max_length=max_len,\n        padding='max_length',\n        return_offsets_mapping=return_offsets_mapping)\n\n    offset_mapping = encoded['offset_mapping']\n    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n\n    label = np.zeros(len(offset_mapping))\n    label[ignore_idxes] = -1\n    if annotation_length != 0:\n        # 每个 feature 可能在同一条 patient note 出现多次\n        for location in location_list:\n            # 注意，可能有些location 存在\";\"，如第 8478 条，表示对应的 annotation 存在跳跃\n            # ['79 94;100 116']  -> [['79 94'], ['100' '116']]\n            for loc in [s.split() for s in location.split(';')]:\n                token_start_idx = -1\n                token_end_idx = -1\n                char_start_idx, char_end_idx = int(loc[0]), int(loc[1])\n                ######## offset_mapping 里面， 每一组(start, end)可能会包含前置的空格\n                # token_start_index 不能超过界限， 并且其对应单词的首个 char 的位置不能大于 answer 的 start_char\n                for idx in range(len(offset_mapping)):\n                    if (token_start_idx == -1) & (char_start_idx < offset_mapping[idx][0]):\n                        token_start_idx = idx - 1\n                    if (token_end_idx == -1) & (char_end_idx <= offset_mapping[idx][1]):\n                        token_end_idx = idx + 1\n                if token_start_idx == -1:\n                    token_start_idx = token_end_idx\n                if (token_start_idx != -1) & (token_end_idx != -1):\n                    label[token_start_idx:token_end_idx] = 1.0\n    return torch.tensor(label, dtype=torch.float)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:36:59.663879Z","iopub.execute_input":"2022-04-23T16:36:59.664658Z","iopub.status.idle":"2022-04-23T16:36:59.677761Z","shell.execute_reply.started":"2022-04-23T16:36:59.66462Z","shell.execute_reply":"2022-04-23T16:36:59.676906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NBMEInferDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        super().__init__()\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        single_data = self.df.iloc[index]\n        inputs = tokenize(self.tokenizer, single_data, self.max_len, return_offsets_mapping=False)\n        return inputs\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:36:59.681333Z","iopub.execute_input":"2022-04-23T16:36:59.682024Z","iopub.status.idle":"2022-04-23T16:36:59.695832Z","shell.execute_reply.started":"2022-04-23T16:36:59.681995Z","shell.execute_reply":"2022-04-23T16:36:59.695069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:36:59.697144Z","iopub.execute_input":"2022-04-23T16:36:59.697503Z","iopub.status.idle":"2022-04-23T16:36:59.706376Z","shell.execute_reply.started":"2022-04-23T16:36:59.697466Z","shell.execute_reply":"2022-04-23T16:36:59.705506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NBMEModel(pl.LightningModule):\n    def __init__(self, config, model_config_path=None, pretrained=False, weight_path=None):\n        super().__init__()\n        self.save_hyperparameters('config')\n\n        if model_config_path:\n            self.model_config = torch.load(model_config_path)\n        else:\n            self.model_config = AutoConfig.from_pretrained(config.model, output_hidden_states=True)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(config.model, config=self.model_config)\n        else:\n            self.model = AutoModel.from_config(self.model_config)\n\n        self.fc = nn.Linear(self.model_config.hidden_size, 1)\n\n        # TODO multi_dropout / layer norm\n        self.dropout_0 = nn.Dropout(config.fc_dropout / 2.)\n        self.dropout_1 = nn.Dropout(config.fc_dropout / 1.5)\n        self.dropout_2 = nn.Dropout(config.fc_dropout)\n        self.dropout_3 = nn.Dropout(config.fc_dropout * 1.5)\n        self.dropout_4 = nn.Dropout(config.fc_dropout * 2.)     \n        self.__init_weight(self.fc)\n        self.__set_metrics()\n \n        if config.label_smooth:\n            self.criterion = LabelSmoothLoss(smoothing=config.smoothing, loss_func=nn.BCEWithLogitsLoss(reduction=\"none\"))\n        else:\n            self.criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n\n        if hasattr(self.hparams.config, 'fgm') and self.hparams.config.fgm:\n            self.automatic_optimization = False\n            self.fgm = FGM(self)\n            \n        if weight_path != None:\n            weight = torch.load(weight_path, map_location='cpu')\n            if 'state_dict' in weight.keys():\n                weight = weight['state_dict']\n            self.load_state_dict(weight)\n\n\n    def __set_metrics(self):\n        self.train_losses = AverageMeter()\n        self.val_losses = AverageMeter()\n        self.val_acc = AverageMeter()\n\n        self.train_losses.reset()\n        self.val_losses.reset()\n        self.val_acc.reset()\n\n    def __init_weight(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states, pooler_output = outputs[0], outputs[1]\n        output_0 = self.fc(self.dropout_0(last_hidden_states))\n        output_1 = self.fc(self.dropout_1(last_hidden_states))\n        output_2 = self.fc(self.dropout_2(last_hidden_states))\n        output_3 = self.fc(self.dropout_3(last_hidden_states))\n        output_4 = self.fc(self.dropout_4(last_hidden_states))\n        logits = (output_0 + output_1 + output_2 + output_3 + output_4) / 5\n        return logits\n\n    def training_step(self, batch, batch_idx):\n        inputs, labels = batch\n        y_preds = self.forward(inputs)\n        loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n        self.train_losses.update(loss.item(), len(labels))\n        self.log('train/avg_loss', self.train_losses.avg)\n        # 因为 optimizer 有 3 组参数，所有 get_last_lr() 会返回含有 3 个元素的列表\n        en_lr = self.trainer.lr_scheduler_configs[0].scheduler.get_last_lr()[0]\n        de_lr = self.trainer.lr_scheduler_configs[0].scheduler.get_last_lr()[-1]\n        self.log('train/en_lr', en_lr, prog_bar=True)\n        self.log('train/de_lr', de_lr, prog_bar=True)\n\n        if (self.trainer.global_step) % self.hparams.config.print_freq == 0:\n            # if (self.trainer.global_step + 1) % self.hparams.config.print_freq == 0:\n            self.print('Global step:{global_step}.'\n                       'Train Loss: {loss.val:.4f}(avg: {loss.avg:.4f}) '\n                       'Encoder LR: {en_lr:.8f}, Decoder LR: {de_lr:.8f}'\n                       .format(global_step=self.trainer.global_step,\n                           loss=self.train_losses,\n                               en_lr=en_lr,\n                               de_lr=de_lr))\n        # 如果没有FGM，在这里就可以返回loss\n        # 为了使用FGM，这里要手动进行求导和优化器更新\n        if self.hparams.config.fgm:\n            # loss regularization， 但是不加效果要更好一些\n            # if self.hparams.config.gradient_accumulation_steps > 1:\n            #     loss = loss / self.hparams.config.gradient_accumulation_steps\n            self.manual_backward(loss)\n            torch.nn.utils.clip_grad_norm(self.parameters(), self.hparams.config.max_grad_norm)\n            # 这里不能用 global_step ，否则因为关闭了自动优化，global_step 只能在 step 之后才会更新，会陷入死循环\n            if (batch_idx + 1) % self.hparams.config.gradient_accumulation_steps == 0:\n            # if (self.trainer.global_step + 1) % self.hparams.config.gradient_accumulation_steps == 0:\n                self.fgm.attack()\n                y_preds_adv = self.forward(inputs)\n                loss_adv = self.criterion(y_preds_adv.view(-1, 1), labels.view(-1, 1))\n                loss_adv = torch.masked_select(loss_adv, labels.view(-1, 1) != -1).mean()\n                self.manual_backward(loss_adv)\n                self.fgm.restore()\n\n                opt = self.optimizers()\n                opt.step()\n                opt.zero_grad()\n                sch = self.lr_schedulers()\n                sch.step()\n\n        return loss\n\n    def training_epoch_end(self, outs):\n        torch.cuda.empty_cache()\n\n    def validation_step(self, batch, batch_idx):\n        inputs, labels = batch\n        y_preds = self.forward(inputs)\n        loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n        self.val_losses.update(loss.item(), len(labels))\n        self.log('val/avg_loss', self.val_losses.avg)\n        return loss, y_preds.sigmoid().cpu().numpy()\n\n    def validation_epoch_end(self, outs):\n        val_df = self.trainer.datamodule.val_df\n        val_labels = create_labels_for_scoring_n(val_df)\n        valid_features, valid_texts = val_df['feature_text'], val_df['pn_history']\n        preds = np.concatenate([item[1] for item in outs])\n        val_loss_avg = self.val_losses.avg\n        #  ======================== scoring ============================\n        char_probs = get_char_probs(valid_texts, preds, self.trainer.datamodule.tokenizer)\n        results = get_results(char_probs)\n        predictions = get_predictions(results)\n        score = get_score(val_labels, predictions)\n        self.log(f'val/loss_avg', val_loss_avg)\n        self.log(f'val/score', score)\n        self.print(f'Global step:{self.trainer.global_step}.\\n Val loss avg: {val_loss_avg}, score: {score}')\n\n        self.val_losses.reset()\n        self.val_acc.reset()\n\n    def predict_step(self, batch, batch_idx, dataloader_idx= None):\n        inputs = batch\n        y_preds = self.forward(inputs)\n        return y_preds.sigmoid().cpu().numpy()\n\n    def configure_optimizers(self):\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        encoder_lr = self.hparams.config.encoder_lr\n        decoder_lr = self.hparams.config.decoder_lr\n        num_cycles = self.hparams.config.num_cycles\n        # end_lr = self.hparams.config.min_lr\n        weight_decay = self.hparams.config.weight_decay\n        eps = self.hparams.config.eps\n        betas = self.hparams.config.betas\n        optimizer_parameters = [\n            {'params': [p for n, p in self.model.named_parameters()\n                        if not any(nd in n for nd in no_decay)],\n             'lr':encoder_lr , 'weight_decay': weight_decay,\n             },\n            {'params': [p for n, p in self.model.named_parameters()\n                        if any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': 0.0,\n             },\n            {'params': [p for n, p in self.named_parameters() if \"model\" not in n],\n             'lr': decoder_lr, 'weight_decay': 0.0,\n             }\n        ]\n        optimizer = AdamW(optimizer_parameters,\n                          lr=encoder_lr, eps=eps, betas=betas)\n\n        if self.trainer.max_steps == None or self.trainer.max_epochs != None:\n            # 注意，因为使用FGM需要关闭自动优化，传入 trainer 的 accumulate_grad_batches 是None\n            # 因此这里计算不能使用 trainer 的参数，要使用 config 里的参数\n            # max_steps = (\n            #         len(self.trainer.datamodule.train_dataloader()) * self.trainer.max_epochs\n            #         // self.trainer.accumulate_grad_batches\n            # )\n            max_steps = (\n                    len(self.trainer.datamodule.train_dataloader()) * self.trainer.max_epochs\n                    // self.hparams.config.gradient_accumulation_steps\n            )\n        else:\n            max_steps = self.trainer.max_steps\n\n        warmup_steps = self.hparams.config.warmup_steps\n        if isinstance(warmup_steps, float):\n            warmup_steps = int(warmup_steps * max_steps)\n\n        print(f'====== Max steps: {max_steps},\\t Warm up steps: {warmup_steps} =========')\n\n        if self.hparams.config.scheduler == 'linear':\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=warmup_steps, num_training_steps=max_steps,\n            )\n        elif self.hparams.config.scheduler == 'cosine':\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer, num_warmup_steps=warmup_steps, num_training_steps=max_steps,\n                                num_cycles=num_cycles\n            )\n        else:\n            scheduler = None\n        sched = {\n            'scheduler': scheduler, 'interval': 'step'\n        }\n        return ([optimizer], [sched])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:36:59.707831Z","iopub.execute_input":"2022-04-23T16:36:59.708196Z","iopub.status.idle":"2022-04-23T16:36:59.753469Z","shell.execute_reply.started":"2022-04-23T16:36:59.70816Z","shell.execute_reply":"2022-04-23T16:36:59.752643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#     cfg = create_cfg('test')\ncfg = CFG\npl.seed_everything(cfg.seed)\ndm = NBMEDataModule(cfg, prepare_train=False)\ndm.prepare_data()\n\ntest_df = dm.test\nvalid_features, valid_texts = test_df['feature_text'], test_df['pn_history']\nweight_paths = []\nfor p in cfg.weight_path:\n    weight_paths.extend(list(Path(p).rglob('*.ckpt')))\n# weight_paths = list(Path(cfg.weight_path).rglob('*.ckpt'))\nweight_paths","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:36:59.754948Z","iopub.execute_input":"2022-04-23T16:36:59.755212Z","iopub.status.idle":"2022-04-23T16:37:02.1189Z","shell.execute_reply.started":"2022-04-23T16:36:59.755177Z","shell.execute_reply":"2022-04-23T16:37:02.118094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_score = [float(re.search('score([\\d.]*)', weight_path.stem).group(1)) for weight_path in weight_paths]\ncv_score = torch.tensor(cv_score)\ncv_score[-1] = 0.8830\ncv_score","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:42:06.338629Z","iopub.execute_input":"2022-04-23T16:42:06.338906Z","iopub.status.idle":"2022-04-23T16:42:06.348597Z","shell.execute_reply.started":"2022-04-23T16:42:06.338878Z","shell.execute_reply":"2022-04-23T16:42:06.347655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = nn.functional.softmax(cv_score/0.01, dim=0).float().numpy()\nweights","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:42:10.95909Z","iopub.execute_input":"2022-04-23T16:42:10.959342Z","iopub.status.idle":"2022-04-23T16:42:10.966033Z","shell.execute_reply.started":"2022-04-23T16:42:10.959313Z","shell.execute_reply":"2022-04-23T16:42:10.965038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor weight_path in weight_paths:\n    weight_name = weight_path.name\n    print(f\"Using weight from {weight_name}.\")\n\n    model = NBMEModel(cfg, model_config_path=None, pretrained=False, weight_path=weight_path)\n    # model_weight = torch.load(weight, map_location='cpu')\n    # model.load_state_dict(model_weight)\n    trainer = pl.Trainer(\n        gpus=[0],\n        default_root_dir=cfg.output_dir,\n    )\n    prediction = trainer.predict(model, datamodule=dm)\n    prediction = np.concatenate([batch_pred for batch_pred in prediction]).squeeze(axis=-1)\n    # prediction = prediction.reshape((len(test_df), cfg.max_len))\n    char_prob = get_char_probs(valid_features, valid_texts, prediction, dm.tokenizer)\n\n    predictions.append(char_prob)\n\n    del model, trainer, prediction, char_prob\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T07:38:45.133269Z","iopub.execute_input":"2022-04-22T07:38:45.13353Z","iopub.status.idle":"2022-04-22T07:40:46.199465Z","shell.execute_reply.started":"2022-04-22T07:38:45.133496Z","shell.execute_reply":"2022-04-22T07:40:46.198629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions = np.asarray(predictions)\n# predictions = np.matmul(weights, predictions.transpose(1, 0, 2)).squeeze()\n# predictions = torch.tensor(predictions, dtype=torch.float32)\n# predictions = torch.matmul(ws, predictions.permute(1, 0, 2)).squeeze().numpy()\npredictions = np.asarray(predictions)\npredictions = np.sum([w * p for w, p in zip(weights, predictions)], axis=0)\n\nres = get_results(predictions, th=0.5)\ndm.submission['location'] = res\ndm.submission[['id', 'location']].to_csv('submission.csv', index=False)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-22T07:40:46.201027Z","iopub.execute_input":"2022-04-22T07:40:46.201514Z","iopub.status.idle":"2022-04-22T07:40:46.219374Z","shell.execute_reply.started":"2022-04-22T07:40:46.20147Z","shell.execute_reply":"2022-04-22T07:40:46.218616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res","metadata":{"execution":{"iopub.status.busy":"2022-04-22T07:40:46.220852Z","iopub.execute_input":"2022-04-22T07:40:46.221121Z","iopub.status.idle":"2022-04-22T07:40:46.230475Z","shell.execute_reply.started":"2022-04-22T07:40:46.221084Z","shell.execute_reply":"2022-04-22T07:40:46.229716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res","metadata":{"execution":{"iopub.status.busy":"2022-04-22T07:40:46.232096Z","iopub.execute_input":"2022-04-22T07:40:46.232494Z","iopub.status.idle":"2022-04-22T07:40:46.238812Z","shell.execute_reply.started":"2022-04-22T07:40:46.232453Z","shell.execute_reply":"2022-04-22T07:40:46.237775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}