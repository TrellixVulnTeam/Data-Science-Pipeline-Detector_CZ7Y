{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport random\nimport time\nimport os\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom ast import literal_eval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-27T19:43:13.753445Z","iopub.execute_input":"2022-02-27T19:43:13.753752Z","iopub.status.idle":"2022-02-27T19:43:13.761711Z","shell.execute_reply.started":"2022-02-27T19:43:13.753721Z","shell.execute_reply":"2022-02-27T19:43:13.760679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_patient_notes = pd.read_csv(\"/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv\")\ndf_features = pd.read_csv(\"/kaggle/input/nbme-score-clinical-patient-notes/features.csv\")\ndf_train = pd.read_csv(\"/kaggle/input/nbme-score-clinical-patient-notes/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nbme-score-clinical-patient-notes/test.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:43:15.674581Z","iopub.execute_input":"2022-02-27T19:43:15.675568Z","iopub.status.idle":"2022-02-27T19:43:16.583664Z","shell.execute_reply.started":"2022-02-27T19:43:15.675518Z","shell.execute_reply":"2022-02-27T19:43:16.58275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_string(text):\n    #text = text.lower()\n    #text = text.replace(\"\\r\", \"\")\n    #text = text.replace(\"\\n\", \" \")\n    #TODO\n    # - Split for ages so 17-yo 17yo etc = 17-yo\n    #word_tokens = word_tokenize(text)    \n    #return ' '.join(word_tokens)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:43:17.976823Z","iopub.execute_input":"2022-02-27T19:43:17.977131Z","iopub.status.idle":"2022-02-27T19:43:17.98227Z","shell.execute_reply.started":"2022-02-27T19:43:17.977098Z","shell.execute_reply":"2022-02-27T19:43:17.980991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#Clean the patient notes\ndf_patient_notes[\"pn_history_clean\"] = df_patient_notes[\"pn_history\"].apply(clean_string)\ndf_train[\"annotation_clean\"] = df_train[\"annotation\"].apply(clean_string)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:43:20.029655Z","iopub.execute_input":"2022-02-27T19:43:20.029986Z","iopub.status.idle":"2022-02-27T19:43:20.067031Z","shell.execute_reply.started":"2022-02-27T19:43:20.029951Z","shell.execute_reply":"2022-02-27T19:43:20.066335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_note_length = df_patient_notes[\"pn_history\"].apply(len).max()\nprint(f\"Max note length: {max_note_length}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:43:22.523933Z","iopub.execute_input":"2022-02-27T19:43:22.524653Z","iopub.status.idle":"2022-02-27T19:43:22.557704Z","shell.execute_reply.started":"2022-02-27T19:43:22.524603Z","shell.execute_reply":"2022-02-27T19:43:22.557062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PREPARE TARGET**","metadata":{}},{"cell_type":"code","source":"df_train[\"location\"] = df_train[\"location\"].str.replace(\";\", \"', '\")\ndf_train[\"location_list\"] = [literal_eval(x) for x in df_train[\"location\"]]\ndf_train[\"annotation_clean\"] = [literal_eval(x) for x in df_train[\"annotation_clean\"]]\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:43:24.728817Z","iopub.execute_input":"2022-02-27T19:43:24.729535Z","iopub.status.idle":"2022-02-27T19:43:25.025546Z","shell.execute_reply.started":"2022-02-27T19:43:24.729485Z","shell.execute_reply":"2022-02-27T19:43:25.024554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = df_train[\"feature_num\"].unique()\nnum_of_features = len(features)\nprint(f\"Number of unique features: {num_of_features}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:43:27.669497Z","iopub.execute_input":"2022-02-27T19:43:27.669817Z","iopub.status.idle":"2022-02-27T19:43:27.67761Z","shell.execute_reply.started":"2022-02-27T19:43:27.669759Z","shell.execute_reply":"2022-02-27T19:43:27.676667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npn_nums = df_train[\"pn_num\"].unique()\nempty_label = [-1 for i in range(0,max_note_length)]\nempty_labels =  [empty_label for i in range(0,len(df_patient_notes))]\ndf_patient_notes[\"label\"] = empty_labels\nlabels = []\n\n#list of features (to generate onehot 2D encoding from continuous integers of featuers)\nfeature_mapping = df_features[\"feature_num\"].unique() \n\nfor pn_num in pn_nums:\n    features_for_pn = df_train[df_train[\"pn_num\"] == pn_num]\n    patient_note = df_patient_notes[df_patient_notes[\"pn_num\"] == pn_num].iloc[0] \n    \n    label = np.full((len(patient_note[\"pn_history_clean\"])), -1)\n    for feature_i in features_for_pn.index:\n        feature_label = features_for_pn.at[feature_i, \"feature_num\"]\n        feature_label = np.where(feature_mapping == feature_label)[0][0]\n        locations = features_for_pn.at[feature_i, \"location_list\"]\n        \n        #print(locations)\n        for location in locations:\n            #print(location)\n            start, end = int(location.split(\" \")[0]), int(location.split(\" \")[1])\n            #print(\"jou?\")\n            for lbl_i in range(start, min(end, len(label))):\n                label[lbl_i] = feature_label\n    df_patient_notes.at[df_patient_notes[df_patient_notes[\"pn_num\"] == pn_num].index[0], \"label\"] = label\n    #df_patient_notes.at[df_patient_notes[\"pn_num\"] == pn_num, \"label\"] = label\n    \n#tokenizer.sequences_to_texts([]).split(\" \")\n","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:43:32.518406Z","iopub.execute_input":"2022-02-27T19:43:32.518694Z","iopub.status.idle":"2022-02-27T19:43:34.465741Z","shell.execute_reply.started":"2022-02-27T19:43:32.518655Z","shell.execute_reply":"2022-02-27T19:43:34.465067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n           ' ', ',', '-', '.']\nalphabet_size = len(alphabet) + 2 # + 1 for oov + 1 for mask (=0)\ndef char_to_int(char):\n    if char.lower() in alphabet:\n        return alphabet.index(char.lower()) +1\n    return alphabet_size-1\n\ndef list_of_chars_to_num(arr):\n    return [char_to_int(char) for char in arr]\ndf_patient_notes[\"pn_history_padded\"] = df_patient_notes[\"pn_history_clean\"].apply(list).apply(list_of_chars_to_num)\ndf_patient_notes[\"pn_history_padded\"] = sequence.pad_sequences( df_patient_notes[\"pn_history_padded\"], maxlen=max_note_length).tolist()\ndf_patient_notes[\"label\"] = sequence.pad_sequences( df_patient_notes[\"label\"], maxlen=max_note_length).tolist()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:43:35.581005Z","iopub.execute_input":"2022-02-27T19:43:35.581575Z","iopub.status.idle":"2022-02-27T19:44:30.282169Z","shell.execute_reply.started":"2022-02-27T19:43:35.581534Z","shell.execute_reply":"2022-02-27T19:44:30.281142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_2D_target_from_label_array(label_list):\n    arr = np.array(label_list)\n    b = np.zeros((arr.size, num_of_features+1))\n    b[np.arange(arr.size),arr] = 1\n    return b\n\ndef get_2D_input_from_char_integers(integer_list):\n    arr = np.array(integer_list)\n    b = np.zeros((arr.size, alphabet_size))\n    b[np.arange(arr.size),arr] = 1\n    return b","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:44:30.283938Z","iopub.execute_input":"2022-02-27T19:44:30.284173Z","iopub.status.idle":"2022-02-27T19:44:30.290839Z","shell.execute_reply.started":"2022-02-27T19:44:30.284144Z","shell.execute_reply":"2022-02-27T19:44:30.289992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainable_cases = df_train[\"pn_num\"].unique()\ndf_notes_train = df_patient_notes[df_patient_notes[\"pn_num\"].isin(trainable_cases)].copy()\ndf_notes_train[\"label_2D\"] = df_notes_train[\"label\"].apply(get_2D_target_from_label_array)\ndf_notes_train[\"pn_history_padded\"] = df_notes_train[\"pn_history_padded\"].apply(np.array)\ndf_notes_train[\"pn_history_padded_2D\"] = df_notes_train[\"pn_history_padded\"].apply(get_2D_input_from_char_integers)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:44:30.292228Z","iopub.execute_input":"2022-02-27T19:44:30.294135Z","iopub.status.idle":"2022-02-27T19:44:31.468985Z","shell.execute_reply.started":"2022-02-27T19:44:30.294084Z","shell.execute_reply":"2022-02-27T19:44:31.468018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MODEL**","metadata":{}},{"cell_type":"code","source":"def get_model():\n    input1 = layers.Input(shape=(max_note_length,))\n   # x = layers.CategoryEncoding(input_shape=(max_note_length,), num_tokens=alphabet_size, output_mode=\"one_hot\")(input1)\n    x = layers.Embedding(input_dim=alphabet_size, output_dim=alphabet_size, input_length=max_note_length, mask_zero=True)(input1)\n    x = layers.Conv1D(filters=512, kernel_size=7, padding='same', activation='elu')(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Conv1D(filters=256, kernel_size=7, padding='same', activation='elu')(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='elu')(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='elu')(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Conv1D(filters=num_of_features+1, kernel_size=3, padding='same', activation='softmax')(x)\n    \n    model = tf.keras.Model(input1, x)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:59:30.950545Z","iopub.execute_input":"2022-02-27T19:59:30.950926Z","iopub.status.idle":"2022-02-27T19:59:30.962366Z","shell.execute_reply.started":"2022-02-27T19:59:30.950894Z","shell.execute_reply":"2022-02-27T19:59:30.961526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:59:38.520099Z","iopub.execute_input":"2022-02-27T19:59:38.520655Z","iopub.status.idle":"2022-02-27T19:59:38.619193Z","shell.execute_reply.started":"2022-02-27T19:59:38.520603Z","shell.execute_reply":"2022-02-27T19:59:38.618264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n              loss=tf.keras.losses.CategoricalCrossentropy(\n    from_logits=False, label_smoothing=0.0, axis=-1,\n    name='categorical_crossentropy'\n))\n\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n              loss=tf.keras.losses.MeanSquaredError())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(df_notes_train[\"pn_history_padded\"].tolist())\nY = np.array(df_notes_train[\"label_2D\"].tolist())","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:59:53.38962Z","iopub.execute_input":"2022-02-27T19:59:53.390067Z","iopub.status.idle":"2022-02-27T19:59:53.973484Z","shell.execute_reply.started":"2022-02-27T19:59:53.390034Z","shell.execute_reply":"2022-02-27T19:59:53.972841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X, Y, validation_split=0.2, shuffle= True,epochs=2, batch_size=256)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:59:55.235887Z","iopub.execute_input":"2022-02-27T19:59:55.236319Z","iopub.status.idle":"2022-02-27T20:02:35.45011Z","shell.execute_reply.started":"2022-02-27T19:59:55.236288Z","shell.execute_reply":"2022-02-27T20:02:35.449313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samples = df_notes_train.sample(n=10)\nsamples_x = np.array(samples[\"pn_history_padded\"].tolist())\nsamples_y_gt = np.array(samples[\"label_2D\"].tolist())\nsamples_y = model.predict(samples_x)\n\n\nfor i in range(0, len(samples)):\n    print(f\"------- SAMPLE {i}\")\n    print(samples[\"pn_history_clean\"].iloc[i])\n    sample_labels = np.argmax(samples_y[i], axis=1)\n    print(sample_labels)\n    #print(samples_y[i][0])\n    #print(samples_y[i][25])\n    #print(samples_y[i][45])\n    sample_labels = np.argmax(samples_y_gt[i], axis=1)\n    #print(sample_labels)\n    #for wrd in range(0, len(sample_labels)):\n    #    if sample_labels[wrd] != num_of_features:\n    #        print(f\"Word at {wrd} - feature: {feature_mapping[sample_labels[wrd]]}\")\n    #        print(tokenizer.sequences_to_texts([[samples[\"pn_history_tokenized\"].tolist()[i][wrd]]]))\n    #        print(df_features[df_features[\"feature_num\"] == feature_mapping[sample_labels[wrd]]].iloc[0][\"feature_text\"])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-27T20:07:17.771101Z","iopub.execute_input":"2022-02-27T20:07:17.771462Z","iopub.status.idle":"2022-02-27T20:07:18.123932Z","shell.execute_reply.started":"2022-02-27T20:07:17.771425Z","shell.execute_reply":"2022-02-27T20:07:18.123115Z"},"trusted":true},"execution_count":null,"outputs":[]}]}