{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"padding: 5px; min-height:60px; margin: 10px; border-radius: 5px; background: #b3ffb3; display: flex;\n  justify-content: center;\n  align-content: center;\n  flex-direction: column;\">\n    <h1 style=\"margin:auto; text-align:center;color: black;\"> PyTorch QA/NER hybrid approach to identify important spans </h1>\n</div>\n\n## ****** Update (ver. 22) ******\n\nI compared mine to ynakama's and I realized I was ignoring the examples with empty annotations. This version includes those samples and also has some improvements for the model and metric computation.\n\nNow there is better CV/LB: 5 fold average CV = 0.86, 5 fold average LB = 0.86\n\n---\n## Description\n\nThis approach is kind of like QA and kind of like NER, so I'm calling it QA/NER hybrid. It is like NER because each token will be classified and it is like QA because I am putting the feature text at the beginning of the text (similar to how the question is put in front of the context). The idea is that the classifier will indicate 1 or 0 if that specific token is important for the feature that is put in the input. The feature text will help guide the model to find the right spans. To the extent of my knowledge this is how multi-span QA is done because the typical extractive QA is mainly good for single span prediction.\n\n\nThis uses the Hugging Face Trainer to do 5-fold validation of various base models. This is just a starting point, so please comment if you notice anything that could be improved! There are lots of possibilities for boosting the score so this should be an interesting competition! Happy Kaggling ü¶¢ (it's supposed to be a Goose). To see the actual training results, check out the versions mentioned in the updates below or check out the [Weights and Biases Report at the end of this notebook.](#Weights-and-Biases-Report)\n\nNote: I'm using base models but performance will likely improve with model size!\n\nThis notebook is heavily based off of [my notebook](https://www.kaggle.com/nbroad/bigbird-ner-training-pt-gpu-feedback-prize), the work of others in the Feedback Prize competition, and some of the [Hugging Face example scripts.](https://github.com/huggingface/transformers/tree/master/examples/pytorch) \n\nThe inference notebook is [here](https://www.kaggle.com/nbroad/qa-ner-hybrid-infer-nbme)   \nThere is a bit of a discrepancy between CV (0.830) and LB (.809)\n\n\n\n\n\n## ****** Update (ver. 12) ******\n\nI tried 5 different models including `xlnet` and `mpnet` but they failed miserably. Version 14 swaps those two out for `bert` and `roberta`. Results in [Weights and Biases Report at the end of this notebook.](#Weights-and-Biases-Report)\n\n## ****** Update (ver. 14) ******\nI thought it would be interesting to show how flexible this notebook is by training 5 different models on 5 different folds. The model definition doesn't change and it still plugs in nicely to the trainer. The models used are: [`bert-base-cased`, `albert-base-v2`, `google/electra-base-discriminator`, `microsoft/deberta-v3-base`, `roberta-base`]. The fast tokenizer for DeBERTa v2/3 is not in the current version of `transformers` but I attach a dataset and add a cell to allow it to be used here. I increase the max length to 512, but I'm not padding every sample to max length, just to the longest sequence in a batch. I use the `group_by_length` flag in `TrainingArguments` which speeds up training by reducing unnecessary padding. Results in [Weights and Biases Report at the end of this notebook.](#Weights-and-Biases-Report)\n\n## ****** Update (ver. 15/16) ******\nI realized that there are some subtle differences in tokenizers that can make the labeling step break. `roberta` will tokenize `This is a sentence` as `['This', 'ƒ†is', 'ƒ†a', 'ƒ†sentence']` and the offset mappings will start the last three tokens at the letter and not the whitespace. Specifically, the offset mapping for the second token is (5,7) and not (4,7) where 5 is the index of `i`.  `deberta-v3` on the other hand will start the offset on the whitespace. The same sentence is tokenized as `['‚ñÅThis', '‚ñÅis', '‚ñÅa', '‚ñÅsentence']` and the offset mapping for the second token is (4,7). This makes it slightly more annoying to label and decode. This version fixes that labeling and decoding process. So far I have only noticed this in DeBERTa, but I've changed the tokenize function and the decoding function.\n\n\n## ****** Update (ver. 17) ******\n\nTrying a BioBert/PubmedBert model for 5 folds.\n\n## ****** Update (ver. 18) ******\n\nThanks to @ryotak12's [discussion](https://www.kaggle.com/c/nbme-score-clinical-patient-notes/discussion/305599), I realized I made a mistake when making folds. I previously split across case_num and feature_num, but this leaks notes between folds. It has now been changed to StratifiedGroupKFold. These leaks are likely the reason for the large discrepancy between train CV and lb.  \nFurthermore, I now use the annotation corrections by @yasufuminakama [link here](https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train?scriptVersionId=87264998&cellId=17) and I also include some pseudolabeling that I did using my 5-folds of deberta-v3-base (from Version 16). \nThis run uses 5 different models, same as ver 14 but I swap bert-base for biobert/pubmedbert\n\n\n## ****** Update (ver. 20) ******\n\nSame as 18 but with 5 folds of deberta-v3-base\n\n## ****** Update (ver. 21) ******\n\nSame as 20 but no pseudolabels and this time, it actually uses the label corrections","metadata":{}},{"cell_type":"markdown","source":"## Steps to include fast tokenizer for deberta v2 or v3\n\nThis must be done before importing transformers","metadata":{}},{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path / convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in [\n    \"tokenization_deberta_v2.py\",\n    \"tokenization_deberta_v2_fast.py\",\n    \"deberta__init__.py\",\n]:\n    if str(filename).startswith(\"deberta\"):\n        filepath = deberta_v2_path / str(filename).replace(\"deberta\", \"\")\n    else:\n        filepath = deberta_v2_path / filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir / filename, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:08.225942Z","iopub.execute_input":"2022-03-23T02:51:08.226196Z","iopub.status.idle":"2022-03-23T02:51:08.239332Z","shell.execute_reply.started":"2022-03-23T02:51:08.226168Z","shell.execute_reply":"2022-03-23T02:51:08.238628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports, setup, and arguments hidden in next cell","metadata":{}},{"cell_type":"code","source":"import os\nfrom typing import Any\nfrom datetime import datetime\nfrom collections import Counter\nfrom pathlib import Path\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom itertools import chain\nfrom functools import partial\nfrom ast import literal_eval\n\nimport torch\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_recall_fscore_support\nimport plotly.express as px\nimport plotly.offline as pyo\n\npyo.init_notebook_mode()\nimport pandas as pd\nimport numpy as np\nfrom datasets import load_dataset, Dataset\n\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    HfArgumentParser,\n    Trainer,\n    TrainingArguments,\n    set_seed,\n    logging,\n)\nfrom transformers.file_utils import ModelOutput\n\nlogging.set_verbosity(logging.WARNING)\n%env TOKENIZERS_PARALLELISM=false\n\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n\n    model_name_or_path: str = field(\n        metadata={\n            \"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"\n        }\n    )\n\n\n@dataclass\nclass DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n\n    k_folds: int = field(\n        default=5, metadata={\"help\": \"How many folds for kfold validation\"}\n    )\n    num_proc: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    max_seq_length: int = field(\n        default=None,\n        metadata={\n            \"help\": \"The maximum total input sequence length after tokenization. If set, sequences longer \"\n            \"than this will be truncated, sequences shorter will be padded.\"\n        },\n    )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-23T02:51:08.275137Z","iopub.execute_input":"2022-03-23T02:51:08.275444Z","iopub.status.idle":"2022-03-23T02:51:08.343964Z","shell.execute_reply.started":"2022-03-23T02:51:08.275416Z","shell.execute_reply":"2022-03-23T02:51:08.343224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"DEBUG = False\n\n# all_models = [\n#     'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n#     'albert-base-v2',\n#     'google/electra-base-discriminator',\n#     'microsoft/deberta-v3-base',\n#     'roberta-base'\n# ]\nall_models = [\"../input/deberta-v3-base/deberta-v3-base\"] * 5\n\nmodel_args = ModelArguments(\n    model_name_or_path=all_models[0],\n)\ndata_args = DataTrainingArguments(\n    k_folds=5,\n    max_seq_length=512,\n    num_proc=2,\n)\ntraining_args = TrainingArguments(\n    output_dir=\"model\",\n    do_train=True,\n    do_eval=True,\n    do_predict=True,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    gradient_accumulation_steps=2,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    num_train_epochs=5,\n    lr_scheduler_type=\"linear\",\n    warmup_ratio=0.1,\n    logging_steps=75,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"no\",\n    seed=18,\n    fp16=False,\n    report_to=\"wandb\",\n    group_by_length=True,\n)\nset_seed(training_args.seed)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:08.345592Z","iopub.execute_input":"2022-03-23T02:51:08.346075Z","iopub.status.idle":"2022-03-23T02:51:08.356449Z","shell.execute_reply.started":"2022-03-23T02:51:08.346039Z","shell.execute_reply":"2022-03-23T02:51:08.355661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Quick EDA","metadata":{}},{"cell_type":"code","source":"feats_df = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/features.csv\")\nnotes_df = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/patient_notes.csv\")\ntrain_df = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:08.450867Z","iopub.execute_input":"2022-03-23T02:51:08.451453Z","iopub.status.idle":"2022-03-23T02:51:08.779869Z","shell.execute_reply.started":"2022-03-23T02:51:08.451409Z","shell.execute_reply":"2022-03-23T02:51:08.779115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### features.csv \n\nDescription from hosts\n> `features.csv` - The rubric of features (or key concepts) for each clinical case.  \n`feature_num` - A unique identifier for each feature.  \n`case_num` - A unique identifier for each case.  \n`feature_text` - A description of the feature.  \n\nThe `feature_text` values will be prepended onto the texts before being tokenized, similar to QA.","metadata":{}},{"cell_type":"code","source":"display(feats_df.head())\n\n# 131/143 of the features are unique and it looks like some have OR delimiting multiple names\nlen(feats_df), feats_df.feature_text.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:08.781615Z","iopub.execute_input":"2022-03-23T02:51:08.781954Z","iopub.status.idle":"2022-03-23T02:51:08.797935Z","shell.execute_reply.started":"2022-03-23T02:51:08.781915Z","shell.execute_reply":"2022-03-23T02:51:08.796954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### patient_notes.csv\n\nDescription from hosts  \n> `patient_notes.csv` - A collection of about 40,000 Patient Note history portions. Only a subset of these have features annotated. You may wish to apply unsupervised learning techniques on the notes without annotations. The patient notes in the test set are not included in the public version of this file.  \n`pn_num` - A unique identifier for each patient note.  \n`case_num` - A unique identifier for the clinical case a patient note represents.  \n`pn_history` - The text of the encounter as recorded by the test taker.  \n\nThis is the file that we will use as the text to be tokenized. The `num` columns will be used to link each text to `train.csv`\n\nIt looks like there are only 10 different cases and about 42k different notes.","metadata":{}},{"cell_type":"code","source":"display(notes_df.head())\nprint(\"DataFrame shape\", notes_df.shape)\nprint(\"Unique case_num values\", notes_df.case_num.unique())\nprint(\"Number of unique pn_num\", notes_df.pn_num.nunique())\nprint(\"Number of unique note texts\", notes_df.pn_history.nunique())","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:08.79913Z","iopub.execute_input":"2022-03-23T02:51:08.799606Z","iopub.status.idle":"2022-03-23T02:51:08.940633Z","shell.execute_reply.started":"2022-03-23T02:51:08.799568Z","shell.execute_reply":"2022-03-23T02:51:08.939782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Somewhat unequal distribution of notes for each case","metadata":{}},{"cell_type":"code","source":"px.histogram(notes_df, x=\"case_num\", color=\"case_num\")","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:08.944797Z","iopub.execute_input":"2022-03-23T02:51:08.946913Z","iopub.status.idle":"2022-03-23T02:51:09.731472Z","shell.execute_reply.started":"2022-03-23T02:51:08.946868Z","shell.execute_reply":"2022-03-23T02:51:09.730803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## `train.csv`\n\nDescription from hosts\n> `train.csv` - Feature annotations for 1000 of the patient notes, 100 for each of ten cases.  \n`id` - Unique identifier for each patient note / feature pair.  \n`pn_num` - The patient note annotated in this row.  \n`feature_num` - The feature annotated in this row.  \n`case_num` - The case to which this patient note belongs.  \n`annotation` - The text(s) within a patient note indicating a feature. A feature may be indicated multiple times within a single note.  \n`location` - Character spans indicating the location of each annotation within the note. Multiple spans may be needed to represent an annotation, in which case the spans are delimited by a semicolon ;.","metadata":{}},{"cell_type":"code","source":"print(train_df.shape)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:09.733579Z","iopub.execute_input":"2022-03-23T02:51:09.734313Z","iopub.status.idle":"2022-03-23T02:51:09.747492Z","shell.execute_reply.started":"2022-03-23T02:51:09.734273Z","shell.execute_reply":"2022-03-23T02:51:09.74674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking for blank annotations\n\nAbout 4.4k rows have blank annotations.","metadata":{}},{"cell_type":"code","source":"blank_annotations = train_df[\"annotation\"] == \"[]\"\nblank_locations = train_df[\"location\"] == \"[]\"\nboth_blank = (train_df[\"annotation\"] == train_df[\"location\"]) & blank_annotations\n\nsum(blank_annotations), sum(blank_locations), sum(both_blank)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:09.748832Z","iopub.execute_input":"2022-03-23T02:51:09.749269Z","iopub.status.idle":"2022-03-23T02:51:09.778017Z","shell.execute_reply.started":"2022-03-23T02:51:09.749228Z","shell.execute_reply":"2022-03-23T02:51:09.777284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Looking at distribution of case numbers in train.csv","metadata":{}},{"cell_type":"code","source":"px.histogram(train_df, x=\"case_num\", color=\"case_num\")","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:09.779317Z","iopub.execute_input":"2022-03-23T02:51:09.779687Z","iopub.status.idle":"2022-03-23T02:51:09.916773Z","shell.execute_reply.started":"2022-03-23T02:51:09.779651Z","shell.execute_reply":"2022-03-23T02:51:09.916105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Looking at distribution of patient note numbers in train.csv","metadata":{}},{"cell_type":"code","source":"px.histogram(train_df, x=\"pn_num\", color=\"case_num\")","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:09.917855Z","iopub.execute_input":"2022-03-23T02:51:09.918284Z","iopub.status.idle":"2022-03-23T02:51:10.058711Z","shell.execute_reply.started":"2022-03-23T02:51:09.918246Z","shell.execute_reply":"2022-03-23T02:51:10.057948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Equal numbers of features for each case_num","metadata":{}},{"cell_type":"code","source":"px.histogram(train_df, x=\"feature_num\", color=\"case_num\", nbins=1000)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:10.059925Z","iopub.execute_input":"2022-03-23T02:51:10.060545Z","iopub.status.idle":"2022-03-23T02:51:10.204762Z","shell.execute_reply.started":"2022-03-23T02:51:10.060504Z","shell.execute_reply":"2022-03-23T02:51:10.203976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data cleanup\n\nThe annotation and location columns are loaded as strings. This turns them back into lists.","metadata":{}},{"cell_type":"code","source":"train_df[\"anno_list\"] = [literal_eval(x) for x in train_df.annotation]\ntrain_df[\"loc_list\"]  = [literal_eval(x) for x in train_df.location]\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:10.205886Z","iopub.execute_input":"2022-03-23T02:51:10.20672Z","iopub.status.idle":"2022-03-23T02:51:10.445889Z","shell.execute_reply.started":"2022-03-23T02:51:10.20668Z","shell.execute_reply":"2022-03-23T02:51:10.445211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stratified KFold\n\nWithout any leaks this time ;)  \nThanks @theoviel https://www.kaggle.com/c/nbme-score-clinical-patient-notes/discussion/305599#1678215","metadata":{}},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=data_args.k_folds, random_state=training_args.seed, shuffle=True)\n\nnotes_df[\"fold\"] = -1\n\nfor fold, (_, val_idx) in enumerate(skf.split(notes_df, y=notes_df[\"case_num\"])):\n    notes_df.loc[val_idx, \"fold\"] = fold\n    \ncounts = notes_df.groupby([\"fold\", \"pn_num\"], as_index=False).count()\n\n# If the number of rows is the same as the number of \n# unique pn_num, then each pn_num is only in one fold.\n# Also if all the counts=1\nprint(counts.shape, counts.pn_num.nunique(), counts.case_num.unique())\ncounts","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:10.447308Z","iopub.execute_input":"2022-03-23T02:51:10.447854Z","iopub.status.idle":"2022-03-23T02:51:10.501948Z","shell.execute_reply.started":"2022-03-23T02:51:10.447812Z","shell.execute_reply":"2022-03-23T02:51:10.501102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged = train_df.merge(notes_df, how=\"left\")\nmerged = merged.merge(feats_df, how=\"left\")\n\nmerged.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:10.503243Z","iopub.execute_input":"2022-03-23T02:51:10.503584Z","iopub.status.idle":"2022-03-23T02:51:10.54785Z","shell.execute_reply.started":"2022-03-23T02:51:10.503544Z","shell.execute_reply":"2022-03-23T02:51:10.546959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correcting some annotations\n\nHuge shoutout to @yasufuminakama for this work: https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train?scriptVersionId=87264998&cellId=17","metadata":{}},{"cell_type":"code","source":"# incorrect annotations\nmerged.loc[338, \"anno_list\"] =  '[\"father heart attack\"]'\nmerged.loc[338, \"loc_list\"] =  '[\"764 783\"]'\n\nmerged.loc[621, \"anno_list\"] =  '[\"for the last 2-3 months\", \"over the last 2 months\"]'\nmerged.loc[621, \"loc_list\"] =  '[\"77 100\", \"398 420\"]'\n\nmerged.loc[655, \"anno_list\"] =  '[\"no heat intolerance\", \"no cold intolerance\"]'\nmerged.loc[655, \"loc_list\"] =  '[\"285 292;301 312\", \"285 287;296 312\"]'\n\nmerged.loc[1262, \"anno_list\"] =  '[\"mother thyroid problem\"]'\nmerged.loc[1262, \"loc_list\"] =  '[\"551 557;565 580\"]'\n\nmerged.loc[1265, \"anno_list\"] =  '[\\'felt like he was going to \"pass out\"\\']'\nmerged.loc[1265, \"loc_list\"] =  '[\"131 135;181 212\"]'\n\nmerged.loc[1396, \"anno_list\"] =  '[\"stool , with no blood\"]'\nmerged.loc[1396, \"loc_list\"] =  '[\"259 280\"]'\n\nmerged.loc[1591, \"anno_list\"] =  '[\"diarrhoe non blooody\"]'\nmerged.loc[1591, \"loc_list\"] =  '[\"176 184;201 212\"]'\n\nmerged.loc[1615, \"anno_list\"] =  '[\"diarrhea for last 2-3 days\"]'\nmerged.loc[1615, \"loc_list\"] =  '[\"249 257;271 288\"]'\n\nmerged.loc[1664, \"anno_list\"] =  '[\"no vaginal discharge\"]'\nmerged.loc[1664, \"loc_list\"] =  '[\"822 824;907 924\"]'\n\nmerged.loc[1714, \"anno_list\"] =  '[\"started about 8-10 hours ago\"]'\nmerged.loc[1714, \"loc_list\"] =  '[\"101 129\"]'\n\nmerged.loc[1929, \"anno_list\"] =  '[\"no blood in the stool\"]'\nmerged.loc[1929, \"loc_list\"] =  '[\"531 539;549 561\"]'\n\nmerged.loc[2134, \"anno_list\"] =  '[\"last sexually active 9 months ago\"]'\nmerged.loc[2134, \"loc_list\"] =  '[\"540 560;581 593\"]'\n\nmerged.loc[2191, \"anno_list\"] =  '[\"right lower quadrant pain\"]'\nmerged.loc[2191, \"loc_list\"] =  '[\"32 57\"]'\n\nmerged.loc[2553, \"anno_list\"] =  '[\"diarrhoea no blood\"]'\nmerged.loc[2553, \"loc_list\"] =  '[\"308 317;376 384\"]'\n\nmerged.loc[3124, \"anno_list\"] =  '[\"sweating\"]'\nmerged.loc[3124, \"loc_list\"] =  '[\"549 557\"]'\n\nmerged.loc[3858, \"anno_list\"] =  '[\"previously as regular\", \"previously eveyr 28-29 days\", \"previously lasting 5 days\", \"previously regular flow\"]'\nmerged.loc[3858, \"loc_list\"] =  '[\"102 123\", \"102 112;125 141\", \"102 112;143 157\", \"102 112;159 171\"]'\n\nmerged.loc[4373, \"anno_list\"] =  '[\"for 2 months\"]'\nmerged.loc[4373, \"loc_list\"] =  '[\"33 45\"]'\n\nmerged.loc[4763, \"anno_list\"] =  '[\"35 year old\"]'\nmerged.loc[4763, \"loc_list\"] =  '[\"5 16\"]'\n\nmerged.loc[4782, \"anno_list\"] =  '[\"darker brown stools\"]'\nmerged.loc[4782, \"loc_list\"] =  '[\"175 194\"]'\n\nmerged.loc[4908, \"anno_list\"] =  '[\"uncle with peptic ulcer\"]'\nmerged.loc[4908, \"loc_list\"] =  '[\"700 723\"]'\n\nmerged.loc[6016, \"anno_list\"] =  '[\"difficulty falling asleep\"]'\nmerged.loc[6016, \"loc_list\"] =  '[\"225 250\"]'\n\nmerged.loc[6192, \"anno_list\"] =  '[\"helps to take care of aging mother and in-laws\"]'\nmerged.loc[6192, \"loc_list\"] =  '[\"197 218;236 260\"]'\n\nmerged.loc[6380, \"anno_list\"] =  '[\"No hair changes\", \"No skin changes\", \"No GI changes\", \"No palpitations\", \"No excessive sweating\"]'\nmerged.loc[6380, \"loc_list\"] =  '[\"480 482;507 519\", \"480 482;499 503;512 519\", \"480 482;521 531\", \"480 482;533 545\", \"480 482;564 582\"]'\n\nmerged.loc[6562, \"anno_list\"] =  '[\"stressed due to taking care of her mother\", \"stressed due to taking care of husbands parents\"]'\nmerged.loc[6562, \"loc_list\"] =  '[\"290 320;327 337\", \"290 320;342 358\"]'\n\nmerged.loc[6862, \"anno_list\"] =  '[\"stressor taking care of many sick family members\"]'\nmerged.loc[6862, \"loc_list\"] =  '[\"288 296;324 363\"]'\n\nmerged.loc[7022, \"anno_list\"] =  '[\"heart started racing and felt numbness for the 1st time in her finger tips\"]'\nmerged.loc[7022, \"loc_list\"] =  '[\"108 182\"]'\n\nmerged.loc[7422, \"anno_list\"] =  '[\"first started 5 yrs\"]'\nmerged.loc[7422, \"loc_list\"] =  '[\"102 121\"]'\n\nmerged.loc[8876, \"anno_list\"] =  '[\"No shortness of breath\"]'\nmerged.loc[8876, \"loc_list\"] =  '[\"481 483;533 552\"]'\n\nmerged.loc[9027, \"anno_list\"] =  '[\"recent URI\", \"nasal stuffines, rhinorrhea, for 3-4 days\"]'\nmerged.loc[9027, \"loc_list\"] =  '[\"92 102\", \"123 164\"]'\n\nmerged.loc[9938, \"anno_list\"] =  '[\"irregularity with her cycles\", \"heavier bleeding\", \"changes her pad every couple hours\"]'\nmerged.loc[9938, \"loc_list\"] =  '[\"89 117\", \"122 138\", \"368 402\"]'\n\nmerged.loc[9973, \"anno_list\"] =  '[\"gaining 10-15 lbs\"]'\nmerged.loc[9973, \"loc_list\"] =  '[\"344 361\"]'\n\nmerged.loc[10513, \"anno_list\"] =  '[\"weight gain\", \"gain of 10-16lbs\"]'\nmerged.loc[10513, \"loc_list\"] =  '[\"600 611\", \"607 623\"]'\n\nmerged.loc[11551, \"anno_list\"] =  '[\"seeing her son knows are not real\"]'\nmerged.loc[11551, \"loc_list\"] =  '[\"386 400;443 461\"]'\n\nmerged.loc[11677, \"anno_list\"] =  '[\"saw him once in the kitchen after he died\"]'\nmerged.loc[11677, \"loc_list\"] =  '[\"160 201\"]'\n\nmerged.loc[12124, \"anno_list\"] =  '[\"tried Ambien but it didnt work\"]'\nmerged.loc[12124, \"loc_list\"] =  '[\"325 337;349 366\"]'\n\nmerged.loc[12279, \"anno_list\"] =  '[\"heard what she described as a party later than evening these things did not actually happen\"]'\nmerged.loc[12279, \"loc_list\"] =  '[\"405 459;488 524\"]'\n\nmerged.loc[12289, \"anno_list\"] =  '[\"experienced seeing her son at the kitchen table these things did not actually happen\"]'\nmerged.loc[12289, \"loc_list\"] =  '[\"353 400;488 524\"]'\n\nmerged.loc[13238, \"anno_list\"] =  '[\"SCRACHY THROAT\", \"RUNNY NOSE\"]'\nmerged.loc[13238, \"loc_list\"] =  '[\"293 307\", \"321 331\"]'\n\nmerged.loc[13297, \"anno_list\"] =  '[\"without improvement when taking tylenol\", \"without improvement when taking ibuprofen\"]'\nmerged.loc[13297, \"loc_list\"] =  '[\"182 221\", \"182 213;225 234\"]'\n\nmerged.loc[13299, \"anno_list\"] =  '[\"yesterday\", \"yesterday\"]'\nmerged.loc[13299, \"loc_list\"] =  '[\"79 88\", \"409 418\"]'\n\nmerged.loc[13845, \"anno_list\"] =  '[\"headache global\", \"headache throughout her head\"]'\nmerged.loc[13845, \"loc_list\"] =  '[\"86 94;230 236\", \"86 94;237 256\"]'\n\nmerged.loc[14083, \"anno_list\"] =  '[\"headache generalized in her head\"]'\nmerged.loc[14083, \"loc_list\"] =  '[\"56 64;156 179\"]'","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:10.549544Z","iopub.execute_input":"2022-03-23T02:51:10.549806Z","iopub.status.idle":"2022-03-23T02:51:10.631075Z","shell.execute_reply.started":"2022-03-23T02:51:10.549771Z","shell.execute_reply":"2022-03-23T02:51:10.630417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged[\"anno_list\"] = [\n    literal_eval(x) if isinstance(x, str) else x for x in merged[\"anno_list\"]\n]\nmerged[\"loc_list\"] = [\n    literal_eval(x) if isinstance(x, str) else x for x in merged[\"loc_list\"]\n]\n\n# Before version 21, I mistakenly removed these\n# merged = merged[merged[\"anno_list\"].map(len)!=0].copy().reset_index(drop=True)\n\nmerged.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:10.634606Z","iopub.execute_input":"2022-03-23T02:51:10.634816Z","iopub.status.idle":"2022-03-23T02:51:10.675117Z","shell.execute_reply.started":"2022-03-23T02:51:10.634791Z","shell.execute_reply":"2022-03-23T02:51:10.67439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing and Adding Labels\n\nSince the labeling is given to us at the character level, the tokenizer needs to have `return_offsets_mapping=True` which returns the start and end indexes for each token. These indexes can then map the char-level labels to tokens. The loss for the model must be calculated at the token level.\n\nHere are the 3 scenarios where I mark a token as a label.\n\n\n\n`token_start, token_end` are the start and end indexes of the token. start is inclusive, end is exclusive, just like indexing a string.  \n`label_start, label_end` are the start and end indexes of the label. start is inclusive, end is exclusive, just like indexing a string.\n\n1. `token_start <= label_start < token_end`  \nThe token span overlaps with the start of the label span.\n2. `token_start < label_end <= token_end`  \nThe token span overlaps with the end of the label span.\n3. `label_start <= token_start < label_end`  \nIf it doesn't fall into (1) or (2), then the token span is entirely in the label span.","metadata":{}},{"cell_type":"code","source":"def location_to_ints(loc_list):\n    to_return = []\n\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n\n    return to_return\n\n\ndef process_feature_text(text):\n    text = text.replace(\"-OR-\", \" or \")\n    return text.replace(\"-\", \" \")\n\n\ndef tokenize(example, tokenizer):\n\n    tokenized_inputs = tokenizer(\n        example[\"feature_text\"],\n        example[\"pn_history\"],\n        truncation=\"only_second\",\n        max_length=data_args.max_seq_length,\n        padding=False,\n        return_offsets_mapping=True,\n    )\n\n    # labels should be float\n    labels = [0.0] * len(tokenized_inputs[\"input_ids\"])\n    tokenized_inputs[\"locations\"] = location_to_ints(example[\"loc_list\"])\n    tokenized_inputs[\"sequence_ids\"] = tokenized_inputs.sequence_ids()\n\n    if len(tokenized_inputs[\"locations\"]) > 0:\n        for idx, (seq_id, offsets) in enumerate(\n            zip(tokenized_inputs[\"sequence_ids\"], tokenized_inputs[\"offset_mapping\"])\n        ):\n            if seq_id is None or seq_id == 0:\n                # don't calculate loss on question part or special tokens\n                labels[idx] = -100.0\n                continue\n\n            token_start, token_end = offsets\n            for label_start, label_end in tokenized_inputs[\"locations\"]:\n                if (\n                    token_start <= label_start < token_end\n                    or token_start < label_end <= token_end\n                    or label_start <= token_start < label_end\n                ):\n                    labels[idx] = 1.0  # labels should be float\n\n    tokenized_inputs[\"labels\"] = labels\n\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:10.676552Z","iopub.execute_input":"2022-03-23T02:51:10.677062Z","iopub.status.idle":"2022-03-23T02:51:10.688086Z","shell.execute_reply.started":"2022-03-23T02:51:10.677025Z","shell.execute_reply":"2022-03-23T02:51:10.687325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:10.69019Z","iopub.execute_input":"2022-03-23T02:51:10.69161Z","iopub.status.idle":"2022-03-23T02:51:10.70959Z","shell.execute_reply.started":"2022-03-23T02:51:10.691583Z","shell.execute_reply":"2022-03-23T02:51:10.708843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (\n    \"deberta-v2\" in model_args.model_name_or_path\n    or \"deberta-v3\" in model_args.model_name_or_path\n):\n    from transformers.models.deberta_v2 import DebertaV2TokenizerFast\n\n    tokenizer = DebertaV2TokenizerFast.from_pretrained(model_args.model_name_or_path)\nelse:\n    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:10.710516Z","iopub.execute_input":"2022-03-23T02:51:10.71293Z","iopub.status.idle":"2022-03-23T02:51:11.748898Z","shell.execute_reply.started":"2022-03-23T02:51:10.712895Z","shell.execute_reply":"2022-03-23T02:51:11.748064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Double-checking alignment is good\n\nIt will print out a random one each time, so you can keep running it to check as many as you want.","metadata":{}},{"cell_type":"code","source":"random_sample = merged.sample(n=1).iloc[0]\nexample = {\n    \"feature_text\": random_sample.feature_text,\n    \"pn_history\": random_sample.pn_history,\n    \"loc_list\": random_sample.loc_list,\n    \"annotations\": random_sample.anno_list,\n}\nprint(example, \"\\n\\n\")\ntokenized = partial(tokenize, tokenizer=tokenizer)(example)\n\n\ntokens = tokenizer.tokenize(\n    example[\"feature_text\"], example[\"pn_history\"], add_special_tokens=True\n)\n\nprint(\"Locations\")\nprint(example[\"loc_list\"], \"\\n\")\n\nprint(\"Annotations\")\nprint(example[\"annotations\"], \"\\n\")\n\nprint(\"Token | Label | Token Offsets\")\nzipped = list(zip(tokens, tokenized[\"labels\"], tokenized[\"offset_mapping\"]))\n[x for x in zipped if x[1] > 0]","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:11.750214Z","iopub.execute_input":"2022-03-23T02:51:11.751534Z","iopub.status.idle":"2022-03-23T02:51:11.770364Z","shell.execute_reply.started":"2022-03-23T02:51:11.751492Z","shell.execute_reply":"2022-03-23T02:51:11.769561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndataset = Dataset.from_pandas(\n    merged[\n        [\n            \"id\",\n            \"case_num\",\n            \"pn_num\",\n            \"feature_num\",\n            \"loc_list\",\n            \"pn_history\",\n            \"feature_text\",\n            \"fold\",\n        ]\n    ]\n)\n\nif DEBUG:\n    dataset = dataset.shuffle().select(range(1000))\n# This can take up to a minute\ntokenized_dataset = dataset.map(\n    partial(tokenize, tokenizer=tokenizer),\n    desc=\"Tokenizing and adding labels\",\n    num_proc=data_args.num_proc,\n    batched=False,\n)\ntokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:51:11.771658Z","iopub.execute_input":"2022-03-23T02:51:11.772101Z","iopub.status.idle":"2022-03-23T02:52:12.43971Z","shell.execute_reply.started":"2022-03-23T02:51:11.772064Z","shell.execute_reply":"2022-03-23T02:52:12.438897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenized_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:52:12.44153Z","iopub.execute_input":"2022-03-23T02:52:12.441817Z","iopub.status.idle":"2022-03-23T02:52:12.45024Z","shell.execute_reply.started":"2022-03-23T02:52:12.441775Z","shell.execute_reply":"2022-03-23T02:52:12.448371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How long are the texts?\n\n(Before ver. 12) Using roberta, a maximum sequence length of 384 would probably be fine. I chose 416 just to make sure nothing gets truncated.   \n(After ver. 12) I'm testing multiple different tokenizers, so now I'm dynamically padding to the longest.\n\nI'm not padding to max length across all samples, just within a batch. This will save compute time. Moreover, I've used the `group_by_length` flag in the `TrainingArguments` which means the excess padding will be limited, speeding up training even more.","metadata":{}},{"cell_type":"code","source":"tokenized_lengths = [len(x) for x in tokenized_dataset[\"input_ids\"]]\n\nprint(\"The longest is\", max(tokenized_lengths))\n\npx.histogram(x=tokenized_lengths, labels={\"x\":\"tokenized_length\"})","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:52:12.451961Z","iopub.execute_input":"2022-03-23T02:52:12.452811Z","iopub.status.idle":"2022-03-23T02:52:14.063213Z","shell.execute_reply.started":"2022-03-23T02:52:12.452773Z","shell.execute_reply":"2022-03-23T02:52:14.062552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup Weights and Biases for tracking experiments\n\nIf you put `report_to=\"none\"` in the `TrainingArguments` then it won't use Weights and Biases. I like using it because it helps keep track of experiments.","metadata":{}},{"cell_type":"code","source":"if \"wandb\" in training_args.report_to:\n    !pip install -U wandb -qq\n    import wandb\n\n    from kaggle_secrets import UserSecretsClient\n\n    user_secrets = UserSecretsClient()\n    wandb_key = user_secrets.get_secret(\"wandb\")\n\n    os.environ[\"WANDB_PROJECT\"] = \"NBME\"\n    os.environ[\"WANDB_RUN_GROUP\"] = \"hybrid_\" + datetime.now().strftime(\n        \"%Y-%m-%d %H:%M\"\n    )\n    wandb.login(key=wandb_key)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:52:14.06448Z","iopub.execute_input":"2022-03-23T02:52:14.064865Z","iopub.status.idle":"2022-03-23T02:52:23.17341Z","shell.execute_reply.started":"2022-03-23T02:52:14.064831Z","shell.execute_reply":"2022-03-23T02:52:23.172633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model backbone flexibility\n\nThis custom model is a bit funky because I tried to make it versatile to whichever model you would like to use (bert, roberta, electra, etc.). It works by pulling the proper classes based on the model_type specified in the config object. If you know of a better way, by all means please share! Unfortunately there are minor differences in how each model is set up, so there are exceptions here and there for individual models.","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass TokenClassifierOutput(ModelOutput):\n    \"\"\"\n    Base class for outputs of token classification models.\n    Args:\n        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) :\n            Classification loss.\n        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n            Classification scores (before crf).\n        crf (crf output)\n    \"\"\"\n\n    loss: Any = None\n    proba: Any = None\n\n\n# Functions that are similar across all models\ndef __init__(self, config):\n    super(self.PreTrainedModel, self).__init__(config)\n\n    kwargs = {\"add_pooling_layer\": False}\n    if config.model_type not in {\"bert\", \"roberta\"}:\n        kwargs = {}\n    setattr(self, self.backbone_name, self.ModelClass(config, **kwargs))\n\n    classifier_dropout_name = None\n    for key in dir(config):\n        if (\"classifier\" in key or \"hidden\" in key) and \"dropout\" in key:\n            if getattr(config, key) is not None:\n                classifier_dropout_name = key\n                break\n\n    if classifier_dropout_name is None:\n        raise ValueError(\"Cannot infer dropout name in config\")\n    classifier_dropout = getattr(config, classifier_dropout_name)\n    self.dropout = torch.nn.Dropout(classifier_dropout)\n    self.classifier = torch.nn.Linear(config.hidden_size, 1)\n\n\ndef forward(\n    self,\n    input_ids=None,\n    attention_mask=None,\n    token_type_ids=None,\n    position_ids=None,\n    labels=None,\n):\n\n    outputs = getattr(self, self.backbone_name)(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n\n    sequence_output = outputs[0]\n\n    sequence_output = self.dropout(sequence_output)\n    logits = self.classifier(sequence_output)\n\n    loss = None\n    if labels is not None:\n        loss_fct = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n        loss = loss_fct(logits.view(-1, 1), labels.view(-1, 1))\n\n        # this ignores the part of the sequence that got -100 as labels\n        loss = torch.masked_select(loss, labels.view(-1, 1) > -1).mean()\n\n    return TokenClassifierOutput(\n        loss=loss,\n        proba=logits.sigmoid(),\n    )","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:57:59.060371Z","iopub.execute_input":"2022-03-23T02:57:59.060752Z","iopub.status.idle":"2022-03-23T02:57:59.085619Z","shell.execute_reply.started":"2022-03-23T02:57:59.060711Z","shell.execute_reply":"2022-03-23T02:57:59.084792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(config, init=False):\n    model_type = type(config).__name__[: -len(\"config\")]\n    if model_type == \"Bart\":\n        name = f\"{model_type}PretrainedModel\"\n    else:\n        name = f\"{model_type}PreTrainedModel\"\n    PreTrainedModel = getattr(__import__(\"transformers\", fromlist=[name]), name)\n    name = f\"{model_type}Model\"\n    ModelClass = getattr(__import__(\"transformers\", fromlist=[name]), name)\n\n    model = type(\n        \"CustomModel\",\n        (PreTrainedModel,),\n        {\"__init__\": __init__, \"forward\": forward},\n    )\n\n    model._keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    model._keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    model.PreTrainedModel = PreTrainedModel\n    model.ModelClass = ModelClass\n    model.backbone_name = config.model_type\n\n    # changes deberta-v2 --> deberta\n    if \"deberta\" in model.backbone_name:\n        model.backbone_name = \"deberta\"\n\n    if init:\n        return model(config)\n    return model\n\n\ndef get_pretrained(model_name_or_path, config, **kwargs):\n\n    model = get_model(config, init=False)\n\n    return model.from_pretrained(\n        pretrained_model_name_or_path=model_name_or_path,\n        config=config,\n        **kwargs,\n    )","metadata":{"execution":{"iopub.status.busy":"2022-03-23T03:14:28.605261Z","iopub.execute_input":"2022-03-23T03:14:28.605645Z","iopub.status.idle":"2022-03-23T03:14:28.627763Z","shell.execute_reply.started":"2022-03-23T03:14:28.605604Z","shell.execute_reply":"2022-03-23T03:14:28.625485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass CustomDataCollator(DataCollatorForTokenClassification):\n    \"\"\"\n    Data collator that will dynamically pad the inputs received, as well as the labels.\n    Have to modify to make label tensors float and not int.\n    \"\"\"\n\n    tokenizer\n    padding = True\n    max_length = None\n    pad_to_multiple_of = None\n    label_pad_token_id = -100\n    return_tensors = \"pt\"\n\n    def torch_call(self, features):\n        batch = super().torch_call(features)\n        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n\n        batch[label_name] = torch.tensor(batch[label_name], dtype=torch.float32)\n\n        return batch","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:58:06.406912Z","iopub.execute_input":"2022-03-23T02:58:06.407158Z","iopub.status.idle":"2022-03-23T02:58:06.415212Z","shell.execute_reply.started":"2022-03-23T02:58:06.407128Z","shell.execute_reply":"2022-03-23T02:58:06.414494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting locations based on predictions\n\nFor each token if the value after a logit goes through a sigmoid is > 0.5, then it is an important token. This is a simple approach, and it would be good to test out different numbers in CV.","metadata":{}},{"cell_type":"code","source":"def kaggle_metrics(eval_prediction, dataset):\n    \"\"\"\n    For `compute_metrics`\n\n    Use partial for the args and kwargs to pass other data\n    into the `compute_metrics` function.\n    \"\"\"\n\n    pred_idxs = get_location_predictions(eval_prediction.predictions, dataset)\n\n    all_labels = []\n    all_preds = []\n    for preds, locations, text in zip(\n        pred_idxs,\n        dataset[\"locations\"],\n        dataset[\"pn_history\"],\n    ):\n\n        num_chars = len(text)\n        char_labels = np.zeros((num_chars), dtype=bool)\n\n        for start, end in locations:\n            char_labels[start:end] = 1\n\n        char_preds = np.zeros((num_chars))\n\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n            if (\n                text[start_idx].isspace()\n                and start_idx > 0\n                and not char_preds[start_idx - 1]\n            ):\n                char_preds[start_idx] = 0\n\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n\n    results = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\")\n\n    return {\"precision\": results[0], \"recall\": results[1], \"f1\": results[2]}\n\n\ndef get_location_predictions(preds, dataset):\n    \"\"\"\n    Finds the prediction indexes at the character level.\n    \"\"\"\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(\n        preds, dataset[\"offset_mapping\"], dataset[\"sequence_ids\"]\n    ):\n        start_idx = None\n        current_preds = []\n        for p, o, s_id in zip(pred, offsets, seq_ids):\n            if s_id is None or s_id == 0:\n                continue\n\n            if p > 0.5:\n                if start_idx is None:\n                    start_idx = o[0]\n                end_idx = o[1]\n            elif start_idx is not None:\n                current_preds.append((start_idx, end_idx))\n                start_idx = None\n\n        if start_idx is not None:\n            current_preds.append((start_idx, end_idx))\n\n        all_predictions.append(current_preds)\n\n    return all_predictions","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:58:07.608968Z","iopub.execute_input":"2022-03-23T02:58:07.609344Z","iopub.status.idle":"2022-03-23T02:58:07.624002Z","shell.execute_reply.started":"2022-03-23T02:58:07.609307Z","shell.execute_reply":"2022-03-23T02:58:07.623401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train!\n\nThis will train all k folds and saving each model. ","metadata":{}},{"cell_type":"code","source":"if DEBUG:\n    training_args.num_train_epochs = 1\n\nprevious_config = None\nfor fold, model_name in zip(range(data_args.k_folds), all_models):\n\n    \"\"\"\n    This seems to get reset after each fold and can print out a lot of\n    information that I don't really care about. When debugging, you should\n    definitely not hide these messages though üòâ\n    \"\"\"\n    if not DEBUG:\n        logging.set_verbosity(logging.CRITICAL)\n\n    print(f\"Starting training for fold {fold} using {model_name}\")\n\n    config = AutoConfig.from_pretrained(\n        model_name,\n    )\n    using_deberta_v2_3 = \"deberta-v2\" in model_name or \"deberta-v3\" in model_name\n\n    # Only re-run when the config changes\n    if previous_config is None or previous_config != config.__dict__:\n\n        if using_deberta_v2_3:\n            from transformers.models.deberta_v2 import DebertaV2TokenizerFast\n\n            tokenizer = DebertaV2TokenizerFast.from_pretrained(model_name)\n        else:\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        data_collator = CustomDataCollator(\n            tokenizer,\n            pad_to_multiple_of=8 if training_args.fp16 else None,\n            padding=\"longest\",\n        )\n\n        print(\"Tokenizing dataset\")\n        tokenized_dataset = dataset.map(\n            partial(\n                tokenize,\n                tokenizer=tokenizer,\n            ),\n            desc=\"Tokenizing and adding labels\",\n            num_proc=4,\n        )\n\n    model_args.model_name_or_path = model_name  # So wandb will track it\n\n    if \"wandb\" in training_args.report_to:\n        wandb_config = {\n            **model_args.__dict__,\n            **data_args.__dict__,\n            **training_args.__dict__,\n            **config.__dict__,\n        }\n        wandb_config[\"fold\"] = fold\n        wandb.init(config=wandb_config, group=os.environ[\"WANDB_RUN_GROUP\"])\n\n    model = get_pretrained(model_name, config)\n\n    train_dataset = tokenized_dataset.filter(lambda x: x[\"fold\"] != fold, num_proc=data_args.num_proc)\n    eval_dataset = tokenized_dataset.filter(lambda x: x[\"fold\"] == fold, num_proc=data_args.num_proc)\n\n    compute_metrics = partial(kaggle_metrics, dataset=eval_dataset)\n\n    # Initialize our Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n    trainer.save_model(f\"fold{fold}\")\n\n    if \"wandb\" in training_args.report_to:\n        wandb.finish()\n\n    previous_config = config.__dict__","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-03-23T02:58:13.434985Z","iopub.execute_input":"2022-03-23T02:58:13.435432Z","iopub.status.idle":"2022-03-23T03:13:32.316204Z","shell.execute_reply.started":"2022-03-23T02:58:13.435389Z","shell.execute_reply":"2022-03-23T03:13:32.315066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Weights and Biases Report\n\n<iframe src=\"https://wandb.ai/nbroad/NBME/reports/Hybrid-QA-NER-Train-Results--VmlldzoxNTE4Mjk1\" style=\"border:none;height:1024px;width:100%\">","metadata":{}}]}