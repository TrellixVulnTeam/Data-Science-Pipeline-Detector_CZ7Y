{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## This is the partner notebook to [my training notebook](https://www.kaggle.com/nbroad/qa-ner-hybrid-train-nbme)\n\n(Version 26) 5 folds of deberta-v3-base on leakless-training, annotation corrections, no pseudolabels, AND blank annotations. CV/LB is now 0.86/0.86  \n(Version 25) 5 folds of deberta-v3-base on leakless-training, annotation corrections, and no pseudolabels.  \n(Version 24) 5 folds of deberta-v3-base on leakless-training, no annotation corrections, and some pseudolabels.  \n(Version 23) Same as Version 21 except biobert/pubmedbert instead of bert-base, leakless-training, annotation corrections, and some pseudolabels.  \n(Version 21) 1 fold of bert-base, albert-v2-base, electra-base, deberta-v3-base, roberta-base (similar to versions 11-13) but with corrected training and decoding. I added some additional logic to the `add_to_char_preds` because it was skipping over spaces and the answers were then getting broken up across spaces. (lb .838)  \n(Version 17) 5 folds of deberta-v3-base trained for 5 epochs (lb .826) (with fixed training and decoding)\n##### ****Version 14/15/16 had been trained with a bug related to the offset mapping. This is fixed in Version 17  ****\n(Version 15) 5 folds of deberta-v3-base trained for 3 epochs  (lb .659)  \n(Version 14) 5 folds of deberta-v3-base trained for 5 epochs (lb .671)  \n\n(Version 11-13) This is using 1 fold of bert-base, albert-v2-base, electra-base, deberta-v3-base, roberta-base. This is just an experiment and there is plenty of submission time to do at least 5 folds of each!  (lb .797)\n\n(Version 10) This is using a 5 fold roberta base model so the score will likely go up when using a large model. Please feel free to leave any comments or questions! (lb .809)","metadata":{}},{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:59:18.042842Z","iopub.execute_input":"2022-03-23T11:59:18.043245Z","iopub.status.idle":"2022-03-23T11:59:18.076361Z","shell.execute_reply.started":"2022-03-23T11:59:18.043146Z","shell.execute_reply":"2022-03-23T11:59:18.075406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom functools import partial\nimport itertools\nfrom typing import Any\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\nfrom datasets import Dataset\nfrom transformers import (\n    Trainer,\n    TrainingArguments,\n    DataCollatorForTokenClassification,\n    AutoTokenizer,\n    AutoConfig,\n    logging,\n)\nfrom transformers.file_utils import ModelOutput\n\n\nlogging.set_verbosity(logging.WARNING)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:59:18.081309Z","iopub.execute_input":"2022-03-23T11:59:18.081778Z","iopub.status.idle":"2022-03-23T11:59:21.055131Z","shell.execute_reply.started":"2022-03-23T11:59:18.081741Z","shell.execute_reply":"2022-03-23T11:59:21.054407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    \n    max_length = 512\n    n_folds = 5\n    model_path = \"../input/5-deb-v3/fold{fold}\"\n    args = TrainingArguments(\n        output_dir=\".\",\n        per_device_eval_batch_size=64,\n        dataloader_num_workers=2,\n    )\n    num_proc = 2","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:59:21.058372Z","iopub.execute_input":"2022-03-23T11:59:21.058576Z","iopub.status.idle":"2022-03-23T11:59:21.086792Z","shell.execute_reply.started":"2022-03-23T11:59:21.058549Z","shell.execute_reply":"2022-03-23T11:59:21.086028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/test.csv\")\nnotes_df = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/patient_notes.csv\") \nfeats_df = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/features.csv\")\nmerged = test_df.merge(notes_df, how=\"left\")   \nmerged = merged.merge(feats_df, how=\"left\")\n\n# if we sort by length of texts, then there should be less unnecessary padding --> faster inference\nmerged[\"length\"] = [len(x) for x in merged[\"pn_history\"]]\nmerged = merged.sort_values(by=\"length\")\n\nmerged.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:59:21.08898Z","iopub.execute_input":"2022-03-23T11:59:21.089247Z","iopub.status.idle":"2022-03-23T11:59:21.436975Z","shell.execute_reply.started":"2022-03-23T11:59:21.089193Z","shell.execute_reply":"2022-03-23T11:59:21.436202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_feature_text(text):\n    return text.replace(\"-OR-\", \" or \").replace(\"-\", \" \")\n\ndef tokenize(examples, tokenizer, max_length):\n    tokenized_inputs =  tokenizer(\n        examples[\"feature_text\"],\n        examples[\"pn_history\"],\n        padding=True,\n        max_length=max_length,\n        truncation=\"only_second\",\n        return_offsets_mapping=True\n    )\n    tokenized_inputs[\"sequence_ids\"] = [tokenized_inputs.sequence_ids(i) for i in range(len(tokenized_inputs[\"input_ids\"]))]\n    return tokenized_inputs\n\nds = Dataset.from_pandas(merged)\n\nds = ds.map(lambda x: {\"feature_text\": process_feature_text(x[\"feature_text\"])}, num_proc=CFG.num_proc)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:59:21.438347Z","iopub.execute_input":"2022-03-23T11:59:21.438623Z","iopub.status.idle":"2022-03-23T11:59:21.715329Z","shell.execute_reply.started":"2022-03-23T11:59:21.438586Z","shell.execute_reply":"2022-03-23T11:59:21.714499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass TokenClassifierOutput(ModelOutput):\n    \"\"\"\n    Base class for outputs of token classification models.\n    Args:\n        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) :\n            Classification loss.\n        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n            Classification scores (before crf).\n    \"\"\"\n\n    loss: Any = None\n    logits: Any = None\n\n\n# Functions that are similar across all models\ndef __init__(self, config):\n    super(self.PreTrainedModel, self).__init__(config)\n\n    kwargs = {\"add_pooling_layer\": False}\n    if config.model_type not in {\"bert\", \"roberta\"}:\n        kwargs = {}\n    setattr(self, self.backbone_name, self.ModelClass(config, **kwargs))\n\n    self.classifier = torch.nn.Linear(config.hidden_size, 1)\n\n\ndef forward(\n    self,\n    input_ids=None,\n    attention_mask=None,\n    token_type_ids=None,\n    position_ids=None,\n    labels=None,\n):\n\n    outputs = getattr(self, self.backbone_name)(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n\n    sequence_output = outputs[0]\n\n    logits = self.classifier(sequence_output)\n\n    loss = None\n\n    return TokenClassifierOutput(\n        loss=loss,\n        logits=logits.sigmoid(),\n    )","metadata":{"execution":{"iopub.status.busy":"2022-03-23T12:05:20.128837Z","iopub.execute_input":"2022-03-23T12:05:20.129116Z","iopub.status.idle":"2022-03-23T12:05:20.141442Z","shell.execute_reply.started":"2022-03-23T12:05:20.12908Z","shell.execute_reply":"2022-03-23T12:05:20.140757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(config, init=False):\n    model_type = type(config).__name__[: -len(\"config\")]\n    if model_type == \"Bart\":\n        name = f\"{model_type}PretrainedModel\"\n    else:\n        name = f\"{model_type}PreTrainedModel\"\n    PreTrainedModel = getattr(__import__(\"transformers\", fromlist=[name]), name)\n    name = f\"{model_type}Model\"\n    ModelClass = getattr(__import__(\"transformers\", fromlist=[name]), name)\n\n    model = type(\n        \"CustomModel\",\n        (PreTrainedModel,),\n        {\"__init__\": __init__, \"forward\": forward},\n    )\n\n    model._keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    model._keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    model.PreTrainedModel = PreTrainedModel\n    model.ModelClass = ModelClass\n    model.backbone_name = config.model_type\n\n    # changes deberta-v2 --> deberta\n    if \"deberta\" in model.backbone_name:\n        model.backbone_name = \"deberta\"\n\n    if init:\n        return model(config)\n    return model\n\n\ndef get_pretrained(model_name_or_path, config, **kwargs):\n\n    model = get_model(config, init=False)\n\n    return model.from_pretrained(\n        pretrained_model_name_or_path=model_name_or_path,\n        config=config,\n        **kwargs,\n    )","metadata":{"execution":{"iopub.status.busy":"2022-03-23T12:05:21.555303Z","iopub.execute_input":"2022-03-23T12:05:21.555892Z","iopub.status.idle":"2022-03-23T12:05:21.565093Z","shell.execute_reply.started":"2022-03-23T12:05:21.555851Z","shell.execute_reply":"2022-03-23T12:05:21.564191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_char_preds(preds, dataset):\n    \"\"\"\n    Finds the prediction indexes at the character level.\n    \"\"\"\n    all_predictions = []\n    for pred, offsets, seq_ids, text in zip(\n        preds, dataset[\"offset_mapping\"], dataset[\"sequence_ids\"], dataset[\"pn_history\"]\n    ):\n        char_preds = np.zeros((len(text)))\n        for p, o, s_id in zip(pred, offsets, seq_ids):\n            if s_id is None or s_id == 0:\n                continue\n\n            start, end = o\n            char_preds[start:end] = p\n\n        all_predictions.append(char_preds)\n\n    return all_predictions\n\n# https://www.kaggle.com/code/yasufuminakama?scriptVersionId=87264998&cellId=11\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-03-23T12:05:21.86151Z","iopub.execute_input":"2022-03-23T12:05:21.862152Z","iopub.status.idle":"2022-03-23T12:05:21.873761Z","shell.execute_reply.started":"2022-03-23T12:05:21.862112Z","shell.execute_reply":"2022-03-23T12:05:21.872913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env TOKENIZERS_PARALLELISM=false\n\nall_char_preds = [np.zeros(len(t)) for t in ds[\"pn_history\"]]\n\nfor fold in range(CFG.n_folds):\n\n    # This gets reset each loop for some reason\n    logging.set_verbosity(logging.WARNING)\n    \n    model_path = CFG.model_path.format(fold=fold)\n\n    config = AutoConfig.from_pretrained(model_path)\n    print(f\"Getting predictions for {config._name_or_path}\")\n    \n    if \"deberta-v2\" in config.model_type:\n        from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n        tokenizer = DebertaV2TokenizerFast.from_pretrained(model_path)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n    tokenized_ds = ds.map(\n        partial(tokenize, tokenizer=tokenizer, max_length=CFG.max_length), \n        batched=True,\n        num_proc=CFG.num_proc,\n    )\n    \n    model = get_model(config=config, init=True)\n    model.load_state_dict(torch.load(model_path+\"/pytorch_model.bin\"))\n        \n    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=\"longest\")\n    \n    trainer = Trainer(\n        model=model,\n        args=CFG.args,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n    )\n\n    results = trainer.predict(tokenized_ds)\n    \n    char_preds = get_char_preds(results.predictions, tokenized_ds)\n    all_char_preds = [x+y for x,y in zip(all_char_preds, char_preds)]\n\n    torch.cuda.empty_cache()\nall_char_preds = [cp/CFG.n_folds for cp in all_char_preds]","metadata":{"execution":{"iopub.status.busy":"2022-03-23T12:14:45.732313Z","iopub.execute_input":"2022-03-23T12:14:45.733134Z","iopub.status.idle":"2022-03-23T12:15:15.778412Z","shell.execute_reply.started":"2022-03-23T12:14:45.733095Z","shell.execute_reply":"2022-03-23T12:15:15.777595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_predictions = get_results(all_char_preds)\n\nsubmission_df = pd.DataFrame(data={\n            \"id\": tokenized_ds[\"id\"], \n            \"location\": location_predictions\n        })\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\ndisplay(submission_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-03-23T12:15:18.46099Z","iopub.execute_input":"2022-03-23T12:15:18.461287Z","iopub.status.idle":"2022-03-23T12:15:18.477423Z","shell.execute_reply.started":"2022-03-23T12:15:18.461248Z","shell.execute_reply":"2022-03-23T12:15:18.476638Z"},"trusted":true},"execution_count":null,"outputs":[]}]}