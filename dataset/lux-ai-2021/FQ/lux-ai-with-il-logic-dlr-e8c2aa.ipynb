{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install kaggle-environments -U > /dev/null 2>&1\n!cp -r ../input/lux-ai-2021/* .","metadata":{"execution":{"iopub.status.busy":"2021-12-06T02:45:22.675975Z","iopub.execute_input":"2021-12-06T02:45:22.676481Z","iopub.status.idle":"2021-12-06T02:45:31.67266Z","shell.execute_reply.started":"2021-12-06T02:45:22.67639Z","shell.execute_reply":"2021-12-06T02:45:31.671793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"todo：需要把所有的dest判断都与in——city结合。允许在城市上堆叠\n\n==============================================================================\n网络输入修改为19层\n=========================================================\n2021.12.03 18:00\n最新情况\n\n让网络支持了citytile的选择\n\n夜间出城逻辑改回了yzy写的\n\ncart彻底没了，除非网络输出再加一维。而且其行为逻辑也不太好想\n\n此版本是训练一遍的情况。我正在跑25遍的。\n=============================================================================================================================\n增加了unit找城市避难的功能，把相关的超参数改成了20到40天（即：允许考虑救援和避难的时间段）\n\n\n救援改成了多unit救1个city。但救援会出现city之间共用unit的情况,我就先按城市规模大小排序，先救大城市\n\n\n关于cart仅仅加了 build时0.1的概率为cart，0.9为worker。所以cart现在是不会动的\n\n\n怀疑有个bug（step out city应该是白天吧），我自行改了，并打上了？？？的注释\n\n\n现在的问题主要是不用其他数据集的话，没法改变网络的功能，使得cart只能纯规则实现，也完全没考虑道路等级问题\n\n\n此notebook是把训练次数改为1的结果，暂时还没训25次，那样大约需要1小时。\n\n\ncart一个可能的逻辑：车为空时出去找worker，找到了就强制要求worker transfer 尤其是把不是木头的资源给cart（因为这些如果被用来build city就浪费了）\n车基本满了，就找城市回去。如果cart能用别的方法实现那更好，不行就把这个实现算了。毕竟分数是一方面，咱们的工作如果没涉及cart，最后给分怕是要吃亏。\n\n还有一个观察到的问题，有的city最后会剩余大量资源。这本来是网络应该考虑到的因素，也许应该用规则强制一下。但不知道怎么改。","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport json\nfrom pathlib import Path\nimport os\nimport random\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2021-12-06T02:45:31.67439Z","iopub.execute_input":"2021-12-06T02:45:31.67465Z","iopub.status.idle":"2021-12-06T02:45:36.879949Z","shell.execute_reply.started":"2021-12-06T02:45:31.674622Z","shell.execute_reply":"2021-12-06T02:45:36.879215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed_value):\n    #random will be the same if getting the same seed_value \n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    #Sets the seed for generating random numbers for the current GPU\n    torch.manual_seed(seed_value)\n    #set Python Hash Seed to a specific number,making it able to appear again\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    #indicate if cuda is available \n    if torch.cuda.is_available(): \n        #Sets the seed for generating random numbers for the current GPU\n        torch.cuda.manual_seed(seed_value)\n        #Sets the seed for generating random numbers on all GPUs.\n        torch.cuda.manual_seed_all(seed_value)\n        #if True, causes cuDNN to only use deterministic convolution algorithms\n        torch.backends.cudnn.deterministic = True\n        # if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.\n        torch.backends.cudnn.benchmark = True\n\nseed = 42\nseed_everything(seed)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T02:45:36.881213Z","iopub.execute_input":"2021-12-06T02:45:36.881474Z","iopub.status.idle":"2021-12-06T02:45:36.933446Z","shell.execute_reply.started":"2021-12-06T02:45:36.881441Z","shell.execute_reply":"2021-12-06T02:45:36.932757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"def to_label(action):\n    #将action按照空格分成一个长为3的字符串列表\n    strs = action.split(' ')\n    unit_id = strs[1]\n    \n    if strs[0]==\"r\":\n        return 0,0,1,0,int(strs[1]),int(strs[2])\n    if strs[0]==\"bw\":\n        return 0,0,1,1,int(strs[1]),int(strs[2])\n    \n    #m代表move，说明是一个可动单位\n    if strs[0] == 'm':\n        #north、south、west、east\n        label = {'c': None, 'n': 0, 's': 1, 'w': 2, 'e': 3}[strs[2]]\n    elif strs[0] == 'bcity':\n        label = 4\n    else:\n        label = None\n    #return unit_id, label, is_citytile, buildworker, x, y\n    return unit_id, label, 0, 0, 0, 0\n\n#判断资源是否所有单位全部耗尽的函数（即一方全部消失在黑暗中，游戏结束）\ndef depleted_resources(obs):\n    for u in obs['updates']:\n        if u.split(' ')[0] == 'r':\n            return False\n    return True\n\n\ndef create_dataset_from_json(episode_dir, team_name='Toad Brigade'): \n    obses = {}\n    samples = []\n    append = samples.append\n    #从../input/lux-ai-episodes寻找训练模型，并加入episodes中\n    episodes = [path for path in Path(episode_dir).glob('*.json') if 'output' not in path.name]\n    #tqdm为一个显示进度条的库，使得处理进度GUI化\n    for filepath in tqdm(episodes): \n        with open(filepath) as f:\n            #json_load表示从json文件中读取的数据形成的对象\n            json_load = json.load(f)\n\n        ep_id = json_load['info']['EpisodeId']\n        #返回rewards数组最大值的索引（本质上是选择最后得分更高的player进行学习）\n        index = np.argmax([r or 0 for r in json_load['rewards']])\n        #仅使用自己team定义的json文件\n        if json_load['info']['TeamNames'][index] != team_name:\n            continue\n        #对一个json文件的每一步进行操作\n        for i in range(len(json_load['steps'])-1):\n            #如果在该步执行后存在该\"Active\"状态说明我方存活\n            if json_load['steps'][i][index]['status'] == 'ACTIVE':\n                #如果该步状态后依然我方active，load下一步状态的所有动作，放入actions中\n                actions = json_load['steps'][i+1][index]['action']\n                #load当前步骤后的全地图资源状态\n                obs = json_load['steps'][i][0]['observation']\n                \n                if depleted_resources(obs):\n                    break\n                \n                obs['player'] = index\n                #将obs重新整合为一个字典\n                obs = dict([\n                    (k,v) for k,v in obs.items() \n                    if k in ['step', 'updates', 'player', 'width', 'height']\n                ])\n                obs_id = f'{ep_id}_{i}'\n                #obses为所有obs字典的列表集合\n                obses[obs_id] = obs\n                                \n                for action in actions:\n                    #label为当前执行动作对应的下标\n                    unit_id, label, is_citytile, buildworker, x, y = to_label(action)\n                    if label is not None:\n                        if (not is_citytile):\n                            #如果动作有方向或者待在city中，那么添加到列表samples中，定义的append类似于C的宏定义\n                            append((0,0,0,obs_id, unit_id, label))\n                            #LuxDataset里面调用is_citytile, x, y, obs_id, unit_id, action = self.samples[idx]\n                        else:\n                            append((1,x,y,obs_id, 0, 5+buildworker))\n                        \n#最终得到全地图每一步执行后的包括资源、各player单位的字典集合obses,所有存活单位每一回合的移动samples\n    return obses, samples","metadata":{"execution":{"iopub.status.busy":"2021-12-06T02:45:36.935455Z","iopub.execute_input":"2021-12-06T02:45:36.935715Z","iopub.status.idle":"2021-12-06T02:45:36.953479Z","shell.execute_reply.started":"2021-12-06T02:45:36.935683Z","shell.execute_reply":"2021-12-06T02:45:36.952774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"episode_dir = '../input/lux-ai-episodes'\nobses, samples = create_dataset_from_json(episode_dir)\nprint('obses:', len(obses), 'samples:', len(samples))","metadata":{"execution":{"iopub.status.busy":"2021-12-06T02:45:36.954556Z","iopub.execute_input":"2021-12-06T02:45:36.954982Z","iopub.status.idle":"2021-12-06T02:45:41.647147Z","shell.execute_reply.started":"2021-12-06T02:45:36.954947Z","shell.execute_reply":"2021-12-06T02:45:41.646439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#将所有sample的最后一个label，即移动方向制作成list\nlabels = [sample[-1] for sample in samples]\nactions = ['north', 'south', 'west', 'east', 'bcity','research','buildworker']\n#打印各个方向动作的总次数\nfor value, count in zip(*np.unique(labels, return_counts=True)):\n    print(f'{actions[value]:^7}: {count:>3}')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T02:45:41.648646Z","iopub.execute_input":"2021-12-06T02:45:41.64915Z","iopub.status.idle":"2021-12-06T02:45:41.692056Z","shell.execute_reply.started":"2021-12-06T02:45:41.649111Z","shell.execute_reply":"2021-12-06T02:45:41.690423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Input For Training","metadata":{}},{"cell_type":"code","source":"# Input for Neural Network\ndef make_input(is_citytile, obs, unit_id, x, y):\n    #x,y coordiantes\n    width, height = obs['width'], obs['height']\n    #//为整数除法，相当于逻辑右移一位\n    x_shift = (32 - width) // 2\n    y_shift = (32 - height) // 2\n    cities = {}\n    #制作一个三维全为浮点数0的列表\n    b = np.zeros((19, 32, 32), dtype=np.float32)\n    \n    for update in obs['updates']:\n        #split方法通过指定分隔符分隔字符串\n        strs = update.split(' ')\n        input_identifier = strs[0]\n        #a player unit which contains all information\n        #str[1]为0时表示worker，为1时表示cart\n        if input_identifier == 'u':\n            x = int(strs[4]) + x_shift\n            y = int(strs[5]) + y_shift\n            wood = int(strs[7])\n            coal = int(strs[8])\n            uranium = int(strs[9])\n            if (unit_id == strs[3] and not is_citytile):\n                # Position and Cargo\n                b[:2, x, y] = (\n                    1,#在第0层有unit的位置标上1\n                    (wood + coal + uranium) / 100#在第1层对应有unit的位置，标记其装载材料的比例\n                )\n            else:\n                # Units\n                team = int(strs[2])\n                cooldown = float(strs[6])\n                idx = 2 + (team - obs['player']) % 2 * 3#结果为2或者5\n                #在第2层标记非unit_id的本方单位位置\n                #在第5层标记非unit_id的敌方单位位置\n                #在第3层标记敌我的cooldown / 6，第4层标记其装载材料的比例\n                b[idx, x, y] = 1\n                b[3:5,x,y]=(\n                    cooldown / 6,\n                    (wood + coal + uranium) / 100\n                )\n                \n        elif input_identifier == 'ct':\n            # CityTiles\n            team = int(strs[1])\n            city_id = strs[2]\n            idx=0\n            x = int(strs[3]) + x_shift\n            y = int(strs[4]) + y_shift\n            if (is_citytile and x == int(strs[3]) and y == int(strs[4])):\n                b[6:8,x,y]=(\n                    1,\n                    cities[city_id]\n                )\n            else:\n                idx = 8 + (team - obs['player']) % 2 * 2\n            #第8层标记本方citytile位置\n            #第10层标记敌方citytile位置\n            #第9层标记该city在黑夜能持续几天\n            #如果恰好是xy处的citytile，就放在6和7层\n            b[idx, x, y] = 1\n            b[9,x,y]=cities[city_id]\n        elif input_identifier == 'r':\n            # Resources\n            r_type = strs[1]\n            x = int(strs[2]) + x_shift\n            y = int(strs[3]) + y_shift\n            amt = int(float(strs[4]))\n            #第12、13、14层分别放wood、coal、uranium的位置极其装载比例\n            b[{'wood': 12, 'coal': 13, 'uranium': 14}[r_type], x, y] = amt / 800\n        elif input_identifier == 'rp':\n            # Research Points\n            team = int(strs[1])\n            rp = int(strs[2])\n            #第15层放我方研究点数\n            #第16层放敌方研究点数\n            b[15 + (team - obs['player']) % 2, :] = min(rp, 200) / 200\n        elif input_identifier == 'c':\n            # Cities参数计算\n            city_id = strs[2]\n            fuel = float(strs[3])\n            lightupkeep = float(strs[4])\n            cities[city_id] = min(fuel / lightupkeep, 10) / 10\n    \n    # Day/Night Cycle\n    b[11, :] = obs['step'] % 40 / 40\n    # Turns\n    b[17, :] = obs['step'] / 360\n    # Map Size\n    b[18, x_shift:32 - x_shift, y_shift:32 - y_shift] = 1\n\n    return b\n\n\nclass LuxDataset(Dataset):\n    def __init__(self, obses, samples):\n        self.obses = obses\n        self.samples = samples\n        \n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        is_citytile, x, y, obs_id, unit_id, action = self.samples[idx]\n        obs = self.obses[obs_id]\n        state = make_input(is_citytile, obs, unit_id, x, y)\n        \n        return state, action","metadata":{"execution":{"iopub.status.busy":"2021-12-06T02:45:41.693672Z","iopub.execute_input":"2021-12-06T02:45:41.69412Z","iopub.status.idle":"2021-12-06T02:45:41.717543Z","shell.execute_reply.started":"2021-12-06T02:45:41.694085Z","shell.execute_reply":"2021-12-06T02:45:41.716839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model for Training","metadata":{}},{"cell_type":"code","source":"# Neural Network for Lux AI\n#class torch.nn.Module是所有网络的基类，所有模型也应该继承这个类。\n#此为基础的卷积层模型\nclass BasicConv2d(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel_size, bn):\n        super().__init__()\n        #padding(int or tuple, optional) - 输入的每一条边补充0的层数,这是为了防止卷积核的大小与步长使得一些边缘数据无法被遍历，所以需要进行填充\n        #kerner_size(int or tuple) - 卷积核的尺寸\n        #输入的尺度是(N, C_in,H,W)，输出尺度（N,C_out,H_out,W_out）\n        #这里padding填充这个数量的目的是保证在一次卷积操作后其尺寸与原尺寸保持相同\n        self.conv = nn.Conv2d(\n            input_dim, output_dim, \n            kernel_size=kernel_size, \n            padding=(kernel_size[0] // 2, kernel_size[1] // 2)\n        )\n        \n        #对小批量(mini-batch)3d数据组成的4d输入进行批标准化(Batch Normalization)操作\n        #在每一个小批量（mini-batch）数据中，计算输入各个维度的均值和标准差\n        #在训练时，该层计算每次输入的均值与方差，并进行移动平均。移动平均默认的动量值为0.1。\n        #输入输出相同\n        #BatchNorm2d函数是在使用卷积计算后进行归一化，防止relu前的数据过大导致网络不稳定\n        self.bn = nn.BatchNorm2d(output_dim) if bn else None\n\n    def forward(self, x):\n        h = self.conv(x)\n        h = self.bn(h) if self.bn is not None else h\n        return h\n\n\nclass LuxNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        layers, filters = 12, 32\n        #model initialization\n        self.conv0 = BasicConv2d(19, filters, (3, 3), True)\n        #将submodules保存在一个list中。ModuleList可以像一般的Python list一样被索引，即每一个都是用于存储一个layer的\n        self.blocks = nn.ModuleList([BasicConv2d(filters, filters, (3, 3), True) for _ in range(layers)])\n        #nn.Linear（）是用于设置网络中的全连接层的，需要注意在二维图像处理的任务中\n        #全连接层的输入与输出一般都设置为二维张量，形状通常为[batch_size, size]\n        #其中前两个参数分别为in的size，7为输出的size\n        self.head_p = nn.Linear(filters, 7, bias=False)\n\n    #神经网络的前向传播\n    def forward(self, x):\n        #此为激励层的内容，relu_是一种非线性激活函数，本质上在深度神经网络上可以逼近任何类型的函数\n        #relu函数的第二个参数inplace默认值为false，表示不改变输入的数据\n        h = F.relu_(self.conv0(x))\n        for block in self.blocks:\n            h = F.relu_(h + block(h))\n        #view函数相当于resize的功能，将原来的tensor变换成指定的维度\n        #这里是将激励值乘入输入x，再将其重新张成h.size(0)*h.size(1)*剩余的形式并求和\n        h_head = (h * x[:,:1]).view(h.size(0), h.size(1), -1).sum(-1)\n        #调用head_p函数张成二维向量\n        #上面说的不太对，view成二维(h.size(0),filters)，再通过全连接层head_p，规模变为(h.size(0),7)\n        p = self.head_p(h_head)\n        return p","metadata":{"execution":{"iopub.status.busy":"2021-12-06T02:45:41.719123Z","iopub.execute_input":"2021-12-06T02:45:41.719426Z","iopub.status.idle":"2021-12-06T02:45:41.732905Z","shell.execute_reply.started":"2021-12-06T02:45:41.719394Z","shell.execute_reply":"2021-12-06T02:45:41.732125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs):\n    best_acc = 0.0\n    #动态改变学习率的sheduler\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.1)\n    \n    for epoch in range(num_epochs):\n        #显式地指定model使用gpu\n        model.cuda()\n        #在Pytorch训练中有两种模式，train为训练模式，eval为评估模式\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                #在使用pytorch构建神经网络的时候，训练过程中会在程序上方添加一句model.train()，作用是启用batch normalization和dropout\n                model.train()\n            else:\n                #测试过程中会使用model.eval()，这时神经网络会沿用batch normalization的值，并不使用dropout，即我们这里不是在训练模型\n                #那么就不需要进行dropout改变模型，故调用该函数保证dropout不变\n                model.eval()\n                \n            epoch_loss = 0.0\n            epoch_acc = 0\n            \n            dataloader = dataloaders_dict[phase]\n            #对模型进行多次训练\n            for item in tqdm(dataloader, leave=False):\n                states = item[0].cuda().float()\n                actions = item[1].cuda().long()\n                #将模型的参数梯度初始化为0\n                optimizer.zero_grad()\n                #在我们处于训练模式下时，允许计算局部梯度\n                with torch.set_grad_enabled(phase == 'train'):\n                    policy = model.forward(states)\n                    #criterion为损失函数，计算损失\n                    loss = criterion(policy, actions)\n                    #返回所有张量最大值\n                    _, preds = torch.max(policy, 1)\n\n                    #如果处于训练模式下，损失函数backward进行反向传播梯度的计算，并使用优化器的step函数来更新参数\n                    if phase == 'train':\n                        #当前Variable对leaf variable求偏导，计算好梯度\n                        loss.backward()\n                        #根据前面求出的梯度更新参数\n                        optimizer.step()\n\n                    epoch_loss += loss.item() * len(policy)\n                    epoch_acc += torch.sum(preds == actions.data)\n            #归一化\n            data_size = len(dataloader.dataset)\n            epoch_loss = epoch_loss / data_size\n            epoch_acc = epoch_acc.double() / data_size\n            if phase == 'train':\n                scheduler.step()\n            print(f'Epoch {epoch + 1}/{num_epochs} | {phase:^5} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}')\n        #产生了更好结果的情况\n        if epoch_acc > best_acc:\n            traced = torch.jit.trace(model.cpu(), torch.rand(1, 19, 32, 32))\n            traced.save('model.pth')\n            best_acc = epoch_acc","metadata":{"execution":{"iopub.status.busy":"2021-12-06T02:45:41.735717Z","iopub.execute_input":"2021-12-06T02:45:41.735922Z","iopub.status.idle":"2021-12-06T02:45:41.750067Z","shell.execute_reply.started":"2021-12-06T02:45:41.735892Z","shell.execute_reply":"2021-12-06T02:45:41.749312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LuxNet()\n#train_test_split将原始数据按照比例划分为“测试集”“训练集”\n# test_size：样本占比，如果是整数的话就是样本的数量\n\n# random_state：是随机数的种子。\n# 随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。\n#stratify是为了保持split前类的分布，保证其在训练集中的分布比例不变\ntrain, val = train_test_split(samples, test_size=0.15, random_state=42, stratify=labels)\nbatch_size = 64\n#shuffle为false表示不打乱传入的数据\n#num_workers表示了会有多少进程共同执行\ntrain_loader = DataLoader(\n    LuxDataset(obses, train), \n    batch_size=batch_size, \n    shuffle=True, \n    num_workers=2\n)\nval_loader = DataLoader(\n    LuxDataset(obses, val), \n    batch_size=batch_size, \n    shuffle=False, \n    num_workers=2\n)\ndataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n#交叉熵损失函数，一种使用对数的损失函数\ncriterion = nn.CrossEntropyLoss()\n#利用Adam(Adaptive Moment Estimation)进行优化\n#它的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳\n#lr为学习率，过高会导致不稳定与过拟合\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\ntrain_model(model, dataloaders_dict, criterion, optimizer, num_epochs=25)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-06T02:45:41.752812Z","iopub.execute_input":"2021-12-06T02:45:41.753203Z","iopub.status.idle":"2021-12-06T03:34:09.033623Z","shell.execute_reply.started":"2021-12-06T02:45:41.753169Z","shell.execute_reply":"2021-12-06T03:34:09.032771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"lux_ai_2021\", configuration={\"width\": 24, \"height\": 24, \"loglevel\": 2, \"annotations\": True}, debug=True)\nsteps = env.run(['agent.py', 'agent.py'])\nenv.render(mode=\"ipython\", width=1200, height=800)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T11:00:51.927358Z","iopub.execute_input":"2021-12-06T11:00:51.927842Z","iopub.status.idle":"2021-12-06T11:00:52.744701Z","shell.execute_reply.started":"2021-12-06T11:00:51.927807Z","shell.execute_reply":"2021-12-06T11:00:52.743961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -czf submission.tar.gz *","metadata":{"execution":{"iopub.status.busy":"2021-12-06T03:34:48.516116Z","iopub.execute_input":"2021-12-06T03:34:48.516376Z","iopub.status.idle":"2021-12-06T03:34:49.3033Z","shell.execute_reply.started":"2021-12-06T03:34:48.516344Z","shell.execute_reply":"2021-12-06T03:34:49.30231Z"},"trusted":true},"execution_count":null,"outputs":[]}]}