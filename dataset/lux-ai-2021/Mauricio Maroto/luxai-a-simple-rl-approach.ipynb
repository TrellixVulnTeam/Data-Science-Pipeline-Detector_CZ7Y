{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-21T20:18:17.660357Z","iopub.execute_input":"2021-11-21T20:18:17.660698Z","iopub.status.idle":"2021-11-21T20:18:17.695697Z","shell.execute_reply.started":"2021-11-21T20:18:17.660614Z","shell.execute_reply":"2021-11-21T20:18:17.695038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install kaggle-environments -U\n!cp -r ../input/lux-ai-2021/* .","metadata":{"execution":{"iopub.status.busy":"2021-11-21T20:18:17.697983Z","iopub.execute_input":"2021-11-21T20:18:17.69839Z","iopub.status.idle":"2021-11-21T20:18:27.184795Z","shell.execute_reply.started":"2021-11-21T20:18:17.698354Z","shell.execute_reply":"2021-11-21T20:18:27.183786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile my_agent.py\n\nTRAIN = False\n\nfrom itertools import chain, repeat\nfrom statistics import mean, stdev, median, mode\n# from time import monotonic\n# from pprint import pprint\n# from random import randint, random\n\nfrom kaggle_environments import make\n\n#from net import NN\n\n# from lux import annotate\n# from lux.constants import Constants\nfrom lux.game import Game\n# from lux.game_constants import GAME_CONSTANTS\n# from lux.game_map import RESOURCE_TYPES, Cell, Position\n#\n# import log  # no qa\n# from optuna import create_study, exceptions\n\n# import logging as lg\nfrom itertools import repeat\n#\nimport torch.nn as nn\nfrom torch.nn import functional as f\nfrom torch import as_tensor, cat, argmax, ones, tensor, mean as pt_mean, stack, log, zeros, no_grad, load, save\nfrom torch.optim import Adam\nfrom torch.distributions import Categorical\n\n\n# !pip install kaggle-environments -U > /dev/null 2>&1\n# !cp -r ../input/lux-ai-2021/* .\n\nclass NN(nn.Module):\n    def __init__(self,\n                 input_size=9,\n                 out_channels=32,\n                 kernel_size=2,\n                 hidden_layers=10,\n                 w_output_dim=7,\n                 ct_output_dim=2,\n                 kt_output_dim=2,\n                 ):\n        super().__init__()\n        self.out_channels = out_channels\n        self.w_output_dim = w_output_dim\n        self.ct_output_dim = ct_output_dim\n        self.kt_output_dim = kt_output_dim\n\n        # self.linear = nn.Linear()\n\n        self.to_conv2d = nn.Conv2d(in_channels=input_size,\n                                   out_channels=out_channels,\n                                   kernel_size=kernel_size)\n        self.conv2d = nn.Conv2d(in_channels=out_channels,\n                                out_channels=out_channels,\n                                kernel_size=kernel_size)\n\n        self.conv2ds = nn.ModuleList(\n            [self.conv2d for _ in range(hidden_layers)])\n        # self.shape_helper = ones((1, 32, 32))\n\n        self.helper = 3200\n\n        self.to_w = nn.Linear(self.helper, 64)\n        self.w_output = nn.Linear(64, self.w_output_dim)\n\n        self.to_ct = nn.Linear(self.helper, 64)\n        self.ct_output = nn.Linear(64, self.ct_output_dim)\n\n        self.to_kt = nn.Linear(self.helper, 64)\n        self.kt_output = nn.Linear(64, self.kt_output_dim)\n\n    def forward(self, x):\n        \"\"\"Forward/Predict\"\"\"\n        step_size = x.shape[0]\n        x = f.selu(self.to_conv2d(x))\n\n        for conv2d in self.conv2ds:\n            x = f.max_pool2d(f.selu(conv2d(x)), kernel_size=2, stride=1)\n        x = x.reshape(step_size, x.shape[1] * x.shape[2] * x.shape[3])\n\n        x_w = self.to_w(x)\n        logits = self.w_output(x_w).reshape(step_size, -1)\n        w_probs = f.softmax(logits, dim=-1).squeeze()\n        w_action = Categorical(w_probs).sample()\n\n        x_ct = self.to_ct(x)\n        logits = self.ct_output(x_ct).reshape(step_size, -1)\n        ct_probs = f.softmax(logits, dim=-1).squeeze()\n        ct_action = Categorical(ct_probs).sample()\n\n        return w_probs, w_action, ct_probs, ct_action\n\n\n### HP ###\nLOCALITY = 15\nTRAIN_EVERY = 40  # multiple of 360\nLR = 0.0001\nQ_BASE = 0.4\nDISC_FACTOR = 0.9999\n##########\n\n\ndef id_day_night(step):\n    res = (step % 40)\n    if res >= 0 and res < 30:\n        return 1\n    else:\n        return 2\n\n\ndef create_entity_maps(obs):\n    maps = [zeros((32, 32)) for i in range(9)]\n\n    for entity in (obs['updates']):\n        strs = entity.split(' ')\n        input_identifier = strs[0]\n\n        if input_identifier == 'r':\n            r_type = strs[1]\n            x = int(strs[2])\n            y = int(strs[3])\n            amt = int(float(strs[4]))\n            maps[0][x, y] = 1\n            maps[1][x, y] = amt / 800\n\n        if input_identifier == 'u':\n            team = 1 if int(strs[2]) == 0 else -1\n            x = int(strs[4])\n            y = int(strs[5])\n            cooldown = int(strs[6])\n            wood = int(strs[7])\n            coal = int(strs[8])\n            uranium = int(strs[9])\n            maps[2][x, y] = 1 * team\n            maps[3][x, y] = (wood + coal + uranium + 1) / 100 * team\n            maps[4][x, y] = cooldown * team\n\n        elif input_identifier == 'ct':\n            team = 1 if int(strs[1]) == 0 else -1\n            x = int(strs[3])\n            y = int(strs[4])\n            cooldown = int(strs[5])\n            maps[5][x, y] = 1 * team\n            maps[6][x, y] = cooldown * team\n\n        #####\n        # TODO Add city info\n        #####\n        # elif input_identifier == 'c':\n        #     team = 1 if int(strs[1]) == 0 else -1\n        #     fuel = int(strs[3])\n        #     lightkeepup = int(strs[4])\n        #     maps[7][:, :] = fuel / 100\n        #     maps[8][:, :] = lightkeepup / 100\n\n        elif input_identifier == 'ccd':\n            x = int(strs[1])\n            y = int(strs[2])\n            level = int(strs[3])\n            maps[7][x, y] = level\n\n    # Additional maps\n    maps[8][:, :] = id_day_night(obs['step'])\n\n    return maps\n\n\ndef create_local_entity_maps(unit,\n                             maps,\n                             pad_amt=LOCALITY + 1,\n                             locality=LOCALITY):\n    x = unit.pos.x + pad_amt\n    y = unit.pos.y + pad_amt\n\n    x_lb = x - locality\n    x_ub = x + locality + 1\n    y_lb = y - locality\n    y_ub = y + locality + 1\n\n    pad = nn.ConstantPad2d(pad_amt, 0)\n\n    new_maps = []\n    for map_ in maps:\n        new_maps.append(pad(map_)[x_lb:x_ub, y_lb:y_ub])\n\n    return new_maps\n\n\ndef make_board_inputs(maps):\n    inputs = stack(maps)\n    inputs = inputs.unsqueeze(0)\n    return inputs\n\n\ndef buffer_data():\n    \"\"\"Store and buffer data helper net\"\"\"\n    bag = []\n    while True:\n        data = yield bag\n        bag.append(data)\n\n\ndef calc_qvals(rewards, disc_factor=DISC_FACTOR, q_baseline=Q_BASE):\n    w = []\n    sum_rew = 0.0\n    for reward in reversed(rewards):\n        sum_rew *= disc_factor\n        sum_rew += reward\n        w.append(sum_rew)\n    w = list(reversed(w))\n    w = tensor(w)\n    w = w - w.quantile(q_baseline)\n    # w = (w - w.mean()) / (w.std() + 0.1)\n    w = w.tolist()\n    return w\n\n\ndef raw_action_to_unit_action(entity_id, entity, raw_action):\n    if entity_id == 'u':\n        if raw_action == 0:\n            action = entity.move('n')\n        if raw_action == 1:\n            action = entity.move('w')\n        if raw_action == 2:\n            action = entity.move('s')\n        if raw_action == 3:\n            action = entity.move('e')\n        if raw_action == 4:\n            action = entity.move('c')\n        if raw_action == 5:\n            action = entity.build_city()\n        if raw_action == 6:\n            action = entity.pillage()\n        return action\n    if entity_id == 'ct':\n        if raw_action == 0:\n            action = entity.research()\n        if raw_action == 1:\n            action = entity.build_worker()\n        return action\n\n\ngame_state = None\nnet = NN()\ntry:\n    net = net.cuda()\n    print('=> Net created')\n    sd = load('/kaggle/input/net-model/net_state_dict.pth')\n    net.load_state_dict(sd)\n    print('=> Net loaded from checkpoint')\nexcept BaseException as e:\n    print('=> Error =', e)\n    net = net.cuda()\n\noptimizer = Adam(net.parameters(), lr=LR)  # , lr=LR Adam AdamW Adamax\nboard_inputs = None\npseudo_rews = None\ncts = None\nunits = None\n\n\ndef my_agent(observation, configuration=None):\n    global game_state\n    global net\n    global board_inputs\n    global pseudo_rews\n    global cts\n    global units\n\n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n\n        cts = 0\n        # units = 1\n    else:\n        game_state._update(observation[\"updates\"])\n    player = game_state.players[observation.player]\n\n    # Prep\n    maps = create_entity_maps(observation)\n\n    units = [unit for unit in player.units]\n    units = zip(repeat('u', len(units)), units)\n\n    cts = [ct for c in list(player.cities.values()) for ct in c.citytiles]\n    cts = zip(repeat('ct', len(cts)), cts)\n\n    entities = chain(units, cts)\n\n    # Actions\n    actions = []\n    for entity_id, entity in entities:\n        maps_ = create_local_entity_maps(entity, maps)\n        board_inputs = make_board_inputs(maps_).cuda()\n        if entity_id == 'u':\n            #    if entity.can_act():\n            with no_grad():\n                # move, pillage, build-ct\n                _, w_action, _, _ = net(board_inputs)\n                action = raw_action_to_unit_action(entity_id, entity, w_action)\n                actions.append(action)\n        if entity_id == 'ct':\n            with no_grad():\n                # research, build-worker, \n                _, _, _, ct_action = net(board_inputs)\n                action = raw_action_to_unit_action(entity_id, entity, ct_action)\n                actions.append(action)\n\n    cts = len([ct for c in list(player.cities.values()) for ct in c.citytiles])\n\n    # Rewards\n    pseudo_rews = [c.fuel/10 for c in list(player.cities.values())]\n    pseudo_rews = pseudo_rews if (len(pseudo_rews) > 0) else [1]\n    pseudo_rews = [elem if elem != 0 else 1 for elem in pseudo_rews]\n    # a = 2\n    # # Generalized Mean\n    # pseudo_rews = (sum([ elem ** a for elem in pseudo_rews ]) / len(pseudo_rews)) ** (1/a)\n    pseudo_rews = sum(pseudo_rews) * cts\n\n    # Report\n    print(cts, end=' ')\n    print(len(player.units), end=' ')\n    print(f'{pseudo_rews:.0f}', end=' - ')\n\n    # Complete\n    if (observation['step'] + 1) == 360:\n        print('===> Board Ended Complete')\n\n    return actions\n\n\nrews = []\ngames = 1\nwhile TRAIN:\n    # for game in range(1, N_GAMES + 1):\n    # buffer\n    buff = buffer_data()\n    next(buff)\n\n    # ENV\n    env = make(\"lux_ai_2021\",\n                configuration={\n                    \"loglevel\": 1,\n                    \"annotations\": True\n                },\n                debug=True)\n\n    # ENV TRAINER\n    trainer = env.train([None, 'simple_agent'])\n    obs, done = trainer.reset(), False\n    steps = 1\n\n    # LOOP\n    print('=> New board')\n    while not done:\n        actions = my_agent(obs)\n        obs, reward, done, info = trainer.step(actions)\n\n        buff.send((board_inputs, pseudo_rews, cts))\n\n        if steps % TRAIN_EVERY == 0:\n            data = buff.send(None)[:-1]\n            board_inputs_ = [board_inputs_ for board_inputs_, _, _ in data]\n            board_inputs_ = stack(board_inputs_).squeeze(1).cuda()\n            w_probs, w_action, ct_probs, ct_action = net(board_inputs_)\n            \n            w_probs = w_probs[range(len(w_probs)), w_action]\n            log_w_probs = log(w_probs)\n\n            ct_probs = ct_probs[range(len(ct_probs)), ct_action]\n            log_ct_probs = log(ct_probs)\n\n            pseudo_rews_ = [pseudo_rews_ for _, pseudo_rews_, _ in data]\n            rews.append(mean(pseudo_rews_))\n            qvals = calc_qvals(pseudo_rews_)\n            qvals = tensor(qvals).cuda()\n\n            w_loss = -(log_w_probs * qvals)\n            ct_loss = -(log_ct_probs * qvals)\n            loss = stack((w_loss, ct_loss))\n            loss = stack((w_loss, ct_loss)).mean(dim=0)\n            loss = loss.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(net.parameters(), 1)\n            optimizer.step()\n\n            print(f'Last Rewards: {mean(rews[-100:]):.2f}')\n\n            # Reset buff\n            buff = buffer_data()\n            next(buff)\n            board_inputs_ = None\n            pseudo_rews_ = None\n            cts_ = None\n\n            if (mean(rews[-100:]) > 750) or (games >= 500):\n                print()\n                print('  ===>  Saving model')\n                save(net.state_dict(), 'net_state_dict.pth')\n                done = True\n                TRAIN = False\n\n        steps += 1\n    games += 1","metadata":{"execution":{"iopub.status.busy":"2021-11-21T20:24:10.224713Z","iopub.execute_input":"2021-11-21T20:24:10.225022Z","iopub.status.idle":"2021-11-21T20:24:10.242018Z","shell.execute_reply.started":"2021-11-21T20:24:10.224988Z","shell.execute_reply":"2021-11-21T20:24:10.24095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"lux_ai_2021\",\n                configuration={\n                    \"loglevel\": 1,\n                    \"annotations\": True\n                },\n                debug=True)\na_run = env.run(['my_agent.py', 'my_agent.py'])\nenv.render(mode=\"ipython\", width=1100, height=500)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T20:24:32.813254Z","iopub.execute_input":"2021-11-21T20:24:32.813521Z","iopub.status.idle":"2021-11-21T20:24:49.580131Z","shell.execute_reply.started":"2021-11-21T20:24:32.813491Z","shell.execute_reply":"2021-11-21T20:24:49.579472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -czf submission.tar.gz *","metadata":{"execution":{"iopub.status.busy":"2021-11-21T20:25:27.291112Z","iopub.execute_input":"2021-11-21T20:25:27.291706Z","iopub.status.idle":"2021-11-21T20:25:27.993691Z","shell.execute_reply.started":"2021-11-21T20:25:27.291665Z","shell.execute_reply":"2021-11-21T20:25:27.992715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How would you enhance it? Feel free to.","metadata":{}}]}