{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/glmcdona/LuxPythonEnvGym.git","metadata":{"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2021-11-03T23:31:20.04258Z","iopub.execute_input":"2021-11-03T23:31:20.042962Z","iopub.status.idle":"2021-11-03T23:31:37.663602Z","shell.execute_reply.started":"2021-11-03T23:31:20.04287Z","shell.execute_reply":"2021-11-03T23:31:37.662786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-03T23:31:37.665531Z","iopub.execute_input":"2021-11-03T23:31:37.665795Z","iopub.status.idle":"2021-11-03T23:31:37.681679Z","shell.execute_reply.started":"2021-11-03T23:31:37.665764Z","shell.execute_reply":"2021-11-03T23:31:37.680781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport gym\nfrom gym import spaces\nfrom gym.wrappers import FlattenObservation\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom luxai2021.game.actions import MoveAction, SpawnCityAction, TransferAction, PillageAction, Action, SpawnWorkerAction, SpawnCartAction, ResearchAction\nfrom luxai2021.game.constants import LuxMatchConfigs_Default\nfrom luxai2021.game.game import Game\nfrom luxai2021.game.constants import Constants\nfrom luxai2021.game.unit import Unit\nfrom luxai2021.game.cell import Cell\nfrom luxai2021.game.city import City, CityTile\nfrom typing import Any, Callable, Optional, Sequence, Text, Union, List, Tuple\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.env_checker import check_env\nfrom stable_baselines3.common.noise import NormalActionNoise\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.results_plotter import load_results, ts2xy\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common import results_plotter\nimport tensorboard as tb","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-03T23:46:55.382141Z","iopub.execute_input":"2021-11-03T23:46:55.382724Z","iopub.status.idle":"2021-11-03T23:46:55.392996Z","shell.execute_reply.started":"2021-11-03T23:46:55.382678Z","shell.execute_reply":"2021-11-03T23:46:55.392046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reward Function","metadata":{}},{"cell_type":"code","source":"\ndef build_worker_reward(game : Game, unit_id : str, team : int) -> float:\n    \"\"\"calculate the reward for a worker\n\n    Parameters\n    ----------\n    game : Game\n        The game object\n    unit_id : str\n        the id of the worker such as 'u_1'\n    team : int\n        the integer id of the team\n\n    Returns\n    -------\n    float\n        the float value of the reward\n    \"\"\"    \n    unit = my_get_unit(game, unit_id)\n    ctt = my_get_citytiles(game, team)\n    reward_fg = game.stats['teamStats'][team]['fuelGenerated']\n    reward_cfv = 0.9 * (unit.get_cargo_fuel_value() if unit is not None else 0.0)\n    reward = -1.0 + reward_fg + reward_cfv\n    return reward\n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T23:46:56.046071Z","iopub.execute_input":"2021-11-03T23:46:56.047037Z","iopub.status.idle":"2021-11-03T23:46:56.053162Z","shell.execute_reply.started":"2021-11-03T23:46:56.046982Z","shell.execute_reply":"2021-11-03T23:46:56.052135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility Functions","metadata":{}},{"cell_type":"code","source":"def my_is_cell_free(cell : Cell) -> bool:\n    return not (cell.has_resource() or cell.is_city_tile() or cell.has_units())\n\ndef my_get_citytiles(game : Game, team : int)  -> List[CityTile]  :    \n    \"\"\"gets all city tiles belonging to a team\n\n    Parameters\n    ----------\n    game : Game\n        The game object\n    team : int\n        the integer id of the team\n\n    Returns\n    -------\n    list\n        a list of CityTile\n    \"\"\"\n    city_tiles = []\n    for city in game.cities.values():\n        if city.team == team:\n            for citycell in city.city_cells:\n                city_tiles.append(citycell.city_tile)\n    return city_tiles\n \n    \ndef discretize_direction(direction_char):\n    if Constants.DIRECTIONS.CENTER == direction_char:\n        return [1, 0, 0, 0, 0]\n    elif Constants.DIRECTIONS.NORTH == direction_char:\n        return [0, 1, 0, 0, 0]\n    elif Constants.DIRECTIONS.WEST == direction_char:\n        return [0, 0, 1, 0, 0]\n    elif Constants.DIRECTIONS.SOUTH == direction_char:\n        return [0, 0, 0, 1, 0]\n    elif Constants.DIRECTIONS.EAST == direction_char:\n        return [0, 0, 0, 0, 1]\n    else:\n        raise ValueError(\"discretize_direction\")\n    \n\ndef get_closest_city_position_from_unit(city : City, unit : Unit):\n    \"\"\"gets the position of the closest city cell of a city from a unit\n\n    Parameters\n    ----------\n    city : City\n        The city object\n    unit : Unit\n        the unit object\n\n    Returns\n    -------\n    tuple\n        a tuple containing the city, the closest city cell, and the distance to the closest city cell\n    \"\"\"\n    distances = [ (ctc, unit.pos.distance_to(ctc.pos)) for ctc in city.city_cells ]\n    if len(distances) == 0:\n        return None\n    else:\n        closest = min(distances, key = lambda t: t[1])\n        return (city, closest[0], closest[1])\n        \n\ndef my_get_unit(game : Game, unit_id : str) -> Unit:\n    \"\"\"gets a unit given this unit id\n\n    Parameters\n    ----------\n    game : Game\n        The game object\n    unit_id : str\n        the id of a unit, such as 'u_1'\n\n    Returns\n    -------\n    Unit\n        a unit object\n    \"\"\"\n    if unit_id in game.state['teamStates'][0]['units'].keys():\n        return game.state['teamStates'][0]['units'][unit_id]\n    elif unit_id in game.state['teamStates'][1]['units'].keys():\n        return game.state['teamStates'][1]['units'][unit_id]\n    else:\n        return None\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T23:46:57.709545Z","iopub.execute_input":"2021-11-03T23:46:57.709941Z","iopub.status.idle":"2021-11-03T23:46:57.725204Z","shell.execute_reply.started":"2021-11-03T23:46:57.709911Z","shell.execute_reply":"2021-11-03T23:46:57.724311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LuxAI Environment","metadata":{}},{"cell_type":"code","source":"class SimpleWorkerDictEnv(gym.Env):\n    \n    def __init__(self, configs : dict):\n        \"\"\"\n        Parameters\n        ----------\n        configs : dict\n            The game configurations\n        \"\"\"\n        self._configs = configs\n        self._game = None\n        game_info_state = {\n            'game_percent' : gym.spaces.Box(low=0.0, high=1.0, shape=(1, ), dtype=np.float32),\n            'daynight' : gym.spaces.Box(low=0.0, high=1.0, shape=(2, ), dtype=np.float32),\n            'coal_researched_percent' : gym.spaces.Box(low=0.0, high=1.0, shape=(1, ), dtype=np.float32),\n            'uranium_researched_percent' : gym.spaces.Box(low=0.0, high=1.0, shape=(1, ), dtype=np.float32),\n            'coal_researched' : gym.spaces.Box(low=0.0, high=1.0, shape=(1, ), dtype=np.float32),\n            'uranium_researched' : gym.spaces.Box(low=0.0, high=1.0, shape=(1, ), dtype=np.float32),\n            'can_act' : gym.spaces.Box(low=0.0, high=1.0, shape=(1, ), dtype=np.float32),\n            'can_build' : gym.spaces.Box(low=0.0, high=1.0, shape=(1, ), dtype=np.float32)\n            }\n        observation_space = {\n            'game_info_state' : gym.spaces.Dict(game_info_state),\n            'closest_wood1' : gym.spaces.Box(low=-1.0, high=1.0, shape=(7, ), dtype=np.float32),\n            'closest_citytile1' : gym.spaces.Box(low=-1.0, high=1.0, shape=(9, ), dtype=np.float32),\n            'closest_free_cell1' : gym.spaces.Box(low=-1.0, high=1.0, shape=(6, ), dtype=np.float32)\n                }\n        self.observation_space = gym.spaces.Dict(observation_space)\n        self.action_space = spaces.Discrete(6)\n        self._episode_ended = True\n        self._worker_being_trained = None\n        \n        \n    def scalar_to_action_worker(self, worker : Unit, action : int ) -> Action :\n        \"\"\"Converts an action represented to an action represented as an Action object\n\n        Parameters\n        ----------\n        worker : unit\n            the id of the worker\n        action : int32\n            the action\n            \n        Returns\n        -------\n        Action\n            an Action object\n\n        Raises\n        ------\n        ValueError\n            If the int action is not supported\n        \"\"\"\n        # add type checking\n        #unit_id = self._worker_for_training.id\n        unit_id = worker.id\n        team = worker.team\n        unit = my_get_unit(self._game, worker.id)\n        if unit is None:\n            return None\n        # dont move\n        if action == 0:\n            #ma = MoveAction(team, unit_id, Constants.DIRECTIONS.CENTER)\n            #return ma\n            return None\n        # move north\n        elif action == 1:\n            ma = MoveAction(team, unit_id, Constants.DIRECTIONS.NORTH)\n            return ma\n        # move east\n        elif action == 2:\n            ma = MoveAction(team, unit_id, Constants.DIRECTIONS.EAST)\n            return ma\n        # move south\n        elif action == 3:\n            ma = MoveAction(team, unit_id, Constants.DIRECTIONS.SOUTH)\n            return ma\n        # move west\n        elif action == 4:\n            ma = MoveAction(team, unit_id, Constants.DIRECTIONS.WEST)\n            return ma\n        # build city\n        elif action == 5:\n            sca = SpawnCityAction(team, unit_id)\n            return sca\n        else:\n            raise ValueError(f\"invalid action {action}\")\n         \n    def reset(self):\n        \"\"\"Resets the environment. called when either the game ended, or the worker dies.\n            \n        Raises\n        ------\n        ValueError\n            If no worker is available for training\n        \"\"\"\n        if self._game is None or self._game.match_over() or self._episode_ended:\n            self._game = Game(self._configs)\n        self._episode_ended = False\n        self._team = random.sample([0, 1], 1)[0]\n        self._other = 0 if self._team == 1 else 1\n        \n        all_unit_ids_team = list(self._game.state[\"teamStates\"][self._team][\"units\"].keys())\n        \n        all_workers_team = []\n        \n        \n        for unit_id_team in all_unit_ids_team:\n            unit = my_get_unit(self._game, unit_id_team)\n            if unit.is_worker():\n                all_workers_team.append(unit)\n            elif unit.is_cart():\n                all_carts_team.append(unit)\n            else:\n                raise ValueError(\"_reset all_unit_ids_team\")\n                \n        if len(all_workers_team) > 0:            \n            self._worker_being_trained = np.random.choice(all_workers_team)\n            wob = build_worker_observation(self._game, self._configs, self._worker_being_trained)\n            if not self.observation_space.contains(wob):\n                raise ValueError(f\"SimpleWorkerDictEnv.build_worker_observation {wob}\")\n            return wob\n        else:\n            raise ValueError(\"reset no worker available\")\n        \n        \n    def step(self, action : int) -> Tuple[dict, np.float32, bool, dict]:\n        \"\"\"performs one step in the environment, meaning performing the action and returning an observation of the resulting state\n\n        Parameters\n        ----------\n        action : int32\n            the action\n            \n        Returns\n        -------\n        Tuple\n            a Tuple object containing a new observation, the reward, a boolean \"is the game done\", and a dict containing debug info\n\n        Raises\n        ------\n        ValueError\n            If the int action is not supported\n        \"\"\"\n        if not self.action_space.contains(action):\n            raise ValueError(f\"SimpleWorkerDictEnv._step {action}\")\n        \n        action = self.scalar_to_action_worker(self._worker_being_trained, \n                                              action)                 \n        actions = [ action ]\n        self._episode_ended = self._game.run_turn_with_actions(actions)\n        observation = build_worker_observation(self._game, self._configs, self._worker_being_trained)\n        if not self.observation_space.contains(observation):\n            raise ValueError(f\"SimpleWorkerDictEnv.build_worker_observation {observation}\")\n        self._last_reward = build_worker_reward(self._game, self._worker_being_trained.id, self._worker_being_trained.team)\n        test = my_get_unit(self._game, self._worker_being_trained.id)\n        if test is None:\n            self._episode_ended = True\n        return (observation, self._last_reward, self._episode_ended, {})\n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T23:47:10.376927Z","iopub.execute_input":"2021-11-03T23:47:10.377255Z","iopub.status.idle":"2021-11-03T23:47:10.409624Z","shell.execute_reply.started":"2021-11-03T23:47:10.377217Z","shell.execute_reply":"2021-11-03T23:47:10.408657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Observation","metadata":{}},{"cell_type":"code","source":"def build_worker_observation(game : Game, configs : dict, worker : Unit):\n    \"\"\"builds an observation matching the observation space for a given worker.\n\n    Parameters\n    ----------\n    game : Game\n        The game object\n    configs : dict\n        the configs of the game\n    worker : Unit\n        the unit object\n\n    Returns\n    -------\n    dict\n        a dictionary with a np.array for the closest wood cell and the closest city cell\n    \"\"\"\n    game_info_state = build_worker_observation_game_info_state(game, configs, worker)\n    \n    closest_woods = build_worker_observation_closest_resources(game, configs, worker, Constants.RESOURCE_TYPES.WOOD, 5)\n    closest_wood1 = closest_woods[0]\n    \n    closest_citytiles = build_worker_observation_citytiles(game, worker, worker.team, 3)\n    closest_citytile1 = closest_citytiles[0]\n    \n    closest_free_cells = build_worker_observation_free_cells(game, worker, 3)\n    closest_free_cell1 = closest_free_cells[0]\n    \n    \n    wob = {\n        'game_info_state' : game_info_state,\n        'closest_wood1' : closest_wood1,\n        'closest_citytile1' : closest_citytile1,\n        'closest_free_cell1' : closest_free_cell1\n        }\n    return wob            \n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T23:47:10.849151Z","iopub.execute_input":"2021-11-03T23:47:10.849439Z","iopub.status.idle":"2021-11-03T23:47:10.857438Z","shell.execute_reply.started":"2021-11-03T23:47:10.849406Z","shell.execute_reply":"2021-11-03T23:47:10.856509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build State observation","metadata":{}},{"cell_type":"code","source":"def build_worker_observation_game_info_state(game : Game, configs : dict, worker : Unit):\n    gif = {\n        'game_percent' : [ game.state['turn'] / configs['parameters']['MAX_DAYS'] ],\n        'daynight' : [ 0 if game.is_night() else 1, 1 if game.is_night() else 0 ],\n        'coal_researched_percent' : [ game.state['teamStates'][worker.team]['researchPoints'] / configs['parameters']['RESEARCH_REQUIREMENTS']['COAL'] ],\n        'uranium_researched_percent' : [ game.state['teamStates'][worker.team]['researchPoints'] / configs['parameters']['RESEARCH_REQUIREMENTS']['URANIUM'] ],\n        'coal_researched' : [ 1 if game.state['teamStates'][worker.team]['researched']['coal'] else 0 ] ,\n        'uranium_researched' : [ 1 if game.state['teamStates'][worker.team]['researched']['uranium'] else 0 ],\n        'can_act' : [ 1 if worker.can_act() else 0],\n        'can_build' : [ 1 if worker.can_build(game.map) else 0]\n        }\n    return gif\n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T23:47:11.327312Z","iopub.execute_input":"2021-11-03T23:47:11.328259Z","iopub.status.idle":"2021-11-03T23:47:11.337178Z","shell.execute_reply.started":"2021-11-03T23:47:11.328212Z","shell.execute_reply":"2021-11-03T23:47:11.336265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Free Cells Observations","metadata":{}},{"cell_type":"code","source":"def build_worker_observation_free_cell(unit, cellinfo, free_cell_distance_mean, free_cell_distance_max):\n    if cellinfo is None:\n        return np.array([-1] * 6, dtype=np.float32)\n    direction = unit.pos.direction_to(cellinfo[0].pos)\n    distance = unit.pos.distance_to(cellinfo[0].pos)\n    direction_discrete = discretize_direction(direction)\n    norm_distance = (distance - free_cell_distance_mean) / free_cell_distance_max\n    \n    wofc = np.array(direction_discrete + [norm_distance ], dtype=np.float32)\n    return wofc\n\n\ndef build_worker_observation_free_cells(game : Game, unit : Unit, nbcells : int):\n    free_cell_distance_max = np.sqrt(game.map.width**2 + game.map.height**2)\n    free_cell_distance_mean = free_cell_distance_max / 2\n    list_cells = []\n    for x in range(game.map.width):\n        for y in range(game.map.height):\n            cell = game.map.get_cell(x, y)\n            if my_is_cell_free(cell):\n                distance = cell.pos.distance_to(unit.pos)\n                list_cells.append( (cell, distance))\n    list_cells = np.array(list_cells)\n    sort_indices = np.argsort(list_cells[:,1])\n    sorted_cells = list_cells[sort_indices]\n    obslist = []\n    for i in range(nbcells):\n        cellinfo = sorted_cells[i] if i < len(sorted_cells) else None\n        obs = build_worker_observation_free_cell(unit, cellinfo,\n                                                free_cell_distance_mean, free_cell_distance_max)\n        obslist.append(obs)\n    return obslist\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-03T23:50:50.428132Z","iopub.execute_input":"2021-11-03T23:50:50.428433Z","iopub.status.idle":"2021-11-03T23:50:50.440301Z","shell.execute_reply.started":"2021-11-03T23:50:50.4284Z","shell.execute_reply":"2021-11-03T23:50:50.439663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Resources Observations","metadata":{}},{"cell_type":"code","source":"def build_worker_observation_closest_resource_cell(unit, resource_cell, \n                                                   distance_mean, distance_max,\n                                                   res_amount_mean, res_amount_max):\n    if resource_cell is None:\n        return np.array([-1] * 7, dtype=np.float32)\n    direction = unit.pos.direction_to(resource_cell.pos)\n    distance = unit.pos.distance_to(resource_cell.pos)\n    res_amount = resource_cell.resource.amount\n    \n    direction_discrete = discretize_direction(direction)\n    norm_distance = (distance - distance_mean) / distance_max\n    norm_amount = (res_amount - res_amount_mean) / res_amount_max\n    \n    wocrc = np.array(direction_discrete + [norm_distance, norm_amount ], dtype=np.float32)\n    return wocrc\n  \n\n    \ndef build_worker_observation_closest_resources(game, configs, unit, resource_type, nbresources):\n    if resource_type == Constants.RESOURCE_TYPES.WOOD:\n        amount_mean = configs[\"parameters\"][\"MAX_WOOD_AMOUNT\"] / 2 * 1.1\n        amount_max = configs[\"parameters\"][\"MAX_WOOD_AMOUNT\"] * 1.1\n    elif resource_type == Constants.RESOURCE_TYPES.COAL:\n        amount_mean = configs[\"parameters\"][\"MAX_COAL_AMOUNT\"] / 2 * 1.1\n        amount_max = configs[\"parameters\"][\"MAX_COAL_AMOUNT\"] * 1.1\n    elif resource_type == Constants.RESOURCE_TYPES.URANIUM:\n        amount_mean = configs[\"parameters\"][\"MAX_URANIUM_AMOUNT\"] / 2 * 1.1\n        amount_max = configs[\"parameters\"][\"MAX_URANIUM_AMOUNT\"] * 1.1\n    else:\n        raise ValueError(\"build_worker_observation_resource\")\n    \n    distance_max = np.sqrt(game.map.width**2 + game.map.height**2)\n    distance_mean = distance_max / 2\n        \n    resource_cells = game.map.resources_by_type[resource_type]\n    resource_distances = np.array([ (cell, unit.pos.distance_to(cell.pos) ) for cell in resource_cells ])\n    sort_indices = np.argsort(resource_distances[:,1])\n    sorted_resources = resource_distances[sort_indices]\n    obslist = []\n    for i in range(nbresources):\n        resource_cell = sorted_resources[i][0] if i < len(sorted_resources) else None\n        obs = build_worker_observation_closest_resource_cell(unit, resource_cell,\n                                                         distance_mean, distance_max, \n                                                         amount_mean, amount_max)\n        obslist.append(obs)\n    return obslist\n    \n   \n    \n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T23:50:51.002109Z","iopub.execute_input":"2021-11-03T23:50:51.002599Z","iopub.status.idle":"2021-11-03T23:50:51.017717Z","shell.execute_reply.started":"2021-11-03T23:50:51.00256Z","shell.execute_reply":"2021-11-03T23:50:51.016583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build CityTile Observations","metadata":{}},{"cell_type":"code","source":"def build_worker_observation_citytile(unit, cityinfo, \n                                        city_fuel_mean, city_fuel_max, \n                                        city_upkeep_mean, city_upkeep_max,\n                                        city_size_mean, city_size_max,\n                                        city_distance_mean, city_distance_max):\n    if cityinfo is None:\n        return np.array([-1] * 9, dtype=np.float32)\n    direction = unit.pos.direction_to(cityinfo[1].pos)\n    distance = unit.pos.distance_to(cityinfo[1].pos)\n    fuel = cityinfo[0].fuel\n    upkeep = cityinfo[0].get_light_upkeep()\n    city_size = len(cityinfo[0].city_cells)\n    \n    direction_discrete = discretize_direction(direction)\n    norm_distance = (distance - city_distance_mean) / city_distance_max\n    norm_upkeep = (upkeep - city_upkeep_mean) / city_upkeep_max\n    norm_fuel = (fuel - city_size_mean) / city_fuel_max\n    norm_city_size = (city_size - city_size_mean) / city_size_max\n    \n    woc = np.array(direction_discrete + [norm_distance, norm_fuel, norm_upkeep, norm_city_size ], dtype=np.float32)\n    return woc\n\n\ndef build_worker_observation_citytiles(game : Game, unit : Unit, team_id : int, nbcities : int):\n    city_fuel_mean = 0\n    city_fuel_max = 10000\n    city_upkeep_mean = 0\n    city_upkeep_max = 200\n    city_size_mean = 1\n    city_size_max = 10\n    #city_distance_mean = 0\n    #city_distance_max = 10\n    city_distance_max = np.sqrt(game.map.width**2 + game.map.height**2)\n    city_distance_mean = city_distance_max / 2\n    \n    cities = np.array([  get_closest_city_position_from_unit(city, unit) for city in game.cities.values() if city.team == team_id ])\n    if len(cities) == 0:\n        return  [ np.array([-1.0] * 9, np.float32) for i in range(nbcities) ]\n    sort_indices = np.argsort(cities[:,2])\n    sorted_cities = cities[sort_indices]\n    obslist = []\n    for i in range(nbcities):\n        cityinfo = sorted_cities[i] if i < len(sorted_cities) else None\n        obs = build_worker_observation_citytile(unit, cityinfo,\n                                                city_fuel_mean, city_fuel_max,\n                                                city_upkeep_mean, city_upkeep_max,\n                                                city_size_mean, city_size_max,\n                                                city_distance_mean, city_distance_max)\n        obslist.append(obs)\n    return obslist\n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T23:50:51.369238Z","iopub.execute_input":"2021-11-03T23:50:51.369496Z","iopub.status.idle":"2021-11-03T23:50:51.383797Z","shell.execute_reply.started":"2021-11-03T23:50:51.369468Z","shell.execute_reply":"2021-11-03T23:50:51.382759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the model","metadata":{}},{"cell_type":"code","source":"swde = SimpleWorkerDictEnv(LuxMatchConfigs_Default)\nmswde = Monitor(swde, './logdir')\nenv = make_vec_env(FlattenObservation, \n                       n_envs=4, \n                       env_kwargs={'env' : Monitor(SimpleWorkerDictEnv(LuxMatchConfigs_Default), './logdir')})\nmodel = PPO(\"MlpPolicy\", env, verbose=1, device=\"cpu\", tensorboard_log=\"./tensorboard/\")\nmodel.learn(total_timesteps=int(1e6), n_eval_episodes = 360)\nmodel.save(\"best_ppo_worker_py37_20211103_001\")\nprint(\"done.\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-03T23:50:52.065094Z","iopub.execute_input":"2021-11-03T23:50:52.065596Z","iopub.status.idle":"2021-11-03T23:51:35.793982Z","shell.execute_reply.started":"2021-11-03T23:50:52.065555Z","shell.execute_reply":"2021-11-03T23:51:35.792367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot the training steps","metadata":{}},{"cell_type":"code","source":"def moving_average(values, window):\n    \"\"\"\n    Smooth values by doing a moving average\n    :param values: (numpy array)\n    :param window: (int)\n    :return: (numpy array)\n    \"\"\"\n    weights = np.repeat(1.0, window) / window\n    return np.convolve(values, weights, 'valid')\n\n\ndef plot_results(log_folder, title='Learning Curve'):\n    \"\"\"\n    plot the results\n\n    :param log_folder: (str) the save location of the results to plot\n    :param title: (str) the title of the task to plot\n    \"\"\"\n    x, y = ts2xy(load_results(log_folder), 'timesteps')\n    y = moving_average(y, window=50)\n    # Truncate x\n    x = x[len(x) - len(y):]\n\n    fig = plt.figure(title)\n    plt.plot(x, y)\n    plt.xlabel('Number of Timesteps')\n    plt.ylabel('Rewards')\n    plt.title(title + \" Smoothed\")\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-03T23:51:35.796201Z","iopub.execute_input":"2021-11-03T23:51:35.796799Z","iopub.status.idle":"2021-11-03T23:51:35.805939Z","shell.execute_reply.started":"2021-11-03T23:51:35.79675Z","shell.execute_reply":"2021-11-03T23:51:35.804879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results(\".\")","metadata":{"execution":{"iopub.status.busy":"2021-11-03T23:51:35.807372Z","iopub.execute_input":"2021-11-03T23:51:35.807711Z","iopub.status.idle":"2021-11-03T23:51:36.078488Z","shell.execute_reply.started":"2021-11-03T23:51:35.807667Z","shell.execute_reply":"2021-11-03T23:51:36.077861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate the model","metadata":{}},{"cell_type":"code","source":"eval_swde = SimpleWorkerDictEnv(LuxMatchConfigs_Default)\neval_swfe = FlattenObservation(eval_swde)\neval_menv = Monitor(eval_swfe, './logdir')\n\nmodel2 = PPO.load(\"best_ppo_worker_py37_20211103_001\")\nmean_reward, std_reward = evaluate_policy(model2, eval_menv, n_eval_episodes=20)\nprint(f'Mean reward: {mean_reward:,.0f} +/- {std_reward:,.0f}')","metadata":{"execution":{"iopub.status.busy":"2021-11-03T23:51:42.282433Z","iopub.execute_input":"2021-11-03T23:51:42.282865Z","iopub.status.idle":"2021-11-03T23:51:58.238027Z","shell.execute_reply.started":"2021-11-03T23:51:42.282832Z","shell.execute_reply":"2021-11-03T23:51:58.237116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"scrolled":true},"execution_count":null,"outputs":[]}]}