{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport random\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom torchvision import transforms\nimport zipfile","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-02T06:48:41.24034Z","iopub.execute_input":"2021-12-02T06:48:41.240791Z","iopub.status.idle":"2021-12-02T06:48:42.724288Z","shell.execute_reply.started":"2021-12-02T06:48:41.240681Z","shell.execute_reply":"2021-12-02T06:48:42.723393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/datasets/'\nif not os.path.exists(data_dir):\n    os.mkdir(data_dir)\nwith zipfile.ZipFile('../input/lux-ai-datasets-city/datasets.zip') as z:\n    z.extractall(data_dir)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T06:48:42.725636Z","iopub.execute_input":"2021-12-02T06:48:42.725981Z","iopub.status.idle":"2021-12-02T06:49:19.918199Z","shell.execute_reply.started":"2021-12-02T06:48:42.725945Z","shell.execute_reply":"2021-12-02T06:49:19.917156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\n\nseed = 42\nseed_everything(seed)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T06:49:19.92008Z","iopub.execute_input":"2021-12-02T06:49:19.924854Z","iopub.status.idle":"2021-12-02T06:49:20.077204Z","shell.execute_reply.started":"2021-12-02T06:49:19.924822Z","shell.execute_reply":"2021-12-02T06:49:20.066564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LuxAugment:\n    def __init__(self):\n        pass\n    \n    def __call__(self, sample):\n        state, action = sample\n        # Flip vertically\n        if random.random() < 0.5:\n            state = np.flip(state, axis=1).copy()\n            action = np.flip(action, axis=0).copy()\n\n        # Flip horizontally\n        if random.random() < 0.5:\n            state = np.flip(state, axis=2).copy()\n            action = np.flip(action, axis=1).copy()\n\n        # Rotate 90 degrees\n        if random.random() < 0.5:\n            state = np.rot90(state, axes=(1, 2)).copy()\n            action = np.rot90(action, axes=(0, 1)).copy()\n        \n        return state, action\n\n\nclass LuxDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.samples = os.listdir(data_dir)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        data_path = os.path.join(self.data_dir, self.samples[idx])\n        data = np.load(data_path)\n        sample = data['state'], data['action']\n        if self.transform is not None:\n            sample = self.transform(sample)\n        \n        return sample","metadata":{"execution":{"iopub.status.busy":"2021-12-02T06:49:20.078898Z","iopub.execute_input":"2021-12-02T06:49:20.079246Z","iopub.status.idle":"2021-12-02T06:49:20.203811Z","shell.execute_reply.started":"2021-12-02T06:49:20.079209Z","shell.execute_reply":"2021-12-02T06:49:20.202304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        factor = 2 if bilinear else 1\n        self.down3 = Down(256, 512 // factor)\n        self.up1 = Up(512, 256 // factor, bilinear)\n        self.up2 = Up(256, 128 // factor, bilinear)\n        self.up3 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x = self.up1(x4, x3)\n        x = self.up2(x, x2)\n        x = self.up3(x, x1)\n        logits = self.outc(x)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2021-12-02T06:49:20.207094Z","iopub.execute_input":"2021-12-02T06:49:20.207555Z","iopub.status.idle":"2021-12-02T06:49:20.251199Z","shell.execute_reply.started":"2021-12-02T06:49:20.207504Z","shell.execute_reply":"2021-12-02T06:49:20.24998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, n_classes):\n        super(DiceLoss, self).__init__()\n        self.n_classes = n_classes\n\n    def _one_hot_encoder(self, input_tensor):\n        tensor_list = []\n        for i in range(self.n_classes):\n            temp_prob = input_tensor == i\n            tensor_list.append(temp_prob.unsqueeze(1))\n        output_tensor = torch.cat(tensor_list, dim=1)\n        return output_tensor.float()\n\n    def _dice_loss(self, score, target):\n        target = target.float()\n        smooth = 1e-5\n        intersect = torch.sum(score * target)\n        y_sum = torch.sum(target * target)\n        z_sum = torch.sum(score * score)\n        loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n        loss = 1 - loss\n        return loss\n\n    def forward(self, inputs, target):\n        inputs = torch.softmax(inputs, dim=1)\n        target = self._one_hot_encoder(target)\n        assert inputs.size() == target.size(), 'predict {} & target {} shape do not match'.format(inputs.size(), target.size())\n        class_wise_dice = []\n        loss = 0.0\n        for i in range(0, self.n_classes):\n            dice = self._dice_loss(inputs[:, i], target[:, i])\n            class_wise_dice.append(1.0 - dice.item())\n            loss += dice\n        return loss / self.n_classes","metadata":{"execution":{"iopub.status.busy":"2021-12-02T06:49:20.258201Z","iopub.execute_input":"2021-12-02T06:49:20.261311Z","iopub.status.idle":"2021-12-02T06:49:20.291808Z","shell.execute_reply.started":"2021-12-02T06:49:20.261271Z","shell.execute_reply":"2021-12-02T06:49:20.289698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloader, criterion, optimizer, scheduler, num_epochs):\n    model.train()\n    model.cuda()\n    \n    for epoch in range(num_epochs):                \n        epoch_loss = 0.0\n        \n        for item in tqdm(dataloader, leave=False):\n            states = item[0].cuda().float()\n            actions = item[1].cuda().long()\n\n            optimizer.zero_grad()\n\n            policy = model(states)\n            loss = criterion(policy, actions)\n\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item() * len(policy)\n\n        epoch_loss = epoch_loss / len(dataloader.dataset)\n        print(f'Epoch {epoch + 1}/{num_epochs} | Loss: {epoch_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}')\n        scheduler.step()\n    \n    model.eval()\n    model.cpu()\n    traced = torch.jit.trace(model, torch.rand(1, model.n_channels, 32, 32))\n    traced.save('model.pth')","metadata":{"execution":{"iopub.status.busy":"2021-12-02T06:49:20.308036Z","iopub.execute_input":"2021-12-02T06:49:20.314301Z","iopub.status.idle":"2021-12-02T06:49:20.34908Z","shell.execute_reply.started":"2021-12-02T06:49:20.314261Z","shell.execute_reply":"2021-12-02T06:49:20.34703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = UNet(n_channels=16, n_classes=3)\ndataloader = DataLoader(\n    LuxDataset(data_dir, transforms.Compose([LuxAugment()])),\n    batch_size=128, \n    shuffle=True, \n    num_workers=2\n)\ncriterion = DiceLoss(model.n_classes)\noptimizer = optim.AdamW(model.parameters(), lr=1e-3)\nscheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, eta_min=1e-6)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T06:49:20.370155Z","iopub.execute_input":"2021-12-02T06:49:20.375161Z","iopub.status.idle":"2021-12-02T06:49:20.727099Z","shell.execute_reply.started":"2021-12-02T06:49:20.375104Z","shell.execute_reply":"2021-12-02T06:49:20.72595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(model, dataloader, criterion, optimizer, scheduler, num_epochs=50)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T06:49:20.731543Z","iopub.execute_input":"2021-12-02T06:49:20.731902Z","iopub.status.idle":"2021-12-02T06:51:17.028457Z","shell.execute_reply.started":"2021-12-02T06:49:20.731866Z","shell.execute_reply":"2021-12-02T06:51:17.025887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}