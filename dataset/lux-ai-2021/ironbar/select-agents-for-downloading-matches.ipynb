{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Simulations Episode Scraper Match Downloader","metadata":{}},{"cell_type":"markdown","source":"This notebook downloads episodes using Kaggle's GetEpisodeReplay API and the [Meta Kaggle](https://www.kaggle.com/kaggle/meta-kaggle) dataset.\n\n**To run this notebook you WILL need to re-add the Meta Kaggle dataset. After opening your copy of the notebook, click \"+ Add data\" top right in the notebook editor.\n**\n\nMeta Kaggle is refreshed daily, but sometimes misses daily refreshes for a few days.\n\nWhy download replays?\n- Train your ML/RL model\n- Inspect the performance of yours and others agents\n- To add to your ever growing json collection \n\nOnly one scraping strategy is implemented: For each top scoring submission, download all missing matches, move on to next submission.\n\nOther scraping strategies can be implemented, but not here. Like download max X matches per submission or per team per day, or ignore certain teams or ignore where some scores < X, or only download some teams.\n\nTodo:\n- Add teamid's once meta kaggle add them. Edit: it's been a long time, it doesn't look like Kaggle is adding this.","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport requests\nimport json\nimport datetime\nimport time\nimport glob\nimport collections\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-29T08:50:44.15463Z","iopub.execute_input":"2021-11-29T08:50:44.154946Z","iopub.status.idle":"2021-11-29T08:50:44.160333Z","shell.execute_reply.started":"2021-11-29T08:50:44.154918Z","shell.execute_reply":"2021-11-29T08:50:44.159268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"## You should configure these to your needs. Choose one of ...\n# 'hungry-geese', 'rock-paper-scissors', santa-2020', 'halite', 'google-football'\nCOMP = 'lux-ai-2021'","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:50:44.162516Z","iopub.execute_input":"2021-11-29T08:50:44.163281Z","iopub.status.idle":"2021-11-29T08:50:44.172627Z","shell.execute_reply.started":"2021-11-29T08:50:44.163238Z","shell.execute_reply":"2021-11-29T08:50:44.171726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT =\"../working/\"\nMETA = \"../input/meta-kaggle/\"\nMATCH_DIR = '../working/'\nbase_url = \"https://www.kaggle.com/requests/EpisodeService/\"\nget_url = base_url + \"GetEpisodeReplay\"\nBUFFER = 1\nCOMPETITIONS = {\n    'lux-ai-2021': 30067,\n    'hungry-geese': 25401,\n    'rock-paper-scissors': 22838,\n    'santa-2020': 24539,\n    'halite': 18011,\n    'google-football': 21723\n}","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-29T08:50:44.174045Z","iopub.execute_input":"2021-11-29T08:50:44.174764Z","iopub.status.idle":"2021-11-29T08:50:44.18366Z","shell.execute_reply.started":"2021-11-29T08:50:44.17473Z","shell.execute_reply":"2021-11-29T08:50:44.18275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Episodes\nepisodes_df = pd.read_csv(META + \"Episodes.csv\")\n\n# Load EpisodeAgents\nepagents_df = pd.read_csv(META + \"EpisodeAgents.csv\")\n\nprint(f'Episodes.csv: {len(episodes_df)} rows before filtering.')\nprint(f'EpisodeAgents.csv: {len(epagents_df)} rows before filtering.')\n\nepisodes_df = episodes_df[episodes_df.CompetitionId == COMPETITIONS[COMP]] \nepagents_df = epagents_df[epagents_df.EpisodeId.isin(episodes_df.Id)]\n\nprint(f'Episodes.csv: {len(episodes_df)} rows after filtering for {COMP}.')\nprint(f'EpisodeAgents.csv: {len(epagents_df)} rows after filtering for {COMP}.')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-29T08:50:44.184684Z","iopub.execute_input":"2021-11-29T08:50:44.185112Z","iopub.status.idle":"2021-11-29T08:51:06.409607Z","shell.execute_reply.started":"2021-11-29T08:50:44.185078Z","shell.execute_reply":"2021-11-29T08:51:06.407116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's add creation date to the agents.","metadata":{}},{"cell_type":"code","source":"episodes_df['CreateTime'] = pd.to_datetime(episodes_df['CreateTime'])\nepisode_id_to_create_time = episodes_df.set_index('Id')[['CreateTime']]\nepisode_id_to_create_time = {key: value for key, value in zip(episode_id_to_create_time.index, episode_id_to_create_time.CreateTime)}","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.410395Z","iopub.status.idle":"2021-11-29T08:51:06.410858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_id_to_episode_id = epagents_df.groupby('SubmissionId').head()[['SubmissionId', 'EpisodeId']]\nsubmission_id_to_create_time = {}\nfor submission_id, episode_id in zip(submission_id_to_episode_id.SubmissionId, submission_id_to_episode_id.EpisodeId):\n    submission_id_to_create_time[submission_id] = episode_id_to_create_time[episode_id]","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.41193Z","iopub.status.idle":"2021-11-29T08:51:06.412318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epagents_df['CreateTime'] = epagents_df.SubmissionId.apply(lambda x: submission_id_to_create_time[x])","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.413408Z","iopub.status.idle":"2021-11-29T08:51:06.413826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have tried getting the name of the teams but luxai competition is not there yet, maybe they update that only when the challenge has ended. Below there are some tries but none of them worked, I had to scrap kaggle website.","metadata":{}},{"cell_type":"code","source":"# teams = pd.read_csv(META + \"Teams.csv\")\n#teams = teams[teams.CompetitionId == COMPETITIONS[COMP]]","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.414798Z","iopub.status.idle":"2021-11-29T08:51:06.415165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have also tried getting it from the submissions dataframe but it does not involve episodes.","metadata":{}},{"cell_type":"code","source":"#submissions = pd.read_csv(META + \"Submissions.csv\")\n#submissions.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.416019Z","iopub.status.idle":"2021-11-29T08:51:06.416376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So it seems I cannot get that information from the meta dataset. Maybe I can use the kaggle api instead, or scrap the web.","metadata":{}},{"cell_type":"markdown","source":"## Data inspection","metadata":{}},{"cell_type":"markdown","source":"This shows that `episodes_df` has information about the competition, whereas `epagents_df` does not. That explains why we needed to use both to be able to filter by the competition of interest.\n\nAfter that filtering we don't probably need `episodes_df` anymore.","metadata":{}},{"cell_type":"code","source":"episodes_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.417069Z","iopub.status.idle":"2021-11-29T08:51:06.417433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epagents_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.418274Z","iopub.status.idle":"2021-11-29T08:51:06.418676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Leaderboard replication","metadata":{}},{"cell_type":"markdown","source":"Let's see how many unique agents are there, and try to create the last version of the leaderboard.","metadata":{}},{"cell_type":"code","source":"len(epagents_df.SubmissionId.unique())","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.419712Z","iopub.status.idle":"2021-11-29T08:51:06.420076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is very interesting, we only have 10k unique agents, while the number of matches is 1.4M. So that means that each agent plays around 100 matches. Let's verify that.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nplt.hist(epagents_df.SubmissionId.value_counts(), bins=50);","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.420928Z","iopub.status.idle":"2021-11-29T08:51:06.421304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a lot of agents with 0 or 1 matches, so if we exclude those we can see a uniform distribution, probably related to the date of agent submission.","metadata":{}},{"cell_type":"code","source":"leaderboard = epagents_df.groupby('SubmissionId').tail(1)\nleaderboard = leaderboard[~leaderboard.UpdatedScore.isna()]\nleaderboard = leaderboard.sort_values('UpdatedScore', ascending=False)\nprint(len(leaderboard))\nleaderboard","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.422235Z","iopub.status.idle":"2021-11-29T08:51:06.422649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we have 8k agents with a numeric score. Let's see the distribution of scores.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nplt.hist(leaderboard.UpdatedScore, bins=1000, log=True, cumulative=-1, histtype='stepfilled');\nplt.grid()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.423458Z","iopub.status.idle":"2021-11-29T08:51:06.423875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are around 250 agents above 1500 score.","metadata":{}},{"cell_type":"markdown","source":"## Getting the name of the team that made the submission","metadata":{}},{"cell_type":"markdown","source":"The only way I have found to do that is to use the notebook that scraps the kaggle website.\n\nhttps://www.kaggle.com/yalikesifulei/bot-statistics-with-selenium-beautiful-soup","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install selenium\n!apt-get update \n!apt install chromium-chromedriver -y\n!pip install BeautifulSoup4","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.42467Z","iopub.status.idle":"2021-11-29T08:51:06.425026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import lru_cache\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport time\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.425844Z","iopub.status.idle":"2021-11-29T08:51:06.426207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getSoup(sub_id, verbose=False):\n    options = webdriver.ChromeOptions()\n    options.add_argument('--headless')\n    options.add_argument('--no-sandbox')\n    options.add_argument('--disable-dev-shm-usage')\n    browser = webdriver.Chrome(options=options)\n\n    URL = 'https://www.kaggle.com/c/lux-ai-2021/submissions?dialog=episodes-submission-'\n\n    if verbose: print('Loading submission page...')\n    browser.get(URL + str(sub_id))\n    time.sleep(2)\n\n    if verbose: print('Scrolling results...')\n    scrolling_element = browser.find_element(\n        webdriver.common.by.By.XPATH,\n        \"//div[@class='mdc-dialog__surface']\")\n    if verbose:\n        generator = tqdm(range(100))\n    else:\n        generator = range(100)\n    for k in generator:\n        browser.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrolling_element)\n    time.sleep(1)\n\n    if verbose: print('Parsing page...')\n    html_source = browser.page_source\n    soup = BeautifulSoup(html_source, 'html.parser')\n    if verbose: print('Done!')\n    \n    return soup\n\ndef get_team_name_from_soup(soup):\n    team_names = []\n    for span in soup.select('span[class*=\"sc-\"]'):\n        text = span.get_text()\n        if 'vs' in text and '[' in text and 'ago' not in text:\n            for part in text.split(' vs '):\n                part_split = part.split(' ')\n                team_name = ' '.join(part_split[1:-2])\n                team_names.append(team_name)\n                \n    team_name = max(set(team_names), key = team_names.count)\n    return team_name\n\n@lru_cache(maxsize=1000)\ndef get_team_name(submission_id):\n    soup = getSoup(submission_id)\n    return get_team_name_from_soup(soup)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.427092Z","iopub.status.idle":"2021-11-29T08:51:06.42745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating a csv for later downloading the files","metadata":{}},{"cell_type":"markdown","source":"To download the files I need the episode id. I only want to download matches from the agents with the highest ranking.\n\nThus I'm going to update `epagents_df` to include the final ranking and later remove agents with low score.","metadata":{}},{"cell_type":"code","source":"submission_id_to_final_scores = {key: value for key, value in zip(leaderboard.SubmissionId, leaderboard.UpdatedScore)}\n\nepagents_df['FinalScore'] = epagents_df['SubmissionId'].apply(lambda x: submission_id_to_final_scores.get(x, -100))","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.428499Z","iopub.status.idle":"2021-11-29T08:51:06.428906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SCORE_THRESHOLD = 1750\nselection = epagents_df[epagents_df['FinalScore'] > SCORE_THRESHOLD]\nselection.sort_values('FinalScore', ascending=False, inplace=True)\nlen(selection),  len(selection.SubmissionId.unique())","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.429905Z","iopub.status.idle":"2021-11-29T08:51:06.430272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is far from elegant but it works. It seems that kaggle does not like receiving too many requests.","metadata":{}},{"cell_type":"code","source":"while 1:\n    try:\n        for submission_id in tqdm(selection['SubmissionId'].unique()):\n            get_team_name(submission_id)\n        break\n    except ValueError:\n        time.sleep(300)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.431083Z","iopub.status.idle":"2021-11-29T08:51:06.431447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_id_to_team = {submission_id: get_team_name(submission_id) for submission_id in tqdm(selection['SubmissionId'].unique())}","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.432217Z","iopub.status.idle":"2021-11-29T08:51:06.432641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\nselection['Team'] = selection['SubmissionId'].progress_apply(lambda x: submission_id_to_team[x])","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.433302Z","iopub.status.idle":"2021-11-29T08:51:06.43371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selection.to_csv('agent_selection_%s.csv' % time.strftime(\"%Y%m%d\"), index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.434408Z","iopub.status.idle":"2021-11-29T08:51:06.434822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selection.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.435704Z","iopub.status.idle":"2021-11-29T08:51:06.436064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selection.tail()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.437023Z","iopub.status.idle":"2021-11-29T08:51:06.437385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selection.groupby('SubmissionId').tail(1)[['FinalScore', 'UpdatedConfidence', 'Team', 'CreateTime', 'SubmissionId']].head(50)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.438329Z","iopub.status.idle":"2021-11-29T08:51:06.438766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selection.groupby('SubmissionId').tail(1)[['FinalScore', 'UpdatedConfidence', 'Team', 'CreateTime', 'SubmissionId']].to_csv(\n    'leaderboard_%s.csv' % time.strftime(\"%Y%m%d\"), index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('submission_id_to_team.json', 'w') as f:\n    json.dump({int(key): value for key, value in submission_id_to_team.items()}, f)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:51:06.43963Z","iopub.status.idle":"2021-11-29T08:51:06.440004Z"},"trusted":true},"execution_count":null,"outputs":[]}]}