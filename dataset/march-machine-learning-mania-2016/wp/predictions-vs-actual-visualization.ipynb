{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn import metrics\n\n## Purpose of this chart is to get an understanding of how predicted probabilities translate to actual wins\n## As well as an understanding of the prediction distribution across percentages\n## I've also added calcs for accuracy, auc & maximum miss\n\n## NOTE - in order to run this need to have a completed model, then create a dataframe \n## that contains the predicted chance of winning & actual outcome for each game in the test set\n\n## Create dataframe with predicted win % and actual result for each game from predicted test set\n## I've used random numbers as a dummy, and included my actual results in the comments\n\npredTestComp = pd.DataFrame({'pred' : np.random.uniform(0, 1, 20000),\n                             'Win': np.random.randint(0, 2, 20000)})\n\n## Round prediction % for ease of interpretation\n\npredTestComp['predRound'] = np.round(predTestComp['pred'], decimals=2) \n\n## Find Difference in predictions vs actuals, as well as max miss & print results\n## Also find number of \"correct\" predictions - above 50% and win / below 50% and loss\n## Then print the results\n    \npredTestComp['diff'] = abs(predTestComp['Win'] - predTestComp['pred'])\nmaximum = np.round(max(predTestComp['diff']),decimals = 4)\ndef f(df3):\n    if df3['diff'] < 0.5:\n        val = 1\n    else:\n        val = 0\n    return val\n\npredTestComp['bin'] = predTestComp.apply(f, axis=1)\naccuracy = np.round(sum(predTestComp['bin']) / len(predTestComp),decimals = 4)\nprint('The Biggest Upset Was', maximum)\nprint('Accuracy was', accuracy)\n    \n## Now Calc AUC\n    \ny = np.array(predTestComp['Win']+1)\npred = np.array(predTestComp['pred'])\nfpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\nauc = np.round(metrics.auc(fpr, tpr),decimals=4)\nprint('AUC was', auc)\npredTestComp.drop(['Type'],inplace=True,axis=1,errors='ignore')\npredTestComp.drop(['id'],inplace=True,axis=1,errors='ignore')\n\n## Group predictions by predicted win percentage, then find the total number of wins, games & average win percentage \n## For each rounded prediction value (0-100%)\n\ngrouped = predTestComp.groupby(['predRound'])['Win'].agg(['sum', 'count', 'mean']).reset_index()\n\n## Create subplots that have win% on the x axis, and number of games on the secondary axis\n\nfig, tsax = plt.subplots(figsize=(12,5))\nbarax = tsax.twinx()\n\n## Create bar chart based on the count of games for each predicted percentage\n\nbarax.bar(grouped.index, grouped['count'], facecolor=(0.5, 0.5, 0.5), alpha=0.3) \n\n## Create line chart that shows the average win percentage by predicted percentage\n\nfig.tight_layout()\ntsax.plot(grouped.index, grouped['mean'], color = 'b')\n\n## Set axis & data point labels as well as tick distribution\n\nbarax.set_ylabel('Number of Games')\nbarax.xaxis.tick_top()\ntsax.set_ylabel('Win %')\ntsax.set_xlabel('Predicted Win %')\ntsax.set_xlim([0, 101])\ntsax.set_ylim([0, 1])\nplt.xticks(np.arange(0, 101, 10))\npercListX = ['0%', '10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%']\npercListY = ['0%', '20%', '40%', '60%', '80%', '100%']\ntsax.set_xticklabels(percListX)\n#tsax.set_yticklabels(percListY)\n\n## Put line graph in front of bar chart\n\ntsax.set_zorder(barax.get_zorder()+1) \ntsax.patch.set_visible(False) # hide the 'canvas' \n\n## Create legend labels - necessary because it's a subplot\n\nline_patch = mpatches.Patch(color='blue', label='Percentage of Games Won')\nbar_patch = mpatches.Patch(color='gray', label='Number of Games')\nplt.legend(handles=[line_patch, bar_patch], loc = 'upper center')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}