{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1> Severstal: Steel Defect Detection</h1>"},{"metadata":{},"cell_type":"markdown","source":"* Steel Defect Detection is a competition hosted on kaggle by one of the largest steel manufacture company <b>Severstal</b> to help make production of steel more efficient.\n* Steel is one of the most important building materials of modern times as it is resistant to natural and man-made wear out.\n* Severstal uses images from high frequency cameras to power a defect detection algorithm.\n* In this competition, we have to improve the algorithm by localizing and classifying surface defects on a steel sheet."},{"metadata":{},"cell_type":"markdown","source":"<h2> About the Competition</h2>"},{"metadata":{},"cell_type":"markdown","source":"* This is a Kernels-only competition.\n* We are allowed to train a model offline, upload it as a dataset, and  then use it.\n* CPU or GPU Kernels should not be more than 1 hour run-time.\n* Internet must be turned off.\n* For submission, we need to output a file named submission.csv"},{"metadata":{},"cell_type":"markdown","source":"<h2>About the Data </h2>"},{"metadata":{},"cell_type":"markdown","source":"<h3>Files</h3>\n<ol>\n<li>train_images/ - folder of training images</li>\n<li>test_images/ - folder of test images (you are segmenting and classifying these images)</li>\n<li>train.csv - training annotations which provide segments for defects (ClassId = [1, 2, 3, 4])</li>\n<li>sample_submission.csv - a sample submission file in the correct format; note, each ImageId 4 rows, one for each of the 4 defect classes</li>\n    </ol>"},{"metadata":{},"cell_type":"markdown","source":"<h3> Featues</h3>\n<ol>\n    <li><b> ImageId_ClassId:</b> Images are named with a unique ImageId. For each image we must segment defects of each class (ClassId = [1, 2, 3, 4])</li>\n    <li><b> EncodedPixels: </b> Run-length encoding is used on the pixel values. It contains pairs of values that contain a start position and a run length. </li>\n</ol>"},{"metadata":{},"cell_type":"markdown","source":"<h3> Some important information </h3>\n<ul>\n<li> Each image may have no defects, a defect of a single class, or defects of multiple classes.<br>\n<li> Each row in submission represents a single predicted defect segmentation for the given ImageId, and predicted ClassId.<br>\n<li> Submission file should have the same number of rows as <i>num_images * num_defect_classes</i>.\n</ul>"},{"metadata":{},"cell_type":"markdown","source":"<h2> 1. Business Problem</h2>"},{"metadata":{},"cell_type":"markdown","source":"<h3>1.1 Problem Statement</h3>\n<p>\n     The challenge is to provide an effective and robust approach to detect the location and classify defects using computer vision and machine learning.\n</p>"},{"metadata":{},"cell_type":"markdown","source":"<h3>1.2 Business Objective and constraint</h3>\n\n__Objectives__:\n1. Predict the location and type of defects on a steel sheet.\n2. Maximize the Dice coefficient.\n\n__Constraints__:\n1. Some form of interpretability.\n2. Low Latency application."},{"metadata":{},"cell_type":"markdown","source":"<h2> 2. Machine Learning Problem </h2>"},{"metadata":{},"cell_type":"markdown","source":"<h3>2.1 Mapping the real world problem to a Machine Learning Problem </h3>\n<p>\nAs the problem involves predicting the location and type of defect on a steel sheet.Thus, it is a multi label classification and a regression problem at the same time.</p>"},{"metadata":{},"cell_type":"markdown","source":"<h3> 2.1.2 Performance metric </h3>\n<p>\nThe <b>Dice coefficient</b> is used as the performance metric in this competition. It can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. \n</p>\n<br>\n<pre> The formula is given by:   2 ∗ (|X∩Y|) / (|X|+|Y|)</pre>\nReference: https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient"},{"metadata":{},"cell_type":"markdown","source":"<h1> 3. Exploratory Data Analysis </h1>"},{"metadata":{},"cell_type":"markdown","source":"# Utility Functions\n* The first utility function converts rle to mask. \n* The second converts a mask to its contour. \n* The third enlarges a mask. \n* The second and third together put blank space between defect and mask contour for better visualization."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":false,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"# https://www.kaggle.com/titericz/building-and-visualizing-masks\ndef rle2maskResize(rle):\n    # CONVERT RLE TO MASK \n    if (pd.isnull(rle))|(rle==''): \n        return np.zeros((128,800) ,dtype=np.uint8)\n    \n    height= 256\n    width = 1600\n    mask= np.zeros( width*height ,dtype=np.uint8)\n\n    array = np.asarray([int(x) for x in rle.split()])\n    starts = array[0::2]-1\n    lengths = array[1::2]    \n    for index, start in enumerate(starts):\n        mask[int(start):int(start+lengths[index])] = 1\n    \n    return mask.reshape( (height,width), order='F' )[::2,::2]\n\ndef mask2contour(mask, width=3):\n    # CONVERT MASK TO ITS CONTOUR\n    w = mask.shape[1]\n    h = mask.shape[0]\n    mask2 = np.concatenate([mask[:,width:],np.zeros((h,width))],axis=1)\n    mask2 = np.logical_xor(mask,mask2)\n    mask3 = np.concatenate([mask[width:,:],np.zeros((width,w))],axis=0)\n    mask3 = np.logical_xor(mask,mask3)\n    return np.logical_or(mask2,mask3) \n\ndef mask2pad(mask, pad=2):\n    # ENLARGE MASK TO INCLUDE MORE SPACE AROUND DEFECT\n    w = mask.shape[1]\n    h = mask.shape[0]\n    \n    # MASK UP\n    for k in range(1,pad,2):\n        temp = np.concatenate([mask[k:,:],np.zeros((k,w))],axis=0)\n        mask = np.logical_or(mask,temp)\n    # MASK DOWN\n    for k in range(1,pad,2):\n        temp = np.concatenate([np.zeros((k,w)),mask[:-k,:]],axis=0)\n        mask = np.logical_or(mask,temp)\n    # MASK LEFT\n    for k in range(1,pad,2):\n        temp = np.concatenate([mask[:,k:],np.zeros((h,k))],axis=1)\n        mask = np.logical_or(mask,temp)\n    # MASK RIGHT\n    for k in range(1,pad,2):\n        temp = np.concatenate([np.zeros((h,k)),mask[:,:-k]],axis=1)\n        mask = np.logical_or(mask,temp)\n    \n    return mask ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>3.1 Importing Libraries</h2>"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"# import basics\nimport numpy as np, pandas as pd, os, gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom glob import glob\n\n# import plotting\nfrom matplotlib import pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib\nimport seaborn as sns\n\n# import image manipulation\nfrom PIL import Image \nimport cv2\n\nimport json\nimport keras\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers import Dropout\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.losses import binary_crossentropy\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\npath = '../input/severstal-steel-defect-detection/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>3.2 Load Data</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# set paths to train and test image datasets\nTRAIN_PATH =  path+'train_images/'\nTEST_PATH =  path+'test_images/'\n\n# load dataframe with train labels\ntrain_df = pd.read_csv( path+'train.csv')\ntrain_fns = sorted(glob(TRAIN_PATH + '*.jpg'))\ntest_fns = sorted(glob(TEST_PATH + '*.jpg'))\n\nprint('There are {} images in the train set.'.format(len(train_fns)))\nprint('There are {} images in the test set.'.format(len(test_fns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plotting a pie chart which demonstrates train and test sets\nlabels = 'Train', 'Test'\nsizes = [len(train_fns), len(test_fns)]\nexplode = (0, 0.1)\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\nax.axis('equal')\nax.set_title('Train and Test Sets')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>3.3 Explore Labels and Masks</h2>\n<h3>3.3.1 Explore empty masks:</h3>"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('There are {} rows with empty segmentation maps.'.format(len(train_df) - train_df.EncodedPixels.count()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plotting a pie chart\nlabels = 'Non-empty', 'Empty'\nsizes = [train_df.EncodedPixels.count(), len(train_df) - train_df.EncodedPixels.count()]\nexplode = (0, 0.1)\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\nax.axis('equal')\nax.set_title('Non-empty and Empty Masks')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>3.3.2 Explore the labels:</h3>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# split column\nsplit_df = train_df[\"ImageId_ClassId\"].str.split(\"_\", n = 1, expand = True)\n\n# add new columns to train_df\ntrain_df['Image'] = split_df[0]\ntrain_df['Label'] = split_df[1]\n\n# check the result\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Analyse the number of labels for each defect type\ndefect1 = train_df[train_df['Label'] == '1'].EncodedPixels.count()\ndefect2 = train_df[train_df['Label'] == '2'].EncodedPixels.count()\ndefect3 = train_df[train_df['Label'] == '3'].EncodedPixels.count()\ndefect4 = train_df[train_df['Label'] == '4'].EncodedPixels.count()\n\nlabels_per_image = train_df.groupby('Image')['EncodedPixels'].count()\n\nno_defects = labels_per_image[labels_per_image == 0].count()\n\nprint('There are {} defect1 images'.format(defect1))\nprint('There are {} defect2 images'.format(defect2))\nprint('There are {} defect3 images'.format(defect3))\nprint('There are {} defect4 images'.format(defect4))\nprint('There are {} images with no defects'.format(no_defects))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# plotting a pie chart\nlabels = 'Defect 1', 'Defect 2', 'Defect 3', 'Defect 4', 'No defects'\nsizes = [defect1, defect2, defect3, defect4, no_defects]\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\nax.axis('equal')\nax.set_title('Defect Types')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation:\nThe dataset is very imbalanced."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Number of Labels per Image\nprint('There are {} images with no labels'.format(labels_per_image[labels_per_image == 0].count()))\nprint('There are {} images with 1 label'.format(labels_per_image[labels_per_image == 1].count()))\nprint('There are {} images with 2 labels'.format(labels_per_image[labels_per_image == 2].count()))\nprint('There are {} images with 3 labels'.format(labels_per_image[labels_per_image == 3].count()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation:\n* Almost half of images doesn't contain any defects.\n* Most of images with defects contain the defects of only one type."},{"metadata":{},"cell_type":"markdown","source":"<h2>3.4 Analyze Images</h2>"},{"metadata":{},"cell_type":"markdown","source":"All images have the same size in our train and test dataset. **WIDTH = 1600 and HEIGHT = 256**"},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_mask(image_filename):\n    '''\n    Function to plot an image and segmentation masks.\n    INPUT:\n        image_filename - filename of the image (with full path)\n    '''\n    img_id = image_filename.split('/')[-1]\n    image = Image.open(image_filename)\n    train = train_df.fillna('-1')\n    rle_masks = train[(train['Image'] == img_id) & (train['EncodedPixels'] != '-1')]['EncodedPixels'].values\n    \n    defect_types = train[(train['Image'] == img_id) & (train['EncodedPixels'] != '-1')]['Label'].values\n    \n    if (len(rle_masks) > 0):\n        fig, axs = plt.subplots(1, 1 + len(rle_masks), figsize=(20, 3))\n\n        axs[0].imshow(image)\n        axs[0].axis('off')\n        axs[0].set_title('Original Image')\n\n        for i in range(0, len(rle_masks)):\n            mask = rle2maskResize(rle_masks[i])\n            axs[i + 1].imshow(image)\n            axs[i + 1].imshow(mask, alpha = 0.5, cmap = \"Reds\")\n            axs[i + 1].axis('off')\n            axs[i + 1].set_title('Mask with defect #{}'.format(defect_types[i]))\n\n        plt.suptitle('Image with defect masks')\n    else:\n        fig, axs = plt.subplots(figsize=(20, 3))\n        axs.imshow(image)\n        axs.axis('off')\n        axs.set_title('Original Image without Defects')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot image example with one defects\nfor image_code in train_df.Image.unique():\n    if (train_df.groupby(['Image'])['EncodedPixels'].count().loc[image_code] == 1):\n        plot_mask(TRAIN_PATH + image_code)\n        break;","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot image example with more than one defects\nfor image_code in train_df.Image.unique():\n    if (train_df.groupby(['Image'])['EncodedPixels'].count().loc[image_code] > 1):\n        plot_mask(TRAIN_PATH + image_code)\n        break;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>4. Preprocessing</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df = pd.read_csv('../input/severstal-steel-defect-detection/train.csv')\ntrain_df['ImageId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\ntrain_df['ClassId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[1])\ntrain_df['hasMask'] = ~ train_df['EncodedPixels'].isna()\n\nprint(train_df.shape)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"mask_count_df = train_df.groupby('ImageId').agg(np.sum).reset_index()\nmask_count_df.sort_values('hasMask', ascending=False, inplace=True)\nprint(mask_count_df.shape)\nmask_count_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub_df = pd.read_csv('../input/severstal-steel-defect-detection/sample_submission.csv')\nsub_df['ImageId'] = sub_df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\ntest_imgs = pd.DataFrame(sub_df['ImageId'].unique(), columns=['ImageId'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utility Functions\n\nSource: https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode"},{"metadata":{},"cell_type":"markdown","source":"## Mask encoding and decoding"},{"metadata":{"trusted":false},"cell_type":"code","source":"def mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle2mask(mask_rle, shape=(256,1600)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (width,height) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def build_masks(rles, input_shape):\n    depth = len(rles)\n    height, width = input_shape\n    masks = np.zeros((height, width, depth))\n    \n    for i, rle in enumerate(rles):\n        if type(rle) is str:\n            masks[:, :, i] = rle2mask(rle, (width, height))\n    \n    return masks\n\ndef build_rles(masks):\n    width, height, depth = masks.shape\n    \n    rles = [mask2rle(masks[:, :, i])\n            for i in range(depth)]\n    \n    return rles","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loss function\n\nSource for `bce_dice_loss`: https://lars76.github.io/neural-networks/object-detection/losses-for-segmentation/"},{"metadata":{"trusted":false},"cell_type":"code","source":"def dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sample Test\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_filename = 'db4867ee8.jpg'\nsample_image_df = train_df[train_df['ImageId'] == sample_filename]\nsample_path = f\"../input/severstal-steel-defect-detection/train_images/{sample_image_df['ImageId'].iloc[0]}\"\nsample_img = cv2.imread(sample_path)\nsample_rles = sample_image_df['EncodedPixels'].values\nsample_masks = build_masks(sample_rles, input_shape=(256, 1600))\n\nfig, axs = plt.subplots(5, figsize=(12, 12))\naxs[0].imshow(sample_img)\naxs[0].axis('off')\n\nfor i in range(4):\n    axs[i+1].imshow(sample_masks[:, :, i])\n    axs[i+1].axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Generator"},{"metadata":{"trusted":false},"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, df, target_df=None, mode='fit',\n                 base_path='../input/severstal-steel-defect-detection/train_images',\n                 batch_size=32, dim=(256, 1600), n_channels=1,\n                 n_classes=4, random_state=2019, shuffle=True):\n        self.dim = dim\n        self.batch_size = batch_size\n        self.df = df\n        self.mode = mode\n        self.base_path = base_path\n        self.target_df = target_df\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.random_state = random_state\n        \n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_batch = [self.list_IDs[k] for k in indexes]\n        \n        X = self.__generate_X(list_IDs_batch)\n        \n        if self.mode == 'fit':\n            y = self.__generate_y(list_IDs_batch)\n            return X, y\n        \n        elif self.mode == 'predict':\n            return X\n\n        else:\n            raise AttributeError('The mode parameter should be set to \"fit\" or \"predict\".')\n        \n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.seed(self.random_state)\n            np.random.shuffle(self.indexes)\n    \n    def __generate_X(self, list_IDs_batch):\n        'Generates data containing batch_size samples'\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        \n        # Generate data\n        for i, ID in enumerate(list_IDs_batch):\n            im_name = self.df['ImageId'].iloc[ID]\n            img_path = f\"{self.base_path}/{im_name}\"\n            img = self.__load_grayscale(img_path)\n            \n            # Store samples\n            X[i,] = img\n\n        return X\n    \n    def __generate_y(self, list_IDs_batch):\n        y = np.empty((self.batch_size, *self.dim, self.n_classes), dtype=int)\n        \n        for i, ID in enumerate(list_IDs_batch):\n            im_name = self.df['ImageId'].iloc[ID]\n            image_df = self.target_df[self.target_df['ImageId'] == im_name]\n            \n            rles = image_df['EncodedPixels'].values\n            masks = build_masks(rles, input_shape=self.dim)\n            \n            y[i, ] = masks\n\n        return y\n    \n    def __load_grayscale(self, img_path):\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        img = img.astype(np.float32) / 255.\n        img = np.expand_dims(img, axis=-1)\n\n        return img\n    \n    def __load_rgb(self, img_path):\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = img.astype(np.float32) / 255.\n\n        return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"BATCH_SIZE = 16\n\ntrain_idx, val_idx = train_test_split(\n    mask_count_df.index, random_state=2019, test_size=0.15\n)\n\ntrain_generator = DataGenerator(\n    train_idx, \n    df=mask_count_df,\n    target_df=train_df,\n    batch_size=BATCH_SIZE, \n    n_classes=4\n)\n\nval_generator = DataGenerator(\n    val_idx, \n    df=mask_count_df,\n    target_df=train_df,\n    batch_size=BATCH_SIZE, \n    n_classes=4\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Architecture\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"def build_model(input_shape):\n    inputs = Input(input_shape)\n\n    c1 = Conv2D(8, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (inputs)\n    c1 = Dropout(0.1) (c1)\n    c1 = Conv2D(8, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n    p1 = MaxPooling2D((2, 2)) (c1)\n    \n    c2 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n    c2 = Dropout(0.1) (c2)\n    c2 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n    p2 = MaxPooling2D((2, 2)) (c2)\n\n    c3 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n    c3 = Dropout(0.2) (c3)\n    c3 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n    p3 = MaxPooling2D((2, 2)) (c3)\n    \n    c4 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n    c4 = Dropout(0.2) (c4)\n    c4 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n    \n    c5 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n    c5 = Dropout(0.3) (c5)\n    c5 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n    \n    u6 = Conv2DTranspose(64,(2, 2), strides=(2, 2),padding='same') (c5)\n    u6 = concatenate([u6, c4])\n    c6 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n    c6 = Dropout(0.2) (c6)\n    c6 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n    \n    u7 = Conv2DTranspose(32,(2, 2), strides=(2, 2),padding='same') (c6)\n    u7 = concatenate([u7, c3])\n    c7 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n    c7 = Dropout(0.2) (c7)\n    c7 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n    \n    u8 = Conv2DTranspose(16,(2, 2), strides=(2, 2),padding='same') (c7)\n    u8 = concatenate([u8, c2])\n    c8 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n    c8 = Dropout(0.1) (c8)\n    c8 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n    \n    u9 = Conv2DTranspose(8,(2, 2), strides=(2, 2),padding='same') (c8)\n    u9 = concatenate([u9, c1], axis=3)\n    c9 = Conv2D(8, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n    c9 = Dropout(0.1) (c9)\n    c9 = Conv2D(8, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n    \n    outputs = Conv2D(4, (1, 1), activation='sigmoid') (c9)\n\n    model = Model(inputs=[inputs], outputs=[outputs])\n    model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_coef])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = build_model((256, 1600, 1))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"checkpoint = ModelCheckpoint(\n    'model.h5', \n    monitor='val_dice_coef', \n    verbose=0, \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto'\n)\n\nhistory = model.fit_generator(\n    train_generator,\n    validation_data=val_generator,\n    callbacks=[checkpoint],\n    use_multiprocessing=False,\n    workers=1,\n    epochs=9\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation & Submission"},{"metadata":{"trusted":false},"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['dice_coef', 'val_dice_coef']].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.load_weights('model.h5')\ntest_df = []\n\nfor i in range(0, test_imgs.shape[0], 500):\n    batch_idx = list(\n        range(i, min(test_imgs.shape[0], i + 500))\n    )\n    \n    test_generator = DataGenerator(\n        batch_idx,\n        df=test_imgs,\n        shuffle=False,\n        mode='predict',\n        base_path='../input/severstal-steel-defect-detection/test_images',\n        target_df=sub_df,\n        batch_size=1,\n        n_classes=4\n    )\n    \n    batch_pred_masks = model.predict_generator(\n        test_generator, \n        workers=1,\n        verbose=1,\n        use_multiprocessing=False\n    )\n    \n    for j, b in tqdm(enumerate(batch_idx)):\n        filename = test_imgs['ImageId'].iloc[b]\n        image_df = sub_df[sub_df['ImageId'] == filename].copy()\n        \n        pred_masks = batch_pred_masks[j, ].round().astype(int)\n        pred_rles = build_rles(pred_masks)\n        \n        image_df['EncodedPixels'] = pred_rles\n        test_df.append(image_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_df = pd.concat(test_df)\ntest_df.drop(columns='ImageId', inplace=True)\ntest_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_df.head(40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://mc.ai/understand-semantic-segmentation-with-the-fully-convolutional-network-u-net-step-by-step/"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"1a26195172a5423ebb841e2762595bb1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6acdd2876bcb4a15a75e0af78354b285":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"6c28c6dcbf0b49d7918d987d88084898":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d5ea8fc461542b38bf9d0385d98346b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7b3d02f14964d8298710c69e75b4e82","placeholder":"​","style":"IPY_MODEL_f2dc053eecb24137a4fc42a7b2e30139","value":" 0/1801 [00:00&lt;?, ?it/s]"}},"af4661c4735d4a14bb305b1952bd304c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"  0%","description_tooltip":null,"layout":"IPY_MODEL_1a26195172a5423ebb841e2762595bb1","max":1801,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6acdd2876bcb4a15a75e0af78354b285","value":0}},"bea05ba276774d2c902666e31565ecbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af4661c4735d4a14bb305b1952bd304c","IPY_MODEL_8d5ea8fc461542b38bf9d0385d98346b"],"layout":"IPY_MODEL_6c28c6dcbf0b49d7918d987d88084898"}},"e7b3d02f14964d8298710c69e75b4e82":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2dc053eecb24137a4fc42a7b2e30139":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}