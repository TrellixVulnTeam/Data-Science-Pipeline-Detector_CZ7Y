{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Steel Adversarial Validation\nIn this kernel, we compare the training images to the test images for Kaggle's \"Steel Defect Detection\" competition. We observe that there is a significant difference. If you select an image at random with 50/50 chance of being train or test, a classifier can distinquish whether it is a train or test image with 85% accuracy! Why is that?"},{"metadata":{},"cell_type":"markdown","source":"# Prepare Images\nIn this kernel we randomly select 1801 training images. Then we have an equal number of train and test images. Note that if we compare 1801 random train images with a different 1801 random train images, then a classifier cannot do better than 50% detection. See Appendix. "},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import os, numpy as np\nfrom PIL import Image \n\nTRAIN_IMG = os.listdir('../input/severstal-steel-defect-detection/train_images')\nTEST_IMG = os.listdir('../input/severstal-steel-defect-detection/test_images')\nprint('Original train count =',len(TRAIN_IMG),', Original test count =',len(TEST_IMG))\nprint('New train count = 1801 , New test count = 1801')\nos.mkdir('../tmp/')\nos.mkdir('../tmp/train_images/')\nr = np.random.choice(TRAIN_IMG,len(TEST_IMG),replace=False)\nfor i,f in enumerate(r):\n    img = Image.open('../input/severstal-steel-defect-detection/train_images/'+f)\n    img.save('../tmp/train_images/'+f)\nos.mkdir('../tmp/test_images/')\nfor i,f in enumerate(TEST_IMG):\n    img = Image.open('../input/severstal-steel-defect-detection/test_images/'+f)\n    img.save('../tmp/test_images/'+f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Adversarial Classifier\nTo distinguish train images from test images, we will use pretrained Xception."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import pandas as pd\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras import layers\nfrom keras.callbacks import LearningRateScheduler\nimport matplotlib.pyplot as plt, time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"from keras import applications\nbase_model = applications.Xception(weights=None, input_shape=(256, 256, 3), include_top=False)\nbase_model.load_weights('../input/keras-pretrained-models/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')\nbase_model.trainable = False\nx = base_model.output\nx = layers.Flatten()(x)\nx = layers.Dense(1024, activation=\"relu\")(x)\nx = layers.Dropout(0.5)(x)\npredictions = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(input = base_model.input, output = predictions)\nmodel.compile(loss='binary_crossentropy', optimizer = \"adam\", metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_dir = '../tmp/'\nimg_height = 256; img_width = 256\nbatch_size = 32; nb_epochs = 15\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n    horizontal_flip=True,\n    validation_split=0.2) # set validation split\n\ntrain_generator = train_datagen.flow_from_directory(\n    img_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='training') # set as training data\n\nvalidation_generator = train_datagen.flow_from_directory(\n    img_dir, # same directory as training data\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='validation') # set as validation data\n\nannealer = LearningRateScheduler(lambda x: 0.0001 * 0.95 ** x)\nh = model.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples // batch_size,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples // batch_size,\n    epochs = nb_epochs,\n    callbacks = [annealer],\n    verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.plot(h.history['acc'],label='Train ACC')\nplt.plot(h.history['val_acc'],label='Val ACC')\nplt.title('TRAIN COMPARED WITH TEST. Training History')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nThis is concerning. What is different between the training images and test images that a classifier can tell them apart with 85% accuracy? Note that if we compare a random 1801 training images with another random 1801 training images, a classifier can not distinquish the two groups better than 50%. See Appendix below.\n\n# Appendix\nFor comparision, we demonstrate that a classifier cannot distinquish one random group of 1801 training images from another random group of 1801 training images better than 50%. Therefore it is significant that we can distinquish train from test with 85% accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"! rm -r ../tmp\n# COMPARE 1801 RANDOM TRAIN WITH 1801 RANDOM TRAIN\nTRAIN_IMG = os.listdir('../input/severstal-steel-defect-detection/train_images')\nos.mkdir('../tmp/')\nos.mkdir('../tmp/train_images/')\nr = np.random.choice(TRAIN_IMG,3602,replace=False)\nfor i,f in enumerate(r[:1801]):\n    img = Image.open('../input/severstal-steel-defect-detection/train_images/'+f)\n    img.save('../tmp/train_images/'+f)\nos.mkdir('../tmp/test_images/')\nfor i,f in enumerate(r[1801:]):\n    img = Image.open('../input/severstal-steel-defect-detection/train_images/'+f)\n    img.save('../tmp/test_images/'+f)\n    \n# BUILD XCEPTION CLASSIFIER\nbase_model = applications.Xception(weights=None, input_shape=(256, 256, 3), include_top=False)\nbase_model.load_weights('../input/keras-pretrained-models/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')\nbase_model.trainable = False\nx = base_model.output\nx = layers.Flatten()(x)\nx = layers.Dense(1024, activation=\"relu\")(x)\nx = layers.Dropout(0.5)(x)\npredictions = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(input = base_model.input, output = predictions)\nmodel.compile(loss='binary_crossentropy', optimizer = \"adam\", metrics=['accuracy'])\n\n# DATA PIPELINE\nimg_dir = '../tmp/'\nimg_height = 256; img_width = 256\nbatch_size = 32; nb_epochs = 15\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n    horizontal_flip=True,\n    validation_split=0.2) # set validation split\n\ntrain_generator = train_datagen.flow_from_directory(\n    img_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='training') # set as training data\n\nvalidation_generator = train_datagen.flow_from_directory(\n    img_dir, # same directory as training data\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='validation') # set as validation data\n\n# TRAIN CLASSIFIER\nannealer = LearningRateScheduler(lambda x: 0.0001 * 0.95 ** x)\nh = model.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples // batch_size,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples // batch_size,\n    epochs = nb_epochs,\n    callbacks = [annealer],\n    verbose=2)\n\n# PLOT RESULTS\nplt.figure(figsize=(15,5))\nplt.plot(h.history['acc'],label='Train ACC')\nplt.plot(h.history['val_acc'],label='Val ACC')\nplt.title('TRAIN COMPARED WITH TRAIN. Training History')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}