{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Changelog**\n\n* RAdam optimizer (practice implementation by inheriting torch Adam and overriding step method)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nfrom fastai.vision import *\nimport seaborn as sns\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Quick Look at the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\n\nshape = (1600, 256)\n\ndata_folder = Path('/kaggle/input/severstal-steel-defect-detection/')\n\ntrain_data = pd.read_csv(data_folder / 'train.csv')\ntrain_data = pd.concat([train_data, train_data.ImageId_ClassId.str.split('_', expand=True).rename(columns={0: 'ImageId', 1: 'ClassId'})], axis=1)\n\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_data[['ImageId', 'ClassId', 'EncodedPixels']]\nmask_map = train_df.set_index(['ImageId', 'ClassId'])\n\nmask_map.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How many images have masks?"},{"metadata":{"trusted":true},"cell_type":"code","source":"imgs_with_masks = train_data.ImageId.drop_duplicates().isin(train_data.dropna().ImageId.drop_duplicates()).sum()\ntotal_images = train_data.ImageId.drop_duplicates().shape[0]\n\nprint(f'{imgs_with_masks}/{total_images} --- {imgs_with_masks / total_images * 100} %')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How many masks per image?"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_data.dropna()[['ImageId', 'ClassId']].groupby('ImageId').count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Utilities"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    \n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n \ndef rle2mask(mask_rle, shape=(1600,256)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (width,height) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n        \n    return img.reshape(shape).T\n\ndef decode_image_mask(img_id, shape=(1600,256)):\n    masks = []\n    for cl in range(1,5):\n        cl_str = str(cl)\n\n        cl_rle = mask_map.loc[(img_id, cl_str)].values[0]\n\n        if not isinstance(cl_rle, str):\n            cl_mask = np.zeros(shape).T\n        else:\n            cl_mask = rle2mask(cl_rle, shape)\n\n        cl_mask = cl_mask[np.newaxis,:,:] * cl\n        masks.append(cl_mask)\n\n    masks = np.concatenate(masks, axis=0)\n    masks = np.sum(masks, axis=0)\n\n    return masks\n\ndef encode_image_mask(mask_tensor, shape=(1600,256)):\n    mask_array = np.array(mask_tensor)\n    mask_array = mask_array.argmax(axis=0)  \n        \n    rles = []\n    for i in range(1,5):\n        mask = (mask_array == i).astype(np.uint8)\n        mask = cv2.resize(mask, shape, cv2.INTER_NEAREST)\n        \n        rle = mask2rle(mask)\n        rle = np.nan if rle == '' else rle\n        rles.append(rle)\n    \n    return rles","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"import fastai\n\ndef transform(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n    if not tfms: tfms=(None,None)\n    assert is_listy(tfms) and len(tfms) == 2\n    self.train.transform(tfms[0], **kwargs)\n    self.valid.transform(tfms[1], **kwargs)\n    kwargs['tfm_y'] = False # Test data has no labels\n    if self.test: self.test.transform(tfms[1], **kwargs)\n    return self\n\nfastai.data_block.ItemLists.transform = transform\n\nclass ServerstalSegmentationLabelList(SegmentationLabelList):\n    \n    def open(self, image_id:str):\n        mask = decode_image_mask(image_id, shape)[np.newaxis,:,:]\n        mask = torch.tensor(mask, dtype=torch.float)\n        \n        return ImageSegment(mask)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train on All Images"},{"metadata":{},"cell_type":"markdown","source":"### Create Validation Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n_, valid_ids = train_test_split(train_df.ImageId.drop_duplicates(), test_size=0.2)\n\ntrain_df['is_valid'] = train_df.ImageId.isin(valid_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bs = 16\nsize = (128,800)\ndata = (\n    SegmentationItemList.from_df(train_df, data_folder, cols='ImageId', folder='train_images')\n    .split_from_df('is_valid')\n    .label_from_df('ImageId', label_cls=ServerstalSegmentationLabelList, classes=[0,1,2,3,4])\n    .add_test_folder(data_folder / 'test_images', label=None)\n    .transform(get_transforms(flip_vert=True), size=size, tfm_y=True)\n    .databunch(bs=bs)\n    .normalize(imagenet_stats)\n)\n\ndata.show_batch(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport torch\nfrom torch.optim import Adam\n\n\nclass RAdam(Adam):\n    \n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, amsgrad=False):\n        super(RAdam, self).__init__(params, lr=lr, betas=betas, eps=eps, \n                                   weight_decay=weight_decay)\n        \n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n\n                state = self.state[p]\n\n                beta1, beta2 = group['betas']\n                \n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    # Max SMA\n                    state['max_sma'] = 2/(1 - beta2) - 1\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                \n                state['step'] += 1\n\n                if group['weight_decay'] != 0:\n                    grad.add_(group['weight_decay'], p.data)\n                \n                max_sma = state['max_sma']\n                \n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                \n                bias_correction1 = 1 - beta1 ** state['step']\n                \n                beta2_t = beta2 ** state['step']\n                approx_sma = max_sma - 2 * state['step'] * beta2_t / beta2_t\n                \n                if approx_sma > 4:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    bias_correction2 = 1 - beta2 ** state['step']\n                    var_rectification = math.sqrt(\n                                                    ((approx_sma - 4) * (approx_sma - 2) * max_sma) \n                                                    / ((max_sma - 4) * (max_sma - 2) * approx_sma)\n                                                 )\n                    step_size = group['lr'] * var_rectification * math.sqrt(bias_correction2) / bias_correction1\n\n                    p.data.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p.data.add_(-group['lr'] * exp_avg / bias_correction1)\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = models.resnet34\n\nlearn = unet_learner(data, base_model, model_dir='/kaggle/working', opt_func=RAdam, metrics=[dice])\nlearn.path = Path('/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_find(learn)\n\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(9, 1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\n\nlr_find(learn)\n\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(8, slice(4e-6/20, 4e-6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.export()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load_learner('/kaggle/working')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}