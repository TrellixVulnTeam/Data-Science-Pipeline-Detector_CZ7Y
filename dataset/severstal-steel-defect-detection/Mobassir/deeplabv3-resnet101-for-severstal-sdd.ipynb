{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Motivation"},{"metadata":{},"cell_type":"markdown","source":"**In my earlier discussion post titled [DRN — Dilated Residual Networks (Image Classification & Semantic Segmentation)](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/111546) ** a kaggler name Qishen Ha (@haqishen) said \"If you want to try out dilated convolution, Deeplabv3 is a good sample to learn\" :) so i decided to make this kernel for my fellow kaggle mates and also self teaching deeplabv3 a little bit :)"},{"metadata":{},"cell_type":"markdown","source":"**Note : i am completely new in deep learning world and has very little knowledge about this field,if you find any implementation error or bug please let me know in the comment box and it will be highly appreciated**"},{"metadata":{},"cell_type":"markdown","source":"#  Algorithms for Image segmentation"},{"metadata":{},"cell_type":"markdown","source":"Image segmentation is a long standing computer Vision problem. Quite a few algorithms have been designed to solve this task, such as the Watershed algorithm, Image thresholding , K-means clustering, Graph partitioning methods, etc.\n\nMany deep learning architectures (like fully connected networks for image segmentation) have also been proposed, but Google’s DeepLab model has given the best results till date. That’s why we’ll focus on using DeepLab.DeepLab uses atrous convolution with rates 6, 12 and 18."},{"metadata":{},"cell_type":"markdown","source":"# Why using DEEPLABV3-RESNET101?"},{"metadata":{},"cell_type":"markdown","source":"*For the semantic segmentation model, GluonCV-Torch mainly supports pre-trained FCN, PSPNet and DeepLab-V3. DeepLab-V3 is a very common open source model, which has very good effect on semantic segmentation tasks. The pre-training effects of these three models in the Pascal VOC dataset are shown below, where Pascal VOC contains 20 categories of images:*\n\n![](http://www.programmersought.com/images/427/d258324e597cccec479b706fa7a2a04b.png)"},{"metadata":{},"cell_type":"markdown","source":"**The following shows the effects of three semantic segmentation models in the ADE20K dataset, where ADE20K is a scene resolution dataset published by MIT that contains a variety of scenarios, including people, backgrounds, and objects.**\n\n![](http://www.programmersought.com/images/928/522173f7eab9d4d27b9bbdd0b833fde8.png)"},{"metadata":{},"cell_type":"markdown","source":"*  Source of informations above : http://www.programmersought.com/article/5710352893/"},{"metadata":{},"cell_type":"markdown","source":"*  <font size=\"4\" color=\"green\">Google's Deeplabv3 uses dilated convolution,lets first talk little bit about dilated convolutions!</font>"},{"metadata":{},"cell_type":"markdown","source":"# Dilated Convolution"},{"metadata":{},"cell_type":"markdown","source":" equation of DilatedNet:\n \n![](https://miro.medium.com/proxy/1*mlHFvK6H_wMCyURSZNZWGQ.png) \n\n\nThe left one is the standard convolution. The right one is the dilated convolution. We can see that at the summation, it is s+lt=p that we will skip some points during convolution."},{"metadata":{},"cell_type":"markdown","source":"1. When l=1, it is standard convolution\n\n2. When l>1, it is dilated convolution."},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/1185/1*btockft7dtKyzwXqfq70_w.gif)\n                  **Standard Convolution (l=1) (Left) Dilated Convolution (l=2) (Right)**"},{"metadata":{},"cell_type":"markdown","source":"* The above illustrate an example of dilated convolution when l=2. We can see that the receptive field is larger compared with the standard one."},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/proxy/1*tnDNIyPePgHvb8JIx8SbqA.png)\n\n**l=1 (left), l=2 (Middle), l=4 (Right)**"},{"metadata":{},"cell_type":"markdown","source":" The above figure shows more examples about the receptive field."},{"metadata":{},"cell_type":"markdown","source":"# Reasons of Dilated Convolution?"},{"metadata":{},"cell_type":"markdown","source":"1. It is found that with small output feature map obtained at the end of the network, the accuracy is reduced in semantic segmentation.\n\n2. In FCN, it also shows that when 32× upsampling is needed, we can only get a very rough segmentation results. Thus, a larger output feature map is desired.\n\n3. A naive approach is to simply remove subsampling (striding) steps in the network in order to increase the resolution of feature map. However, this also reduces the receptive field which severely reduces the amount of context. such reduction in receptive field is an unacceptable price to pay for higher resolution.\n\n4. For this reason, dilated convolutions are used to increase the receptive field of the higher layers, compensating for the reduction in receptive field induced by removing subsampling.\n\n5. And it is found that using dilated convolution can also help for image classification task in this paper.\n"},{"metadata":{},"cell_type":"markdown","source":"## Dilated Residual Networks (DRN)"},{"metadata":{},"cell_type":"markdown","source":"1. In the paper(mentioned below), it uses d as dilation factor.\n\n2. When d=1, it is standard convolution.\n\n3. When d>1, it is dilated convolution."},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/794/1*-67TMJkhBO3sTtzAg2oUHg.png)"},{"metadata":{},"cell_type":"markdown","source":"SOURCES : \n1. https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5\n2. https://towardsdatascience.com/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5"},{"metadata":{},"cell_type":"markdown","source":"I  highly recommend you to check this link : [Semantic Segmentation: Introduction to the Deep Learning Technique Behind Google Pixel’s Camera!](https://www.analyticsvidhya.com/blog/2019/02/tutorial-semantic-segmentation-google-deeplab/)\ni have used some of the contents from that link,definitely a great resource for understanding deeplabv3 well"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"6\" color=\"red\">Please UPVOTE if you find this kernel Helpful!</font>"},{"metadata":{},"cell_type":"markdown","source":"**IMPORTS**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import print_function\n\nfrom collections import defaultdict, deque\nimport datetime\nimport pickle\nimport time\nimport torch.distributed as dist\nimport errno\nfrom fastai import metrics\n\nimport cv2\nimport collections\nimport os\nimport numpy as np\n\nimport torch.utils.data\nfrom PIL import Image, ImageFile\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom torchvision import transforms\nimport torchvision\nimport random\nfrom torch.utils.data import DataLoader, Dataset, sampler\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport cv2\nimport pdb\nimport time\nimport warnings\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader, Dataset, sampler\nfrom matplotlib import pyplot as plt\nfrom albumentations import (HorizontalFlip,VerticalFlip,Cutout,SmallestMaxSize,\n                            ToGray, ShiftScaleRotate, Blur,Normalize, Resize, Compose, GaussNoise)\nfrom albumentations.pytorch import ToTensor\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nfrom tqdm import tqdm_notebook\nimport cv2\nfrom PIL import Image\n\nfrom torchvision import models\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.utils.data as utils\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import platform\nprint(f'Python version: {platform.python_version()}')\nprint(f'PyTorch version: {torch.__version__}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=43):\n    '''\n      Make PyTorch deterministic.\n    '''    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    torch.backends.cudnn.deterministic = True\n\nseed_everything()\n\nIS_DEBUG = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Utility**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n\n    def f(x):\n        if x >= warmup_iters:\n            return 1\n        alpha = float(x) / warmup_iters\n        return warmup_factor * (1 - alpha) + alpha\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loss Function**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DiceLoss(torch.nn.Module):\n    def __init__(self):\n        super(DiceLoss, self).__init__()\n \n    def forward(self, logits, targets):\n        ''' fastai.metrics.dice uses argmax() which is not differentiable, so it \n          can NOT be used in training, however it can be used in prediction.\n          see https://github.com/fastai/fastai/blob/master/fastai/metrics.py#L53\n        '''\n        N = targets.size(0)\n        preds = torch.sigmoid(logits)\n        #preds = logits.argmax(dim=1) # do NOT use argmax in training, because it is NOT differentiable\n        # https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/keras/backend.py#L96\n        EPSILON = 1e-7\n \n        preds_flat = preds.view(N, -1)\n        targets_flat = targets.view(N, -1)\n \n        intersection = (preds_flat * targets_flat).sum()#.float()\n        union = (preds_flat + targets_flat).sum()#.float()\n        \n        loss = (2.0 * intersection + EPSILON) / (union + EPSILON)\n        loss = 1 - loss / N\n        return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Function for training**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef train_one_epoch(model, optimizer, data_loader, device, epoch):\n    model.train()\n    loss_func = DiceLoss() #nn.BCEWithLogitsLoss() \n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1. / 1000\n        warmup_iters = min(1000, len(data_loader) - 1)\n\n        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n\n    lossf=None\n    inner_tq = tqdm(data_loader, total=len(data_loader), leave=False, desc= f'Iteration {epoch}')\n    for images, masks in inner_tq:\n        y_preds = model(images.to(device))\n        y_preds = y_preds['out'][:, 1, :, :] #\n\n        loss = loss_func(y_preds, masks.to(device))\n\n        if torch.cuda.device_count() > 1:\n            loss = loss.mean() # mean() to average on multi-gpu.\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        if lossf:\n            lossf = 0.98*lossf+0.02*loss.item()\n        else:\n            lossf = loss.item()\n        inner_tq.set_postfix(loss = lossf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Mask Decoder**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle2mask(rle, imgshape):\n    width = imgshape[0]\n    height= imgshape[1]\n    \n    mask= np.zeros( width*height ).astype(np.uint8)\n    \n    array = np.asarray([int(x) for x in rle.split()])\n    starts = array[0::2]\n    lengths = array[1::2]\n\n    current_position = 0\n    for index, start in enumerate(starts):\n        mask[int(start):int(start+lengths[index])] = 1\n        current_position += lengths[index]\n        \n    return np.flipud( np.rot90( mask.reshape(height, width), k=1 ) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Steel Dataset paths**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir('../input/severstal-steel-defect-detection/'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npath = '../input/severstal-steel-defect-detection/'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = pd.read_csv(path + 'train.csv')\nprint(len(tr))\ntr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = tr[tr['EncodedPixels'].notnull()].reset_index(drop=True)\n\n#df_train1 = df_train[df_train['ImageId_ClassId'].apply(lambda x: x.split('_')[1] == '1')].reset_index(drop=True)\n#df_train2 = df_train[df_train['ImageId_ClassId'].apply(lambda x: x.split('_')[1] == '2')].reset_index(drop=True)\n#df_train3 = df_train[df_train['ImageId_ClassId'].apply(lambda x: x.split('_')[1] == '3')].reset_index(drop=True)\n#df_train4 = df_train[df_train['ImageId_ClassId'].apply(lambda x: x.split('_')[1] == '4')].reset_index(drop=True)\n\n#df_train = tr[tr['EncodedPixels']].reset_index(drop=True)\n#df_train = tr\nprint(len(df_train))\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Steel DataLoader**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageData(Dataset):\n    def __init__(self, df, transform, subset=\"train\"):\n        super().__init__()\n        self.df = df\n        self.transform = transform\n        self.subset = subset\n        \n        if self.subset == \"train\":\n            self.data_path = path + 'train_images/'\n        elif self.subset == \"test\":\n            self.data_path = path + 'test_images/'\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):                      \n        fn = self.df['ImageId_ClassId'].iloc[index].split('_')[0]         \n        img = Image.open(self.data_path + fn)\n        img = self.transform(img)\n\n        if self.subset == 'train': \n            mask = rle2mask(self.df['EncodedPixels'].iloc[index], (256, 1600))\n            mask = transforms.ToPILImage()(mask)            \n            mask = self.transform(mask)\n            return img, mask\n        else: \n            mask = None\n            return img  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_transf = transforms.Compose([\n                                  transforms.Scale((256, 1600)),\n                                  #HorizontalFlip(p=0.5),\n                                  #VerticalFlip(p = 0.5),\n                                  #Blur(),\n                                  #Cutout(),\n                                  #ShiftScaleRotate(),\n                                  #GaussNoise(),\n                                  #ToGray(),\n                                  transforms.ToTensor()])\ntrain_data = ImageData(df = df_train, transform = data_transf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Understanding the DeepLab Model Architecture\n\nDeepLab V3 uses ImageNet’s pretrained Resnet-101 with atrous convolutions as its main feature extractor. In the modified ResNet model, the last ResNet block uses atrous convolutions with different dilation rates. It uses Atrous Spatial Pyramid Pooling and bilinear upsampling for the decoder module on top of the modified ResNet block.\n\nDeepLab V3+ uses Aligned Xception as its main feature extractor, with the following modifications:\n\n1. All max pooling operations are replaced by depthwise separable convolution with striding\n2. Extra batch normalization and ReLU activation are added after each 3 x 3 depthwise convolution\n3. Depth of the model is increased without changing the entry flow network structure"},{"metadata":{},"cell_type":"markdown","source":"![](https://i0.wp.com/s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/01/semantic_8.png?resize=649%2C333&ssl=1)"},{"metadata":{},"cell_type":"markdown","source":"**[DeepLabV3 model with a ResNet-101 backbone](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_ft = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False, num_classes=4)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_ft.to(device)\nNUM_GPUS = torch.cuda.device_count()\nif NUM_GPUS > 1:\n    model_ft = torch.nn.DataParallel(model_ft)\n_ = model_ft.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=4, shuffle=True, num_workers=NUM_GPUS,drop_last=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# construct an optimizer\nparams = [p for p in model_ft.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.0001, momentum=0.9, weight_decay=0.0005)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=5,\n                                               gamma=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 2\nfor epoch in range(num_epochs):\n    train_one_epoch(model_ft, optimizer, data_loader, device, epoch)\n    lr_scheduler.step()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor param in model_ft.parameters():\n    param.requires_grad = False\nmodel_ft.to(torch.device('cuda'))\n#assert model_ft.training == False\n\nmodel_ft.eval()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"6\" color=\"green\">Please UPVOTE if you find this kernel Helpful!</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model_ft.state_dict(), 'deeplabv3Resnet101.pth')\ntorch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References \n* [mask-rcnn with augmentation and multiple masks](https://www.kaggle.com/abhishek/mask-rcnn-with-augmentation-and-multiple-masks)\n\n* [SIIM-DeepLabV3](https://www.kaggle.com/soulmachine/siim-deeplabv3)\n\n* [PyTorch Starter (U-Net ResNet)](https://www.kaggle.com/ateplyuk/pytorch-starter-u-net-resnet)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}