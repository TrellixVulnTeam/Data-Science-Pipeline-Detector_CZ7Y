{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Imports","metadata":{}},{"cell_type":"code","source":"# some basic imports\nimport pandas as pd\nimport numpy as np\nimport os\nimport cv2\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:05:56.663141Z","iopub.execute_input":"2022-01-12T09:05:56.663431Z","iopub.status.idle":"2022-01-12T09:05:56.973318Z","shell.execute_reply.started":"2022-01-12T09:05:56.663381Z","shell.execute_reply":"2022-01-12T09:05:56.972612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imports for building the network\nimport tensorflow as tf\nfrom tensorflow import reduce_sum\nfrom tensorflow.keras.backend import pow\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Concatenate, Add, Flatten\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:05:56.976928Z","iopub.execute_input":"2022-01-12T09:05:56.977161Z","iopub.status.idle":"2022-01-12T09:05:59.327356Z","shell.execute_reply.started":"2022-01-12T09:05:56.977118Z","shell.execute_reply":"2022-01-12T09:05:59.32658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Configurations","metadata":{}},{"cell_type":"code","source":"# Kernel Configurations\nmake_submission = False # used to turn off lengthy model analysis so a submission version doesn't run into memory error\nload_pretrained_model = True # load a pre-trained model\nsave_model = True # save the model after training\ntrain_dir = '../input/severstal-steel-defect-detection/' # directory of training images\npretrained_model_path = '../input/severstal-pretrained-model/ResUNetSteel_z.h5' # path of pretrained model\nmodel_save_path = '../input/k/hzouari/resunet-a-baseline-on-tensorflow/ResUNetSteel_hamza.h5' # path of model to save\ntrain_image_dir = os.path.join(train_dir, 'train_images') # ","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:05:59.329376Z","iopub.execute_input":"2022-01-12T09:05:59.329686Z","iopub.status.idle":"2022-01-12T09:05:59.335242Z","shell.execute_reply.started":"2022-01-12T09:05:59.329638Z","shell.execute_reply":"2022-01-12T09:05:59.334425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# network configuration parameters\n# original image is 1600x256, so we will resize it\nimg_w = 800 # resized weidth\nimg_h = 256 # resized height\nbatch_size = 12\nepochs = 25\n# batch size for training unet\nk_size = 3 # kernel size 3x3\nval_size = .20 # split of training set between train and validation set\n# we will repeat the images with lower samples to make the training process more fair\nrepeat = False","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:05:59.336966Z","iopub.execute_input":"2022-01-12T09:05:59.337544Z","iopub.status.idle":"2022-01-12T09:05:59.344848Z","shell.execute_reply.started":"2022-01-12T09:05:59.337495Z","shell.execute_reply":"2022-01-12T09:05:59.344206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load & Transform train.csv","metadata":{}},{"cell_type":"code","source":"# load full data and label no mask as -1\ntrain_df = pd.read_csv('../input/trainnig/train.csv').fillna(-1)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:00.821331Z","iopub.execute_input":"2022-01-12T09:06:00.821663Z","iopub.status.idle":"2022-01-12T09:06:01.260808Z","shell.execute_reply.started":"2022-01-12T09:06:00.821609Z","shell.execute_reply":"2022-01-12T09:06:01.259994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image id and class id are two seperate entities and it makes it easier to split them up in two columns\n#train_df['ImageId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n#train_df['ClassId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[1])\n# lets create a dict with class id and encoded pixels and group all the defaults per image\ntrain_df['ClassId_EncodedPixels'] = train_df.apply(lambda row: (row['ClassId'], row['EncodedPixels']), axis = 1)\ngrouped_EncodedPixels = train_df.groupby('ImageId')['ClassId_EncodedPixels'].apply(list)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:02.619744Z","iopub.execute_input":"2022-01-12T09:06:02.620054Z","iopub.status.idle":"2022-01-12T09:06:03.589637Z","shell.execute_reply.started":"2022-01-12T09:06:02.620003Z","shell.execute_reply":"2022-01-12T09:06:03.588796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Utility Functions for RLE Encoding & Decoding","metadata":{}},{"cell_type":"code","source":"# from https://www.kaggle.com/robertkag/rle-to-mask-converter\ndef rle_to_mask(rle_string,height,width):\n    '''\n    convert RLE(run length encoding) string to numpy array\n\n    Parameters: \n    rleString (str): Description of arg1 \n    height (int): height of the mask\n    width (int): width of the mask \n\n    Returns: \n    numpy.array: numpy array of the mask\n    '''\n    rows, cols = height, width\n    if rle_string == -1:\n        return np.zeros((height, width))\n    else:\n        rleNumbers = [int(numstring) for numstring in rle_string.split(' ')]\n        rlePairs = np.array(rleNumbers).reshape(-1,2)\n        img = np.zeros(rows*cols,dtype=np.uint8)\n        for index,length in rlePairs:\n            index -= 1\n            img[index:index+length] = 255\n        img = img.reshape(cols,rows)\n        img = img.T\n        return img","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:03.660374Z","iopub.execute_input":"2022-01-12T09:06:03.660685Z","iopub.status.idle":"2022-01-12T09:06:03.668845Z","shell.execute_reply.started":"2022-01-12T09:06:03.660635Z","shell.execute_reply":"2022-01-12T09:06:03.668042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thanks to the authors of: https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\ndef mask_to_rle(mask):\n    '''\n    Convert a mask into RLE\n    \n    Parameters: \n    mask (numpy.array): binary mask of numpy array where 1 - mask, 0 - background\n\n    Returns: \n    sring: run length encoding \n    '''\n    pixels= mask.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:04.511894Z","iopub.execute_input":"2022-01-12T09:06:04.512192Z","iopub.status.idle":"2022-01-12T09:06:04.518975Z","shell.execute_reply.started":"2022-01-12T09:06:04.512142Z","shell.execute_reply":"2022-01-12T09:06:04.518051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Generator\n\nTo push the data to our model, we will create a custom data generator. A generator lets us load data progressively, instead of loading it all into memory at once. A custom generator allows us to also fit in more customization during the time of loading the data. As the model is being procssed in the GPU, we can use a custom generator to pre-process images via a generator. At this time, we can also take advantage multiple processors to parallelize our pre-processing.","metadata":{}},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, list_ids, labels, image_dir, batch_size=32,\n                 img_h=256, img_w=512, shuffle=True):\n        \n        self.list_ids = list_ids\n        self.labels = labels\n        self.image_dir = image_dir\n        self.batch_size = batch_size\n        self.img_h = img_h\n        self.img_w = img_w\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    \n    def __len__(self):\n        'denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_ids)) / self.batch_size)\n    \n    def __getitem__(self, index):\n        'generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # get list of IDs\n        list_ids_temp = [self.list_ids[k] for k in indexes]\n        # generate data\n        X, y = self.__data_generation(list_ids_temp)\n        # return data \n        return X, y\n    \n    def on_epoch_end(self):\n        'update ended after each epoch'\n        self.indexes = np.arange(len(self.list_ids))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n    def __data_generation(self, list_ids_temp):\n        'generate data containing batch_size samples'\n        X = np.empty((self.batch_size, self.img_h, self.img_w, 1))\n        y = np.empty((self.batch_size, self.img_h, self.img_w, 4))\n        \n        for idx, id in enumerate(list_ids_temp):\n            file_path =  os.path.join(self.image_dir, id)\n            image = cv2.imread(file_path, 0)\n            image_resized = cv2.resize(image, (self.img_w, self.img_h))\n            image_resized = np.array(image_resized, dtype=np.float64)\n            # standardization of the image\n            image_resized -= image_resized.mean()\n            image_resized /= image_resized.std()\n            \n            mask = np.empty((img_h, img_w, 4))\n            \n            for idm, image_class in enumerate(['1','2','3','4']):\n                rle = self.labels.get(id + '_' + image_class)\n                # if there is no mask create empty mask\n                if rle is None:\n                    class_mask = np.zeros((1600, 256))\n                else:\n                    class_mask = rle_to_mask(rle, width=1600, height=256)\n             \n                class_mask_resized = cv2.resize(class_mask, (self.img_w, self.img_h))\n                mask[...,idm] = class_mask_resized\n            \n            X[idx,] = np.expand_dims(image_resized, axis=2)\n            y[idx,] = mask\n        \n        # normalize Y\n        y = (y > 0).astype(int)\n            \n        return X, y","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:06.810905Z","iopub.execute_input":"2022-01-12T09:06:06.811208Z","iopub.status.idle":"2022-01-12T09:06:06.830648Z","shell.execute_reply.started":"2022-01-12T09:06:06.811154Z","shell.execute_reply":"2022-01-12T09:06:06.829859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a dict of all the masks\nmasks = {}\nfor index, row in train_df[train_df['EncodedPixels']!=-1].iterrows():\n    masks[row['ImageId_ClassId']] = row['EncodedPixels']","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:08.699878Z","iopub.execute_input":"2022-01-12T09:06:08.700179Z","iopub.status.idle":"2022-01-12T09:06:09.259089Z","shell.execute_reply.started":"2022-01-12T09:06:08.700128Z","shell.execute_reply":"2022-01-12T09:06:09.25834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# repeat low represented samples more frequently to balance our dataset\nif repeat:\n    class_1_img_id = train_df[(train_df['EncodedPixels']!=-1) & (train_df['ClassId']=='1')]['ImageId'].values\n    class_1_img_id = np.repeat(class_1_img_id, class_1_repeat)\n    class_2_img_id = train_df[(train_df['EncodedPixels']!=-1) & (train_df['ClassId']=='2')]['ImageId'].values\n    class_2_img_id = np.repeat(class_2_img_id, class_2_repeat)\n    class_3_img_id = train_df[(train_df['EncodedPixels']!=-1) & (train_df['ClassId']=='3')]['ImageId'].values\n    class_3_img_id = np.repeat(class_3_img_id, class_3_repeat)\n    class_4_img_id = train_df[(train_df['EncodedPixels']!=-1) & (train_df['ClassId']=='4')]['ImageId'].values\n    class_4_img_id = np.repeat(class_4_img_id, class_4_repeat)\n    train_image_ids = np.concatenate([class_1_img_id, class_2_img_id, class_3_img_id, class_4_img_id])\nelse:\n    # split the training data into train and validation set (stratified)\n    train_image_ids = train_df['ImageId'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:10.881684Z","iopub.execute_input":"2022-01-12T09:06:10.881981Z","iopub.status.idle":"2022-01-12T09:06:10.892878Z","shell.execute_reply.started":"2022-01-12T09:06:10.881932Z","shell.execute_reply":"2022-01-12T09:06:10.892067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val = train_test_split(train_image_ids, test_size=val_size, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:12.211875Z","iopub.execute_input":"2022-01-12T09:06:12.212186Z","iopub.status.idle":"2022-01-12T09:06:12.218951Z","shell.execute_reply.started":"2022-01-12T09:06:12.212132Z","shell.execute_reply":"2022-01-12T09:06:12.217978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'img_h': img_h,\n          'img_w': img_w,\n          'image_dir': train_image_dir,\n          'batch_size': batch_size,\n          'shuffle': True}\n\n# Get Generators\ntraining_generator = DataGenerator(X_train, masks, **params)\nvalidation_generator = DataGenerator(X_val, masks, **params)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:12.579828Z","iopub.execute_input":"2022-01-12T09:06:12.580139Z","iopub.status.idle":"2022-01-12T09:06:12.585943Z","shell.execute_reply.started":"2022-01-12T09:06:12.580086Z","shell.execute_reply":"2022-01-12T09:06:12.58514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check out the shapes\nx, y = training_generator.__getitem__(0)\nprint(x.shape, y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:13.699802Z","iopub.execute_input":"2022-01-12T09:06:13.700116Z","iopub.status.idle":"2022-01-12T09:06:14.20753Z","shell.execute_reply.started":"2022-01-12T09:06:13.700065Z","shell.execute_reply":"2022-01-12T09:06:14.206657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize steel image with four classes of faults in seperate columns\ndef viz_steel_img_mask(img, masks):\n    img = cv2.cvtColor(img.astype('float32'), cv2.COLOR_BGR2RGB)\n    fig, ax = plt.subplots(nrows=1, ncols=4, sharey=True, figsize=(20,10))\n    cmaps = [\"Reds\", \"Blues\", \"Greens\", \"Purples\"]\n    for idx, mask in enumerate(masks):\n        ax[idx].imshow(img)\n        ax[idx].imshow(mask, alpha=0.3, cmap=cmaps[idx])","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:15.31267Z","iopub.execute_input":"2022-01-12T09:06:15.312965Z","iopub.status.idle":"2022-01-12T09:06:15.319693Z","shell.execute_reply.started":"2022-01-12T09:06:15.312915Z","shell.execute_reply":"2022-01-12T09:06:15.318661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets visualize some images with their faults to make sure our data generator is working like it should\nfor ix in range(0,batch_size):\n    if y[ix].sum() > 0:\n        img = x[ix]\n        masks_temp = [y[ix][...,i] for i in range(0,4)]\n        viz_steel_img_mask(img, masks_temp)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:15.720628Z","iopub.execute_input":"2022-01-12T09:06:15.720951Z","iopub.status.idle":"2022-01-12T09:06:22.9614Z","shell.execute_reply.started":"2022-01-12T09:06:15.720884Z","shell.execute_reply":"2022-01-12T09:06:22.960357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Resunet\n\nIn this section we will define the building blocks for our network and train our network.","metadata":{}},{"cell_type":"code","source":"def bn_act(x, act=True):\n    'batch normalization layer with an optinal activation layer'\n    x = tf.keras.layers.BatchNormalization()(x)\n    if act == True:\n        x = tf.keras.layers.Activation('relu')(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:22.963312Z","iopub.execute_input":"2022-01-12T09:06:22.963761Z","iopub.status.idle":"2022-01-12T09:06:22.969641Z","shell.execute_reply.started":"2022-01-12T09:06:22.96358Z","shell.execute_reply":"2022-01-12T09:06:22.968835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv_block(x, filters, kernel_size=3, padding='same', strides=1):\n    'convolutional layer which always uses the batch normalization layer'\n    conv = bn_act(x)\n    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n    return conv","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:22.9715Z","iopub.execute_input":"2022-01-12T09:06:22.971938Z","iopub.status.idle":"2022-01-12T09:06:22.980355Z","shell.execute_reply.started":"2022-01-12T09:06:22.971753Z","shell.execute_reply":"2022-01-12T09:06:22.979283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stem(x, filters, kernel_size=3, padding='same', strides=1):\n    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n    conv = conv_block(conv, filters, kernel_size, padding, strides)\n    shortcut = Conv2D(filters, kernel_size=1, padding=padding, strides=strides)(x)\n    shortcut = bn_act(shortcut, act=False)\n    output = Add()([conv, shortcut])\n    return output","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:22.982122Z","iopub.execute_input":"2022-01-12T09:06:22.982895Z","iopub.status.idle":"2022-01-12T09:06:22.991963Z","shell.execute_reply.started":"2022-01-12T09:06:22.982593Z","shell.execute_reply":"2022-01-12T09:06:22.991171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def residual_block(x, filters, kernel_size=3, padding='same', strides=1):\n    res = conv_block(x, filters, k_size, padding, strides)\n    res = conv_block(res, filters, k_size, padding, 1)\n    shortcut = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n    shortcut = bn_act(shortcut, act=False)\n    output = Add()([shortcut, res])\n    return output","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:22.995394Z","iopub.execute_input":"2022-01-12T09:06:22.995652Z","iopub.status.idle":"2022-01-12T09:06:23.002666Z","shell.execute_reply.started":"2022-01-12T09:06:22.995609Z","shell.execute_reply":"2022-01-12T09:06:23.001836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def upsample_concat_block(x, xskip):\n    u = UpSampling2D((2,2))(x)\n    c = Concatenate()([u, xskip])\n    return c","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:23.004236Z","iopub.execute_input":"2022-01-12T09:06:23.004762Z","iopub.status.idle":"2022-01-12T09:06:23.01378Z","shell.execute_reply.started":"2022-01-12T09:06:23.004709Z","shell.execute_reply":"2022-01-12T09:06:23.013046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ResUNet(img_h, img_w):\n    f = [16, 32, 64, 128, 256]\n    inputs = Input((img_h, img_w, 1))\n    \n    ## Encoder\n    e0 = inputs\n    e1 = stem(e0, f[0])\n    e2 = residual_block(e1, f[1], strides=2)\n    e3 = residual_block(e2, f[2], strides=2)\n    e4 = residual_block(e3, f[3], strides=2)\n    e5 = residual_block(e4, f[4], strides=2)\n    \n    ## Bridge\n    b0 = conv_block(e5, f[4], strides=1)\n    b1 = conv_block(b0, f[4], strides=1)\n    \n    ## Decoder\n    u1 = upsample_concat_block(b1, e4)\n    d1 = residual_block(u1, f[4])\n    \n    u2 = upsample_concat_block(d1, e3)\n    d2 = residual_block(u2, f[3])\n    \n    u3 = upsample_concat_block(d2, e2)\n    d3 = residual_block(u3, f[2])\n    \n    u4 = upsample_concat_block(d3, e1)\n    d4 = residual_block(u4, f[1])\n    \n    outputs = tf.keras.layers.Conv2D(4, (1, 1), padding=\"same\", activation=\"sigmoid\")(d4)\n    model = tf.keras.models.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:23.016699Z","iopub.execute_input":"2022-01-12T09:06:23.017536Z","iopub.status.idle":"2022-01-12T09:06:23.02917Z","shell.execute_reply.started":"2022-01-12T09:06:23.016875Z","shell.execute_reply":"2022-01-12T09:06:23.028535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss Functions\n\nAs our classes are highly imbalanced and we are stacking four output layers at once, it is even more important to get the loss function right. Here I will aggregate important loss functions so you can reuse and experiment with them along with me.","metadata":{}},{"cell_type":"code","source":"# Dice similarity coefficient loss, brought to you by: https://github.com/nabsabraham/focal-tversky-unet\ndef dsc(y_true, y_pred):\n    smooth = 1.\n    y_true_f = Flatten()(y_true)\n    y_pred_f = Flatten()(y_pred)\n    intersection = reduce_sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (reduce_sum(y_true_f) + reduce_sum(y_pred_f) + smooth)\n    return score\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dsc(y_true, y_pred)\n    return loss\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:23.660603Z","iopub.execute_input":"2022-01-12T09:06:23.6609Z","iopub.status.idle":"2022-01-12T09:06:23.668344Z","shell.execute_reply.started":"2022-01-12T09:06:23.660845Z","shell.execute_reply":"2022-01-12T09:06:23.667286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Focal Tversky loss, brought to you by:  https://github.com/nabsabraham/focal-tversky-unet\ndef tversky(y_true, y_pred, smooth=1e-6):\n    y_true_pos = tf.keras.layers.Flatten()(y_true)\n    y_pred_pos = tf.keras.layers.Flatten()(y_pred)\n    y_true_pos = tf.cast(y_true_pos, tf.float32)\n    y_pred_pos = tf.cast(y_pred_pos, tf.float32)\n    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos) \n    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n    false_pos = tf.reduce_sum((1-y_true_pos)*y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n\ndef tversky_loss(y_true, y_pred):\n    return 1 - tversky(y_true,y_pred)\n\ndef focal_tversky_loss(y_true,y_pred):\n    pt_1 = tversky(y_true, y_pred)\n    gamma = 0.75\n    return tf.keras.backend.pow((1-pt_1), gamma)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:24.259945Z","iopub.execute_input":"2022-01-12T09:06:24.260266Z","iopub.status.idle":"2022-01-12T09:06:24.272226Z","shell.execute_reply.started":"2022-01-12T09:06:24.26021Z","shell.execute_reply":"2022-01-12T09:06:24.271571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compile & Fit The Model","metadata":{}},{"cell_type":"code","source":"model = ResUNet(img_h=img_h, img_w=img_w)\nadam = tf.keras.optimizers.Adam(lr = 0.05, epsilon = 0.1)\nmodel.compile(optimizer=adam, loss=focal_tversky_loss, metrics=[tversky])","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:27.780326Z","iopub.execute_input":"2022-01-12T09:06:27.780707Z","iopub.status.idle":"2022-01-12T09:06:30.084108Z","shell.execute_reply.started":"2022-01-12T09:06:27.78066Z","shell.execute_reply":"2022-01-12T09:06:30.083305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pretrained_model_path\nif load_pretrained_model:\n    try:\n        model.load_weights(\"../input/k/hzouari/resunet-a-baseline-on-tensorflow/ResUNetSteel_hamza.h5\")\n        print('pre-trained model loaded!')\n    except OSError:\n        print('You need to run the model and load the trained model')","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:30.086927Z","iopub.execute_input":"2022-01-12T09:06:30.087393Z","iopub.status.idle":"2022-01-12T09:06:34.572646Z","shell.execute_reply.started":"2022-01-12T09:06:30.087342Z","shell.execute_reply":"2022-01-12T09:06:34.571808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=epochs, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T20:05:52.964176Z","iopub.execute_input":"2022-01-09T20:05:52.964777Z","iopub.status.idle":"2022-01-09T22:42:21.009555Z","shell.execute_reply.started":"2022-01-09T20:05:52.964712Z","shell.execute_reply":"2022-01-09T22:42:21.005659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if save_model: \n    model.save(\"./ResUNetSteel_hamza.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-01-09T22:43:24.967838Z","iopub.execute_input":"2022-01-09T22:43:24.968242Z","iopub.status.idle":"2022-01-09T22:43:31.887779Z","shell.execute_reply.started":"2022-01-09T22:43:24.968181Z","shell.execute_reply":"2022-01-09T22:43:31.886857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Insights\n\nIn this section we take a look at the performance of our model and visually inspect how our predictions look like.","metadata":{}},{"cell_type":"code","source":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nplt.plot(history.history['tversky'])\nplt.plot(history.history['val_tversky'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\n\n# summarize history for loss\nplt.subplot(1,2,2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-09T22:43:50.272993Z","iopub.execute_input":"2022-01-09T22:43:50.273487Z","iopub.status.idle":"2022-01-09T22:43:50.803419Z","shell.execute_reply.started":"2022-01-09T22:43:50.273398Z","shell.execute_reply":"2022-01-09T22:43:50.802338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a function to plot image with mask and image with predicted mask next to each other\ndef viz_single_fault(img ,mask, pred, image_class):\n    \n    fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(15,5))\n    \n    cmaps = [\"Reds\", \"Blues\", \"Greens\", \"Purples\"]\n    \n    ax[0].imshow(img)\n    ax[0].imshow(mask, alpha=0.3, cmap=cmaps[image_class-1])\n    ax[0].set_title('Mask - Defect Class %s' % image_class)\n    \n    ax[1].imshow(img)\n    ax[1].imshow(pred, alpha=0.3, cmap=cmaps[image_class-1])\n    ax[1].set_title('Predicted Mask - Defect Class %s' % image_class)\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:42.361235Z","iopub.execute_input":"2022-01-12T09:06:42.361586Z","iopub.status.idle":"2022-01-12T09:06:42.369972Z","shell.execute_reply.started":"2022-01-12T09:06:42.361522Z","shell.execute_reply":"2022-01-12T09:06:42.369002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To get a better understanding of our model we will calculate the IoU score. If you are unfamiliar with the concept of IoU, you can read more of it here: http://ronny.rest/tutorials/module/localization_001/iou/.","metadata":{}},{"cell_type":"code","source":"# https://www.jeremyjordan.me/evaluating-image-segmentation-models/\ndef calculate_iou(target, prediction):\n    intersection = np.logical_and(target, prediction)\n    union = np.logical_or(target, prediction)\n    if np.sum(union) == 0:\n        iou_score = 0\n    else:\n        iou_score = np.sum(intersection) / np.sum(union)\n    return iou_score","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:44.464287Z","iopub.execute_input":"2022-01-12T09:06:44.464636Z","iopub.status.idle":"2022-01-12T09:06:44.471435Z","shell.execute_reply.started":"2022-01-12T09:06:44.464576Z","shell.execute_reply":"2022-01-12T09:06:44.470632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if make_submission == False:\n    # lets loop over the predictions and print 5 of each image cases with defects\n    count = 0\n    # a list to keep count of the number of plots made per image class\n    class_viz_count = [0,0,0,0]\n    # to keep the total iou score per image class\n    class_iou_score = [0, 0, 0, 0]\n    # to keep sum of mask pixels per image class\n    class_mask_sum = [0, 0, 0, 0]\n    # to keep sum of predicted mask pixels per image class\n    class_pred_sum = [0, 0, 0, 0]\n\n    # loop over to all the batches in one epoch \n    for i in range(0, validation_generator.__len__()):\n        # get a batch of image, true mask, and predicted mask\n        x, y = validation_generator.__getitem__(i)\n        predictions = model.predict(x)\n\n        # loop through x to get all the images in the batch\n        for idx, val in enumerate(x):\n            # we are only interested if there is a fault. if we are dropping images with no faults before this will become redundant\n            if y[idx].sum() > 0: \n                # get an image and convert to make it matplotlib.pyplot friendly\n                img = x[idx]\n                img = cv2.cvtColor(img.astype('float32'), cv2.COLOR_BGR2RGB)\n                # loop over the four ourput layers to create a list of all the masks for this image\n                masks_temp = [y[idx][...,i] for i in range(0,4)]\n                # loop over the four output layers to create a list of all the predictions for this image\n                preds_temp = [predictions[idx][...,i] for i in range(0,4)]\n                # turn to binary (prediction) mask \n                preds_temp = [p > .5 for p in preds_temp]\n\n                for i, (mask, pred) in enumerate(zip(masks_temp, preds_temp)):\n                    image_class = i + 1\n                    class_iou_score[i] += calculate_iou(mask, pred)\n                    class_mask_sum[i] += mask.sum()\n                    class_pred_sum[i] += pred.sum()\n                    if mask.sum() > 0 and class_viz_count[i] < 5:\n                        viz_single_fault(img, mask, pred, image_class)\n                        class_viz_count[i] += 1\n                        ","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:06:45.422228Z","iopub.execute_input":"2022-01-12T09:06:45.422546Z","iopub.status.idle":"2022-01-12T09:08:18.607678Z","shell.execute_reply.started":"2022-01-12T09:06:45.422471Z","shell.execute_reply":"2022-01-12T09:08:18.606923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize the IoU, Sum of pixel for mask given, and sum of pixel for the prediction to better understand the performance of our model.","metadata":{}},{"cell_type":"code","source":" if make_submission == False:\n    class_ids = [1,2,3,4]\n    plt.figure(figsize=(20,5))\n    plt.subplot(1,3,1)\n    y_pos = np.arange(len(class_ids))\n    plt.bar(y_pos, class_iou_score)\n    plt.xticks(y_pos, class_ids)\n    plt.title('IoU score per class')\n    plt.ylabel('IoU Sum')\n    plt.xlabel('class id')\n    plt.subplot(1,3,2)\n    plt.bar(y_pos, class_mask_sum)\n    plt.xticks(y_pos, class_ids)\n    plt.title('labeled mask pixel sum per class')\n    plt.ylabel('pixel sum')\n    plt.xlabel('class id')\n    plt.ticklabel_format(axis='y',style='sci',scilimits=(1,4))\n    plt.subplot(1,3,3)\n    plt.bar(y_pos, class_pred_sum)\n    plt.xticks(y_pos, class_ids)\n    plt.title('predicted mask pixel sum per class')\n    plt.ylabel(' pixel sum')\n    plt.xlabel('class id')\n    plt.ticklabel_format(axis='y',style='sci',scilimits=(1,4))\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-09T22:45:31.622973Z","iopub.execute_input":"2022-01-09T22:45:31.62333Z","iopub.status.idle":"2022-01-09T22:45:32.078296Z","shell.execute_reply.started":"2022-01-09T22:45:31.623268Z","shell.execute_reply":"2022-01-09T22:45:32.077183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Its obviously and unfortunately clear that we don't see enough sample of the other classes so we don't make a any prediction on them. I do wish we made some prediciton on class 4 but oh well, life building AI isn't easy =( ","metadata":{}},{"cell_type":"markdown","source":"## Making Predictions & Submission File\n\nNow that our model is trained lets make a submission and file it in!","metadata":{}},{"cell_type":"code","source":"# return tensor in the right shape for prediction \ndef get_test_tensor(img_dir, img_h, img_w, channels=1):\n\n    X = np.empty((1, img_h, img_w, channels))\n    # Store sample\n    image = cv2.imread(img_dir, 0)\n    image_resized = cv2.resize(image, (img_w, img_h))\n    image_resized = np.array(image_resized, dtype=np.float64)\n    # normalize image\n    image_resized -= image_resized.mean()\n    image_resized /= image_resized.std()\n    \n    X[0,] = np.expand_dims(image_resized, axis=2)\n\n    return X","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:08:18.609506Z","iopub.execute_input":"2022-01-12T09:08:18.609825Z","iopub.status.idle":"2022-01-12T09:08:18.618081Z","shell.execute_reply.started":"2022-01-12T09:08:18.609748Z","shell.execute_reply":"2022-01-12T09:08:18.616932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this is an awesome little function to remove small spots in our predictions\n\nfrom skimage import morphology\n\ndef remove_small_regions(img, size):\n    \"\"\"Morphologically removes small (less than size) connected regions of 0s or 1s.\"\"\"\n    img = morphology.remove_small_objects(img, size)\n    img = morphology.remove_small_holes(img, size)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:08:18.619601Z","iopub.execute_input":"2022-01-12T09:08:18.620161Z","iopub.status.idle":"2022-01-12T09:08:20.088183Z","shell.execute_reply.started":"2022-01-12T09:08:18.619975Z","shell.execute_reply":"2022-01-12T09:08:20.087458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\n# get all files using glob\ntest_files = ['../input/test-now/Rutilea_640.jpg']\n#[f for f in glob.glob('../input/severstal-steel-defect-detection/test_images/' + \"*.jpg\", recursive=True)]","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:08:20.103141Z","iopub.execute_input":"2022-01-12T09:08:20.103681Z","iopub.status.idle":"2022-01-12T09:08:20.11016Z","shell.execute_reply.started":"2022-01-12T09:08:20.10363Z","shell.execute_reply":"2022-01-12T09:08:20.109352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = []\n\n# a function to apply all the processing steps necessery to each of the individual masks\ndef process_pred_mask(pred_mask):\n    \n    pred_mask = cv2.resize(pred_mask.astype('float32'),(327, 684))\n    pred_mask = (pred_mask > .5).astype(int)\n    #pred_mask = remove_small_regions(pred_mask, 0.02 * np.prod(512)) * 255\n    pred_mask = mask_to_rle(pred_mask)\n    \n    return pred_mask\n\n# loop over all the test images\nfor f in test_files:\n    # get test tensor, output is in shape: (1, 256, 512, 3)\n    test = get_test_tensor(f, img_h, img_w) \n    # get prediction, output is in shape: (1, 256, 512, 4)\n    pred_masks = model.predict(test) \n    # get a list of masks with shape: 256, 512\n    pred_masks = [pred_masks[0][...,i] for i in range(0,4)]\n    # apply all the processing steps to each of the mask\n    pred_masks = [process_pred_mask(pred_mask) for pred_mask in pred_masks]\n    # get our image id\n    id = f.split('/')[-1]\n    # create ImageId_ClassId and get the EncodedPixels for the class ID, and append to our submissions list\n    [submission.append((id+'_%s' % (k+1), pred_mask)) for k, pred_mask in enumerate(pred_masks)]","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:08:20.112042Z","iopub.execute_input":"2022-01-12T09:08:20.112557Z","iopub.status.idle":"2022-01-12T09:08:20.563192Z","shell.execute_reply.started":"2022-01-12T09:08:20.112292Z","shell.execute_reply":"2022-01-12T09:08:20.562279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert to a csv\nsubmission_df = pd.DataFrame(submission, columns=['ImageId_ClassId', 'EncodedPixels'])\n# check out some predictions and see if RLE looks ok\nsubmission_df[ submission_df['EncodedPixels'] != ''].head()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:08:20.564706Z","iopub.execute_input":"2022-01-12T09:08:20.565011Z","iopub.status.idle":"2022-01-12T09:08:20.580024Z","shell.execute_reply.started":"2022-01-12T09:08:20.564962Z","shell.execute_reply":"2022-01-12T09:08:20.579212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a look at our submission \nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:08:20.58163Z","iopub.execute_input":"2022-01-12T09:08:20.582277Z","iopub.status.idle":"2022-01-12T09:08:20.591953Z","shell.execute_reply.started":"2022-01-12T09:08:20.581904Z","shell.execute_reply":"2022-01-12T09:08:20.591014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# write it out\nsubmission_df.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:08:20.593395Z","iopub.execute_input":"2022-01-12T09:08:20.593879Z","iopub.status.idle":"2022-01-12T09:08:20.825756Z","shell.execute_reply.started":"2022-01-12T09:08:20.593703Z","shell.execute_reply":"2022-01-12T09:08:20.825081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pm = submission_df.iloc[2,1]","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:08:20.828335Z","iopub.execute_input":"2022-01-12T09:08:20.828921Z","iopub.status.idle":"2022-01-12T09:08:20.83331Z","shell.execute_reply.started":"2022-01-12T09:08:20.828867Z","shell.execute_reply":"2022-01-12T09:08:20.832369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimg = test\n\nimg = cv2.cvtColor(img.astype('float32'), cv2.COLOR_BGR2RGB)\n\npm_treated = rle_to_mask(pm,327,684)\n\nfig, ax= plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(20,5))\n\nframe = cv2.imread('../input/rectangle/3.jpg')\n\ncmaps = [\"Reds\", \"Blues\", \"Greens\", \"Purples\"]\n\nax[0].imshow(frame)\nax[0].imshow(pm_treated, alpha=0.3, cmap=cmaps[2])\nax[0].set_title('Mask - Defect Class %s' % 2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T09:08:20.834587Z","iopub.execute_input":"2022-01-12T09:08:20.835013Z","iopub.status.idle":"2022-01-12T09:08:21.209945Z","shell.execute_reply.started":"2022-01-12T09:08:20.83482Z","shell.execute_reply":"2022-01-12T09:08:21.205752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}