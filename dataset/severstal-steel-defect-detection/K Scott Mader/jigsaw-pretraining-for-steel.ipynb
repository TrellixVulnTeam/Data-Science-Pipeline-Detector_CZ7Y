{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overview\nExperiment with the ideas from [Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles](https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5) by Mehdi Noroozi and Paolo Favaro. \n\nThe core idea is to take an image, break it up into square tiles, scramble the tiles and have a model learn to unscramble them $\\rightarrow$ \"solving a jigsaw puzzle\". The features the model learns on the individual tiles should then be useful for describing the important information in the image.\n\n## Overview\n- Create and test the jigsaw puzzle code (and the reconstruction)\n- Setup scrambling\n- Build a training and validation dataset\n- Train the model to unscramble\n- Evaluate performance and visualize what the model has done\n- Use the features to try and classify digits"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (8, 8)\nplt.rcParams[\"figure.dpi\"] = 125\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})\nplt.rcParams['image.cmap'] = 'gray' # grayscale looks better\nfrom itertools import cycle\nprop_cycle = plt.rcParams['axes.prop_cycle']\ncolors = prop_cycle.by_key()['color']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# tests help notebooks stay managable\nimport doctest\nimport copy\nimport functools\n\ndef autotest(func):\n    globs = copy.copy(globals())\n    globs.update({func.__name__: func})\n    doctest.run_docstring_examples(\n        func, globs, verbose=True, name=func.__name__)\n    return func","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport os\nfrom skimage.io import imread\nfrom skimage.util import montage\nfrom skimage.color import label2rgb\nfrom itertools import product\nfrom tqdm import tqdm_notebook\nfrom IPython.display import clear_output\nimport torch\nimport tensorflow as tf\nimport random\nseed = 2019\nrandom.seed(seed)\nos.environ[\"PYTHONHASHSEED\"] = str(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntf.random.set_random_seed(seed)\nclear_output()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data_dir = Path('..') / 'input' / 'severstal-steel-defect-detection'\n# Load the data\ntrain_df = pd.read_csv(data_dir / \"train.csv\")\ntrain_df['ImageId'] = train_df['ImageId_ClassId'].map(lambda x: x.split('_')[0])\ntrain_df['ClassId'] = train_df['ImageId_ClassId'].map(lambda x: x.split('_')[-1])\ntrain_df['image_path'] = train_df['ImageId'].map(lambda x: str(data_dir / 'train_images' / x))\ntrain_df.drop('ImageId_ClassId', axis=1, inplace=True)\nflat_train_df = train_df.pivot_table(index=['ImageId', 'image_path'], columns='ClassId', values='EncodedPixels', aggfunc='first')\nflat_train_df['defects_count'] = flat_train_df.applymap(lambda x: len(x) if isinstance(x, str) else 0).sum(axis=1)\nflat_train_df = flat_train_df.reset_index().sort_values('defects_count', ascending=False)\nprint(flat_train_df.shape)\nflat_train_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_mask(c_row, mask_channel):\n    '''Given a row index, return image_id and mask (256, 1600, 1)'''\n    # 4:class 1～4 (ch:0～3)\n    mask = np.zeros(256 * 1600, dtype=np.bool)\n    if c_row[mask_channel] is not np.nan:\n        label = c_row[mask_channel].split(\" \")\n        positions = map(int, label[0::2])\n        length = map(int, label[1::2])\n        \n        for pos, le in zip(positions, length):\n            mask[pos:(pos + le)] = 1\n    return mask.reshape(256, 1600, order='F')\ndef idx_mask(in_mask):\n    return (1+np.argmax(in_mask, -1))*np.max(in_mask, -1)\ndef full_mask(c_row):\n    return np.stack([make_mask(c_row, '{}'.format(i)) for i in range(1, 5)], -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_row = flat_train_df.sample(1).iloc[0]\nrand_img = imread(rand_row['image_path'], as_gray=True)\nrand_mask = full_mask(rand_row)\nplt.imshow(label2rgb(label=idx_mask(rand_mask), image=rand_img, bg_label=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# calculate for all rows\nif False:\n    flat_train_df['mask_image'] = flat_train_df.apply(full_mask, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Jigsaw Code\nHere we write the jigsaw code to break the image up into a bunch of little pieces (```cut_jigsaw```) and reassemble the pieces back into an image (```jigsaw_to_image```). The methods right now are very simple, but are fairly easy to read and include a few basic test-cases. The code is implemented in a very intuitive, but inefficient manner. This should certainly be optimized before use on real problems."},{"metadata":{"trusted":true},"cell_type":"code","source":"@autotest\ndef cut_jigsaw(\n    in_image, # type: np.ndarray\n    x_wid, # type: int\n    y_wid,# type: int\n    gap=False,\n    jitter=False,\n    jitter_dim=None, # type: Optional[int]\n):\n    # type: (...) -> List[np.ndarray]\n    \"\"\"Cuts the image into little pieces\n    :param in_image: the image to cut-apart\n    :param x_wid: the size of the piece in x\n    :param y_wid: the size of the piece in y\n    :param gap: if there is a gap between tiles\n    :param jitter: if the positions should be moved around\n    :param jitter_dim: amount to jitter (default is x_wid or y_wid/2)\n    :return : a 4D array with tiles x x_wid x y_wid * d\n    Examples\n    >>> test_image = np.arange(20).reshape((4, 5)).astype(int)\n    >>> test_image\n    array([[ 0,  1,  2,  3,  4],\n           [ 5,  6,  7,  8,  9],\n           [10, 11, 12, 13, 14],\n           [15, 16, 17, 18, 19]])\n    >>> cut_jigsaw(test_image, 2, 2, False, False)\n    array([[[ 0,  1],\n            [ 5,  6]],\n    <BLANKLINE>\n           [[ 2,  3],\n            [ 7,  8]],\n    <BLANKLINE>\n           [[10, 11],\n            [15, 16]],\n    <BLANKLINE>\n           [[12, 13],\n            [17, 18]]])\n    >>> cut_jigsaw(test_image, 2, 2, True, False)\n    array([[[ 0,  1],\n            [ 5,  6]],\n    <BLANKLINE>\n           [[ 3,  4],\n            [ 8,  9]],\n    <BLANKLINE>\n           [[10, 11],\n            [15, 16]],\n    <BLANKLINE>\n           [[13, 14],\n            [18, 19]]])\n    >>> np.random.seed(0)\n    >>> cut_jigsaw(test_image, 2, 2, True, True, 1)\n    array([[[ 1,  2],\n            [ 6,  7]],\n    <BLANKLINE>\n           [[ 7,  8],\n            [12, 13]],\n    <BLANKLINE>\n           [[ 5,  6],\n            [10, 11]],\n    <BLANKLINE>\n           [[ 7,  8],\n            [12, 13]]])\n    \"\"\"\n    if len(in_image.shape)==2:\n        in_image = np.expand_dims(in_image, -1)\n        expand = True\n    else:\n        expand = False\n    x_size, y_size, d_size = in_image.shape\n    out_tiles = []\n    x_chunks = x_size//x_wid\n    y_chunks = y_size//y_wid\n    out_tiles = np.zeros((x_chunks*y_chunks, x_wid, y_wid, d_size), dtype=in_image.dtype)\n    if gap:\n        # we calculate the maximum gap and \n        x_gap = x_size-x_chunks*x_wid\n        y_gap = y_size-y_chunks*y_wid\n    else:\n        x_gap, y_gap = 0, 0\n    x_jitter = x_wid//2 if jitter_dim is None else jitter_dim\n    y_jitter = y_wid//2 if jitter_dim is None else jitter_dim\n    for idx, (i, j) in enumerate(product(range(x_chunks), range(y_chunks))):\n        x_start = i*x_wid+min(x_gap, i)\n        y_start = j*y_wid+min(y_gap, j)\n        if jitter:\n            x_range = max(x_start-x_jitter, 0), min(x_start+x_jitter+1, x_size-x_wid)\n            y_range = max(y_start-y_jitter, 0), min(y_start+y_jitter+1, y_size-y_wid)\n            \n            x_start = np.random.choice(range(*x_range)) if x_range[1]>x_range[0] else x_start\n            y_start = np.random.choice(range(*y_range)) if y_range[1]>y_range[0] else y_start\n            \n        out_tiles[idx, :, :, :] = in_image[x_start:x_start+x_wid, y_start:y_start+y_wid, :]\n    \n    return out_tiles[:, :, :, 0] if expand else out_tiles\n                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@autotest\ndef jigsaw_to_image(\n    in_tiles, # type: np.ndarray\n    out_x, # type: int\n    out_y, # type: int\n    gap=False\n):\n    # type: (...) -> np.ndarray\n    \"\"\"Reassembles little pieces into an image\n    :param in_tiles: the tiles to reassemble\n    :param out_x: the size of the image in x (default is calculated automatically)\n    :param out_y: the size of the image in y\n    :param gap: if there is a gap between tiles\n    :return : an image from the tiles\n    Examples\n    >>> test_image = np.arange(20).reshape((4, 5)).astype(int)\n    >>> test_image\n    array([[ 0,  1,  2,  3,  4],\n           [ 5,  6,  7,  8,  9],\n           [10, 11, 12, 13, 14],\n           [15, 16, 17, 18, 19]])\n    >>> js_pieces = cut_jigsaw(test_image, 2, 2, False, False)\n    >>> jigsaw_to_image(js_pieces, 4, 5)\n    array([[ 0,  1,  2,  3,  0],\n           [ 5,  6,  7,  8,  0],\n           [10, 11, 12, 13,  0],\n           [15, 16, 17, 18,  0]])\n    >>> js_gap_pieces = cut_jigsaw(test_image, 2, 2, True, False)\n    >>> jigsaw_to_image(js_gap_pieces, 4, 5, True)\n    array([[ 0,  1,  0,  3,  4],\n           [ 5,  6,  0,  8,  9],\n           [10, 11,  0, 13, 14],\n           [15, 16,  0, 18, 19]])\n    >>> np.random.seed(0)\n    >>> js_gap_pieces = cut_jigsaw(test_image, 2, 2, False, True)\n    >>> jigsaw_to_image(js_gap_pieces, 4, 5, False)\n    array([[ 1,  2,  6,  7,  0],\n           [ 6,  7, 11, 12,  0],\n           [ 6,  7,  7,  8,  0],\n           [11, 12, 12, 13,  0]])\n    \"\"\"\n    if len(in_tiles.shape)==3:\n        in_tiles = np.expand_dims(in_tiles, -1)\n        expand = True\n    else:\n        expand = False\n    tile_count, x_wid, y_wid, d_size = in_tiles.shape\n    x_chunks = out_x//x_wid\n    y_chunks = out_y//y_wid\n    out_image = np.zeros((out_x, out_y, d_size), dtype=in_tiles.dtype)\n    \n    if gap:\n        x_gap = out_x-x_chunks*x_wid\n        y_gap = out_y-y_chunks*y_wid\n    else:\n        x_gap, y_gap = 0, 0\n        \n    for idx, (i, j) in enumerate(product(range(x_chunks), range(y_chunks))):\n        x_start = i*x_wid+min(x_gap, i)\n        y_start = j*y_wid+min(y_gap, j)\n        out_image[x_start:x_start+x_wid, y_start:y_start+y_wid] = in_tiles[idx, :, :]\n    \n    return out_image[:, :, 0] if expand else out_image\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Jigsaw on Steel Images\nHere we start the actual code. We have some predefined constants below for the size of various layers and tiles. These should be optimized to be well suited for the problem at hand"},{"metadata":{"trusted":true},"cell_type":"code","source":"TILE_X = 128\nTILE_Y = 128\nJITTER_SIZE = 16\nTRAIN_TILE_COUNT = 2**11\nVALID_TILE_COUNT = 2**9\nKEEP_RANDOM_PERM = 200\nLATENT_SIZE = 32\nBIG_LATENT_SIZE = 64\nNR_EPOCHS = 15\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, m_axs = plt.subplots(6, 4, figsize=(15, 25))\nfor img_idx, c_axs in enumerate(m_axs.T, 1):\n    x_img = imread(flat_train_df.iloc[img_idx]['image_path'])\n    c_axs[0].imshow(x_img)\n    c_axs[0].set_title('Input')\n    out_tiles = cut_jigsaw(x_img, TILE_X, TILE_Y, gap=False) \n    for k, c_ax in zip(range(out_tiles.shape[0]), c_axs[1:-1]):\n        c_ax.matshow(out_tiles[k, :, :, 0])\n    recon_img = jigsaw_to_image(out_tiles, x_img.shape[0], x_img.shape[1])\n    c_axs[-1].imshow(recon_img[:, :, 0])\n    c_axs[-1].set_title('Reconstruction')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scramble Combinations\nWe have $24!$ different possible permutations, but that is too many and is probably not a great problem to solve (since it is under-constained, there are alot of permutations where it would be hard to know what exactly is being matched to what."},{"metadata":{"trusted":true},"cell_type":"code","source":"@autotest\ndef get_rand_perms(n, k):\n    \"\"\"Get k random permutations of n numbers\n    >>> get_rand_perms(3, 2)\n    [[0, 1, 2], [0, 2, 1]]\n    >>> from itertools import permutations\n    >>> nine_perms = np.array(list(permutations(range(9), 9)))\n    >>> random.seed(2019)\n    >>> keep_nine_perm = nine_perms[0:1, :].tolist()+random.sample(nine_perms.tolist(), 99)\n    >>> # np.allclose(get_rand_perms(9, 100), keep_nine_perm, atol=0.5)\n    \"\"\"\n    random.seed(2019)\n    all_perm = [np.arange(n).tolist()]\n    for i in range(k-1):\n        rem_nums = set(range(n))\n        all_perm.append(random.sample(range(n), n))\n    return all_perm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keep_perm = get_rand_perms(out_tiles.shape[0], KEEP_RANDOM_PERM)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Show combinations\nHere we can show combinations along with various instances of jitter noise to see how much that affects the reconstruction"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, m_axs = plt.subplots(5, 5, figsize=(15, 10))\nx_img = np.expand_dims(imread(flat_train_df.iloc[img_idx]['image_path'], as_gray=True), -1)\nfor i, c_axs in enumerate(m_axs.T):\n    out_tiles = cut_jigsaw(x_img, TILE_X, TILE_Y, gap=False, jitter=i>0, jitter_dim=JITTER_SIZE) \n    for j, (c_ax, c_perm) in enumerate(zip(c_axs, keep_perm)): \n        scrambled_tiles = out_tiles[c_perm]\n        recon_img = jigsaw_to_image(scrambled_tiles, x_img.shape[0], x_img.shape[1])\n        c_ax.imshow(recon_img.squeeze())\n        c_ax.set_title('Permutation:#{}\\nJitter:{}'.format(j, i>0))\n        c_ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation\nIn order to train a model we need to pre-compute a whole bunch of data to train models with"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nflat_train_df['defects_count'].hist(bins=30)\ntrain_frames_df, valid_frames_df = train_test_split(flat_train_df, \n                                                    test_size=0.3, \n                                                    random_state=2019, \n                                                    stratify=pd.qcut(flat_train_df['defects_count'], 10)\n                                                   )\nprint(train_frames_df.shape, valid_frames_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_tiles = cut_jigsaw(x_img, TILE_X, TILE_Y, gap=False) \n\ndef _generate_batch(in_idx, is_valid=False):\n    np.random.seed(in_idx)\n    if is_valid:\n        img_idx = np.random.choice(range(valid_frames_df.shape[0]))\n        c_row = valid_frames_df.iloc[img_idx]\n    else:\n        img_idx = np.random.choice(range(train_frames_df.shape[0]))\n        c_row = train_frames_df.iloc[img_idx]\n    x_img = np.expand_dims(imread(c_row['image_path'], as_gray=True), -1)\n    out_tiles = cut_jigsaw(x_img, TILE_X, TILE_Y, gap=True, jitter=JITTER_SIZE>0, jitter_dim=JITTER_SIZE) \n    perm_idx = np.random.choice(range(len(keep_perm)))\n    c_perm = keep_perm[perm_idx]\n    return out_tiles[c_perm], perm_idx\n\ndef make_tile_group(tile_count, is_valid=False):\n    c_tiles = np.zeros((tile_count,)+out_tiles.shape, dtype='float32')\n    c_perms = np.zeros((tile_count,), dtype='int')\n    for i in tqdm_notebook(range(tile_count)):\n        # should be parallelized\n        c_tiles[i], c_perms[i] = _generate_batch(i, is_valid=is_valid)\n    return c_tiles, c_perms\ntrain_tiles, train_perms = make_tile_group(TRAIN_TILE_COUNT)\nvalid_tiles, valid_perms = make_tile_group(VALID_TILE_COUNT, is_valid=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building\n## Encoder Model\nWe first build the tile encoder model to come up with a feature representation of the tiles"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models, layers\ntile_encoder = models.Sequential(name='TileEncoder')\n# we use None to make the model more usuable later\ntile_encoder.add(layers.BatchNormalization(input_shape=(None, None)+(train_tiles.shape[-1],)))\nfor i in range(6):\n    tile_encoder.add(layers.Conv2D(8*2**i, (3,3), padding='same', activation='linear'))\n    tile_encoder.add(layers.BatchNormalization())\n    tile_encoder.add(layers.MaxPool2D(2,2))\n    tile_encoder.add(layers.LeakyReLU(0.1))\n\ntile_encoder.add(layers.Conv2D(LATENT_SIZE, (1,1), activation='linear'))\ntile_encoder.add(layers.BatchNormalization())\ntile_encoder.add(layers.LeakyReLU(0.1))\nclear_output() # some annoying loading/warnings come up","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tile_encoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model Input Shape:', train_tiles.shape[2:], \n      '-> Model Output Shape:', tile_encoder.predict(np.zeros((1,)+train_tiles.shape[2:])).shape[1:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Big Jigsaw Permutation Model\nHere we reuse (shared-weights) the tile-encoder to process a number of tiles and predict which permutation is most likely"},{"metadata":{"trusted":true},"cell_type":"code","source":"big_in = layers.Input(train_tiles.shape[1:], name='All_Tile_Input')\nfeat_vec = []\nfor k in range(train_tiles.shape[1]):\n    lay_x = layers.Lambda(lambda x: x[:, k], name='Select_{}_Tile'.format(k))(big_in)\n    feat_x = tile_encoder(lay_x)\n    feat_vec += [layers.GlobalAvgPool2D()(feat_x)]\nfeat_cat = layers.concatenate(feat_vec)\nfeat_dr = layers.Dropout(0.5)(feat_cat)\nfeat_latent = layers.Dense(BIG_LATENT_SIZE)(feat_dr)\nfeat_latent_dr = layers.Dropout(0.5)(feat_latent)\nout_pred = layers.Dense(KEEP_RANDOM_PERM, activation='softmax')(feat_latent_dr)\nbig_model = models.Model(inputs=[big_in], outputs=[out_pred])\nbig_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy', 'sparse_top_k_categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.vis_utils import model_to_dot\nfrom IPython.display import Image\ndot_model = model_to_dot(big_model, show_shapes=True)\ndot_model.set_rankdir('LR')\nImage(dot_model.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Show Output\nIn order to show the model output we need to descramble the image with the given scrambling code. Given that a scrambling is a mapping from $i\\rightarrow j$ we need to make a reverse mapping for each combination"},{"metadata":{"trusted":true},"cell_type":"code","source":"reversed_keep_perm = [[c_dict[j] for j in range(out_tiles.shape[0])]\n                      for c_dict in [{j: i for i, j in enumerate(c_perm)}\n                                     for c_perm in keep_perm]]\nfor i in range(3):\n    print('forward', keep_perm[i], 'reversed', reversed_keep_perm[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_model_output(image_count=4, perm_count=3): \n    fig, m_axs = plt.subplots(image_count, perm_count+1, figsize=(5*(perm_count+1), 2*image_count))\n    [c_ax.axis('off') for c_ax in m_axs.flatten()]\n    for img_idx, c_axs in enumerate(m_axs):\n        img_idx = np.random.choice(range(flat_train_df.shape[0]))\n        c_row = flat_train_df.iloc[img_idx]\n        x_img = np.expand_dims(imread(c_row['image_path'], as_gray=True), -1)\n        perm_idx = np.random.choice(range(len(keep_perm)))\n        c_axs[0].imshow(x_img.squeeze())\n        \n        c_axs[0].set_title('Input #{}'.format(perm_idx))\n        # generate tiles\n        out_tiles = cut_jigsaw(x_img, TILE_X, TILE_Y, gap=True, jitter=JITTER_SIZE>0, jitter_dim=JITTER_SIZE)\n        # scramble tiles\n        \n        c_perm = keep_perm[perm_idx]\n        scr_tiles = out_tiles[c_perm]\n        # get model prediction\n        out_pred = big_model.predict(np.expand_dims(scr_tiles, 0))[0]\n        for c_ax, k_idx in zip(c_axs[1:], np.argsort(-1*out_pred)):\n            pred_rev_perm = reversed_keep_perm[k_idx]\n            recon_img = jigsaw_to_image(scr_tiles[pred_rev_perm], x_img.shape[0], x_img.shape[1])\n            c_ax.imshow(recon_img.squeeze())\n            c_ax.set_title('Pred: #{} ({:2.2%})'.format(k_idx, out_pred[k_idx]))\nshow_model_output()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_results = big_model.fit(train_tiles, train_perms, \n                            validation_data=(valid_tiles, valid_perms),\n                                 batch_size=24,\n                                 epochs=NR_EPOCHS)\nclear_output()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.semilogy(fit_results.history['loss'], label='Training')\nax1.semilogy(fit_results.history['val_loss'], label='Validation')\nax1.legend()\nax1.set_title('Loss')\nax2.plot(fit_results.history['sparse_categorical_accuracy'], label='Training')\nax2.plot(fit_results.history['val_sparse_categorical_accuracy'], label='Validation')\nax2.legend()\nax2.set_title('Accuracy')\nax2.set_ylim(0, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_model_output(image_count=10, perm_count=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tile_encoder.save('tile_encoder.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Did we learn useful intermediate representations?\nSo we have a nice pretrained model that seems to have figured out how to solve the jigsaw puzzle (sometimes). Can we do anything with it?\n- Use the model to calculate features segmentation from feature map\n- See if the feature space has anything meaningful"},{"metadata":{},"cell_type":"markdown","source":"## Look at the filters\nWe can examine the filters and try to see what the model was doing?"},{"metadata":{"trusted":true},"cell_type":"code","source":"conv_weight_dict = {(idx, k.name): k.get_weights() for idx, k in enumerate(tile_encoder.layers) if isinstance(k, layers.Conv2D)}\nprint(conv_weight_dict.keys())\nfig, m_axs = plt.subplots(2, 3, figsize=(20, 10))\nfor c_ax, ((idx, lay_name), [W, b]) in zip(m_axs.flatten(), conv_weight_dict.items()):\n    c_ax.set_title('{} #{}\\n{}'.format(lay_name, idx, W.shape))\n    flat_W = W.reshape((W.shape[0], W.shape[1], -1)).swapaxes(0, 2).swapaxes(1,2)\n    if flat_W.shape[1]>1 or flat_W.shape[2]>1:\n        pad_W = np.pad(flat_W, [(0, 0), (1, 1), (1,1)], mode='constant', constant_values=np.NAN)\n        pad_W = montage(pad_W, fill=np.NAN, grid_shape=(W.shape[2], W.shape[3]))\n    else:\n        pad_W = W[0, 0]\n    c_ax.imshow(pad_W, vmin=-1, vmax=1, cmap='RdBu')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Find the most activating imaging channels\nWe can run all of the images through the model and record all of the intermediate points"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_tiles = train_tiles.reshape((-1, train_tiles.shape[2], train_tiles.shape[3], train_tiles.shape[4]))\nprint(full_tiles.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gp_outputs = []\nfor k in tile_encoder.layers:\n    if isinstance(k, layers.LeakyReLU):\n        c_output = k.get_output_at(0)\n        c_smooth = layers.AvgPool2D((2, 2))(c_output)\n        c_gp = layers.GlobalMaxPool2D(name='GP_{}'.format(k.name))(c_smooth)\n        gp_outputs += [c_gp]\nactivation_tile_encoder = models.Model(inputs = tile_encoder.inputs, \n                                       outputs = gp_outputs)\nactivation_maps = dict(zip(activation_tile_encoder.output_names, \n                           activation_tile_encoder.predict(full_tiles, batch_size=256, verbose=True)))\n\nfor k, v in activation_maps.items():\n    print(k, v.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Activated Neurons\nHere we show each intermediate layer (panel) with each neuron/depth-channel (row) and the top-n images for activating that pattern (columns). Each row should more or less represent the kinds of images that particular neuron is sensitive too."},{"metadata":{"trusted":true},"cell_type":"code","source":"keep_top_n = 5\nfig, m_axs = plt.subplots(1, len(activation_maps), figsize=(20, 20))\nfor c_ax, (k, v) in zip(m_axs.T, activation_maps.items()):\n    c_ax.set_title(k)\n    active_rows = []\n    for i in range(v.shape[1]):\n        top_idx = np.argsort(-np.abs(v[:, i]))[:keep_top_n]\n        active_rows += [full_tiles[top_idx, :, :, 0]]\n    c_ax.imshow(montage(np.concatenate(active_rows, 0), grid_shape=(v.shape[1], keep_top_n), padding_width=1))\n    c_ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert into a Segmentation Model\nWe throw in global average pooling to turn the output of the `tile_encoder` into a single feature-vector. We can then use this feature vector as a basis classifying image."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_img.shape, '->', tile_encoder.predict(np.expand_dims(x_img, 0)).shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_in = layers.Input(x_img.shape)\ntile_encoder.trainable=False\nfull_feat_mat = tile_encoder(img_in)\nseg_img = layers.Conv2D(1, (1, 1), activation='sigmoid')(full_feat_mat)\nus_out = layers.UpSampling2D((64, 64))(seg_img)\nimage_encoder = models.Model(inputs=[img_in], outputs=[us_out], name='SegmentImage')\nimage_encoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_gen(in_df, batch_size):\n    while True:\n        c_batch = in_df.sample(batch_size)\n        yield np.stack(c_batch['image_path'].map(lambda x: np.expand_dims(imread(x, as_gray=True), -1)), 0), \\\n            np.sum(np.stack(c_batch.apply(full_mask, axis=1).values, 0), axis=-1, keepdims=True)\ntrain_gen = data_gen(train_frames_df, 8)\nvalid_gen = data_gen(valid_frames_df, 8)\nsamp_X, samp_y = next(train_gen)\nprint(samp_X.shape, samp_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\ndef dice_coef(y_true, y_pred, smooth=1):\n    \"\"\"\n    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n    \"\"\"\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1-dice_coef(y_true, y_pred)\n\nimage_encoder.compile(optimizer='adam', loss=dice_coef_loss, metrics=['binary_accuracy', 'mae', dice_coef])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def montage_tile(in_img):\n    batch_size = in_img.shape[0]\n    return montage(in_img[...,0], grid_shape=(batch_size, 1))\ndef show_batch(in_gen):\n    samp_X, samp_y = next(in_gen)\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 25))\n    ax1.imshow(montage_tile(samp_X), cmap='gray')\n    ax1.axis('off')\n    ax2.imshow(montage_tile(samp_y), cmap='viridis', vmin=0, vmax=1)\n    ax2.axis('off')\n    ax3.imshow(montage_tile(image_encoder.predict(samp_X)), cmap='viridis', vmin=0, vmax=1)\n    ax3.set_title('Prediction')\n    ax3.axis('off')\nshow_batch(valid_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seg_results = image_encoder.fit_generator(train_gen, \n                            validation_data=valid_gen,\n                                          steps_per_epoch=50,\n                                          validation_steps=10,\n                                 epochs=NR_EPOCHS)\nclear_output()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.semilogy(seg_results.history['loss'], label='Training')\nax1.semilogy(seg_results.history['val_loss'], label='Validation')\nax1.legend()\nax1.set_title('Loss')\nax2.plot(seg_results.history['binary_accuracy'], label='Training')\nax2.plot(seg_results.history['val_binary_accuracy'], label='Validation')\nax2.legend()\nax2.set_title('Accuracy')\nax2.set_ylim(0, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_batch(valid_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_encoder.save('encoder_model.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"16b2a3f5b40640feb90cd733c367efcc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"192234eb0c0448ffaf46b977cb1e0547":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23f49a55778c4dd5ad9ece112834809f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27708127852b416081f4425e60495184":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e11af07054fd4d9796ba57ea39283fc1","IPY_MODEL_f82d374f3ce046cb92079af4c5ece081"],"layout":"IPY_MODEL_192234eb0c0448ffaf46b977cb1e0547"}},"30379099aecb475eb8ccd05f2a3d6184":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7a1f3adb731f421aa1b0d74101d3001d","IPY_MODEL_8c29670e973a46918a246be0b08549b3"],"layout":"IPY_MODEL_16b2a3f5b40640feb90cd733c367efcc"}},"49e3e38b00f743b18323fc0aea042402":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6a5679c467d844bd8f10bd0eaff9d2f4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ebcd0bf9d4c40bfac245e5fa6cc43a8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fe792a114ef4eb2972f3065c03cdacc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"730489bd2b484005a54104b1c20ace71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a1f3adb731f421aa1b0d74101d3001d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_23f49a55778c4dd5ad9ece112834809f","max":2048,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aa2b7334faf349ff938bdb967eb458f9","value":2048}},"83e8808bc4274cd398e8081fde53e3e4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c29670e973a46918a246be0b08549b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_730489bd2b484005a54104b1c20ace71","placeholder":"​","style":"IPY_MODEL_6ebcd0bf9d4c40bfac245e5fa6cc43a8","value":"100% 2048/2048 [00:09&lt;00:00, 219.10it/s]"}},"aa2b7334faf349ff938bdb967eb458f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e11af07054fd4d9796ba57ea39283fc1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_83e8808bc4274cd398e8081fde53e3e4","max":16384,"min":0,"orientation":"horizontal","style":"IPY_MODEL_49e3e38b00f743b18323fc0aea042402","value":16384}},"f82d374f3ce046cb92079af4c5ece081":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a5679c467d844bd8f10bd0eaff9d2f4","placeholder":"​","style":"IPY_MODEL_6fe792a114ef4eb2972f3065c03cdacc","value":"100% 16384/16384 [01:46&lt;00:00, 153.30it/s]"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}