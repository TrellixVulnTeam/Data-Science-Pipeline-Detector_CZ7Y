{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overview\nThe primary goal of this competition is identification and segmentation of surface defects in flat sheet steel. In contrast to previous segmentation competitions, such as [Airbus Ship Detection Challenge](https://www.kaggle.com/c/airbus-ship-detection/discussion) and [TGS Salt Identification Challenge](https://www.kaggle.com/c/tgs-salt-identification-challenge), as well as the ongoing [SIIM-ACR Pneumothorax Segmentation](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation) challenge, here there are several classes of defects must be predicted. The kernel below provides a Unet starter code written with use of fast.ai library and incorporation of Hypercolumns method. Additional details of this method could be found in [this kernel](https://www.kaggle.com/iafoss/hypercolumns-pneumothorax-fastai-0-831-lb). \n\nIn contrast to Pneumothorax challenge, where segmentation may require information from entire image, here surface defects can be identified locally. Therefore, splitting of images into crops and training only on images with nonzero masks may be an effective strategy. In particular, it allows to train the model with using large enough batches and keeping only parts of images containing useful information. Hard negative examples identified based on the model prediction on crops with empty masks are essential for training, and the training set is composed of positive examples, images with texture, and 12000 most difficult negative crops. Despite training is done on crops, the predictions are generated based on full size test images ([check this kernel](https://www.kaggle.com/iafoss/severstal-fast-ai-256x256-crops-sub)). Though, the model could be fine-tuned on full 256x1600 images to further boost the performance. Such an approach was successfully used in [6th place solution](https://www.kaggle.com/c/airbus-ship-detection/discussion/71782#latest-558831) in Ship Detection challenge. Therefore, the train/val split is done based on full images (avoid sharing crops between train and val). The dataset for this kernel with 256x256 crops containing defects is prepared [here](https://www.kaggle.com/iafoss/256x256-images-with-defects). That kernel also provides function for RLE from/to masks conversion, generates masks, and computes the pixel statistics. Because of the 1-hour time limit in the submission kernel (20 min for prediction on the public portion of the test set), I wrote a [separate kernel](https://www.kaggle.com/iafoss/severstal-fast-ai-256x256-crops-sub) for generation of the submission based on models trained here.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# when set true, the index is removed from the negative tiles (this is already done for +ve tiles)\n# - as a result no -ve images are ever added to the validation set\n# - also positive and negative tiles can come from the same original images - with old code tiles\n#   from same image could end up both training and validation sets, resulting in data leakage\nUSE_ORIGINAL_VALIDATION = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NotebookTitle = \"Steel Defect Segmentation - Training - Original Validation\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport fastai\nfrom fastai.vision import *\nfrom fastai.callbacks import SaveModelCallback\nimport gc\nimport os\nfrom sklearn.model_selection import KFold\nfrom PIL import Image\nimport zipfile\nimport io\nimport cv2\nimport warnings\nfrom radam import RAdam\nwarnings.filterwarnings(\"ignore\")\n\nfastai.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sz = 256\nbs = 16\nnfolds = 4\n\nSEED = 2019\nTRAIN = '../input/severstal-256x256-images-with-defects/images/'\nMASKS = '../input/severstal-256x256-images-with-defects/masks/'\nTRAIN_N = '../input/severstal-256x256-images-with-defects/images_n/'\nHARD_NEGATIVE = '../input/hard-negative-severstal-crops/pred.csv'\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    #tf.set_random_seed(seed)\nseed_everything(SEED)\ntorch.backends.cudnn.benchmark = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#the code below modifies fast.ai functions to incorporate Hcolumns into fast.ai Dynamic Unet\n\nfrom fastai.vision.learner import create_head, cnn_config, num_features_model, create_head\nfrom fastai.callbacks.hooks import model_sizes, hook_outputs, dummy_eval, Hook, _hook_inner\nfrom fastai.vision.models.unet import _get_sfs_idxs, UnetBlock\n\nclass Hcolumns(nn.Module):\n    def __init__(self, hooks:Collection[Hook], nc:Collection[int]=None):\n        super(Hcolumns,self).__init__()\n        self.hooks = hooks\n        self.n = len(self.hooks)\n        self.factorization = None \n        if nc is not None:\n            self.factorization = nn.ModuleList()\n            for i in range(self.n):\n                self.factorization.append(nn.Sequential(\n                    conv2d(nc[i],nc[-1],3,padding=1,bias=True),\n                    conv2d(nc[-1],nc[-1],3,padding=1,bias=True)))\n                #self.factorization.append(conv2d(nc[i],nc[-1],3,padding=1,bias=True))\n        \n    def forward(self, x:Tensor):\n        n = len(self.hooks)\n        out = [F.interpolate(self.hooks[i].stored if self.factorization is None\n            else self.factorization[i](self.hooks[i].stored), scale_factor=2**(self.n-i),\n            mode='bilinear',align_corners=False) for i in range(self.n)] + [x]\n        return torch.cat(out, dim=1)\n\nclass DynamicUnet_Hcolumns(SequentialEx):\n    \"Create a U-Net from a given architecture.\"\n    def __init__(self, encoder:nn.Module, n_classes:int, blur:bool=False, blur_final=True, \n                 self_attention:bool=False,\n                 y_range:Optional[Tuple[float,float]]=None,\n                 last_cross:bool=True, bottle:bool=False, **kwargs):\n        imsize = (256,256)\n        sfs_szs = model_sizes(encoder, size=imsize)\n        sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs)))\n        self.sfs = hook_outputs([encoder[i] for i in sfs_idxs])\n        x = dummy_eval(encoder, imsize).detach()\n\n        ni = sfs_szs[-1][1]\n        middle_conv = nn.Sequential(conv_layer(ni, ni*2, **kwargs),\n                                    conv_layer(ni*2, ni, **kwargs)).eval()\n        x = middle_conv(x)\n        layers = [encoder, batchnorm_2d(ni), nn.ReLU(), middle_conv]\n\n        self.hc_hooks = [Hook(layers[-1], _hook_inner, detach=False)]\n        hc_c = [x.shape[1]]\n        \n        for i,idx in enumerate(sfs_idxs):\n            not_final = i!=len(sfs_idxs)-1\n            up_in_c, x_in_c = int(x.shape[1]), int(sfs_szs[idx][1])\n            do_blur = blur and (not_final or blur_final)\n            sa = self_attention and (i==len(sfs_idxs)-3)\n            unet_block = UnetBlock(up_in_c, x_in_c, self.sfs[i], final_div=not_final, \n                blur=blur, self_attention=sa, **kwargs).eval()\n            layers.append(unet_block)\n            x = unet_block(x)\n            self.hc_hooks.append(Hook(layers[-1], _hook_inner, detach=False))\n            hc_c.append(x.shape[1])\n\n        ni = x.shape[1]\n        if imsize != sfs_szs[0][-2:]: layers.append(PixelShuffle_ICNR(ni, **kwargs))\n        if last_cross:\n            layers.append(MergeLayer(dense=True))\n            ni += in_channels(encoder)\n            layers.append(res_block(ni, bottle=bottle, **kwargs))\n        hc_c.append(ni)\n        layers.append(Hcolumns(self.hc_hooks, hc_c))\n        layers += [conv_layer(ni*len(hc_c), n_classes, ks=1, use_activ=False, **kwargs)]\n        if y_range is not None: layers.append(SigmoidRange(*y_range))\n        super().__init__(*layers)\n\n    def __del__(self):\n        if hasattr(self, \"sfs\"): self.sfs.remove()\n            \ndef unet_learner(data:DataBunch, arch:Callable, pretrained:bool=True, blur_final:bool=True,\n        norm_type:Optional[NormType]=NormType, split_on:Optional[SplitFuncOrIdxList]=None, \n        blur:bool=False, self_attention:bool=False, y_range:Optional[Tuple[float,float]]=None, \n        last_cross:bool=True, bottle:bool=False, cut:Union[int,Callable]=None, \n        hypercolumns=True, **learn_kwargs:Any)->Learner:\n    \"Build Unet learner from `data` and `arch`.\"\n    meta = cnn_config(arch)\n    body = create_body(arch, pretrained, cut)\n    M = DynamicUnet_Hcolumns if hypercolumns else DynamicUnet\n    model = to_device(M(body, n_classes=data.c, blur=blur, blur_final=blur_final,\n        self_attention=self_attention, y_range=y_range, norm_type=norm_type, \n        last_cross=last_cross, bottle=bottle), data.device)\n    learn = Learner(data, model, **learn_kwargs)\n    learn.split(ifnone(split_on, meta['split']))\n    if pretrained: learn.freeze()\n    apply_init(model[2], nn.init.kaiming_normal_)\n    return learn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function below computes multiclass dice averaged for each image-class pair. This metric is used in the competition, though one should not be confused with the val score in this kernel and the model performance on the public LB. In particular, images used in training are represented by only a part of all crops (positive examples + images with texture + 12000 hard negative examples).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice(input:Tensor, targs:Tensor, iou:bool=False, eps:float=1e-8)->Rank0Tensor:\n    n,c = targs.shape[0], input.shape[1]\n    input = input.argmax(dim=1).view(n,-1)\n    targs = targs.view(n,-1)\n    intersect,union = [],[]\n    for i in range(1,c):\n        intersect.append(((input==i) & (targs==i)).sum(-1).float())\n        union.append(((input==i).sum(-1) + (targs==i).sum(-1)).float())\n    intersect = torch.stack(intersect)\n    union = torch.stack(union)\n    if not iou: return ((2.0*intersect + eps) / (union+eps)).mean()\n    else: return ((intersect + eps) / (union - intersect + eps)).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class SegmentationLabelList(SegmentationLabelList):\n    def open(self, fn): return open_mask(fn, div=True)\n    \nclass SegmentationItemList(SegmentationItemList):\n    _label_cls = SegmentationLabelList\n\n# Setting transformations on masks to False on test set\ndef transform(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n    if not tfms: tfms=(None,None)\n    assert is_listy(tfms) and len(tfms) == 2\n    self.train.transform(tfms[0], **kwargs)\n    self.valid.transform(tfms[1], **kwargs)\n    kwargs['tfm_y'] = False # Test data has no labels\n    if self.test: self.test.transform(tfms[1], **kwargs)\n    return self\nfastai.data_block.ItemLists.transform = transform\n\ndef open_mask(fn:PathOrStr, div:bool=True, convert_mode:str='L', cls:type=ImageSegment,\n        after_open:Callable=None)->ImageSegment:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        #generate empty mask if file doesn't exist\n        x = PIL.Image.open(fn).convert(convert_mode) \\\n          if Path(fn).exists() \\\n          else PIL.Image.fromarray(np.zeros((sz,sz)).astype(np.uint8))\n    if after_open: x = after_open(x)\n    x = pil2tensor(x,np.float32)\n    return cls(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(HARD_NEGATIVE)\ndf['index'] = df.index\ndf.plot(x='index', y='pixels', kind = 'line');\nplt.yscale('log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To fix problem with Fast.ai display of tiled images\nimport matplotlib\ncmap = matplotlib.colors.ListedColormap(['grey', 'blue', 'red', 'green', 'yellow'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats = ([0.396,0.396,0.396], [0.179,0.179,0.179])\n#check https://www.kaggle.com/iafoss/256x256-images-with-defects for stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The code below eliminates sharing patches of the same image across folds","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a set of image names (without tile indexes) e.g. 'b69aaf096'\nimg_p = set([p.stem[:-2] for p in Path(TRAIN).ls()])\n\n#select 12000 of the most difficult negative exaples\nneg = list(pd.read_csv(HARD_NEGATIVE).head(12000).fname)\nneg = [Path(TRAIN_N)/f for f in neg]\n\n# set of negative tiles - these have retained their indexes?\n# - does the hard negative list only contain one tile per image?\n\n# set of negative tiles \n# - for some reason these have retained their indexes\n# - as a result the set 'img_n' contains multiple entries from the same image\n# - keeping the indexes len(img_n) = 12000 (i.e. the full set)\n# - removing the indexes len(img_n) = 6732\n\nif USE_ORIGINAL_VALIDATION:\n    img_n = set([p.stem for p in neg])\nelse:\n    img_n = set([p.stem[:-2] for p in neg])\n\nprint(\"Negative Set:\",len(img_n),\" (out of 12000)\")\n\n# create a set that contains entries that are in either the positive OR the negative sets\n# - i.e. duplicates will be dropped \n# (duplicates are possible since negative set can contain tiles from parts of a defect image where the mask is empty)\n# - however, since the indexes are retained on negative tiles, the names will never match\nimg_set = img_p | img_n\n\n# sorting the sets converts them back to lists\nimg_p_list = sorted(img_p)\nimg_n_list = sorted(img_n)\n\n# combine the list of positive and negative images\n# - when indexes haven't been removed from negative, this list will be identical to the OR'd set\n# - when indexes HAVE been removed it will be different, since duplicates between +ve and -ve may exist\nimg_list = img_p_list + img_n_list\n\n# MY CODE - use the set, which removes duplicates from the +ve and -ve sets, rather than ANDed\nif USE_ORIGINAL_VALIDATION == False:\n    img_list = sorted(img_set)\n\nprint(\"OR'd Set: \",len(img_set))\nprint(\"AND'd Set: \",len(img_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# where nfolds=4\nkf = KFold(n_splits=nfolds, shuffle=True, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Examine what happens for fold 0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# valid_idx = list(kf.split(list(range(len(img_list)))))[fold][1]\n\n# the length of the list of +ve and -ve\n# - so for positive is the image names\n# - for negative is the tile names\n# = 19260 \nlen_img_list = len(img_list)\n\n# creates a list of all indexes from 0 to len_img_list\nlist_img_list = list(range(len_img_list))\n\n# create a list of indexes for the 4 folds\nfold_lists = list(kf.split(list_img_list))\n\n# get the indexes for the first fold - this contains 2 arrays - the training indexes and the validation indexes\nfold_list_0 = fold_lists[fold]\n\n# get the indexes of the first fold's validation set\nvalid_idx = fold_list_0[1]\nprint(f'Length of validation indexes = {len(valid_idx)}')\n\n# the set of tile names\\image names for this fold's validation set\nvalid = set([img_list[i] for i in valid_idx])\n\nprint(f'Validation Set for fold 0: {list(valid)[:10]}')\n\n# now compare the stem names of all images in the positive and negative sets\n# against the names in the validation set\n# - since the negative images haven't had their index removed in the validation set\n# they will never match with the stem names and so none will be added to these indexes\n# - the size of this list is larger than the size of the valid indexes above because all positive\n# tiles in the valid list are added\n# - the only tiles from the negative set that are added are ones from images that also had defect tiles (i.e. in the positive set)\nvalid_idx = []\nfor i,p in enumerate(Path(TRAIN).ls() + neg):\n    if p.stem[:-2] in valid: \n        valid_idx.append(i)\n\n        \nprint(f'Length of validation indexes after stem compare: {len(valid_idx)}')    \n\n# tile image '7e0a4584c_0' is from the negative set and is one of the images for fold 0's validation set\n# - therefore all '7e0a4584c' images should be added to the validation set\n# - however, since no match will occur in the 'if' statement above, no indexes from negative set are added\n\n# Create databunch\nsl = SegmentationItemList.from_folder(TRAIN)\n\nprint(f'Number of positive images: {len(Path(TRAIN).ls())}')\nprint(f'Number of negative images: {len(neg)}')\n\n# add the negative images to the item list\nsl.items = np.array((list(sl.items) + neg))\n\n# the total size is all positive and negative tiles = 31695\nprint(f'Total size of all data in item list: {sl.items.shape}')\n\ndata = (sl.split_by_idx(valid_idx)\n    .label_from_func(lambda x : str(x).replace('/images', '/masks'), classes=[0,1,2,3,4])\n    .transform(get_transforms(xtra_tfms=dihedral()), size=sz, tfm_y=True)\n    .databunch(path=Path('.'), bs=bs)\n    .normalize(stats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show the train\\validation split - with 4 folds this should be about 3:1\n# - with old code = 25644 : 6051 = 4:1\n# - with new code = 23667 : 8028 = 3:1\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(fold):\n    #split with making sure that crops of the same original image \n    #are not shared between folds, so additional training and validation \n    #could be done on full images later\n    \n    # get the validation indexes of the specified fold\n    valid_idx = list(kf.split(list(range(len(img_list)))))[fold][1]\n    \n    # get the set of tile names\\image names for this fold's validation set\n    # - because indexes weren't removed from negative tiles these are still present\n    # in the names added to this validation set\n    valid = set([img_list[i] for i in valid_idx])\n    \n    # now removes the index from all tiles in both the positive and negative sets\n    # - however, since the negative tiles haven't had their index removed in the validation set\n    # their indexes will never be added to valid_idx\n    valid_idx = []\n    for i,p in enumerate(Path(TRAIN).ls() + neg):\n        if p.stem[:-2] in valid: valid_idx.append(i)\n            \n    # Create databunch\n    sl = SegmentationItemList.from_folder(TRAIN)\n    sl.items = np.array((list(sl.items) + neg))\n    data = (sl.split_by_idx(valid_idx)\n        .label_from_func(lambda x : str(x).replace('/images', '/masks'), classes=[0,1,2,3,4])\n        .transform(get_transforms(xtra_tfms=dihedral()), size=sz, tfm_y=True)\n        .databunch(path=Path('.'), bs=bs)\n        .normalize(stats))\n    return data\n\n# Display some images with masks - set the cmap to see the different defects\n# NOTE:- with USE_ORIGINAL_VALIDATION = False there will be more validation images without defects\n# since images from the negative set are also now included\nget_data(0).show_batch(cmap=cmap,vmax=5)\n# get_data(0).show_batch()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"@dataclass\nclass CSVLogger(LearnerCallback):\n    def __init__(self, learn, filename= 'history'):\n        self.learn = learn\n        self.path = self.learn.path/f'{filename}.csv'\n        self.file = None\n\n    @property\n    def header(self):\n        return self.learn.recorder.names\n\n    def read_logged_file(self):\n        return pd.read_csv(self.path)\n\n    def on_train_begin(self, metrics_names: StrList, **kwargs: Any) -> None:\n        self.path.parent.mkdir(parents=True, exist_ok=True)\n        e = self.path.exists()\n        self.file = self.path.open('a')\n        if not e: self.file.write(','.join(self.header) + '\\n')\n\n    def on_epoch_end(self, epoch: int, smooth_loss: Tensor, last_metrics: MetricsList, **kwargs: Any) -> bool:\n        self.write_stats([epoch, smooth_loss] + last_metrics)\n\n    def on_train_end(self, **kwargs: Any) -> None:\n        self.file.flush()\n        self.file.close()\n\n    def write_stats(self, stats: TensorOrNumList) -> None:\n        stats = [str(stat) if isinstance(stat, int) else f'{stat:.6f}'\n                 for name, stat in zip(self.header, stats)]\n        str_stats = ','.join(stats)\n        self.file.write(str_stats + '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function below generates predictions with 3 fold TTA (horizontal flip, vertical flip, and both). The default fast.ai implementation is too memory hungry since predictions for all images and TTA folds are kept in memory. So it becomes hardly possible to generate a prediction for a reasonable number of high resolution images.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction with flip TTA\ndef model_pred(learn:fastai.basic_train.Learner,F_save,\n        ds_type:fastai.basic_data.DatasetType=DatasetType.Valid, \n        tta:bool=True): #if use train dl, disable shuffling\n    learn.model.eval();\n    dl = learn.data.dl(ds_type)\n    #sampler = dl.batch_sampler.sampler\n    #dl.batch_sampler.sampler = torch.utils.data.sampler.SequentialSampler(sampler.data_source)\n    name_list = [Path(n).stem for n in dl.dataset.items]\n    num_batchs = len(dl)\n    t = progress_bar(iter(dl), leave=False, total=num_batchs)\n    count = 0\n    with torch.no_grad():\n        for x,y in t:\n            x = x.cuda()\n            py = torch.softmax(learn.model(x),dim=1).permute(0,2,3,1).detach()\n            if tta:\n                flips = [[-1],[-2],[-2,-1]]\n                for f in flips:\n                    py += torch.softmax(torch.flip(learn.model(torch.flip(x,f)),f),dim=1)\\\n                      .permute(0,2,3,1).detach()\n                py /= (1+len(flips))\n            py = py.cpu().numpy()\n            batch_size = len(py)\n            for i in range(batch_size):\n                taget = y[i].detach().cpu().numpy() if y is not None else None\n                F_save(py[i],taget,name_list[count])\n                count += 1\n    #dl.batch_sampler.sampler = sampler\n    \ndef save_img(data,name,out):\n    data = data[:,:,1:]\n    img = cv2.imencode('.png',(data*255).astype(np.uint8))[1]\n    out.writestr(name, img)\n    \n#dice for threshold selection\ndef dice_np(pred, targs, noise_th = 0, eps=1e-7):\n    targs = targs[0,:,:]\n    c = pred.shape[-1]\n    pred = np.argmax(pred, axis=-1)\n    dices = []\n    for i in range(1,c):\n        if (pred==i).sum() > noise_th:\n            intersect = ((pred==i) & (targs==i)).sum().astype(np.float)\n            union = ((pred==i).sum() + (targs==i).sum()).astype(np.float)\n            dices.append((2.0*intersect + eps) / (union + eps))\n        else: dices.append( 1.0 if (targs==i).sum() == 0 else 0.0)\n    return np.array(dices).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Check log{fold}.csv files in the output to see details of training for each fold.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get rid of garbage before training starts\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dices = []\nnoise_ths = np.arange(0, 1501, 125)\n\nwith zipfile.ZipFile('val_masks_tta.zip', 'w') as archive_out:\n    \n    #the function to save val masks and dices\n    def to_mask(yp, y, id):\n        name = id + '.png'\n        save_img(yp,name,archive_out)\n        dices_th = []\n        for noise_th in noise_ths:\n            dices_th.append(dice_np(yp,y,noise_th))\n        dices.append(dices_th)\n        \n    \n    data = get_data(fold)\n    learn = unet_learner(data, models.resnet34, metrics=[dice], opt_func=RAdam)\n    learn.clip_grad(1.0);\n    logger = CSVLogger(learn,f'log{fold}')\n\n    #fit the decoder part of the model keeping the encode frozen\n    lr = 1e-3\n    learn.fit_one_cycle(4, lr, callbacks = [logger])\n    \n    #fit entire model with saving on the best epoch\n    learn.unfreeze()\n    learn.fit_one_cycle(15, slice(lr/50, lr/2), callbacks = [logger])\n        \n    #save model\n    learn.save('fold'+str(fold));\n    np.save('items_fold'+str(fold), data.valid_ds.items)\n     \n    #run TTA prediction on val, save masks and dices\n    model_pred(learn,to_mask)\n    \n    gc.collect()\n    torch.cuda.empty_cache()\ndices = np.array(dices).mean(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this competition FP predictions are heavily penalized: if a model predicts even a single pixel for an image without particular kind of defect, the score of the predicted mask goes to zero. Meanwhile, if model predicts zero pixels for such mask, the score is 1. It is a series issue given the fraction of empty masks (~0.85 according to empty mask benchmark submission). To eliminate FP, we can impose a threshold on the minimum number of predicted pixels in a mask. If the prediction has less pixels, they are considered as noise and removed. The plot below depicts the optimal value of noise_th. However, you should keep in mind that the optimal value of noise_th is larger if the prediction is generated not on crops but on full images. In particular, according to my experiments, the optimal threshold for full size images is approximetly 4 times larger.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best_dice = dices.max()\nbest_thr = noise_ths[dices.argmax()]\nplt.figure(figsize=(8,4))\nplt.plot(noise_ths, dices)\nplt.vlines(x=best_thr, ymin=dices.min(), ymax=dices.max())\nplt.text(best_thr+50, best_dice-0.01, f'DICE = {best_dice:.3f}', fontsize=14);\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}