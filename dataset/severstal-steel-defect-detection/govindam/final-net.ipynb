{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load in \n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the \"../input/\" directory.\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras.backend as K\n#K.image_data_format('tf')\n# from keras.models import Model\n# from keras.layers import Input, ZeroPadding2D, Concatenate, Add, Flatten\n# from keras.layers.core import Dropout, Activation \n# from keras.layers.convolutional import UpSampling2D, Conv2D\n# from keras.layers.pooling import AveragePooling2D, MaxPooling2D\n# from keras.layers.normalization import BatchNormalization\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = '../input/severstal-steel-defect-detection/' \ntrain_image_dir = os.path.join(train_dir, 'train_images') \ntrain_df = pd.read_csv(os.path.join(train_dir, 'train.csv')).fillna(-1)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['ImageId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\ntrain_df['ClassId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[1])\n# lets create a dict with class id and encoded pixels and group all the defaults per image\ntrain_df['ClassId_EncodedPixels'] = train_df.apply(lambda row: (row['ClassId'], row['EncodedPixels']), axis = 1)\ngrouped_EncodedPixels = train_df.groupby('ImageId')['ClassId_EncodedPixels'].apply(list)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from https://www.kaggle.com/robertkag/rle-to-mask-converter\ndef rle_to_mask(rle_string,height,width):\n    '''\n    convert RLE(run length encoding) string to numpy array\n\n    Parameters: \n    rleString (str): Description of arg1 \n    height (int): height of the mask\n    width (int): width of the mask \n\n    Returns: \n    numpy.array: numpy array of the mask\n    '''\n    rows, cols = height, width\n    if rle_string == -1:\n        return np.zeros((height, width))\n    else:\n        rleNumbers = [int(numstring) for numstring in rle_string.split(' ')]\n        rlePairs = np.array(rleNumbers).reshape(-1,2)\n        img = np.zeros(rows*cols,dtype=np.uint8)\n        for index,length in rlePairs:\n            index -= 1\n            img[index:index+length] = 255\n        img = img.reshape(cols,rows)\n        img = img.T\n        return img\n\ndef mask_to_rle(mask):\n    '''\n    Convert a mask into RLE\n    \n    Parameters: \n    mask (numpy.array): binary mask of numpy array where 1 - mask, 0 - background\n\n    Returns: \n    sring: run length encoding \n    '''\n    pixels= mask.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# network configuration parameters\n# original image is 1600x256, so we will resize it\nimg_w = 800 # resized weidth\nimg_h = 256 # resized height\nbatch_size = 10\nepochs = 5\n# batch size for training unet\nk_size = 3 # kernel size 3x3\nval_size = .20 # split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\nimg_datagen = ImageDataGenerator(\n    vertical_flip=True)\n    #horizontal_flip=True)\n\nmask_datagen = ImageDataGenerator(\n    vertical_flip=True)\n    #horizontal_flip=True)\n\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, list_ids, labels, image_dir, mode='train', batch_size=32,\n                 img_h=256, img_w=512, shuffle=True):\n        \n        self.mode = mode\n        self.list_ids = list_ids\n        self.labels = labels\n        self.image_dir = image_dir\n        self.batch_size = batch_size\n        self.img_h = img_h\n        self.img_w = img_w\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    \n    def __len__(self):\n        'denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_ids)) / self.batch_size)\n    \n    def __getitem__(self, index):\n        'generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # get list of IDs\n        list_ids_temp = [self.list_ids[k] for k in indexes]\n        # generate data\n        X, y = self.__data_generation(list_ids_temp)\n        # return data \n        return X, y\n    \n    def on_epoch_end(self):\n        'update ended after each epoch'\n        self.indexes = np.arange(len(self.list_ids))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n    def __data_generation(self, list_ids_temp):\n        'generate data containing batch_size samples'\n        X = np.empty((self.batch_size, self.img_h, self.img_w, 1))\n        y = np.empty((self.batch_size, self.img_h, self.img_w, 4))\n        \n        for idx, id in enumerate(list_ids_temp):\n            file_path =  os.path.join(self.image_dir, id)\n            image = cv2.imread(file_path, 0)\n            image_resized = cv2.resize(image, (self.img_w, self.img_h))\n            image_resized = np.array(image_resized, dtype=np.float64)\n            # standardization of the image\n            image_resized -= image_resized.mean()\n            image_resized /= image_resized.std()\n            \n            mask = np.empty((img_h, img_w, 4))\n            \n            for idm, image_class in enumerate(['1','2','3','4']):\n                rle = self.labels.get(id + '_' + image_class)\n                # if there is no mask create empty mask\n                if rle is None:\n                    class_mask = np.zeros((1600, 256))\n                else:\n                    class_mask = rle_to_mask(rle, width=1600, height=256)\n             \n                class_mask_resized = cv2.resize(class_mask, (self.img_w, self.img_h))\n                mask[...,idm] = class_mask_resized\n            \n            seed = 10\n#             if self.mode == 'train':\n#                 if np.random.randn() > 0:\n#                     image_resized = img_datagen.random_transform(image_resized, seed = seed) \n#                     mask = mask_datagen.random_transform(mask, seed = seed)\n#                     #params = datagen.get_random_transform(image_resized.shape, seed = seed) \n#                     #image_resized = datagen.apply_transform(image_resized, params)\n#                     #mask = datagen.apply_transform(mask, params)\n            \n            X[idx,] = np.expand_dims(image_resized, axis=2)\n            y[idx,] = mask\n        \n        # normalize Y\n        y = (y > 0).astype(int)\n            \n        return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a dict of all the masks\nmasks = {}\nfor index, row in train_df[train_df['EncodedPixels']!=-1].iterrows():\n    masks[row['ImageId_ClassId']] = row['EncodedPixels']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_image_ids = train_df['ImageId'].unique()\nX_train, X_val = train_test_split(train_image_ids, test_size=val_size, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'img_h': img_h,\n          'img_w': img_w,\n          'image_dir': train_image_dir,\n          'batch_size': batch_size,\n          'shuffle': True}\n\n# Get Generators\ntraining_generator = DataGenerator(X_train, masks, mode='train', **params)\nvalidation_generator = DataGenerator(X_val, masks, mode='validation', **params)\n\n# training_generator = DataGenerator(X_train, masks, **params)\n# validation_generator = DataGenerator(X_val, masks, **params)\n\nx, y = training_generator.__getitem__(0)\nprint(x.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.layers import Input, Conv2D, Add, Flatten, UpSampling2D, Concatenate, Activation,BatchNormalization\ndef resnet_block(inputs,  filters):\n    x = conv_block(inputs, filters)\n    x = conv_block(x, filters)\n    x = Add()([inputs, x])\n    return x\n\ndef downsample(input, filters, kernel=3, stride=2):    \n    x = conv_block(input,filters, stride=2)\n    return x\n\ndef upsample_conv(input, filters, rate=2):    \n    # downsample\n    x = BatchNormalization()(input)\n    x = Activation('relu')(x)    \n    x = UpSampling2D((rate, rate))(x)\n    #x = keras.layers.Conv2D(filters, 3, strides=(1,1), padding='same', \n    #                        use_bias=False,kernel_initializer='he_uniform')(x)\n    return x\n\ndef concat_conv2x2(b,e2,e4,filters):\n    x = Concatenate(axis=-1)([b, e2])\n    y = Concatenate(axis=-1)([x, e4])\n    out = conv_block(y, filters, stride = 1, kernel = 3)\n    return out\n\ndef concat_conv(a,b,filters):\n    x = Concatenate()([a, b])\n    out = conv_block(x, filters, stride = 1, kernel = 1)\n    return out\n\ndef upsample_concat(input, y, filters):\n    x = upsample_conv(input, filters, rate=2)\n    #return layers.Concatenate()([x, y])\n    #return layers.Add()([x, y])\n    return concat_conv(x,y,filters)\n\ndef upsample_downsample_add(b, e4, e2, filters, rate=4):\n    #print('This before b is', b.shape)\n    #print('Filters', filters)\n    b = upsample_conv(b,filters)\n    e2 = conv_block(e2, filters, stride = 4, kernel = 5)\n    #print('This upscaled b is', b.shape)\n    #print('This e2 is', e2.shape)\n    #print('This e4 is', e4.shape)\n    \n    #add =  layers.Add()([b, y])\n    #return layers.Concatenate()([add, x])\n    #return layers.Add()([add, x])\n    return concat_conv2x2(b,e2,e4,filters)\n    \n\ndef upsample_upsample_add(d2, e2, e4, filters, rate=2):  \n    #print('This d2 before is', d2.shape)\n    #print('This e2 before is', e2.shape)\n    d2 = upsample_conv(d2, filters)\n    e4 = upsample_conv(e4, filters, rate=4)\n    #print('This d2 is', d2.shape)\n    #print('This e2 is', e2.shape)\n    #print('This e4 is', e4.shape)\n    #add =  layers.Add()([x, y])\n    #return layers.Concatenate()([add, input])\n    #return layers.Add()([add, input])\n    return concat_conv2x2(d2, e2, e4,filters)\n    \n\n\ndef conv_block(input, filters, stride = 1, kernel = 3): \n    x = BatchNormalization()(input)\n    x = Activation('relu')(x)    \n    x = Conv2D(filters, kernel, strides = stride, padding='same', use_bias=False, \n                      kernel_initializer='he_normal')(x)\n    return x\n\ndef first_block(input, filters, stride = 1, kernel=3):\n    #print(input.shape)\n    #x = Conv2D(filters=filters, kernel_size=kernel_size , padding='same', strides=stride)(inputs)\n    x = Conv2D(filters, kernel, strides=stride, padding='same', use_bias=False,kernel_initializer='he_normal')(input)\n   # x = layers.Conv2D(filters, kernel, strides = (stride, stride), padding='same', use_bias=False, \n   #                   kernel_initializer='he_normal')(input)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    return x\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\ndef ResUNet(img_h, img_w):\n    f = [16, 32, 64, 128, 256, 512]\n    inputs = Input((img_h, img_w, 1))\n    \n    ## Encoder\n    e0 = inputs\n    e1_ = first_block(e0, f[0])\n    e1 = resnet_block(e1_, f[0])\n    \n    e2_ = downsample(e1_, f[1])\n    e2 = resnet_block(e2_, f[1])\n    #print('Shape of e2 is', e2)\n    \n    e3_ = downsample(e2, f[2])\n    e3 = resnet_block(e3_, f[2])\n    \n    e4_ = downsample(e3, f[3])\n    e4 = resnet_block(e4_, f[3])\n    \n    e5 = downsample(e4, f[4])\n    e5_ = resnet_block(e5, f[4])\n    \n    \n    ## Bridge\n    b0 = conv_block(e5_, f[5])\n    b1 = conv_block(b0, f[5])\n    #b1 = b0\n    \n    ## Decoder\n    u1 = upsample_downsample_add(b1, e4, e2, f[4])\n    d1 = resnet_block(u1, f[4])\n    \n    u2 = upsample_concat(d1, e3, f[3])\n    d2 = resnet_block(u2, f[3])\n    \n    u3 = upsample_upsample_add(d2, e2, e4, f[2])\n    d3 = resnet_block(u3, f[2])\n    \n    u4 = upsample_concat(d3, e1,f[1])\n    d4 = resnet_block(u4, f[1])\n    \n    outputs = BatchNormalization()(d4)\n    outputs = Conv2D(filters = 4, strides = (1, 1), kernel_size = 3, padding=\"same\", activation='sigmoid')(outputs)\n    \n    #outputs = Activation('sigmoid')(outputs)\n    model = Model(inputs, outputs)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nmodel = ResUNet(img_h=img_h, img_w=img_w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow\nfrom keras.layers import Flatten\ndef tversky(y_true, y_pred, smooth=1e-6):\n    y_true_pos = tf.keras.layers.Flatten()(y_true)\n    y_pred_pos = Flatten()(y_pred)\n    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos)\n    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n    false_pos = tf.reduce_sum((1-y_true_pos)*y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n\ndef tversky_loss(y_true, y_pred):\n    return 1 - tversky(y_true,y_pred)\n\ndef focal_tversky_loss(y_true,y_pred):\n    pt_1 = tversky(y_true, y_pred)\n    gamma = 0.75\n    return tf.keras.backend.pow((1-pt_1), gamma)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\nadam = keras.optimizers.Adam(lr = 0.05, epsilon = 0.1)\nmodel.compile(optimizer='adam', loss=focal_tversky_loss, metrics=[tversky])\n#filepath = \"/kaggle/output/saved-model-{epoch:02d}-{val_acc:.2f}.hdf5\"\n#checkpoint = ModelCheckpoint(filepath, monitor='val_tversky', verbose=1, period=1, save_best_only=False, mode='max')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_path = '../input/pretrainedmodel/bestmodel2.hdf5'\nload_pretrained_model = True\nif load_pretrained_model:\n    try:\n        model.load_weights(model_path)\n        print('Model loaded successfully')\n    except OSError:\n        print('Error in model loading')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=epochs, verbose=1,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"/kaggle/working/bestmodel3.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=epochs, verbose=1,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"/kaggle/working/bestmodel4.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=epochs, verbose=1,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"/kaggle/working/bestmodel3.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# return tensor in the right shape for prediction \ndef get_test_tensor(img_dir, img_h, img_w, channels=1):\n\n    X = np.empty((1, img_h, img_w, channels))\n    # Store sample\n    image = cv2.imread(img_dir, 0)\n    image_resized = cv2.resize(image, (img_w, img_h))\n    image_resized = np.array(image_resized, dtype=np.float64)\n    # normalize image\n    image_resized -= image_resized.mean()\n    image_resized /= image_resized.std()\n    \n    X[0,] = np.expand_dims(image_resized, axis=2)\n\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage import morphology\n\ndef remove_small_regions(img, size):\n    \"\"\"Morphologically removes small (less than size) connected regions of 0s or 1s.\"\"\"\n    img = morphology.remove_small_objects(img, size)\n    img = morphology.remove_small_holes(img, size)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\n# get all files using glob\ntest_files = [f for f in glob.glob('../input/severstal-steel-defect-detection/test_images/' + \"*.jpg\", recursive=True)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = []\n\n# a function to apply all the processing steps necessery to each of the individual masks\ndef process_pred_mask(pred_mask):\n    \n    pred_mask = cv2.resize(pred_mask.astype('float32'),(1600, 256))\n    pred_mask = (pred_mask > .5).astype(int)\n    pred_mask = remove_small_regions(pred_mask, 0.02 * np.prod(512)) * 255\n    pred_mask = mask_to_rle(pred_mask)\n    \n    return pred_mask\n\n# loop over all the test images\nfor f in test_files:\n    # get test tensor, output is in shape: (1, 256, 512, 3)\n    test = get_test_tensor(f, img_h, img_w) \n    # get prediction, output is in shape: (1, 256, 512, 4)\n    pred_masks = model.predict(test) \n    # get a list of masks with shape: 256, 512\n    pred_masks = [pred_masks[0][...,i] for i in range(0,4)]\n    # apply all the processing steps to each of the mask\n    pred_masks = [process_pred_mask(pred_mask) for pred_mask in pred_masks]\n    # get our image id\n    id = f.split('/')[-1]\n    # create ImageId_ClassId and get the EncodedPixels for the class ID, and append to our submissions list\n    [submission.append((id+'_%s' % (k+1), pred_mask)) for k, pred_mask in enumerate(pred_masks)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to a csv\nsubmission_df = pd.DataFrame(submission, columns=['ImageId_ClassId', 'EncodedPixels'])\n# check out some predictions and see if RLE looks ok\nsubmission_df[ submission_df['EncodedPixels'] != ''].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a look at our submission \nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# write it out\nsubmission_df.to_csv('./submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}