{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Small improvement of most popular public script\n\nThe most popular public srcipt was [mlcomp catalyst infer](http://www.kaggle.com/lightforever/severstal-mlcomp-catalyst-infer-0-90672). We can see that it scores **0.90672 on public** and **0.88780 on private** LB. \n\nYou can notice that this script containes following post processing:\n- for every channel - binarization higher than threshold for this channel;\n- if there are less \"1\" pixels than threshold -> set this channel to all zeros.\n\nThat's ok than you have 1 mask per image, but in this competition often **we have many components in each mask per image**. So it's good to check **every component of mask** on the threshold, not the whole sum of all components in image mask.\n\nHere is post processing method from [another popular inference script](https://www.kaggle.com/rishabhiitbhu/unet-pytorch-inference-kernel) "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport cv2\n\n\ndef post_process(probability, threshold, min_size):\n    \"\"\"\n    Post processing of each predicted mask, components with lesser number of pixels\n    than `min_size` are ignored.\n        \n    probability: np.array\n    threshold: int\n    min_size: int\n    \"\"\"\n    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = np.zeros((256, 1600), np.float32)\n\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            predictions[p] = 1\n\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a result this post processing **removes some noise** from predictions - the case then there are many components but some of them very small and they are just a noise. Original post processing don't drop them as it looks at whole number of pixles not every component.\n\nAfter that we have small improvement on private score from **0.88780 to 0.88830 on private**, but not the public score (**0.90672 to 0.90540**) and that's the problem, you could inspect predictions and check the noise removal - we removed some noise and got worse public LB score - overfitting here we go =)\n\nWith this post processing we got more robust model, and that's good.\n\nUnfortunately submissions were disabled when i wrote this text =( but you can check the private score in the second version of this script."},{"metadata":{},"cell_type":"markdown","source":"## Offline installation of packages"},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"!mkdir -p /tmp/pip/cache/\n!cp /kaggle/input/segmentation-models-zip-003/efficientnet_pytorch-0.4.0.xyz /tmp/pip/cache/efficientnet_pytorch-0.4.0.tar.gz\n!cp /kaggle/input/segmentation-models-zip-003/pretrainedmodels-0.7.4.xyz /tmp/pip/cache/pretrainedmodels-0.7.4.tar.gz\n!cp /kaggle/input/segmentation-models-zip-003/segmentation_models_pytorch-0.0.3.xyz /tmp/pip/cache/segmentation_models_pytorch-0.0.3.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"!pip install --no-index --find-links /tmp/pip/cache/ efficientnet-pytorch\n!pip install --no-index --find-links /tmp/pip/cache/ segmentation-models-pytorch\n!pip install /kaggle/input/tta-pytorch/ttach-0.0.1-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import required libraries"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport matplotlib.pyplot as plt\n\nimport albumentations as albu\nfrom tqdm import tqdm_notebook\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.jit import load\n\nimport ttach as tta\nfrom severstal_utils import rle2mask, mask2rle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load models"},{"metadata":{"trusted":true},"cell_type":"code","source":"unet_se_resnext50_32x4d = \\\n    load('/kaggle/input/severstalmodels/unet_se_resnext50_32x4d.pth').cuda()\nunet_mobilenet2 = load('/kaggle/input/severstalmodels/unet_mobilenet2.pth').cuda()\nunet_resnet34 = load('/kaggle/input/severstalmodels/unet_resnet34.pth').cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MLComp code\n\nHere i copied some code from mlcomp package to not install it fully and save some time while running kernel"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from typing import Callable, Dict\nfrom albumentations import ImageOnlyTransform\n\ndef flip(x, dim):\n    indices = [slice(None)] * x.dim()\n    indices[dim] = torch.arange(x.size(dim) - 1, -1, -1,\n                                dtype=torch.long, device=x.device)\n    return x[tuple(indices)]\n\nclass ChannelTranspose(ImageOnlyTransform):\n    def get_transform_init_args_names(self):\n        return ()\n\n    def get_params_dependent_on_targets(self, params):\n        pass\n\n    def __init__(self, axes=(2, 0, 1)):\n        super().__init__(always_apply=True)\n        self.axes = axes\n\n    def apply(self, img, **params):\n        return np.transpose(img, self.axes)\n\n\nclass ImageDataset(Dataset):\n    def __init__(\n            self,\n            *,\n            img_folder: str,\n            fold_csv: str = None,\n            fold_number: int = None,\n            is_test: bool = False,\n            gray_scale: bool = False,\n            num_classes=2,\n            max_count=None,\n            meta_cols=(),\n            transforms=None,\n            postprocess_func: Callable[[Dict], Dict] = None,\n            include_image_orig=False\n    ):\n        self.img_folder = img_folder\n\n        if fold_csv:\n            df = pd.read_csv(fold_csv)\n            if fold_number is not None:\n                if is_test:\n                    self.data = df[df['fold'] == fold_number]\n                else:\n                    self.data = df[df['fold'] != fold_number]\n            else:\n                self.data = df\n        else:\n            self.data = pd.DataFrame(\n                {'image': os.listdir(img_folder)}).sort_values(by='image')\n\n        self.data = self.data.to_dict(orient='row')\n        if max_count is not None:\n            self.apply_max_count(max_count)\n\n        for row in self.data:\n            self.preprocess_row(row)\n\n        self.transforms = transforms\n        self.gray_scale = gray_scale\n        self.num_classes = num_classes\n        self.meta_cols = meta_cols\n        self.postprocess_func = postprocess_func\n        self.include_image_orig = include_image_orig\n\n    def apply_max_count(self, max_count):\n        if isinstance(max_count, Number):\n            self.data = self.data[:max_count]\n        else:\n            data = defaultdict(list)\n            for row in self.data:\n                data[row['label']].append(row)\n            min_index = np.argmin(max_count)\n            min_count = len(data[min_index])\n            for k, v in data.items():\n                count = int(min_count * (max_count[k] / max_count[min_index]))\n                data[k] = data[k][:count]\n\n            self.data = [v for i in range(len(data)) for v in data[i]]\n\n    def preprocess_row(self, row: dict):\n        row['image'] = os.path.join(self.img_folder, row['image'])\n\n    def __len__(self):\n        return len(self.data)\n\n    def _get_item_before_transform(self, row: dict, item: dict):\n        pass\n\n    def _get_item_after_transform(self, row: dict,\n                                  transformed: dict,\n                                  res: dict):\n        if 'label' in row:\n            res['targets'] = ast.literal_eval(str(row['label']))\n            if isinstance(res['targets'], list):\n                res['targets'] = np.array(res['targets'], dtype=np.float32)\n\n    def __getitem__(self, index):\n        row = self.data[index]\n        image = self.read_image_file(row['image'], self.gray_scale)\n        item = {'image': image}\n\n        self._get_item_before_transform(row, item)\n\n        if self.transforms:\n            item = self.transforms(**item)\n        if self.gray_scale:\n            item['image'] = np.expand_dims(item['image'], axis=0)\n        res = {\n            'features': item['image'].astype(np.float32),\n            'image_file': row['image']\n        }\n        if self.include_image_orig:\n            res['image'] = image\n\n        for c in self.meta_cols:\n            res[c] = row[c]\n\n        self._get_item_after_transform(row, item, res)\n        if self.postprocess_func:\n            res = self.postprocess_func(res)\n        return res\n\n    @staticmethod\n    def read_image_file(path: str, gray_scale=False):\n        if path.endswith('.tiff') and not gray_scale:\n            return tifffile.imread(path)\n        elif path.endswith('.npy'):\n            return np.load(path)\n        else:\n            if gray_scale:\n                img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n                assert img is not None, \\\n                    f'Image at path {path} does not exist'\n                return img.astype(np.uint8)\n            else:\n                img = cv2.imread(path)\n                assert img is not None, \\\n                    f'Image at path {path} does not exist'\n                return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            \n            \nclass TtaWrap(Dataset):\n    def __init__(self, dataset: Dataset, tfms=()):\n        self.dataset = dataset\n        self.tfms = tfms\n\n    def __getitem__(self, item):\n        return self.dataset[item]\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def inverse(self, a: np.array):\n        last_dim = len(a.shape) - 1\n        for t in self.tfms:\n            if isinstance(t, albu.HorizontalFlip):\n                a = flip(a, last_dim)\n            elif isinstance(t, albu.VerticalFlip):\n                a = flip(a, last_dim - 1)\n            elif isinstance(t, albu.Transpose):\n                axis = (0, 1, 3, 2) if len(a.shape) == 4 else (0, 2, 1)\n                a = a.permute(*axis)\n\n        return a","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Models' mean aggregator"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"class Model:\n    def __init__(self, models):\n        self.models = models\n    \n    def __call__(self, x):\n        res = []\n        x = x.cuda()\n        with torch.no_grad():\n            for m in self.models:\n                res.append(m(x))\n        res = torch.stack(res)\n        return torch.mean(res, dim=0)\n\nmodel = Model([unet_se_resnext50_32x4d, unet_mobilenet2, unet_resnet34])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create TTA transforms, datasets, loaders, add correct**** post process"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def create_transforms(additional):\n    res = list(additional)\n    # add necessary transformations\n    res.extend([\n        albu.Normalize(\n            mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n        ),\n        ChannelTranspose()\n    ])\n    res = albu.Compose(res)\n    return res\n\nimg_folder = '/kaggle/input/severstal-steel-defect-detection/test_images'\nbatch_size = 2\nnum_workers = 0\n\n# Different transforms for TTA wrapper\ntransforms = [\n    [],\n    [albu.HorizontalFlip(p=1)]\n]\n\ntransforms = [create_transforms(t) for t in transforms]\ndatasets = [TtaWrap(ImageDataset(img_folder=img_folder, transforms=t), tfms=t) for t in transforms]\nloaders = [DataLoader(d, num_workers=num_workers, batch_size=batch_size, shuffle=False) for d in datasets]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loaders' mean aggregator"},{"metadata":{"trusted":true},"cell_type":"code","source":"thresholds = [0.50, 0.50, 0.50, 0.50]\nmin_area = [600, 600, 1000, 2000]\n\nres = []\n# Iterate over all TTA loaders\ntotal = len(datasets[0])//batch_size\nfor loaders_batch in tqdm_notebook(zip(*loaders), total=total):\n    preds = []\n    image_file = []\n    for i, batch in enumerate(loaders_batch):\n        features = batch['features'].cuda()\n        p = torch.sigmoid(model(features))\n        # inverse operations for TTA\n        p = datasets[i].inverse(p)\n        preds.append(p)\n        image_file = batch['image_file']\n    \n    # TTA mean\n    preds = torch.stack(preds)\n    preds = torch.mean(preds, dim=0)\n    preds = preds.detach().cpu().numpy()\n    \n    # Batch post processing\n    for p, file in zip(preds, image_file):\n        file = os.path.basename(file)\n        # Image postprocessing\n        for i in range(4):\n            p_channel = p[i]\n            imageid_classid = file+'_'+str(i+1)\n            \n            # HERE we change original post processing (commented) \n            p_channel = post_process(p_channel, thresholds[i], min_area[i])\n            #p_channel = (p_channel>thresholds[i]).astype(np.uint8)\n            #if p_channel.sum() < min_area[i]:\n            #    p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n\n            res.append({\n                'ImageId_ClassId': imageid_classid,\n                'EncodedPixels': mask2rle(p_channel)\n            })\n        \ndf = pd.DataFrame(res)\ndf.to_csv('submission.csv', index=False)\t","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.DataFrame(res)\ndf = df.fillna('')\ndf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Histogram of predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"df['Image'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[0])\ndf['Class'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[1])\ndf['empty'] = df['EncodedPixels'].map(lambda x: not x)\ndf[df['empty'] == False]['Class'].value_counts()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2d558d43f12745d8a826eb84f7dcd0b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"60b23d613e584fa0abb75460b84b1d80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3c6cad600e04ebfb71a6c202a8eb513","IPY_MODEL_d6bd450456d24af1b8f635c10fee4445"],"layout":"IPY_MODEL_a7f23b4d7ebb4499b486f861d1c52b77"}},"6feb39965873459cb61d32fbbc2c2225":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f23b4d7ebb4499b486f861d1c52b77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c389718f59fc45eba86cebc46dc72f99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6bd450456d24af1b8f635c10fee4445":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c389718f59fc45eba86cebc46dc72f99","placeholder":"​","style":"IPY_MODEL_dcff928e32c743529e899d038dee4e66","value":" 901/? [05:36&lt;00:00,  2.67it/s]"}},"dcff928e32c743529e899d038dee4e66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3c6cad600e04ebfb71a6c202a8eb513":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6feb39965873459cb61d32fbbc2c2225","max":900,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d558d43f12745d8a826eb84f7dcd0b2","value":900}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}