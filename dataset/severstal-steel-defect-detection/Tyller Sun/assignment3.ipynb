{"cells":[{"metadata":{},"cell_type":"markdown","source":" # **This is assignment 3, which is about detecting mask. **\nIn order to finish this code, I have get a great of idea from other people's notebook.\n*  https://www.kaggle.com/go1dfish/clear-mask-visualization-and-simple-eda\n* https://www.kaggle.com/renzestruiksma/severstal-mask-r-cnn-training-tutorial\n* https://www.kaggle.com/ekhtiar/resunet-a-baseline-on-tensorflow\n* https://www.kaggle.com/aleksandradeis/steel-defect-detection-eda\n* On top of that, I use \nhttps://www.kaggle.com/c14103/keras-starter-u-net-with-pretrained\n as my basis of this project, must of my code I credit by him.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" # Part 1 read in the data and VGG19 model\n it can be download in https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nfrom tqdm import tqdm_notebook\nimport cv2\n\nimport keras\nfrom keras.applications.vgg19 import VGG19\nfrom keras.layers.convolutional import Conv2DTranspose\nfrom keras.layers.merge import concatenate\nfrom keras.layers import UpSampling2D, Conv2D, Activation, Input, Dropout, MaxPooling2D\nfrom keras import layers\nfrom keras import Model\nfrom keras import backend as K\nfrom keras.layers.core import Lambda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = VGG19(weights='imagenet', include_top=False)\n \nfor layer in model.layers:\n    print(layer)\n\nmodel.save(\"severstal_vgg19.h5\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Load_vgg19(img_input):\n    #block 1\n    x = layers.Conv2D(64, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block1_conv1')(img_input)\n    x = layers.Conv2D(64, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block1_conv2')(x)\n    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n    #block 2\n    x = layers.Conv2D(128, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block2_conv1')(x)\n    x = layers.Conv2D(128, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block2_conv2')(x)\n    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n\n    #block 3\n    x = layers.Conv2D(256, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block3_conv1')(x)\n    x = layers.Conv2D(256, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block3_conv2')(x)\n    x = layers.Conv2D(256, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block3_conv3')(x)\n    \n    x = layers.Conv2D(256, (3, 3),\n                          activation='relu',\n                          padding='same',\n                          name='block3_conv4')(x)\n    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n\n    #block 4\n    x = layers.Conv2D(512, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block4_conv1')(x)\n    x = layers.Conv2D(512, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block4_conv2')(x)\n    x = layers.Conv2D(512, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block4_conv3')(x)\n   \n    x = layers.Conv2D(512, (3, 3),\n                          activation='relu',\n                          padding='same',\n                          name='block4_conv4')(x)\n    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n\n    #block 5\n    x = layers.Conv2D(512, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block5_conv1')(x)\n    x = layers.Conv2D(512, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block5_conv2')(x)\n    x = layers.Conv2D(512, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block5_conv3')(x)\n  \n    x = layers.Conv2D(512, (3, 3),\n                          activation='relu',\n                          padding='same',\n                          name='block5_conv4')(x)\n    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n\n    model = Model(inputs=[img_input], outputs=x)\n\n    # Load pretrained\n    pretrained_model = VGG19(include_top=False)\n\n    for layer, pretrained_layer in zip(\n            model.layers[2:], pretrained_model.layers[2:]):\n        layer.set_weights(pretrained_layer.get_weights())\n        \n    imagenet_weights = pretrained_model.layers[1].get_weights()\n    init_bias = imagenet_weights[1]\n    init_kernel = np.average(imagenet_weights[0], axis=2)\n    #print(init_kernel.shape)\n    init_kernel = np.reshape(\n        init_kernel,\n        (init_kernel.shape[0],\n            init_kernel.shape[1],\n            1,\n            init_kernel.shape[2]))\n    init_kernel = np.dstack([init_kernel] * img_input.shape.as_list()[-1])  # input image is grayscale\n    model.layers[1].set_weights([init_kernel, init_bias])\n    return model\n\n\nif 1:\n    img = layers.Input(shape=(128, 128, 4))\n    pretrained = Load_vgg19(img)\n    for layer in pretrained.layers:\n        print(layer.name, pretrained.get_layer(layer.name))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"instead of only using VGG19, I have add several layer in it, below is the summary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = pd.read_csv('/kaggle/input/assignment-3/QBS assignment 3/train.csv')\nprint(len(tr))\ntr.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA\nby using groupby, I found out that the data is quite skewwed, which there exist a great deal of classid 3 data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Classid_ = tr.groupby(\"ClassId\").count() / len(tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Classid_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"since the image can't be read without .JPG, so I change the imageId and add \"JPG\" to it . On top of that, I also seperate it into four different column, which make me train the data easier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_train = tr[tr['EncodedPixels'].notnull()].reset_index(drop=True)\n#print(len(df_train))\ndf_train = tr\nfor i in range(len(df_train[\"ImageId\"])):\n    df_train[\"ImageId\"][i] =  str(df_train[\"ImageId\"][i]) + \".JPG\"\n#df_train['ImageId'], df_train['ClassId'] = zip(*df_train['ImageId_ClassId'].str.split('_')) #split imageId and classId\ndf_train['ClassId'] = df_train['ClassId'].astype(int)\ndf_train = df_train.pivot(index='ImageId',columns='ClassId',values='EncodedPixels') #remap\ndf_train['defects'] = df_train.count(axis=1) #count on defect type\ndf_train = df_train[df_train['defects'] > 0]\nprint(len(df_train))\ndf_train.head()\n#print(df_train.iloc[6666-1].name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # put mask to the picture\n what I have done is to put the mask part into different color, and the place without mask into purple","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle2mask(rle, imgshape):\n    width = imgshape[0]\n    height= imgshape[1]\n    \n    mask= np.zeros( width*height ).astype(np.uint8)\n    if rle is not np.nan:\n        array = np.asarray([int(x) for x in rle.split()])\n        starts = array[0::2]\n        lengths = array[1::2]\n\n        current_position = 0\n        for index, start in enumerate(starts):\n            mask[int(start):int(start+lengths[index])] = 1\n            current_position += lengths[index]\n        \n    return np.flipud( np.rot90( mask.reshape(height, width), k=1 ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_scale = 2\nimg_size = (1600 // img_scale,256 // img_scale)\nclasses_num = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#contrast enhancing\n\ndo_enhance = True\n\ngamma = 1.2\ninverse_gamma = 1.0 / gamma\nlook_up_table = np.array([((i/255.0) ** inverse_gamma) * 255.0 for i in np.arange(0,256,1)]).astype(\"uint8\")\nclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n\ndef contrast_enhancement(img):\n    if not do_enhance:\n        return img\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n    img[:,:,0] = clahe.apply(img[:,:,0])\n    img = cv2.cvtColor(img, cv2.COLOR_YUV2RGB)\n    return img\n\ndef gamma_correction(img):\n    if not do_enhance:\n        return img\n    return cv2.LUT(img.astype('uint8'), look_up_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.iloc[2].name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ndef keras_generator(batch_size):\n    while True:\n        x_batch = []\n        y_batch = []\n        \n        for i in range(batch_size): \n            flip = int(100)\n            if random.uniform(0,1) > 0.5:\n                flip = random.randint(-1,1)\n            #print(flip)\n                \n            \n            fn = df_train.iloc[i].name\n            print(fn)\n            img = cv2.imread( '/kaggle/input/assignment-3/QBS assignment 3/train_images/'+fn )\n            #plt.subplot(3,1,1)\n            #plt.imshow(img)\n            img = gamma_correction(img)\n            #plt.subplot(3,1,2)\n            #plt.imshow(img)\n            img = contrast_enhancement(img)\n            #plt.subplot(3,1,3)\n            #plt.imshow(img)\n            #break\n            #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)     \n            #mask_gt = np.zeros( shape = img.shape[0:2] ).astype(np.uint8) + 1\n            #mask_gt = cv2.resize(mask_gt, img_size,cv2.INTER_NEAREST)\n            #if flip != 100:\n            #    mask_gt = cv2.flip(mask_gt,flip)\n            #mask_gt = np.expand_dims(mask_gt,-1) #background\n            #masks = [mask_gt]\n            masks = []\n            for cls in range(0,classes_num):\n                mask = rle2mask(df_train[cls+1].iloc[i], img.shape)\n                mask = np.squeeze(mask)\n                mask = cv2.resize(mask, img_size,cv2.INTER_NEAREST)\n                if flip != 100:\n                    mask = cv2.flip(mask,flip)\n                mask = np.expand_dims(mask,-1)\n               # mask_gt[mask != 0] = 0 #move pixel from gt\n                masks.append(mask)\n\n            mask = np.concatenate(masks,axis=-1)\n            img = cv2.resize(img, img_size,cv2.INTER_AREA)\n            if flip != 100:\n                img = cv2.flip(img,flip)\n            x_batch += [img]\n            y_batch += [mask]\n                                    \n        x_batch = np.array(x_batch) / 255.0\n        y_batch = np.array(y_batch)\n\n        #yield x_batch, np.expand_dims(y_batch, -1)\n        yield x_batch, y_batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x, y in keras_generator(4):\n    break\n    \nprint(x.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this is the result I have dome","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_image_id = 3\nplt.subplot(classes_num+1,1,1)\nplt.imshow(x[test_image_id])\nfor k in range(classes_num):\n    plt.subplot(classes_num+1,1,k+2)\n    plt.imshow(np.squeeze(y[test_image_id,:,:,k]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def segment_cross_entropy(features, labels):\n    features = K.softmax(features, axis=1)\n    features = K.reshape(features,[-1])\n    labels = K.reshape(labels,[-1])\n    return K.categorical_crossentropy(labels, features, axis=-1)\n        \n#true_dist, coding_dist = np.ones((4,5,6,6))/2, np.ones((4,5,6,6))/2\n#loss = segment_cross_entropy(coding_dist, coding_dist)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we now add new model in it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model\ndef get_net_raw(img_size,classes_num):\n    inputs = Input((img_size[1], img_size[0], 3))\n    #s = Lambda(lambda x: x / 255) (inputs)\n\n    c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (inputs)\n    c1 = Dropout(0.1) (c1)\n    c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n    p1 = MaxPooling2D((2, 2)) (c1)\n\n    c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n    c2 = Dropout(0.1) (c2)\n    c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n    p2 = MaxPooling2D((2, 2)) (c2)\n\n    c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n    c3 = Dropout(0.2) (c3)\n    c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n    p3 = MaxPooling2D((2, 2)) (c3)\n\n    c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n    c4 = Dropout(0.2) (c4)\n    c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\n    c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n    c5 = Dropout(0.3) (c5)\n    c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n\n    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n    u6 = concatenate([u6, c4])\n    c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n    c6 = Dropout(0.2) (c6)\n    c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n\n    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n    u7 = concatenate([u7, c3])\n    c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n    c7 = Dropout(0.2) (c7)\n    c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n\n    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n    u8 = concatenate([u8, c2])\n    c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n    c8 = Dropout(0.1) (c8)\n    c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n\n    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n    u9 = concatenate([u9, c1], axis=3)\n    c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n    c9 = Dropout(0.1) (c9)\n    c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n\n    #outputs = Conv2D(4, (1, 1), activation='sigmoid') (c9)\n    outputs = Conv2D(classes_num,(1,1),activation = 'sigmoid')(c9)  #todos: try softmax\n\n    model = Model(inputs=[inputs], outputs=[outputs])\n    #model.compile(optimizer='adam', loss='binary_crossentropy')\n    #model.compile(optimizer='adam',loss='categorical_crossentropy')\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model\nfrom keras import optimizers\ndef get_net_pretrained(img_size,classes_num):\n    layers_feat = 'block1_conv2,block2_conv2,block3_conv4,block4_conv4,block5_conv4'.split(',')\n    \n    inputs = Input((img_size[1], img_size[0], 3))\n    #s = Lambda(lambda x: x / 255) (inputs)\n    pretrained = Load_vgg19(inputs)\n    for layer in pretrained.layers:\n        layer.trainable=False #freeze\n    c1 = pretrained.get_layer(layers_feat[0]).output #1x\n    c2 = pretrained.get_layer(layers_feat[1]).output #2x\n    c3 = pretrained.get_layer(layers_feat[2]).output #4x\n    c4 = pretrained.get_layer(layers_feat[3]).output #8x\n    c5 = pretrained.get_layer(layers_feat[4]).output #16x\n    c5 = Dropout(0.3) (c5)\n    \n    #decoder\n    u6 = Conv2DTranspose(128*2, (2, 2), strides=(2, 2), padding='same') (c5) #8x\n    u6 = concatenate([u6, c4])\n    c6 = Conv2D(128*2, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n    c6 = Dropout(0.2) (c6)\n    c6 = Conv2D(128*2, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n\n    u7 = Conv2DTranspose(64*2, (2, 2), strides=(2, 2), padding='same') (c6) #4x\n    u7 = concatenate([u7, c3])\n    c7 = Conv2D(64*2, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n    c7 = Dropout(0.2) (c7)\n    c7 = Conv2D(64*2, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n\n    u8 = Conv2DTranspose(32*2, (2, 2), strides=(2, 2), padding='same') (c7) #2x\n    u8 = concatenate([u8, c2])\n    c8 = Conv2D(32*2, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n    c8 = Dropout(0.1) (c8)\n    c8 = Conv2D(32*2, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n\n    u9 = Conv2DTranspose(16*2, (2, 2), strides=(2, 2), padding='same') (c8) #1x\n    u9 = concatenate([u9, c1], axis=3)\n    c9 = Conv2D(16*2, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n    c9 = Dropout(0.1) (c9)\n    c9 = Conv2D(16*2, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n\n    #outputs = Conv2D(4, (1, 1), activation='sigmoid') (c9)\n    outputs = Conv2D(classes_num,(1,1),activation = 'sigmoid')(c9)  #todos: try softmax\n\n    model = Model(inputs=[inputs], outputs=[outputs])\n\n    #model.compile(optimizer='adam', loss='binary_crossentropy')\n    #model.compile(optimizer='adam',loss='categorical_crossentropy')\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_net_pretrained(img_size,classes_num)\nsgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(optimizer=sgd, loss='binary_crossentropy')\nfor layer in model.layers:\n    print(layer.name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training\nwe now feed out data into the model, here we use keras generator. The function of keras_generator is to find the image and put mask onit, which let me to train the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Fit model\nbatch_size = 32\nresults = model.fit_generator(keras_generator(batch_size), \n                              steps_per_epoch=100,\n                              epochs=1) \n\nmodel.save(\"severstal_s4.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we try to test whether out model can work. x is an array which has no important meaning. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(x)\nprint(pred.shape,)\nplt.imshow(np.squeeze(pred[3,:,:,3]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testfiles=os.listdir(\"/kaggle/input/assignment-3/QBS assignment 3/test_images/\")\nlen(testfiles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nif 0:\n    import gc\n    #del pred\n    #del x\n    gc.collect()\n    #print(testfiles)\n    test_img = []\n    for fn in tqdm_notebook(testfiles):\n            img = cv2.imread( '../input/test_images/'+fn )\n            img = cv2.resize(img,img_size) \n            print(img)\n            test_img.append(img)\n\n    print(len(test_img))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef mask2rle(img):\n    tmp = np.rot90( np.flipud( img ), k=3 )\n    rle = []\n    lastColor = 0;\n    startpos = 0\n    endpos = 0\n\n    tmp = tmp.reshape(-1,)   \n    inds = np.argwhere(tmp == 1)\n    if len(inds) == 0:\n        return ' '.join([])\n    inds = list(map(lambda x: x[0], inds))\n    last = inds[0]\n   # pdb.set_trace()\n    for k in range(1,len(inds)):\n        if inds[k] == inds[k-1] + 1:\n            continue\n        rle.append( str(last)+' '+str(inds[k-1]-last+1) )\n        last = inds[k]\n    return \" \".join(rle)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"below are all the test file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"testfiles","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we now create a submission file including imageid, classid, and encodedpixals","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission[\"ImageId\"] = 0\nsubmission[\"ClassId\"] = 0\nsubmission[\"EncodedPixels\"] = 0\nsubmission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"below we predict the testing image, which I use too criteria\n1. the prediction score is >0.9\n2. the total score of the image >3500 (since the mask will happen in a large area\n> the reason why I use this criteria is due to my experience, since I have sent it to TA mailbox several time, and I found that this score work well","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport pdb\nthresh_score = 0.9\nthresh_num = 3500\npred_rle = []\n\ntest_img = []\nfor fn in tqdm_notebook(testfiles):\n        #print('/kaggle/input/assignment-3/QBS assignment 3/test_images'+fn)\n        img = cv2.imread( '/kaggle/input/assignment-3/QBS assignment 3/test_images/'+fn )\n        #print(img)\n        img = gamma_correction(img)\n        img = contrast_enhancement(img)\n        img = cv2.resize(img,img_size)       \n        #test_img.append(img)   \n        scores = model.predict(np.asarray([img]))\n        #print(scores)\n        scores = np.squeeze(scores)\n        pred = np.argmax(scores,axis=-1)\n        #print(scores.shape)\n        for cls in range(0, classes_num):\n            submission\n            mask = np.squeeze(pred == cls).astype(np.uint8)\n            score = scores[:,:,cls]\n            #print(mask.shape,'---')\n            mask[score < thresh_score] = 0\n            if np.sum(mask) < thresh_num:\n                mask = mask * 0\n            mask = cv2.resize(mask, (1600, 256), cv2.INTER_NEAREST)\n            rle = mask2rle(mask)\n            s = pd.Series({'ImageId':fn, 'EncodedPixels':rle, 'ClassId': cls})\n            submission = submission.append(s, ignore_index=True)\n            print(rle)\n            pred_rle.append(rle)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(list(submission[\"EncodedPixels\"])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(submission)):\n    submission[\"ImageId\"][i] = submission[\"ImageId\"][i].split(\".\")[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* I drop out the row which has no EncodedPixels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(submission)):\n    if submission[\"EncodedPixels\"][i] == \"\":\n        submission = submission.drop([i])\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}