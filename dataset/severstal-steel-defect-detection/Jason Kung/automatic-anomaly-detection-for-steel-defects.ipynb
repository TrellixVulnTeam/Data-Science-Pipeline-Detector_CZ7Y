{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary library packages\n\nimport cv2 \nimport keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport pandas_profiling as pp\nimport tensorflow as tf\nimport time\n\nfrom skimage.io import imread\nfrom sklearn import metrics, preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Input, Convolution2D, GlobalAveragePooling2D, Activation, Dense, BatchNormalization, Dropout, MaxPooling2D, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.python.keras.engine.base_layer import Layer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Severstal Steel Dataset","metadata":{}},{"cell_type":"markdown","source":"**1. Data Visualisation**","metadata":{}},{"cell_type":"code","source":"# Define directories for training and test images\n# Display the training and sample submission CSV file\n\ntrain_dir = '/kaggle/input/severstal-steel-defect-detection/train_images'\ntest_dir = '/kaggle/input/severstal-steel-defect-detection/test_images'\ntrain_df = pd.read_csv('/kaggle/input/severstal-steel-defect-detection/train.csv')\nsubmission_df = pd.read_csv('/kaggle/input/severstal-steel-defect-detection/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display dataframe for training images\n\n# pd.set_option('display.max_rows', None)\ntrain_df.head(len(train_df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a profile report of the training dataset\n\npp.ProfileReport(train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary of the training dataset\n\ntrain_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display dataframe for testing images\n\n# pd.set_option('display.max_rows', None)\nsubmission_df.head(len(submission_df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read a sample training image with its information displayed\n\ndisplay(train_df.iloc[1111])\nimage = imread(train_dir + '/' + train_df['ImageId'][1111])\n# image = imread(train_dir + '/' + '282cd397d.jpg')\n\n#The size of original image is (256, 1600)\n\nplt.figure(figsize = None) \nplt.imshow(image)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the dimension information for a sample training image\n\ninput_shape = image.shape\ninput_shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read multiple samples of training images\n\ntraining = []\ntraining.append(imread(train_dir + '/' + train_df['ImageId'][1111]))\ntraining.append(imread(train_dir + '/' + train_df['ImageId'][2222]))\ntraining.append(imread(train_dir + '/' + train_df['ImageId'][3333]))\ntraining.append(imread(train_dir + '/' + train_df['ImageId'][4444]))\ntraining.append(imread(train_dir + '/' + train_df['ImageId'][5555]))\n\nlabels = [] \nlabels.append(train_df['ClassId'][1111])\nlabels.append(train_df['ClassId'][2222])\nlabels.append(train_df['ClassId'][3333])\nlabels.append(train_df['ClassId'][4444])\nlabels.append(train_df['ClassId'][5555])\n\n# Resized training images\n\nplt.figure(figsize = [25,4]) \n\nfor x in range(0,5):\n    plt.subplot(3, 3,x + 1)\n    plt.subplots_adjust(hspace = 1.0)\n    plt.imshow(training[x])\n    plt.title(\"Defect Class: {}\".format(labels[x]))\n    x += 1\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the dict with classId and Encoded pixels and group all together\n# Creating the dict\n# train_df['ClassId_EncodedPixels'] = train_df.apply(lambda row: [row['ClassId'], row['EncodedPixels']], axis = 1)\n\n# Grouping together\n# grouped_EncodedPixels = train_df.groupby('ImageId')['ClassId_EncodedPixels'].apply(list)\n\n# Display the respective information of the training dataset\nprint('Number of unique images: %s' % len(train_df['ImageId'].drop_duplicates()))\nprint('Number of images having at least one defect: %s' % len(train_df[train_df['EncodedPixels'] != -1]['ImageId'].unique()))\nprint('Total training samples: %s' % len(train_df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the bar graph the number of training images for each class\n\ntrain_df[\"ClassId\"].value_counts().plot(kind = 'barh')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain the number of training images for each class\n\nclass_3, class_1, class_4, class_2 = train_df['ClassId'].value_counts()\nprint(\"Number of Class 1 defects: \", class_1)\nprint(\"Number of Class 2 defects: \", class_2)\nprint(\"Number of Class 3 defects: \", class_3)\nprint(\"Number of Class 4 defects: \", class_4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. Train-test Split**","metadata":{}},{"cell_type":"code","source":"# Function for getting the training ImageId in array form\n\ndef get_ImageArray():\n    \n    # Looping counter\n    image_index = 0\n    \n    # Obtain the total number of images in training set\n    total_image_rows = len(train_df['ImageId'])\n    \n    # Create an empty array named labels for inserting the ImageId\n    images = []\n    \n    # Append each image into the images array and iterate until the total number of training images\n    for image_index in range(total_image_rows):\n        img_arr = cv2.imread(train_dir + '/' + train_df['ImageId'][image_index])\n        img_arr = cv2.resize(img_arr, (120,120))\n        images.append(img_arr)\n        image_index += 1\n        \n    # Return the labels as the function output\n    return images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assigning the output of get_ImageArray() function to variable images\n\nimages = get_ImageArray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for getting the ClassId in array form\n\ndef get_ClassId():\n    \n    # Looping counter\n    class_index = 0\n    \n    # Obtain the total number of images in training set\n    total_class_rows = len(train_df['ClassId'])\n    \n    # Create an empty array named labels for inserting the ClassId\n    class_labels = []\n    \n    # Append each label into the labels array and iterate until the total number of training images\n    for class_index in range(total_class_rows):\n        class_labels.append(train_df['ClassId'][class_index])\n        class_index += 1\n        \n    # Return the labels as the function output\n    return class_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assigning the output of get_ClassId() function to variable class_labels\n\nclass_labels = get_ClassId()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n\nencoder = LabelEncoder()\n\nX = np.array(images)\nX = X/255\n\nY = encoder.fit_transform(class_labels)\nY = to_categorical(Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Divide the training images into training set and test set\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size = 0.15, shuffle = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain the splitting results for the training set and test set\n\nprint(\"x_train shape:\", X_train.shape)\nprint(\"x_valid shape:\", X_valid.shape)\nprint(\"y_train shape:\", Y_train.shape)\nprint(\"y_valid shape:\", Y_valid.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. Methodology 1 - EffcientNetV2**","metadata":{}},{"cell_type":"code","source":"# https://colab.research.google.com/github/google/automl/blob/master/efficientnetv2/tfhub.ipynb#scrollTo=9f3yBUvkd_VJ\n\nimport itertools\nimport os\n\nimport matplotlib.pylab as plt\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nprint('TF version:', tf.__version__)\nprint('Hub version:', hub.__version__)\nprint('Physical devices:', tf.config.list_physical_devices())\n\ndef get_hub_url_and_isize(model_name, ckpt_type, hub_type):\n  if ckpt_type == '1k':\n    ckpt_type = ''  # json doesn't support empty string\n  else:\n    ckpt_type = '-' + ckpt_type  # add '-' as prefix\n  \n  hub_url_map = {\n    'efficientnetv2-b0': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b0/{hub_type}',\n    'efficientnetv2-b1': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b1/{hub_type}',\n    'efficientnetv2-b2': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b2/{hub_type}',\n    'efficientnetv2-b3': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b3/{hub_type}',\n    'efficientnetv2-s':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-s/{hub_type}',\n    'efficientnetv2-m':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-m/{hub_type}',\n    'efficientnetv2-l':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-l/{hub_type}',\n\n    'efficientnetv2-b0-21k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b0-21k/{hub_type}',\n    'efficientnetv2-b1-21k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b1-21k/{hub_type}',\n    'efficientnetv2-b2-21k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b2-21k/{hub_type}',\n    'efficientnetv2-b3-21k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b3-21k/{hub_type}',\n    'efficientnetv2-s-21k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-s-21k/{hub_type}',\n    'efficientnetv2-m-21k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-m-21k/{hub_type}',\n    'efficientnetv2-l-21k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-l-21k/{hub_type}',\n    'efficientnetv2-xl-21k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-xl-21k/{hub_type}',\n\n    'efficientnetv2-b0-21k-ft1k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b0-21k-ft1k/{hub_type}',\n    'efficientnetv2-b1-21k-ft1k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b1-21k-ft1k/{hub_type}',\n    'efficientnetv2-b2-21k-ft1k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b2-21k-ft1k/{hub_type}',\n    'efficientnetv2-b3-21k-ft1k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b3-21k-ft1k/{hub_type}',\n    'efficientnetv2-s-21k-ft1k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-s-21k-ft1k/{hub_type}',\n    'efficientnetv2-m-21k-ft1k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-m-21k-ft1k/{hub_type}',\n    'efficientnetv2-l-21k-ft1k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-l-21k-ft1k/{hub_type}',\n    'efficientnetv2-xl-21k-ft1k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-xl-21k-ft1k/{hub_type}',\n      \n    # efficientnetv1\n    'efficientnet_b0': f'https://tfhub.dev/tensorflow/efficientnet/b0/{hub_type}/1',\n    'efficientnet_b1': f'https://tfhub.dev/tensorflow/efficientnet/b1/{hub_type}/1',\n    'efficientnet_b2': f'https://tfhub.dev/tensorflow/efficientnet/b2/{hub_type}/1',\n    'efficientnet_b3': f'https://tfhub.dev/tensorflow/efficientnet/b3/{hub_type}/1',\n    'efficientnet_b4': f'https://tfhub.dev/tensorflow/efficientnet/b4/{hub_type}/1',\n    'efficientnet_b5': f'https://tfhub.dev/tensorflow/efficientnet/b5/{hub_type}/1',\n    'efficientnet_b6': f'https://tfhub.dev/tensorflow/efficientnet/b6/{hub_type}/1',\n    'efficientnet_b7': f'https://tfhub.dev/tensorflow/efficientnet/b7/{hub_type}/1',\n  }\n  \n  image_size_map = {\n    'efficientnetv2-b0': 224,\n    'efficientnetv2-b1': 240,\n    'efficientnetv2-b2': 260,\n    'efficientnetv2-b3': 300,\n    'efficientnetv2-s':  384,\n    'efficientnetv2-m':  480,\n    'efficientnetv2-l':  480,\n    'efficientnetv2-xl':  512,\n  \n    'efficientnet_b0': 224,\n    'efficientnet_b1': 240,\n    'efficientnet_b2': 260,\n    'efficientnet_b3': 300,\n    'efficientnet_b4': 380,\n    'efficientnet_b5': 456,\n    'efficientnet_b6': 528,\n    'efficientnet_b7': 600,\n  }\n  \n  hub_url = hub_url_map.get(model_name + ckpt_type)\n  image_size = image_size_map.get(model_name, 224)\n  return hub_url, image_size\n\n\ndef get_imagenet_labels(filename):\n  labels = []\n  with open(filename, 'r') as f:\n    for line in f:\n      labels.append(line.split('\\t')[1][:-1])  # split and remove line break.\n  return labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hub_url = 'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b0/feature-vector'\ndo_fine_tuning = True\nmodel = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(input_shape = [224, 224, 3]),\n    hub.KerasLayer(hub_url, trainable = do_fine_tuning),\n    tf.keras.layers.Dropout(rate = 0.2),\n    tf.keras.layers.Dense(4, activation = 'softmax'),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n  optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9), \n  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n  metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps_per_epoch = 189\nvalidation_steps = 34","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 25 #@param {type:\"integer\"}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, Y_train, epochs = num_epochs, steps_per_epoch = steps_per_epoch, validation_data = (X_valid, Y_valid), validation_steps = validation_steps, batch_size = 32, verbose = 1).history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the graphs for the metrics of interest\n\nacc = history['accuracy']\nval_acc = history['val_accuracy']\nloss = history['loss']\nval_loss = history['val_loss']\n\nepochs = range(1,len(acc) + 1)\n\nplt.plot(epochs, acc,'r',label = 'Training Accuracy')\nplt.plot(epochs,val_acc,'b',label = 'Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs,loss,'r',label = 'Training loss')\nplt.plot(epochs,val_loss,'b',label = 'Validation Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.tensorflow.org/api_docs/python/tf/keras/Model\n# print(model.metrics_names)\n# compute predictions\n\nresults = model.evaluate(X_valid, Y_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for getting the testing ImageId in array form\n\ndef get_testImageArray():\n    \n    # Looping counter\n    image_index = 0\n    \n    # Obtain the total number of images in training set\n    total_image_rows = len(submission_df['ImageId'])\n    \n    # Create an empty array named labels for inserting the ImageId\n    test_images = []\n    \n    # Append each image into the images array and iterate until the total number of training images\n    for image_index in range(total_image_rows):\n        testimg_arr = cv2.imread(test_dir + '/' + submission_df['ImageId'][image_index])\n        testimg_arr = cv2.resize(testimg_arr, (120,120))\n        test_images.append(testimg_arr)\n        image_index += 1\n        \n    # Return the labels as the function output\n    return test_images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = get_testImageArray()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n\nencoder = LabelEncoder()\n\nX_test = np.array(X_test)\nX_test = X_test/255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(X_test)\npredictions = np.argmax(pred, axis = 1)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = ['1','2','3','4']\nclasses = np.array(classes)\npred_name = classes[predictions]\nprint(pred_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['ClassId'] = pred_name","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('./submission.csv', index = False)\nsubmission_df","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display dataframe for testing images\n\npd.set_option('display.max_rows', None)\nsubmission_df.head(len(submission_df))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. Run-length Encoding**","metadata":{}},{"cell_type":"code","source":"def mask_to_rle(image):\n    # The input image is expressed as a numpy array\n    # Masked pixels are expressed as value '1' where the background pixels will be set as '0'\n    # Returns run length as string formatted\n    \n    pixels = image.flatten() # Decoded pixels after flattening the masked image\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1 # This record is the position where the bit value starts to change, here +1 is for position adjustment\n\n    runs[1::2] -= runs[::2] # runs[1::2] = runs[1::2] - runs[::2]\n    # For example, the result before running is runs = [1 3 13 15] \n    # Each number in runs represents the position where the pixel value changes\n    # The result after running is runs = [ 1  2 13  2]\n    # Means that starting from the first position, a total of 2 bits are the same, so use 3 - 1 to get 2.\n    # Means that starting from the 13th position, a total of 2 bits are the same, so use 15 - 13 to get 2.\n    # Correspond to the two 11 at the head and the end above\n    \n    # Python sequence slice addresses can be written as a[start:end:step] and any of start, stop or end can be dropped. \n    # For example, a[::3] is every third element of the sequence.\n    return ' '.join(str(x) for x in runs)\n \ndef rle_to_mask(mask_rle, shape = (256, 1600)):\n    # The mask_rle is the run-length as string formated (start length)\n    # image.shape is the dimension of the image in array form\n    # Returns a masked image expressed in numpy array, with masked pixels as '1' and background pixels as '0'\n\n    s = mask_rle.split()\n    # starts is the list of the position numbers of the changed pixels\n    starts, lengths = [np.asarray(x, dtype = int) for x in (s[0:][::2], s[1:][::2])]  \n    # s[0:][::2] - This is a list of position numbers of changed pixels, for example, we have encoded pixels as [1, 13]\n    # s[1:][::2] - This is a list of the length of the same pixel (recording the consecutive equivalent pixels behind each changed pixel separately Continuous length of value)\n    \n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype = np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EncodedPixels = train_df['EncodedPixels'][22]\nEncodedPixels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(EncodedPixels.split()))\nEncodedPixels.split()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rle = list(map(int, EncodedPixels.split()))\nrle","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pixel,pixel_count = [],[]\n[pixel.append(rle[i]) if i % 2 == 0 else pixel_count.append(rle[i]) for i in range(0, len(rle))]\nprint('Pixel starting points:\\n',pixel)\nprint('Pixel counting:\\n', pixel_count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rle_pixels = [list(range(pixel[i],pixel[i] + pixel_count[i])) for i in range(0, len(pixel))]\nprint('rle_pixels\\n:', rle_pixels)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rle_mask_pixels = sum(rle_pixels,[]) \nprint('rle mask pixels:\\n', rle_mask_pixels)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read a sample training image with its information displayed\n\ndisplay(train_df.iloc[22])\nimage = imread(train_dir + '/' + train_df['ImageId'][22])\nprint(image.shape)\n\nplt.figure(figsize=None) #The size of original image is (1600, 256)\nplt.imshow(image)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_img = np.zeros((256*1600,1), dtype = int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_img[rle_mask_pixels] = 255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = train_dir + '/' + train_df['ImageId'][22]\n\nl,b = cv2.imread(image).shape[0], cv2.imread(image).shape[1]\nmask = np.reshape(mask_img, (b, l)).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_to_rle(rle_to_mask(train_df['EncodedPixels'].iloc[0])) == train_df['EncodedPixels'].iloc[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_to_rle(rle_to_mask('1 1')) == '1 1'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rle_to_mask(train_df['EncodedPixels'].iloc[1111])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_to_rle(rle_to_mask(train_df['EncodedPixels'].iloc[1111]))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References","metadata":{}},{"cell_type":"markdown","source":"1. Main Reference Link 1: https://medium.com/analytics-vidhya/severstal-steel-defect-detection-2d2e836855c2\n2. Main Reference link 2: https://medium.com/@saivenkat_/a-detailed-case-study-on-severstal-steel-defect-detection-can-we-detect-and-classify-defects-in-2844402392cc\n3. Main Reference Link 3: https://medium.com/@guildbilla/steel-defect-detection-image-segmentation-using-keras-dae8b4f986f0\n4. Main Reference Link 4: https://www.kaggle.com/kenmatsu4/visualize-steel-defect\n5. Main Reference Link 5: https://www.kaggle.com/yasserhessein/severstal-steel-defect-detection-with-xception\n6. Main Reference Link 6: http://faculty.neu.edu.cn/me/songkc/Research.html\n7. Main Reference Link 7: https://zhuanlan.zhihu.com/p/92547750\n8. Secondary Reference Link 1: https://github.com/nikhilroxtomar/Multiclass-Segmentation-in-Unet\n9. Secondary Reference Link 2 (for multiclass classification): https://www.kaggle.com/lsind18/gemstones-multiclass-classification-cnn , https://www.kaggle.com/yemregundogmus/car-forecast-with-multi-class-classification\n10. Supportive Reference Link 1: https://www.kaggle.com/arjunrao2000/beginners-guide-efficientnet-with-keras/notebook\n11. Supportive Reference Link 2: https://colab.research.google.com/drive/1vzEDAX-3ol7gcZ7qmKuwn8zUld524sUZ#scrollTo=e4wFQJkBiFWE https://blog.roboflow.com/how-to-train-efficientnet/\n12. Supportive Reference Link 3: https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114\n13. Supportive Reference Link 4: https://towardsdatascience.com/building-convolutional-neural-networks-in-python-using-keras-7e4652f6456f\n\n**Image Segmentation** <br>\n1. Reference Link 1: https://medium.com/analytics-vidhya/generating-masks-from-encoded-pixels-semantic-segmentation-18635e834ad0 <br> \n2. Reference Link 2: https://www.kaggle.com/paulorzp/run-length-encode-and-decode <br>\n3. Reference Link 3: https://www.programmersought.com/article/51974806761/ <br>\n4. Reference Link 4: https://developer.nvidia.com/blog/building-image-segmentation-faster-using-jupyter-notebooks-from-ngc/","metadata":{}}]}