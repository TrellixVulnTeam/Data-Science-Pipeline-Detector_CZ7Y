{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's define a semantic segmentation framework with the UNet architecture with just two downsampling and two upsampling portions. Downsampling and Upsampling by a factor of four"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass tinyUNet(torch.nn.Module):\n    def __init__(self,in_channels=3,\n                 filters1=64,filters2 = 128,filters3=256,\n                 out_classes=5,\n                 filter_size=3,Pools=4):\n        super(tinyUNet,self).__init__()\n        self.Conv1 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(in_channels),\n            torch.nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=filters1,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters1,\n                out_channels=filters1,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Down1 = torch.nn.MaxPool2d(Pools)\n        self.Conv2 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(filters1),\n            torch.nn.Conv2d(\n                in_channels=filters1,\n                out_channels=filters2,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters2,\n                out_channels=filters2,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Down2 = torch.nn.MaxPool2d(Pools)\n        self.Conv3 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(filters2),\n            torch.nn.Conv2d(\n                in_channels=filters2,\n                out_channels=filters3,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters3,\n                out_channels=filters3,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Up1 = torch.nn.ConvTranspose2d(\n            in_channels=filters3,\n            out_channels=filters2,\n            kernel_size=Pools,\n            stride=Pools,\n            padding=0)\n        self.Conv4 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(filters2+filters2),\n            torch.nn.Conv2d(\n                in_channels=filters2+filters2,\n                out_channels=filters2,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters2,\n                out_channels=filters2,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Up2 = torch.nn.ConvTranspose2d(\n            in_channels=filters2,\n            out_channels=filters1,\n            kernel_size=Pools,\n            stride=Pools,\n            padding=0)\n        self.Conv5 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(filters1+filters1),\n            torch.nn.Conv2d(\n                in_channels=filters1+filters1,\n                out_channels=filters1,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters1,\n                out_channels=filters1,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Out = torch.nn.Conv2d(in_channels=filters1,\n                                   out_channels=out_classes,\n                                   kernel_size=1,\n                                   padding=0,\n                                   stride=1)\n    \n    def forward(self,Input):\n        self.Conv1Out = self.Conv1((Input))\n        self.Down1Out = self.Down1(self.Conv1Out)\n        self.Conv2Out = self.Conv2(self.Down1Out)\n        self.Down2Out = self.Down2(self.Conv2Out)\n        self.Conv3Out = self.Conv3(self.Down2Out)\n        self.Up1Out = self.Up1(self.Conv3Out)\n        self.Conv4Out = self.Conv4(torch.cat((self.Conv2Out,self.Up1Out),dim=1))\n        self.Up2Out = self.Up2(self.Conv4Out)\n        self.Conv5Out = self.Conv5(torch.cat((self.Conv1Out,self.Up2Out),dim=1))\n        self.Logit = self.Out(self.Conv5Out)\n        return self.Logit\n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Instantiate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"Model = tinyUNet().to('cuda')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load up the weights from a model that was trained on the entire dataset without any regularization or data augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"Model.load_state_dict(torch.load('/kaggle/input/sss-v2-model-training-data-augmentation-no-reg/miniUNet8'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model.eval()\nModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls /kaggle/input/severstal-steel-defect-detection/ -al","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The model outputs Masks for each class so we need to convert this in to Run Length Encodings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks @rakhlin for sharing!\n# https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n\ndef rle_encoding(x):\n    '''\n    x: numpy array of shape (height, width), 1 - mask, 0 - background\n    Returns run length as list\n    '''\n    dots = np.where(x.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b+1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir /kaggle/test_masks","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's get all the output masks and save them to a directory for run length encoding later on"},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname,_,filenames in os.walk('/kaggle/input/severstal-steel-defect-detection/test_images/'):\n    files = len(filenames)\n    for i in range(0,files):\n        Istack = []\n        Istack.append(plt.imread(dirname+filenames[i]))\n        testTensor = torch.cuda.FloatTensor(\n            np.swapaxes(\n                np.swapaxes(\n                    np.stack(\n                        Istack,axis=0),1,3),2,3))\n        pred = Model(testTensor).detach().cpu().numpy()\n        np.save('/kaggle/test_masks/'+str(filenames[i]),pred)\n        print(\"Completion: {}%\".format(100*(i+1)/files),end='\\r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Change the format of the output to NHWC since pytorch use NCHW format, and then get the run length encodings for each mask except the last one. We don't need the mask that defines when it's not a part of any class"},{"metadata":{"trusted":true},"cell_type":"code","source":"Predictions = []\n\nfor dirname,_,filenames in os.walk('/kaggle/test_masks/'):\n    for idx,filename in enumerate(filenames):\n        Masks = np.load(dirname+filename)\n        MasksHWC = np.squeeze(np.argmax(Masks,axis=1))\n        for i in range(4):\n            RLE = str(rle_encoding(MasksHWC==i)).replace(',','').replace('[','').replace(']','')\n            Predictions.append([filename[:-4]+'_'+str(i+1),RLE])\n        print(\"Completion : {}%\".format(100*(idx+1)/len(filenames)),end='\\r')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create a DataFrame with the data and save to a csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"PDF = pd.DataFrame(Predictions,columns = ['ImageId_ClassId','EncodedPixels'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PDF.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PDF.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A Check for sanity reasons"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv('submission.csv').head(40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}