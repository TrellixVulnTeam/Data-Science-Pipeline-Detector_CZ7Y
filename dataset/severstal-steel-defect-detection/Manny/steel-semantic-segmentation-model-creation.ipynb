{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport torch \n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's read the csv just to see what the responses look like. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DF = pd.read_csv('/kaggle/input/train.csv')\nDF[~DF.EncodedPixels.isna()].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF['Class'] = DF.ImageId_ClassId.apply(lambda x: x.split('_')[1])\nDF['FileName'] = DF.ImageId_ClassId.apply(lambda x: x.split('_')[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interesting. In order to get a semantic segmentation going. I'm gong to have to convert this to masks. \n### It seems that we have 5 classes. 4 classes of defects and one class of no defect. We're going to need that fifth class for our softmax classifier at the end\n### Before I jump into that, let me see if the image sizes are consistent\n### Let's walk through all the images and get a sense of their sizes"},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"for dirname ,_ , filenames in os.walk('/kaggle/input/train_images/'):\n    Data = []\n    for i,filename in enumerate(filenames):\n        print(\"Completion : {}%\".format(round((i+1)/len(filenames)*100)),end='\\r')\n        I = plt.imread(os.path.join(dirname,filename))\n        Data.append(list(I.shape)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### So reading through all those images is $\\textit{quite}$ a challenge. It takes a while to loop through it all"},{"metadata":{"trusted":true},"cell_type":"code","source":"D = np.array(Data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Sizes = pd.DataFrame(D,columns=['Height','Width','Channels'])\nSizes.hist()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So all the images are the same size. That makes my life a little easier\n### And now for the masks:"},{"metadata":{"trusted":true},"cell_type":"code","source":"WIDTH = int(Sizes.Width.mode())\nHEIGHT = int(Sizes.Height.mode())\n\n# Thank you @robertkag\n# https://www.kaggle.com/robertkag/rle-to-mask-converter\ndef rleToMask(rleString,height,width):\n    rows,cols = height,width\n    rleNumbers = [int(numstring) for numstring in rleString.split(' ')]\n    rlePairs = np.array(rleNumbers).reshape(-1,2)\n    img = np.zeros(rows*cols,dtype=np.uint8)\n    for index,length in rlePairs:\n        index -= 1\n        img[index:index+length] = 1\n    img = img.reshape(cols,rows)\n    img = img.T\n    return img\n  \nI = rleToMask(DF[~DF.EncodedPixels.isna()].sample().EncodedPixels.values[0],HEIGHT,WIDTH)\nplt.imshow(I)\nplt.show()\n\ndel I,D,Sizes,Data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### So I'm going to walk through every filename, get the coordinates of each mask, convert them to numpy arrays, join them with the rest of the masks to get a tensor representing the output class of each pixel"},{"metadata":{"trusted":true},"cell_type":"code","source":"! mkdir /kaggle/train_masks\n! echo \"These are masks as numpy arrayas\" > /kaggle/train_masks/README.MD","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For each file in training folder: \n1. find the run length encodings of each class and convert to mask.\n2. stack all masks together. \n3. find the non defective class by checking which pixels are not assigned any class, and creating another mask for those pixels. \n4. Stack again to make final output tensor\n5. Take the argmax of that in the channel axis, PyTorch doesn't need one hot encoded masks"},{"metadata":{"trusted":true},"cell_type":"code","source":"Sample = DF[~DF.EncodedPixels.isna()].sample()\nfileName = Sample.FileName.values[0]\n# print(fileName)\nSampleDF = DF[DF.FileName==fileName][['EncodedPixels','Class']]\nSampleDF.head()\n\nTensor = np.zeros((HEIGHT,WIDTH,4),dtype=np.uint8)\nfor i,j in SampleDF.values:\n    if str(i) == 'nan':\n        pass\n    else:\n        Tensor[:,:,int(j)-1] = rleToMask(i,HEIGHT,WIDTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"I = plt.imread(os.path.join('/kaggle/input/train_images/',fileName))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Sample = DF[~DF.EncodedPixels.isna()].sample()\nfileName = Sample.FileName.values[0]\n# print(fileName)\nSampleDF = DF[DF.FileName==fileName][['EncodedPixels','Class']]\nSampleDF.head()\n\nTensor = np.zeros((HEIGHT,WIDTH,4),dtype=np.uint8)\nfor i,j in SampleDF.values:\n    if str(i) == 'nan':\n        pass\n    else:\n        Tensor[:,:,int(j)-1] = rleToMask(i,HEIGHT,WIDTH)\n\nTensor = np.expand_dims(Tensor.argmax(axis=2),axis=2)\n# print(Tensor.shape)\nI = plt.imread(os.path.join('/kaggle/input/train_images/',fileName))\n# print(np.squeeze(Tensor).shape)\nfor i in np.unique(Tensor):\n    tempI=I.copy()\n    tempI[:,:,0] = 255*np.squeeze(Tensor==i)+(1-np.squeeze(Tensor==i))*I[:,:,0]\n    plt.imshow(tempI)\n    plt.title('Class {}'.format(i+1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def checkSegmentation(I,Tensor):\n    \n    for i in np.unique(Tensor):\n        tempI=I.copy()\n        tempI[:,:,0] = 255*np.squeeze(Tensor==i)+(1-np.squeeze(Tensor==i))*I[:,:,0]\n        plt.imshow(tempI)\n        plt.title('Segmentation for Class {}'.format(i+1))\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NoClass = ~np.prod(Tensor,axis=2,keepdims=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NoClass = (Tensor.sum(axis=2,keepdims=1)<1).astype(np.uint8)\nplt.imshow(np.squeeze(NoClass))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TotalTensor = np.dstack((Tensor,NoClass))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TotalTensor.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getMaskTensor(name):\n    SampleDF = DF[DF.FileName==name][['EncodedPixels','Class']]\n    SampleDF.head()\n\n    Tensor = np.zeros((HEIGHT,WIDTH,4),dtype=np.uint8)\n    for i,j in SampleDF.values:\n        if str(i) == 'nan':\n            pass\n        else:\n            Tensor[:,:,int(j)-1] = rleToMask(i,HEIGHT,WIDTH)\n    NoClass = (Tensor.sum(axis=2,keepdims=1)<1).astype(np.uint8)\n    TotalTensor = np.dstack((Tensor,NoClass))\n    return np.expand_dims(TotalTensor.argmax(axis=2),axis=2)\n\nmaskTensor = getMaskTensor(fileName)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in np.unique(maskTensor):\n    plt.imshow(maskTensor[:,:,0]==i)\n    plt.title('Class {}'.format(i))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MaskTensor = getMaskTensor(fileName)\nnp.save('/kaggle/working/train_masks/'+fileName,MaskTensor)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TheMask = np.load('/kaggle/working/train_masks/'+fileName+'.npy')\nTheMask.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx,filename in enumerate([_,filenames,_ in os.walk('/kaggle/input/train_images/')][1]):\n    print(\"{} Files done\".format(idx),end='\\r')\n    MaskTensor = getMaskTensor(filename)\n    np.save('/kaggle/train_masks/'+filename,MaskTensor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cool Beans! Now we have training inputs and their corresponding Outputs in another folder. Let's define a Data Loading function to get our inputs and outputs. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def DataLoader(DirectoryInputs='/kaggle/input/train_images/',\n               DirectoryOutputs='/kaggle/train_masks/',sampleSize=3):\n    filenames = np.array(list(os.walk(DirectoryInputs))[0][2])\n    sampleIDX = np.random.randint(0,len(filenames),size=sampleSize,dtype=np.uint32)\n#     print(filenames)\n#     print(type(sampleIDX))\n    InputTensors = []\n    OutputTensors = []\n    for i in filenames[[int(x) for x in sampleIDX]]:\n        InputTensors.append(plt.imread(os.path.join(DirectoryInputs,i)))\n        OutputTensors.append(np.load(os.path.join(DirectoryOutputs,i+'.npy')))\n        \n    Input = np.stack(InputTensors,axis=0)\n    Output = np.stack(OutputTensors,axis=0)\n\n    return Input,Output\n\nInput,Output = DataLoader(sampleSize=5)\n\nfor im in range(Input.shape[0]):\n    checkSegmentation(Input[im],Output[im])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## After all that clean up, we can finally define a model! Woohoo! \n## I know, you may be thinking \"What about the data augmentation steps?!\". I want to define a baseline model first. I'm going to add data augmentation in subsequent modelling efforts.\n## Let's get to it\n# First model: just a convolution and a softmax\n\n### I'm not splitting my data to train-val just yet as this model is so basic there is abosolutely no way it would overfit. We'll do that when I get serious with this."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"class ConvSoftMax(torch.nn.Module):\n    def __init__(self,in_channels=3,out_classes=5,filter_size=7):\n        super(ConvSoftMax,self).__init__()\n        self.Conv = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(num_features=in_channels),\n            torch.nn.Conv2d(in_channels=in_channels,\n                            out_channels=out_classes,\n                            stride=1,\n                            kernel_size=filter_size,\n                            padding=(filter_size-1)//2)\n        )\n        self.Final = torch.nn.Softmax(dim=1)\n        \n    def imScaler(self,X):\n        self.ScaledInput = 2 * (X/255) - 1\n        return self.ScaledInput\n    \n    def forward(self,Input):\n        self.Logit = self.Conv(self.imScaler(Input))\n        return self.Logit\n  \n    def predict(self,Input):\n        self.Logit = self.forward(Input)\n        self.Output = self.Final(self.Logit)\n        return self.Output\n    "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"BaselineModel = ConvSoftMax(filter_size=21).to('cuda')"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"BaselineModel"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"BaselineModel.forward(torch.rand(5,3,100,100).to('cuda')).shape"},{"metadata":{"trusted":true},"cell_type":"code","source":"def torchDataLoader(DirectoryInputs='/kaggle/input/train_images/',\n                    DirectoryOutputs='/kaggle/train_masks/',sampleSize=5):\n    Input,Output = DataLoader(DirectoryInputs,DirectoryOutputs,sampleSize)\n    \n    InputTorchTensor = torch.cuda.FloatTensor(Input)\n    OutputTorchTensor = torch.cuda.LongTensor(Output)\n    InputTorchTensor = InputTorchTensor.transpose(1,3).transpose(2,3)\n    OutputTorchTensor = OutputTorchTensor.transpose(1,3).transpose(2,3)\n    return InputTorchTensor,OutputTorchTensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X,Y = torchDataLoader()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X, Y\ntorch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"lossFunc = torch.nn.CrossEntropyLoss()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"LR = 1e-2\noptim = torch.optim.Adam(BaselineModel.parameters(),lr=LR)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"batchSize=10\nepochs = 2000\nLosses = torch.empty(epochs)\nfor i in range(epochs):\n    X,Y = torchDataLoader(sampleSize=batchSize)\n    \n    logit = BaselineModel.forward(X)\n    print('Completion: {}%'.format(round((i+1)/epochs*100)),end='\\r')\n    loss = lossFunc(logit,torch.squeeze(Y))\n#     print(loss)\n    Losses[i] = loss\n    loss.backward()\n    \n    optim.step()\n    \n    optim.zero_grad()\n    \nnumpyLosses = Losses.detach().cpu().numpy()\nplt.plot(numpyLosses)\ndel Losses,loss,logit\ntorch.cuda.empty_cache()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Let's try it out on an image"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"X,Y = torchDataLoader(sampleSize=10)\n\npredY = torch.argmax(BaselineModel.predict(X),dim=1,keepdim=True)\n\nX = X.transpose(3,1).transpose(1,2).detach().cpu().numpy().astype(np.uint8)\npredY = predY.transpose(3,1).transpose(1,2).detach().cpu().numpy().astype(np.uint8)\nY = Y.transpose(3,1).transpose(1,2).detach().cpu().numpy().astype(np.uint8)\n\nfor im in range(X.shape[0]):\n    print(\"Image: {}\".format(im+1))\n    checkSegmentation(X[im],Y[im])"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"for im in range(X.shape[0]):\n    print(\"Image: {}\".format(im+1))\n    checkSegmentation(X[im],predY[im])"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"markdown","source":"del BaselineModel,X,Y,predY\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()/(1024**3)"},{"metadata":{},"cell_type":"markdown","source":"## So I failed to realize that what we're looking for is the minority class. \n## The simple logistic regression, consistently predicted everything as part of the majority class\n## So now this is about to get serious. Let's start going deeper and add one extra layer convolutional layer. See how it goes"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"class JustConv2(torch.nn.Module):\n    def __init__(self,in_channels=3,\n                 filters=64,\n                 out_classes=5,\n                 filter_size1=3,\n                 filter_size2=5):\n        super(JustConv2,self).__init__()\n        self.Conv = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(num_features=in_channels),\n            torch.nn.Conv2d(in_channels=in_channels,\n                            out_channels=filters,\n                            stride=1,\n                            kernel_size=filter_size1,\n                            padding=(filter_size1-1)//2),\n            torch.nn.ReLU(),\n            torch.nn.BatchNorm2d(num_features=filters),\n            torch.nn.Conv2d(in_channels=filters,\n                            out_channels=out_classes,\n                            stride=1,\n                            kernel_size=filter_size2,\n                            padding=(filter_size2-1)//2)\n        )\n        self.Final = torch.nn.Softmax(dim=1)\n        \n    def imScaler(self,X):\n        self.ScaledInput = 2 * (X/255) - 1\n        return self.ScaledInput\n    \n    def forward(self,Input):\n        self.Logit = self.Conv(self.imScaler(Input))\n        return self.Logit\n  \n    def predict(self,Input):\n        self.Logit = self.forward(Input)\n        self.Output = self.Final(self.Logit)\n        return self.Output\n    "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"torch.cuda.empty_cache()\nModel2 = JustConv2().to('cuda')"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Model2 = JustConv2(filter_size1=7,filter_size2=11).to('cuda')\ntorch.cuda.empty_cache()\nLR = 1e-2\nlossFunc = torch.nn.CrossEntropyLoss()\noptim = torch.optim.Adam(Model2.parameters(),lr=LR)\nbatchSize=15\nepochs = 3000\nLosses = torch.empty(epochs)\nfor i in range(epochs):\n    X,Y = torchDataLoader(sampleSize=batchSize)\n    \n    logit = Model2.forward(X)\n    print('Training Completion: {}% Epoch: {}'.\n          format(round((i+1)/epochs*100),i+1),end='\\r')\n    loss = lossFunc(logit,torch.squeeze(Y))\n#     print(loss)\n    Losses[i] = loss\n    loss.backward()\n    optim.step()\n\n    optim.zero_grad()\n    \nnumpyLosses = Losses.detach().cpu().numpy()\nplt.plot(numpyLosses)\n\ndel  Losses,loss,logit, optim, X, Y,lossFunc\ntorch.cuda.empty_cache()\nprint(\"\\nCUDA memory used: \")\nprint(torch.cuda.memory_allocated()/(1024**3))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"X,Y = torchDataLoader(sampleSize=10)\n\npredY = torch.argmax(Model2.predict(X),dim=1,keepdim=True)\n\nX = X.transpose(3,1).transpose(1,2).detach().cpu().numpy().astype(np.uint8)\npredY = predY.transpose(3,1).transpose(1,2).detach().cpu().numpy().astype(np.uint8)\nY = Y.transpose(3,1).transpose(1,2).detach().cpu().numpy().astype(np.uint8)\n\nfor im in range(X.shape[0]):\n    print(\"Image {} True Segmentation\".format(im+1))\n    checkSegmentation(X[im],Y[im])\n    print(\"Image {} Predicted Segmentation\".format(im+1))\n    checkSegmentation(X[im],predY[im])"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Still not enough juice.\n\n## So here's what I'm going to do: I need to capture the overall context of the image before I segment\n\n## I need pooling layers! Pooling layers help to downsample and help me get feature representation of the image that is invariant to small scale translations. \n\n## When I get to this downsampled version of the image that aggregates all the features from different parts of the image, I'll need to upsample to get back to the original size of the input image and then make a classification\n\n## I'll be using a simpler version of UNet with just two downsampling and two upsampling blocks"},{"metadata":{"trusted":true},"cell_type":"code","source":"class tinyUNet(torch.nn.Module):\n    def __init__(self,in_channels=3,\n                 filters1=64,filters2 = 128,filters3=256,\n                 out_classes=5,\n                 filter_size=3,Pools=4):\n        super(tinyUNet,self).__init__()\n        self.Conv1 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(in_channels),\n            torch.nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=filters1,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters1,\n                out_channels=filters1,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Down1 = torch.nn.MaxPool2d(Pools)\n        self.Conv2 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(filters1),\n            torch.nn.Conv2d(\n                in_channels=filters1,\n                out_channels=filters2,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters2,\n                out_channels=filters2,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Down2 = torch.nn.MaxPool2d(Pools)\n        self.Conv3 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(filters2),\n            torch.nn.Conv2d(\n                in_channels=filters2,\n                out_channels=filters3,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters3,\n                out_channels=filters3,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Up1 = torch.nn.ConvTranspose2d(\n            in_channels=filters3,\n            out_channels=filters2,\n            kernel_size=Pools,\n            stride=Pools,\n            padding=0)\n        self.Conv4 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(filters2+filters2),\n            torch.nn.Conv2d(\n                in_channels=filters2+filters2,\n                out_channels=filters2,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters2,\n                out_channels=filters2,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Up2 = torch.nn.ConvTranspose2d(\n            in_channels=filters2,\n            out_channels=filters1,\n            kernel_size=Pools,\n            stride=Pools,\n            padding=0)\n        self.Conv5 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(filters1+filters1),\n            torch.nn.Conv2d(\n                in_channels=filters1+filters1,\n                out_channels=filters1,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters1,\n                out_channels=filters1,\n                kernel_size = filter_size,\n                padding=(filter_size-1)//2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Out = torch.nn.Conv2d(in_channels=filters1,\n                                   out_channels=out_classes,\n                                   kernel_size=1,\n                                   padding=0,\n                                   stride=1)\n        \n        self.Final = torch.nn.Softmax(dim=1)\n        \n    def imScaler(self,X):\n        self.ScaledInput = 2 * (X/255) - 1\n        return self.ScaledInput\n    \n    def forward(self,Input):\n        self.Conv1Out = self.Conv1(self.imScaler(Input))\n        self.Down1Out = self.Down1(self.Conv1Out)\n        self.Conv2Out = self.Conv2(self.Down1Out)\n        self.Down2Out = self.Down2(self.Conv2Out)\n        self.Conv3Out = self.Conv3(self.Down2Out)\n        self.Up1Out = self.Up1(self.Conv3Out)\n        self.Conv4Out = self.Conv4(torch.cat((self.Conv2Out,self.Up1Out),dim=1))\n        self.Up2Out = self.Up2(self.Conv4Out)\n        self.Conv5Out = self.Conv5(torch.cat((self.Conv1Out,self.Up2Out),dim=1))\n        self.Logit = self.Out(self.Conv5Out)\n        return self.Logit\n  \n    def predict(self,Input):\n        self.Logit = self.forward(Input)\n        self.Output = self.Final(self.Logit)\n        return self.Output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"theNet = tinyUNet().to('cuda')\ntorch.cuda.empty_cache()\nLR = 1e-3\nlossFunc = torch.nn.CrossEntropyLoss()\noptim = torch.optim.Adam(theNet.parameters(),lr=LR)\nbatchSize=7\nepochs = int(2e4)\nLosses = torch.empty(epochs)\nfor i in range(epochs):\n    X,Y = torchDataLoader(sampleSize=batchSize)\n    \n    logit = theNet.forward(X)\n    print('Training Completion: {}% Epoch: {}'.\n          format(round((i+1)/epochs*100),i+1),end='\\r')\n    loss = lossFunc(logit,torch.squeeze(Y))\n    Losses[i] = loss.detach().cpu()\n    loss.backward()\n    optim.step()\n    optim.zero_grad()\n    del loss ,logit\nnumpyLosses = Losses.detach().cpu().numpy()\nplt.plot(numpyLosses)\n\ndel Losses, optim, lossFunc\ntorch.cuda.empty_cache()\nprint(\"\\nCUDA memory allocated: \")\nprint(torch.cuda.memory_allocated()/(1024**3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# del logit , loss\ntheNet = tinyUNet().to('cuda')\ntorch.cuda.empty_cache()\nLR = 1e-3\nlossFunc = torch.nn.CrossEntropyLoss()\noptim = torch.optim.Adam(theNet.parameters(),lr=LR)\nbatchSize=40\nepochs = 5000\nLosses = torch.ones(epochs)\n\nX,Y = torchDataLoader(sampleSize=batchSize)\n\npred = torch.randn(3,3,1000,1000).to('cuda')\n\ndel Losses , pred, theNet,lossFunc,optim, X, Y\ntorch.cuda.empty_cache()\nprint(\"\\nCUDA memory allocated: \")\nprint(torch.cuda.memory_allocated()/(1024**3))"},{"metadata":{"trusted":true},"cell_type":"code","source":"X,Y = torchDataLoader(sampleSize=2)\n\npredY = torch.argmax(theNet.forward(X),dim=1,keepdim=True)\n\nX = X.transpose(3,1).transpose(1,2).detach().cpu().numpy().astype(np.uint8)\npredY = predY.transpose(3,1).transpose(1,2).detach().cpu().numpy().astype(np.uint8)\nY = Y.transpose(3,1).transpose(1,2).detach().cpu().numpy().astype(np.uint8)\n\nfor im in range(X.shape[0]):\n    print(\"Image {} True Segmentation\".format(im+1))\n    checkSegmentation(X[im],Y[im])\n    print(\"Image {} Predicted Segmentation\".format(im+1))\n    checkSegmentation(X[im],predY[im])\n    \ndel X,Y,predY","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(theNet.state_dict(),'miniUNet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(theNet.state_dict(),'/miniUNet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks @rakhlin for sharing!\n# https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n\ndef rle_encoding(x):\n    '''\n    x: numpy array of shape (height, width), 1 - mask, 0 - background\n    Returns run length as list\n    '''\n    dots = np.where(x.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b+1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}