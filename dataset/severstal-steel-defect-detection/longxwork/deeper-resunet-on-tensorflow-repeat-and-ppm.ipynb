{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nBecause of memory issues, I moved the EDA into a seperate kernel, [Mask of Steel - EDA & FP Mining](https://www.kaggle.com/ekhtiar/mask-of-steel-eda-fp-mining). In this notebook I we will focus only on implementing a CNN architecture (ResUNet) to identify these defects! The original paper that proposes this architecture is [ResUNet-a: a deep learning framework for semantic segmentation of remotely sensed data](https://arxiv.org/abs/1904.00592).\n\n**If you like my work please upvote this Kernel. This encourages or motivates people like me, who contributes to Kaggle on their own time with the intention to share knowledge, to continue the effort. Furthermore, if I made a mistake or can do something more, please leave a comment in the comments section to help me out. Many thanks in advance!**","metadata":{}},{"cell_type":"markdown","source":"#### Experiment Log\n\n1. After pre-training our model for 50 epoch, I managed to load that and make a submission in this version of the [kernel](https://www.kaggle.com/ekhtiar/eda-resunet-tensorflow-keras?scriptVersionId=18052749). In this kernel we just had 5 additional epochs on top of the pre-trained model and the score was 0.71469.\n2. I switched to grayscale from RGB, and re-ran the experiment from point 2 in this [kernel](https://www.kaggle.com/ekhtiar/resunet-a-baseline-on-tensorflow?scriptVersionId=18651595). This made no difference, so we are switching to grayscale. \n3. I increased the number of filters by a factor of 32, to see if having a huge number of filters per layer would improve performance. This rather lowered the scor to 0.68188, and you can see it in this [kernel](https://www.kaggle.com/ekhtiar/resunet-a-baseline-on-tensorflow?scriptVersionId=18672228).\n4. I used standardized values of the input image in this [kernel](https://www.kaggle.com/ekhtiar/resunet-a-baseline-on-tensorflow?scriptVersionId=18751443), which lead to a improved the score to 0.83109. \n5. Its very obvious that for Image class 3 we are doing a very good job. For the rest of the classes, we are not even being triggered. This is due to the imbalance in our classes. By just repeating the examples with fewer representation multiple times, in this [kernel](https://www.kaggle.com/ekhtiar/mask-eda-fp-mining-and-resunet-a-baseline?scriptVersionId=18276330) we were able to make predictions for class 4 as well as 3. However, class 1 and class 2 was still not predicted by the network. This is probably due to the fact the area for class 1 and 2 is very tiny. However, the score was very poor (0.60075), and can be seen in this [kernel](https://www.kaggle.com/ekhtiar/resunet-a-baseline-on-tensorflow?scriptVersionId=18320089). Obviously this is due to overfitting, and we need to augment our samples as we repeat them. \n\nPS: *Be careful with submission of this Kernel, as it is very prone to throwing Kernen threw exception error. It is very hard to debug it as it doesn't provide any logs!!! That is why I had to make so many changes in between version 16 to 19. I had to turn off the IoU score calculation part for it to work. The train set for this Kernel is much larger than the one provided, and for submission of the score, it will run it over the full test set. So becareful about running out of memory, I guess.*","metadata":{}},{"cell_type":"markdown","source":"#### Imports","metadata":{}},{"cell_type":"code","source":"# some basic imports\nimport pandas as pd\nimport numpy as np\nimport os\nimport cv2\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:11.267603Z","iopub.execute_input":"2022-05-23T11:36:11.267893Z","iopub.status.idle":"2022-05-23T11:36:11.62494Z","shell.execute_reply.started":"2022-05-23T11:36:11.267844Z","shell.execute_reply":"2022-05-23T11:36:11.624035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imports for building the network\nimport tensorflow as tf\nfrom tensorflow import reduce_sum\nfrom tensorflow.keras.backend import pow\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Concatenate, Add, Flatten, AveragePooling2D\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:11.627608Z","iopub.execute_input":"2022-05-23T11:36:11.628129Z","iopub.status.idle":"2022-05-23T11:36:13.963439Z","shell.execute_reply.started":"2022-05-23T11:36:11.628078Z","shell.execute_reply":"2022-05-23T11:36:13.962602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Configurations","metadata":{}},{"cell_type":"code","source":"# Kernel Configurations\nmake_submission = False # used to turn off lengthy model analysis so a submission version doesn't run into memory error\nload_pretrained_model = True # load a pre-trained model\nsave_model = True # save the model after training\ntrain_dir = '../input/severstal-steel-defect-detection/' # directory of training images\npretrained_model_path = '../input/repeat10-30-1-3-ppm/ResUNetSteel_w800e50_z.h5' # path of pretrained model\nmodel_save_path = './ResUNetSteel_w800e50_z.h5' # path of model to save\ntrain_image_dir = os.path.join(train_dir, 'train_images') # ","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:13.964871Z","iopub.execute_input":"2022-05-23T11:36:13.965142Z","iopub.status.idle":"2022-05-23T11:36:13.970926Z","shell.execute_reply.started":"2022-05-23T11:36:13.965096Z","shell.execute_reply":"2022-05-23T11:36:13.969307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# network configuration parameters\n# original image is 1600x256, so we will resize it\nimg_w = 800 # resized weidth\nimg_h = 256 # resized height\nbatch_size = 10\nepochs = 5\n# batch size for training unet\nk_size = 3 # kernel size 3x3\nval_size = .20 # split of training set between train and validation set\n# we will repeat the images with lower samples to make the training process more fair\nrepeat = True\n# only valid if repeat is True\nclass_1_repeat = 10 # repeat class 1 examples x times\nclass_2_repeat = 30\nclass_3_repeat = 1\nclass_4_repeat = 3","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:13.972673Z","iopub.execute_input":"2022-05-23T11:36:13.973114Z","iopub.status.idle":"2022-05-23T11:36:13.983009Z","shell.execute_reply.started":"2022-05-23T11:36:13.97292Z","shell.execute_reply":"2022-05-23T11:36:13.982166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load & Transform train.csv","metadata":{}},{"cell_type":"code","source":"# load full data and label no mask as -1\ntrain_df = pd.read_csv('../input/original-data/original_train.csv').fillna(-1)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:13.98692Z","iopub.execute_input":"2022-05-23T11:36:13.987324Z","iopub.status.idle":"2022-05-23T11:36:14.375059Z","shell.execute_reply.started":"2022-05-23T11:36:13.987263Z","shell.execute_reply":"2022-05-23T11:36:14.374371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image id and class id are two seperate entities and it makes it easier to split them up in two columns\ntrain_df['ImageId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\ntrain_df['ClassId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[1])\n# lets create a dict with class id and encoded pixels and group all the defaults per image\ntrain_df['ClassId_EncodedPixels'] = train_df.apply(lambda row: (row['ClassId'], row['EncodedPixels']), axis = 1)\ngrouped_EncodedPixels = train_df.groupby('ImageId')['ClassId_EncodedPixels'].apply(list)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:14.37806Z","iopub.execute_input":"2022-05-23T11:36:14.378385Z","iopub.status.idle":"2022-05-23T11:36:16.83724Z","shell.execute_reply.started":"2022-05-23T11:36:14.378309Z","shell.execute_reply":"2022-05-23T11:36:16.836277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Utility Functions for RLE Encoding & Decoding","metadata":{}},{"cell_type":"code","source":"# from https://www.kaggle.com/robertkag/rle-to-mask-converter\ndef rle_to_mask(rle_string,height,width):\n    '''\n    convert RLE(run length encoding) string to numpy array\n\n    Parameters: \n    rleString (str): Description of arg1 \n    height (int): height of the mask\n    width (int): width of the mask \n\n    Returns: \n    numpy.array: numpy array of the mask\n    '''\n    rows, cols = height, width\n    if rle_string == -1:\n        return np.zeros((height, width))\n    else:\n        rleNumbers = [int(numstring) for numstring in rle_string.split(' ')]\n        rlePairs = np.array(rleNumbers).reshape(-1,2)\n        img = np.zeros(rows*cols,dtype=np.uint8)\n        for index,length in rlePairs:\n            index -= 1\n            img[index:index+length] = 255\n        img = img.reshape(cols,rows)\n        img = img.T\n        return img","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:16.838823Z","iopub.execute_input":"2022-05-23T11:36:16.839121Z","iopub.status.idle":"2022-05-23T11:36:16.847869Z","shell.execute_reply.started":"2022-05-23T11:36:16.839073Z","shell.execute_reply":"2022-05-23T11:36:16.846926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thanks to the authors of: https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\ndef mask_to_rle(mask):\n    '''\n    Convert a mask into RLE\n    \n    Parameters: \n    mask (numpy.array): binary mask of numpy array where 1 - mask, 0 - background\n\n    Returns: \n    sring: run length encoding \n    '''\n    pixels= mask.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:16.849706Z","iopub.execute_input":"2022-05-23T11:36:16.850282Z","iopub.status.idle":"2022-05-23T11:36:16.858362Z","shell.execute_reply.started":"2022-05-23T11:36:16.850232Z","shell.execute_reply":"2022-05-23T11:36:16.857671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Generator\n\nTo push the data to our model, we will create a custom data generator. A generator lets us load data progressively, instead of loading it all into memory at once. A custom generator allows us to also fit in more customization during the time of loading the data. As the model is being procssed in the GPU, we can use a custom generator to pre-process images via a generator. At this time, we can also take advantage multiple processors to parallelize our pre-processing.","metadata":{}},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, list_ids, labels, image_dir, batch_size=32,\n                 img_h=256, img_w=512, shuffle=True):\n        \n        self.list_ids = list_ids\n        self.labels = labels\n        self.image_dir = image_dir\n        self.batch_size = batch_size\n        self.img_h = img_h\n        self.img_w = img_w\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    \n    def __len__(self):\n        'denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_ids)) / self.batch_size)\n    \n    def __getitem__(self, index):\n        'generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # get list of IDs\n        list_ids_temp = [self.list_ids[k] for k in indexes]\n        # generate data\n        X, y = self.__data_generation(list_ids_temp)\n        # return data \n        return X, y\n    \n    def on_epoch_end(self):\n        'update ended after each epoch'\n        self.indexes = np.arange(len(self.list_ids))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n    def __data_generation(self, list_ids_temp):\n        'generate data containing batch_size samples'\n        X = np.empty((self.batch_size, self.img_h, self.img_w, 1))\n        y = np.empty((self.batch_size, self.img_h, self.img_w, 4))\n        \n        for idx, id in enumerate(list_ids_temp):\n            file_path =  os.path.join(self.image_dir, id)\n            image = cv2.imread(file_path, 0)\n                    \n            image_resized = cv2.resize(image, (self.img_w, self.img_h))\n            image_resized = np.array(image_resized, dtype=np.float64)\n            # standardization of the image\n            image_resized -= image_resized.mean()\n            image_resized /= image_resized.std()\n            \n            mask = np.empty((img_h, img_w, 4))\n            \n            for idm, image_class in enumerate(['1','2','3','4']):\n                rle = self.labels.get(id + '_' + image_class)\n                # if there is no mask create empty mask\n                if rle is None:\n                    class_mask = np.zeros((1600, 256))\n                else:\n                    class_mask = rle_to_mask(rle, width=1600, height=256)\n             \n                class_mask_resized = cv2.resize(class_mask, (self.img_w, self.img_h))\n                mask[...,idm] = class_mask_resized\n            \n            X[idx,] = np.expand_dims(image_resized, axis=2)\n            y[idx,] = mask\n        \n        # normalize Y\n        y = (y > 0).astype(int)\n            \n        return X, y","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:16.859861Z","iopub.execute_input":"2022-05-23T11:36:16.860387Z","iopub.status.idle":"2022-05-23T11:36:16.881093Z","shell.execute_reply.started":"2022-05-23T11:36:16.860199Z","shell.execute_reply":"2022-05-23T11:36:16.880351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGeneratorWithImageId(tf.keras.utils.Sequence):\n    def __init__(self, list_ids, labels, image_dir, batch_size=32,\n                 img_h=256, img_w=512, shuffle=True):\n        \n        self.list_ids = list_ids\n        self.labels = labels\n        self.image_dir = image_dir\n        self.batch_size = batch_size\n        self.img_h = img_h\n        self.img_w = img_w\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    \n    def __len__(self):\n        'denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_ids)) / self.batch_size)\n    \n    def __getitem__(self, index):\n        'generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # get list of IDs\n        list_ids_temp = [self.list_ids[k] for k in indexes]\n        # generate data\n        X, y = self.__data_generation(list_ids_temp)\n        # return data \n        return X, y, list_ids_temp\n    \n    def on_epoch_end(self):\n        'update ended after each epoch'\n        self.indexes = np.arange(len(self.list_ids))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n    def __data_generation(self, list_ids_temp):\n        'generate data containing batch_size samples'\n        X = np.empty((self.batch_size, self.img_h, self.img_w, 1))\n        y = np.empty((self.batch_size, self.img_h, self.img_w, 4))\n        \n        for idx, id in enumerate(list_ids_temp):\n            file_path =  os.path.join(self.image_dir, id)\n            image = cv2.imread(file_path, 0)\n                    \n            image_resized = cv2.resize(image, (self.img_w, self.img_h))\n            image_resized = np.array(image_resized, dtype=np.float64)\n            # standardization of the image\n            image_resized -= image_resized.mean()\n            image_resized /= image_resized.std()\n            \n            mask = np.empty((img_h, img_w, 4))\n            \n            for idm, image_class in enumerate(['1','2','3','4']):\n                rle = self.labels.get(id + '_' + image_class)\n                # if there is no mask create empty mask\n                if rle is None:\n                    class_mask = np.zeros((1600, 256))\n                else:\n                    class_mask = rle_to_mask(rle, width=1600, height=256)\n             \n                class_mask_resized = cv2.resize(class_mask, (self.img_w, self.img_h))\n                mask[...,idm] = class_mask_resized\n            \n            X[idx,] = np.expand_dims(image_resized, axis=2)\n            y[idx,] = mask\n        \n        # normalize Y\n        y = (y > 0).astype(int)\n            \n        return X, y","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:16.88267Z","iopub.execute_input":"2022-05-23T11:36:16.883139Z","iopub.status.idle":"2022-05-23T11:36:16.907241Z","shell.execute_reply.started":"2022-05-23T11:36:16.882916Z","shell.execute_reply":"2022-05-23T11:36:16.906282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a dict of all the masks\nmasks = {}\nfor index, row in train_df[train_df['EncodedPixels']!=-1].iterrows():\n    masks[row['ImageId_ClassId']] = row['EncodedPixels']","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:16.91103Z","iopub.execute_input":"2022-05-23T11:36:16.91129Z","iopub.status.idle":"2022-05-23T11:36:17.482581Z","shell.execute_reply.started":"2022-05-23T11:36:16.911246Z","shell.execute_reply":"2022-05-23T11:36:17.481838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_image_ids = train_df['ImageId'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:17.48411Z","iopub.execute_input":"2022-05-23T11:36:17.48552Z","iopub.status.idle":"2022-05-23T11:36:17.49463Z","shell.execute_reply.started":"2022-05-23T11:36:17.485451Z","shell.execute_reply":"2022-05-23T11:36:17.493957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val = train_test_split(train_image_ids, test_size=val_size, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:17.496125Z","iopub.execute_input":"2022-05-23T11:36:17.496451Z","iopub.status.idle":"2022-05-23T11:36:17.505066Z","shell.execute_reply.started":"2022-05-23T11:36:17.496396Z","shell.execute_reply":"2022-05-23T11:36:17.504356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_train_df = train_df[train_df['ImageId'].isin(X_train)]","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:17.506753Z","iopub.execute_input":"2022-05-23T11:36:17.507206Z","iopub.status.idle":"2022-05-23T11:36:17.521166Z","shell.execute_reply.started":"2022-05-23T11:36:17.507046Z","shell.execute_reply":"2022-05-23T11:36:17.52043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# repeat low represented samples more frequently to balance our dataset\nif repeat:\n    class_1_img_id = sub_train_df[(sub_train_df['EncodedPixels']!=-1) & (sub_train_df['ClassId']=='1')]['ImageId'].values\n    class_1_img_id = np.repeat(class_1_img_id, class_1_repeat)\n    class_2_img_id = sub_train_df[(sub_train_df['EncodedPixels']!=-1) & (sub_train_df['ClassId']=='2')]['ImageId'].values\n    class_2_img_id = np.repeat(class_2_img_id, class_2_repeat)\n    class_3_img_id = sub_train_df[(sub_train_df['EncodedPixels']!=-1) & (sub_train_df['ClassId']=='3')]['ImageId'].values\n    class_3_img_id = np.repeat(class_3_img_id, class_3_repeat)\n    class_4_img_id = sub_train_df[(sub_train_df['EncodedPixels']!=-1) & (sub_train_df['ClassId']=='4')]['ImageId'].values\n    class_4_img_id = np.repeat(class_4_img_id, class_4_repeat)\n    has_defects_img_id = np.concatenate([class_1_img_id, class_2_img_id, class_3_img_id, class_4_img_id])\n    no_defects_img_id = np.setdiff1d(sub_train_df['ImageId'].unique(), has_defects_img_id)\n    sub_train_image_ids = np.concatenate([no_defects_img_id, has_defects_img_id])\nelse:\n    # split the training data into train and validation set (stratified)\n    sub_train_image_ids = sub_train_df['ImageId'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:17.522589Z","iopub.execute_input":"2022-05-23T11:36:17.523157Z","iopub.status.idle":"2022-05-23T11:36:20.364427Z","shell.execute_reply.started":"2022-05-23T11:36:17.523104Z","shell.execute_reply":"2022-05-23T11:36:20.363492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = sub_train_image_ids","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:20.369466Z","iopub.execute_input":"2022-05-23T11:36:20.371239Z","iopub.status.idle":"2022-05-23T11:36:20.376721Z","shell.execute_reply.started":"2022-05-23T11:36:20.371198Z","shell.execute_reply":"2022-05-23T11:36:20.375886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'img_h': img_h,\n          'img_w': img_w,\n          'image_dir': train_image_dir,\n          'batch_size': batch_size,\n          'shuffle': True}\n\n# Get Generators\ntraining_generator = DataGenerator(X_train, masks, **params)\nvalidation_generator = DataGenerator(X_val, masks, **params)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:20.381357Z","iopub.execute_input":"2022-05-23T11:36:20.381663Z","iopub.status.idle":"2022-05-23T11:36:20.392522Z","shell.execute_reply.started":"2022-05-23T11:36:20.381606Z","shell.execute_reply":"2022-05-23T11:36:20.391447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check out the shapes\nx, y = training_generator.__getitem__(0)\nprint(x.shape, y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:20.396714Z","iopub.execute_input":"2022-05-23T11:36:20.39724Z","iopub.status.idle":"2022-05-23T11:36:20.81743Z","shell.execute_reply.started":"2022-05-23T11:36:20.397085Z","shell.execute_reply":"2022-05-23T11:36:20.816614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize steel image with four classes of faults in seperate columns\ndef viz_steel_img_mask(img, masks):\n    img = cv2.cvtColor(img.astype('float32'), cv2.COLOR_BGR2RGB)\n    fig, ax = plt.subplots(nrows=1, ncols=4, sharey=True, figsize=(20,10))\n    cmaps = [\"Reds\", \"Blues\", \"Greens\", \"Purples\"]\n    for idx, mask in enumerate(masks):\n        ax[idx].imshow(img)\n        ax[idx].imshow(mask, alpha=0.3, cmap=cmaps[idx])","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:20.818723Z","iopub.execute_input":"2022-05-23T11:36:20.819189Z","iopub.status.idle":"2022-05-23T11:36:20.826842Z","shell.execute_reply.started":"2022-05-23T11:36:20.819137Z","shell.execute_reply":"2022-05-23T11:36:20.825901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets visualize some images with their faults to make sure our data generator is working like it should\nfor ix in range(0,batch_size):\n    if y[ix].sum() > 0:\n        img = x[ix]\n        masks_temp = [y[ix][...,i] for i in range(0,4)]\n        viz_steel_img_mask(img, masks_temp)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:20.828274Z","iopub.execute_input":"2022-05-23T11:36:20.828822Z","iopub.status.idle":"2022-05-23T11:36:26.203663Z","shell.execute_reply.started":"2022-05-23T11:36:20.828659Z","shell.execute_reply":"2022-05-23T11:36:26.202652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Resunet\n\nIn this section we will define the building blocks for our network and train our network.","metadata":{}},{"cell_type":"code","source":"def bn_act(x, act=True):\n    'batch normalization layer with an optinal activation layer'\n    x = tf.keras.layers.BatchNormalization()(x)\n    if act == True:\n        x = tf.keras.layers.Activation('relu')(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:26.205292Z","iopub.execute_input":"2022-05-23T11:36:26.205793Z","iopub.status.idle":"2022-05-23T11:36:26.213312Z","shell.execute_reply.started":"2022-05-23T11:36:26.205589Z","shell.execute_reply":"2022-05-23T11:36:26.212471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv_block(x, filters, kernel_size=3, padding='same', strides=1):\n    'convolutional layer which always uses the batch normalization layer'\n    conv = bn_act(x)\n    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n    return conv","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:26.214716Z","iopub.execute_input":"2022-05-23T11:36:26.215261Z","iopub.status.idle":"2022-05-23T11:36:26.227825Z","shell.execute_reply.started":"2022-05-23T11:36:26.215202Z","shell.execute_reply":"2022-05-23T11:36:26.22712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stem(x, filters, kernel_size=3, padding='same', strides=1):\n    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n    conv = conv_block(conv, filters, kernel_size, padding, strides)\n    shortcut = Conv2D(filters, kernel_size=1, padding=padding, strides=strides)(x)\n    shortcut = bn_act(shortcut, act=False)\n    output = Add()([conv, shortcut])\n    return output","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:26.231689Z","iopub.execute_input":"2022-05-23T11:36:26.231972Z","iopub.status.idle":"2022-05-23T11:36:26.241454Z","shell.execute_reply.started":"2022-05-23T11:36:26.231924Z","shell.execute_reply":"2022-05-23T11:36:26.240626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def residual_block(x, filters, kernel_size=3, padding='same', strides=1):\n    res = conv_block(x, filters, k_size, padding, strides)\n    res = conv_block(res, filters, k_size, padding, 1)\n    shortcut = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n    shortcut = bn_act(shortcut, act=False)\n    output = Add()([shortcut, res])\n    return output","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:26.244444Z","iopub.execute_input":"2022-05-23T11:36:26.244706Z","iopub.status.idle":"2022-05-23T11:36:26.252839Z","shell.execute_reply.started":"2022-05-23T11:36:26.244658Z","shell.execute_reply":"2022-05-23T11:36:26.2518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def upsample_concat_block(x, xskip):\n    u = UpSampling2D((2,2))(x)\n    c = Concatenate()([u, xskip])\n    return c","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:26.254401Z","iopub.execute_input":"2022-05-23T11:36:26.255021Z","iopub.status.idle":"2022-05-23T11:36:26.263011Z","shell.execute_reply.started":"2022-05-23T11:36:26.254939Z","shell.execute_reply":"2022-05-23T11:36:26.262058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def PPM(x):\n    h = tf.keras.backend.int_shape(x)[1]\n    w = tf.keras.backend.int_shape(x)[2]\n    bins = [1,2,4,8]\n    channel = tf.keras.backend.int_shape(x)[3]\n    reduction_dim = int(channel/len(bins))\n    out = x\n    for bin in bins:\n        pool_size = [int(np.round(float(h)/bin)), int(np.round(float(w)/bin))]\n        strides = pool_size\n        u = AveragePooling2D(pool_size, strides, padding='valid')(x)\n        u = Conv2D(reduction_dim, (1, 1), padding='same')(u)\n        u = tf.keras.layers.BatchNormalization()(u)\n        u = tf.keras.layers.Activation('relu')(u)\n        u = UpSampling2D((int(h/bin),int(w/bin)), interpolation = 'bilinear')(u)\n        out = Concatenate()([out, u])\n    out = tf.keras.layers.Conv2D(channel, (1, 1), padding=\"same\")(out)\n    return out","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:26.264653Z","iopub.execute_input":"2022-05-23T11:36:26.26519Z","iopub.status.idle":"2022-05-23T11:36:26.279165Z","shell.execute_reply.started":"2022-05-23T11:36:26.265137Z","shell.execute_reply":"2022-05-23T11:36:26.278263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ResUNet(img_h, img_w):\n    f = [16, 32, 64, 128, 256, 512]\n    inputs = Input((img_h, img_w, 1))\n    \n    ## Encoder\n    e0 = inputs\n    e1 = stem(e0, f[0])\n    e2 = residual_block(e1, f[1], strides=2)\n    e3 = residual_block(e2, f[2], strides=2)\n    e4 = residual_block(e3, f[3], strides=2)\n    e5 = residual_block(e4, f[4], strides=2)\n    e6 = residual_block(e5, f[5], strides=2)\n    \n    ## Bridge\n    b0 = conv_block(e6, f[5], strides=1)\n    b1 = conv_block(b0, f[5], strides=1)\n    \n    ## Decoder\n    u1 = upsample_concat_block(b1, e5)\n    d1 = residual_block(u1, f[5])\n    \n    u2 = upsample_concat_block(d1, e4)\n    d2 = residual_block(u2, f[4])\n    \n    u3 = upsample_concat_block(d2, e3)\n    d3 = residual_block(u3, f[3])\n    \n    u4 = upsample_concat_block(d3, e2)\n    d4 = residual_block(u4, f[2])\n    f4 = PPM(d4)\n    \n    u5 = upsample_concat_block(f4, e1)\n    d5 = residual_block(u5, f[1])\n    \n    outputs = tf.keras.layers.Conv2D(4, (1, 1), padding=\"same\", activation=\"sigmoid\")(d5)\n    model = tf.keras.models.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:26.280745Z","iopub.execute_input":"2022-05-23T11:36:26.281351Z","iopub.status.idle":"2022-05-23T11:36:26.297496Z","shell.execute_reply.started":"2022-05-23T11:36:26.281272Z","shell.execute_reply":"2022-05-23T11:36:26.29679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ResUNet(img_h=img_h, img_w=img_w)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:26.300884Z","iopub.execute_input":"2022-05-23T11:36:26.301132Z","iopub.status.idle":"2022-05-23T11:36:29.28749Z","shell.execute_reply.started":"2022-05-23T11:36:26.301086Z","shell.execute_reply":"2022-05-23T11:36:29.286776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss Functions\n\nAs our classes are highly imbalanced and we are stacking four output layers at once, it is even more important to get the loss function right. Here I will aggregate important loss functions so you can reuse and experiment with them along with me.","metadata":{}},{"cell_type":"code","source":"# Dice similarity coefficient loss, brought to you by: https://github.com/nabsabraham/focal-tversky-unet\ndef dsc(y_true, y_pred):\n    smooth = 1.\n    y_true_f = Flatten()(y_true)\n    y_pred_f = Flatten()(y_pred)\n    intersection = reduce_sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (reduce_sum(y_true_f) + reduce_sum(y_pred_f) + smooth)\n    return score\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dsc(y_true, y_pred)\n    return loss\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:29.290915Z","iopub.execute_input":"2022-05-23T11:36:29.291154Z","iopub.status.idle":"2022-05-23T11:36:29.299933Z","shell.execute_reply.started":"2022-05-23T11:36:29.291106Z","shell.execute_reply":"2022-05-23T11:36:29.299231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Focal Tversky loss, brought to you by:  https://github.com/nabsabraham/focal-tversky-unet\ndef tversky(y_true, y_pred, smooth=1e-6):\n    y_true_pos = tf.keras.layers.Flatten()(y_true)\n    y_pred_pos = tf.keras.layers.Flatten()(y_pred)\n    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos)\n    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n    false_pos = tf.reduce_sum((1-y_true_pos)*y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n\ndef tversky_loss(y_true, y_pred):\n    return 1 - tversky(y_true,y_pred)\n\ndef focal_tversky_loss(y_true,y_pred):\n    pt_1 = tversky(y_true, y_pred)\n    gamma = 0.75\n    return tf.keras.backend.pow((1-pt_1), gamma)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:29.301286Z","iopub.execute_input":"2022-05-23T11:36:29.30159Z","iopub.status.idle":"2022-05-23T11:36:29.313491Z","shell.execute_reply.started":"2022-05-23T11:36:29.301543Z","shell.execute_reply":"2022-05-23T11:36:29.312675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compile & Fit The Model","metadata":{}},{"cell_type":"code","source":"model = ResUNet(img_h=img_h, img_w=img_w)\nadam = tf.keras.optimizers.Adam(lr = 0.05, epsilon = 0.1)\nmodel.compile(optimizer=adam, loss=focal_tversky_loss, metrics=[tversky])","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:29.315183Z","iopub.execute_input":"2022-05-23T11:36:29.315722Z","iopub.status.idle":"2022-05-23T11:36:32.464976Z","shell.execute_reply.started":"2022-05-23T11:36:29.315671Z","shell.execute_reply":"2022-05-23T11:36:32.464234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if load_pretrained_model:\n    try:\n        model.load_weights(pretrained_model_path)\n        print('pre-trained model loaded!')\n    except OSError:\n        print('You need to run the model and load the trained model')","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:32.47025Z","iopub.execute_input":"2022-05-23T11:36:32.470522Z","iopub.status.idle":"2022-05-23T11:36:39.524048Z","shell.execute_reply.started":"2022-05-23T11:36:32.470474Z","shell.execute_reply":"2022-05-23T11:36:39.523106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint(model_save_path, monitor='val_tversky', verbose=1, save_best_only=True, mode='max')","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:36:39.525659Z","iopub.execute_input":"2022-05-23T11:36:39.525966Z","iopub.status.idle":"2022-05-23T11:36:39.531071Z","shell.execute_reply.started":"2022-05-23T11:36:39.525916Z","shell.execute_reply":"2022-05-23T11:36:39.52994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=epochs, verbose=1, callbacks=[checkpoint])","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-05-23T11:36:39.532746Z","iopub.execute_input":"2022-05-23T11:36:39.533095Z","iopub.status.idle":"2022-05-23T11:37:04.863771Z","shell.execute_reply.started":"2022-05-23T11:36:39.533033Z","shell.execute_reply":"2022-05-23T11:37:04.862324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history, index=range(1, epochs+1)).to_csv('./history.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:37:04.864732Z","iopub.status.idle":"2022-05-23T11:37:04.865359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history, index=range(1, epochs+1))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:37:04.866506Z","iopub.status.idle":"2022-05-23T11:37:04.867072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if save_model: \n#     model.save(model_save_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T11:37:04.868358Z","iopub.status.idle":"2022-05-23T11:37:04.86894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Insights\n\nIn this section we take a look at the performance of our model and visually inspect how our predictions look like.","metadata":{}},{"cell_type":"code","source":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nplt.plot(history.history['tversky'])\nplt.plot(history.history['val_tversky'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\n\n# summarize history for loss\nplt.subplot(1,2,2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-23T11:37:04.870048Z","iopub.status.idle":"2022-05-23T11:37:04.870633Z"},"trusted":true},"execution_count":null,"outputs":[]}]}