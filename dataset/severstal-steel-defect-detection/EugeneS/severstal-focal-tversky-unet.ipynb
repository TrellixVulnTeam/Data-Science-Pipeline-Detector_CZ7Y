{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n\nimport cv2\nimport json\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nfrom collections import Counter\nfrom PIL import Image\nimport math\nimport seaborn as sns\nfrom collections import defaultdict\nfrom pathlib import Path\nimport cv2\nfrom tqdm import tqdm\n\nimport albumentations as A\n\nfrom keras.utils import Sequence\n\nfrom IPython.display import clear_output\npd.set_option(\"display.max_rows\", 101)\nplt.style.use('ggplot')\n\nPATH = \"../input/severstal-steel-defect-detection/\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(f\"{PATH}/train.csv\")\ntest_df = pd.read_csv(f\"{PATH}/sample_submission.csv\")\ntrain_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RESTRUCTURE TRAIN DATAFRAME\ntrain_df['ImageId'] = train_df['ImageId_ClassId'].map(lambda x: x.split('.')[0]+'.jpg')\ntrain2 = pd.DataFrame({'ImageId':train_df['ImageId'][::4]})\ntrain2['e1'] = train_df['EncodedPixels'][::4].values\ntrain2['e2'] = train_df['EncodedPixels'][1::4].values\ntrain2['e3'] = train_df['EncodedPixels'][2::4].values\ntrain2['e4'] = train_df['EncodedPixels'][3::4].values\ntrain2.reset_index(inplace=True,drop=True)\ntrain2.fillna('',inplace=True); \ntrain2['count'] = np.sum(train2.iloc[:,1:]!='',axis=1).values\ntrain2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RESTRUCTURE TRAIN DATAFRAME\ntest_df['ImageId'] = test_df['ImageId_ClassId'].map(lambda x: x.split('.')[0]+'.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = train2[train2['count'] > 0]\ntrain2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mask processing"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# https://www.kaggle.com/titericz/building-and-visualizing-masks\ndef rle2maskResize(rle):\n    # CONVERT RLE TO MASK \n    if (pd.isnull(rle))|(rle==''): \n        return np.zeros((256,1600) ,dtype=np.uint8)\n    \n    height= 256\n    width = 1600\n    mask= np.zeros(width*height ,dtype=np.uint8)\n\n    array = np.asarray([int(x) for x in rle.split()])\n    starts = array[0::2]-1\n    lengths = array[1::2]    \n    for index, start in enumerate(starts):\n        mask[int(start):int(start+lengths[index])] = 1\n    \n    return mask.reshape((height,width), order='F')#[::2,::2]\n\ndef mask2contour(mask, width=3):\n    # CONVERT MASK TO ITS CONTOUR\n    w = mask.shape[1]\n    h = mask.shape[0]\n    mask2 = np.concatenate([mask[:,width:],np.zeros((h,width))],axis=1)\n    mask2 = np.logical_xor(mask,mask2)\n    mask3 = np.concatenate([mask[width:,:],np.zeros((width,w))],axis=0)\n    mask3 = np.logical_xor(mask,mask3)\n    return np.logical_or(mask2,mask3) \n\ndef mask2pad(mask, pad=2):\n    # ENLARGE MASK TO INCLUDE MORE SPACE AROUND DEFECT\n    w = mask.shape[1]\n    h = mask.shape[0]\n    \n    # MASK UP\n    for k in range(1,pad,2):\n        temp = np.concatenate([mask[k:,:],np.zeros((k,w))],axis=0)\n        mask = np.logical_or(mask,temp)\n    # MASK DOWN\n    for k in range(1,pad,2):\n        temp = np.concatenate([np.zeros((k,w)),mask[:-k,:]],axis=0)\n        mask = np.logical_or(mask,temp)\n    # MASK LEFT\n    for k in range(1,pad,2):\n        temp = np.concatenate([mask[:,k:],np.zeros((h,k))],axis=1)\n        mask = np.logical_or(mask,temp)\n    # MASK RIGHT\n    for k in range(1,pad,2):\n        temp = np.concatenate([np.zeros((h,k)),mask[:,:-k]],axis=1)\n        mask = np.logical_or(mask,temp)\n    \n    return mask \n\ndef mask2rle(img):\n    tmp = np.rot90( np.flipud( img ), k=3 )\n    rle = []\n    lastColor = 0;\n    startpos = 0\n    endpos = 0\n\n    tmp = tmp.reshape(-1,1)   \n    for i in range( len(tmp) ):\n        if (lastColor==0) and tmp[i]>0:\n            startpos = i\n            lastColor = 1\n        elif (lastColor==1)and(tmp[i]==0):\n            endpos = i-1\n            lastColor = 0\n            rle.append( str(startpos)+' '+str(endpos-startpos+1) )\n    return \" \".join(rle)\n\ndef rle2mask(rle, imgshape):\n    width = imgshape[0]\n    height= imgshape[1]\n    \n    mask= np.zeros( width*height ).astype(np.uint8)\n    \n    array = np.asarray([int(x) for x in rle.split()])\n    starts = array[0::2]\n    lengths = array[1::2]\n\n    current_position = 0\n    for index, start in enumerate(starts):\n        mask[int(start):int(start+lengths[index])] = 1\n        current_position += lengths[index]\n        \n    return np.flipud( np.rot90( mask.reshape(height, width), k=1 ) )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data preprocces functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper function for data visualization\ndef visualize(gray=False, **images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(12, 12))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(n, 1, i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(name)\n        if gray: \n            plt.imshow(image.squeeze())\n        else:\n            plt.imshow(image)\n    plt.show()\n    \n# helper function for data visualization    \ndef denormalize(x):\n    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n    x_max = np.percentile(x, 98)\n    x_min = np.percentile(x, 2)    \n    x = (x - x_min) / (x_max - x_min)\n    x = x.clip(0, 1)\n    return x\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Dataset:\n    \"\"\"\n    Args:\n        images_dir (str): path to images folder\n        masks_dir (str): path to segmentation masks folder\n        class_values (list): values of classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n    \n    \n    def __init__(self, df, subset='train', augmentation=None, preprocessing=None):\n        \n        self.CLASSES = ['e1', 'e2', 'e3', 'e4']\n        self.df = df\n        self.subset = subset\n        \n        if self.subset == \"train\":\n            self.data_path = PATH + 'train_images/'\n            self.images_fps = [os.path.join(self.data_path, image_id) for image_id in self.df['ImageId']]\n        elif self.subset == \"test\":\n            self.data_path = PATH + 'test_images/'\n            self.images_fps = [os.path.join(self.data_path, image_id) for image_id in self.df['ImageId'].unique()]\n            \n        self.masks_fps = self.df.drop('ImageId', 1)\n        \n        \n        # convert str names to class values on masks\n#         self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n        \n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read data\n        image = cv2.imread(self.images_fps[i], cv2.IMREAD_GRAYSCALE)\n#         print(image.shape)\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if len(image.shape) == 2:\n            image = image[..., np.newaxis]\n#             print(image.shape)\n        if self.subset == 'train':\n            masks = np.zeros((256, 1600, len(self.CLASSES)), dtype=np.uint8)\n\n            # extract certain classes from mask (e.g. cars)\n            for j in range(len(self.CLASSES)):\n                masks[..., j] = rle2maskResize(self.df[f'e{j+1}'].iloc[i])\n\n    #         # add background if mask is not binary\n    #         if masks.shape[-1] != 1:\n    #             background = 1 - masks.sum(axis=-1, keepdims=True)\n    #             masks = np.concatenate((masks, background), axis=-1)\n\n            # apply augmentations\n            if self.augmentation:\n                sample = self.augmentation(image=image, mask=masks)\n                image, masks = sample['image'], sample['mask']\n\n            # apply preprocessing\n            if self.preprocessing:\n                sample = self.preprocessing(image=image, mask=masks)\n                image, masks = sample['image'], sample['mask']\n\n            return image/255., masks\n        \n        else:\n            return image/255.\n        \n    def __len__(self):\n        return len(self.images_fps)\n    \n    \nclass Dataloder(Sequence):\n    \"\"\"Load data from dataset and form batches\n    \n    Args:\n        dataset: instance of Dataset class for image loading and preprocessing.\n        batch_size: Integet number of images in batch.\n        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n    \"\"\"\n    \n    def __init__(self, dataset, batch_size=16, shuffle=False, subset='train'):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.indexes = np.arange(len(dataset))\n        self.subset = subset\n        self.on_epoch_end()\n\n    def __getitem__(self, i):\n#         if self.__len__()-1 == i:\n#             self.indexes = np.random.permutation(self.indexes)\n        # collect batch data\n        start = i * self.batch_size\n        stop = (i + 1) * self.batch_size\n        data = []\n        for j in range(start, stop):\n            data.append(self.dataset[j]) #(self.dataset[j][0], list(self.dataset[j][1].transpose(2,0,1))))\n        \n        if self.subset == 'test':\n        # transpose list of lists\n            clear_output()\n            print(i)\n            return np.array(data)\n        \n        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n        #batch = list(map(lambda x: list(x[1].squeeze()), batch))\n        return batch\n    \n    def __len__(self):\n        \"\"\"Denotes the number of batches per epoch\"\"\"\n        return len(self.indexes) // self.batch_size\n    \n    def on_epoch_end(self):\n        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n        if self.shuffle:\n            self.indexes = np.random.permutation(self.indexes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def round_clip_0_1(x, **kwargs):\n    return x.round().clip(0, 1)\n\n# define heavy augmentations\ndef get_training_augmentation():\n    train_transform = [\n\n        A.VerticalFlip(p=0.5),\n        A.HorizontalFlip(p=0.5),\n        A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=0.5, border_mode=0),\n        A.GridDistortion(p=0.5),\n        A.OpticalDistortion(p=0.4, distort_limit=2, shift_limit=0.5)\n    ]\n    return A.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n    test_transform = [\n        A.PadIfNeeded(256, 1600)\n    ]\n    return A.Compose(test_transform)\n\ndef get_preprocessing(preprocessing_fn):\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        A.Lambda(image=preprocessing_fn),\n    ]\n    \n    return A.Compose(_transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at data we have\ndataset = Dataset(train2, augmentation=get_training_augmentation())\nprint(len(dataset))\nimage, mask = dataset[0] # get some sample\nvisualize(gray=True,\n    image=image, \n    e1=mask[..., 0].squeeze(),\n    e2=mask[..., 1].squeeze(),\n    e3=mask[..., 2].squeeze(),\n    e4=mask[..., 3].squeeze(),\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_idx, valid_idx = train_test_split(np.arange(len(train2)), test_size=0.05)\n\ntrain_dataset = Dataset(train2.iloc[train_idx], augmentation=get_training_augmentation())\nvalid_dataset = Dataset(train2.iloc[valid_idx])\n\n\ntrain_dataloader = Dataloder(train_dataset, batch_size=8, shuffle=True)\nvalid_dataloader = Dataloder(valid_dataset, batch_size=1, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Losses"},{"metadata":{"trusted":true},"cell_type":"code","source":"import h5py\n\nfrom keras.models import Model\nfrom keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose, Dense, Reshape\nfrom keras.layers import Activation, add, multiply, Lambda, concatenate\nfrom keras.layers import AveragePooling2D, average, UpSampling2D, Dropout, Flatten, Add, Maximum\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.initializers import glorot_normal, random_normal, random_uniform\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\nfrom keras.losses import binary_crossentropy\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.layers.normalization import BatchNormalization \nfrom keras.applications import VGG19, densenet\nfrom keras.models import load_model\nfrom keras.utils.generic_utils import get_custom_objects\n\nimport tensorflow as tf \n\nimport matplotlib.pyplot as plt \nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve # roc curve tools\n\n\nK.set_image_data_format('channels_last')  # TF dimension ordering in this code\nkinit = 'glorot_normal'\n\nepsilon = 1e-5\nsmooth = 1\n\nclass Swish(Activation):\n    \n    def __init__(self, activation, **kwargs):\n        super(Swish, self).__init__(activation, **kwargs)\n        self.__name__ = 'SWISH'\n\ndef swish(x):\n    return (K.sigmoid(x) * x)\n\nget_custom_objects().update({'swish': Swish(swish)})\n\n\ndef dsc_np(y_true, y_pred):\n    smooth = 1.\n    y_true_f = y_true.flatten()\n    y_pred_f = y_pred.flatten()\n    intersection = np.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n    return 1 - score\n\n\ndef dsc(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dsc(y_true, y_pred)\n    return loss\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss\n\ndef confusion(y_true, y_pred):\n    smooth=1\n    y_pred_pos = K.clip(y_pred, 0, 1)\n    y_pred_neg = 1 - y_pred_pos\n    y_pos = K.clip(y_true, 0, 1)\n    y_neg = 1 - y_pos\n    tp = K.sum(y_pos * y_pred_pos)\n    fp = K.sum(y_neg * y_pred_pos)\n    fn = K.sum(y_pos * y_pred_neg) \n    prec = (tp + smooth)/(tp+fp+smooth)\n    recall = (tp+smooth)/(tp+fn+smooth)\n    return prec, recall\n\ndef tp(y_true, y_pred):\n    smooth = 1\n    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n    y_pos = K.round(K.clip(y_true, 0, 1))\n    tp = (K.sum(y_pos * y_pred_pos) + smooth)/ (K.sum(y_pos) + smooth) \n    return tp \n\ndef tn(y_true, y_pred):\n    smooth = 1\n    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n    y_pred_neg = 1 - y_pred_pos\n    y_pos = K.round(K.clip(y_true, 0, 1))\n    y_neg = 1 - y_pos \n    tn = (K.sum(y_neg * y_pred_neg) + smooth) / (K.sum(y_neg) + smooth )\n    return tn \n\ndef tversky(y_true, y_pred):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n\ndef tversky_loss(y_true, y_pred):\n    return 1 - tversky(y_true,y_pred)\n\ndef focal_tversky(y_true,y_pred):\n    pt_1 = tversky(y_true, y_pred)\n    gamma = 0.75\n    return K.pow((1-pt_1), gamma)\n\n\nclass RAdam(optimizers.Optimizer):\n    \"\"\"RAdam optimizer.\n    # Arguments\n        lr: float >= 0. Learning rate.\n        beta_1: float, 0 < beta < 1. Generally close to 1.\n        beta_2: float, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n        decay: float >= 0. Learning rate decay over each update.\n        weight_decay: float >= 0. Weight decay for each param.\n        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n            algorithm from the paper \"On the Convergence of Adam and\n            Beyond\".\n        total_steps: int >= 0. Total number of training steps. Enable warmup by setting a positive value.\n        warmup_proportion: 0 < warmup_proportion < 1. The proportion of increasing steps.\n        min_lr: float >= 0. Minimum learning rate after warmup.\n    # References\n        - [Adam - A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8)\n        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n        - [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf)\n    \"\"\"\n\n    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., weight_decay=0., amsgrad=False,\n                 total_steps=0, warmup_proportion=0.1, min_lr=0., **kwargs):\n        super(RAdam, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.lr = K.variable(lr, name='lr')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n            self.total_steps = K.variable(total_steps, name='total_steps')\n            self.warmup_proportion = K.variable(warmup_proportion, name='warmup_proportion')\n            self.min_lr = K.variable(min_lr, name='min_lr')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n        self.initial_weight_decay = weight_decay\n        self.initial_total_steps = total_steps\n        self.amsgrad = amsgrad\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n\n        if self.initial_total_steps > 0:\n            warmup_steps = self.total_steps * self.warmup_proportion\n            lr = K.switch(\n                t <= warmup_steps,\n                lr * (t / warmup_steps),\n                self.min_lr + (lr - self.min_lr) * (1.0 - K.minimum(t, self.total_steps) / self.total_steps),\n            )\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='m_' + str(i)) for (i, p) in enumerate(params)]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='v_' + str(i)) for (i, p) in enumerate(params)]\n\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='vhat_' + str(i)) for (i, p) in enumerate(params)]\n        else:\n            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        beta_1_t = K.pow(self.beta_1, t)\n        beta_2_t = K.pow(self.beta_2, t)\n\n        sma_inf = 2.0 / (1.0 - self.beta_2) - 1.0\n        sma_t = sma_inf - 2.0 * t * beta_2_t / (1.0 - beta_2_t)\n\n        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n\n            m_corr_t = m_t / (1.0 - beta_1_t)\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                v_corr_t = K.sqrt(vhat_t / (1.0 - beta_2_t) + self.epsilon)\n                self.updates.append(K.update(vhat, vhat_t))\n            else:\n                v_corr_t = K.sqrt(v_t / (1.0 - beta_2_t) + self.epsilon)\n\n            r_t = K.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                         (sma_t - 2.0) / (sma_inf - 2.0) *\n                         sma_inf / sma_t)\n\n            p_t = K.switch(sma_t >= 5, r_t * m_corr_t / v_corr_t, m_corr_t)\n\n            if self.initial_weight_decay > 0:\n                p_t += self.weight_decay * p\n\n            p_t = p - lr * p_t\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"ACT = 'swish'\n\ndef expend_as(tensor, rep,name):\n    my_repeat = Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={'repnum': rep},  name='psi_up'+name)(tensor)\n    return my_repeat\n\n\ndef AttnGatingBlock(x, g, inter_shape, name):\n    ''' take g which is the spatially smaller signal, do a conv to get the same\n    number of feature channels as x (bigger spatially)\n    do a conv on x to also get same geature channels (theta_x)\n    then, upsample g to be same size as x \n    add x and g (concat_xg)\n    relu, 1x1 conv, then sigmoid then upsample the final - this gives us attn coefficients'''\n    \n    shape_x = K.int_shape(x)  # 32\n    shape_g = K.int_shape(g)  # 16\n\n    theta_x = Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same', name='xl'+name)(x)  # 16\n    shape_theta_x = K.int_shape(theta_x)\n\n    phi_g = Conv2D(inter_shape, (1, 1), padding='same')(g)\n    upsample_g = Conv2DTranspose(inter_shape, (3, 3),strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),padding='same', name='g_up'+name)(phi_g)  # 16\n\n    concat_xg = add([upsample_g, theta_x])\n    act_xg = Activation(ACT)(concat_xg)\n    psi = Conv2D(1, (1, 1), padding='same', name='psi'+name)(act_xg)\n    sigmoid_xg = Activation('sigmoid')(psi)\n    shape_sigmoid = K.int_shape(sigmoid_xg)\n    upsample_psi = UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n\n    upsample_psi = expend_as(upsample_psi, shape_x[3],  name)\n    y = multiply([upsample_psi, x], name='q_attn'+name)\n\n    result = Conv2D(shape_x[3], (1, 1), padding='same',name='q_attn_conv'+name)(y)\n    result_bn = BatchNormalization(name='q_attn_bn'+name)(result)\n    return result_bn\n\ndef UnetConv2D(input, outdim, is_batchnorm, name):\n    x = Conv2D(outdim, (3, 3), strides=(1, 1), kernel_initializer=kinit, padding=\"same\", name=name+'_1')(input)\n    if is_batchnorm:\n        x =BatchNormalization(name=name + '_1_bn')(x)\n    x = Activation(ACT,name=name + '_1_act')(x)\n\n    x = Conv2D(outdim, (3, 3), strides=(1, 1), kernel_initializer=kinit, padding=\"same\", name=name+'_2')(x)\n    if is_batchnorm:\n        x = BatchNormalization(name=name + '_2_bn')(x)\n    x = Activation(ACT, name=name + '_2_act')(x)\n    return x\n\n\ndef UpConv(x, n_exp, name, is_batchnorm=True):\n    for i in range(n_exp-2):\n        x = Conv2DTranspose(4, (4,4), strides=(2,2), padding='same', kernel_initializer=kinit)(x)\n        if is_batchnorm:\n            x = BatchNormalization(name=name + '_bn' + str(i))(x)\n        x = Activation(ACT)(x)\n    x = Conv2DTranspose(4, (4,4), strides=(2,2), padding='same', activation=ACT, kernel_initializer=kinit)(x)\n    return x\n\ndef UnetGatingSignal(input, is_batchnorm, name):\n    ''' this is simply 1x1 convolution, bn, activation '''\n    shape = K.int_shape(input)\n    x = Conv2D(shape[3] * 1, (1, 1), strides=(1, 1), padding=\"same\",  kernel_initializer=kinit, name=name + '_conv')(input)\n    if is_batchnorm:\n        x = BatchNormalization(name=name + '_bn')(x)\n    x = Activation(ACT, name = name + '_act')(x)\n    return x\n\n\ndef attn_reg(opt,input_size, lossfxn):\n    \n    img_input = Input(shape=input_size, name='input_scale1')\n    scale_img_2 = AveragePooling2D(pool_size=(2, 2), name='input_scale2')(img_input)\n    scale_img_3 = AveragePooling2D(pool_size=(2, 2), name='input_scale3')(scale_img_2)\n    scale_img_4 = AveragePooling2D(pool_size=(2, 2), name='input_scale4')(scale_img_3)\n\n    conv1 = UnetConv2D(img_input, 16, is_batchnorm=True, name='conv1')\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    \n    input2 = Conv2D(32, (3, 3), padding='same', activation=ACT, name='conv_scale2')(scale_img_2)\n    input2 = concatenate([input2, pool1], axis=3)\n    conv2 = UnetConv2D(input2, 32, is_batchnorm=True, name='conv2')\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    \n    input3 = Conv2D(64, (3, 3), padding='same', activation=ACT, name='conv_scale3')(scale_img_3)\n    input3 = concatenate([input3, pool2], axis=3)\n    conv3 = UnetConv2D(input3, 64, is_batchnorm=True, name='conv3')\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    \n    input4 = Conv2D(128, (3, 3), padding='same', activation=ACT, name='conv_scale4')(scale_img_4)\n    input4 = concatenate([input4, pool3], axis=3)\n    conv4 = UnetConv2D(input4, 32, is_batchnorm=True, name='conv4')\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n        \n    center = UnetConv2D(pool4, 256, is_batchnorm=True, name='center')\n    \n    g1 = UnetGatingSignal(center, is_batchnorm=True, name='g1')\n    attn1 = AttnGatingBlock(conv4, g1, 64, '_1')\n    up1 = concatenate([Conv2DTranspose(16, (3,3), strides=(2,2), padding='same', activation=ACT, kernel_initializer=kinit)(center), attn1], name='up1')\n\n    g2 = UnetGatingSignal(up1, is_batchnorm=True, name='g2')\n    attn2 = AttnGatingBlock(conv3, g2, 32, '_2')\n    up2 = concatenate([Conv2DTranspose(32, (3,3), strides=(2,2), padding='same', activation=ACT, kernel_initializer=kinit)(up1), attn2], name='up2')\n\n    g3 = UnetGatingSignal(up1, is_batchnorm=True, name='g3')\n    attn3 = AttnGatingBlock(conv2, g3, 16, '_3')\n    up3 = concatenate([Conv2DTranspose(16, (3,3), strides=(2,2), padding='same', activation=ACT, kernel_initializer=kinit)(up2), attn3], name='up3')\n\n    up4 = concatenate([Conv2DTranspose(16, (3,3), strides=(2,2), padding='same', activation=ACT, kernel_initializer=kinit)(up3), conv1], name='up4')\n    \n    conv6 = UnetConv2D(up1, 32, is_batchnorm=True, name='conv6')\n    conv7 = UnetConv2D(up2, 32, is_batchnorm=True, name='conv7')\n    conv8 = UnetConv2D(up3, 32, is_batchnorm=True, name='conv8')\n    conv9 = UnetConv2D(up4, 32, is_batchnorm=True, name='conv9')\n\n    out6 = Conv2D(1, (1, 1), activation=ACT, name='pred1')(conv6)\n    out7 = Conv2D(1, (1, 1), activation=ACT, name='pred2')(conv7)\n    out8 = Conv2D(1, (1, 1), activation=ACT, name='pred3')(conv8)\n    out9 = Conv2D(4, (1, 1), activation=ACT, name='final')(conv9)\n    \n    out6 = UpConv(out6, 4, name='total_final6')\n    out7 = UpConv(out7, 3, name='total_final7')\n    out8 = UpConv(out8, 2, name='total_final8')\n    \n    out = Add()([out6,out7,out8,out9])\n    \n    out = Activation('softmax')(out)\n#     out = Lambda(lambda x: K.cast(x > 0.5, dtype='float16'), output_shape=(256, 1600, 4))(out)\n    \n    model = Model(inputs=[img_input], outputs=[out])\n \n    loss = {'pred1':lossfxn,\n            'pred2':lossfxn,\n            'pred3':lossfxn,\n            'final': tversky_loss}\n    \n#     loss_weights = {'pred1':1,\n#                     'pred2':1,\n#                     'pred3':1,\n#                     'final':1}\n#     model.compile(optimizer=opt, loss=tversky_loss, loss_weights=loss_weights, metrics=[dsc])\n    model.compile(optimizer=opt, loss=focal_tversky, metrics=[dice_loss])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_size = (256, 1600, 1)\n#adam = RAdam(3e-4, decay=0.99, min_lr=1e-6)\nadam = Adam(3e-4)\nes_callback = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\ncheckpointer = ModelCheckpoint(filepath='w_tversky_gray.hdf5', verbose=1, save_best_only=True)\nreduce_lr = ReduceLROnPlateau(patience=7, factor=0.5, min_lr=1e-6)\ncallbacks = [es_callback, checkpointer, reduce_lr]\n\nmodel = attn_reg(adam, input_size, focal_tversky)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train model\nhistory = model.fit_generator(\n    train_dataloader, \n    steps_per_epoch=len(train_dataloader), \n    epochs=15, \n    callbacks=callbacks, \n    validation_data=valid_dataloader, \n    validation_steps=len(valid_dataloader),\n)\n# model.load_weights(\"../input/weights-gray/w_tversky_gray.hdf5\")\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_dataset = Dataset(test_df, 'test')\n# test_dataloader = Dataloder(test_dataset, batch_size=1, shuffle=False, subset='test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from tqdm import tqdm_notebook as tqdm\n\n# idx = test_df.ImageId.unique()\n# for pct in tqdm(range(len(test_dataset)), total=len(test_dataset)):\n    \n#     X = test_dataset[pct]\n#     msk = model.predict(X[np.newaxis, ...]).squeeze()\n#     msk = (msk > 0.5).astype(int)\n#     suma = np.sum(msk.reshape((-1, 4)), axis=0)\n#     #print(suma)\n#     results = [mask2rle(msk[..., m]) if suma[m] > 1000 else ' ' for m in range(4)]\n#     test_df.loc[test_df.ImageId == idx[pct], 'EncodedPixels'] = results\n    \n# test_df.drop('ImageId', 1, inplace=True)\n# test_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# thrs = [0.46, 0.5]\n# results = {}\n# for thr in thrs:\n#     print(thr)\n#     preds = []\n#     for pct in range(100):\n#         X, y = test_dataset[pct]\n#         pred = model.predict(X[np.newaxis, ...])\n#         score = dsc_np(y[np.newaxis, ...], (pred > thr).astype(int))\n#         preds.append(score)\n        \n#     med = np.median(np.array(preds))\n#     mean = np.mean(np.array(preds))\n#     std = np.std(np.array(preds))\n#     results[str(thr)] = (mean, med, std)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}