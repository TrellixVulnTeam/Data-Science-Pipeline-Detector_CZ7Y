{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Abstract\nThe aim of this notebook is to check how error is distributed in this task. This notebook is based on the model trained and inferenced by [@rishabhiitbhu](https://www.kaggle.com/rishabhiitbhu) as well as mask visualization by [@go1dfish](https://www.kaggle.com/go1dfish). So, please, give them some love too.\n* https://www.kaggle.com/rishabhiitbhu/unet-pytorch-inference-kernel\n* https://www.kaggle.com/go1dfish/clear-mask-visualization-and-simple-eda"},{"metadata":{},"cell_type":"markdown","source":"## Model Inference\nGiven the fact that naive baseline (no masks predicted) is this task scores around 85+, I was amazed that some participants trained their segmentation models and scored way below or slightly above this score. Even today, one week prior to deadline, top single model (no classification or blend or TTA added) scores around 88+ only. What lies behind this fact and how can we improve our scores? \n\nIn my experiment I stuck to following pipeline:\n* I took model trained by [@rishabhiitbhu](https://www.kaggle.com/rishabhiitbhu).\n* Used it to predict masks on entire train dataset.\n* Compared actual masks vs predicted.\n\n[*Note: Yes, I know that the most appropriate way here is to check masks separately on train and val subsets of original train dataset. Due to lack of free time and kaggle GPU limits I stuck to my pipeline anyway.*]\n### Code part\nBasically, code below (with minor fixes) is original inference code from U-net 0.88+ model. I used it to predict masks for entire train dataset. Nothing interesing, thus code is hidden."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!pip install ../input/pretrainedmodels/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4/ > /dev/null\npackage_path = '../input/unetmodelscript'\nimport sys\nsys.path.append(package_path)\n\nimport pdb\nimport os\nimport cv2\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm_notebook as tqdm\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader, Dataset\nfrom albumentations import (Normalize, Compose)\nfrom albumentations.pytorch import ToTensor\nimport torch.utils.data as data\nfrom model import Unet\n\n#https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\ndef mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\nclass TestDataset(Dataset):\n    '''Dataset for test prediction'''\n    def __init__(self, root, df, mean, std):\n        self.root = root\n        df['ImageId'] = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n        self.fnames = df['ImageId'].unique().tolist()\n        self.num_samples = len(self.fnames)\n        self.transform = Compose(\n            [\n                Normalize(mean=mean, std=std, p=1),\n                ToTensor(),\n            ]\n        )\n\n    def __getitem__(self, idx):\n        fname = self.fnames[idx]\n        path = os.path.join(self.root, fname)\n        image = cv2.imread(path)\n        images = self.transform(image=image)[\"image\"]\n        return fname, images\n\n    def __len__(self):\n        return self.num_samples\n    \ndef post_process(probability, threshold, min_size):\n    '''Post processing of each predicted mask, components with lesser number of pixels\n    than `min_size` are ignored'''\n    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = np.zeros((256, 1600), np.float32)\n    num = 0\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            predictions[p] = 1\n            num += 1\n    return predictions, num\n\nsample_submission_path = '../input/severstal-steel-defect-detection/train.csv'\ntest_data_folder = \"../input/severstal-steel-defect-detection/train_images\"\n\n# initialize test dataloader\nbest_threshold = 0.5\nnum_workers = 2\nbatch_size = 4\nprint('best_threshold', best_threshold)\nmin_size = 3500\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)\ndf = pd.read_csv(sample_submission_path)\ntestset = DataLoader(\n    TestDataset(test_data_folder, df, mean, std),\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True\n)\n\n# Initialize mode and load trained weights\nckpt_path = \"../input/unetstartermodelfile/model.pth\"\ndevice = torch.device(\"cuda\")\nmodel = Unet(\"resnet18\", encoder_weights=None, classes=4, activation=None)\nmodel.to(device)\nmodel.eval()\nstate = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\nmodel.load_state_dict(state[\"state_dict\"])\n\n# start prediction\npredictions = []\nfor i, batch in enumerate(tqdm(testset)):\n    fnames, images = batch\n    batch_preds = torch.sigmoid(model(images.to(device)))\n    batch_preds = batch_preds.detach().cpu().numpy()\n    for fname, preds in zip(fnames, batch_preds):\n        for cls, pred in enumerate(preds):\n            pred, num = post_process(pred, best_threshold, min_size)\n            rle = mask2rle(pred)\n            name = fname + f\"_{cls+1}\"\n            predictions.append([name, rle])\n\npreds = pd.DataFrame(predictions, columns=['ImageId_ClassId', 'EncodedPixels'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below provided code draws original and predicted masks on pictures. Source of code listed at the beginning of this kernel. Nothing interesting also. Code is hidden."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\npd.set_option(\"display.max_rows\", 101)\nimport os\nprint(os.listdir(\"../input\"))\nimport cv2\nimport json\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams[\"font.size\"] = 15\nimport seaborn as sns\nfrom collections import Counter\nfrom PIL import Image\nimport math\nimport seaborn as sns\nfrom collections import defaultdict\nfrom pathlib import Path\nimport cv2\n\ntrain_df = pd.read_csv(\"../input/severstal-steel-defect-detection/train.csv\")\n\npalet = [(249, 192, 12), (0, 185, 241), (114, 0, 218), (249,50,12)]\n\ndef name_and_mask(start_idx):\n    col = start_idx\n    img_names = [str(i).split(\"_\")[0] for i in train_df.iloc[col:col+4, 0].values]\n    if not (img_names[0] == img_names[1] == img_names[2] == img_names[3]):\n        raise ValueError\n\n    labels = train_df.iloc[col:col+4, 1]\n    mask = np.zeros((256, 1600, 4), dtype=np.uint8)\n\n    for idx, label in enumerate(labels.values):\n        if label is not np.nan:\n            mask_label = np.zeros(1600*256, dtype=np.uint8)\n            label = label.split(\" \")\n            positions = map(int, label[0::2])\n            length = map(int, label[1::2])\n            for pos, le in zip(positions, length):\n                mask_label[pos-1:pos+le-1] = 1\n            mask[:, :, idx] = mask_label.reshape(256, 1600, order='F')\n    return img_names[0], mask\n\ndef show_mask_image(col):\n    name, mask = name_and_mask(col)\n    img = cv2.imread(str(train_path / name))\n    fig, ax = plt.subplots(figsize=(10, 10))\n\n    for ch in range(4):\n        contours, _ = cv2.findContours(mask[:, :, ch], cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n        for i in range(0, len(contours)):\n            cv2.polylines(img, contours[i], True, palet[ch], 2)\n    ax.set_title('True mask of '+name)\n    ax.imshow(img)\n    plt.show()\n    \ntrain_path = Path(\"../input/severstal-steel-defect-detection/train_images/\")\n\nidx_no_defect = []\nidx_class_1 = []\nidx_class_2 = []\nidx_class_3 = []\nidx_class_4 = []\nidx_class_multi = []\nidx_class_triple = []\n\nfor col in range(0, len(train_df), 4):\n    img_names = [str(i).split(\"_\")[0] for i in train_df.iloc[col:col+4, 0].values]\n    if not (img_names[0] == img_names[1] == img_names[2] == img_names[3]):\n        raise ValueError\n        \n    labels = train_df.iloc[col:col+4, 1]\n    if labels.isna().all():\n        idx_no_defect.append(col)\n    elif (labels.isna() == [False, True, True, True]).all():\n        idx_class_1.append(col)\n    elif (labels.isna() == [True, False, True, True]).all():\n        idx_class_2.append(col)\n    elif (labels.isna() == [True, True, False, True]).all():\n        idx_class_3.append(col)\n    elif (labels.isna() == [True, True, True, False]).all():\n        idx_class_4.append(col)\n    elif labels.isna().sum() == 1:\n        idx_class_triple.append(col)\n    else:\n        idx_class_multi.append(col)\n        \npred_df = preds.copy()\npred_df = pred_df.replace('', np.nan, regex=True)\n\ndef name_and_mask_pred(start_idx):\n    col = start_idx\n    img_names = [str(i).split(\"_\")[0] for i in pred_df.iloc[col:col+4, 0].values]\n    if not (img_names[0] == img_names[1] == img_names[2] == img_names[3]):\n        raise ValueError\n\n    labels = pred_df.iloc[col:col+4, 1]\n    mask = np.zeros((256, 1600, 4), dtype=np.uint8)\n\n    for idx, label in enumerate(labels.values):\n        if label is not np.nan:\n            mask_label = np.zeros(1600*256, dtype=np.uint8)\n            label = label.split(\" \")\n            positions = map(int, label[0::2])\n            length = map(int, label[1::2])\n            for pos, le in zip(positions, length):\n                mask_label[pos-1:pos+le-1] = 1\n            mask[:, :, idx] = mask_label.reshape(256, 1600, order='F')\n    return img_names[0], mask\n\ndef show_mask_image_pred(col):\n    name, mask = name_and_mask_pred(col)\n    img = cv2.imread(str(train_path / name))\n    fig, ax = plt.subplots(figsize=(10, 10))\n\n    for ch in range(4):\n        contours, _ = cv2.findContours(mask[:, :, ch], cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n        for i in range(0, len(contours)):\n            cv2.polylines(img, contours[i], True, palet[ch], 2)\n    ax.set_title('Pred mask of '+name)\n    ax.imshow(img)\n    plt.show()\n    \nidx_no_defect_pred = []\nidx_class_1_pred = []\nidx_class_2_pred = []\nidx_class_3_pred = []\nidx_class_4_pred = []\nidx_class_multi_pred = []\nidx_class_triple_pred = []\n\nfor col in range(0, len(pred_df), 4):\n    img_names = [str(i).split(\"_\")[0] for i in pred_df.iloc[col:col+4, 0].values]\n    if not (img_names[0] == img_names[1] == img_names[2] == img_names[3]):\n        raise ValueError\n        \n    labels = pred_df.iloc[col:col+4, 1]\n    if labels.isna().all():\n        idx_no_defect_pred.append(col)\n    elif (labels.isna() == [False, True, True, True]).all():\n        idx_class_1_pred.append(col)\n    elif (labels.isna() == [True, False, True, True]).all():\n        idx_class_2_pred.append(col)\n    elif (labels.isna() == [True, True, False, True]).all():\n        idx_class_3_pred.append(col)\n    elif (labels.isna() == [True, True, True, False]).all():\n        idx_class_4_pred.append(col)\n    elif labels.isna().sum() == 1:\n        idx_class_triple_pred.append(col)\n    else:\n        idx_class_multi_pred.append(col)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Masks visualization\nLet's finally draw some masks, shall we?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from random import sample \n\ndefects = list(set(idx_class_1).intersection(set((idx_class_1_pred))))\nlen(idx_class_1), len(idx_class_1_pred), len(defects)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx in sample(defects,10):\n    show_mask_image_pred(idx)\n    show_mask_image(idx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! Out of 769 original defects of the Class 1 our model was able to predict only 17! Maybe universal 3500 threshold is not good idea? It is also worth to mention that our model predicts very sharp masks whereas original masks seems to be smoothed. "},{"metadata":{"trusted":true},"cell_type":"code","source":"defects = list(set(idx_class_2).intersection(set((idx_class_2_pred))))\nlen(idx_class_2), len(idx_class_2_pred), len(defects)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another wow! Zero masks correctly predicted for Class 2. It is clearly not a good idea to use universal threshold. Maybe it is better idea to stick to thresholds mentioned [here](https://www.kaggle.com/bigironsphere/boost-your-score-with-pixel-counts-0-886-0-888)?"},{"metadata":{"trusted":true},"cell_type":"code","source":"defects = list(set(idx_class_3).intersection(set((idx_class_3_pred))))\nlen(idx_class_3), len(idx_class_3_pred), len(defects)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx in sample(defects,10):\n    show_mask_image_pred(idx)\n    show_mask_image(idx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems pretty ok to me. Most masks are predicted very well for Class 3. Yet we again can see sharpness of predicted masks versus original ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"defects = list(set(idx_class_4).intersection(set((idx_class_4_pred))))\nlen(idx_class_4), len(idx_class_4_pred), len(defects)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx in sample(defects,10):\n    show_mask_image_pred(idx)\n    show_mask_image(idx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most masks are predicted for Class 4 with somewhat accuracy. It is also worth to mention that model predicted slightly more masks than actual number of masks for this class. Now, let's look to multiclass cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"defects = list(set(idx_class_multi).intersection(set((idx_class_multi_pred))))\nlen(idx_class_multi), len(idx_class_multi_pred), len(defects)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx in sample(defects,10):\n    show_mask_image_pred(idx)\n    show_mask_image(idx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In previous case we have more masks that actual number of cases of Class 4. Here we have way less masks for multiclass. Maybe it is the problem?"},{"metadata":{"trusted":true},"cell_type":"code","source":"defects = list(set(idx_class_4_pred).difference(set((idx_class_4))))\nprint(len(defects))\nfor idx in sample(defects,10):\n    show_mask_image_pred(idx)\n    show_mask_image(idx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yep. As predicted, when we dealt with multiclass (particularly Class 3 + Class 4) model seems to miss to predict Class 3 correctly alongside with Class 4."},{"metadata":{},"cell_type":"markdown","source":"## Conslusion (Key points)\n* Since we deal with channel-averaged Dice coefficient, it is crucial to deal something with Class 1 and Class 2. Our model performs sadly when it comes to prediction of these two classes. Maybe some additional classification for these classes + extreme thresholds will do the job?\n* Universal threshold is not good 100%. Given the fact that model missed almost all cases of the first two classes with universal threshold of 3500, we should develop thresholds for each class separately in this task.\n* There is some mismatch when it comes to Class 3 + Class 4. Model seems to miss most cases when both classes present on the picture. Will additional classifier (for Class 3 + Class 4 presence) help here?\n* Blend, TTA, Classification. Making your model robust (rather then complicated) seems to be the key in this competition."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}