{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import abc\nimport yaml\nimport torchvision.transforms as T\nimport torchvision.models as models\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch\nimport random\nimport pandas as pd\nimport os\nimport numpy as np\nimport re\nimport math\nimport collections\nfrom functools import partial\nfrom copy import deepcopy\nimport logging.config\nimport datetime\nimport cv2\nimport warnings\nfrom albumentations import HorizontalFlip, Normalize, Compose, ImageOnlyTransform\nfrom albumentations.pytorch import ToTensor\nfrom collections import Sequence\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom torch.jit import load\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils import model_zoo\nfrom pathlib import Path\nfrom os.path import join\nfrom collections import OrderedDict\nfrom scipy import ndimage","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BasicConv2d(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, bias=False) # verify bias false\n        self.bn = nn.BatchNorm2d(out_planes,\n                                 eps=0.001, # value found in tensorflow\n                                 momentum=0.1, # default pytorch value\n                                 affine=True)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Mixed_5b(nn.Module):\n\n    def __init__(self):\n        super(Mixed_5b, self).__init__()\n\n        self.branch0 = BasicConv2d(192, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(192, 48, kernel_size=1, stride=1),\n            BasicConv2d(48, 64, kernel_size=5, stride=1, padding=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(192, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(192, 64, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block35(nn.Module):\n\n    def __init__(self, scale=1.0):\n        super(Block35, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(320, 32, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 48, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(48, 64, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.conv2d = nn.Conv2d(128, 320, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_6a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_6a, self).__init__()\n\n        self.branch0 = BasicConv2d(320, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Block17(nn.Module):\n\n    def __init__(self, scale=1.0):\n        super(Block17, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(1088, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 128, kernel_size=1, stride=1),\n            BasicConv2d(128, 160, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(160, 192, kernel_size=(7,1), stride=1, padding=(3,0))\n        )\n\n        self.conv2d = nn.Conv2d(384, 1088, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_7a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_7a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(288, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch3 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block8(nn.Module):\n\n    def __init__(self, scale=1.0, noReLU=False):\n        super(Block8, self).__init__()\n\n        self.scale = scale\n        self.noReLU = noReLU\n\n        self.branch0 = BasicConv2d(2080, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(2080, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1,3), stride=1, padding=(0,1)),\n            BasicConv2d(224, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n        )\n\n        self.conv2d = nn.Conv2d(448, 2080, kernel_size=1, stride=1)\n        if not self.noReLU:\n            self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        if not self.noReLU:\n            out = self.relu(out)\n        return out\n\n\nclass InceptionResNetV2(nn.Module):\n\n    def __init__(self, num_classes=1001):\n        super(InceptionResNetV2, self).__init__()\n        # Special attributs\n        self.input_space = None\n        self.input_size = (299, 299, 3)\n        self.mean = None\n        self.std = None\n        # Modules\n        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)\n        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)\n        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\n        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)\n        self.maxpool_5a = nn.MaxPool2d(3, stride=2)\n        self.mixed_5b = Mixed_5b()\n        self.repeat = nn.Sequential(\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17)\n        )\n        self.mixed_6a = Mixed_6a()\n        self.repeat_1 = nn.Sequential(\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10)\n        )\n        self.mixed_7a = Mixed_7a()\n        self.repeat_2 = nn.Sequential(\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20)\n        )\n        self.block8 = Block8(noReLU=True)\n        self.conv2d_7b = BasicConv2d(2080, 1536, kernel_size=1, stride=1)\n        self.avgpool_1a = nn.AvgPool2d(8, count_include_pad=False)\n        self.last_linear = nn.Linear(1536, num_classes)\n\n    def features(self, input):\n        x = self.conv2d_1a(input)\n        x = self.conv2d_2a(x)\n        x = self.conv2d_2b(x)\n        x = self.maxpool_3a(x)\n        x = self.conv2d_3b(x)\n        x = self.conv2d_4a(x)\n        x = self.maxpool_5a(x)\n        x = self.mixed_5b(x)\n        x = self.repeat(x)\n        x = self.mixed_6a(x)\n        x = self.repeat_1(x)\n        x = self.mixed_7a(x)\n        x = self.repeat_2(x)\n        x = self.block8(x)\n        x = self.conv2d_7b(x)\n        return x\n\n    def logits(self, features):\n        x = self.avgpool_1a(features)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\nclass InceptionResNetV2Encoder(InceptionResNetV2):\n\n    def __init__(self, in_channels=3, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.in_channels = in_channels \n        self.pretrained = False\n\n        # correct paddings\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m.kernel_size == (3, 3):\n                    m.padding = (1, 1)\n            if isinstance(m, nn.MaxPool2d):\n                m.padding = (1, 1)\n\n        # remove linear layers\n        del self.avgpool_1a\n        del self.last_linear\n\n    def forward(self, x):\n        x = self.conv2d_1a(x)\n        x = self.conv2d_2a(x)\n        x = self.conv2d_2b(x)\n        x0 = x\n\n        x = self.maxpool_3a(x)\n        x = self.conv2d_3b(x)\n        x = self.conv2d_4a(x)\n        x1 = x\n\n        x = self.maxpool_5a(x)\n        x = self.mixed_5b(x)\n        x = self.repeat(x)\n        x2 = x\n\n        x = self.mixed_6a(x)\n        x = self.repeat_1(x)\n        x3 = x\n\n        x = self.mixed_7a(x)\n        x = self.repeat_2(x)\n        x = self.block8(x)\n        x = self.conv2d_7b(x)\n        x4 = x\n\n        features = [x4, x3, x2, x1, x0]\n        return features\n\n\ninception_encoders = {\n    'inceptionresnetv2': {\n        'encoder': InceptionResNetV2Encoder,\n        'pretrained_settings': {\n            'imagenet': {\n                'url': 'http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth',\n                'input_space': 'RGB',\n                'input_size': [3, 299, 299],\n                'input_range': [0, 1],\n                'mean': [0.5, 0.5, 0.5],\n                'std': [0.5, 0.5, 0.5],\n                'num_classes': 1000\n            },\n        },\n        'out_shapes': (1536, 1088, 320, 192, 64),\n        'params': {\n            'num_classes': 1000,\n        }\n    }\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BasicConv2d(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, bias=False) # verify bias false\n        self.bn = nn.BatchNorm2d(out_planes,\n                                 eps=0.001, # value found in tensorflow\n                                 momentum=0.1, # default pytorch value\n                                 affine=True)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Mixed_3a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_3a, self).__init__()\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n        self.conv = BasicConv2d(64, 96, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        x0 = self.maxpool(x)\n        x1 = self.conv(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed_4a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_4a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 64, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(64, 64, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(64, 96, kernel_size=(3,3), stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed_5a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_5a, self).__init__()\n        self.conv = BasicConv2d(192, 192, kernel_size=3, stride=2)\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.conv(x)\n        x1 = self.maxpool(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Inception_A(nn.Module):\n\n    def __init__(self):\n        super(Inception_A, self).__init__()\n        self.branch0 = BasicConv2d(384, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(384, 96, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Reduction_A(nn.Module):\n\n    def __init__(self):\n        super(Reduction_A, self).__init__()\n        self.branch0 = BasicConv2d(384, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(384, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(224, 256, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Inception_B(nn.Module):\n\n    def __init__(self):\n        super(Inception_B, self).__init__()\n        self.branch0 = BasicConv2d(1024, 384, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(224, 256, kernel_size=(7,1), stride=1, padding=(3,0))\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(192, 224, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(224, 224, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(224, 256, kernel_size=(1,7), stride=1, padding=(0,3))\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(1024, 128, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Reduction_B(nn.Module):\n\n    def __init__(self):\n        super(Reduction_B, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1024, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(256, 320, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(320, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Inception_C(nn.Module):\n\n    def __init__(self):\n        super(Inception_C, self).__init__()\n\n        self.branch0 = BasicConv2d(1536, 256, kernel_size=1, stride=1)\n\n        self.branch1_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch1_1a = BasicConv2d(384, 256, kernel_size=(1,3), stride=1, padding=(0,1))\n        self.branch1_1b = BasicConv2d(384, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n\n        self.branch2_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch2_1 = BasicConv2d(384, 448, kernel_size=(3,1), stride=1, padding=(1,0))\n        self.branch2_2 = BasicConv2d(448, 512, kernel_size=(1,3), stride=1, padding=(0,1))\n        self.branch2_3a = BasicConv2d(512, 256, kernel_size=(1,3), stride=1, padding=(0,1))\n        self.branch2_3b = BasicConv2d(512, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(1536, 256, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n\n        x1_0 = self.branch1_0(x)\n        x1_1a = self.branch1_1a(x1_0)\n        x1_1b = self.branch1_1b(x1_0)\n        x1 = torch.cat((x1_1a, x1_1b), 1)\n\n        x2_0 = self.branch2_0(x)\n        x2_1 = self.branch2_1(x2_0)\n        x2_2 = self.branch2_2(x2_1)\n        x2_3a = self.branch2_3a(x2_2)\n        x2_3b = self.branch2_3b(x2_2)\n        x2 = torch.cat((x2_3a, x2_3b), 1)\n\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass InceptionV4(nn.Module):\n\n    def __init__(self, num_classes=1001):\n        super(InceptionV4, self).__init__()\n        # Special attributs\n        self.input_space = None\n        self.input_size = (299, 299, 3)\n        self.mean = None\n        self.std = None\n        # Modules\n        self.features = nn.Sequential(\n            BasicConv2d(3, 32, kernel_size=3, stride=2),\n            BasicConv2d(32, 32, kernel_size=3, stride=1),\n            BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            Mixed_3a(),\n            Mixed_4a(),\n            Mixed_5a(),\n            Inception_A(),\n            Inception_A(),\n            Inception_A(),\n            Inception_A(),\n            Reduction_A(), # Mixed_6a\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Reduction_B(), # Mixed_7a\n            Inception_C(),\n            Inception_C(),\n            Inception_C()\n        )\n        self.last_linear = nn.Linear(1536, num_classes)\n\n    def logits(self, features):\n        #Allows image of any size to be processed\n        adaptiveAvgPoolWidth = features.shape[2]\n        x = F.avg_pool2d(features, kernel_size=adaptiveAvgPoolWidth)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\nclass InceptionV4Encoder(InceptionV4):\n\n    def __init__(self, in_channels=3, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.in_channels = in_channels\n        self.pretrained = False\n        self.features[0] = BasicConv2d(self.in_channels, 32, kernel_size=3, stride=2, padding=1)\n        self.features[1] = BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n\n        self.chunks = [3, 5, 9, 15]\n\n        # correct paddings\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m.kernel_size == (3, 3):\n                    m.padding = (1, 1)\n            if isinstance(m, nn.MaxPool2d):\n                m.padding = (1, 1)\n\n        # remove linear layers\n        del self.last_linear\n\n    def forward(self, x):\n        x0 = self.features[:self.chunks[0]](x)\n        x1 = self.features[self.chunks[0]:self.chunks[1]](x0)\n        x2 = self.features[self.chunks[1]:self.chunks[2]](x1)\n        x3 = self.features[self.chunks[2]:self.chunks[3]](x2)\n        x4 = self.features[self.chunks[3]:](x3)\n\n        features = [x4, x3, x2, x1, x0]\n        return features\n\n\ninceptionv4_encoders = {\n    'inceptionv4': {\n        'encoder': InceptionV4Encoder,\n        'pretrained_settings': {\n            'imagenet': {\n                'url': 'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth',\n                'input_space': 'RGB',\n                'input_size': [3, 299, 299],\n                'input_range': [0, 1],\n                'mean': [0.5, 0.5, 0.5],\n                'std': [0.5, 0.5, 0.5],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (1536, 1024, 384, 192, 64),\n        'params': {\n            'num_classes': 1001,\n        }\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nResNet code gently borrowed from\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n\"\"\"\n\n__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n\npretrained_settings = {\n    'senet154': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet50': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet101': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet152': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext50_32x4d': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext101_32x4d': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    Base class for bottlenecks that implements `forward()` method.\n    \"\"\"\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    \"\"\"\n    Bottleneck for SENet154.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    \"\"\"\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    \"\"\"\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        \"\"\"\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        \"\"\"\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings['num_classes'], \\\n        'num_classes should be {}, but is {}'.format(\n            settings['num_classes'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings['url']))\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\n\n\ndef senet154(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['senet154'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet50'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet101'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet152'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- weights.py --\n\ndef cycle_rgb_weights(weights, n):\n    \"\"\"Repeat RGB weights n times. Assumes channels are dim 1\"\"\"\n    slices = [(c % 3, c % 3 + 1) for c in range(n)]  # slice a:a+1 to keep dims\n    new_weights = torch.cat([\n        weights[:, a:b, :, :] for a, b in slices\n    ], dim=1)\n    return new_weights\n\n\ndef avg_rgb_weights(weights):\n    new_weights = weights.mean(dim=1, keepdim=True)\n    return new_weights\n\n\ndef transfer_weights(pretrained, replacement, method='cycle'):\n    \"\"\"\n    Transform pretrained weights to be used for a layer with a different number of channels.\n    \"\"\"\n    if method == 'cycle':\n        n = replacement.in_channels\n        weights = cycle_rgb_weights(pretrained.weight, n)\n    elif method == 'avg':\n        weights = avg_rgb_weights(pretrained.weight)\n    else:\n        raise NotImplementedError('`method` must be \"cycle\" or \"avg\", received {}'.format(method))\n    replacement.weight = nn.Parameter(weights)\n    return replacement\n\n# -- model.py --\n\nclass Model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n    def initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n# -- encoder_decoder.py --\n\nclass EncoderDecoder(Model):\n\n    def __init__(self, encoder, decoder, activation):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n        if callable(activation) or activation is None:\n            self.activation = activation\n        elif activation == 'softmax':\n            self.activation = nn.Softmax(dim=1)\n        elif activation == 'sigmoid':\n            self.activation = nn.Sigmoid()\n        else:\n            raise ValueError('Activation should be \"sigmoid\"/\"softmax\"/callable/None')\n\n    def forward(self, x):\n        \"\"\"Sequentially pass `x` trough model`s `encoder` and `decoder` (return logits!)\"\"\"\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n    def predict(self, x):\n        \"\"\"Inference method. Switch model to `eval` mode, call `.forward(x)`\n        and apply activation function (if activation is not `None`) with `torch.no_grad()`\n\n        Args:\n            x: 4D torch tensor with shape (batch_size, channels, height, width)\n\n        Return:\n            prediction: 4D torch tensor with shape (batch_size, classes, height, width)\n\n        \"\"\"\n        if self.training:\n            self.eval()\n\n        with torch.no_grad():\n            x = self.forward(x)\n            if self.activation:\n                x = self.activation(x)\n\n        return x\n\n    \n# -- efficientnet.py\n\nclass EfficientNetEncoder(nn.Module):\n    \"\"\"\n    Implementation taken from: https://github.com/lukemelas/EfficientNet-PyTorch\n    \"\"\"\n\n    def __init__(self,\n        width_coeff,\n        depth_coeff,\n        image_size,\n        dropout_rate,\n        drop_connect_rate,\n        block_chunks,\n        in_channels = 3  # rgb\n    ):\n        super().__init__()\n        self._blocks_args = self.get_block_args()\n        self._global_params = get_global_params(width_coeff, depth_coeff, image_size,\n                                                dropout_rate, drop_connect_rate)\n        self.block_chunks = block_chunks\n        self.in_channels = in_channels\n\n        # Get static or dynamic convolution depending on image size\n        Conv2d = get_same_padding_conv2d(image_size=self._global_params.image_size)\n\n        # Batch norm parameters\n        bn_mom = 1 - self._global_params.batch_norm_momentum\n        bn_eps = self._global_params.batch_norm_epsilon\n\n        # Stem\n        out_channels = round_filters(32, self._global_params)  # number of output channels\n        self._conv_stem = Conv2d(self.in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n        # Build blocks\n        self._blocks = nn.ModuleList([])\n        for block_args in self._blocks_args:\n\n            # Update block input and output filters based on depth multiplier.\n            block_args = block_args._replace(\n                input_filters=round_filters(block_args.input_filters, self._global_params),\n                output_filters=round_filters(block_args.output_filters, self._global_params),\n                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n            )\n\n            # The first block needs to take care of stride and filter size increase.\n            self._blocks.append(MBConvBlock(block_args, self._global_params))\n            if block_args.num_repeat > 1:\n                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n            for _ in range(block_args.num_repeat - 1):\n                self._blocks.append(MBConvBlock(block_args, self._global_params))\n\n        n_blocks = len(self._blocks)\n        assert self.block_chunks[-1] == n_blocks, f'{self.block_chunks[-1]}, {n_blocks}'\n        self._out_shapes = [self._bn0.num_features]\n        self._out_shapes += [self._blocks[i - 1]._bn2.num_features for i in self.block_chunks[1:]]\n        self._out_shapes = list(reversed(self._out_shapes))\n        # precalc drop connect rates\n        self.drop_connect_rates = np.arange(0, n_blocks, dtype=np.float) / n_blocks\n        self.drop_connect_rates *= self._global_params.drop_connect_rate\n\n    def forward_blocks(self, x, start_idx, end_idx):\n        for idx in range(start_idx, end_idx):\n            x = self._blocks[idx](x, self.drop_connect_rates[idx])\n        return x\n\n    def forward(self, x):\n        x0 = relu_fn(self._bn0(self._conv_stem(x)))\n        x1 = self.forward_blocks(x0, self.block_chunks[0], self.block_chunks[1])\n        x2 = self.forward_blocks(x1, self.block_chunks[1], self.block_chunks[2])\n        x3 = self.forward_blocks(x2, self.block_chunks[2], self.block_chunks[3])\n        x4 = self.forward_blocks(x3, self.block_chunks[3], self.block_chunks[4])\n        return [x4, x3, x2, x1, x0]\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('_conv_head.weight')\n        state_dict.pop('_bn1.weight')\n        state_dict.pop('_bn1.bias')\n        state_dict.pop('_bn1.running_mean')\n        state_dict.pop('_bn1.running_var')\n        state_dict.pop('_bn1.num_batches_tracked')\n        state_dict.pop('_fc.bias')\n        state_dict.pop('_fc.weight')\n\n        if self.in_channels != 3:\n            state_dict = self.modify_in_channel_weights(state_dict, self.in_channels)\n\n        super().load_state_dict(state_dict, **kwargs)\n\n    def modify_in_channel_weights(self, state_dict, in_channels):\n        pretrained = state_dict['_conv_stem.weight']\n        cycled_weights = cycle_rgb_weights(pretrained, in_channels)\n        state_dict['_conv_stem.weight'] = cycled_weights\n        return state_dict\n\n    def get_block_args(self):\n        blocks_args = [\n            'r1_k3_s11_e1_i32_o16_se0.25',\n            'r2_k3_s22_e6_i16_o24_se0.25',\n            'r2_k5_s22_e6_i24_o40_se0.25',\n            'r3_k3_s22_e6_i40_o80_se0.25',\n            'r3_k5_s11_e6_i80_o112_se0.25',\n            'r4_k5_s22_e6_i112_o192_se0.25',\n            'r1_k3_s11_e6_i192_o320_se0.25',\n        ]\n        blocks_args = BlockDecoder.decode(blocks_args)\n        return blocks_args\n\n    def info(self):\n        msg = '== EfficientNetEncoder ==\\n\\n'\n        msg += 'x0: conv_stem\\n'\n        chunks = zip(self.block_chunks[:-1], self.block_chunks[1:])\n        for i, (start, end) in enumerate(chunks):\n            msg += f'x{i+1}: blocks[{start}:{end}]\\n'\n        msg += '\\n'\n        msg += f'Out shapes (x4, x3, x2, x1, x0): {self._out_shapes}\\n'\n        msg += str(self._global_params)\n        print(msg)\n\n\nurl_map = {\n    'efficientnet-b0': '../input/efficientnet-pytorch/efficientnet-b0-355c32eb.pth',\n    'efficientnet-b1': '../input/efficientnet-pytorch/efficientnet-b1-f1951068.pth',\n    'efficientnet-b2': '../input/efficientnet-pytorch/efficientnet-b2-8bb594d6.pth',\n    'efficientnet-b3': '../input/efficientnet-pytorch/efficientnet-b3-5fb5a3c3.pth',\n    'efficientnet-b4': '../input/efficientnet-pytorch/efficientnet-b4-6ed6700e.pth',\n    'efficientnet-b5': '../input/efficientnet-pytorch/efficientnet-b5-b6417697.pth',\n    'efficientnet-b6': '../input/efficientnet-pytorch/efficientnet-b6-c76e70fd.pth',\n    'efficientnet-b7': '../input/efficientnet-pytorch/efficientnet-b7-dcc49843.pth',\n}\n\nefficientnet_encoders = {\n    'efficientnet-b0': {\n        'encoder': EfficientNetEncoder,\n        'pretrained_settings': {\n            'imagenet': {\n                'url': url_map['efficientnet-b0'],\n                'input_space': 'RGB',\n                'input_size': [3, 224, 224],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (320, 112, 40, 24, 32),\n        'params': {\n            'width_coeff': 1.0,\n            'depth_coeff': 1.0,\n            'image_size': 224,\n            'dropout_rate': 0.2,\n            'drop_connect_rate': 0.2,\n            'block_chunks': [0, 2, 5, 11, 16]\n        },\n    },\n\n    'efficientnet-b1': {\n        'encoder': EfficientNetEncoder,\n        'pretrained_settings': {\n            'imagenet': {\n                'url': url_map['efficientnet-b1'],\n                'input_space': 'RGB',\n                'input_size': [3, 240, 240],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (320, 112, 40, 24, 32),\n        'params': {\n            'width_coeff': 1.0,\n            'depth_coeff': 1.1,\n            'image_size': 240,\n            'dropout_rate': 0.2,\n            'drop_connect_rate': 0.2,\n            'block_chunks': [0, 3, 8, 16, 23]\n        },\n    },\n\n    'efficientnet-b2': {\n        'encoder': EfficientNetEncoder,\n        'pretrained_settings': {\n            'imagenet': {\n                'url': url_map['efficientnet-b2'],\n                'input_space': 'RGB',\n                'input_size': [3, 260, 260],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (352, 120, 48, 24, 32),\n        'params': {\n            'width_coeff': 1.1,\n            'depth_coeff': 1.2,\n            'image_size': 260,\n            'dropout_rate': 0.3,\n            'drop_connect_rate': 0.2,\n            'block_chunks': [0, 3, 8, 16, 23]\n        },\n    },\n\n    'efficientnet-b3': {\n        'encoder': EfficientNetEncoder,\n        'pretrained_settings': {\n            'imagenet': {\n                'url': url_map['efficientnet-b3'],\n                'input_space': 'RGB',\n                'input_size': [3, 300, 300],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (384, 136, 48, 32, 40),\n        'params': {\n            'width_coeff': 1.2,\n            'depth_coeff': 1.4,\n            'image_size': 300,\n            'dropout_rate': 0.3,\n            'drop_connect_rate': 0.2,\n            'block_chunks': [0, 3, 8, 18, 26]\n        },\n    },\n\n    'efficientnet-b4': {\n        'encoder': EfficientNetEncoder,\n        'pretrained_settings': {\n            'imagenet': {\n                'url': url_map['efficientnet-b4'],\n                'input_space': 'RGB',\n                'input_size': [3, 380, 380],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (448, 160, 56, 32, 48),\n        'params': {\n            'width_coeff': 1.4,\n            'depth_coeff': 1.8,\n            'image_size': 380,\n            'dropout_rate': 0.4,\n            'drop_connect_rate': 0.2,\n            'block_chunks': [0, 5, 10, 22, 32]\n        },\n    },\n\n    'efficientnet-b5': {\n        'encoder': EfficientNetEncoder,\n        'pretrained_settings': {\n            'imagenet': {\n                'url': url_map['efficientnet-b5'],\n                'input_space': 'RGB',\n                'input_size': [3, 456, 456],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (512, 176, 64, 40, 48),\n        'params': {\n            'width_coeff': 1.6,\n            'depth_coeff': 2.2,\n            'image_size': 456,\n            'dropout_rate': 0.4,\n            'drop_connect_rate': 0.2,\n            'block_chunks': [0, 6, 13, 27, 39]\n        },\n    },\n\n    'efficientnet-b6': {\n        'encoder': EfficientNetEncoder,\n        'pretrained_settings': {\n            'imagenet': {\n                'url': url_map['efficientnet-b6'],\n                'input_space': 'RGB',\n                'input_size': [3, 528, 528],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (576, 200, 72, 40, 56),\n        'params': {\n            'width_coeff': 1.8,\n            'depth_coeff': 2.6,\n            'image_size': 528,\n            'dropout_rate': 0.5,\n            'drop_connect_rate': 0.2,\n            'block_chunks': [0, 7, 15, 31, 45]\n        },\n    },\n\n    'efficientnet-b7': {\n        'encoder': EfficientNetEncoder,\n        'pretrained_settings': {\n            'imagenet': {\n                'url': url_map['efficientnet-b7'],\n                'input_space': 'RGB',\n                'input_size': [3, 600, 600],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (640, 224, 80, 48, 64),\n        'params': {\n            'width_coeff': 2.0,\n            'depth_coeff': 3.1,\n            'image_size': 600,\n            'dropout_rate': 0.5,\n            'drop_connect_rate': 0.2,\n            'block_chunks': [0, 9, 18, 38, 55]\n        },\n    },\n}\n\n\nclass MBConvBlock(nn.Module):\n    \"\"\"\n    Mobile Inverted Residual Bottleneck Block\n\n    Args:\n        block_args (namedtuple): BlockArgs, see above\n        global_params (namedtuple): GlobalParam, see above\n\n    Attributes:\n        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n    \"\"\"\n\n    def __init__(self, block_args, global_params):\n        super().__init__()\n        self._block_args = block_args\n        self._bn_mom = 1 - global_params.batch_norm_momentum\n        self._bn_eps = global_params.batch_norm_epsilon\n        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n        self.id_skip = block_args.id_skip  # skip connection and drop connect\n\n        # Get static or dynamic convolution depending on image size\n        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n\n        # Expansion phase\n        inp = self._block_args.input_filters  # number of input channels\n        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n        if self._block_args.expand_ratio != 1:\n            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        # Depthwise convolution phase\n        k = self._block_args.kernel_size\n        s = self._block_args.stride\n        self._depthwise_conv = Conv2d(\n            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n            kernel_size=k, stride=s, bias=False)\n        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        # Squeeze and Excitation layer, if desired\n        if self.has_se:\n            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n\n        # Output phase\n        final_oup = self._block_args.output_filters\n        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n    def forward(self, inputs, drop_connect_rate=None):\n        \"\"\"\n        :param inputs: input tensor\n        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n        :return: output of block\n        \"\"\"\n\n        # Expansion and Depthwise Convolution\n        x = inputs\n        if self._block_args.expand_ratio != 1:\n            x = relu_fn(self._bn0(self._expand_conv(inputs)))\n        x = relu_fn(self._bn1(self._depthwise_conv(x)))\n\n        # Squeeze and Excitation\n        if self.has_se:\n            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n            x_squeezed = self._se_expand(relu_fn(self._se_reduce(x_squeezed)))\n            x = torch.sigmoid(x_squeezed) * x\n\n        x = self._bn2(self._project_conv(x))\n\n        # Skip connection and drop connect\n        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n            if drop_connect_rate:\n                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n            x = x + inputs  # skip connection\n        return x\n\n\n########################################################################\n############### HELPERS FUNCTIONS FOR MODEL ARCHITECTURE ###############\n########################################################################\n\n\n# Parameters for the entire model (stem, all blocks, and head)\nGlobalParams = collections.namedtuple('GlobalParams', [\n    'batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate',\n    'num_classes', 'width_coefficient', 'depth_coefficient',\n    'depth_divisor', 'min_depth', 'drop_connect_rate', 'image_size'])\n\n\n# Parameters for an individual model block\nBlockArgs = collections.namedtuple('BlockArgs', [\n    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n    'expand_ratio', 'id_skip', 'stride', 'se_ratio'])\n\n\n# Change namedtuple defaults\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n\n\ndef relu_fn(x):\n    \"\"\" Swish activation function \"\"\"\n    return x * torch.sigmoid(x)\n\n\ndef round_filters(filters, global_params):\n    \"\"\" Calculate and round number of filters based on depth multiplier. \"\"\"\n    multiplier = global_params.width_coefficient\n    if not multiplier:\n        return filters\n    divisor = global_params.depth_divisor\n    min_depth = global_params.min_depth\n    filters *= multiplier\n    min_depth = min_depth or divisor\n    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n        new_filters += divisor\n    return int(new_filters)\n\n\ndef round_repeats(repeats, global_params):\n    \"\"\" Round number of filters based on depth multiplier. \"\"\"\n    multiplier = global_params.depth_coefficient\n    if not multiplier:\n        return repeats\n    return int(math.ceil(multiplier * repeats))\n\n\ndef drop_connect(inputs, p, training):\n    \"\"\" Drop connect. \"\"\"\n    if not training: return inputs\n    batch_size = inputs.shape[0]\n    keep_prob = 1 - p\n    random_tensor = keep_prob\n    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n    binary_tensor = torch.floor(random_tensor)\n    output = inputs / keep_prob * binary_tensor\n    return output\n\n\ndef get_same_padding_conv2d(image_size=None):\n    \"\"\" Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n        Static padding is necessary for ONNX exporting of models. \"\"\"\n    if image_size is None:\n        return Conv2dDynamicSamePadding\n    else:\n        return partial(Conv2dStaticSamePadding, image_size=image_size)\n\nclass Conv2dDynamicSamePadding(nn.Conv2d):\n    \"\"\" 2D Convolutions like TensorFlow, for a dynamic image size \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]]*2\n\n    def forward(self, x):\n        ih, iw = x.size()[-2:]\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h > 0 or pad_w > 0:\n            x = F.pad(x, [pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2])\n        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass Conv2dStaticSamePadding(nn.Conv2d):\n    \"\"\" 2D Convolutions like TensorFlow, for a fixed image size\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, image_size=None, **kwargs):\n        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n\n        # Calculate padding based on image size and save it\n        assert image_size is not None\n        ih, iw = image_size if type(image_size) == list else [image_size, image_size]\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h > 0 or pad_w > 0:\n            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n        else:\n            self.static_padding = Identity()\n\n    def forward(self, x):\n        x = self.static_padding(x)\n        x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n        return x\n\n\nclass Identity(nn.Module):\n    def __init__(self,):\n        super(Identity, self).__init__()\n\n    def forward(self, input):\n        return input\n\n\n########################################################################\n############## HELPERS FUNCTIONS FOR LOADING MODEL PARAMS ##############\n########################################################################\n\n\nclass BlockDecoder(object):\n    \"\"\" Block Decoder for readability, straight from the official TensorFlow repository \"\"\"\n\n    @staticmethod\n    def _decode_block_string(block_string):\n        \"\"\" Gets a block through a string notation of arguments. \"\"\"\n        assert isinstance(block_string, str)\n\n        ops = block_string.split('_')\n        options = {}\n        for op in ops:\n            splits = re.split(r'(\\d.*)', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n        # Check stride\n        assert (('s' in options and len(options['s']) == 1) or\n                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n\n        return BlockArgs(\n            kernel_size=int(options['k']),\n            num_repeat=int(options['r']),\n            input_filters=int(options['i']),\n            output_filters=int(options['o']),\n            expand_ratio=int(options['e']),\n            id_skip=('noskip' not in block_string),\n            se_ratio=float(options['se']) if 'se' in options else None,\n            stride=[int(options['s'][0])])\n\n    @staticmethod\n    def _encode_block_string(block):\n        \"\"\"Encodes a block to a string.\"\"\"\n        args = [\n            'r%d' % block.num_repeat,\n            'k%d' % block.kernel_size,\n            's%d%d' % (block.strides[0], block.strides[1]),\n            'e%s' % block.expand_ratio,\n            'i%d' % block.input_filters,\n            'o%d' % block.output_filters\n        ]\n        if 0 < block.se_ratio <= 1:\n            args.append('se%s' % block.se_ratio)\n        if block.id_skip is False:\n            args.append('noskip')\n        return '_'.join(args)\n\n    @staticmethod\n    def decode(string_list):\n        \"\"\"\n        Decodes a list of string notations to specify blocks inside the network.\n\n        :param string_list: a list of strings, each string is a notation of block\n        :return: a list of BlockArgs namedtuples of block args\n        \"\"\"\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n        return blocks_args\n\n    @staticmethod\n    def encode(blocks_args):\n        \"\"\"\n        Encodes a list of BlockArgs to a list of strings.\n\n        :param blocks_args: a list of BlockArgs namedtuples of block args\n        :return: a list of strings, each string is a notation of block\n        \"\"\"\n        block_strings = []\n        for block in blocks_args:\n            block_strings.append(BlockDecoder._encode_block_string(block))\n        return block_strings\n\n\ndef get_global_params(width_coeff, depth_coeff, image_size, dropout_rate, drop_connect_rate):\n    \"\"\" Map EfficientNet model name to parameter coefficients. \"\"\"\n\n    global_params = GlobalParams(\n        batch_norm_momentum=0.99,\n        batch_norm_epsilon=1e-3,\n        dropout_rate=dropout_rate,\n        drop_connect_rate=drop_connect_rate,\n        num_classes=1000,\n        width_coefficient=width_coeff,\n        depth_coefficient=depth_coeff,\n        depth_divisor=8,\n        min_depth=None,\n        image_size=image_size,\n    )\n    return global_params\n    \n# -- blocks.py --\n\nclass swish(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = torch.sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\n\nclass Swish(torch.nn.Module):\n    \"\"\"\n    https://github.com/qubvel/segmentation_models.pytorch/pull/85/files\n    \"\"\"\n    def forward(self, x):\n        return swish.apply(x)\n\n\nclass Conv2dReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0,\n                 stride=1, use_batchnorm=True, **batchnorm_params):\n\n        super().__init__()\n\n        layers = [\n            nn.Conv2d(in_channels, out_channels, kernel_size,\n                              stride=stride, padding=padding, bias=not (use_batchnorm)),\n            nn.ReLU(inplace=True),\n        ]\n\n        if use_batchnorm:\n            layers.insert(1, nn.BatchNorm2d(out_channels, **batchnorm_params))\n\n        self.block = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass SCSEModule(nn.Module):\n    def __init__(self, ch, re=16):\n        super().__init__()\n        self.cSE = nn.Sequential(nn.AdaptiveAvgPool2d(1),\n                                 nn.Conv2d(ch, ch//re, 1),\n                                 nn.ReLU(inplace=True),\n                                 nn.Conv2d(ch//re, ch, 1),\n                                 nn.Sigmoid()\n                                )\n        self.sSE = nn.Sequential(nn.Conv2d(ch, ch, 1),\n                                 nn.Sigmoid())\n\n    def forward(self, x):\n        return x * self.cSE(x) + x * self.sSE(x)\n\n\n# -- senet.py \n\nclass SENetEncoder(SENet):\n\n    def __init__(self, in_channels=3, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pretrained = False\n        self.in_channels = in_channels\n\n        del self.last_linear\n        del self.avg_pool\n        self.layer0[0] = nn.Conv2d(in_channels, 64, (7, 7), (2, 2), (3, 3), bias=False)\n\n    def forward(self, x):\n        for module in self.layer0[:-1]:\n            x = module(x)\n\n        x0 = x\n        x = self.layer0[-1](x)\n        x1 = self.layer1(x)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        features = [x4, x3, x2, x1, x0]\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('last_linear.bias')\n        state_dict.pop('last_linear.weight')\n\n        if self.in_channels != 3:\n            state_dict = self.modify_in_channel_weights(state_dict, self.in_channels)\n\n        super().load_state_dict(state_dict, **kwargs)\n\n    def modify_in_channel_weights(self, state_dict, in_channels):\n        self.layer0[0] = nn.Conv2d(in_channels, 64, (7, 7), (2, 2), (3, 3), bias=False)\n        pretrained = state_dict['layer0.conv1.weight']\n        cycled_weights = cycle_rgb_weights(pretrained, in_channels)\n        state_dict['layer0.conv1.weight'] = cycled_weights\n        return state_dict\n    \nsenet_encoders = {\n    'senet154': {\n        'encoder': SENetEncoder,\n        'pretrained_settings': pretrained_settings['senet154'],\n        'out_shapes': (2048, 1024, 512, 256, 128),\n        'params': {\n            'block': SEBottleneck,\n            'dropout_p': 0.2,\n            'groups': 64,\n            'layers': [3, 8, 36, 3],\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet50': {\n        'encoder': SENetEncoder,\n        'pretrained_settings': pretrained_settings['se_resnet50'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 4, 6, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet101': {\n        'encoder': SENetEncoder,\n        'pretrained_settings': pretrained_settings['se_resnet101'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 4, 23, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet152': {\n        'encoder': SENetEncoder,\n        'pretrained_settings': pretrained_settings['se_resnet152'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 8, 36, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnext50_32x4d': {\n        'encoder': SENetEncoder,\n        'pretrained_settings': pretrained_settings['se_resnext50_32x4d'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNeXtBottleneck,\n            'layers': [3, 4, 6, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 32,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnext101_32x4d': {\n        'encoder': SENetEncoder,\n        'pretrained_settings': pretrained_settings['se_resnext101_32x4d'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNeXtBottleneck,\n            'layers': [3, 4, 23, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 32,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n}    \n    \n# -- __init__.py\n\nencoders = {}\nencoders.update(efficientnet_encoders)\nencoders.update(senet_encoders)\nencoders.update(inceptionv4_encoders)\nencoders.update(inception_encoders)\n\ndef get_encoder(name, encoder_weights=None, in_channels=3):\n    Encoder = encoders[name]['encoder']\n    encoder = Encoder(in_channels=in_channels, **encoders[name]['params'])\n    encoder.out_shapes = encoders[name]['out_shapes']\n\n    if encoder_weights is not None:\n        settings = encoders[name]['pretrained_settings'][encoder_weights]\n        r = encoder.load_state_dict(torch.load(settings['url']))\n        print(f'Load result: {r}')\n    return encoder\n\n\ndef get_encoder_names():\n    return list(encoders.keys())\n\n\ndef get_preprocessing_params(encoder_name, pretrained='imagenet'):\n    settings = encoders[encoder_name]['pretrained_settings']\n\n    if pretrained not in settings.keys():\n        raise ValueError('Avaliable pretrained options {}'.format(settings.keys()))\n\n    formatted_settings = {}\n    formatted_settings['input_space'] = settings[pretrained].get('input_space')\n    formatted_settings['input_range'] = settings[pretrained].get('input_range')\n    formatted_settings['mean'] = settings[pretrained].get('mean')\n    formatted_settings['std'] = settings[pretrained].get('std')\n    return formatted_settings\n\n\ndef get_preprocessing_fn(encoder_name, pretrained='imagenet'):\n    params = get_preprocessing_params(encoder_name, pretrained=pretrained)\n    return functools.partial(preprocess_input, **params)\n\n\n# -- decoder.py\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_batchnorm=True, attention_type=None):\n        super().__init__()\n        if attention_type is None:\n            self.attention1 = nn.Identity()\n            self.attention2 = nn.Identity()\n        elif attention_type == 'scse':\n            self.attention1 = SCSEModule(in_channels)\n            self.attention2 = SCSEModule(out_channels)\n\n        self.block = nn.Sequential(\n            Conv2dReLU(in_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm),\n            Conv2dReLU(out_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm),\n        )\n\n    def forward(self, x):\n        x, skip = x\n\n        if skip is not None:\n            skipsize = (skip.shape[-2], skip.shape[-1])\n            x = F.interpolate(x, size=skipsize, mode='nearest')\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n        else:\n            x = F.interpolate(x, scale_factor=2, mode='nearest')\n\n        x = self.block(x)\n        x = self.attention2(x)\n        return x\n\n\nclass CenterBlock(DecoderBlock):\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass UnetDecoder(Model):\n\n    def __init__(\n            self,\n            encoder_channels,\n            decoder_channels=(256, 128, 64, 32, 16),\n            final_channels=1,\n            use_batchnorm=True,\n            center=False,\n            attention_type=None\n    ):\n        super().__init__()\n\n        if center:\n            channels = encoder_channels[0]\n            self.center = CenterBlock(channels, channels, use_batchnorm=use_batchnorm)\n        else:\n            self.center = None\n\n        in_channels = self.compute_channels(encoder_channels, decoder_channels)\n        out_channels = decoder_channels\n\n        self.layer1 = DecoderBlock(in_channels[0], out_channels[0],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.layer2 = DecoderBlock(in_channels[1], out_channels[1],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.layer3 = DecoderBlock(in_channels[2], out_channels[2],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.layer4 = DecoderBlock(in_channels[3], out_channels[3],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.layer5 = DecoderBlock(in_channels[4], out_channels[4],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.final_conv = nn.Conv2d(out_channels[4], final_channels, kernel_size=(1, 1))\n\n        self.initialize()\n\n    def compute_channels(self, encoder_channels, decoder_channels):\n        channels = [\n            encoder_channels[0] + encoder_channels[1],\n            encoder_channels[2] + decoder_channels[0],\n            encoder_channels[3] + decoder_channels[1],\n            encoder_channels[4] + decoder_channels[2],\n            0 + decoder_channels[3],\n        ]\n        return channels\n\n    def forward(self, x):\n        encoder_head = x[0]\n        skips = x[1:]\n\n        if self.center:\n            encoder_head = self.center(encoder_head)\n\n        x = self.layer1([encoder_head, skips[0]])\n        x = self.layer2([x, skips[1]])\n        x = self.layer3([x, skips[2]])\n        x = self.layer4([x, skips[3]])\n        x = self.layer5([x, None])\n        x = self.final_conv(x)\n\n        return x\n\nclass Conv2dWS(nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        super(Conv2dWS, self).__init__(in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, groups, bias)\n\n    def forward(self, x):\n        weight = self.weight\n        weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2,\n                                  keepdim=True).mean(dim=3, keepdim=True)\n        weight = weight - weight_mean\n        std = weight.view(weight.size(0), -1).std(dim=1).view(-1, 1, 1, 1) + 1e-5\n        weight = weight / std.expand_as(weight)\n        return F.conv2d(x, weight, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n\nclass Conv3x3GNReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, weight_std=False, upsample=False, activation='relu'):\n\n        if weight_std:\n            Conv2d = Conv2dWS\n        else:\n            Conv2d = nn.Conv2d\n\n        if activation == 'relu':\n            relu_fn = nn.ReLU(inplace=True)\n        elif activation == 'swish':\n            relu_fn = Swish()\n        else:\n            raise ValueError(f'`activation` must be \"relu\" or \"swish\"')\n\n        super().__init__()\n        self.upsample = upsample\n        self.block = nn.Sequential(\n            Conv2d(in_channels, out_channels, (3, 3),\n                              stride=1, padding=1, bias=False),\n            nn.GroupNorm(32, out_channels),\n            relu_fn,\n        )\n\n    def forward(self, x):\n        x = self.block(x)\n        if self.upsample:\n            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n        return x\n\n\n\nclass FPNBlock(nn.Module):\n    def __init__(self, pyramid_channels, skip_channels):\n        super().__init__()\n        self.skip_conv = nn.Conv2d(skip_channels, pyramid_channels, kernel_size=1)\n\n    def forward(self, x):\n        x, skip = x\n\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n        skip = self.skip_conv(skip)\n\n        x = x + skip\n        return x\n\n\nclass SegmentationBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, weight_std, n_upsamples=0, activation='relu'):\n        super().__init__()\n\n        blocks = [\n            Conv3x3GNReLU(in_channels, out_channels, upsample=bool(n_upsamples), activation=activation)\n        ]\n\n        if n_upsamples > 1:\n            for _ in range(1, n_upsamples):\n                blocks.append(Conv3x3GNReLU(\n                    out_channels, out_channels, weight_std, upsample=True, activation=activation))\n\n        self.block = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass FPNDecoder(Model):\n\n    def __init__(\n            self,\n            encoder_channels,\n            pyramid_channels=256,\n            segmentation_channels=128,\n            final_upsampling=4,\n            final_channels=1,\n            dropout=0.2,\n            weight_std=False,\n            merge_policy='add',\n            activation='relu'\n    ):\n        super().__init__()\n\n        if merge_policy not in ['add', 'cat']:\n            raise ValueError(\"`merge_policy` must be one of: ['add', 'cat'], got {}\".format(merge_policy))\n        self.merge_policy = merge_policy\n\n        self.final_upsampling = final_upsampling\n        self.conv1 = nn.Conv2d(encoder_channels[0], pyramid_channels, kernel_size=(1, 1))\n\n        self.p4 = FPNBlock(pyramid_channels, encoder_channels[1])\n        self.p3 = FPNBlock(pyramid_channels, encoder_channels[2])\n        self.p2 = FPNBlock(pyramid_channels, encoder_channels[3])\n\n        self.s5 = SegmentationBlock(pyramid_channels, segmentation_channels, weight_std, 3, activation)\n        self.s4 = SegmentationBlock(pyramid_channels, segmentation_channels, weight_std, 2, activation)\n        self.s3 = SegmentationBlock(pyramid_channels, segmentation_channels, weight_std, 1, activation)\n        self.s2 = SegmentationBlock(pyramid_channels, segmentation_channels, weight_std, 0, activation)\n\n        self.dropout = nn.Dropout2d(p=dropout, inplace=True)\n\n        if self.merge_policy == 'cat':\n            segmentation_channels *= 4\n\n        self.final_conv = nn.Conv2d(segmentation_channels, final_channels, kernel_size=1, padding=0)\n\n        self.initialize()\n\n    def forward(self, x):\n        c5, c4, c3, c2, _ = x\n\n        p5 = self.conv1(c5)\n        p4 = self.p4([p5, c4])\n        p3 = self.p3([p4, c3])\n        p2 = self.p2([p3, c2])\n\n        s5 = self.s5(p5)\n        s4 = self.s4(p4)\n        s3 = self.s3(p3)\n        s2 = self.s2(p2)\n\n        if self.merge_policy == 'add':\n            x = s5 + s4 + s3 + s2\n        elif self.merge_policy == 'cat':\n            x = torch.cat([s5, s4, s3, s2], dim=1)\n\n        x = self.dropout(x)\n        x = self.final_conv(x)\n\n        if self.final_upsampling is not None and self.final_upsampling > 1:\n            x = F.interpolate(x, scale_factor=self.final_upsampling, mode='bilinear', align_corners=True)\n        return x\n    \n# -- Model\n    \nclass Unet(EncoderDecoder):\n    \"\"\"Unet_ is a fully convolution neural network for image semantic segmentation\n\n    Args:\n        encoder_name: name of classification model (without last dense layers) used as feature\n            extractor to build segmentation model.\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        decoder_channels: list of numbers of ``Conv2D`` layer filters in decoder blocks\n        decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n            is used.\n        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n        activation: activation function used in ``.predict(x)`` method for inference.\n            One of [``sigmoid``, ``softmax``, callable, None]\n        center: if ``True`` add ``Conv2dReLU`` block on encoder head (useful for VGG models)\n        attention_type: attention module used in decoder of the model\n            One of [``None``, ``scse``]\n\n    Returns:\n        ``torch.nn.Module``: **Unet**\n\n    .. _Unet:\n        https://arxiv.org/pdf/1505.04597\n\n    \"\"\"\n\n    def __init__(\n            self,\n            encoder_name='resnet34',\n            encoder_weights='imagenet',\n            decoder_use_batchnorm=True,\n            decoder_channels=(256, 128, 64, 32, 16),\n            classes=1,\n            activation='sigmoid',\n            center=False,  # usefull for VGG models\n            attention_type=None,\n            in_channels=3\n    ):\n        encoder = get_encoder(\n            encoder_name,\n            encoder_weights=encoder_weights,\n            in_channels=in_channels\n        )\n\n        decoder = UnetDecoder(\n            encoder_channels=encoder.out_shapes,\n            decoder_channels=decoder_channels,\n            final_channels=classes,\n            use_batchnorm=decoder_use_batchnorm,\n            center=center,\n            attention_type=attention_type\n        )\n\n        super().__init__(encoder, decoder, activation)\n\n        self.name = 'u-{}'.format(encoder_name)\n\n        \nclass FPN(EncoderDecoder):\n    \"\"\"FPN_ is a fully convolution neural network for image semantic segmentation\n    Args:\n        encoder_name: name of classification model (without last dense layers) used as feature\n                extractor to build segmentation model.\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        decoder_pyramid_channels: a number of convolution filters in Feature Pyramid of FPN_.\n        decoder_segmentation_channels: a number of convolution filters in segmentation head of FPN_.\n        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n        dropout: spatial dropout rate in range (0, 1).\n        activation: activation function used in ``.predict(x)`` method for inference.\n            One of [``sigmoid``, ``softmax``, callable, None]\n\n    Returns:\n        ``torch.nn.Module``: **FPN**\n\n    .. _FPN:\n        http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n\n    \"\"\"\n\n    def __init__(\n            self,\n            encoder_name='resnet34',\n            encoder_weights='imagenet',\n            decoder_pyramid_channels=256,\n            decoder_segmentation_channels=128,\n            decoder_merge_policy='add',\n            decoder_activation='relu',\n            classes=1,\n            dropout=0.2,\n            activation='sigmoid',\n            in_channels=3,\n            weight_std=False,\n            final_upsampling=4,\n    ):\n        encoder = get_encoder(\n            encoder_name,\n            encoder_weights=encoder_weights,\n            in_channels=in_channels\n        )\n\n        decoder = FPNDecoder(\n            encoder_channels=encoder.out_shapes,\n            pyramid_channels=decoder_pyramid_channels,\n            segmentation_channels=decoder_segmentation_channels,\n            final_channels=classes,\n            dropout=dropout,\n            weight_std=weight_std,\n            final_upsampling=4,\n            merge_policy=decoder_merge_policy,\n            activation=decoder_activation\n        )\n\n        super().__init__(encoder, decoder, activation)\n\n        self.name = 'fpn-{}'.format(encoder_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ------------------------------------------------------------\n# Classes\n# ------------------------------------------------------------\n\n# -- /home/khornlund/bb/kaggle/aptos/aptos/base/base_trainer.py -- START --\n\nclass BaseTrainer:\n    \"\"\"\n    Base class for all trainers\n    \"\"\"\n    def __init__(self, model, loss, metrics, optimizer, resume, config, device):\n        self.logger = setup_logger(self, verbose=config['training']['verbose'])\n        self.model = model\n        self.device = device\n        self.loss = loss\n        self.metrics = metrics\n        self.optimizer = optimizer\n        self.config = config\n\n        cfg_trainer = config['training']\n        self.epochs = cfg_trainer['epochs']\n        self.save_period = cfg_trainer['save_period']\n        self.monitor = cfg_trainer.get('monitor', 'off')\n\n        # configuration to monitor model performance and save best\n        if self.monitor == 'off':\n            self.mnt_mode = 'off'\n            self.mnt_best = 0\n        else:\n            self.mnt_mode, self.mnt_metric = self.monitor.split()\n            assert self.mnt_mode in ['min', 'max']\n\n            self.mnt_best = math.inf if self.mnt_mode == 'min' else -math.inf\n            self.early_stop = cfg_trainer.get('early_stop', math.inf)\n\n        self.start_epoch = 1\n\n        # setup directory for checkpoint saving\n        self.checkpoint_dir, writer_dir = trainer_paths(config)\n        # setup visualization writer instance\n        self.writer = TensorboardWriter(writer_dir, self.logger, cfg_trainer['tensorboard'])\n\n        # Save configuration file into checkpoint directory:\n        config_save_path = os.path.join(self.checkpoint_dir, 'config.yaml')\n        with open(config_save_path, 'w') as handle:\n            yaml.dump(config, handle, default_flow_style=False)\n\n        if resume:\n            self._resume_checkpoint(resume)\n\n    def train(self):\n        \"\"\"\n        Full training logic\n        \"\"\"\n        self.logger.info('Starting training...')\n        for epoch in range(self.start_epoch, self.epochs + 1):\n            result = self._train_epoch(epoch)\n\n            # save logged informations into log dict\n            log = {'epoch': epoch}\n            for key, value in result.items():\n                if key == 'metrics':\n                    log.update({\n                        mtr.__name__: value[i] for i, mtr in enumerate(self.metrics)})\n                elif key == 'val_metrics':\n                    log.update({\n                        'val_' + mtr.__name__: value[i] for i, mtr in enumerate(self.metrics)})\n                else:\n                    log[key] = value\n\n            # print logged informations to the screen\n            for key, value in log.items():\n                self.logger.info(f'{str(key):15s}: {value}')\n\n            # evaluate model performance according to configured metric,\n            # save best checkpoint as model_best\n            best = False\n            if self.mnt_mode != 'off':\n                try:\n                    # check whether model performance improved or not, according\n                    # to specified metric(mnt_metric)\n                    improved = (self.mnt_mode == 'min' and log[self.mnt_metric] < self.mnt_best) or\\\n                               (self.mnt_mode == 'max' and log[self.mnt_metric] > self.mnt_best)\n                except KeyError:\n                    self.logger.warning(f\"Warning: Metric '{self.mnt_metric}' is not found. Model \"\n                                        \"performance monitoring is disabled.\")\n                    self.mnt_mode = 'off'\n                    improved = False\n                    not_improved_count = 0\n\n                if improved:\n                    self.mnt_best = log[self.mnt_metric]\n                    not_improved_count = 0\n                    best = True\n                else:\n                    not_improved_count += 1\n\n                if not_improved_count > self.early_stop:\n                    self.logger.info(\"Validation performance didn\\'t improve for \"\n                                     f\"{self.early_stop} epochs. Training stops.\")\n                    break\n\n            if epoch % self.save_period == 0:\n                self._save_checkpoint(epoch, save_best=best)\n\n    def _train_epoch(self, epoch):\n        \"\"\"\n        Training logic for an epoch\n\n        :param epoch: Current epoch number\n        \"\"\"\n        raise NotImplementedError\n\n    def _save_checkpoint(self, epoch, save_best=False):\n        \"\"\"\n        Saving checkpoints\n\n        :param epoch: current epoch number\n        :param log: logging information of the epoch\n        :param save_best: if True, rename the saved checkpoint to 'model_best.pth'\n        \"\"\"\n        arch = type(self.model).__name__\n        state = {\n            'arch': arch,\n            'epoch': epoch,\n            'state_dict': self.model.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n            'monitor_best': self.mnt_best,\n            'config': self.config,\n        }\n        filename = os.path.join(self.checkpoint_dir, f'checkpoint-epoch{epoch}.pth')\n        torch.save(state, filename)\n        self.logger.info(f\"Saving checkpoint: {filename} ...\")\n        if save_best:\n            best_path = os.path.join(self.checkpoint_dir, 'model_best.pth')\n            torch.save(state, best_path)\n            self.logger.info(f'Saving current best: {best_path}')\n\n    def _resume_checkpoint(self, resume_path):\n        \"\"\"\n        Resume from saved checkpoints\n\n        :param resume_path: Checkpoint path to be resumed\n        \"\"\"\n        self.logger.info(f'Loading checkpoint: {resume_path}')\n        checkpoint = torch.load(resume_path)\n        self.start_epoch = checkpoint['epoch'] + 1\n        self.mnt_best = checkpoint['monitor_best']\n\n        # load architecture params from checkpoint.\n        if checkpoint['config']['arch'] != self.config['arch']:\n            self.logger.warning(\"Warning: Architecture configuration given in config file is \"\n                                \"different from that of checkpoint. This may yield an \"\n                                \"exception while state_dict is being loaded.\")\n        self.model.load_state_dict(checkpoint['state_dict'])\n\n        # load optimizer state from checkpoint only when optimizer type is not changed.\n        if checkpoint['config']['optimizer']['type'] != self.config['optimizer']['type']:\n            self.logger.warning(\"Warning: Optimizer type given in config file is different from \"\n                                \"that of checkpoint. Optimizer parameters not being resumed.\")\n        else:\n            self.optimizer.load_state_dict(checkpoint['optimizer'])\n\n        self.logger.info(f'Checkpoint \"{resume_path}\" (epoch {self.start_epoch}) loaded')\n\n# -- /home/khornlund/bb/kaggle/aptos/aptos/base/base_trainer.py -- END --\n# -- /home/khornlund/bb/kaggle/aptos/aptos/data_loader/augmentation.py -- START --\n\n# MEAN = [0.3439]\n# STD = [0.0383]\n\nclass ChannelTranspose(ImageOnlyTransform):\n    def get_transform_init_args_names(self):\n        return ()\n\n    def get_params_dependent_on_targets(self, params):\n        pass\n\n    def __init__(self, axes=(2, 0, 1)):\n        super().__init__(always_apply=True)\n        self.axes = axes\n\n    def apply(self, img, **params):\n        return np.transpose(img, self.axes)\n\n    \nclass AugmentationBase(abc.ABC):\n\n    MEAN = [0.3439]\n    STD  = [0.0383]\n\n    H = 256\n    W = 1600\n\n    def __init__(self):\n        self.transform = self.notimplemented\n\n    def build_transforms(self, train):\n        if train:\n            self.transform = self.build_train()\n        else:\n            self.transform = self.build_test()\n\n    @abc.abstractmethod\n    def build_train(self):\n        pass\n\n    def build_test(self):\n        return Compose([\n            Normalize(mean=self.MEAN, std=self.STD),\n            ToTensor(),\n        ])\n\n    def notimplemented(self, *args, **kwargs):\n        raise Exception('You must call `build_transforms()` before using me!')\n\n    def __call__(self, *args, **kwargs):\n        return self.transform(*args, **kwargs)\n\n    def copy(self):\n        return deepcopy(self)\n\n\nclass LightTransforms(AugmentationBase):\n\n    def __init__(self):\n        super().__init__()\n\n    def build_train(self):\n        return Compose([\n            HorizontalFlip(p=0.5),\n            Normalize(mean=self.MEAN, std=self.STD),\n            ToTensor(),\n        ])\n    \n    \nclass CompatabilityTransforms(AugmentationBase):\n\n    def __init__(self):\n        super().__init__()\n\n    def build_train(self):\n        return Compose([\n            HorizontalFlip(p=0.5),\n            Normalize(mean=self.MEAN, std=self.STD),\n            ToTensor(),\n        ])\n    \n    def build_test(self):\n        return Compose([\n            Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ChannelTranspose()\n        ])\n\n\n# -- /home/khornlund/bb/kaggle/aptos/aptos/data_loader/augmentation.py -- END --\n# -- /home/khornlund/bb/kaggle/aptos/aptos/data_loader/datasets.py -- START --\n\nclass SteelDataset(Dataset):\n\n    img_folder = 'implement me!'\n    N_CLASSES = 4\n    rle_cols = [f'rle{i}' for i in range(N_CLASSES)]\n\n    def __init__(self, df, data_dir, transforms):\n        self.df = df\n        self.data_dir = data_dir / self.img_folder\n        self.transforms = transforms\n        self.fnames = self.df.index.tolist()\n\n    def read_greyscale(self, idx):\n        f = self.fnames[idx]\n        return f, cv2.imread(str(self.data_dir / f))[:, :, 0:1]  # select one channel\n\n    def rle(self, idx):\n        return self.df.iloc[idx][self.rle_cols]\n\n    def __len__(self):\n        return len(self.fnames)\n\n\nclass SteelDatasetTest(SteelDataset):\n\n    img_folder = 'test_images'\n\n    def __init__(self, df, data_dir, transforms):\n        super().__init__(df, data_dir, transforms)\n        self.transforms.build_transforms(train=False)\n\n    def __getitem__(self, idx):\n        f, image = self.read_greyscale(idx)\n        images = self.transforms(image=image)[\"image\"]\n        return f, images\n\n\n# -- /home/khornlund/bb/kaggle/aptos/aptos/data_loader/datasets.py -- END --\n# -- /home/khornlund/bb/kaggle/aptos/aptos/data_loader/data_loaders.py -- START --\n\nclass SteelDataLoader(DataLoader):\n\n    train_csv = 'train.csv'\n    test_csv  = 'sample_submission.csv'\n\n    def __init__(self, transforms, data_dir, batch_size, shuffle,\n                 validation_split, nworkers, pin_memory=True, train=True, alpha=None, balance=None\n    ):  # noqa\n        self.transforms, self.shuffle = transforms, shuffle\n        self.bs, self.nworkers, self.pin_memory = batch_size, nworkers, pin_memory\n        self.data_dir = Path(data_dir)\n\n        self.train_df, self.val_df = self.load_df(train, validation_split)\n\n        if train:\n            dataset = SteelDatasetTrainVal(self.train_df, self.data_dir, transforms.copy(), True)\n        else:\n            dataset = SteelDatasetTest(self.train_df, self.data_dir, transforms.copy())\n\n        if train and balance is not None and alpha is not None:\n            class_idxs = self.sort_classes(self.train_df)\n            n_batches = self.train_df.shape[0] // batch_size\n            sampler = SamplerFactory(2).get(class_idxs, batch_size, n_batches, alpha, balance)\n            super().__init__(dataset, batch_sampler=sampler,\n                            num_workers=nworkers, pin_memory=pin_memory)\n        else:\n            super().__init__(dataset, batch_size, shuffle=shuffle,\n                            num_workers=nworkers, pin_memory=pin_memory)\n\n    def load_df(self, train, validation_split):\n        csv_filename = self.train_csv if train else self.test_csv\n        df = pd.read_csv(self.data_dir / csv_filename)\n        df['ImageId'], df['ClassId'] = zip(*df['ImageId_ClassId'].str.split('_'))\n        df['ClassId'] = df['ClassId'].astype(int)\n        df = df.pivot(index='ImageId', columns='ClassId', values='EncodedPixels')\n        df.columns = [f'rle{c}' for c in range(4)]\n        df['defects'] = df.count(axis=1)\n\n        # add classification columns\n        for c in range(4):\n            df[f'c{c}'] = df[f'rle{c}'].apply(lambda rle: not pd.isnull(rle))\n\n        if train and validation_split > 0:\n            return train_test_split(df, test_size=validation_split, stratify=df[\"defects\"])\n\n        return df, pd.DataFrame({})\n\n    def sort_classes(self, df):\n        counts = {c: df[f'c{c}'].sum() for c in range(4)}\n        sorted_classes = sorted(counts.items(), key=lambda kv: kv[1])\n\n        def assign_min_sample_class(row, sorted_classes):\n            for c, _ in sorted_classes:\n                if row[f'c{c}']:\n                    return c\n            return -1\n\n        df['sample_class'] = df.apply(\n            lambda row: assign_min_sample_class(row, sorted_classes), axis=1)\n        class_idxs = [list(np.where(df['sample_class'] == c)[0]) for c in range(-1, 4)]\n        return class_idxs\n\n    def split_validation(self):\n        if self.val_df.empty:\n            return None\n        else:\n            dataset = SteelDatasetTrainVal(\n                self.val_df, self.data_dir, self.transforms.copy(), False)\n            return DataLoader(dataset, self.bs // 2,\n                              num_workers=self.nworkers, pin_memory=self.pin_memory)\n\n\n# -- /home/khornlund/bb/kaggle/aptos/aptos/data_loader/data_loaders.py -- END --\n# -- /home/khornlund/bb/kaggle/aptos/aptos/data_loader/preprocess.py -- START --\n\nclass RLE:\n    \"\"\"\n    Encapsulates run-length-encoding functionality.\n    \"\"\"\n\n    MASK_H = 256\n    MASK_W = 1600\n\n    @classmethod\n    def from_str(cls, s):\n        if s != s:\n            return cls()\n        list_ = [int(n) for n in s.split(' ')]\n        return cls.from_list(list_)\n\n    @classmethod\n    def from_mask(cls, mask):\n        pixels = mask.T.flatten()\n        pixels = np.concatenate([[0], pixels, [0]])\n        runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n        runs[1::2] -= runs[::2]\n        return cls.from_list(runs)\n\n    @classmethod\n    def from_list(cls, list_):\n        n_items = int(len(list_) / 2)\n        items = np.zeros((n_items, 2), dtype=np.uint64)\n        for i in range(n_items):\n            items[i, 0] = list_[i * 2]\n            items[i, 1] = list_[i * 2 + 1]\n        return cls(items)\n\n    def __init__(self, items=np.zeros((0, 0))):\n        self._items = items\n\n    @property\n    def items(self):\n        return self._items\n\n    def __iter__(self):\n        for idx, item in enumerate(self.items):\n            yield (item[0], item[1])  # run, length\n\n    def __len__(self):\n        return self.items.shape[0]\n\n    def to_mask(self):\n        mask = np.zeros(self.MASK_H * self.MASK_W, dtype=np.uint8)\n        for run, length in self:\n            run = int(run - 1)\n            end = int(run + length)\n            mask[run:end] = 1\n        return mask.reshape(self.MASK_H, self.MASK_W, order='F')\n\n    def to_str_list(self):\n        list_ = []\n        for run, length in self:\n            list_.append(str(run))\n            list_.append(str(length))\n        return list_\n\n    def __str__(self):\n        if len(self) == 0:\n            return ''\n        return ' '.join(self.to_str_list())\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass PostProcessor:\n\n    N_CLASSES = 4\n\n    def __init__(self, p_thresh=None, min_class_sizes=None):\n        self.p_thresh = np.array(p_thresh)\n        self.min_class_sizes = np.array(min_class_sizes)\n        print(self.p_thresh)\n        print(self.min_class_sizes)\n        \n    def process(self, probabilities):\n        mask = probabilities > self.p_thresh[:, np.newaxis, np.newaxis]\n        for c in range(self.N_CLASSES):\n            if mask[c, :, :].sum() < self.min_class_sizes[c]:\n                mask[c, :, :] = 0  # wipe the predictions\n        return mask\n\n# -- /home/khornlund/bb/kaggle/aptos/aptos/data_loader/preprocess.py -- END --\n\n# -- /home/khornlund/bb/kaggle/aptos/aptos/utils/saving.py -- START --\n\ndef ensure_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n    return path\n\n\ndef ensure_parent(path):\n    parent = os.path.sep.join(path.split(os.path.sep)[:-1])\n    ensure_dir(parent)\n    return path\n\n\ndef arch_path(config):\n    return ensure_dir(join(config['save_dir'], config['name']))\n\n\ndef arch_datetime_path(config):\n    start_time = datetime.datetime.now().strftime('%m%d_%H%M%S')\n    return ensure_dir(join(arch_path(config), start_time))\n\n\ndef log_path(config):\n    return ensure_dir(join(arch_path(config), 'logs'))\n\n\ndef trainer_paths(config):\n    \"\"\"Returns the paths to save checkpoints and tensorboard runs. eg.\n    saved/Mnist_LeNet/<start time>/checkpoints\n    saved/Mnist_LeNet/<start time>/runs\n    \"\"\"\n    arch_datetime = arch_datetime_path(config)\n    return (\n        ensure_dir(join(arch_datetime, 'checkpoints')),\n        ensure_dir(join(arch_datetime, 'runs'))\n    )\n\n# -- /home/khornlund/bb/kaggle/aptos/aptos/utils/saving.py -- END --\n\n# -- /home/khornlund/bb/kaggle/aptos/aptos/utils/logger.py -- START --\n\nLOG_CONFIG = \"\"\"\nversion: 1\ndisable_existing_loggers: False\nformatters:\n    simple:\n        format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\nhandlers:\n    console:\n        class: logging.StreamHandler\n        level: DEBUG\n        formatter: simple\n        stream: ext://sys.stdout\n\nroot:\n    level: INFO\n    handlers: [console]\n\"\"\"\n\ndef setup_logging(run_config, log_config=LOG_CONFIG, default_level=logging.INFO):\n    \"\"\"\n    Setup logging configuration\n    \"\"\"\n    config = yaml.safe_load(log_config)\n    logging.config.dictConfig(config)\n\n\nlogging_level_dict = {\n    0: logging.WARNING,\n    1: logging.INFO,\n    2: logging.DEBUG\n}\n\n\ndef setup_logger(cls, verbose=0):\n    logger = logging.getLogger(cls.__class__.__name__)\n    if verbose not in logging_level_dict:\n        raise KeyError(f'verbose option {verbose} for {cls} not valid. '\n                       f'Valid options are {logging_level_dict.keys()}.')\n    logger.setLevel(logging_level_dict[verbose])\n    return logger\n\n# -- /home/khornlund/bb/kaggle/aptos/aptos/utils/logger.py -- END --\n# -- /home/khornlund/bb/kaggle/aptos/aptos/main.py -- START --\n\ndef build_instance(ctor, name, config, *args):\n    return ctor(*args, **config[name]['args'])\n\n# -- /home/khornlund/bb/kaggle/aptos/aptos/main.py -- END --","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiModel(nn.Module):\n    \n    flips = [None, 3]\n    \n    def __init__(self, models):\n        super().__init__()\n        self.models = models\n        for name, model in self.models.items():\n            print(f'Loaded: {name}')\n    \n    def forward(self, x):\n        res = []\n        for name, cfg in self.models.items():\n            m = cfg['model']\n            for flip in self.flips:\n                if flip is None:\n                    output = m(x)\n                else:\n                    output = m(torch.flip(x, dims=[flip]))\n                    output = torch.flip(output, dims=[flip])\n                res.append(torch.sigmoid(output))\n        res = torch.stack(res, dim=0)\n        res = self.reduce_mean(res)\n        res = self.zero_nonmax(res)\n        # assert res.size(0) == x.size(0), res.size()\n        # assert res.size(1) == 4, res.size()\n        # assert res.size(2) == x.size(2), res.size()\n        # assert res.size(3) == x.size(3), res.size()\n        return res\n    \n    def reduce_mean(self, t):\n        return torch.mean(t, dim=0)\n    \n    def reduce_rms(self, t):\n        tsq = torch.mul(t, t)\n        return torch.sqrt(tsq.mean(dim=0))\n    \n    def reduce_rmc(self, t):\n        tcb = torch.mul(t, torch.mul(t, t))\n        cube_root = torch.tensor(1/3).cuda()\n        return torch.pow(tcb.mean(dim=0), cube_root)\n    \n    def zero_nonmax(self, t):\n        m = torch.max(t, dim=1, keepdim=True)[0]  # max prediction per class\n        t[t < m] = 0  # zero nonmax elements\n        # print('returning from zero_nonmax')\n        return t\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Runner:\n\n    def predict(self, config):\n        setup_logging(config)\n        self.logger = setup_logger(self, config['testing']['verbose'])\n        self._seed_everything(config['seed'])\n\n        self.logger.info(f'Using config:\\n{config}')\n\n        pp = build_instance(PostProcessor, 'postprocess', config)\n        tsfm = LightTransforms()\n        \n        self.logger.debug('Getting data_loader instance')\n        data_loader = SteelDataLoader(\n            tsfm,\n            config['testing']['data_dir'],\n            batch_size=config['testing']['batch_size'],\n            validation_split=0.0,\n            train=False,\n            shuffle=False,\n            nworkers=config['testing']['num_workers']\n        )\n\n        self.logger.debug('Building model architecture')\n        models = {}\n        for arch, cfg in config['models'].items():\n            self.logger.debug(f'Building {arch}')\n            if cfg['type'] == 'Unet':\n                model = build_instance(Unet, arch, config['models'])\n            elif cfg['type'] == 'FPN':\n                model = build_instance(FPN, arch, config['models'])\n            model, device = self._prepare_device(model, config['n_gpu'])\n            model_checkpoint = config['models'][arch]['checkpoint']\n            self.logger.debug(f'Loading checkpoint {model_checkpoint}')\n            checkpoint = torch.load(model_checkpoint, map_location=device)\n            model.load_state_dict(checkpoint['state_dict'])\n            model.eval()\n            models[arch] = {\n                'model': model,\n                # 'weight': cfg['weight']\n            }\n            \n        model = MultiModel(models)\n        model.eval()\n\n        for name, cfg in model.models.items():\n            assert not cfg['model'].training, 'model is still in training mode!'\n        \n        predictions = []\n\n        preds = torch.zeros(len(data_loader.dataset))\n        self.logger.debug('Starting...')\n        with torch.no_grad():\n            for i, (fs, data) in enumerate(tqdm(data_loader)):\n                data = data.to(device)\n                output = model(data)\n                batch_preds = output.detach().cpu().numpy()\n                for (f, preds) in zip(fs, batch_preds):\n                    masks = pp.process(preds)\n                    for class_, mask in enumerate(masks):\n                        rle = str(RLE.from_mask(mask))\n                        name = f + f\"_{class_+1}\"\n                        predictions.append([name, rle])\n                        \n        # save predictions to submission.csv\n        df = pd.DataFrame(predictions, columns=['ImageId_ClassId', 'EncodedPixels'])\n        return df\n\n    def _prepare_device(self, model, n_gpu_use):\n        device, device_ids = self._get_device(n_gpu_use)\n        model = model.to(device)\n        return model, device\n\n    def _get_device(self, n_gpu_use):\n        \"\"\"\n        setup GPU device if available, move model into configured device\n        \"\"\"\n        n_gpu = torch.cuda.device_count()\n        if n_gpu_use > 0 and n_gpu == 0:\n            self.logger.warning(\"Warning: There\\'s no GPU available on this machine,\"\n                                \"training will be performed on CPU.\")\n            n_gpu_use = 0\n        if n_gpu_use > n_gpu:\n            self.logger.warning(f\"Warning: The number of GPU\\'s configured to use is {n_gpu_use}, \"\n                                f\"but only {n_gpu} are available on this machine.\")\n            n_gpu_use = n_gpu\n        device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')\n        list_ids = list(range(n_gpu_use))\n        self.logger.info(f'Using device: {device}, {list_ids}')\n        return device, list_ids\n\n    def _seed_everything(self, seed):\n        self.logger.info(f'Using random seed: {seed}')\n        random.seed(seed)\n        os.environ['PYTHONHASHSEED'] = str(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ------------------------------------------------------------\n# Config\n# ------------------------------------------------------------\n    \nconfig_str = \"\"\"\nname: sever\nn_gpu: 1\nsave_dir: saved/\nseed: 1234\napex: O1\n\nmodels:\n\n  # -- Unet ----------------------------------------------------------------\n\n  # LB 0.91727\n  unet-se_resnext50_32x4d-1021-092425-LB-0.91727:\n    type: Unet\n    args:\n      encoder_name: se_resnext50_32x4d\n      classes: 4\n      encoder_weights: null\n      in_channels: 1\n      activation: null\n    checkpoint: ../input/1021-092425/checkpoint-epoch144.pth\n    \n  # LB 0.91685\n  unet-se_resnext50_32x4d-1021-220445-LB-0.91685:\n    type: Unet\n    args:\n      encoder_name: se_resnext50_32x4d\n      classes: 4\n      encoder_weights: null\n      in_channels: 1\n      activation: null\n    checkpoint: ../input/1021-220445/checkpoint-epoch200.pth\n\n  # LB 0.91665\n  unet-b5-1021-215447-LB-0.91665:\n    type: Unet\n    args:\n      encoder_name: efficientnet-b5\n      classes: 4\n      encoder_weights: imagenet\n      in_channels: 1\n      activation: null\n    checkpoint: ../input/1021-215447/checkpoint-epoch228.pth\n\n  # -- FPN -----------------------------------------------------------------\n\n  # LB 0.91656\n  fpn-se_resnext50_32x4d-1018_082516-LB-0.91656:\n    type: FPN\n    args:\n      encoder_name: se_resnext50_32x4d\n      classes: 4\n      encoder_weights: null\n      in_channels: 1\n      activation: null\n      weight_std: false\n      decoder_merge_policy: cat\n    checkpoint: ../input/1018_082516/checkpoint-epoch186.pth\n    \n  # LB 0.91634\n  fpn-se_resnext50_32x4d-1020-222927-LB-0.91634:\n    type: FPN\n    args:\n      encoder_name: se_resnext50_32x4d\n      classes: 4\n      encoder_weights: null\n      in_channels: 1\n      activation: null\n      weight_std: false\n      decoder_merge_policy: cat\n    checkpoint: ../input/1020-222927/checkpoint-epoch162.pth\n\n  # LB 0.91744\n  fpn-se_resnext50_32x4d-1022-120054-LB-0.91744:\n    type: FPN\n    args:\n      encoder_name: se_resnext50_32x4d\n      classes: 4\n      encoder_weights: null\n      in_channels: 1\n      activation: null\n      weight_std: false\n      decoder_merge_policy: cat\n    checkpoint: ../input/1022-120054/checkpoint-epoch159.pth\n\n  # LB ??\n  fpn-b5-1024-084704:\n    type: FPN\n    args:\n      encoder_name: efficientnet-b5\n      classes: 4\n      encoder_weights: imagenet\n      in_channels: 1\n      activation: null\n      weight_std: false\n      decoder_merge_policy: cat\n    checkpoint: ../input/1024-084704/checkpoint-epoch151.pth\n\n  # LB ??\n  fpn-inceptionv4-1023-230053:\n    weight: 1.0\n    type: FPN\n    args:\n      encoder_name: inceptionv4\n      classes: 4\n      encoder_weights: null\n      in_channels: 1\n      activation: null\n      weight_std: false\n      decoder_merge_policy: cat\n    checkpoint: ../input/1023-230053/checkpoint-epoch168.pth\n\ntesting:\n  data_dir: /kaggle/input/severstal-steel-defect-detection\n  batch_size: 16\n  num_workers: 2\n  verbose: 2\n  \npostprocess:\n  args:\n    p_thresh: [0.50, 0.50, 0.50, 0.50]\n    min_class_sizes: [800, 800, 1500, 2500]\n\"\"\"\n\n# ------------------------------------------------------------\n# Execution\n# ------------------------------------------------------------\n\nconfig = yaml.safe_load(config_str)\n\ntry:\n    df = Runner().predict(config)\nexcept Exception as ex:\n    print(f'Caught exception: {ex}')\n\nprint(config)\nprint(df.head(20))\ndf.to_csv(\"submission.csv\", index=False)\nprint(f'Finished saving predictions to \"submission.csv\"')\ndf_orig = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = df_orig.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['ImageId'], df['ClassId'] = zip(*df['ImageId_ClassId'].str.split('_'))\n# df['ClassId'] = df['ClassId'].astype(int)\n# df = df.pivot(index='ImageId', columns='ClassId', values='EncodedPixels')\n# df.columns = [f'rle{c}' for c in range(4)]\n# df['defects'] = df.count(axis=1)\n# for c in range(4):\n#     df[f'c{c}'] = df[f'rle{c}'].apply(lambda rle: rle != '')\n\n# counts = {c: df[f'c{c}'].sum() for c in range(4)}\n# counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# [0.50, 0.50, 0.50, 0.50]\n# {0: 91, 1: 1, 2: 571, 3: 110}\n\n# [0.45, 0.45, 0.45, 0.45]\n# {0: 92, 1: 1, 2: 574, 3: 110}\n\n# [0.40, 0.40, 0.40, 0.40]\n# {0: 94, 1: 1, 2: 579, 3: 110}\n\n# [0.35, 0.35, 0.35, 0.35]\n# {0: 95, 1: 1, 2: 581, 3: 110}\n\n# cube root\n# {0: 105, 1: 4, 2: 600, 3: 112}","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}