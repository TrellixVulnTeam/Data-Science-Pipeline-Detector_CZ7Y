{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Convert pretrained PyTorch RGB Model to Single Channel Grey\n\n## The Problem\n\nChallenge participants will be using models pretrained on RGB data where the first convolution takes in 3 channels.\nThe competition data is fundamentally single channel. The images are RGB JPEGS, the R, G, and B values are always the same.\n\nThis is inefficient because:\n- Data augmentation transforms will be performed on 3x more data then necessary\n- Data transfer to the GPU is 3x larger than necessary\n- Flops of the first convolution are approximatley 3x more than necessary\n\n## The Solution\n\nConvert the first convolution to single channel by summing over the channels.\n\nBelow I demonstrated PyTorch surgery on a pre-trained network to address this. Outputs are not exactly the same due to the complexity of whitening with different means and variances for R, G, and B. The final plot should demonstrate that the outputs are approximately the same, and certainly close enough for fine tuning in a dramatically different domain."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport torchvision\nimport torch.nn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Demonstration Image"},{"metadata":{"trusted":true},"cell_type":"code","source":"pil_im = Image.open(os.path.join('../input/test_images/b51cdf84f.jpg'))\npil_im","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rgb = torchvision.models.resnet18(pretrained=True)\nmodel_rgb.eval()\n\ntransforms_rgb = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(224),\n    torchvision.transforms.CenterCrop(224),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_grey = torchvision.models.resnet18(pretrained=True)\nweight_rgb = model_grey.conv1.weight\n\n# Sum over the weights to convert the kernel\nweight_grey = weight_rgb.sum(dim=1, keepdim=True)\nbias = model_grey.conv1.bias\n\n# Instantiate a new convolution module and set weights\nmodel_grey.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nmodel_grey.conv1.weight = torch.nn.Parameter(weight_grey)\nmodel_grey.conv1.bias = bias\n\nmodel_grey.eval()\n\ntransforms_grey = torchvision.transforms.Compose([\n    # Reduce to a single channel. We could use x.convert('L') here, but this is probably fewer operations and the result is the same.\n    torchvision.transforms.Lambda(lambda x: Image.fromarray(np.asarray(pil_im)[:, :, 0], mode='L')),\n    \n    # These image transforms are now on a single channel image instead of 3\n    torchvision.transforms.Resize(224),\n    torchvision.transforms.CenterCrop(224),\n    torchvision.transforms.ToTensor(),\n    \n    # Normalization means and stds can be taken as the average of the rgb values\n    torchvision.transforms.Normalize(mean=[np.mean([0.485, 0.456, 0.406])], std=[np.mean([0.229, 0.224, 0.225])])\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tensor_rgb = transforms_rgb(pil_im)\nbatch_tensor_rgb = tensor_rgb.unsqueeze(0)\n\nwith torch.no_grad():\n    classes_rgb = model_rgb(batch_tensor_rgb)\n    classes_rgb = classes_rgb.squeeze().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tensor_grey = transforms_grey(pil_im)\nbatch_tensor_grey = tensor_grey.unsqueeze(0)\n\nwith torch.no_grad():\n    classes_grey = model_grey(batch_tensor_grey)\n    classes_grey = classes_grey.squeeze().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 8))\nplt.scatter(classes_rgb, classes_grey)\nplt.xlabel('RGB Model')\nplt.ylabel('Grey Model')\nplt.title('Converted Model ImageNet Class Logit Comparison')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"import urllib.request\nimport json\n\nresponse = urllib.request.urlopen('https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt')\ndata = response.read()\nlabel_mapping = eval(data)\n\nlabel_mapping[np.argmax(classes_grey)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the curious... it's a fountain."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}