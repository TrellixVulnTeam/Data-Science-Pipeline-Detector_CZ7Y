{"cells":[{"metadata":{},"cell_type":"markdown","source":"The Original kernel has been adapted to use 256x256 crops taken in steps of 64 px. When using the full image the author observed that the test and validation images could be distinguished by a classifier. For the 256x256 crops that does not seem to be the case.\n\nSee the original kernel to get the full context."},{"metadata":{},"cell_type":"markdown","source":"# Prepare Images\nThis section has been adapted so that crops are saved instead of full images"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import os, numpy as np\nfrom PIL import Image \n\nTRAIN_IMG = os.listdir('../input/severstal-steel-defect-detection/train_images')\nTEST_IMG = os.listdir('../input/severstal-steel-defect-detection/test_images')\nprint('Original train count =',len(TRAIN_IMG),', Original test count =',len(TEST_IMG))\nprint('New train count = 1801 , New test count = 1801')\nos.system('rm -rf ../tmp')\nos.mkdir('../tmp/')\nos.mkdir('../tmp/train_images/')\nr = np.random.choice(TRAIN_IMG,len(TEST_IMG),replace=False)\nfor i,f in enumerate(r):\n    img = Image.open('../input/severstal-steel-defect-detection/train_images/'+f)\n    # select crop starting point randomly\n    for i_start in range(0,1600-256+1,64):\n        img = img.crop((i_start, 0, i_start+256, 256))\n        img.save('../tmp/train_images/crop_'+str(i_start)+f)\nos.mkdir('../tmp/test_images/')\nfor i,f in enumerate(TEST_IMG):\n    img = Image.open('../input/severstal-steel-defect-detection/test_images/'+f)\n    for i_start in range(0,1600-256+1,64):\n        img = img.crop((i_start, 0, i_start+256, 256))\n        img.save('../tmp/test_images/crop_'+str(i_start)+f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Adversarial Classifier\nTo distinguish train images from test images, we will use pretrained Xception."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import pandas as pd\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras import layers\nfrom keras.callbacks import LearningRateScheduler\nimport matplotlib.pyplot as plt, time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"from keras import applications\nbase_model = applications.Xception(weights=None, input_shape=(256, 256, 3), include_top=False)\nbase_model.load_weights('../input/keras-pretrained-models/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')\nbase_model.trainable = False\nx = base_model.output\nx = layers.Flatten()(x)\nx = layers.Dense(1024, activation=\"relu\")(x)\nx = layers.Dropout(0.5)(x)\npredictions = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(input = base_model.input, output = predictions)\nmodel.compile(loss='binary_crossentropy', optimizer = \"adam\", metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_dir = '../tmp/'\nimg_height = 256; img_width = 256\nbatch_size = 16; nb_epochs = 5\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n    horizontal_flip=True,\n    validation_split=0.2) # set validation split\n\ntrain_generator = train_datagen.flow_from_directory(\n    img_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='training') # set as training data\n\nvalidation_generator = train_datagen.flow_from_directory(\n    img_dir, # same directory as training data\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='validation') # set as validation data\n\nannealer = LearningRateScheduler(lambda x: 0.0001 * 0.95 ** x)\nh = model.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples // batch_size,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples // batch_size,\n    epochs = nb_epochs,\n    callbacks = [annealer],\n    verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.plot(h.history['acc'],label='Train ACC')\nplt.plot(h.history['val_acc'],label='Val ACC')\nplt.title('TRAIN COMPARED WITH TEST. Training History')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nWhile the whole image seems to be different in train and test sets there is no such evidence present for the 256x256 crops.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}