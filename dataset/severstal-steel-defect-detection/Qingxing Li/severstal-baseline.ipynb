{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf # tensorflow\nfrom tensorflow import reduce_sum\nfrom tensorflow.keras.backend import pow\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Concatenate, Add, Flatten\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.activations import relu\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.initializers import he_normal\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nimport cv2\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt # plt显示图片\nimport matplotlib.image as mpimg # mpimg 用于读取图片\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    if 'test_images' in dirname:\n        for filename in filenames:\n            file = os.path.join(dirname,filename)\n            print(file)\n            img = mpimg.imread(file)\n            print(img.shape)\n            plt.imshow(img)\n            break\n    continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Kernel Configurations\nmake_submission = False # used to turn off lengthy model analysis so a submission version doesn't run into memory error\nload_pretrained_model = True # load a pre-trained model\nsave_model = True # save the model after training\ntrain_dir = '../input/severstal-steel-defect-detection/' # directory of training images\n# pretrained_model_path = '../input/severstal-pretrained-model/ResUNetSteel_z.h5' # path of pretrained model\nmodel_save_path = './ResUNetSteel_w800e50_z.h5' # path of model to save\ntrain_image_dir = os.path.join(train_dir, 'train_images') # ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# network configuration parameters\n# original image is 1600x256, so we will resize it\nimg_w = 512 # resized weidth\nimg_h = 256 # resized height\nbatch_size = 16\nepochs = 10\n# batch size for training unet\nk_size = 3 # kernel size 3x3\nval_size = 0.2 # split of training set between train and validation set\n# we will repeat the images with lower samples to make the training process more fair\nrepeat = False\n# only valid if repeat is True\nclass_1_repeat = 1 # repeat class 1 examples x times\nclass_2_repeat = 1\nclass_3_repeat = 1\nclass_4_repeat = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Processing the Training Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load full data and label no mask as -1\ntrain_df = pd.read_csv(os.path.join(train_dir, 'train.csv')).fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['ImageId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\ntrain_df['ClassId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[1])\ntrain_df['ClassId_EncodedPixels'] = train_df.apply(lambda row: (row['ClassId'], row['EncodedPixels']), axis = 1)\ngrouped_EncodedPixels = train_df.groupby('ImageId')['ClassId_EncodedPixels'].apply(list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('ClassId_EncodedPixels:',train_df.ClassId_EncodedPixels)\nprint('grouped_EncodedPixels:',grouped_EncodedPixels)\nprint('\\n')\nprint(train_df['ImageId'].size)\ntrain_img_list = np.unique(train_df['ImageId'])\nprint(train_img_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #  use imggenerator to process the images\n# dg_args = dict(featurewise_center = False, \n#                   samplewise_center = False,\n#                   rotation_range = 45, \n#                   width_shift_range = 0.1, \n#                   height_shift_range = 0.1, \n#                   shear_range = 0.01,\n#                   zoom_range = [0.9, 1.25],  \n#                   horizontal_flip = True, \n#                   vertical_flip = True,\n#                   fill_mode = 'reflect',\n#                    data_format = 'channels_last')\n# valid_args = dict(\n#                     fill_mode = 'reflect',\n#                    data_format = 'channels_last')\n\n# core_idg = ImageDataGenerator(**dg_args)\n# valid_idg = ImageDataGenerator(**valid_args)\n\n\n# # def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, **dflow_args):\n# #     base_dir = os.path.dirname(in_df[path_col].values[0])\n# #     print('## Ignore next message from keras, values are replaced anyways')\n# #     df_gen = img_data_gen.flow_from_directory(base_dir, \n# #                                      class_mode = 'sparse',\n# #                                     **dflow_args)\n# #     df_gen.filenames = in_df[path_col].values\n# #     df_gen.classes = np.stack(in_df[y_col].values)\n# #     df_gen.samples = in_df.shape[0]\n# #     df_gen.n = in_df.shape[0]\n# #     df_gen._set_index_array()\n# #     df_gen.directory = '' # since we have the full path\n# #     print('Reinserting dataframe: {} images'.format(in_df.shape[0]))\n# #     return df_gen\n\n# # train_gen = flow_from_dataframe(core_idg, train_df, \n# #                              path_col = 'path',\n# #                             y_col = 'has_ship_vec', \n# #                             target_size = (256,800),\n# #                              color_mode = 'rgb',\n# #                             batch_size = 30)\n\n# # # used a fixed dataset for evaluating the algorithm\n# # valid_x, valid_y = next(flow_from_dataframe(valid_idg, \n# #                                valid_df, \n# #                              path_col = 'path',\n# #                             y_col = 'has_ship_vec', \n# #                             target_size = (256,800),\n# #                              color_mode = 'rgb',\n# #                             batch_size = 1000)) # one big batch\n# # print(valid_x.shape, valid_y.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGen(tf.keras.utils.Sequence):\n    def __init__(self, list_ids, labels, image_dir, batch_size=32,\n                 img_h=256, img_w=512, shuffle=True):\n        self.list_ids = list_ids\n        self.labels = labels\n        self.image_dir = image_dir\n        self.batch_size = batch_size\n        self.img_h = img_h\n        self.img_w = img_w\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    \n    def __len__(self):\n        # 代表每个epoch有多少个batch\n        return int(np.floor(len(self.list_ids)) / self.batch_size)\n    \n    def on_epoch_end(self):\n        'update ended after each epoch'\n        # 每个epoch之后重新打乱index\n        self.indexes = np.arange(len(self.list_ids))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n        \n    \n    def __data_generation(self, list_ids_temp):\n        'generate data containing batch_size samples'\n        X = np.empty((self.batch_size, self.img_h, self.img_w, 1))\n        y = np.empty((self.batch_size, self.img_h, self.img_w, 4))\n        \n        for idx, id in enumerate(list_ids_temp):\n            # 对每一张图片\n            file_path =  os.path.join(self.image_dir, id)\n            image = cv2.imread(file_path, 0)\n            image_resized = cv2.resize(image, (self.img_w, self.img_h))\n            image_resized = cv2.GaussianBlur(image_resized,(5,5),0)\n            image_resized = np.array(image_resized, dtype=np.float32)\n            '进行标准化（可能会过拟合，之后考虑一下用整体的mean和方差来进行标准化'\n            image_resized /= 255\n            image_resized -= image_resized.mean()\n            image_resized /= image_resized.std()\n            \n            \n            mask = np.empty((img_h, img_w, 4))\n            \n            for idm, image_class in enumerate(['1','2','3','4']):\n                rle = self.labels.get(id + '_' + image_class)\n                # if there is no mask create empty mask\n                if rle is None:\n                    class_mask = np.zeros((1600, 256))\n                else:\n                    class_mask = rle_to_mask(rle, width=1600, height=256)\n             \n                class_mask_resized = cv2.resize(class_mask, (self.img_w, self.img_h))\n                mask[...,idm] = class_mask_resized\n            \n            X[idx,] = np.expand_dims(image_resized, axis=2)\n            y[idx,] = mask\n        \n        # normalize Y\n        y = (y > 0).astype(int)\n        \n        return X, y\n\n    def __getitem__(self, index):\n        # 生成一个batch的数据\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # get list of IDs\n        list_ids_temp = [self.list_ids[k] for k in indexes]\n        # generate data\n        X, y = self.__data_generation(list_ids_temp)\n        # return data \n        return X, y\n    \n    \ndef rle_to_mask(rle_string,height,width):\n    '''\n    convert RLE(run length encoding) string to numpy array\n\n    Parameters: \n    rleString (str): Description of arg1 \n    height (int): height of the mask\n    width (int): width of the mask \n\n    Returns: \n    numpy.array: numpy array of the mask\n    '''\n    rows, cols = height, width\n    if rle_string == -1:\n        return np.zeros((height, width))\n    else:\n        rleNumbers = [int(numstring) for numstring in rle_string.split(' ')]\n        rlePairs = np.array(rleNumbers).reshape(-1,2)\n        img = np.zeros(rows*cols,dtype=np.uint8)\n        for index,length in rlePairs:\n            index -= 1\n            img[index:index+length] = 255\n        img = img.reshape(cols,rows)\n        img = img.T\n        return img\n    \ndef mask_to_rle(mask):\n    '''\n    Convert a mask into RLE\n    \n    Parameters: \n    mask (numpy.array): binary mask of numpy array where 1 - mask, 0 - background\n\n    Returns: \n    sring: run length encoding \n    '''\n    pixels= mask.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'生成一个包含所有mask的字典'\nmasks = {}\nfor index, row in train_df[train_df['EncodedPixels']!=-1].iterrows():\n    masks[row['ImageId_ClassId']] = row['EncodedPixels']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# repeat low represented samples more frequently to balance our dataset\n\nclass_1_img_id = train_df[(train_df['EncodedPixels']!=-1) & (train_df['ClassId']=='1')]['ImageId'].values\nclass_1_img_id = np.repeat(class_1_img_id, class_1_repeat)\nclass_2_img_id = train_df[(train_df['EncodedPixels']!=-1) & (train_df['ClassId']=='2')]['ImageId'].values\nclass_2_img_id = np.repeat(class_2_img_id, class_2_repeat)\nclass_3_img_id = train_df[(train_df['EncodedPixels']!=-1) & (train_df['ClassId']=='3')]['ImageId'].values\nclass_3_img_id = np.repeat(class_3_img_id, class_3_repeat)\nclass_4_img_id = train_df[(train_df['EncodedPixels']!=-1) & (train_df['ClassId']=='4')]['ImageId'].values\nclass_4_img_id = np.repeat(class_4_img_id, class_4_repeat)\ntrain_image_ids = np.concatenate([class_1_img_id, class_2_img_id, class_3_img_id, class_4_img_id])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'划分训练集和验证集'\nX_train, X_val = train_test_split(train_image_ids, test_size=val_size, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'img_h': img_h,\n          'img_w': img_w,\n          'image_dir': train_image_dir,\n          'batch_size': batch_size,\n          'shuffle': True }\n\n# Get Generators\ntraining_generator = DataGen(X_train, masks, **params)\nvalidation_generator = DataGen(X_val, masks, **params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = training_generator.__getitem__(0)\nprint(x.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize steel image with four classes of faults in seperate columns\ndef viz_steel_img_mask(img, masks):\n    img = cv2.cvtColor(img.astype('float32'), cv2.COLOR_BGR2RGB)\n    fig, ax = plt.subplots(nrows=1, ncols=4, sharey=True, figsize=(20,10))\n    cmaps = [\"Reds\", \"Blues\", \"Greens\", \"Purples\"]\n    for idx, mask in enumerate(masks):\n        ax[idx].imshow(img)\n        ax[idx].imshow(mask, alpha=0.3, cmap=cmaps[idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx in range(0,batch_size):\n    if y[idx].sum() > 0:\n        img = x[idx]\n        masks_temp = [y[idx][...,i] for i in range(0,4)]\n        viz_steel_img_mask(img, masks_temp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model\nbuild a baseline resnet+unet model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bn_act(x, act=True):\n    'batch normalization layer with an optinal activation layer'\n    x = tf.keras.layers.BatchNormalization()(x)\n    if act == True:\n        x = tf.keras.layers.Activation('relu')(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv_block(x, filters, kernel_size=3, padding='same', strides=1):\n    'convolutional layer which always uses the batch normalization layer'\n    conv = bn_act(x)\n    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides, kernel_initializer=\"he_normal\")(conv)\n    return conv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stem(x, filters, kernel_size=3, padding='same', strides=1):\n    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides, kernel_initializer=\"he_normal\")(x)\n    conv = conv_block(conv, filters, kernel_size, padding, strides)\n    shortcut = Conv2D(filters, kernel_size=1, padding=padding, strides=strides, kernel_initializer=\"he_normal\")(x)\n    shortcut = bn_act(shortcut, act=False)\n    output = Add()([conv, shortcut])\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def residual_block(x, filters, kernel_size=3, padding='same', strides=1):\n    res = conv_block(x, filters, k_size, padding, strides)\n    res = conv_block(res, filters, k_size, padding, 1)\n    shortcut = Conv2D(filters, kernel_size, padding=padding, strides=strides, kernel_initializer=\"he_normal\")(x)\n    shortcut = bn_act(shortcut, act=False)\n    output = Add()([shortcut, res])\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def upsample_concat_block(x, xskip):\n    u = UpSampling2D((2,2))(x)\n    c = Concatenate()([u, xskip])\n    return c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ResUNet(img_h, img_w):\n    f = [16, 32, 64, 128, 256, 512]\n    inputs = Input((img_h, img_w, 1))\n    ## Encoder\n    e0 = inputs\n    e1 = stem(e0, f[0])\n    e2 = residual_block(e1, f[1], strides=2)\n    e3 = residual_block(e2, f[2], strides=2)\n    e4 = residual_block(e3, f[3], strides=2)\n    e5 = residual_block(e4, f[4], strides=2)\n    e6 = residual_block(e5, f[5], strides=2)\n    \n    ## Bridge\n    b0 = conv_block(e6, f[5], strides=1)\n    b1 = conv_block(b0, f[5], strides=1)\n    \n    ## Decoder\n    u1 = upsample_concat_block(b1, e5)\n    d1 = residual_block(u1, f[5])\n    \n    u2 = upsample_concat_block(d1, e4)\n    d2 = residual_block(u2, f[4])\n    \n    u3 = upsample_concat_block(d2, e3)\n    d3 = residual_block(u3, f[3])\n    \n    u4 = upsample_concat_block(d3, e2)\n    d4 = residual_block(u4, f[2])\n    \n    u5 = upsample_concat_block(d4, e1)\n    d5 = residual_block(u5, f[1])\n    \n    outputs = tf.keras.layers.Conv2D(4, (1, 1), padding=\"same\", activation=\"sigmoid\", kernel_initializer=\"he_normal\")(d5)\n    model = Model(inputs, outputs)\n    print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ResUNet(img_h,img_w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dice similarity coefficient loss, brought to you by: https://github.com/nabsabraham/focal-tversky-unet\ndef dsc(y_true, y_pred):\n    smooth = 1.\n    y_true_f = Flatten()(y_true)\n    y_pred_f = Flatten()(y_pred)\n    intersection = reduce_sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (reduce_sum(y_true_f) + reduce_sum(y_pred_f) + smooth)\n    return score\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dsc(y_true, y_pred)\n    return loss\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Focal Tversky loss, brought to you by:  https://github.com/nabsabraham/focal-tversky-unet\ndef tversky(y_true, y_pred, smooth=1e-6):\n    y_true_pos = tf.keras.layers.Flatten()(y_true)\n    y_pred_pos = tf.keras.layers.Flatten()(y_pred)\n    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos)\n    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n    false_pos = tf.reduce_sum((1-y_true_pos)*y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n\ndef tversky_loss(y_true, y_pred):\n    return 1 - tversky(y_true,y_pred)\n\ndef focal_tversky_loss(y_true,y_pred):\n    pt_1 = tversky(y_true, y_pred)\n    gamma = 0.75\n    return tf.keras.backend.pow((1-pt_1), gamma)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compile & Fit The Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"adam = tf.keras.optimizers.Adam(lr = 0.05, epsilon = 0.1)\nmodel.compile(optimizer=adam, loss=focal_tversky_loss, metrics=[tversky])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checkpoint = ModelCheckpoint(\"mymodel_1.h5\", monitor='tversky', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n# early = EarlyStopping(monitor='tversky', min_delta=0, patience=10, verbose=1, mode='auto')\n\n# aug = ImageDataGenerator(rotation_range=20, zoom_range=0.10,rescale=1./255,\n#                          width_shift_range=0.1, height_shift_range=0.1, shear_range=0.10,\n#                          horizontal_flip=False, fill_mode=\"nearest\")\n\nmodel.fit_generator(training_generator, validation_data=validation_generator, epochs=20\n#                     ,callbacks = [checkpoint],\n#                     verbose = 2\n                   )\nmodel.save(\"Severstal_baseline2.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# return tensor in the right shape for prediction \ndef get_test_tensor(img_dir, img_h, img_w, channels=1):\n\n    X = np.empty((1, img_h, img_w, channels))\n    # Store sample\n    image = cv2.imread(img_dir, 0)\n    image_resized = cv2.resize(image, (img_w, img_h))\n    image_resized = np.array(image_resized, dtype=np.float64)\n    # normalize image\n    image_resized -= image_resized.mean()\n    image_resized /= image_resized.std()\n    \n    X[0,] = np.expand_dims(image_resized, axis=2)\n\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is an awesome little function to remove small spots in our predictions\n\nfrom skimage import morphology\n\ndef remove_small_regions(img, size):\n    \"\"\"Morphologically removes small (less than size) connected regions of 0s or 1s.\"\"\"\n    img = morphology.remove_small_objects(img, size)\n    img = morphology.remove_small_holes(img, size)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\n# get all files using glob\ntest_files = [f for f in glob.glob('../input/severstal-steel-defect-detection/test_images/' + \"*.jpg\", recursive=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = []\n\n# a function to apply all the processing steps necessery to each of the individual masks\ndef process_pred_mask(pred_mask):\n    \n    pred_mask = cv2.resize(pred_mask.astype('float32'),(1600, 256))\n    pred_mask = (pred_mask > .5).astype(int)\n    pred_mask = remove_small_regions(pred_mask, 0.02 * np.prod(512)) * 255\n    pred_mask = mask_to_rle(pred_mask)\n    \n    return pred_mask\n\n# loop over all the test images\nfor f in test_files:\n    # get test tensor, output is in shape: (1, 256, 512, 3)\n    test = get_test_tensor(f, img_h, img_w) \n    # get prediction, output is in shape: (1, 256, 512, 4)\n    pred_masks = model.predict(test) \n    # get a list of masks with shape: 256, 512\n    pred_masks = [pred_masks[0][...,i] for i in range(0,4)]\n    # apply all the processing steps to each of the mask\n    pred_masks = [process_pred_mask(pred_mask) for pred_mask in pred_masks]\n    # get our image id\n    id = f.split('/')[-1]\n    # create ImageId_ClassId and get the EncodedPixels for the class ID, and append to our submissions list\n    [submission.append((id+'_%s' % (k+1), pred_mask)) for k, pred_mask in enumerate(pred_masks)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to a csv\nsubmission_df = pd.DataFrame(submission, columns=['ImageId_ClassId', 'EncodedPixels'])\n# check out some predictions and see if RLE looks ok\nsubmission_df[ submission_df['EncodedPixels'] != ''].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a look at our submission \nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# write it out\nsubmission_df.to_csv('./submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.fit_generator(training_generator, validation_data=validation_generator, epochs=50\n# #                     ,callbacks = [checkpoint],\n# #                     verbose = 2\n#                    )\n# model.save(\"./Severstal_baseline1.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.save(model_save_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ResUnet = load_model('./MyModel.h5', custom_objects={'focal_tversky_loss':focal_tversky_loss,'tversky':tversky})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ResUnet_history = ResUnet.fit_generator(\n#                             generator=training_generator, \n#                             validation_data=validation_generator,\n#                             epochs=15,\n#                             verbose=1)\n# model.save('./MyModel2.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ResUnet_history = ResUnet.fit_generator(\n#                             generator=training_generator, \n#                             validation_data=validation_generator,\n#                             epochs=30,\n#                             verbose=1)\n# model.save('./MyModel3.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}