{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pixel Thresholds for Score Optimisation\n---\n\nDue to the way in which this competition is scored, predicting even a single pixel in an image with no defect will result in a disproportionate decrease in your score. For this reason, we will typically use a threshold for the minimum number of pixels that must be predicted before an image is said to contain a defect. For example, if a test image prediction contained 1500 pixels of defect type 2, but our threshold was 2000, we would ignore the prediction and simply say that the image has no defect.\n\nMany popular kernels are using a flat threshold  of 3500 pixels across all classes. This seems to me an arbitrary cutoff, and in this kernel I examine the pixel counts per defect type and estimate the optimum thresholds for each one. This will be condensed into a single function for postprocessing. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport cv2\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/severstal-steel-defect-detection/train.csv')\ntrain_df['ImageId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\ntrain_df['ClassId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[1]).astype('int8')\ntrain_df['hasMask'] = ~ train_df['EncodedPixels'].isna()\ntrain_df.drop(['ImageId_ClassId'], axis=1, inplace=True)\ntrain_df.fillna(0, inplace=True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this function I replace 0 values for pixel count with `np.nan`. This is to simplify visualising the pixel counts per defect type in the next section.\n\n# EDA\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"#simplified version of rle2mask() that returns total number of pixels\ndef pixel_count(rle):\n    \n    if np.logical_or(rle==0, rle==''):\n        return(np.nan)\n    array = np.asarray([int(x) for x in rle.split()])\n    lengths = array[1::2]\n    \n    return sum(lengths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEFECT_TYPES = [1, 2, 3, 4]\n\nfor x in DEFECT_TYPES:\n    train_df.loc[train_df.ClassId==x, 'pixel_count'] = train_df.loc[train_df.ClassId==x, 'EncodedPixels'].apply(pixel_count) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Excellent! Now let's look at the pixel count distributions per defect type. The red line indicates the popular cutoff of 3500."},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in DEFECT_TYPES:\n    df = train_df.loc[train_df.ClassId==x, :]\n    ax = df['pixel_count'].plot(kind='hist', bins=100, figsize=(15, 6))\n    df['pixel_count'].plot(kind='kde', ax=ax, secondary_y=True)\n    plt.xlim(left=0)\n    plt.axvline(x=3500, linewidth=2, color='r')\n    plt.title(f'Pixel Distribution for Defect Type {x}')\n    print(f'=====\\nDEFECT TYPE {x}\\n=====')\n    print(df['pixel_count'].describe())\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that for defect types 1 and 2, the 3500 cutoff is very conservative. If the test predictions had the same distribution as the training data, it would remove over half of all predictions! For defect types 3 & 4, which have larger average pixel counts, it is less likely to remove valid predictions but may be too low to function as an effective threshold.\n\n# Threshold Selection\n---\n\nHow to improve our choice of threshold to optimise our score? This will depend a lot on what type of model you are using, and whether it is implicitly designed to make conservative predictions. You may want to experiment with your own models and see how different thresholds affect your model performance when testing them on the training data. In this example I will use the pixel count's tenth percentile for each defect type. "},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLDS = []\n\nfor x in DEFECT_TYPES:\n    pixels = train_df.loc[train_df['ClassId']==x, 'pixel_count']\n    threshold = np.nanquantile(pixels, 0.1)\n    THRESHOLDS.append(int(threshold))\n    print(f'Quintile threshold for defect type {x} = {int(threshold)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The threshold for defect type 4 is rather high, so let's retain the 3500 limit. This works out quite neatly as the approximate 5th percentile for pixel counts in this category."},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLDS[3] = 3500","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation: PyTorch Inference Kernel\n---\nTo see how this affects our submission scores, we can test it on [Rishabh's](https://www.kaggle.com/rishabhiitbhu) excellent kernel on [PyTorch inference](https://www.kaggle.com/rishabhiitbhu/unet-pytorch-inference-kernel). All credit for the code in the hidden cells below is his, so please go and upvote his work!\n\nIn his initial method, predictions are removed if the number of pixels is lower than 3500. For the sake of this evaluation, I'll be leaving all predictions in place at first. We will then create two submissions: one where the 3500 pixel threshold is used in postprocessing, and another when our defect-specific percentile thresholds are used.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip install ../input/pretrainedmodels/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4/ > /dev/null\npackage_path = '../input/unetmodelscript'\nimport sys\nsys.path.append(package_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pdb\nimport cv2\nimport torch\nfrom tqdm import tqdm\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader, Dataset\nfrom albumentations import (Normalize, Compose)\nfrom albumentations.pytorch import ToTensor\nimport torch.utils.data as data\nfrom model import Unet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\ndef mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\nclass TestDataset(Dataset):\n    '''Dataset for test prediction'''\n    def __init__(self, root, df, mean, std):\n        self.root = root\n        df['ImageId'] = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n        self.fnames = df['ImageId'].unique().tolist()\n        self.num_samples = len(self.fnames)\n        self.transform = Compose(\n            [\n                Normalize(mean=mean, std=std, p=1),\n                ToTensor(),\n            ]\n        )\n\n    def __getitem__(self, idx):\n        fname = self.fnames[idx]\n        path = os.path.join(self.root, fname)\n        image = cv2.imread(path)\n        images = self.transform(image=image)[\"image\"]\n        return fname, images\n\n    def __len__(self):\n        return self.num_samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def post_process(probability, threshold, min_size):\n    '''Post processing of each predicted mask, components with lesser number of pixels\n    than `min_size` are ignored'''\n    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = np.zeros((256, 1600), np.float32)\n    num = 0\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            predictions[p] = 1\n            num += 1\n    return predictions, num\n\nsample_submission_path = '../input/severstal-steel-defect-detection/sample_submission.csv'\ntest_data_folder = \"../input/severstal-steel-defect-detection/test_images\"\n\n# initialize test dataloader\nbest_threshold = 0.5\nnum_workers = 2\nbatch_size = 4\nmin_size = 0 #we will use the 3500 threshold later\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)\ndf = pd.read_csv(sample_submission_path)\ntestset = DataLoader(\n    TestDataset(test_data_folder, df, mean, std),\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True\n)\n\n# Initialize mode and load trained weights\nckpt_path = \"../input/unetstartermodelfile/model.pth\"\ndevice = torch.device(\"cuda\")\nmodel = Unet(\"resnet18\", encoder_weights=None, classes=4, activation=None)\nmodel.to(device)\nmodel.eval()\nstate = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\nmodel.load_state_dict(state[\"state_dict\"])\n\n# start prediction\npredictions = []\nfor i, batch in enumerate(testset):\n    fnames, images = batch\n    batch_preds = torch.sigmoid(model(images.to(device)))\n    batch_preds = batch_preds.detach().cpu().numpy()\n    for fname, preds in zip(fnames, batch_preds):\n        for cls, pred in enumerate(preds):\n            pred, num = post_process(pred, best_threshold, min_size)\n            rle = mask2rle(pred)\n            name = fname + f\"_{cls+1}\"\n            predictions.append([name, rle])\n\n# save predictions to submission.csv\nsub_init = pd.DataFrame(predictions, columns=['ImageId_ClassId', 'EncodedPixels'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_init['ClassId'] = sub_init['ImageId_ClassId'].apply(lambda x: x.split('_')[1]).astype('int8')\n\nDEFECT_TYPES = [1, 2, 3, 4]\n\nfor x in DEFECT_TYPES:\n    sub_init.loc[sub_init.ClassId==x, 'pixel_count'] = sub_init.loc[sub_init.ClassId==x, 'EncodedPixels'].apply(pixel_count) \n\nfor i, x in enumerate(DEFECT_TYPES):\n    try:\n        df = sub_init.loc[sub_init.ClassId==x, :]\n        ax = df['pixel_count'].plot(kind='hist', bins=100, figsize=(15, 6))\n        df['pixel_count'].plot(kind='kde', ax=ax, secondary_y=True)\n        plt.xlim(left=0)\n        plt.axvline(x=3500, linewidth=2, color='r')\n        plt.axvline(x=THRESHOLDS[i], linewidth=2, color='g')\n        plt.title(f'Pixel Distribution for Defect Type {x}')\n        print(f'=====\\nDEFECT TYPE {x}\\n=====')\n        print(df['pixel_count'].describe())\n        plt.show()\n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you're wondering why there is no graph for defect type 2, it's because this model hasn't made any predictions for it! This defect is so rare that getting your models to be confident in identifying it will be a major challenge in this competition. The red line in these graphs are the initial 3500 threshold, while the green lines are our optimised ones. The red line isn't visible for defect type 4 since we kept that threshold the same. In the case of defect types 1 & 3, the new threshold should lead to fewer predictions being discarded."},{"metadata":{"trusted":true},"cell_type":"code","source":"INIT_THRESHOLD = np.full(4, 3500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_1 = sub_init.copy()\nsub_2 = sub_init.copy()\n\nfor i in range(len(THRESHOLDS)):\n    threshold_init = INIT_THRESHOLD[i]\n    threshold_new = THRESHOLDS[i]\n    defect = DEFECT_TYPES[i]\n    \n    sub_1.loc[(sub_1['ClassId']==defect) & (sub_1.pixel_count < threshold_init), 'EncodedPixels'] = ''\n    sub_2.loc[(sub_2['ClassId']==defect) & (sub_2.pixel_count < threshold_new), 'EncodedPixels'] = ''\n    \nsub_1.drop(['ClassId', 'pixel_count'], axis=1, inplace=True)    \nsub_2.drop(['ClassId', 'pixel_count'], axis=1, inplace=True)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"initial_sub_len =  len(sub_1.loc[sub_1['EncodedPixels'] != '', :])\nmodified_sub_len = len(sub_2.loc[sub_2['EncodedPixels'] != '', :])\nprint(f'Positive instance case count with 3500 pixel threshold: {initial_sub_len}')\nprint(f'Positive instance case count with new pixel thresholds: {modified_sub_len}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With our optimised thresholds, there are over 100 more positive mask cases in our final submission! Now the two sets of predictions can be submitted and the results compared. This has to be done separately, and the 3500-threshold submission was scored in a previous version."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#original submission\n#sub_1.to_csv('submission.csv', index=False)\n\n#submission with optimised pixel thresholds\nsub_2.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n---\n* PyTorch Inference kernel with 3500 threshold: 0.88607\n* PyTorch Inference kernel with new thresholds: 0.88817!\n\nOptimising the minimum pixel counts for a positive prediction increased the score by 0.0021! Hopefully this procedure will lead to a quick and easy improvement in your own scores. This can be condensed into a single postprocessing function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pixel_postprocess(sub, thresholds):\n    \n    '''\n    sub: submission dataframe that includes model predictions\n    thresholds: list or array of 4 sequential integers for the minimum number of pixels per defect type \n    '''\n    \n    #define function for creating count of pixels \n    def pixel_count(rle):\n        \n        if np.logical_or(rle==0, rle==''):\n            return(np.nan)\n        array = np.asarray([int(x) for x in rle.split()])\n        lengths = array[1::2]\n    \n        return sum(lengths)\n    \n    \n    #this stage used to simplify subsetting by defect type\n    DEFECT_TYPES = [1, 2, 3, 4]\n    sub['ClassId'] = sub['ImageId_ClassId'].apply(lambda x: x.split('_')[1]).astype('int8')\n    \n    #pixels counts for each positive case\n    for x in DEFECT_TYPES:\n        sub.loc[sub.ClassId==x, 'pixel_count'] = sub.loc[sub.ClassId==x, 'EncodedPixels'].apply(pixel_count) \n    \n    #for each defect type and its associated threshold, remove positive instances with pixel_count < threshold\n    for i in range(len(thresholds)):\n        threshold = thresholds[i]\n        defect = DEFECT_TYPES[i]\n        sub.loc[(sub['ClassId']==defect) & (sub.pixel_count < threshold), 'EncodedPixels'] = ''\n    \n    #remove pixel count and ClassId columns so submission has correct fields\n    sub.drop(['ClassId', 'pixel_count'], axis=1, inplace=True)\n    \n    #don't forget to title it \"submission.csv\" when uploading!\n    return(sub)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good luck in the competition!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}