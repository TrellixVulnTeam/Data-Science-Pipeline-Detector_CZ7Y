{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"You can check [my first post about training TRPO agent with the Reinforcement Learning Replications (rl-replicas)](https://www.kaggle.com/yamatokataoka/train-agent-with-rl-replicas).\n\nThe rl-replicas is an on-going project for implementaing key RL algorithms, VPG, TRPO, PPO. You can find more detail here, [https://github.com/yamatokataoka/reinforcement-learning-replications](https://github.com/yamatokataoka/reinforcement-learning-replications)\n\nIn this notebook, you'll learn how to continue your training with pre-trained model with rl-replicas."},{"metadata":{},"cell_type":"markdown","source":"install gfootball required tools"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"# Install:\n# Kaggle environments.\n!git clone -q https://github.com/Kaggle/kaggle-environments.git\n!cd kaggle-environments && pip install .\n\n# GFootball environment.\n!apt-get update -y -qq > /dev/null\n!apt-get install -y -qq libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -q -b v2.8 https://github.com/google-research/football.git\n!mkdir -p football/third_party/gfootball_engine/lib\n\n!wget -q https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip install .","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"install rl-replicas"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install rl-replicas","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train with rl-replicas"},{"metadata":{},"cell_type":"markdown","source":"you'll set up the TRPO algorithm with `ConjugateGradientOptimizer`.\n\nThe trained model is saved in `./trpo/model.pt`.\n\nYou'll use this to continue your training later."},{"metadata":{},"cell_type":"markdown","source":"##### Note\nit's the same code with [my first post](https://www.kaggle.com/yamatokataoka/train-agent-with-rl-replicas#Train-with-rl-replicas)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimport gfootball\nimport gym\nimport torch\nimport torch.nn as nn\n\nfrom rl_replicas.algorithms import TRPO\nfrom rl_replicas.common.policy import Policy\nfrom rl_replicas.common.value_function import ValueFunction\nfrom rl_replicas.common.optimizers import ConjugateGradientOptimizer\nfrom rl_replicas.common.torch_net import mlp\n\nalgorithm_name = 'trpo'\nenvironment_name = 'GFootball-11_vs_11_kaggle-simple115v2-v0'\nepochs = 5\nsteps_per_epoch = 4000\npolicy_network_architecture = [64, 64]\nvalue_function_network_architecture = [64, 64]\nvalue_function_learning_rate = 1e-3\noutput_dir = './trpo'\n\nenv: gym.Env = gym.make(environment_name)\n\npolicy_network = mlp(\n    sizes = [env.observation_space.shape[0]]+policy_network_architecture+[env.action_space.n]\n)\n\npolicy: Policy = Policy(\n    network = policy_network,\n    optimizer = ConjugateGradientOptimizer(params=policy_network.parameters())\n)\n\nvalue_function_network = mlp(\n    sizes = [env.observation_space.shape[0]]+value_function_network_architecture+[1]\n)\nvalue_function: ValueFunction = ValueFunction(\n    network = value_function_network,\n    optimizer = torch.optim.Adam(value_function_network.parameters(), lr=value_function_learning_rate)\n)\n\nmodel: TRPO = TRPO(policy, value_function, env, seed=0)\n\nprint('an experiment to: {}'.format(output_dir))\n\nprint('algorithm:           {}'.format(algorithm_name))\nprint('epochs:              {}'.format(epochs))\nprint('steps_per_epoch:     {}'.format(steps_per_epoch))\nprint('environment:         {}'.format(environment_name))\n\nprint('value_function_learning_rate: {}'.format(value_function_learning_rate))\nprint('policy network:')\nprint(policy.network)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Start learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.learn(\n    epochs=epochs,\n    steps_per_epoch=steps_per_epoch,\n    output_dir=output_dir,\n    model_saving=True\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Continue your training with rl-replicas\n\n"},{"metadata":{},"cell_type":"markdown","source":"You'll initialize all neccessary components for training again.\n\nThen load pre-trained model, `./trpo/model.pt`.\n\nTo continue your work, you need to load state_dict for policy and value function.\n```\nprevious_model = torch.load(previous_model_location)\n\npolicy.network.load_state_dict(previous_model['policy_state_dict'])\npolicy.optimizer.load_state_dict(previous_model['policy_optimizer_state_dict'])\nvalue_function.network.load_state_dict(previous_model['value_fn_state_dict'])\nvalue_function.optimizer.load_state_dict(previous_model['value_fn_optimizer_state_dict'])\n```"},{"metadata":{},"cell_type":"markdown","source":"##### Note\nimplementation for saving `model.pt`is \n\nhttps://github.com/yamatokataoka/reinforcement-learning-replications/blob/master/rl_replicas/common/base_algorithms/on_policy_algorithm.py#L106-L114"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimport gfootball\nimport gym\nimport torch\nimport torch.nn as nn\n\nfrom rl_replicas.algorithms import TRPO\nfrom rl_replicas.common.policy import Policy\nfrom rl_replicas.common.value_function import ValueFunction\nfrom rl_replicas.common.optimizers import ConjugateGradientOptimizer\nfrom rl_replicas.common.torch_net import mlp\n\nalgorithm_name = 'trpo'\nenvironment_name = 'GFootball-11_vs_11_kaggle-simple115v2-v0'\nepochs = 5\nsteps_per_epoch = 4000\npolicy_network_architecture = [64, 64]\nvalue_function_network_architecture = [64, 64]\noutput_dir = './trpo'\n\nprevious_model_location = os.path.join(output_dir, 'model.pt')\n\nenv: gym.Env = gym.make(environment_name)\n\npolicy_network = mlp(\n    sizes = [env.observation_space.shape[0]]+policy_network_architecture+[env.action_space.n]\n)\n\npolicy: Policy = Policy(\n    network = policy_network,\n    optimizer = ConjugateGradientOptimizer(params=policy_network.parameters())\n)\n\nvalue_function_network = mlp(\n    sizes = [env.observation_space.shape[0]]+value_function_network_architecture+[1]\n)\nvalue_function: ValueFunction = ValueFunction(\n    network = value_function_network,\n    optimizer = torch.optim.Adam(value_function_network.parameters())\n)\n    \nif os.path.isfile(previous_model_location):\n    print('Load previous model: {}'.format(previous_model_location))\n    previous_model = torch.load(previous_model_location)\n\n    policy.network.load_state_dict(previous_model['policy_state_dict'])\n    policy.optimizer.load_state_dict(previous_model['policy_optimizer_state_dict'])\n    value_function.network.load_state_dict(previous_model['value_fn_state_dict'])\n    value_function.optimizer.load_state_dict(previous_model['value_fn_optimizer_state_dict'])\n\nmodel: TRPO = TRPO(policy, value_function, env, seed=0)\n\nprint('an experiment to: {}'.format(output_dir))\n\nprint('algorithm:           {}'.format(algorithm_name))\nprint('epochs:              {}'.format(epochs))\nprint('steps_per_epoch:     {}'.format(steps_per_epoch))\nprint('environment:         {}'.format(environment_name))\n\nprint('policy network:')\nprint(policy.network)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Continue your training"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Re-start your training from {} epochs'.format(previous_model['epoch']))\nmodel.learn(\n    epochs=epochs,\n    steps_per_epoch=steps_per_epoch,\n    output_dir=output_dir,\n    model_saving=True\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}