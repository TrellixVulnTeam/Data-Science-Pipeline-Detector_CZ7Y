{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs(\"/kaggle/kaggle_simulations/agent/saved_model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.path.exists(\"/kaggle/kaggle_simulations/agent/saved_model\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# GFootball environment.\n!pip install kaggle_environments\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n!git clone -b v2.3 https://github.com/google-research/football.git\n!mkdir -p football/third_party/gfootball_engine/lib\n!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gym\nimport gfootball\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smm_env = gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")\nsmm_state = smm_env.reset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gym.core import ObservationWrapper\nfrom gym.spaces import Box\n\nimport cv2\n\n\nclass PreprocessGFootball(ObservationWrapper):\n    def __init__(self, env):\n        \"\"\"A gym wrapper that crops, scales image into the desired shapes and optionally grayscales it.\"\"\"\n        ObservationWrapper.__init__(self, env)\n\n        self.img_size = (72, 96, 4)\n        self.observation_space = Box(0.0, 1.0, self.img_size)\n\n    def observation(self, img):\n        \"\"\"what happens to each observation\"\"\"\n        #  * Convert image pixels to (0, 1) range, float32 type.\n        #img = cv2.resize(img, (self.img_size[0], self.img_size[1]))#resizing\n        img = (img/255).astype(np.float32)\n        return np.array(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gym\n# spawn game instance for tests\nenv = gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")  # create raw env\nenv = PreprocessGFootball(env)\n\nobservation_shape = env.observation_space.shape\nn_actions = env.action_space.n\nobs = env.reset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom gym.spaces.box import Box\nfrom gym.core import Wrapper\n\n\nclass FrameBuffer(Wrapper):\n    def __init__(self, env=None, n_frames=4):\n        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n        super(FrameBuffer, self).__init__(env)\n        height, width, n_channels = env.observation_space.shape\n        obs_shape = [height, width, n_channels * n_frames]\n        self.observation_space = Box(0.0, 1.0, obs_shape)\n        self.framebuffer = np.zeros(obs_shape, 'float32')\n\n    def reset(self):\n        \"\"\"resets breakout, returns initial frames\"\"\"\n        self.framebuffer = np.zeros_like(self.framebuffer)\n        self.update_buffer(self.env.reset())\n        return self.framebuffer\n\n    def step(self, action):\n        \"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\n        new_img, reward, done, info = self.env.step(action)\n        self.update_buffer(new_img)\n        return self.framebuffer, reward, done, info\n\n    def update_buffer(self, img):\n        offset = self.env.observation_space.shape[-1]\n        axis = -1\n        cropped_framebuffer = self.framebuffer[:, :, :-offset]\n        self.framebuffer = np.concatenate(\n            [img, cropped_framebuffer], axis=axis)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_env():\n    env = gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")\n    env = PreprocessGFootball(env)\n    env = FrameBuffer(env, n_frames=4)\n    return env","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make_env()\nenv.reset()\nn_actions = env.action_space.n\nstate_dim = env.observation_space.shape\nprint(f\"n_actions : {n_actions}\\nstate_dim : {state_dim}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Conv2D, Dense, Flatten,InputLayer\nfrom keras.models import Sequential, Model\n\nclass Conv_DQN_Model(Model):\n    def __init__(self, state_shape, num_actions):\n        super(Conv_DQN_Model, self).__init__()\n        self.input_layer = InputLayer(input_shape = state_shape,)\n        self.conv1 = Conv2D(filters = 16, kernel_size = (3,3), strides=(2), activation='relu')\n        self.conv2 = Conv2D(filters = 32, kernel_size = (3,3), strides=(2), activation='relu')\n        self.conv3 = Conv2D(filters = 64, kernel_size = (3,3), strides=(2), activation='relu')\n        self.flat = Flatten()\n        self.dense1 = Dense(256,activation='relu')\n        self.output_layer = Dense(n_actions, activation='linear')\n\n    #@tf.function\n    def call(self, inputs):\n        t = self.input_layer(inputs)\n        t = tf.expand_dims(t, axis=0)\n        t = self.conv1(t)\n        t = self.conv2(t)\n        t = self.conv3(t)\n        t = self.flat(t)\n        t = self.dense1(t)\n        output = self.output_layer(t)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DQNAgent:\n    def __init__(self, name, state_shape, n_actions, epsilon=0, reuse=False):\n        \"\"\"A simple DQN agent\"\"\"\n        self.agent = Conv_DQN_Model(state_shape, n_actions)\n        \n        self.epsilon = epsilon\n        \n    def save_agent(self,path) :\n        self.agent.save(path)\n\n    def weights(self) :\n        return self.agent.get_weights()\n    \n    def load_weights(self, _agent) :\n        self.agent.set_weights(_agent.weights())\n    \n    def get_symbolic_qvalues(self, state_t):\n        \"\"\"takes agent's observation, returns qvalues. Both are tf Tensors\"\"\"\n        #<YOUR CODE: apply your network layers here>\n        qvalues = self.agent(state_t)\n\n        assert tf.debugging.is_numeric_tensor(qvalues) and qvalues.shape.ndims == 2, \\\n            \"please return 2d tf tensor of qvalues [you got %s]\" % repr(qvalues)\n        assert int(qvalues.shape[1]) == n_actions\n        return qvalues\n\n    def get_qvalues(self, state_t):\n        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n        qvalues = self.get_symbolic_qvalues(state_t)\n        qvalues = qvalues.numpy()\n        return qvalues\n\n    def sample_actions(self, qvalues):\n        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n        epsilon = self.epsilon\n        batch_size, n_actions = qvalues.shape\n        random_actions = np.random.choice(n_actions, size=batch_size)\n        best_actions = qvalues.argmax(axis=-1)\n        should_explore = np.random.choice([0, 1], batch_size, p=[1-epsilon, epsilon])\n        return np.where(should_explore, random_actions, best_actions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agent = DQNAgent(\"dqn_agent\", state_dim, n_actions, epsilon=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n    rewards = []\n    for _ in range(n_games):\n        s = env.reset()\n        reward = 0\n        for _ in range(t_max):\n            qvalues = agent.get_qvalues(s)\n            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n            s, r, done, _ = env.step(action)\n            reward += r\n            if done:\n                break\n\n        rewards.append(reward)\n    return np.mean(rewards)\nevaluate(env, agent, n_games=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nclass ReplayBuffer(object):\n    def __init__(self, size):\n        \"\"\"Create Replay buffer.\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        \"\"\"\n        self._storage = []\n        self._maxsize = size\n        self._next_idx = 0\n\n    def __len__(self):\n        return len(self._storage)\n\n    def add(self, obs_t, action, reward, obs_tp1, done):\n        data = (obs_t, action, reward, obs_tp1, done)\n\n        if self._next_idx >= len(self._storage):\n            self._storage.append(data)\n        else:\n            self._storage[self._next_idx] = data\n        self._next_idx = (self._next_idx + 1) % self._maxsize\n\n    def _encode_sample(self, idxes):\n        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n        for i in idxes:\n            data = self._storage[i]\n            obs_t, action, reward, obs_tp1, done = data\n            obses_t.append(np.array(obs_t, copy=False))\n            actions.append(np.array(action, copy=False))\n            rewards.append(reward)\n            obses_tp1.append(np.array(obs_tp1, copy=False))\n            dones.append(done)\n        return (\n            np.array(obses_t),\n            np.array(actions),\n            np.array(rewards),\n            np.array(obses_tp1),\n            np.array(dones)\n        )\n\n    def sample(self, batch_size):\n        \"\"\"Sample a batch of experiences.\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        \"\"\"\n        idxes = [\n            random.randint(0, len(self._storage) - 1)\n            for _ in range(batch_size)\n        ]\n        return self._encode_sample(idxes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def play_and_record(agent, env, exp_replay, n_steps=1):\n    \"\"\"\n    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer. \n    Whenever game ends, add record with done=True and reset the game.\n    It is guaranteed that env has done=False when passed to this function.\n\n    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n\n    :returns: return sum of rewards over time\n    \"\"\"\n    # initial state\n    s = env.reset()\n    reward = 0\n    # Play the game for n_steps as per instructions above\n    for step in range(n_steps) :\n        qvalues = agent.get_qvalues([s])\n        action = agent.sample_actions(qvalues)[0]\n          #data = [s,action]\n        _s, _r, _done, _ = env.step(action)\n        exp_replay.add(s,action,_r,_s,_done)\n        reward += _r\n        if _done:\n            s = env.reset()\n    return reward","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing your code. This may take a minute...\nexp_replay = ReplayBuffer(10**3)\nreward = play_and_record(agent, env, exp_replay, n_steps=10**3)\nprint(reward)\n# if you're using your own experience replay buffer, some of those tests may need correction.\n# just make sure you know what your code does","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_network = DQNAgent(\"target_network\", state_dim, n_actions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_weigths_into_target_network(agent, target_network):\n    \"\"\" assign target_network.weights variables to their respective agent.weights values. \"\"\"\n    target_network.load_weights(agent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# placeholders that will be fed with exp_replay.sample(batch_size)\nobs_ph = tf.Variable(tf.random.uniform(state_dim,minval=0,maxval=1,dtype=tf.dtypes.float32),trainable = True)\n#tf.compat.v1.placeholder(tf.float32, shape=(None,) + state_dim)\nactions_ph = tf.Variable(1,trainable=True,dtype=tf.int32)\nrewards_ph = tf.Variable(1,trainable=True,dtype=tf.float32)\nnext_obs_ph = tf.Variable(tf.random.uniform(state_dim,minval=0,maxval=1,dtype=tf.dtypes.float32),trainable = True)\nis_done_ph = tf.Variable(0.,dtype=tf.float32)\n\nis_not_done = 1 - is_done_ph\ngamma = 0.99","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_step(td=False):\n    with tf.GradientTape() as tape:\n        current_qvalues = agent.get_symbolic_qvalues(obs_ph)\n        current_action_qvalues = tf.reduce_sum(tf.one_hot(actions_ph, n_actions) * current_qvalues, axis=1)\n        next_qvalues_target = target_network.get_symbolic_qvalues(next_obs_ph)\n        next_state_values_target = tf.reduce_max(next_qvalues_target,axis=1)\n        reference_qvalues = rewards_ph + gamma * next_state_values_target * is_not_done\n        td_loss = tf.square(current_action_qvalues - reference_qvalues)\n        td_loss = tf.reduce_mean(td_loss)\n        if td : return td_loss\n        #print(\"td_loss\",td_loss)\n    variables = self.model.trainable_variables\n    gradients = tape.gradient(td_loss, variables)\n    self.optimizer.apply_gradients(zip(gradients, variables))\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import trange\nimport pandas as pd\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef moving_average(x, span=100, **kw):\n    return pd.DataFrame({'x': np.asarray(x)}).x.ewm(span=span,ignore_na=True, **kw).mean().values\n\nmean_rw_history = []\ntd_loss_history = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_replay = ReplayBuffer(2500)\nplay_and_record(agent, env, exp_replay, n_steps=10000)\n\n\ndef sample_batch(exp_replay, batch_size):\n    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(batch_size)\n    return {\n        obs_ph: obs_batch,\n        actions_ph: act_batch,\n        rewards_ph: reward_batch,\n        next_obs_ph: next_obs_batch,\n        is_done_ph: is_done_batch,\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tf.compat.v1.disable_tensor_equality()\nfor i in trange(30000):\n    # play\n    play_and_record(agent, env, exp_replay, 10)\n    # train\n    loss_t = train_step(True)\n    td_loss_history.append(loss_t.numpy())\n\n    # adjust agent parameters\n    if i % 500 == 0:\n        # You could think that loading weights onto a target network is simply\n        #     load_weigths_into_target_network(agent, target_network)\n        # but actually calling this function repeatedly creates a TF copy operator\n        # again and again, which bloats memory consumption with each training step.\n        # Instead, you should create 'copy_step' once.\n        #sess.run(copy_step)\n        load_weigths_into_target_network(agent, target_network)\n        agent.epsilon = max(agent.epsilon * 0.99, 0.01)\n        mean_rw_history.append(evaluate(make_env(), agent, n_games=3))\n\n    if i % 100 == 0:\n        clear_output(True)\n        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n        #if len(exp_replay) > 30000 : break\n\n        plt.subplot(1, 2, 1)\n        plt.title(\"mean reward per game\")\n        plt.plot(mean_rw_history)\n        plt.grid()\n\n        assert not np.isnan(False)\n        plt.figure(figsize=[12, 4])\n        plt.subplot(1, 2, 2)\n        plt.title(\"TD loss history (moving average)\")\n        plt.plot(moving_average(np.array(td_loss_history), span=100, min_periods=100))\n        plt.grid()\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs(\"/kaggle_simulations/agent/saved_model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle_simulations/agent\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agent.save_agent(\"/kaggle_simulations/agent/saved_model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile /kaggle_simulations/agent/main.py\n\nimport gym\nimport gfootball\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom gym.core import ObservationWrapper\nfrom gym.spaces import Box\nimport cv2\n\nfrom gym.spaces.box import Box\nfrom gym.core import Wrapper\n\n\nfrom gfootball.env import observation_preprocessing\nfrom tensorflow import keras\n\n\nclass FrameBuffer(Wrapper):\n    def __init__(self, env=gym.Env, n_frames=4):\n        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n        super(FrameBuffer, self).__init__(env)\n        height, width, n_channels = [72, 96, 4]\n        obs_shape = [height, width, n_channels * n_frames]\n        self.observation_space = Box(0.0, 1.0, obs_shape)\n        self.framebuffer = np.zeros(obs_shape, 'float32')\n\n    def reset(self):\n        \"\"\"resets breakout, returns initial frames\"\"\"\n        self.framebuffer = np.zeros_like(self.framebuffer)\n        self.update_buffer(self.env.reset())\n        return self.framebuffer\n\n    def step(self, action):\n        \"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\n        new_img, reward, done, info = self.env.step(action)\n        self.update_buffer(new_img)\n        return self.framebuffer, reward, done, info\n\n    def update_buffer(self, img):\n        offset = 4\n        axis = -1\n        cropped_framebuffer = self.framebuffer[:, :, :-offset]\n        self.framebuffer = np.concatenate(\n            [img, cropped_framebuffer], axis=axis)\n        return self.framebuffer\n\n\nfrom keras.layers import Conv2D, Dense, Flatten,InputLayer\nfrom keras.models import Sequential, Model\n\nclass Conv_DQN_Model(Model):\n    def __init__(self, state_shape=(72,96,16), num_actions=19):\n        super(Conv_DQN_Model, self).__init__()\n        self.input_layer = InputLayer(input_shape = state_shape,batch_size=1)\n        self.conv1 = Conv2D(filters = 16, kernel_size = (3,3), strides=(2), activation='relu')\n        self.conv2 = Conv2D(filters = 32, kernel_size = (3,3), strides=(2), activation='relu')\n        self.conv3 = Conv2D(filters = 64, kernel_size = (3,3), strides=(2), activation='relu')\n        self.flat = Flatten()\n        self.dense1 = Dense(256,activation='relu')\n        self.output_layer = Dense(n_actions, activation='linear')\n\n    #@tf.function\n    def call(self, inputs):\n        t = self.input_layer(inputs)\n        t = tf.expand_dims(t, axis=0)\n        t = self.conv1(t)\n        t = self.conv2(t)\n        t = self.conv3(t)\n        t = self.flat(t)\n        t = self.dense1(t)\n        output = self.output_layer(t)\n        return output\n    \n\nclass DQNAgent:\n    def __init__(self, name=\"agent\", state_shape=(72,96,16), n_actions=19, epsilon=0, reuse=False):\n        \"\"\"A simple DQN agent\"\"\"\n        self.agent = Conv_DQN_Model(state_shape, n_actions)\n        \n        self.epsilon = epsilon\n\n    def weights(self) :\n        return self.agent.get_weights()\n    \n    def load_weights(self, _agent) :\n        self.agent.set_weights(_agent.weights())\n    \n    def get_symbolic_qvalues(self, state_t):\n        \"\"\"takes agent's observation, returns qvalues. Both are tf Tensors\"\"\"\n        #<YOUR CODE: apply your network layers here>\n        qvalues = self.agent(state_t)\n\n        assert tf.debugging.is_numeric_tensor(qvalues) and qvalues.shape.ndims == 2, \\\n            \"please return 2d tf tensor of qvalues [you got %s]\" % repr(qvalues)\n        assert int(qvalues.shape[1]) == n_actions\n        return qvalues\n\n    def get_qvalues(self, state_t):\n        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n        qvalues = self.get_symbolic_qvalues(state_t)\n        qvalues = qvalues.numpy()\n        return qvalues\n\n    def sample_actions(self, qvalues):\n        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n        epsilon = self.epsilon\n        batch_size, n_actions = qvalues.shape\n        random_actions = np.random.choice(n_actions, size=batch_size)\n        best_actions = qvalues.argmax(axis=-1)\n        should_explore = np.random.choice([0, 1], batch_size, p=[1-epsilon, epsilon])\n        return np.where(should_explore, random_actions, best_actions)\n  \n            \ndqn_agent = keras.models.load_model(\"/kaggle_simulations/agent/saved_model\")\nframe_buffer = FrameBuffer() \ndef agent(obs):\n    global dqn_agent\n    global frame_buffer\n    obs = obs['players_raw'][0]\n    obs = observation_preprocessing.generate_smm([obs])[0]\n    obs = (obs/255).astype(np.float32)\n    obs = frame_buffer.update_buffer(obs)\n    #print(obs.shape)    \n    qvalues = dqn_agent(obs).numpy()\n    action = np.argmax(qvalues)\n    \n    return [int(action)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Tuple, Dict, List, Any\n\nfrom kaggle_environments import make\n\nenv = make(\"football\", debug=True,configuration={\"save_video\": True,\n                                                 \"scenario_name\": \"11_vs_11_kaggle\"})\n\n# Define players\nleft_player = \"/kaggle_simulations/agent/main.py\"  # A custom agent, eg. random_agent.py or example_agent.py\nright_player = \"/kaggle_simulations/agent/main.py\"  # eg. A built in 'AI' agent or the agent again\n\n\noutput: List[Tuple[Dict[str, Any], Dict[str, Any]]] = env.run([left_player, right_player])\n\nprint(f\"Final score: {sum([r['reward'] for r in output[0]])} : {sum([r['reward'] for r in output[1]])}\")\nenv.render(mode=\"human\", width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare a submision package containing trained model and the main execution logic.\n!cd /kaggle_simulations/agent && tar -czvf /kaggle/working/submit.tar.gz main.py saved_model","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}