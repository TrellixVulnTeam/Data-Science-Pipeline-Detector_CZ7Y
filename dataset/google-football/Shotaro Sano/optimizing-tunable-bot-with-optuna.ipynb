{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Optimizing Tunable Bot with Optuna\n\nFirst of all, this notebook is based on the following ones.\n- https://www.kaggle.com/eugenkeil/simple-baseline-bot \n- https://www.kaggle.com/david1013/tunable-baseline-bot\n\nUsing [Optuna](https://github.com/optuna/optuna), this notebook demonstrates:\n- automatically optimizing the parameters of the [tunable agent](https://www.kaggle.com/david1013/tunable-baseline-bot),\n- analyzing the search space of the optimization with visualization features,\n- refining the optimization setup based on the visualized results.\n\nAlthough [Optuna](https://github.com/optuna/optuna) is an optimization tool for hyperparameters of ML models, it is applicable to any format of blackbox optimization including the tuning of rule-based agents.\n\nI hope this notebook releases you from the repetitive labor of manual tuning."},{"metadata":{},"cell_type":"markdown","source":"# Install"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"# Install:\n# Kaggle environments.\n!git clone --quiet https://github.com/Kaggle/kaggle-environments.git\n!cd kaggle-environments && pip install -q .\n\n# GFootball environment.\n!apt-get -qq update -y\n!apt-get -qq install -y libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone --quiet -b v2.3 https://github.com/google-research/football.git\n!mkdir -p football/third_party/gfootball_engine/lib\n\n!wget -q https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install -q .\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining Tunable Agent\nThe following script is mostly copied from the [original notebook](https://www.kaggle.com/david1013/tunable-baseline-bot).\nNote that the parameters are passed to the agenet via environment variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile submission.py\n\nfrom math import sqrt\nimport os\n\nfrom kaggle_environments.envs.football.helpers import *\n\nSPRINT_RANGE = float(os.environ[\"SPRINT_RANGE\"])\nSHOT_RANGE_X = float(os.environ[\"SHOT_RANGE_X\"])\nSHOT_RANGE_Y = float(os.environ[\"SHOT_RANGE_Y\"])\nGOALIE_OUT = float(os.environ[\"GOALIE_OUT\"])\nLONG_SHOT_X = float(os.environ[\"LONG_SHOT_X\"])\nLONG_SHOT_Y = float(os.environ[\"LONG_SHOT_Y\"])\n\ndirections = [\n    [Action.TopLeft, Action.Top, Action.TopRight],\n    [Action.Left, Action.Idle, Action.Right],\n    [Action.BottomLeft, Action.Bottom, Action.BottomRight]]\n\ndirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)\n\nenemyGoal = [1, 0]\nGOALKEEPER = 0\n\nshot_range = [[SHOT_RANGE_X, 1], \n              [-SHOT_RANGE_Y, SHOT_RANGE_Y]]\n\ndef inside(pos, area):\n    return area[0][0] <= pos[0] <= area[0][1] and area[1][0] <= pos[1] <= area[1][1]\n\n@human_readable_agent\ndef agent(obs):\n    controlled_player_pos = obs['left_team'][obs['active']]\n    \n    if obs[\"game_mode\"] == GameMode.Penalty:\n        return Action.Shot\n    if obs[\"game_mode\"] == GameMode.Corner:\n        if controlled_player_pos[0] > 0:\n            return Action.Shot\n    if obs[\"game_mode\"] == GameMode.FreeKick:\n        return Action.Shot\n    \n    # Make sure player is running down the field.\n    if  0 < controlled_player_pos[0] < SPRINT_RANGE and Action.Sprint not in obs['sticky_actions']:\n        return Action.Sprint\n    elif SPRINT_RANGE < controlled_player_pos[0] and Action.Sprint in obs['sticky_actions']:\n        return Action.ReleaseSprint\n\n    # If our player controls the ball:\n    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:\n        \n        if inside(controlled_player_pos, shot_range) and controlled_player_pos[0] < obs['ball'][0]:\n            return Action.Shot\n        \n        elif ( abs(obs['right_team'][GOALKEEPER][0] - 1) > GOALIE_OUT   \n                and controlled_player_pos[0] > LONG_SHOT_X and abs(controlled_player_pos[1]) < LONG_SHOT_Y ):\n            return Action.Shot\n        \n        else:\n            xdir = dirsign(enemyGoal[0] - controlled_player_pos[0])\n            ydir = dirsign(enemyGoal[1] - controlled_player_pos[1])\n            return directions[ydir][xdir]\n        \n    # if we we do not have the ball:\n    else:\n        # Run towards the ball.\n        xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])\n        ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])\n        return directions[ydir][xdir]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Running Optuna\nTo run Optuna, you need the following steps:\n\n- define the objective function whose input is a `trial` object,\n- inside the objective function, get parameters to be tried with `suggest` methods, run games with the suggested parameters, and return the obtained reward,\n- invoke `optuna.create_study()` and `study.optimize()` passing the objective function and the number trials (`n_trials`).\n\nThen, Optuna repeatedly runs the objective function `n_trials` times, changing the suggested parameters so that the reward is improved with Bayesian optimization."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nfrom kaggle_environments import make\nimport numpy as np\nimport optuna\n\n# Optuna searches parameters that maximized the returned value from this objective function.\ndef objective(trial):\n    # You can get Optuna's parameter suggestion with the `suggest_float` method.\n    os.environ[\"SPRINT_RANGE\"] = str(trial.suggest_float(\"SPRINT_RANGE\", 0.0, 1.0))\n    os.environ[\"SHOT_RANGE_X\"] = str(trial.suggest_float(\"SHOT_RANGE_X\", 0.0, 1.0))  \n    os.environ[\"SHOT_RANGE_Y\"] = str(trial.suggest_float(\"SHOT_RANGE_Y\", 0.0, 1.0))\n    os.environ[\"GOALIE_OUT\"] = str(trial.suggest_float(\"GOALIE_OUT\", 0.0, 1.0))\n    os.environ[\"LONG_SHOT_X\"] = str(trial.suggest_float(\"LONG_SHOT_X\", 0.0, 1.0))\n    os.environ[\"LONG_SHOT_Y\"] = str(trial.suggest_float(\"LONG_SHOT_Y\", 0.0, 1.0))\n\n    # To reduce the noise in reward, let's run the game 5 times for each trial.\n    rewards = []\n    for _ in range(5):\n        env = make(\"football\", configuration={\"scenario_name\": \"11_vs_11_kaggle\"})\n        result = env.run([\"submission.py\", \"do_nothing\"])\n        rewards.append(result[-1][0][\"reward\"])\n\n    return np.mean(rewards)\n\n# You can run the optimization just passing the objective function and the number of trials to Optuna.\n# Here, Optuna repeats to run the objective function for 35 times.\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=35, show_progress_bar=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Best Setting\nAfter running all trials, you can see the best set of parameters and its reward as follows."},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optimization History\nYou can see the optimization history with the visualization feature of Optuna."},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The pandas export feature, `study.trials_dataframe()`, is also helpful to analyze the optimization history."},{"metadata":{"trusted":true},"cell_type":"code","source":"study.trials_dataframe().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Search Space Visualization\nThe visualization functions help you understand the characteristics of the parameters and the search space."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The importance plot visualizes which parameters has been dominant in the optimization.\noptuna.visualization.plot_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The slice plot shows the objective values along each parameter.\n# Here, let's focus on the most dominant parameter.\noptuna.visualization.plot_slice(study, params=[\"SHOT_RANGE_X\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The parallel coordinate plot shows the relationship among multiple parameters and the objective function.\noptuna.visualization.plot_parallel_coordinate(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Refining The Search Space\nYou can just submit with the best parameters found above, but let's try one more step.\n\nWhen the search space is too large, automatic parameter search requres a lot of trials to converge. Narrowing down the search space with human insights sometimes makes the optimization more efficient.\n\nTaking a close look into the plots above, there're some clues to narrow down the search space:\n- `SHOT_RANGE_X` is the most dominant parameter according to the importance plot;\n- `SHOT_RANGE_X` works better when it is more than `0.5`;\n- ...\n\nLet's refine the search space narrowing down the range of each parameter. (I came up with the following ranges with a bit more trials I locally ran.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    os.environ[\"SPRINT_RANGE\"] = str(trial.suggest_float(\"SPRINT_RANGE\", 0.25, 0.9))\n    os.environ[\"SHOT_RANGE_X\"] = str(trial.suggest_float(\"SHOT_RANGE_X\", 0.5, 1.0))  \n    os.environ[\"SHOT_RANGE_Y\"] = str(trial.suggest_float(\"SHOT_RANGE_Y\", 0.0, 1.0))\n    os.environ[\"GOALIE_OUT\"] = str(trial.suggest_float(\"GOALIE_OUT\", 0.0, 0.4))\n    os.environ[\"LONG_SHOT_X\"] = str(trial.suggest_float(\"LONG_SHOT_X\", 0.25, 0.75))\n    os.environ[\"LONG_SHOT_Y\"] = str(trial.suggest_float(\"LONG_SHOT_Y\", 0.5, 1.0))\n    \n    rewards = []\n    for _ in range(5):\n        env = make(\"football\", configuration={\"scenario_name\": \"11_vs_11_kaggle\"})\n        result = env.run([\"submission.py\", \"do_nothing\"])\n        rewards.append(result[-1][0][\"reward\"])\n\n    return np.mean(rewards)\n\n# You can reuse the study object to run additional 15 trials.\nstudy.optimize(objective, n_trials=15, show_progress_bar=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot the history with the additional trials. Though the results will be affected by random factors, the tuner will tend to suggest better results with the refined search space."},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tips\nFinally, you can pickle the study object as follows so that you continue the analysis of the optimization result or resume the study in your local environment. Please make sure that the library version of Optuna is consistent b/w the Kaggle notebook and your local environment."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nwith open(\"study.pkl\", \"wb\") as fw: \n    pickle.dump(study, fw)\n\n# You can load as:\n# study = pickle.load(open(\"study.pkl\", \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below are some other tips:\n- Outside the Kaggle notebook, you can try the [parallelized optimization](https://optuna.readthedocs.io/en/v2.2.0/tutorial/004_distributed.html) of Optuna, which makes the optimization much faster.\n- Since the total number of trials, 50,  is due to the limitation of notebook runtime, increasing the number may improve the performance giving the tuner more resolution.\n- The noise in the reward affects the best performance especially in a later stage of the optimization. Increasing the number of games in a trial may mitigate the problem.\n- On the other hand, the performance gain with parameter tuning would be just limited after a few hundreds of trials. You might need to improve the base rule or introduce RL learners to go further."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}