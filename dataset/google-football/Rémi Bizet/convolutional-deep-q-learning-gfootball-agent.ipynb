{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook is inspired from [@garethjns ideas & work](https://www.kaggle.com/garethjns/convolutional-deep-q-learner) and [Alexander Van de Kleut's GitHub](https://alexandervandekleut.github.io/) (especially [this article](https://alexandervandekleut.github.io/deep-q-learning/))"},{"metadata":{},"cell_type":"markdown","source":"We first install the required components"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Install:\n# Kaggle environments.\n!git clone https://github.com/Kaggle/kaggle-environments.git\n!cd kaggle-environments && pip install .\n\n# GFootball environment.\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -b v2.7 https://github.com/google-research/football.git\n!mkdir -p football/third_party/gfootball_engine/lib\n\n!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.7.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install . \n\n# Install Gym\n!pip install gym\n\nfrom IPython.display import clear_output\nclear_output()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below are imported the useful packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gfootball\nimport gym\nimport numpy as np\nimport tensorflow as tf\nimport random\nfrom collections import deque","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Q-Learning associates the \"goodness\" of picking an action a_t in a state s_t, while following a certain policy. That 's what we call the Q-Value. The policy is then created by selecting for each state the action maximizing the Q_value. [See here](https://alexandervandekleut.github.io/mdp/) and [here](https://alexandervandekleut.github.io/q-learning-numpy/)"},{"metadata":{},"cell_type":"markdown","source":"We create a ReplayBuffer to remember state transitions. It helps the agent to perform better on old transitions. We sample them randomly to get a nice balance between ancient and new experiences."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ReplayBuffer:\n    def __init__(self, size=1000000):\n        self.memory = deque(maxlen=size)\n        \n    def remember(self, s_t, a_t, r_t, s_t_next, d_t):\n        self.memory.append((s_t, a_t, r_t, s_t_next, d_t))\n        \n    def sample(self,batch_size):\n        batch_size = min(batch_size, len(self.memory))\n        return random.sample(self.memory,batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our Agent will use the SMM representation of the Gfootball environment\n\nFrom [Gfootball GitHub](https://github.com/google-research/football/blob/master/gfootball/doc/observation.md)\nSimplified spacial (minimap) representation of a game state. It consists of several 72 * 96 planes of bytes, filled in with 0s except for:\n\n1st plane: 255s represent positions of players on the left team\n\n2nd plane: 255s represent positions of players on the right team\n\n3rd plane: 255s represent position of a ball\n\n4th plane: 255s represent position of an active player\n\nAs [@garethjns](https://www.kaggle.com/garethjns/convolutional-deep-q-learner) mentionned, Conv2D Layers will have more efficiency by treating only one plane. Thereby we need to create a SplitLayer"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SplitLayer(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n\n    def call(self, inputs):\n        split0,split1,split2,split3 = tf.split(inputs,4,3)\n        return [split0,split1,split2,split3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To determinate Q values we use the neural network described below. Each output of the SplitLayer is going through a sequence of layers built by the build_conv_branch function. The results of each branch is concatened with the others one. After a couple of Dense layers the action chosen is given by the action_output layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvNN:\n\n    def __init__(self,state_shape,num_actions):\n        self.state_shape = state_shape\n        self.num_actions = num_actions\n\n    def _model_architecture(self):\n    \n        frames_input = tf.keras.layers.Input(shape=self.state_shape,name='input',batch_size=1)\n        frames_split = SplitLayer()(frames_input)\n        conv_branches = []\n        for f,frame in enumerate(frames_split):\n            conv_branches.append(self._build_conv_branch(frame,f))\n\n        concat = tf.keras.layers.concatenate(conv_branches)\n        \n        fc0 = tf.keras.layers.Dense(units=8192, name='fc0', \n                                     activation='relu')(concat)\n        fc1 = tf.keras.layers.Dense(units=2048, name='fc1', \n                                     activation='relu')(fc0)\n        fc2 = tf.keras.layers.Dense(units=512, name='fc2', \n                                     activation='relu')(fc1)\n        fc3 = tf.keras.layers.Dense(units=256, name='fc3', \n                                     activation='relu')(fc2)\n        fc4 = tf.keras.layers.Dense(units=64, name='fc4', \n                                     activation='relu')(fc3)\n        \n        action_output = tf.keras.layers.Dense(units=self.num_actions, name='output',\n                                               activation='relu')(fc4)\n        return frames_input,action_output\n        \n    @staticmethod\n    def _build_conv_branch(frame,number):\n        conv1 = tf.keras.layers.Conv2D(16, kernel_size=(8, 8), strides=(4, 4),\n                                        name='conv1_frame'+str(number), padding='same',\n                                        activation='relu')(frame)\n        mp1 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp1_frame\"+str(number))(conv1)\n        conv2 = tf.keras.layers.Conv2D(24, kernel_size=(4, 4), strides=(2, 2),\n                                        name='conv2_frame'+str(number),padding='same',\n                                        activation='relu')(mp1)\n        mp2 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp2_frame\"+str(number))(conv2)\n        conv3 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n                                        name='conv3_frame'+str(number),padding='same',\n                                        activation='relu')(mp2)\n        mp3 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp3_frame\"+str(number))(conv3)\n        conv4 = tf.keras.layers.Conv2D(64, kernel_size=(2, 2), strides=(1, 1),\n                                        name='conv4_frame'+str(number), padding='same',\n                                        activation='relu')(mp3)\n        mp4 = tf.keras.layers.MaxPooling2D(pool_size=1, name=\"mp4_frame\"+str(number))(conv4)\n\n        flatten = tf.keras.layers.Flatten(name='flatten'+str(number))(mp4)\n\n        return flatten\n        \n    def build(self):\n        frames_input,action_output = self._model_architecture()\n        model = tf.keras.Model(inputs=[frames_input], outputs=[action_output])\n        return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The agent starts by exploring the environment, picking random actions. The first Q-values are then determinated. As the epsilon is decaying, it get more interested in using his neural network to estimate the next action to choose. The update method maximizes the next Q-value,judge the current Q-value, calculate the loss function, and minimize it. [See here](https://alexandervandekleut.github.io/function-approximation/) and [here](https://alexandervandekleut.github.io/q-learning-tensorflow/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Agent:\n    def __init__(self, state_shape, num_actions,alpha, gamma, epsilon_i=1.0, epsilon_f=0.01, n_epsilon=0.1):\n        self.epsilon_i = epsilon_i\n        self.epsilon_f = epsilon_f\n        self.n_epsilon = n_epsilon\n        self.epsilon = epsilon_i\n        self.gamma = gamma\n        self.state_shape = state_shape\n        self.num_actions = num_actions\n        self.optimizer = tf.keras.optimizers.Adam(alpha) \n\n        self.Q = ConvNN(state_shape,num_actions).build()\n        self.Q_ = ConvNN(state_shape,num_actions).build()\n        \n    def synchronize(self):\n        self.Q_.set_weights(self.Q.get_weights())\n\n    def act(self, s_t):\n        if np.random.random() < self.epsilon:\n            return np.random.randint(self.num_actions,size=3)\n        return np.argmax(self.Q(s_t), axis=1)\n    \n    def decay_epsilon(self, n):\n        self.epsilon = max(\n            self.epsilon_f, \n            self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f))\n\n    def update(self, s_t, a_t, r_t, s_t_next, d_t):\n        with tf.GradientTape() as tape:\n            Q_next = tf.stop_gradient(tf.reduce_max(self.Q_(s_t_next), axis=1))\n            Q_pred = tf.reduce_sum(self.Q(s_t)*tf.one_hot(a_t, self.num_actions, dtype=tf.float32), axis=1)\n            loss = tf.reduce_mean(0.5*(r_t + (1-d_t)*self.gamma*Q_next - Q_pred)**2)\n        grads = tape.gradient(loss, self.Q.trainable_variables)\n        self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The VectorizedEnvWrapper allows us to use multiple envs in parallel"},{"metadata":{"trusted":true},"cell_type":"code","source":"class VectorizedEnvWrapper(gym.Wrapper):\n    def __init__(self, make_env, num_envs=1):\n        super().__init__(make_env())\n        self.num_envs = num_envs\n        self.envs = [make_env() for env_index in range(num_envs)]\n    \n    def reset(self):\n        return np.asarray([env.reset() for env in self.envs])\n    \n    def reset_at(self, env_index):\n        return self.envs[env_index].reset()\n    \n    def step(self, actions):\n        next_states, rewards, dones, infos = [], [], [], []\n        for env, action in zip(self.envs, actions):\n            next_state, reward, done, info = env.step(action)\n            next_states.append(next_state)\n            rewards.append(reward)\n            dones.append(done)\n            infos.append(info)\n        return np.asarray(next_states), np.asarray(rewards), \\\n            np.asarray(dones), np.asarray(infos)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just dividing by 255 to get tensors filled with 0 and 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"class NormalizationWrapper(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n    \n    def observation(self, obs):\n        return obs/255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We produce a train_agent and the Gfootball env"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_agent = Agent((72,96,4),19,alpha=0.1,gamma=0.95)\ntrain_agent.Q.save_weights(\"QWeights\")\ntrain_agent.Q_.save_weights(\"Q_Weights\")\nsmm_env = VectorizedEnvWrapper(lambda: NormalizationWrapper(gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")),1)\ntrain_buffer = ReplayBuffer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training method is defined below "},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(env=smm_env,T=3001,batch_size=3,sync_every=10,agent=train_agent,buffer=train_buffer):\n    \n    agent.Q.load_weights(\"QWeights\")\n    agent.Q_.load_weights(\"Q_Weights\")\n    rewards = []\n    episode_rewards = 0\n    s_t = env.reset()\n    \n    for t in range(T):\n        if t%sync_every == 0:\n            agent.synchronize()\n        a_t = agent.act(s_t)\n        s_t_next, r_t, d_t, info = env.step(a_t)\n        buffer.remember(s_t, a_t, r_t, s_t_next, d_t)\n        for batch in buffer.sample(batch_size):\n            agent.update(*batch)\n        agent.decay_epsilon(t/T)\n        episode_rewards += r_t\n           \n    for i in range(env.num_envs):\n        rewards.append(episode_rewards[i])\n        episode_rewards[i] = 0\n        s_t[i] = env.reset_at(i)\n        \n    agent.epsilon_i = 1.0\n    agent.epsilon_f = 0.01\n    agent.n_epsilon = 0.1\n    agent.epsilon = agent.epsilon_i\n    \n    return rewards","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We train the model a couple of times"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(2):\n    print(train())\n    train_agent.Q.save_weights(\"QWeights\")\n    train_agent.Q_.save_weights(\"Q_Weights\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To write a submission, we only need a lightened version of the agent, the ConvNN architecture, and to load the trained weights\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile submission.py\n\nfrom gfootball.env import observation_preprocessing\nimport numpy as np\nimport tensorflow as tf\n\nclass SplitLayer(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n\n    def call(self, inputs):\n        split0,split1,split2,split3 = tf.split(inputs,4,3)\n        return [split0,split1,split2,split3]\n    \nclass ConvNN:\n\n    def __init__(self,state_shape,num_actions):\n        self.state_shape = state_shape\n        self.num_actions = num_actions\n\n    def _model_architecture(self):\n    \n        frames_input = tf.keras.layers.Input(shape=self.state_shape,name='input',batch_size=1)\n        frames_split = SplitLayer()(frames_input)\n        conv_branches = []\n        for f,frame in enumerate(frames_split):\n            conv_branches.append(self._build_conv_branch(frame,f))\n\n        concat = tf.keras.layers.concatenate(conv_branches)\n        \n        fc0 = tf.keras.layers.Dense(units=8192, name='fc0', \n                                     activation='relu')(concat)\n        fc1 = tf.keras.layers.Dense(units=2048, name='fc1', \n                                     activation='relu')(fc0)\n        fc2 = tf.keras.layers.Dense(units=512, name='fc2', \n                                     activation='relu')(fc1)\n        fc3 = tf.keras.layers.Dense(units=256, name='fc3', \n                                     activation='relu')(fc2)\n        fc4 = tf.keras.layers.Dense(units=64, name='fc4', \n                                     activation='relu')(fc3)\n        \n        action_output = tf.keras.layers.Dense(units=self.num_actions, name='output',\n                                               activation='relu')(fc4)\n        return frames_input,action_output\n        \n    @staticmethod\n    def _build_conv_branch(frame,number):\n        conv1 = tf.keras.layers.Conv2D(16, kernel_size=(8, 8), strides=(4, 4),\n                                        name='conv1_frame'+str(number), padding='same',\n                                        activation='relu')(frame)\n        mp1 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp1_frame\"+str(number))(conv1)\n        conv2 = tf.keras.layers.Conv2D(24, kernel_size=(4, 4), strides=(2, 2),\n                                        name='conv2_frame'+str(number),padding='same',\n                                        activation='relu')(mp1)\n        mp2 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp2_frame\"+str(number))(conv2)\n        conv3 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n                                        name='conv3_frame'+str(number),padding='same',\n                                        activation='relu')(mp2)\n        mp3 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp3_frame\"+str(number))(conv3)\n        conv4 = tf.keras.layers.Conv2D(64, kernel_size=(2, 2), strides=(1, 1),\n                                        name='conv4_frame'+str(number), padding='same',\n                                        activation='relu')(mp3)\n        mp4 = tf.keras.layers.MaxPooling2D(pool_size=1, name=\"mp4_frame\"+str(number))(conv4)\n\n        flatten = tf.keras.layers.Flatten(name='flatten'+str(number))(mp4)\n\n        return flatten\n        \n    def build(self):\n        frames_input,action_output = self._model_architecture()\n        model = tf.keras.Model(inputs=[frames_input], outputs=[action_output])\n        return model\n\nclass Agent:\n    def __init__(self, state_shape, num_actions):\n        \n        self.state_shape = state_shape\n        self.num_actions = num_actions\n\n        self.Q = ConvNN(state_shape,num_actions).build()\n        self.Q_ = ConvNN(state_shape,num_actions).build()\n        \n    def act(self, s_t):\n        return np.argmax(self.Q(s_t), axis=1)\n    \nDQN_Agent = Agent((72,96,4),19)\n\nDQN_Agent.Q.load_weights(\"QWeights\")\nDQN_Agent.Q_.load_weights(\"Q_Weights\")\n\ndef agent(obs):\n    obs = obs['players_raw'][0]\n    obs = observation_preprocessing.generate_smm([obs])\n    obs = obs/255 #Normalization\n    action = DQN_Agent.act(obs)\n    return [action[0]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see how our agent perform against a 'do_nothing' opponent"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the Environment.\nfrom kaggle_environments import make\nenv = make(\"football\", configuration={\"save_video\": True, \"scenario_name\": \"11_vs_11_kaggle\", \"running_in_notebook\": True})\noutput = env.run([\"/kaggle/working/submission.py\", \"do_nothing\"])[-1]\nprint('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))\nprint('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))\nenv.render(mode=\"human\", width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The agent makes very poor decisions, surely because it hasn't trained enough ([Google Research Paper on Gfootball](https://arxiv.org/pdf/1907.11180.pdf) implies that we need a lot more of training). But I think there are different aspects we can still improve on in order to get slightly better results. Maybe a [CheckpointRewardWrapper](https://github.com/google-research/football/blob/7a5ae3b5c849beb69d15a7c0028f16d7297b1d9f/gfootball/env/wrappers.py#L275) adapted for SMM representations returning a more explicit reward to the agent could help"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}