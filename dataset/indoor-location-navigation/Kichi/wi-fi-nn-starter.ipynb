{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, LSTM, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tqdm import tqdm\n\nimport random\n\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\n\n\nimport itertools\nimport glob\n\nrandom.seed(1)\nnp.random.seed(seed=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ------------------------------------------------------------------------------\n# Read data\n# ------------------------------------------------------------------------------\ntrain_files = sorted(glob.glob('../input/wifi-over-500/train/*_train.csv'))\ntest_files = sorted(glob.glob('../input/wifi-over-500/test/*_test.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def all_get(file_name_train, file_name_test):\n    \n    file_path = file_name_train\n    df_train = pd.read_csv(file_path)\n\n    test_file_path = file_name_test\n    df_test = pd.read_csv(test_file_path)\n\n\n    temp_df_train = df_train.copy()\n    temp_df_test = df_test.copy()\n\n\n\n\n    #target = temp_df_train[['x','y','f']].values / 1000\n    #target = np.asarray(target).astype(np.float64)\n\n    target = []\n    target.append(temp_df_train['x'])\n    target.append(temp_df_train['y'])\n    target.append(temp_df_train['f'])\n\n    #target = np.asarray(target).astype(np.float64)\n\n    path_train_temp = temp_df_train['path'].map(lambda x: int(x, 16))\n    path_train_temp = path_train_temp%100\n    temp_df_train['path'] = path_train_temp\n    temp_df_train = temp_df_train\n    temp_df_train.pop('x')\n    temp_df_train.pop('y')\n    temp_df_train.pop('f')\n    temp_df_train.pop('path')\n    temp_df_train = temp_df_train.drop(temp_df_train.columns[[0]], axis=1)\n    #train_data = np.asarray(temp_df_train).astype(np.float64)\n\n    #path_test_temp = temp_df_test['site_path_timestamp'].map(lambda x: int(x, 16))\n    temp_df_test.pop('site_path_timestamp')\n    temp_df_test = temp_df_test.drop(temp_df_test.columns[[0]], axis=1)\n    #temp_df_test /=1000\n    #temp_df_test = np.asarray(temp_df_test).astype(np.float64)\n\n\n\n    MODEL_PARAM = {\"batch_size\":1024,\n                  \"verbose\":0,\n                  \"epochs\":100}\n    callbacks_list=[EarlyStopping(monitor=\"loss\", min_delta=.00000001, patience=5,mode='auto')]\n\n\n    def build_model() -> Model:\n        # モデル定義\n        model = Sequential()\n        model.add(Dense(512,activation=tf.nn.relu))\n        model.add(Dropout(0.1))\n        model.add(Dense(32, activation=tf.nn.relu))\n        model.add(Dense(1))\n        model.compile(optimizer=\"Adam\", loss='mse', metrics=[\"mae\"])\n\n        return model\n\n\n\n    scores = []\n    model_list = []\n\n    for i in range(2):\n\n        train_x = temp_df_train\n        train_y = target[i]\n\n\n        # KFoldクラスを用いてクロスバリデーションの分割を行う\n        kf = KFold(n_splits=4, shuffle=True, random_state=71)\n        for tr_idx, va_idx in kf.split(train_x):\n            tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n            tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n\n            tr_x = np.asarray(tr_x).astype(np.float64)\n            tr_y = np.asarray(tr_y).astype(np.float64)\n            va_x = np.asarray(va_x).astype(np.float64)\n            va_y = np.asarray(va_y).astype(np.float64)\n\n\n            # 学習の実行、バリデーションデータの予測値の出力、スコアの計算を行う\n            model = build_model()\n            history = model.fit(tr_x, tr_y,\n                              validation_data=(va_x, va_y),\n                              callbacks=callbacks_list,\n                              **MODEL_PARAM)\n            va_pred = model.predict(va_x)\n            score = np.abs(va_y-va_pred)\n            scores.append(score)\n        model_list.append(model)\n\n\n\n    test_predictions_X = model_list[0].predict(temp_df_test)\n    test_predictions_Y = model_list[1].predict(temp_df_test)\n\n\n    train_x = temp_df_train\n    train_y = target[2]\n\n    model = tf.keras.Sequential()\n    # ユニット数が64の全結合層をモデルに追加します：\n    #全結合、入力64次元、出力64次元、活性化関数relu\n    model.add(layers.Dense(128, activation='relu'))\n    #全結合、入力64次元、出力64次元、活性化関数relu\n    model.add(layers.Dense(64, activation='relu'))\n    #全結合、入力64次元、出力2次元、活性化関数softmax\n    model.add(layers.Dense(15,activation=\"softmax\"))\n\n\n\n\n\n    # 定数\n    LOSS = 'categorical_crossentropy'  # 損失関数：多クラス分類用の交差エントロピー\n    METRICS = ['accuracy']                    # 評価関数：正解率\n    OPTIMIZER = tf.keras.optimizers.Adam      # 最適化：\n    LEARNING_RATE = 0.001                     # 学習率： 0.001（学習率の調整）\n\n\n    # 学習方法を定義する\n    model.compile(optimizer='rmsprop',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    train_y_onehot = to_categorical(target[2])\n\n\n\n\n    col_name = [-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9]\n    f = []\n    #df_dlabel = pd.DataFrame(np.arange(-5,10).reshape(1,15))\n    for i, temp_tar in enumerate(target[2]):\n        a = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n        a[temp_tar+5] = 1\n        f.append(a)\n        #df_dlabel.append\n\n\n\n\n    train_y = np.array(f)\n\n\n\n\n    model_list_f = []\n    # 定数（ミニバッチ学習時に必要となるもの）\n    BATCH_SIZE = 96   # バッチサイズ： 96\n    EPOCHS = 100      # エポック数： 100\n\n    # 早期終了\n    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n\n    # 学習する\n    hist = model.fit(x=train_x,                          # 訓練用データ\n                     y=train_y,                          # 訓練用ラベル\n                     validation_split=0.2,               # 精度検証用 20％\n                     batch_size=BATCH_SIZE,              # バッチサイズ\n                     epochs=EPOCHS,                      # エポック数\n                     verbose=0,                          # 実行状況表示\n                     callbacks=[                         # コールバック\n                       es #早期終了\n                     ])\n    model_list_f.append(model)\n\n\n\n    len(model_list_f)\n\n    test_predictions_F = model_list_f[0].predict(temp_df_test)\n\n    np.set_printoptions(threshold=np.inf)\n\n\n\n\n    pred_class_F = np.argmax(test_predictions_F, axis=-1)-5\n\n    return pred_class_F, test_predictions_X,test_predictions_Y\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = list()\nfor temp_train_file,temp_test_file in tqdm(zip(train_files,test_files)):\n    print(temp_train_file)\n    predictions.append(all_get(temp_train_file,temp_test_file))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_f = []\npre_x = []\npre_y = []\n\nfor p in predictions:\n    pre_f.extend(p[0].tolist())\n    \n    for x in p[1]:\n        x = float(x)\n        pre_x.append(x)\n        \n    for y in p[2]:\n        y = float(y)\n        pre_y.append(y)\n        \n\npf = pd.DataFrame(data=pre_f)\npx = pd.DataFrame(data=pre_x)\npy = pd.DataFrame(data=pre_y)\n\n\nsubm = pd.read_csv('../input/indoor-location-navigation/sample_submission.csv', index_col=0)\nsubm['floor'] = pre_f\nsubm['x'] = pre_x\nsubm['y'] = pre_y\n\nsubm.to_csv(\"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}