{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip uninstall fastai -y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import sys,os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sys.path.append('../input/fastaiv1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom collections import defaultdict\nimport os\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport matplotlib.image as immg\nfrom sklearn.model_selection import StratifiedKFold,KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install object-detection-fastai","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from object_detection_fastai.helper.object_detection_helper import *\nfrom object_detection_fastai.loss.RetinaNetFocalLoss import RetinaNetFocalLoss\nfrom object_detection_fastai.models.RetinaNet import RetinaNet\nfrom object_detection_fastai.callbacks.callbacks import BBLossMetrics, BBMetrics, PascalVOCMetric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/nfl-impact-detection/image_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = Path('../input/nfl-impact-detection')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = df.image.value_counts()\ntr = pd.DataFrame({'image':tr.index,'image_count':tr.values})\ntr = tr.sample(frac=1.,random_state=2020).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(n=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_grp = df.groupby(['image'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Printing a sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"name = '58146_002671_Endzone_frame304.jpg'\nloc = '../input/nfl-impact-detection/images/'+name\ntemp = df_grp.get_group(name)\nlb = temp.loc[:, (['label','left', 'width', 'top', 'height'])].values\nimg = immg.imread(loc)\nfig,ax = plt.subplots(figsize=(20,12))\nax.imshow(img)\nfor s in lb:\n    l,b = s[0],s[1:]\n    rect = [b[0],b[2],b[1],b[3]]\n    draw_rect(ax,rect,text=l,text_size=10,color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lbl_img(train):\n    helmet2bbox = {}\n    grp = df.image.unique()\n    tr_gr = df.groupby(['image'])\n    from tqdm.notebook import tqdm\n    for i in tqdm(range(len(grp))):\n        name = str(grp[i])\n        bbox = []\n        lbls = []\n        temp_b = []\n        temp = tr_gr.get_group(grp[i])\n        tt = temp.loc[:, (['label','left', 'width', 'top', 'height'])].values\n        for j in range(len(temp)):\n            lbls.append(tt[j][0])\n            b = tt[j][1:]  \n            t = [b[0],b[2],b[1],b[3]] # x,y, width, height\n            # Currently our coordinates are x,w,l,h and we want x1,y1,x2,y2\n            # To convert it, we need to add our width and height to the respective x and y.\n            t[2],t[3] = t[0]+t[2],t[1]+t[3]  \n            t1 = [t[1],t[0],t[3],t[2]]\n\n            temp_b.append(t1)\n        bbox.append(temp_b)\n        bbox.append(lbls)\n        helmet2bbox[name] = bbox\n    return helmet2bbox","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"helmet2bbox = get_lbl_img(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chk = helmet2bbox['57802_001673_Endzone_frame0932.jpg']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = immg.imread(str(path/'images'/'57802_001673_Endzone_frame0932.jpg'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample Check 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(16,10))\nax.imshow(img)\nlbl,bbxs = chk[1],chk[0]\nfor l, b in zip(lbl,bbxs):\n    rect = [b[0],b[1],b[3]-b[1],b[2]-b[0]]\n    rect1 = [rect[1],rect[0],rect[3],rect[2]]\n    draw_rect(ax,rect1,text=l,text_size=12,color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img.shape[0]/2,img.shape[1]/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_y_func = lambda o: helmet2bbox[Path(o).name] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DataLoader"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (ObjectItemList.from_df(tr.sample(frac=0.5),path, folder = 'images' ,cols='image')\n        #Where are the images? ->\n        .split_by_rand_pct(0.2)                          \n        #How to split in train/valid? -> randomly with the default 20% in valid\n        .label_from_func(get_y_func)\n        #How to find the labels? -> use get_y_func on the file name of the data\n        .transform(size=512,resize_method=ResizeMethod.SQUISH)\n        #.add_test(ts)\n        #Data augmentation? -> Standard transforms; also transform the label images\n        .databunch(bs=2, collate_fn=bb_pad_collate))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.show_batch( 1, figsize = (20,12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data.train_ds),len(data.valid_ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.classes,data.c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size = 512","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Anchors"},{"metadata":{"trusted":true},"cell_type":"code","source":"anchors = create_anchors(sizes=[(32,32),(16,16),(8,8),(4,4)], ratios=[0.5, 1, 2], scales=[0.15, 0.25, 0.35, 0.45, 0.55, 0.75])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(10,10))\nax.imshow(image2np(data.valid_ds[0][0].data))\n\nfor i, bbox in enumerate(anchors[:18]):\n    bb = bbox.numpy()\n    x = (bb[0] + 1) * size / 2 \n    y = (bb[1] + 1) * size / 2 \n    w = bb[2] * size / 2\n    h = bb[3] * size / 2\n    \n    rect = [x,y,w,h]\n    draw_rect(ax,rect)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(anchors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoder and Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_classes = data.train_ds.c\n\ncrit = RetinaNetFocalLoss(anchors)\n\nencoder = create_body(models.resnet18, True, -2)\n\nmodel = RetinaNet(encoder, n_classes=data.train_ds.c, n_anchors=18, sizes=[32,16,8,4], chs=32, final_bias = -4., n_conv = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Learner"},{"metadata":{"trusted":true},"cell_type":"code","source":"voc = PascalVOCMetric(anchors, size, [i for i in data.train_ds.y.classes[1:]])\nlearn = Learner(data,\n                model, \n                loss_func=crit,\n                callback_fns=[BBMetrics],\n                metrics=[voc],\n                model_dir = '/kaggle/working/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.split([model.encoder[6], model.c5top5]);\nlearn.freeze_to(-2)\n#learn = learn.to_fp16()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#learn.unfreeze()\nlearn.fit_one_cycle(3, 1e-3 ,callbacks = [SaveModelCallback(learn, every ='improvement', monitor ='AP-Helmet', name ='best_model',mode='max')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load('best_model');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_results_side_by_side(learn, anchors, detect_thresh=0.3, nms_thresh=0.1, image_count=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}