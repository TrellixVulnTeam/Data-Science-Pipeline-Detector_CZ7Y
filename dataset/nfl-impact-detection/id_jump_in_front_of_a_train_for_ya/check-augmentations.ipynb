{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nimport cv2\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom albumentations.core.transforms_interface import DualTransform\nfrom albumentations.augmentations.bbox_utils import denormalize_bbox, normalize_bbox","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomCutout(DualTransform):\n    \"\"\"\n    Custom Cutout augmentation with handling of bounding boxes \n    Note: (only supports square cutout regions)\n    \n    Author: Kaushal28\n    Reference: https://arxiv.org/pdf/1708.04552.pdf\n    \"\"\"\n    \n    def __init__(\n        self,\n        fill_value=0,\n        bbox_removal_threshold=0.50,\n        min_cutout_size=192,\n        max_cutout_size=512,\n        always_apply=False,\n        p=0.5\n    ):\n        \"\"\"\n        Class construstor\n        \n        :param fill_value: Value to be filled in cutout (default is 0 or black color)\n        :param bbox_removal_threshold: Bboxes having content cut by cutout path more than this threshold will be removed\n        :param min_cutout_size: minimum size of cutout (192 x 192)\n        :param max_cutout_size: maximum size of cutout (512 x 512)\n        \"\"\"\n        super(CustomCutout, self).__init__(always_apply, p)  # Initialize parent class\n        self.fill_value = fill_value\n        self.bbox_removal_threshold = bbox_removal_threshold\n        self.min_cutout_size = min_cutout_size\n        self.max_cutout_size = max_cutout_size\n        \n    def _get_cutout_position(self, img_height, img_width, cutout_size):\n        \"\"\"\n        Randomly generates cutout position as a named tuple\n        \n        :param img_height: height of the original image\n        :param img_width: width of the original image\n        :param cutout_size: size of the cutout patch (square)\n        :returns position of cutout patch as a named tuple\n        \"\"\"\n        position = namedtuple('Point', 'x y')\n        return position(\n            np.random.randint(0, img_width - cutout_size + 1),\n            np.random.randint(0, img_height - cutout_size + 1)\n        )\n        \n    def _get_cutout(self, img_height, img_width):\n        \"\"\"\n        Creates a cutout pacth with given fill value and determines the position in the original image\n        \n        :param img_height: height of the original image\n        :param img_width: width of the original image\n        :returns (cutout patch, cutout size, cutout position)\n        \"\"\"\n        cutout_size = np.random.randint(self.min_cutout_size, self.max_cutout_size + 1)\n        cutout_position = self._get_cutout_position(img_height, img_width, cutout_size)\n        return np.full((cutout_size, cutout_size, 3), self.fill_value), cutout_size, cutout_position\n        \n    def apply(self, image, **params):\n        \"\"\"\n        Applies the cutout augmentation on the given image\n        \n        :param image: The image to be augmented\n        :returns augmented image\n        \"\"\"\n        image = image.copy()  # Don't change the original image\n        self.img_height, self.img_width, _ = image.shape\n        cutout_arr, cutout_size, cutout_pos = self._get_cutout(self.img_height, self.img_width)\n        \n        # Set to instance variables to use this later\n        self.image = image\n        self.cutout_pos = cutout_pos\n        self.cutout_size = cutout_size\n        \n        image[cutout_pos.y:cutout_pos.y+cutout_size, cutout_pos.x:cutout_size+cutout_pos.x, :] = cutout_arr\n        return image\n    \n    def apply_to_bbox(self, bbox, **params):\n        \"\"\"\n        Removes the bounding boxes which are covered by the applied cutout\n        \n        :param bbox: A single bounding box coordinates in pascal_voc format\n        :returns transformed bbox's coordinates\n        \"\"\"\n\n        # Denormalize the bbox coordinates\n        bbox = denormalize_bbox(bbox, self.img_height, self.img_width)\n        x_min, y_min, x_max, y_max = tuple(map(int, bbox))\n\n        bbox_size = (x_max - x_min) * (y_max - y_min)  # width * height\n        overlapping_size = np.sum(\n            (self.image[y_min:y_max, x_min:x_max, 0] == self.fill_value) &\n            (self.image[y_min:y_max, x_min:x_max, 1] == self.fill_value) &\n            (self.image[y_min:y_max, x_min:x_max, 2] == self.fill_value)\n        )\n\n        # Remove the bbox if it has more than some threshold of content is inside the cutout patch\n        if overlapping_size / bbox_size > self.bbox_removal_threshold:\n            return normalize_bbox((0, 0, 0, 0), self.img_height, self.img_width)\n\n        return normalize_bbox(bbox, self.img_height, self.img_width)\n\n    def get_transform_init_args_names(self):\n        \"\"\"\n        Fetches the parameter(s) of __init__ method\n        :returns: tuple of parameter(s) of __init__ method\n        \"\"\"\n        return ('fill_value', 'bbox_removal_threshold', 'min_cutout_size', 'max_cutout_size', 'always_apply', 'p')\n\ndef mixup(images, bboxes, areas, alpha=1.0):\n    \"\"\"\n    Randomly mixes the given list if images with each other\n    \n    :param images: The images to be mixed up\n    :param bboxes: The bounding boxes (labels)\n    :param areas: The list of area of all the bboxes\n    :param alpha: Required to generate image wieghts (lambda) using beta distribution. In this case we'll use alpha=1, which is same as uniform distribution\n    \"\"\"\n    # Generate random indices to shuffle the images\n    indices = torch.randperm(len(images))\n    shuffled_images = images[indices]\n    shuffled_bboxes = bboxes[indices]\n    shuffled_areas = areas[indices]\n    \n    # Generate image weight (minimum 0.4 and maximum 0.6)\n    lam = np.clip(np.random.beta(alpha, alpha), 0.4, 0.6)\n    print(f'lambda: {lam}')\n    \n    # Weighted Mixup\n    mixedup_images = lam*images + (1 - lam)*shuffled_images\n    \n    mixedup_bboxes, mixedup_areas = [], []\n    for bbox, s_bbox, area, s_area in zip(bboxes, shuffled_bboxes, areas, shuffled_areas):\n        mixedup_bboxes.append(bbox + s_bbox)\n        mixedup_areas.append(area + s_area)\n    \n    return mixedup_images, mixedup_bboxes, mixedup_areas, indices.numpy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!ls ../input/nfl-impact-detection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/nfl-impact-detection/train_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.groupby('playID').get_group(82).query('impact==1').iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images_df = pd.read_csv('../input/nfl-impact-detection/image_labels.csv')\nimages_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images_df['image'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imgname = '57583_001841_Endzone_frame43.jpg'\n\nimg = cv2.imread(f'../input/nfl-impact-detection/images/{imgname}')\n\nfig,ax = plt.subplots(figsize=(15,10))\nax.imshow(img)\n\nsub_df = images_df.loc[images_df['image']==imgname][['left','width','top','height']]\nfor i,row in sub_df.iterrows():\n    x,w,y,h = row.values\n    rect = patches.Rectangle((x,y),w,h,\n                             linewidth=1,\n                             edgecolor='r',\n                             facecolor='r',\n                             alpha=0.25,\n                            )\n    ax.add_patch(rect)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pascal_voc, which is [x_min, y_min, x_max, y_max]\n# COCO, which is [x_min, y_min, width, height]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imgname = '57583_001841_Endzone_frame43.jpg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_aug(imgname,aug):\n    image = cv2.imread(f'../input/nfl-impact-detection/images/{imgname}')\n    boxes = images_df.loc[images_df['image']==imgname,['left','top','width','height']].values\n\n    fig,axs = plt.subplots(figsize=(20,7.5),nrows=2,ncols=3)\n    axs = axs.flatten()\n\n    for i in range(6):\n        if i==0:\n            axs[i].imshow(image)\n            sub_df = images_df.loc[images_df['image']==imgname][['left','width','top','height']]\n            for j,row in sub_df.iterrows():\n                x,w,y,h = row.values\n                rect = patches.Rectangle((x,y),w,h,\n                                         linewidth=1,\n                                         edgecolor='r',\n                                         facecolor='r',\n                                         alpha=0.25,\n                                        )\n                axs[i].add_patch(rect)\n        else:\n            aug_result = aug(image=image, bboxes=boxes, labels=np.ones(boxes.shape))\n            axs[i].imshow(aug_result['image'])\n            for x,y,w,h in aug_result['bboxes']:\n                rect = patches.Rectangle((x,y),w,h,\n                                         linewidth=1,\n                                         edgecolor='r',\n                                         facecolor='r',\n                                         alpha=0.25,\n                                        )\n                axs[i].add_patch(rect)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = albumentations.Compose([\n        #albumentations.Resize(512, 512),   # Resize the given 1024 x 1024 image to 512 * 512\n        albumentations.VerticalFlip(1),    # Verticlly flip the image\n    ], bbox_params={'format': 'coco','label_fields': ['labels']})\ncheck_aug(imgname,aug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = albumentations.Compose([\n        #albumentations.Resize(512, 512),   # Resize the given 1024 x 1024 image to 512 * 512\n        albumentations.Blur(blur_limit=(3,17),p=1.),\n    ], bbox_params={'format': 'coco','label_fields': ['labels']})\ncheck_aug(imgname,aug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = albumentations.Compose([\n        #albumentations.Resize(512, 512),   # Resize the given 1024 x 1024 image to 512 * 512\n        albumentations.CLAHE(clip_limit=6.0, tile_grid_size=(8, 8), p=1),\n    ], bbox_params={'format': 'coco','label_fields': ['labels']})\ncheck_aug(imgname,aug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = albumentations.Compose([\n        #albumentations.Resize(512, 512),   # Resize the given 1024 x 1024 image to 512 * 512\n        albumentations.IAASharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1),\n    ], bbox_params={'format': 'coco','label_fields': ['labels']})\ncheck_aug(imgname,aug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = albumentations.Compose([\n        #albumentations.Resize(512, 512),   # Resize the given 1024 x 1024 image to 512 * 512\n        albumentations.GaussNoise(var_limit=(10,10000),p=1),\n    ], bbox_params={'format': 'coco','label_fields': ['labels']})\ncheck_aug(imgname,aug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = albumentations.Compose([\n        #albumentations.Resize(512, 512),   # Resize the given 1024 x 1024 image to 512 * 512\n        albumentations.Rotate(p=1),\n    ], bbox_params={'format': 'coco','label_fields': ['labels']})\ncheck_aug(imgname,aug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = albumentations.Compose([\n        #albumentations.Resize(512, 512),   # Resize the given 1024 x 1024 image to 512 * 512\n        albumentations.HueSaturationValue(p=1),\n    ], bbox_params={'format': 'coco','label_fields': ['labels']})\ncheck_aug(imgname,aug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = albumentations.Compose([\n        #albumentations.Resize(512, 512),   # Resize the given 1024 x 1024 image to 512 * 512\n        albumentations.RGBShift(p=1),\n    ], bbox_params={'format': 'coco','label_fields': ['labels']})\ncheck_aug(imgname,aug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = albumentations.Compose([\n        #albumentations.Resize(512, 512),   # Resize the given 1024 x 1024 image to 512 * 512\n        albumentations.RandomBrightness(limit=(-0.25,0.25),p=1),\n    ], bbox_params={'format': 'coco','label_fields': ['labels']})\ncheck_aug(imgname,aug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = albumentations.Compose([\n        #albumentations.Resize(512, 512),   # Resize the given 1024 x 1024 image to 512 * 512\n        albumentations.RandomContrast(limit=(-0.25,0.25),p=1),\n    ], bbox_params={'format': 'coco','label_fields': ['labels']})\ncheck_aug(imgname,aug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = albumentations.Compose([\n        #albumentations.Resize(512, 512),   # Resize the given 1024 x 1024 image to 512 * 512\n        albumentations.RandomRain(p=1),\n    ], bbox_params={'format': 'coco','label_fields': ['labels']})\ncheck_aug(imgname,aug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = albumentations.Compose([\n        #albumentations.Resize(512, 512),   # Resize the given 1024 x 1024 image to 512 * 512\n        albumentations.VerticalFlip(p=0.5),    # Verticlly flip the image\n        albumentations.HorizontalFlip(p=0.5),\n        albumentations.RandomBrightness(limit=(-0.25,0.25),p=0.5),\n        albumentations.RandomContrast(limit=(-0.25,0.25),p=0.5),\n        albumentations.RGBShift(p=0.5),\n        albumentations.Blur(blur_limit=(3,17),p=0.5),\n        albumentations.GaussNoise(var_limit=(10,1000),p=0.5),\n    ], bbox_params={'format': 'coco','label_fields': ['labels']})\ncheck_aug(imgname,aug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}