{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/icevision-052/fastcore-1.3.2-py3-none-any.whl /kaggle/input/icevision-052/loguru-0.5.3-py3-none-any.whl /kaggle/input/icevision-052/pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl /kaggle/input/icevision-052/imagesize-1.2.0-py2.py3-none-any.whl /kaggle/input/icevision-052/timm-0.4.5-py3-none-any.whl /kaggle/input/icevision-052/omegaconf-2.0.6-py3-none-any.whl /kaggle/input/icevision-052/effdet-0.2.1-py3-none-any.whl /kaggle/input/icevision-052/icevision-0.5.2-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall fastai -y","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Train notebook](https://www.kaggle.com/nikitautin/35th-place-efficientdet-train)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom icevision.all import *\nfrom tqdm.contrib.concurrent import process_map\nfrom functools import partial\nfrom effdet import DetBenchPredict, unwrap_bench","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FRAME_RANGE = 4\nVALID_PERCENT = 0.2\nSIZE = (512, 512)\nCLASSES_NUM = 2\nIMPACT_CLASS = 2\n\nIOU_THR = 0.35\nFILTER_OFFSET = 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = Path(\"/kaggle/input/nfl-impact-detection\")\ntest_video_path = path/'test'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_images(video_name, video_dir, video_labels, out_dir, only_with_impact=True, impact_cls=IMPACT_CLASS):\n    vidcap = cv2.VideoCapture(str(video_dir/video_name))\n    frame = 0\n    while True:\n        read, img = vidcap.read()\n        if not read:\n            break\n        frame += 1\n        if only_with_impact:\n            query_str = 'video == @video_name and frame == @frame and impact == @impact_cls'\n            boxes = video_labels.query(query_str)\n            if len(boxes) == 0:\n                continue\n        image_path = f'{out_dir}/{video_name}'.replace('.mp4', f'_{frame}.png')\n        _ = cv2.imwrite(image_path, img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images_path = Path('/kaggle/working/test_images')\ntest_images_path.mkdir()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_images_part = partial(make_images, video_dir=test_video_path, video_labels=None,\n                           out_dir=test_images_path, only_with_impact=False)\nprocess_map(make_images_part, os.listdir(test_video_path), max_workers=2);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infer_tfms = tfms.A.Adapter([tfms.A.Resize(*SIZE), tfms.A.Normalize()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class InferParser(parsers.Parser, parsers.FilepathMixin, parsers.SizeMixin):\n    def __init__(self, path):\n        self.images = get_image_files(path)\n\n    def __iter__(self):\n        yield from self.images\n\n    def __len__(self):\n        return len(self.images)\n\n    def imageid(self, o) -> Hashable:\n        return o.stem\n\n    def filepath(self, o) -> Union[str, Path]:\n        return o\n\n    def image_width_height(self, o) -> Tuple[int, int]:\n        return get_image_size(self.filepath(o))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parser = InferParser(\"/kaggle/working/test_images\")\nsplitter = SingleSplitSplitter()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infer_rs = parser.parse(data_splitter=splitter, autofix=False)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infer_ds = Dataset(infer_rs, infer_tfms)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.cache/torch/hub/checkpoints\n!cp /kaggle/input/nfl-models/tf_efficientnet_b5_ra-9a3e5369.pth /root/.cache/torch/hub/checkpoints/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = efficientdet.model(model_name=\"tf_efficientdet_d5\", num_classes=CLASSES_NUM, img_size=SIZE, pretrained=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state_dict = torch.load('/kaggle/input/nfl-models/efdb5_512_mixup_2cl_5-10ep.pth', map_location='cuda:0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(state_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infer_dl = efficientdet.infer_dl(infer_ds, batch_size=64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"box_list = []\nscore_list = []\ndetection_threshold = 0.4\n\nwith torch.no_grad():\n    device = torch.device('cuda:0')\n    bench = DetBenchPredict(unwrap_bench(model))\n    bench = bench.eval().to(device)\n\n    for batch, _ in tqdm(infer_dl):\n        imgs, img_info = batch\n        imgs = imgs.to(device)\n        img_info = {k: v.to(device) for k, v in img_info.items()}\n\n        raw_preds = bench(x=imgs, img_info=img_info)\n        dets = raw_preds.detach().cpu().numpy()\n\n        for det in dets:\n            boxes = det[:, :4]\n            scores = det[:, 4]\n            labels = det[:, 5]\n            indexes = np.where((scores > detection_threshold)\n                               & (labels == IMPACT_CLASS))[0]\n            box_list.append(boxes[indexes])\n            score_list.append(scores[indexes])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert bounding boxes from xyxy to xywh and to origin image size:","metadata":{}},{"cell_type":"code","source":"result_image_ids = []\nresults_boxes = []\nresults_scores = []\nfor i in range(len(box_list)):\n    boxes = box_list[i]\n    scores = score_list[i]\n    image_id = infer_rs[i].filepath.name\n    boxes[:, 0] = (boxes[:, 0] * 1280 / SIZE[1])\n    boxes[:, 1] = (boxes[:, 1] * 720 / SIZE[0])\n    boxes[:, 2] = (boxes[:, 2] * 1280 / SIZE[1])\n    boxes[:, 3] = (boxes[:, 3] * 720 / SIZE[0])\n    boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n    boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n    boxes = boxes.astype(np.int32)\n    boxes[:, 0] = boxes[:, 0].clip(min=0, max=1280-1)\n    boxes[:, 2] = boxes[:, 2].clip(min=0, max=1280-1)\n    boxes[:, 1] = boxes[:, 1].clip(min=0, max=720-1)\n    boxes[:, 3] = boxes[:, 3].clip(min=0, max=720-1)\n    result_image_ids += [image_id] * len(boxes)\n    results_boxes.append(boxes)\n    results_scores.append(scores)\n    \nresults_boxes = np.concatenate(results_boxes)\nresults_scores = np.concatenate(results_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(results_boxes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"box_df = pd.DataFrame(results_boxes, columns=['left', 'top', 'width', 'height'])\ntest_df = pd.DataFrame({'scores':results_scores, 'image_name':result_image_ids})\ntest_df = pd.concat([test_df, box_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['gameKey'] = test_df.image_name.str.split('_').str[0].astype(int)\ntest_df['playID'] = test_df.image_name.str.split('_').str[1].astype(int)\ntest_df['view'] = test_df.image_name.str.split('_').str[2]\ntest_df['frame'] = test_df.image_name.str.split('_').str[3].str.replace('.png','').astype(int)\ntest_df['video'] = test_df.image_name.str.rsplit('_',1).str[0] + '.mp4'\ntest_df = test_df[[\"gameKey\",\"playID\",\"view\",\"video\",\"frame\",\"left\",\"width\",\"top\",\"height\",\"scores\"]]\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def iou(bbox1, bbox2):\n    bbox1 = [float(x) for x in bbox1]\n    bbox2 = [float(x) for x in bbox2]\n\n    (x0_1, y0_1, x1_1, y1_1) = bbox1\n    (x0_2, y0_2, x1_2, y1_2) = bbox2\n    x1_1 += x0_1\n    y1_1 += y0_1\n    x1_2 += x0_2\n    y1_2 += y0_2\n\n    # get the overlap rectangle\n    overlap_x0 = max(x0_1, x0_2)\n    overlap_y0 = max(y0_1, y0_2)\n    overlap_x1 = min(x1_1, x1_2)\n    overlap_y1 = min(y1_1, y1_2)\n\n    # check if there is an overlap\n    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:\n            return 0\n\n    # if yes, calculate the ratio of the overlap to each ROI size and the unified size\n    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)\n    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)\n    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)\n    size_union = size_1 + size_2 - size_intersection\n\n    return size_intersection / size_union","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NMS with frame range to filter false impact:","metadata":{}},{"cell_type":"code","source":"keep_idx = []\nfor keys in test_df.groupby(['gameKey', 'playID']).size().to_dict().keys():\n    for view in [\"Endzone\", \"Sideline\"]:\n        tmp_df = test_df.query('gameKey == @keys[0] and playID == @keys[1] and view == @view').copy()\n        while (len(tmp_df) > 0):\n            boxes = tmp_df[[\"left\", \"top\", \"width\", \"height\"]].to_numpy()\n            max_box_idx = tmp_df[\"scores\"].idxmax()\n            max_box = tmp_df[\"scores\"].argmax()\n            ious = np.array(list(map(partial(iou, boxes[max_box]), boxes)))\n            frame = tmp_df.loc[max_box_idx, \"frame\"]\n            m = (ious > IOU_THR) & (tmp_df[\"frame\"].to_numpy() <= frame + FILTER_OFFSET) & (tmp_df[\"frame\"].to_numpy() >= frame - FILTER_OFFSET)\n            keep_idx.append(max_box_idx)\n            tmp_df.drop(tmp_df[m].index, inplace=True)\ntest_df = test_df[test_df.index.isin(keep_idx)].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.drop(columns=['scores'], inplace=True)\ntest_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nflimpact\n\nenv = nflimpact.make_env()\nenv.predict(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/test_images/*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}