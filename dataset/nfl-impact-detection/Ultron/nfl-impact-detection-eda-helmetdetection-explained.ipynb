{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center>NFL 1st and Future - Impact Detection</center></h1>\n<h2><center>Detect helmet impacts in videos of NFL plays</center></h2>"},{"metadata":{},"cell_type":"markdown","source":"<center><img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/12125/logos/header.png?t=2018-11-30-18-08-32\"></center>"},{"metadata":{},"cell_type":"markdown","source":"# About the Competition"},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"color:red\">Work in Progress.</h2>\n<h3 style=\"color:brown\">Shoot your thoughts in the comment section and Don't forget to upvote if you like the notebook :)</h3>"},{"metadata":{},"cell_type":"markdown","source":"## Organisers and additional perks"},{"metadata":{},"cell_type":"markdown","source":"- This competition is part of the **NFL’s annual 1st and Future competition**, which is designed to spur innovation in athlete safety and performance. \n- For the first time this year, 1st and Future will be broadcast in primetime during Super Bowl LV week on NFL Network, and winning Kagglers may have the opportunity to present their computer vision systems as part of this exciting event.  \n- If successful, you could support the NFL’s research programs in a big way: improving athletes' safety. Backed by this research, the NFL may implement rule changes and helmet design improvements to try to better protect the athletes who play the game millions watch each week."},{"metadata":{},"cell_type":"markdown","source":"## What to do?\n\n- We’ll develop a computer vision model that automatically detects helmet impacts that occur on the field. \n- Kick off with the dataset of more than one thousand definitive head impacts from thousands of game images, labeled video from the sidelines and end zones, and player tracking data. \n\n## Data Source\n\n- This information is sourced from the NFL’s Next Gen Stats (NGS) system, which documents the position, speed, acceleration, and orientation for every player on the field during NFL games."},{"metadata":{},"cell_type":"markdown","source":"# Evaluation Metric"},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"color:brown\">Task: </h2>\n<p>Segment helmet collisions in videos of football plays using bounding boxes.</p>\n<h2 style=\"color:brown\">Metric: </h2>\n<p>Evaluated using <em>micro F1 score</em> at an <em>Intersection Over Union</em> threshold of 0.35.</p>"},{"metadata":{},"cell_type":"markdown","source":"## Why F1-Score?\n\n- The main departure from a traditional metric is that some imprecision on the timing of the impact is acceptable. For a given ground truth impact, a prediction within **+/- 4 frames (9 frames total)** within the same play can be accepted as valid without necessarily degrading the score. Assuming the player is moving over the course of those frames, the exact bounding box predicted to achieve an IoU of 1.0 would also vary depending on the frame.\n- As one helmet may partially obscure another from the camera's perspective, both predicted and ground truth bounding boxes may overlap. However, at most one prediction will ever be assigned to a given ground truth box.\n\nThe two criteria described above mean that one or more predictions could theoretically be assigned to more than one ground truth boxes. If this happens, our metric will optimize for the assignments between your prediction(s) and the ground truth boxes that lead to the highest total number of True Positives (thereby maximizing the F1 score). At most one prediction will be assigned to any ground truth box and vice versa."},{"metadata":{},"cell_type":"markdown","source":"## But What is F1 Score?"},{"metadata":{},"cell_type":"markdown","source":"F1 is calculated as follows:\n\\begin{equation}\nF1 = 2 ∗ \\frac{{precision∗recall}} {precision+recall}\n\\end{equation}\n\nwhere:\n\n\\begin{align}\nprecision = \\frac{TP}{TP+FP} \\\\\nrecall = \\frac{{TP}}{TP+FN}\n\\end{align}"},{"metadata":{},"cell_type":"markdown","source":"## What is IoU?"},{"metadata":{},"cell_type":"markdown","source":"The IoU of a proposed bounding box and a ground truth bounding box is calculated as:\n\n\\begin{equation}\nIoU(A,B) = \\frac{{A∩B}}{A∪B}\n\\end{equation}"},{"metadata":{},"cell_type":"markdown","source":"# Make a submission, but how to?"},{"metadata":{},"cell_type":"markdown","source":"Due to the custom metric, this competition relies on an evaluation pipeline which is slightly different than a typical code competition. Your notebook must import and submit via the custom `nflimpact` python module available in Kaggle notebooks.\n\nTo submit, simply add these three lines at the end of your code:"},{"metadata":{},"cell_type":"markdown","source":"`\nimport nflimpact\nenv = nflimpact.make_env()\nenv.predict(df) # df is a pandas dataframe of your entire submission file\n`"},{"metadata":{},"cell_type":"markdown","source":"The dataframe should be in the following format:\n- Each row in your submission represents a single predicted bounding box for the given frame.\n- Note that it is not required to include labels of which players had an impact, only a bounding box where it occurred."},{"metadata":{},"cell_type":"markdown","source":"`\ngameKey,playID,view,video,frame,left,width,top,height\n57590,3607,Endzone,57590_003607_Endzone.mp4,1,1,1,1,1\n57590,3607,Sideline,57590_003607_Sideline.mp4,1,1,1,1,1\n57595,1252,Endzone,57595_001252_Endzone.mp4,1,1,1,1,1\n57595,1252,Sideline,57595_001252_Sideline.mp4,1,1,1,1,1\netc.\n`"},{"metadata":{},"cell_type":"markdown","source":"## More on Guidelinses/submissions:\n\n- CPU Notebook <= 9 hours run-time\n- GPU Notebook <= 9 hours run-time\n- Freely & publicly available external data is allowed, including pre-trained models"},{"metadata":{},"cell_type":"markdown","source":"# Data Overview"},{"metadata":{},"cell_type":"markdown","source":"- We are tasked with identifying helmet collisions in video files. \n- Each play has two associated videos, showing a `sideline` and `endzone` view, and the videos are aligned so that frames correspond between the videos. \n- The training set videos are in `train` with corresponding labels in `train_labels.csv`, while the videos for which you must predict are in the `test` folder.\n- We are also provided an ancillary dataset of images showing helmets with labeled bounding boxes. These files are located in `images` and the bounding boxes in `image_labels.csv`.\n\n<p style=\"color:red\">This is a code competition. When you submit, your model will be rerun on a set of 15 unseen videos located in the same test location. The publicly provided test videos are simply a set of mock plays (copied from the training set) which are not used in scoring.</p>\n\n<p style=\"color:blue\">The dataset provided for this competition has been carefully designed for the purposes of training computer vision models and therefore contains plays that have much higher incidence of helmet impacts than is normal. This dataset should not be used to make inferences about the incidence of helmet impact rates during football games, as it is not a representative sample of those rates.</p>\n\nFiles\n[train/test] mp4 videos of each play. Each play has two copies, one shot from the endzone and the other shot from the sideline. The video pairs are matched frame for frame in time, but different players may be visible in each view. You only need to make predictions for the view that a player is actually visible in."},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_plot(df, col, top_most=50, title=None, is_top=True):\n     \n    if not is_top:\n        temp = df[col].astype(\"str\").value_counts(ascending=True).to_frame().reset_index().head(top_most)\n    else:\n        temp = df[col].astype(\"str\").value_counts().to_frame().reset_index().head(top_most)\n    \n    temp.columns = [col,'count']\n    \n    plt.figure(figsize=(6, 10))\n    sns.barplot(\"count\", col, data=temp, orient=\"h\", order=temp[col].values.tolist())\n    plt.show()\n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\n\nimport cv2\nimport imageio\nimport subprocess\nfrom PIL import Image\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as ptc\n\nfrom IPython.display import Video, display\n\nsns.set_style(\"whitegrid\")\ncolorpal = sns.color_palette(\"husl\", 9)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\nplt.rcParams['figure.dpi'] = 150\nplt.rcParams['figure.figsize'] = 12, 8","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"image_path = \"../input/nfl-impact-detection/images\"\ntrain_videos = \"../input/nfl-impact-detection/train\"\ntest_videos = \"../input/nfl-impact-detection/test\"\nimage_labels = \"../input/nfl-impact-detection/image_labels.csv\"\ntrain_labels = \"../input/nfl-impact-detection/train_labels.csv\"\ntrain_player_tracking = \"../input/nfl-impact-detection/train_player_tracking.csv\"\ntest_player_tracking = \"../input/nfl-impact-detection/test_player_tracking.csv\"\nsample_submissions = \"../input/nfl-impact-detection/sample_submission.csv\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Labels"},{"metadata":{},"cell_type":"markdown","source":"**Helmet tracking and collision labels for the training set.**\n\n- **gameKey:** the ID code for the game.\n\n- **playID:** the ID code for the play.\n\n- **view:** the camera orientation.\n\n- **video:** the filename of the associated video.\n\n- **frame:** the frame number for this play.\n\n- **label:** the associate player's number.\n\n- **[left/width/top/height]:** the specification of the bounding box of the prediction.\n\n- **impact:** an indicator (1 = helmet impact) for bounding boxes associated with helmet impacts\n\n- **impactType:** a description of the type of helmet impact: helmet, shoulder, body, ground, etc.\n\n- **confidence:** 1 = Possible, 2 = Definitive, 3 = Definitive and Obvious\n\n- **visibility:** 0 = Not Visible from View, 1 = Minimum, 2 = Visible, 3 = Clearly Visible\n\nFor the purposes of evaluation, definitive helmet impacts are defined as meeting three criteria:\n\n- `impact = 1`\n- `confidence > 1`\n- `visibility > 0` \n\nThose labels with confidence = 1 document cases in which human labelers asserted it was possible that a helmet impact occurred, but it was not clear that the helmet impact altered the trajectory of the helmet. Those labels with visibility = 0 indicate that although there is reason to believe that an impact occurred to that helmet at that time, the impact itself was not visible from the view."},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_labels = pd.read_csv(train_labels)\ntr_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_labels.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unique Videos\ntr_labels[\"video\"].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have `120` unique videos. Technically Yes, but as per the descirption, we have `60` videos each having `2` view or we can say being captured from `2` view, one is **EndZone** and another is **Sideline**. We can see that the last token of each video file's name depicts from which view it's been captured. We can confirm once."},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_labels[\"video\"].apply(lambda x: x[:12]).nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of unique values in each Column"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tr_labels.nunique().to_frame().rename(columns={0:\"count\"}).style.background_gradient(cmap=\"gnuplot\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 50 gameKeys"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"count_plot(tr_labels, \"gameKey\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 50 PlayIDs"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"count_plot(tr_labels, \"playID\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 50 Labels"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"count_plot(tr_labels, \"label\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Least 50 labels"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"count_plot(tr_labels, \"label\", is_top=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top 50 Videos"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_plot(tr_labels, \"video\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 50 Rare Videos"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_plot(tr_labels, \"video\", is_top=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\nsns.distplot(tr_labels[\"gameKey\"].value_counts(), ax=ax[0, 0], rug=True, color=\"red\")\nax[0, 0].set_title(\"Game Counts\")\nsns.distplot(tr_labels[\"playID\"].value_counts(), ax=ax[0, 1], rug=True, color=\"blue\")\nax[0, 1].set_title(\"Play Counts\")\nsns.distplot(tr_labels[\"label\"].value_counts(), ax=ax[1, 0], rug=True, color=\"green\")\nax[1, 0].set_title(\"Labels Counts\")\nsns.distplot(tr_labels[\"video\"].value_counts(), ax=ax[1, 1], rug=True, color=\"yellow\")\nax[1, 1].set_title(\"Videos Counts\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"_ = sns.catplot(x=\"impactType\", hue=\"visibility\", col=\"view\",\n                data=tr_labels, kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"_ = sns.catplot(x=\"impactType\", hue=\"confidence\", col=\"view\",\n                data=tr_labels, kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"_ = sns.catplot(x=\"view\", hue=\"impactType\", col=\"confidence\",\n                data=tr_labels, kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"_ = sns.catplot(x=\"view\", hue=\"impactType\", col=\"visibility\",\n                data=tr_labels, kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Labels"},{"metadata":{},"cell_type":"markdown","source":"\n**Contains the bounding boxes corresponding to the images.**\n\n- **image:** the image file name.\n\n- **label:** the label type.\n\n- **[left/width/top/height]:** the specification of the bounding box of the label, with left=0 and top=0 being the top left corner."},{"metadata":{"trusted":true},"cell_type":"code","source":"img_labels = pd.read_csv(image_labels)\nimg_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_labels.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.catplot(x=\"label\", data=img_labels, kind=\"count\")\nplt.gcf().set_size_inches(20, 8)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# take a sample image\nridx = np.random.randint(0, len(os.listdir(image_path)))\nimg_fn = os.listdir(image_path)[ridx]\nprint(\"Image: \", img_fn)\nimg_sample = Image.open(os.path.join(image_path, img_fn))\n\nplt.imshow(img_sample)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def add_img_boxes(image_name, image_labels=img_labels):\n    # Set label colors for bounding boxes\n    _, ax = plt.subplots(1)\n    \n    boxes = img_labels.loc[img_labels['image'] == image_name]\n    \n    for j, box in boxes.iterrows():\n        if box.label==\"Helmet\":\n            edc = \"blue\"\n        elif box.label==\"Helmet-Blurred\":\n            edc = \"orange\"\n        elif box.label==\"Helmet-Difficult\":\n            edc = \"green\"\n        elif box.label==\"Helmet-Sideline\":\n            edc = \"red\"\n        else:\n            edc = \"purple\"\n        \n        patch = ptc.Rectangle((box.left, box.top), width=box.width, height=box.height, fill=False, edgecolor=edc)\n        ax.text(box.left, box.top, box.label, fontsize=8, bbox=dict(facecolor=edc, alpha=0.1))\n        ax.add_patch(patch)\n        \n    # Display the image with bounding boxes added\n    ax.imshow(img_sample)\n    ax.set_title(f\"{image_name} with bounded boxes\")\n    plt.show()\n    \nadd_img_boxes(img_fn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Player Tracking"},{"metadata":{},"cell_type":"markdown","source":"**Each player wears a sensor that allows us to precisely locate them on the field; that information is reported in these two files.**\n\n- **gameKey:** the ID code for the game.\n\n- **playID:** the ID code for the play.\n\n- **player:** the player's ID code.\n\n- **time:** timestamp at **10 Hz**.\n\n- **x:** player position along the long axis of the field. See figure below.\n\n- **y:** player position along the short axis of the field. See figure below.\n\n- **s:** speed in yards/second.\n\n- **a:** acceleration in yards/second^2.\n\n- **dis:** distance traveled from prior time point, in yards.\n\n- **o:** orientation of player (deg).\n\n- **dir:** angle of player motion (deg).\n\n- **event:** game events like a snap, whistle, etc."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3258%2F820e86013d48faacf33b7a32a15e814c%2FIncreasing%20Dir%20and%20O.png?generation=1572285857588233&alt=media\">"},{"metadata":{},"cell_type":"markdown","source":"## Train Players tracking"},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_track = pd.read_csv(train_player_tracking)\ntr_track","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Players tracking"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts_track = pd.read_csv(test_player_tracking)\nts_track","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Videos Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a sample image\nridx = np.random.randint(0, len(os.listdir(train_videos)))\nvid_fn = os.listdir(train_videos)[ridx]\nprint(\"Video: \", vid_fn)\n\ndisplay(Video(data=os.path.join(train_videos, vid_fn), embed=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Have a look at sample submission!"},{"metadata":{},"cell_type":"markdown","source":"**A valid sample submission file.**\n\n- **gameKey:** the ID code for the game.\n\n- **playID:** the ID code for the play.\n\n- **view:** the camera orientation.\n\n- **video:** the filename of the associated video.\n\n- **frame:** the frame number for this play.\n\n- **[left/width/top/height]:** the specification of the bounding box of the prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"ss = pd.read_csv(sample_submissions)\nss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As said, we need to predict a single bouding box for the givenn frame. The bouding boxe is represented by `left`, `width`, `top` and `height`."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}