{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n**Main Topic**\n\nThis notebook is for **Detecting Impact using [MMdetection](https://github.com/open-mmlab/mmdetection) without internet** using **train_labels** and **[nfl-frame](https://www.kaggle.com/jinssaa/nfl-frame)**\n\nI refered idea from [2Class Object Detection Training](https://www.kaggle.com/its7171/2class-object-detection-training) by [@tito](https://www.kaggle.com/its7171)\n\nI also used two classes(Impact, Helmet).\n\n**References**\n\n**Chris Deotte, Grand master** : [How to Install Without Internet](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/113195)\n\n**tito, Grand master** : [2Class Object Detection Training](https://www.kaggle.com/its7171/2class-object-detection-training)\n\n[**MMdetection Official Documents**](https://mmdetection.readthedocs.io/)\n"},{"metadata":{},"cell_type":"markdown","source":"# Install MMdetection from scratch\n\n**Version info.**\n\n- MMdetection 2.6.0\n- mmcv-full 1.2.0, torch 1.6, cu102\n\nBecause this Competetion is [Notebook Competetion](https://www.kaggle.com/docs/competitions#notebooks-only-FAQ), we need to inference .mp4 video without interent. \n\nSo I made `*.whl` files to install MMdetection, mmcv-full without internet. you can use this files from [mmdetection-v2.6.0 dataset](https://www.kaggle.com/jinssaa/mmdetectionv260).\n\n- note I think we don't need to `train` without internet so It's better to set local env if you want. this step for `inference` using weight."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"! pip install ../input/mmdetectionv260/addict-2.4.0-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install ../input/mmdetectionv260/mmcv_full-latesttorch1.6.0cu102-cp37-cp37m-manylinux1_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install ../input/mmdetectionv260/mmpycocotools-12.0.3-cp37-cp37m-linux_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install ../input/mmdetectionv260/mmdet-2.6.0-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Set up environment"},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy\nimport json\nimport os.path as osp\nfrom glob import glob\nfrom tqdm import tqdm\n\n# Check Pytorch installation\nimport torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\n\nfrom sklearn.model_selection import train_test_split\n\n# Check MMDetection installation\nimport mmdet\nprint(mmdet.__version__)\n\n# Check mmcv installation\nimport mmcv\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nfrom mmcv import Config\nprint(get_compiling_cuda_version())\nprint(get_compiler_version())\n\nfrom mmdet.datasets import build_dataset, CocoDataset\nfrom mmdet.models import build_detector\nfrom mmdet.datasets.builder import DATASETS\nfrom mmdet.datasets.custom import CustomDataset\nfrom mmdet.apis import train_detector, set_random_seed, init_detector, inference_detector\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nimport nflimpact","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preparation\n\nI refered [2Class Object Detection Training](https://www.kaggle.com/its7171/2class-object-detection-training) by [@tito](https://www.kaggle.com/its7171)\n\nI set +-4 frames from impact as a `impact class`\n\nthere are two classes\n\n* impact : 1 --> impact\n* impact : 2 --> helmet"},{"metadata":{"trusted":true},"cell_type":"code","source":"video_labels = pd.read_csv('../input/nfl-impact-detection/train_labels.csv')\nvideo_labels_with_impact = video_labels[video_labels['impact'] > 0]\nfor row in tqdm(video_labels_with_impact[['video','frame','label']].values):\n    frames = np.array([-4, -3, -2,-1, 1,2, 3, 4])+row[1]\n    video_labels.loc[(video_labels['video'] == row[0]) \n                                 & (video_labels['frame'].isin(frames))\n                                 & (video_labels['label'] == row[2]), 'impact'] = 1\nvideo_labels['image_name'] = video_labels['video'].str.replace('.mp4', '') + '_' + video_labels['frame'].astype(str) + '.jpg'\nvideo_labels = video_labels[video_labels.groupby('image_name')['impact'].transform(\"sum\") > 0].reset_index(drop=True)\nvideo_labels.fillna({'impact': 2}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = video_labels.copy()\ntrain_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_image_lists = list(train_labels['image_name'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split train validation dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images, valid_images = train_test_split(unique_image_lists, test_size = 0.05, random_state=42)\ntrain_isin_filter = train_labels['image_name'].isin(train_images)\nvalid_isin_filter = train_labels['image_name'].isin(valid_images)\n\ntrain_df = train_labels[train_isin_filter]\nvalid_df = train_labels[valid_isin_filter]\ntrain_df = train_df.reset_index(drop=True)\nvalid_df = valid_df.reset_index(drop=True)\nprint(f'train labels: {len(train_df)}, valid labels {len(valid_df)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate COCO Format Json\n\nTo use MMdetection tool box, we need to set-up COCO Format Json for our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_classes(CLASSES):\n    classes = list()      \n    \n    for i, CLASS in enumerate(CLASSES):\n        single_class = {} \n        single_class['id'] = i + 1\n        single_class['name'] = CLASS\n        classes.append(single_class)\n    return classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_objs(df, debug = False):\n    \n    if debug:\n        \n        df = df[:int(len(df)*0.05)]\n        \n    \n    img_lists = list(df['image_name'].unique())\n    imgs = list()\n    objs = list()   \n    \n    ## gen images information\n    for i in tqdm(range(len(img_lists))):\n        '''\n        \n        I just notice that all images were preprocessed image size 720x1280\n        If you want to check real image size, use this code below\n        \n        img = cv2.imread(os.path.join(data_path, img_lists[i]))\n        \n        single_img_obj = {}\n        single_img_obj['file_name'] = img_lists[i]\n        single_img_obj['height'] = img.shape[0]\n        single_img_obj['width'] = img.shape[1]\n        single_img_obj['id'] = i + 1\n        '''\n        \n        single_img_obj = {}\n        single_img_obj['file_name'] = img_lists[i]\n        single_img_obj['height'] = 720 \n        single_img_obj['width'] = 1280\n        single_img_obj['id'] = i + 1        \n        \n        imgs.append(single_img_obj)\n        \n  \n    ## gen objs information    \n    for j in tqdm(range(len(df))):\n        single_obj = {}\n        single_obj['id'] = j + 1\n        single_obj['image_id'] = img_lists.index(df['image_name'][j]) + 1\n        single_obj['category_id'] = int(df['impact'][j] ) ## You need to customize if you want to add some 'impact' class\n        single_obj['area'] = float(df['width'][j]*df['height'][j])\n        single_obj['bbox'] = [int(df['left'][j]), int(df['top'][j]), int(df['width'][j]), int(df['height'][j])]\n        single_obj['iscrowd'] = 0        \n        \n        objs.append(single_obj)\n    \n    print(f'images: {len(imgs)}, objs: {len(objs)}')\n    \n    return imgs, objs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_coco(outpath, classes, objs, imgs, train=True):\n    if train:\n        data_dict = {}\n        data_dict['images'] = []\n        data_dict['annotations'] = []\n        data_dict['categories'] = []\n        data_dict['images'].extend(imgs)\n        data_dict['annotations'].extend(objs)\n        data_dict['categories'].extend(classes)\n        \n    else:\n        data_dict = {}\n        data_dict['images'] = []\n        data_dict['categories'] = []\n        \n        data_dict['images'].extend(imgs)\n        data_dict['categories'].extend(classes)   \n\n    with open(outpath, 'w') as f_out:\n        json.dump(data_dict, f_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CLASSES = ['impact', 'helmet']\nclasses = gen_classes(CLASSES)\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_imgs, train_objs = gen_objs(train_df)\nvalid_imgs, valid_objs = gen_objs(valid_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_coco('train.json', classes, train_objs, train_imgs, train=True)\ngen_coco('valid.json', classes, valid_objs, valid_imgs, train=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Set Dataset\n\nThis class is from [MMdetection COCO.py](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/datasets/coco.py)\n\n\nI just set `CLASSES` only `impact`, `helmet`, So you can customize if you want.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"@DATASETS.register_module()\nclass ImpactDataset(CocoDataset):\n    CLASSES = set(CLASSES)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model build\nIn this notebook, I used cascade_rcnn_r50_fpn for baseline. I trained 9epochs and I got a `` public LB score.\nconfig_file is from [nfl_baseline_cascade_rcnn dataset](https://www.kaggle.com/jinssaa/nflbaselinecascadercnn) which I made for this Notebook.\n\nI saved 9epochs pretrained model with configs"},{"metadata":{"trusted":true},"cell_type":"code","source":"config_file = '../input/nflbaselinecascadercnn/cascade_rcnn_r50_fpn.py'\ncfg = Config.fromfile(config_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg.total_epochs = 1\ncfg.work_dir = './'\ncfg.seed = 0\nset_random_seed(0, deterministic=False)\ncfg.gpu_ids = range(1)\ncfg.data_root = '../input/nfl-frame/'\n\ncfg.data.train.ann_file='./train.json'\ncfg.data.train.img_prefix= cfg.data_root\n\ncfg.data.val.ann_file='./valid.json',\ncfg.data.val.img_prefix= cfg.data_root\n\ncfg.data.test.ann_file='./valid.json', ## I just set validation data for test because it's not our main purpose\ncfg.data.test.img_prefix= cfg.data_root","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can check config"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Config:\\n{cfg.pretty_text}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build dataset\ndatasets = [build_dataset(cfg.data.train)]\n\n# Build the detector\nmodel = build_detector(\n    cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n# Add an attribute for visualization convenience\nmodel.CLASSES = datasets[0].CLASSES\n\n# Create work_dir\nmmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\ntrain_detector(model, datasets, cfg, distributed=False, validate=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test video\n\nLet's test video and visualize"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\n\n# Specify the path to model config and checkpoint file\ncheckpoint_file = '../input/nflbaselinecascadercnn/epoch_9.pth' # 10 epochs\n\n# build the model from a config file and a checkpoint file\nmodel = init_detector(config_file, checkpoint_file, device='cuda:0')\n\n# test a video and show the results\nvideo = mmcv.VideoReader('../input/nfl-impact-detection/test/57906_000718_Endzone.mp4')\n\nims = []\n\nfor frame in video:\n    result = inference_detector(model, frame)\n    single_img = model.show_result(frame, result, wait_time=1)\n    im = plt.imshow(single_img, animated=True)\n    ims.append([im])\n    \nani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n                                repeat_delay=1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Display video\n\nSorry for the low resolution. please let's me know if there a way to improve video rendering resolution !"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(ani.to_html5_video()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}