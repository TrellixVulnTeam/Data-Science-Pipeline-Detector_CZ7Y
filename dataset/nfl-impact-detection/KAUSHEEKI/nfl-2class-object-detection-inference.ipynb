{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook detects 2 class objects.\n- class1: helmet without impact\n- class2: helmet with impact\n\nObject Detection part is based on [EfficientDet notebook](https://www.kaggle.com/shonenkov/training-efficientdet) for [global wheat detection competition](https://www.kaggle.com/c/global-wheat-detection) by [shonenkov](https://www.kaggle.com/shonenkov), which is using [github repos efficientdet-pytorch](https://github.com/rwightman/efficientdet-pytorch) by [@rwightman](https://www.kaggle.com/rwightman).\n\nTraining part can be foud [here](https://www.kaggle.com/its7171/2class-object-detection-training/).","metadata":{}},{"cell_type":"code","source":"!pip install ../input/nfl-lib/timm-0.1.26-py3-none-any.whl\n!tar xfz ../input/nfl-lib/pkgs.tgz\n# for pytorch1.6\ncmd = \"sed -i -e 's/ \\/ / \\/\\/ /' timm-efficientdet-pytorch/effdet/bench.py\"\n!$cmd","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-27T11:44:19.505109Z","iopub.execute_input":"2021-08-27T11:44:19.505518Z","iopub.status.idle":"2021-08-27T11:44:51.221752Z","shell.execute_reply.started":"2021-08-27T11:44:19.505485Z","shell.execute_reply":"2021-08-27T11:44:51.220349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.insert(0, \"timm-efficientdet-pytorch\")\nsys.path.insert(0, \"omegaconf\")\n\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\nimport pandas as pd\nimport gc\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchEval\nfrom effdet.efficientdet import HeadNet\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nDATA_ROOT_PATH = 'test_images'\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseed_everything(SEED)","metadata":{"incorrectly_encoded_metadata":"_kg_hide-input=true _kg_hide-output=true","execution":{"iopub.status.busy":"2021-08-27T11:44:57.388838Z","iopub.execute_input":"2021-08-27T11:44:57.389254Z","iopub.status.idle":"2021-08-27T11:45:01.304919Z","shell.execute_reply.started":"2021-08-27T11:44:57.389205Z","shell.execute_reply":"2021-08-27T11:45:01.303959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mk_images(video_name, video_labels, video_dir, out_dir, only_with_impact=True):\n    video_path=f\"{video_dir}/{video_name}\"\n    video_name = os.path.basename(video_path)\n    vidcap = cv2.VideoCapture(video_path)\n    if only_with_impact:\n        boxes_all = video_labels.query(\"video == @video_name\")\n        print(video_path, boxes_all[boxes_all.impact == 1.0].shape[0])\n    else:\n        print(video_path)\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        frame += 1\n        if only_with_impact:\n            boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n            boxes_with_impact = boxes[boxes.impact == 1.0]\n            if boxes_with_impact.shape[0] == 0:\n                continue\n        img_name = f\"{video_name}_frame{frame}\"\n        image_path = f'{out_dir}/{video_name}'.replace('.mp4',f'_{frame}.png')\n        _ = cv2.imwrite(image_path, img)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:45:07.752119Z","iopub.execute_input":"2021-08-27T11:45:07.75247Z","iopub.status.idle":"2021-08-27T11:45:07.765152Z","shell.execute_reply.started":"2021-08-27T11:45:07.752439Z","shell.execute_reply":"2021-08-27T11:45:07.763699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_dir = DATA_ROOT_PATH\nif not os.path.exists(out_dir):\n    !mkdir -p $out_dir\n    video_dir = '/kaggle/input/nfl-impact-detection/test'\n    uniq_video = [path.split('/')[-1] for path in glob(f'{video_dir}/*.mp4')]\n    for video_name in uniq_video:\n        mk_images(video_name, pd.DataFrame(), video_dir, out_dir, only_with_impact=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:45:21.401581Z","iopub.execute_input":"2021-08-27T11:45:21.401976Z","iopub.status.idle":"2021-08-27T11:47:44.476903Z","shell.execute_reply.started":"2021-08-27T11:45:21.401942Z","shell.execute_reply":"2021-08-27T11:47:44.475825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_valid_transforms():\n    return A.Compose([\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:48:12.132329Z","iopub.execute_input":"2021-08-27T11:48:12.132759Z","iopub.status.idle":"2021-08-27T11:48:12.138725Z","shell.execute_reply.started":"2021-08-27T11:48:12.132727Z","shell.execute_reply":"2021-08-27T11:48:12.137355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self, image_ids, transforms=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:48:19.464679Z","iopub.execute_input":"2021-08-27T11:48:19.46505Z","iopub.status.idle":"2021-08-27T11:48:19.476161Z","shell.execute_reply.started":"2021-08-27T11:48:19.465016Z","shell.execute_reply":"2021-08-27T11:48:19.474698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_net(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n    config.num_classes = 2\n    config.image_size=512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\n\nnet = load_net('../input/nfl-models//best-checkpoint-002epoch.bin')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:48:26.914981Z","iopub.execute_input":"2021-08-27T11:48:26.915372Z","iopub.status.idle":"2021-08-27T11:48:39.7945Z","shell.execute_reply.started":"2021-08-27T11:48:26.91534Z","shell.execute_reply":"2021-08-27T11:48:39.793211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = DatasetRetriever(\n    image_ids=np.array([path.split('/')[-1] for path in glob(f'{DATA_ROOT_PATH}/*.png')]),\n    transforms=get_valid_transforms()\n)\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader = DataLoader(\n    dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:48:46.832074Z","iopub.execute_input":"2021-08-27T11:48:46.832479Z","iopub.status.idle":"2021-08-27T11:48:46.854447Z","shell.execute_reply.started":"2021-08-27T11:48:46.832446Z","shell.execute_reply":"2021-08-27T11:48:46.853187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(images, score_threshold=0.5):\n    images = torch.stack(images).cuda().float()\n    box_list = []\n    score_list = []\n    with torch.no_grad():\n        det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n        for i in range(images.shape[0]):\n            boxes = det[i].detach().cpu().numpy()[:,:4]    \n            scores = det[i].detach().cpu().numpy()[:,4]   \n            label = det[i].detach().cpu().numpy()[:,5]\n            # useing only label = 2\n            indexes = np.where((scores > score_threshold) & (label == 2))[0]\n            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n            box_list.append(boxes[indexes])\n            score_list.append(scores[indexes])\n    return box_list, score_list\nimport matplotlib.pyplot as plt","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-27T11:48:57.966852Z","iopub.execute_input":"2021-08-27T11:48:57.967265Z","iopub.status.idle":"2021-08-27T11:48:57.983525Z","shell.execute_reply.started":"2021-08-27T11:48:57.967232Z","shell.execute_reply":"2021-08-27T11:48:57.98198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check prediction\n\ncnt = 0\nfor images, image_ids in data_loader:\n    box_list, score_list = make_predictions(images, score_threshold=0.4)\n    for i in range(len(images)):\n        sample = images[i].permute(1,2,0).cpu().numpy()\n        boxes = box_list[i].astype(np.int32).clip(min=0, max=511)\n        scores = score_list[i]\n        if len(scores) >= 1:\n            fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n            sample = cv2.resize(sample , (int(1280), int(720)))\n            for box,score in zip(boxes,scores):\n                box[0] = box[0] * 1280 / 512\n                box[1] = box[1] * 720 / 512\n                box[2] = box[2] * 1280 / 512\n                box[3] = box[3] * 720 / 512\n                cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (1, 0, 0), 3)\n            ax.set_axis_off()\n            ax.imshow(sample);\n            cnt += 1\n    if cnt >= 10:\n        break","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:49:04.844952Z","iopub.execute_input":"2021-08-27T11:49:04.845325Z","iopub.status.idle":"2021-08-27T11:49:19.57117Z","shell.execute_reply.started":"2021-08-27T11:49:04.845293Z","shell.execute_reply":"2021-08-27T11:49:19.569746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_image_ids = []\nresults_boxes = []\nresults_scores = []\nfor images, image_ids in data_loader:\n    box_list, score_list = make_predictions(images, score_threshold=0.4)\n    for i, image in enumerate(images):\n        boxes = box_list[i]\n        scores = score_list[i]\n        image_id = image_ids[i]\n        boxes[:, 0] = (boxes[:, 0] * 1280 / 512)\n        boxes[:, 1] = (boxes[:, 1] * 720 / 512)\n        boxes[:, 2] = (boxes[:, 2] * 1280 / 512)\n        boxes[:, 3] = (boxes[:, 3] * 720 / 512)\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        boxes = boxes.astype(np.int32)\n        boxes[:, 0] = boxes[:, 0].clip(min=0, max=1280-1)\n        boxes[:, 2] = boxes[:, 2].clip(min=0, max=1280-1)\n        boxes[:, 1] = boxes[:, 1].clip(min=0, max=720-1)\n        boxes[:, 3] = boxes[:, 3].clip(min=0, max=720-1)\n        result_image_ids += [image_id]*len(boxes)\n        results_boxes.append(boxes)\n        results_scores.append(scores)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:56:08.619721Z","iopub.execute_input":"2021-08-27T11:56:08.620124Z","iopub.status.idle":"2021-08-27T11:56:08.798582Z","shell.execute_reply.started":"2021-08-27T11:56:08.620089Z","shell.execute_reply":"2021-08-27T11:56:08.795748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"box_df = pd.DataFrame(np.concatenate(results_boxes), columns=['left', 'top', 'width', 'height'])\ntest_df = pd.DataFrame({'scores':np.concatenate(results_scores), 'image_name':result_image_ids})\ntest_df = pd.concat([test_df, box_df], axis=1)\n\ntest_df = test_df[test_df.scores > 0.3]\ntest_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:52:16.457811Z","iopub.execute_input":"2021-08-27T11:52:16.458203Z","iopub.status.idle":"2021-08-27T11:52:16.506134Z","shell.execute_reply.started":"2021-08-27T11:52:16.45816Z","shell.execute_reply":"2021-08-27T11:52:16.504597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#gameKey,playID,view,video,frame,left,width,top,height\n#57590,3607,Endzone,57590_003607_Endzone.mp4,1,1,1,1,1\ntest_df['gameKey'] = test_df.image_name.str.split('_').str[0].astype(int)\ntest_df['playID'] = test_df.image_name.str.split('_').str[1].astype(int)\ntest_df['view'] = test_df.image_name.str.split('_').str[2]\ntest_df['frame'] = test_df.image_name.str.split('_').str[3].str.replace('.png','').astype(int)\ntest_df['video'] = test_df.image_name.str.rsplit('_',1).str[0] + '.mp4'\ntest_df = test_df[[\"gameKey\",\"playID\",\"view\",\"video\",\"frame\",\"left\",\"width\",\"top\",\"height\"]]\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:54:22.241965Z","iopub.execute_input":"2021-08-27T11:54:22.242474Z","iopub.status.idle":"2021-08-27T11:54:22.295085Z","shell.execute_reply.started":"2021-08-27T11:54:22.242436Z","shell.execute_reply":"2021-08-27T11:54:22.293776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clearing working dir\n# be careful when running this code on local environment!\n# !rm -rf *\n!mv * /tmp/","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-08-27T11:55:49.192746Z","iopub.execute_input":"2021-08-27T11:55:49.193225Z","iopub.status.idle":"2021-08-27T11:55:50.016594Z","shell.execute_reply.started":"2021-08-27T11:55:49.19319Z","shell.execute_reply":"2021-08-27T11:55:50.015265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nflimpact\nenv = nflimpact.make_env()\nenv.predict(test_df) # df is a pandas dataframe of your entire submission file","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:55:44.644204Z","iopub.execute_input":"2021-08-27T11:55:44.644612Z","iopub.status.idle":"2021-08-27T11:55:44.828464Z","shell.execute_reply.started":"2021-08-27T11:55:44.644563Z","shell.execute_reply":"2021-08-27T11:55:44.826286Z"},"trusted":true},"execution_count":null,"outputs":[]}]}