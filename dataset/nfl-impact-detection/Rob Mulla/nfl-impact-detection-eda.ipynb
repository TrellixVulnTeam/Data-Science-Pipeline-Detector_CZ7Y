{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NFL 1st & Future 2021\n\n\nIn this competition we are provided video clips of NFL plays along with player tracking data. Our goal is to create a model that can produce bounding boxes around players helmets, and identify when collisions occur. In addition to the video clips, we are also given some images with bounding boxes identified to help aid our object detection algorithm.\n\nThis competition is evaluated using a micro F1 score at an Intersection over Union (IoU) threshold of 0.35."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pylab as plt\nimport matplotlib.patches as patches\n\nimport imageio\nimport cv2\nimport subprocess\n\nfrom IPython.display import Video, display\nimport os\n\nsns.set_style(\"whitegrid\")\ncolorpal = sns.color_palette(\"husl\", 9)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Overview\n- `train_labels.csv` - Helmet tracking and collision labels for the training set.\n- `sample_submission.csv` -  A valid sample submission file.\n- `image_labels.csv` - contains the bounding boxes corresponding to the images.\n- `[train/test]_player_tracking.csv` - Each player wears a sensor that allows us to precisely locate them on the field; that information is reported in these two files.\n\nFolders:\n- `/train/` contains the mp4 video files for the training plays. Each play has both an `endzone` and `sideline` view.\n- `/test/` contains the videos for the test set. In the public dataset you only see 2 videos but these are just examples and are actually already in the training set. When your model actually submitted it will run on 15 unseen videos. We are told that 20% of the test videos will product the public LB score, and 80% will produce the private score (3 plays public LB, 12 private).. so there may be some shakeup on the private leaderboard!\n- `/images/` contains the additional annotated images of player helmets."},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/nfl-impact-detection/ -GFlash --color","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"tr_labels = pd.read_csv('../input/nfl-impact-detection/train_labels.csv')\nimg_labels = pd.read_csv('../input/nfl-impact-detection/image_labels.csv')\nss = pd.read_csv('../input/nfl-impact-detection/sample_submission.csv')\n\ntr_tracking = pd.read_csv('../input/nfl-impact-detection/train_player_tracking.csv')\nte_tracking = pd.read_csv('../input/nfl-impact-detection/test_player_tracking.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Understanding the Label and Metric\n- In our submission we are told \"Each row in your submission represents a single predicted bounding box for the given frame. Note that it is not required to include labels of which players had an impact, only a bounding box where it occurred.\"\n\nLets dig deeper into the labels to see some examples for the training data and then gather some statistics about the training set.\n\nWe can see that:\n- Training videos provided are sideline and endzone views for 60 plays, 120 videos in total."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of unique videos\ntr_labels['video'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The length of each play varies. Most plays are around 300 frames, but the longest is over 600 frames."},{"metadata":{"trusted":true},"cell_type":"code","source":"play_frame_count = tr_labels[['gameKey','playID','frame']] \\\n    .drop_duplicates()[['gameKey','playID']] \\\n    .value_counts()\n\nfig, ax = plt.subplots(figsize=(12, 5))\nsns.distplot(play_frame_count, bins=15)\nax.set_title('Distribution of frames per video file')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The bounding box size depends on a number of factors:\n- The ditance of a player from the camera.\n- The camera's angle and zoom relative to the field.\n- One player's helmet may be blocked from view by another player.\n\nWe can calculate the area of each bounding box for the training set by: `width x height`"},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_labels['area'] = tr_labels['width'] * tr_labels['height']\nfig, ax = plt.subplots(figsize=(12, 5))\nsns.distplot(tr_labels['area'].value_counts(),\n             bins=10,\n             color=colorpal[1])\nax.set_title('Distribution bounding box sizes')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_labels['label'].value_counts() \\\n    .sort_values() \\\n    .tail(25).plot(kind='barh',\n                   figsize=(15, 5),\n                   title='Top 25 Box Labels',\n                   color=colorpal[3])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The impacts are labeled by types: Helmet, shoudler, body, etc. We can see the the majority of impact types are with other helmets, but shoulder and body impacts do occur. Our submission does not need to identify the impact type, but it may be helpful information when training models."},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_labels['impactType'].value_counts() \\\n    .plot(kind='bar',\n          title='Impact Type Count',\n          figsize=(12, 4),\n          color=colorpal[4])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Impact Type by Frame\nThis plot shows the relationship between the impact \"label\" and the time within the video. Notice that helmet and shoulder impacts tend to occur earlier in the plays. Body impact next, followed by ground impacts which commonly occur near the middle/end of the play."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, d in tr_labels.groupby('impactType'):\n    if len(d) < 10:\n        continue\n    d['frame'].plot(kind='kde', alpha=1, figsize=(12, 4), label=i,\n                    title='Impact Type by Frame')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Impact Occurance\nNext we will look at the occurance of impact events in the training videos. These events are extremely rare. Out of every 1000 bounding boxes, roughly 2.3 of them involve an impact."},{"metadata":{"trusted":true},"cell_type":"code","source":"pct_impact_occurance = tr_labels[['video','impact']] \\\n    .fillna(0)['impact'].mean() * 100\nprint(f'Of all bounding boxes, {pct_impact_occurance:0.4f}% of them involve an impact event')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also look at the impact percentage by frame in the video."},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_labels[['video','impact','frame']] \\\n    .fillna(0) \\\n    .groupby(['frame']).mean() \\\n    .plot(figsize=(12, 5), title='Occurance of impacts by frame in video.',\n         color=colorpal[6])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pairplot of Bounding Box, Impact vs Non-Impact\nThese plots attempt to quickly identify if there is any commonality between the location of the bounding box and where impacts occur. It appears that the locations tend to be "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(tr_labels[['frame','area',\n                        'left','width',\n                        'top','height',\n                        'impact']] \\\n                .sample(5000).fillna(0),\n             hue='impact')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly we can look at the impact type by bounding box location and area."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(tr_labels[['frame','area',\n                        'left', 'top',\n                        'impactType']].dropna() \\\n             .sample(1000), hue='impactType',\n            plot_kws={'alpha': 0.5})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confidence Label\n1 = Possible, 2 = Definitive, 3 = Definitive and Obvious"},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_labels['confidence'].dropna() \\\n    .astype('int').value_counts() \\\n    .plot(kind='bar',\n          title='Confidence Type Label Count',\n          figsize=(12, 4),\n          color=colorpal[5], rot=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visability Label\nVisibility labels are: 0 = Not Visible from View, 1 = Minimum, 2 = Visible, 3 = Clearly Visible"},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_labels['visibility'].dropna() \\\n    .astype('int').value_counts() \\\n    .plot(kind='bar',\n          title='Visibility Label Count',\n          figsize=(12, 4),\n          color=colorpal[6], rot=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Images\n\nThe images are still photo equivalents of the train/test videos for use making a helmet detector. Lets load an image and plot the image label on it.\n\nFirst we will create a function that highlights the labels on the images, then we can plot 8 example images to get a good understanding of what they look like."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_example_image(img_fn, ax, highlight_color='r', highlight_alpha=0.5):\n    img_data = cv2.imread(f'../input/nfl-impact-detection/images/{img_fn}')\n    ax.imshow(img_data)\n    ax.grid(False)\n\n    # Create a Rectangle patch\n    for i, d in img_labels.loc[img_labels['image'] == img_fn].iterrows():\n\n        rect = patches.Rectangle((d['left'],\n                                  d['top']),\n                                  d['width'],\n                                 d['height'],\n                                 linewidth=1,\n                                 edgecolor=highlight_color,\n                                 facecolor=highlight_color,\n                                alpha=highlight_alpha)\n        ax.add_patch(rect)\n    ax.axis('off')\n    ax.set_title(img_fn)\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loop through 8 example images\nfig, axs = plt.subplots(4, 2, figsize=(14, 16))\naxs = axs.flatten()\ni = 0\nfor example_image in img_labels.sample(8, random_state=999)['image']:\n    plot_example_image(example_image, axs[i])\n    i += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training videos\n\nWe can pull an example video to see what they look like."},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_labels.dropna().iloc[50]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets read the first example's video file."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Modified function from to take single frame.\n# https://www.kaggle.com/samhuddleston/nfl-1st-and-future-getting-started\ndef annotate_frame(video_path: str, video_labels: pd.DataFrame, stop_frame: int) -> str:\n    VIDEO_CODEC = \"MP4V\"\n    HELMET_COLOR = (0, 0, 0)    # Black\n    IMPACT_COLOR = (0, 0, 255)  # Red\n    video_name = os.path.basename(video_path)\n    \n    vidcap = cv2.VideoCapture(video_path)\n    fps = vidcap.get(cv2.CAP_PROP_FPS)\n    width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    output_path = \"labeled_\" + video_name\n    tmp_output_path = \"tmp_\" + output_path\n    output_video = cv2.VideoWriter(tmp_output_path, cv2.VideoWriter_fourcc(*VIDEO_CODEC), fps, (width, height))\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        \n        # We need to add 1 to the frame count to match the label frame index that starts at 1\n        frame += 1\n        if frame != stop_frame:\n            continue\n        \n        # Let's add a frame index to the video so we can track where we are\n        img_name = f\"{video_name}_frame{frame}\"\n        cv2.putText(img, img_name, (0, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, HELMET_COLOR, thickness=2)\n    \n        # Now, add the boxes\n        boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n        for box in boxes.itertuples(index=False):\n            if box.impact == 1 and box.confidence > 1 and box.visibility > 0:    # Filter for definitive head impacts and turn labels red\n                color, thickness = IMPACT_COLOR, 2\n            else:\n                color, thickness = HELMET_COLOR, 1\n            # Add a box around the helmet\n            cv2.rectangle(img, (box.left, box.top), (box.left + box.width, box.top + box.height), color, thickness=thickness)\n            cv2.putText(img, box.label, (box.left, max(0, box.top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, thickness=1)\n        output_video.write(img)\n    output_video.release()\n    \n    # Not all browsers support the codec, we will re-load the file at tmp_output_path and convert to a codec that is more broadly readable using ffmpeg\n    if os.path.exists(output_path):\n        os.remove(output_path)\n    subprocess.run([\"ffmpeg\", \"-i\", tmp_output_path, \"-crf\", \"18\", \"-preset\", \"veryfast\", \"-vcodec\", \"libx264\", output_path])\n    os.remove(tmp_output_path)\n    \n    return output_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"video_name = tr_labels.dropna().reset_index().iloc[50]['video']\nvideo_path = f\"/kaggle/input/nfl-impact-detection/train/{video_name}\"\ndisplay(Video(data=video_path, embed=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annotate_frame(video_path, video_labels=tr_labels,\n               stop_frame=tr_labels.dropna().iloc[50]['frame'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example Single Frame with annotations"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(Video(data='labeled_57584_000336_Endzone.mp4', embed=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How many boxes per frame?"},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_labels['counter'] = 1\ntr_labels.groupby(['frame'])['counter'] \\\n    .sum().plot(title='Bounding Boxes by Video Frame',\n                figsize=(15, 5))\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}