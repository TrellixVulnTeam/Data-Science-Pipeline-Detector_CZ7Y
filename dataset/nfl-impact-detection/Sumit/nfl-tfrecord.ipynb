{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-11-21T09:25:00.567183Z","iopub.status.busy":"2020-11-21T09:25:00.566441Z","iopub.status.idle":"2020-11-21T09:25:00.784308Z","shell.execute_reply":"2020-11-21T09:25:00.783617Z"},"papermill":{"duration":0.235755,"end_time":"2020-11-21T09:25:00.784423","exception":false,"start_time":"2020-11-21T09:25:00.548668","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport pandas as pd\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nRESIZE = 512\nIMG_QUALITY = 95\nDEBUG = False","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-21T09:34:13.797479Z","iopub.status.busy":"2020-11-21T09:34:13.791895Z","iopub.status.idle":"2020-11-21T09:38:26.7424Z","shell.execute_reply":"2020-11-21T09:38:26.741473Z"},"papermill":{"duration":253.037421,"end_time":"2020-11-21T09:38:26.74255","exception":false,"start_time":"2020-11-21T09:34:13.705129","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import math\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\ndef _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _int64_feature(value):\n  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef serialize_example(feature0, feature1, feature2):\n  feature = {\n      'image': _bytes_feature(feature0),\n      'id': _bytes_feature(feature1),\n      'class': _int64_feature(feature2)\n  }\n  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n  return example_proto.SerializeToString()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd /kaggle/working\n\nbase_path = '/kaggle/working'\ntrain_videos_path = \"/kaggle/input/nfl-impact-detection/train\"\n\ndf_labels = pd.read_csv(\"/kaggle/input/nfl-impact-detection/train_labels.csv\")\ndf_labels['impact'] = np.nan_to_num(df_labels['impact'].values)\ncontent_trainlist = \"\"\ncontent_testlist  = \"\"\n\nfor video_index, video_name in enumerate(os.listdir(train_videos_path)):\n    print(video_index)\n    cap = cv2.VideoCapture(os.path.join(train_videos_path,video_name))\n    if (cap.isOpened() == False):\n      print(\"Unable to read camera feed\")\n    frame_no = 0\n    \n    tf_filename = f'{video_name}.tfrec'\n    with tf.io.TFRecordWriter(tf_filename) as wf:\n        while(True):\n            frame_no += 1\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            frame_height, frame_width,_ = frame.shape\n\n            content = \"\"\n            df = df_labels.query(f\"frame=={frame_no}\")\n            df = df[df.video==video_name]\n            \n            img = cv2.resize(frame, (RESIZE, RESIZE))\n            # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  -> Fix:20201121\n            \n            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, IMG_QUALITY))[1].tostring()\n            img_name = str.encode(f\"{video_name}_{frame_no}\")\n            target = int(df.impact.max())\n            example = serialize_example(img, img_name, target)\n            wf.write(example)\n    cap.release()\n#     if video_index > 4:\n#         break","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.087516,"end_time":"2020-11-21T09:38:26.915731","exception":false,"start_time":"2020-11-21T09:38:26.828215","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### image checking"},{"metadata":{"execution":{"iopub.execute_input":"2020-11-21T09:38:27.107807Z","iopub.status.busy":"2020-11-21T09:38:27.096686Z","iopub.status.idle":"2020-11-21T09:38:27.119478Z","shell.execute_reply":"2020-11-21T09:38:27.118815Z"},"papermill":{"duration":0.118755,"end_time":"2020-11-21T09:38:27.119621","exception":false,"start_time":"2020-11-21T09:38:27.000866","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    return image\n\ndef parse_example(example):\n    LABELED_TFREC_FORMAT = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'id': tf.io.FixedLenFeature([], tf.string),\n        'class': tf.io.FixedLenFeature([], tf.int64)\n    }\n    \n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = example['id']\n    target = example['class']\n    return image, label, target\n\ndef display_one(image, title, target, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    plt.title(f'{title}: {target}')\n    return (subplot[0], subplot[1], subplot[2]+1)\n\ndef display_batch_of_images(databatch):\n    images, labels, targets = databatch\n    images = images.numpy()\n    labels = labels.numpy()\n    targets = targets.numpy()\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n    if targets is None:\n        targets = [None for _ in enumerate(targets)]\n        \n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)//rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.2\n    subplot=(rows, cols, 1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label, target) in enumerate(zip(images[:rows*cols], labels[:rows*cols], targets[:rows*cols])):\n        title = label\n        title = title.decode('utf-8')\n        correct = True\n        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one(image, title, target, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0.2, hspace=0.2)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-21T09:38:27.300004Z","iopub.status.busy":"2020-11-21T09:38:27.299254Z","iopub.status.idle":"2020-11-21T09:38:29.59264Z","shell.execute_reply":"2020-11-21T09:38:29.593188Z"},"papermill":{"duration":2.388565,"end_time":"2020-11-21T09:38:29.593334","exception":false,"start_time":"2020-11-21T09:38:27.204769","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"resize_file = \"58106_002918_Sideline.mp4.tfrec\"\ndataset = tf.data.TFRecordDataset([resize_file]).map(parse_example).batch(25)\ndata = iter(dataset)\ndisplay_batch_of_images(next(data))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}