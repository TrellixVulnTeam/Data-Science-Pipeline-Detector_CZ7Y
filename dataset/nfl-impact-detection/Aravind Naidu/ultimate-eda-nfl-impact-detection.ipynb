{"cells":[{"metadata":{},"cell_type":"markdown","source":"### This notebook walks you through the **Exploratory Data Analysis** and details about the data and the problem statement of this challege.\n\n### If you wish to see a much broader version of the same, talking about the approach on the challenge for check my other notebook:\n\n### [**Ultimate Guide to get started on NFL Impact Det.**](https://www.kaggle.com/aravindnaidu/ultimate-guide-to-get-started-on-nfl-impact-det)"},{"metadata":{},"cell_type":"markdown","source":"# 1. Understanding the Competition\nThis competition is part of the NFL’s annual 1st and Future Competition, which has been designed to spur innovation in athlete safety and performance.\n\nThe NFL is actively addressing the need for a computer vision system to detect on-field helmet impacts as part of the “Digital Athlete” platform, and the league is calling on Kagglers to help.\n\nIn this competition, it is expected to develop a computer vision model that automatically detects helmet impacts that occur on the field. The dataset is of more than one thousand definitive head impacts from thousands of game images, labelled video from the sidelines and end zones, and player tracking data.\n\nThe data also documents the position, speed, acceleration, and orientation for every player on the field during NFL games.\n\nThis competition is evaluated using a micro F1 score at an Intersection over Union (IoU) threshold of 0.35.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 2.Understanding Data\n\n### The dataset consists of three types of data:\n\n* **Image Data:**\n    Image Data consist of about 10,000 images and associated helmet labels. This is to be used for building a helmet detection system.\n\n* **Video Data:**\n    Video Data consists of 120 videos (60 plays) from both a sideline and endzone point of view for each play. It has been associated with helmet and helmet impact labels, which has to be used for building a helmet impact detection system.\n\n* **Tracking Data:**\n    Tracking data consists of tracking for all players in the provided 60 plays.\n\n\n### Data files:\n\n* **train_labels.csv** - Helmet tracking and collision labels for the training set.\n* **sample_submission.csv** - A valid sample submission file.\n* **image_labels.csv** - contains the bounding boxes corresponding to the images.\n* **[train/test]_player_tracking.csv** - Each player wears a sensor that allows us to precisely locate them on the field.\n\n\n\n### Folders:\n* **/train/** contains the mp4 video files for the training plays. \n  (Both an endzone and sideline view.)\n    \n* **/test/** contains the videos for the test set. \n    \n* **/images/** contains the additional annotated images of player helmets."},{"metadata":{},"cell_type":"markdown","source":"# 3.Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Libraries\nimport numpy as np \nimport pandas as pd\n\nimport seaborn as sns\n\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.dpi'] = 150\n\nimport cv2\nimport imageio\nfrom IPython.display import Video, display\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Importing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tracking = pd.read_csv('../input/nfl-impact-detection/train_player_tracking.csv')\ntest_tracking = pd.read_csv('../input/nfl-impact-detection/test_player_tracking.csv')\n\n\ntrain_labels = pd.read_csv('../input/nfl-impact-detection/train_labels.csv')\nimage_labels = pd.read_csv('../input/nfl-impact-detection/image_labels.csv')\nvideo_labels = pd.read_csv('/kaggle/input/nfl-impact-detection/train_labels.csv')\n\nsub_sample = pd.read_csv('../input/nfl-impact-detection/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Number of unique elements in each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.nunique().to_frame().rename(columns={0:\"Count\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n\nsns.distplot(train_labels[\"gameKey\"].value_counts(), ax=ax[0, 0], rug=True, color=\"red\")\nax[0, 0].set_title(\"Game Counts\")\n\nsns.distplot(train_labels[\"playID\"].value_counts(), ax=ax[0, 1], rug=True, color=\"blue\")\nax[0, 1].set_title(\"Play Counts\")\n\nsns.distplot(train_labels[\"label\"].value_counts(), ax=ax[1, 0], rug=True, color=\"green\")\nax[1, 0].set_title(\"Labels Counts\")\n\nsns.distplot(train_labels[\"video\"].value_counts(), ax=ax[1, 1], rug=True, color=\"yellow\")\nax[1, 1].set_title(\"Videos Counts\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us know the number of(unique) videos in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels['video'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"120 unique videos that comprise of two views of one game play each.\nTherefore, 60 gameplays with two views of each."},{"metadata":{},"cell_type":"markdown","source":"### Lenght of videos"},{"metadata":{"trusted":true},"cell_type":"code","source":"play_frame_count = train_labels[['gameKey','playID','frame']].drop_duplicates()[['gameKey','playID']].value_counts()\n\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.distplot(play_frame_count, bins=15)\nax.set_title('Distribution of frames per video file')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The videos range from approximately 300 frames to 600 frames per video."},{"metadata":{},"cell_type":"markdown","source":"### Bounding box size\nThis depends on various factors like,\n* The distance between player and camera.\n* The camera's angle and zoom relative to the field.\n* One player's helmet may be blocked from view by another player.\n\n\nHere, we are taking area (width x height) of the bounding box."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels['area'] = train_labels['width'] * train_labels['height']\nfig, ax = plt.subplots(figsize=(10, 5))\n\nsns.distplot(train_labels['area'].value_counts(),\n             bins=10)\nax.set_title('Distribution bounding box sizes')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Impact Type Count\nTypes of Impacts recorded here are:\n* Helmet\n* Shoulder\n* Body\n* Ground\n* Hand\n* shoulder'"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels['impactType'].value_counts().plot(kind='bar',title='Impact Type Count',figsize=(12, 4))\n\nplt.show()\n\ntrain_labels['impactType'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"view\", hue=\"impactType\", col=\"confidence\",\n                data=train_labels, kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"view\", hue=\"impactType\", col=\"visibility\",\n                data=train_labels, kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Impact Occurance Percentage"},{"metadata":{"trusted":true},"cell_type":"code","source":"impact_occ = train_labels[['video','impact']].fillna(0)['impact'].mean() * 100\nprint(f'Of all bounding boxes, {impact_occ:0.4f}% of them involve an impact event')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confidence\n* Possible = 1\n* Definitive = 2\n* Definitive and Obvious = 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels['confidence'].dropna().astype('int').value_counts().plot(kind='bar',\n          title='Confidence Type Label Count',\n          figsize=(12, 4))\nplt.show()\n\ntrain_labels['confidence'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"impactType\", hue=\"confidence\", col=\"view\",\n                data=train_labels, kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visibility\n* Not Visible from View = 0 \n* Minimum = 1 \n* Visible = 2\n* Clearly Visible = 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels['visibility'].dropna() \\\n    .astype('int').value_counts() \\\n    .plot(kind='bar',\n          title='Visibility Label Count',\n          figsize=(12, 4))\nplt.show()\n\ntrain_labels['visibility'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"impactType\", hue=\"visibility\", col=\"view\",\n                data=train_labels, kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Note to the Readers\n\n### **Motivate** and **Support** me in making myself and the community better. \n### Suggest me more upgrades, share my works and also, *UPVOTE* this effort.\n\n\n## Did you upvote or comment yet? Please do... :D"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}