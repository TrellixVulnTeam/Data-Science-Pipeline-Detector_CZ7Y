{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Contents\n* 1. [Understanding the Competition](#1)\n* 2. [Understanding Data](#2)\n* 3. [Libraries](#3)\n* 4. [Importing Data](#4)\n* 5. [Visualizing Data](#5)\n    * 5.1 [Image Data](#6)\n    * 5.2 [Video Data](#7)\n* 6. [Bounding Boxes](#8)\n    * 6.1 [Bounding boxes in Images](#9)\n    \n* 7. [Exploratory Data Analysis](#11)\n\n* [Upcoming Updates](#12)\n* [Note to Readers](#13)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> \n\n# 1. Understanding the Competition\nThis competition is part of the NFL’s annual 1st and Future Competition, which has been designed to spur innovation in athlete safety and performance.\n\nThe NFL is actively addressing the need for a computer vision system to detect on-field helmet impacts as part of the “Digital Athlete” platform, and the league is calling on Kagglers to help.\n\nIn this competition, it is expected to develop a computer vision model that automatically detects helmet impacts that occur on the field. The dataset is of more than one thousand definitive head impacts from thousands of game images, labelled video from the sidelines and end zones, and player tracking data.\n\nThe data also documents the position, speed, acceleration, and orientation for every player on the field during NFL games.\n\nThis competition is evaluated using a micro F1 score at an Intersection over Union (IoU) threshold of 0.35.\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> \n# 2.Understanding Data\n\n### The dataset consists of three types of data:\n\n* **Image Data:**\n    Image Data consist of about 10,000 images and associated helmet labels. This is to be used for building a helmet detection system.\n\n* **Video Data:**\n    Video Data consists of 120 videos (60 plays) from both a sideline and endzone point of view for each play. It has been associated with helmet and helmet impact labels, which has to be used for building a helmet impact detection system.\n\n* **Tracking Data:**\n    Tracking data consists of tracking for all players in the provided 60 plays.\n\n\n### Data files:\n\n* **train_labels.csv** - Helmet tracking and collision labels for the training set.\n* **sample_submission.csv** - A valid sample submission file.\n* **image_labels.csv** - contains the bounding boxes corresponding to the images.\n* **[train/test]_player_tracking.csv** - Each player wears a sensor that allows us to precisely locate them on the field.\n\n\n\n### Folders:\n* **/train/** contains the mp4 video files for the training plays. \n  (Both an endzone and sideline view.)\n    \n* **/test/** contains the videos for the test set. \n    \n* **/images/** contains the additional annotated images of player helmets."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> \n# 3.Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Libraries\nimport numpy as np \nimport pandas as pd\n\nimport seaborn as sns\n\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.dpi'] = 150\n\nimport cv2\nimport imageio\nfrom IPython.display import Video, display\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> \n# 4. Importing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tracking = pd.read_csv('../input/nfl-impact-detection/train_player_tracking.csv')\ntest_tracking = pd.read_csv('../input/nfl-impact-detection/test_player_tracking.csv')\n\n\ntrain_labels = pd.read_csv('../input/nfl-impact-detection/train_labels.csv')\nimage_labels = pd.read_csv('../input/nfl-impact-detection/image_labels.csv')\nvideo_labels = pd.read_csv('/kaggle/input/nfl-impact-detection/train_labels.csv')\n\nsub_sample = pd.read_csv('../input/nfl-impact-detection/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> \n# 5. Visualizing Data\n<a id=\"6\"></a> \n## 5.1 Image Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_labels.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def img_show(index):\n    im = cv2.imread(\"../input/nfl-impact-detection/images/\" + image_labels[\"image\"][index])\n    plt.imshow(im)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_show(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a> \n## 5.2 Video Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in the video labels file\n\nvideo_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def vid_show(index):\n    video_name = video_labels['video'][index]\n    video_path = f\"/kaggle/input/nfl-impact-detection/train/{video_name}\"\n    display(Video(data=video_path, embed=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vid_show(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a>\n# 6. Bounding Boxes"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a>\n## 6.1 Bounding Boxes in Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_labels.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bounding box function for Images\ndef box_image(index):\n    name = image_labels['image'][index]\n    box_color = (0, 0, 0)    # Bounding box color -> Black\n    img = imageio.imread(f\"/kaggle/input/nfl-impact-detection/images/{name}\")\n    image = image_labels.loc[image_labels['image'] == name]\n    for i, j in image.iterrows():\n        color = box_color \n\n        # Add a box around the helmet\n        # Note that cv2.rectangle requires us to specify the top left pixel and the bottom right pixel\n        cv2.rectangle(img, (j.left, j.top), (j.left + j.width, j.top + j.height), color,thickness=1)\n        \n    # Display the image with bounding boxes added\n    plt.imshow(img)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_image(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a>\n# 7. Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Number of unique elements in each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.nunique().to_frame().rename(columns={0:\"Count\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n\nsns.distplot(train_labels[\"gameKey\"].value_counts(), ax=ax[0, 0], rug=True, color=\"red\")\nax[0, 0].set_title(\"Game Counts\")\n\nsns.distplot(train_labels[\"playID\"].value_counts(), ax=ax[0, 1], rug=True, color=\"blue\")\nax[0, 1].set_title(\"Play Counts\")\n\nsns.distplot(train_labels[\"label\"].value_counts(), ax=ax[1, 0], rug=True, color=\"green\")\nax[1, 0].set_title(\"Labels Counts\")\n\nsns.distplot(train_labels[\"video\"].value_counts(), ax=ax[1, 1], rug=True, color=\"yellow\")\nax[1, 1].set_title(\"Videos Counts\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us know the number of(unique) videos in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels['video'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"120 unique videos that comprise of two views of one game play each.\nTherefore, 60 gameplays with two views of each."},{"metadata":{},"cell_type":"markdown","source":"### Lenght of videos"},{"metadata":{"trusted":true},"cell_type":"code","source":"play_frame_count = train_labels[['gameKey','playID','frame']].drop_duplicates()[['gameKey','playID']].value_counts()\n\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.distplot(play_frame_count, bins=15)\nax.set_title('Distribution of frames per video file')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The videos range from approximately 300 frames to 600 frames per video."},{"metadata":{},"cell_type":"markdown","source":"### Bounding box size\nThis depends on various factors like,\n* The distance between player and camera.\n* The camera's angle and zoom relative to the field.\n* One player's helmet may be blocked from view by another player.\n\n\nHere, we are taking area (width x height) of the bounding box."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels['area'] = train_labels['width'] * train_labels['height']\nfig, ax = plt.subplots(figsize=(10, 5))\n\nsns.distplot(train_labels['area'].value_counts(),\n             bins=10)\nax.set_title('Distribution bounding box sizes')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Impact Type Count\nTypes of Impacts recorded here are:\n* Helmet\n* Shoulder\n* Body\n* Ground\n* Hand\n* shoulder'"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels['impactType'].value_counts().plot(kind='bar',title='Impact Type Count',figsize=(12, 4))\n\nplt.show()\n\ntrain_labels['impactType'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"view\", hue=\"impactType\", col=\"confidence\",\n                data=train_labels, kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"view\", hue=\"impactType\", col=\"visibility\",\n                data=train_labels, kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Impact Occurance Percentage"},{"metadata":{"trusted":true},"cell_type":"code","source":"impact_occ = train_labels[['video','impact']].fillna(0)['impact'].mean() * 100\nprint(f'Of all bounding boxes, {impact_occ:0.4f}% of them involve an impact event')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confidence\n* Possible = 1\n* Definitive = 2\n* Definitive and Obvious = 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels['confidence'].dropna().astype('int').value_counts().plot(kind='bar',\n          title='Confidence Type Label Count',\n          figsize=(12, 4))\nplt.show()\n\ntrain_labels['confidence'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"impactType\", hue=\"confidence\", col=\"view\",\n                data=train_labels, kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visibility\n* Not Visible from View = 0 \n* Minimum = 1 \n* Visible = 2\n* Clearly Visible = 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels['visibility'].dropna() \\\n    .astype('int').value_counts() \\\n    .plot(kind='bar',\n          title='Visibility Label Count',\n          figsize=(12, 4))\nplt.show()\n\ntrain_labels['visibility'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"impactType\", hue=\"visibility\", col=\"view\",\n                data=train_labels, kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a> \n# Upcoming updates:\n\n<b> \n* Bounding Box displays in Videos\n* Finding in-depth insights about the dataset\n* Model design and implementation\n* Final submission.\n</b>\n\nAll these updates will be here soon, keep motivating me till then.\n\n\n<a id=\"13\"></a> \n# Note to the Readers\n<b>\nThis is my first attempt on Kaggle, I am still finding my way around over here, motivate me and push me to learn more.\n\nIf you wish to suggest me updates, feel free to do so.\n</b>\n\n## Did you upvote or comment yet? Please do... :D"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}