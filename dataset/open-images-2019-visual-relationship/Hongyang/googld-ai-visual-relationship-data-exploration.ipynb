{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Googld AI Open Images - Visual Relationship Track\n\nThis kernel just explore the dataset for starting the competition in kaggle :)\n\ntrain dataset is here:\nhttps://www.kaggle.com/mahmoudmohsen213/vrd01\n\n## Contents\n1. Description of the Competition\n2. Explore the Train Dataset\n3. Explore the Test Dataset\n\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://github.com/seriousmac/img_link/blob/master/kg/visual_relation_19/0.PNG?raw=true)\n\nThis yearâ€™s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks:\n\n1. Object detection track for detecting bounding boxes around object instances, relaunched from 2018.\n\n2. __Visual relationship detection track for detecting pairs of objects in particular relations, also relaunched from 2018. (This Competition)__\n\n3. Instance segmentation track [Link to be provided when launched on July 1], brand new for 2019.\n\n![](https://github.com/seriousmac/img_link/blob/master/kg/visual_relation_19/1.PNG?raw=true)\n\nIn this track of the Challenge, you are asked to detect pairs of objects and the relationships that connect them.\n\nThe training set contains 329 relationship triplets with 375k training samples. These include both human-object relationships (e.g. \"woman playing guitar\", \"man holding microphone\"), object-object relationships (e.g. \"beer on table\", \"dog inside car\"), and also considers object-attribute relationships (e.g.\"handbag is made of leather\" and \"bench is wooden\").\n\n![](https://github.com/seriousmac/img_link/blob/master/kg/visual_relation_19/2.PNG?raw=true)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport cv2\n\nimport os\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/vrd01\"))\nprint(os.listdir(\"../input/open-images-2019-visual-relationship\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/vrd01/challenge-2018-train-vrd.csv')\nprint('shape of train data frame:', df_train.shape)\ndf_sample = pd.read_csv('../input/open-images-2019-visual-relationship/VRD_sample_submission.csv')\nprint('shape of sample submission data frame:', df_sample.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['RelationshipLabel'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = ['XMin1', 'XMax1', 'YMin1', 'YMax1', 'XMin2', 'XMax2', 'YMin2', 'YMax2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[numerical].hist(bins=15, figsize=(20, 10), layout=(2, 4));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'PredictionString' looks odd for me :(\n\nLet' have a look!"},{"metadata":{"trusted":true},"cell_type":"code","source":"values_what = df_sample[df_sample['ImageId']=='b4c3b52a8723d431']['PredictionString'].values\nvalues = str(values_what)[2:-2].split(' ')\n\nprint('confidence: ', values[0])\n\nprint('label 1: ', values[1])\nprint('XMin1: ', values[2])\nprint('YMin1: ', values[3])\nprint('XMax1: ', values[4])\nprint('YMax1: ', values[5])\nprint('Label 2: ', values[6])\nprint('XMin2: ', values[7])\nprint('YMin2: ', values[8])\nprint('XMax2: ', values[9])\nprint('YMax2: ', values[10])\nprint('Relation Label: ', values[11])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('confidence: ', values[12])\nprint('label 1: ', values[13])\nprint('XMin1: ', values[14])\nprint('YMin1: ', values[15])\nprint('XMax1: ', values[16])\nprint('YMax1: ', values[17])\nprint('Label 2: ', values[18])\nprint('XMin2: ', values[19])\nprint('YMin2: ', values[20])\nprint('XMax2: ', values[21])\nprint('YMax2: ', values[22])\nprint('Relation Label: ', values[23])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look 10 random sample images"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_filenames = os.listdir(\"../input/open-images-2019-visual-relationship/test\")\n\nimport random\nfor i in range(10):\n    index = random.randrange(len(image_filenames))\n    path = \"../input/open-images-2019-visual-relationship/test\" + \"/\" + image_filenames[index]\n    src_img = cv2.imread(path)\n    fig=plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n    plt.imshow(cv2.cvtColor(src_img, cv2.COLOR_BGR2RGB))\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}