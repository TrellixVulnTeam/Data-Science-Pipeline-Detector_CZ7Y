{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nimport gc\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We load the datasets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DATA_FOLDER = '../input/competitive-data-science-predict-future-sales'\nitems = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\nitem_cats = pd.read_csv(os.path.join(DATA_FOLDER, 'item_categories.csv'))\nshops = pd.read_csv(os.path.join(DATA_FOLDER, 'shops.csv'))\ntrain = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv'))\ntest = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))\nsubmission = pd.read_csv(os.path.join(DATA_FOLDER, 'sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It was necessary for me to use this function since the 16 GB of RAM was not being enough"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I print some rows for each dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Items\")\nprint(items.head(2))\nprint(\"\\nItem Catageries\")\nprint(item_cats.tail(2))\nprint(\"\\nShops\")\nprint(shops.sample(n=2))\nprint(\"\\nTraining Data Set\")\nprint(train.sample(n=3,random_state=1))\nprint(\"\\nTest Data Set\")\nprint(test.sample(n=3,random_state=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the plot is possible to see that there are items with strange prices and sales. I decided to remove items with price > 90000 and sales > 900 because it can affect final results."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train.item_price<90001]\ntrain = train[train.item_cnt_day<901]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is one item with price below zero. It will be fill with median."},{"metadata":{"trusted":true},"cell_type":"code","source":"median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0, 'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, I would like to add some new features using shop and item categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\ngrouped_shop_id = pd.DataFrame(train.groupby(['shop_id', 'date_block_num'])['item_cnt_day'].sum().reset_index())\nfig, axes = plt.subplots(nrows=5,ncols=2,sharex=False,sharey=False,figsize=(16,20))\ncount = 0\nnum_graph = 10\nid_per_graph = math.ceil(grouped_shop_id.shop_id.max()/num_graph)\n\nfor i in range(5):\n    for j in range(2):\n        sns.pointplot(x='date_block_num', y='item_cnt_day', hue='shop_id', data=grouped_shop_id[np.logical_and(count*id_per_graph <= grouped_shop_id['shop_id'], grouped_shop_id['shop_id'] < (count+1)*id_per_graph)], ax=axes[i][j])\n        count += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some variables are cleared to save memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"del grouped_shop_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is possible to observe that there is a sales peak in most of shops at the end of each year. Therefore, the date_block_num column will be relevant when generating the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_join_item = pd.merge(train, items, how='left', on=['item_id'])\ntrain_join_item = train_join_item.drop('item_name', axis=1) # no need to use column item_name\ntrain_join_item.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We merged train and ietms datasets to group training data by item_category_id"},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_item_category_id = pd.DataFrame(train_join_item.groupby(['item_category_id', 'date_block_num'])['item_cnt_day'].sum().reset_index())\nfig, axes = plt.subplots(nrows=5,ncols=2,sharex=False,sharey=False,figsize=(16,20))\ncount = 0\nnum_graph = 10\nid_per_graph = math.ceil(grouped_item_category_id.item_category_id.max()/num_graph)\n\nfor i in range(5):\n    for j in range(2):\n        sns.pointplot(x='date_block_num', y='item_cnt_day', hue='item_category_id', data=grouped_item_category_id[np.logical_and(count*id_per_graph <= grouped_item_category_id['item_category_id'], grouped_item_category_id['item_category_id'] < (count+1)*id_per_graph)], ax=axes[i][j])\n        count += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some variables are cleared to save memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"del grouped_item_category_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the item_id graph you can see some peaks at times that coincide with those of the previous (end of the year) or at different times. This consolidates the importance of the date_block_num in the dataset."},{"metadata":{},"cell_type":"markdown","source":"We need to prepare the features."},{"metadata":{},"cell_type":"raw","source":"New features are created using mean encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import product\n\n# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in train['date_block_num'].unique():\n    cur_shops = train.loc[train['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = train.loc[train['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates\ngb = train.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_day':'sum','item_price':np.mean}).reset_index()\n\n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\nall_data = pd.merge(all_data, items, how='left', on=['item_id'])\nall_data = all_data.drop('item_name', axis=1) # no need to use column item_name\n\nprint(all_data.head())\n\nfor type_id in ['item_id', 'shop_id', 'item_category_id']:\n    for column_id, aggregator, aggtype in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:\n        \n        gb = train_join_item.groupby([type_id,'date_block_num']).aggregate(aggregator).reset_index()[[column_id,type_id,'date_block_num']]\n        gb.columns = [type_id+'_'+column_id+'_'+aggtype,type_id,'date_block_num']\n        \n        all_data = pd.merge(all_data, gb, on=['date_block_num',type_id], how='left')\n\nall_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some variables are cleared to save memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"del grid\ndel gb\ndel cur_shops\ndel cur_items","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use the function to reduce memory since the dataset cannot be eliminated."},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_mem_usage(all_data, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create Lag Features for training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_variables  = list(all_data.columns[6:])+['item_cnt_day']\nlags = [1, 2, 3, 6]\nfor lag in lags:\n    sales_new_df = all_data.copy()\n    sales_new_df.date_block_num += lag\n    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]\n    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n    all_data = pd.merge(all_data, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')\n    \nall_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some variables are cleared to save memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"del sales_new_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We fill the na of some characteristics with 0 and others with the median"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feat in all_data.columns:\n    if 'item_cnt' in feat:\n        all_data[feat]=all_data[feat].fillna(0)\n    elif 'item_price' in feat:\n        all_data[feat]=all_data[feat].fillna(all_data[feat].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to drop the variables that we will not be able to have at prediction time, which are the columns that are not lagged, therefore we will have to drop all the lag_variables except \"item_cnt_day\" because it is our target."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = lag_variables[:-1] + ['item_price']\ntraining = all_data.drop(cols_to_drop,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We print the first 5 rows of test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the greater value of date_block_num in the training data was 33, date_block_num for the test data must be 34"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['date_block_num'] = 34","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We join test and items data using item_id column"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.merge(test, items, on='item_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We create Lag Features for test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"for lag in lags:\n\n    sales_new_df = all_data.copy()\n    sales_new_df.date_block_num += lag\n    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]\n    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n    test = pd.merge(test, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We check that all the columns that are in the training set are also in the test set and vice versa."},{"metadata":{},"cell_type":"markdown","source":"We drop the 'ID' and 'iten_name' columns because we don't need it"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.drop(['ID', 'item_name'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_columns = test.columns\ntraining_columns = set(training.drop('item_cnt_day',axis=1).columns)\nprint(len(test_columns))\nprint(len(training_columns))\nfor i in test_columns:\n    assert i in training_columns\nfor i in training_columns:\n    assert i in test_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We fill the na of some characteristics with 0 and others with the median"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feat in test.columns:\n    if 'item_cnt' in feat:\n        test[feat]=test[feat].fillna(0)\n    elif 'item_price' in feat:\n        test[feat]=test[feat].fillna(test[feat].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will validate that the training and test data apparently correspond"},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['shop_id','item_id']+['item_cnt_day_lag_'+str(x) for x in [1,2,3]]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(training[training['shop_id'] == 5][training['item_id'] == 5233][training['date_block_num'] == 33]['item_cnt_day'])\nprint(training[training['shop_id'] == 5][training['item_id'] == 5233][training['date_block_num'] == 32]['item_cnt_day'])\nprint(training[training['shop_id'] == 5][training['item_id'] == 5233][training['date_block_num'] == 31]['item_cnt_day'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The lagged value for shop_id = 5 and item_id = 5233 correspond, therefore it looks ok."},{"metadata":{},"cell_type":"markdown","source":"We split the training set into training and validation.\n- < 33 months for the train\n- 33 month for the validation set "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = training[training.date_block_num < 33].drop(['item_cnt_day'], axis=1)\nY_train = training[training.date_block_num < 33]['item_cnt_day']\nX_valid = training[training.date_block_num == 33].drop(['item_cnt_day'], axis=1)\nY_valid = training[training.date_block_num == 33]['item_cnt_day']\nX_test = test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We save the datasets in .csv files to use them in another notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train.to_csv('X_train.csv', index=False)\n#Y_train.to_csv('Y_train.csv', index=False)\n#X_valid.to_csv('X_valid.csv', index=False)\n#Y_valid.to_csv('Y_valid.csv', index=False)\n#X_test.to_csv('X_test.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We delete the training and test datasets because we don't need it"},{"metadata":{"trusted":true},"cell_type":"code","source":"del training\ndel test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB Model"},{"metadata":{},"cell_type":"markdown","source":"We train the XGB model. After several runs, these were the parameters that delivered the best results"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom xgboost import plot_importance\nimport xgboost as xgb\n\nxgbtrain = xgb.DMatrix(X_train, Y_train)\n\nparam = {'max_depth':10, \n         'subsample':1,\n         'min_child_weight':0.5,\n         'eta':0.3, \n         'num_round':1000, \n         'seed':1,\n         'silent':0,\n         'eval_metric':'rmse'} # random parameters\nmodel = xgb.train(param, xgbtrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We graph the weight of each of the variables to know which are the most important for the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"x=plot_importance(model)\nx.figure.set_size_inches(10, 30) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We review which are the characteristics that have a weight greater than 200."},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.get_score(importance_type='weight')\n\n# list out keys and values separately \nkey_list = list(score.keys()) \nval_list = list(score.values())\n\ntop_score = list(filter(lambda x: x >= 200, val_list))\n\ntop_feat = []\n\nfor i in top_score:\n    feat = key_list[val_list.index(i)]\n    top_feat += [feat]\n    \nprint(top_feat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Predictions***"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbpredict = xgb.DMatrix(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_xgb = model.predict(xgbpredict).clip(0, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model\ndel xgbpredict\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see an overview of the predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(pred_xgb).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df_xgb = pd.DataFrame({'ID':X_test.index,'item_cnt_month': pred_xgb })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df_xgb.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df_xgb.to_csv('submission_xgb.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LINEAR MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lr = lr.predict(X_test).clip(0, 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see an overview of the predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(pred_lr).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df_lr = pd.DataFrame({'ID':X_test.index,'item_cnt_month': pred_lr })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df_lr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df_lr.to_csv('submission_lr.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\nlgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgb = lgb.train(lgb_params, lgb.Dataset(X_train, label=Y_train), 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lgb = model_lgb.predict(X_test).clip(0, 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see an overview of the predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(pred_lgb).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df_lgb = pd.DataFrame({'ID':X_test.index,'item_cnt_month': pred_lgb })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df_lgb.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df_lgb.to_csv('submission_lgb.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble"},{"metadata":{},"cell_type":"markdown","source":"WEIGHTED AVERAGING"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_w_avg = 0.7*pred_xgb + 0.15*pred_lr + 0.15*pred_lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see an overview of the predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(pred_w_avg).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df_w_avg = pd.DataFrame({'ID':X_test.index,'item_cnt_month': pred_w_avg })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df_w_avg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df_w_avg.to_csv('submission_w_avg.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}