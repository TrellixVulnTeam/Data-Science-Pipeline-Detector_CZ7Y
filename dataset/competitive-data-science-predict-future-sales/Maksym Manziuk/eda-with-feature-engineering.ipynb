{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook describes how to make basic eda, data preparation and generating features for predicting month sales. A lot of inspiration and good tips and tricks I got from https://www.kaggle.com/dlarionov/feature-engineering-xgboost\n\nPipeline:\n* Check missing values\n* Handle outliers\n* Cleaning shops/categories\n* Define our target\n* Lags and mean encodings\n* Price trending features\n* Extra features"},{"metadata":{},"cell_type":"markdown","source":"First we import everything we need"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n%matplotlib inline \n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nimport os\nimport gc\nfrom itertools import product\nfrom tqdm import tqdm_notebook\nimport time","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_FOLDER = '../input/'\nsales = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv'))\nshops_df = pd.read_csv(os.path.join(DATA_FOLDER, 'shops.csv'))\ncat_df = pd.read_csv(os.path.join(DATA_FOLDER, 'item_categories.csv'))\nitems_df = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\ntest_df = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sales.isnull().sum())\nprint(shops_df.isnull().sum())\nprint(cat_df.isnull().sum())\nprint(items_df.isnull().sum())\nprint(test_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see there is no missing values so we can move forward and start to get acquainted with the data. Let's print our sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no column that describes month sales for specific item/shop. But there is only column that describe daily sales. So in future we need to calculate how much items was sold in month with respect to shops. But for now let make some analysis."},{"metadata":{},"cell_type":"markdown","source":"**<font size=4>Dealing with outliers</font>**\n\nFirst of all we need to check our data for outliers. Seaborn will help us. Boxplot is a good tool to use for this purpose"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(sales['item_cnt_day'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that we have 2 poits that are very far from another poits. So we consider them as outliers and can remove them."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales[sales['item_cnt_day'] < 900]\nsns.boxplot(sales['item_cnt_day'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do the same for price"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(sales['item_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales[(sales['item_price']<100000) & (sales['item_price']>0)]\n# uper, lower = np.percentile(sales['item_price'], [1, 99])\n# sales['item_price'] = np.clip(sales['item_price'], uper, lower)\nsns.boxplot(sales['item_price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also it is a good practice to scale numeric features (gradient descent will converge faster). In this case I will use MinMax scaler, so price range will be from 0 to 1.\nI will not scale item_cnt_day. I will deal with it later."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler().fit(sales[['item_price']])\nsales['item_price'] = scaler.transform(sales[['item_price']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"4\">Shops and category preprocessing</font>**"},{"metadata":{},"cell_type":"markdown","source":"\nLet's get acquainted with the shops. \nSeveral shops was duplicated. So remove them from our train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Якутск Орджоникидзе, 56\nsales.loc[sales['shop_id'] == 0, 'shop_id'] = 57\ntest_df.loc[test_df['shop_id'] == 0, 'shop_id'] = 57\n# Якутск ТЦ \"Центральный\"\nsales.loc[sales['shop_id'] == 1, 'shop_id'] = 58\ntest_df.loc[test_df['shop_id'] == 1, 'shop_id'] = 58\n# Жуковский ул. Чкалова 39м²\nsales.loc[sales['shop_id'] == 10, 'shop_id'] = 11\ntest_df.loc[test_df['shop_id'] == 10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make some cleaning in shops_df. I noticed that first word in shop_name means city name. Extract it. And make label encoding since city is a categorical type of feature.\nNote that shops with id 9, 12, 55 have not city so I encoded it with 999 which means unknown."},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_df.loc[shops_df['shop_name'] == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops_df['city'] = shops_df['shop_name'].str.split(' ').map(lambda x: x[0])\nshops_df.loc[shops_df['city'] == '!Якутск', 'city'] = 'Якутск'\nshops_df['city_code'] = LabelEncoder().fit_transform(shops_df['city'])\nshops_df.loc[shops_df['shop_id'].isin([9,12, 55]), 'city_code'] = 999\nshops_df = shops_df[['shop_id','city_code']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now see how categories data looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Categories as shops also have duplicates. Get rid of them. Don't forget to alter item_df because it contains column with category id. Some of category names look like \"blabla (Цыфра)\" and \"blabla\". There are equal."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_df.loc[cat_df['item_category_id'] == 8, 'item_category_name'] = 'Билеты'\ncat_df.loc[cat_df['item_category_id'] == 26, 'item_category_name'] = 'Игры Android'\ncat_df.loc[cat_df['item_category_id'] == 27, 'item_category_name'] = 'Игры MAC'\ncat_df.loc[cat_df['item_category_id'] == 31, 'item_category_name'] = 'Игры PC'\ncat_df.loc[cat_df['item_category_id'] == 34, 'item_category_name'] = 'Карты оплаты - Live!'\ncat_df.loc[cat_df['item_category_id'] == 36, 'item_category_name'] = 'Карты оплаты - Windows'\ncat_df.loc[cat_df['item_category_id'] == 44, 'item_category_name'] = 'Карты оплаты - Windows'\ncat_df.loc[cat_df['item_category_id'] == 74, 'item_category_name'] = 'Программы - MAC'\n#43 equals 44\ncat_df.drop(cat_df[cat_df['item_category_id'] == 44].index, inplace=True)\n#75 == 76\ncat_df.drop(cat_df[cat_df['item_category_id'] == 76].index, inplace=True)\n#77 == 78\ncat_df.drop(cat_df[cat_df['item_category_id'] == 78].index, inplace=True)\n\nitems_df.loc[items_df['item_category_id'] == 44, 'item_category_id'] = 43\nitems_df.loc[items_df['item_category_id'] == 76, 'item_category_id'] = 75\nitems_df.loc[items_df['item_category_id'] == 78, 'item_category_id'] = 77","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the category names has stucte \"type - subtype\". Let's extract it and apply label encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_df['split'] = cat_df['item_category_name'].str.split('-')\ncat_df['type'] = cat_df['split'].map(lambda x: x[0].strip())\ncat_df['type_code'] = LabelEncoder().fit_transform(cat_df['type'])\n# if subtype is nan then type\ncat_df['subtype'] = cat_df['split'].map(lambda x: x[1].strip() if len(x) > 1 else 'unknown')\ncat_df['subtype_code'] = LabelEncoder().fit_transform(cat_df['subtype'])\ncat_df = cat_df[['item_category_id','type_code', 'subtype_code']]\n\nitems_df.drop(['item_name'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Useful note**\n\nSince we have a lot of rows and in future we will have new features we will use more and more RAM to hold this data. So I encourage you to manage memory very carefully. For example we have column date_block_num and it has type int64. But range of values are only from 0 to 34, so we simply can change type to int8 for saving memory. I have some cases when notebook kernel was died because of run out of memory. So downcasting types is very important when you have a lot of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['date_block_num'] = sales['date_block_num'].astype(np.int8)\nsales['shop_id'] = sales['shop_id'].astype(np.int16)\nsales['item_id'] = sales['item_id'].astype(np.int32)\n\ntest_df['date_block_num'] = 34\ntest_df['date_block_num'] = test_df['date_block_num'].astype(np.int8)\ntest_df['shop_id'] = test_df['shop_id'].astype(np.int16)\ntest_df['item_id'] = test_df['item_id'].astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=4>Generating target</font>**"},{"metadata":{},"cell_type":"markdown","source":"So now we need compute our target values (month sales) for item/shop. But first let's take a look on our test data. This can help understand in what way we need to construct train data that they would be the same. Let's find out how many unique items in test and how many unique items per shop"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('number of unique items in test: {0}'.format(test_df['item_id'].nunique()))\ntest_df.groupby('shop_id')['item_id'].nunique()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see every shop has the same items. So let use this knowledge for building our train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    #create all posible tuples [shop, item, month]. Shop and item that used in particular month\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compute target value. This value we will be predicted for test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Groupby data to get shop-item-month aggregates\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n#it was created 2 layer columns after aggregating. So let's use the last.\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n\n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\nall_data['target'] = all_data['target'].astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compute some helpful features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#joining test data to all_data\nall_data = pd.concat([all_data, test_df[index_cols]], ignore_index=True, sort=False, keys=index_cols)\nall_data.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['date_block_num'] = all_data['date_block_num'].astype(np.int8)\nall_data['shop_id'] = all_data['shop_id'].astype(np.int16)\nall_data['item_id'] = all_data['item_id'].astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this also help with fighting RAM issues. Don't forget to delete havy objects that you don't need anymore."},{"metadata":{"trusted":true},"cell_type":"code","source":"del grid, gb \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have competition requirement to clip our prediction from 0 to 20. Then let's make it for train data. That's because I didn't make scaling item_cnt_day in previous steps"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['target'] = np.clip(all_data['target'], 0, 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how our target values are changing with respcect to time"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['target'] = all_data['target'].astype(np.float64) # we cannot calculate sum with float16\nsales_per_month = all_data.groupby('date_block_num')['target'].sum()\nall_data['target'] = all_data['target'].astype(np.float16)\nsales_per_month.plot(figsize=(10,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)Here we can see two peaks. This peaks related to 11 and 23 monthes. It is december. There are much more sales before New Year. So it will be reasonable to include this information in our set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"decembers = [11, 23]\nall_data['is_december'] = all_data['date_block_num'].isin(decembers).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Join information about items, shops and categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.merge(all_data, items_df, how='left', on='item_id')\nall_data = pd.merge(all_data, cat_df, how='left', on='item_category_id')\nall_data = pd.merge(all_data, shops_df, how='left', on='shop_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['item_category_id'] = all_data['item_category_id'].astype(np.int8)\nall_data['type_code'] = all_data['type_code'].astype(np.int8)\nall_data['subtype_code'] = all_data['subtype_code'].astype(np.int8)\nall_data['city_code'] = all_data['city_code'].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del items_df\ndel cat_df\ndel shops_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=4>Lag features and mean encoding</font>**"},{"metadata":{},"cell_type":"markdown","source":"This is where the fun begins. Since it is time series based problem it is recomended to add lag features. Lag features it is feature that describe values in previous points of time. For example one month before, two, three or six."},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to create lag features\n\ndef create_lag(df, cols_to_lag, shift_range):\n    print(cols_to_lag)\n    \n    for month_shift in tqdm_notebook(shift_range):\n        train_shift = df[index_cols + cols_to_lag].copy()\n        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n\n        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_lag else x\n        train_shift = train_shift.rename(columns=foo)\n\n        df = pd.merge(df, train_shift, on=index_cols, how='left')\n\n    del train_shift\n    gc.collect()\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point I will introduce **mean encoding** features. But it will be adapted for time series problem. Instead of simply group data by some category and calculate mean of target value and then replace it with category I will do it with respect to month."},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_enc = all_data.groupby('date_block_num').agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and create lags for this feature for previous month, month before previous and further."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = create_lag(all_data, ['date_block_num_enc'], [1, 2, 3, 6, 12])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make another features in this way"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmean_enc = all_data.groupby(['date_block_num', 'item_id']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_item_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'item_id'])\n\nmean_enc = all_data.groupby(['date_block_num', 'shop_id']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_shop_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'shop_id'])\n\nmean_enc = all_data.groupby(['date_block_num', 'item_category_id']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_cat_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'item_category_id'])\n\nmean_enc = all_data.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_shop_cat_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'shop_id','item_category_id'])\n\nmean_enc = all_data.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_shop_type_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'shop_id','type_code'])\n\nmean_enc = all_data.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_shop_subtype_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'shop_id','subtype_code'])\n\nmean_enc = all_data.groupby(['date_block_num', 'city_code']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_city_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'city_code'])\n\nmean_enc = all_data.groupby(['date_block_num', 'item_id','city_code']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_item_city_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'item_id','city_code'])\n\nmean_enc = all_data.groupby(['date_block_num', 'type_code']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_type_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'type_code'])\n\nmean_enc = all_data.groupby(['date_block_num', 'subtype_code']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_subtype_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'subtype_code'])\n\ndel mean_enc\ngc.collect()\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And create lags features for this mean encodings"},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_features = ['date_block_num_item_enc', 'date_block_num_shop_enc', 'date_block_num_cat_enc',\n               'date_block_num_shop_cat_enc', 'date_block_num_shop_type_enc', 'date_block_num_shop_subtype_enc',\n               'date_block_num_city_enc', 'date_block_num_item_city_enc', 'date_block_num_type_enc',\n               'date_block_num_subtype_enc']\nall_data = create_lag(all_data, lag_features, [1, 2, 3, 6, 12])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove mean encoding for current month since for test data it is unknown."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.drop(lag_features + ['date_block_num_enc'], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=4>Create super interesting features</font>**"},{"metadata":{},"cell_type":"markdown","source":"Price trend for the last six months for shops."},{"metadata":{"trusted":true},"cell_type":"code","source":"#find out shop mean price for all time and shop mean price for every month\nts = time.time()\n\ngroup = sales.groupby(['shop_id']).agg({'item_price':['mean']})\ngroup.columns = ['shop_price_mean']\n\n#another way to get rid of 2 layer columns\ngroup.reset_index(inplace=True)\nall_data = pd.merge(all_data, group, how='left', on=['shop_id'])\nall_data['shop_price_mean'] = all_data['shop_price_mean'].astype(np.float32)\n\ngroup = sales.groupby(['date_block_num', 'shop_id']).agg({'item_price':['mean']})\ngroup.columns = ['date_shop_price_mean']\ngroup.reset_index(inplace=True)\n\nall_data = pd.merge(all_data, group, how='left', on=['date_block_num', 'shop_id'])\nall_data['date_shop_price_mean'] = all_data['date_shop_price_mean'].astype(np.float16)\n\n# create lags \nlags = [1, 2, 3, 4, 5, 6]\nall_data = create_lag(all_data, ['date_shop_price_mean'],lags)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find out how much shop mean month price different from the 'all time' shop mean price , and normilize it.\nts = time.time()\n\nfor i in lags:\n    all_data['delta_shop_price_lag_'+str(i)] = \\\n        (all_data['date_shop_price_mean_lag_'+str(i)] - all_data['shop_price_mean']) / all_data['shop_price_mean']\n\n\ndel group\ngc.collect()\n\ndelta_cols = [col  for col in all_data.columns.values if col.startswith('delta_shop_price_lag_')]\ndate_item_price_cols = [col  for col in all_data.columns.values if col.startswith('date_shop_price_mean_lag_')]\n\n#fillna(method='backfill') doesn't support float16, so change it to float32\nall_data[delta_cols] = all_data[delta_cols].astype(np.float32) \n\n#get first non nan value in a row.\nall_data['delta_shop_price_lag'] = all_data[delta_cols].fillna(method='backfill', axis=1).iloc[:, 0]\n\n#fill it with zeros if it did not found non nan\nall_data['delta_shop_price_lag'] = all_data['delta_shop_price_lag'].fillna(0).astype(np.float16)\n\n#and remove feature we used to calclulate this\ncols_to_drop = delta_cols + date_item_price_cols +['date_shop_price_mean', 'shop_price_mean']\nall_data.drop(cols_to_drop, axis=1, inplace=True)\ntime.time()-ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make the same calculation for items"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = sales.groupby(['item_id']).agg({'item_price':['mean']})\ngroup.columns = ['item_price_mean']\ngroup.reset_index(inplace=True)\nall_data = pd.merge(all_data, group, how='left', on=['item_id'])\nall_data['item_price_mean'] = all_data['item_price_mean'].astype(np.float16)\n\ngroup = sales.groupby(['date_block_num', 'item_id']).agg({'item_price':['mean']})\ngroup.columns = ['date_item_price_mean']\ngroup.reset_index(inplace=True)\n\nall_data = pd.merge(all_data, group, how='left', on=['date_block_num', 'item_id'])\nall_data['date_item_price_mean'] = all_data['date_item_price_mean'].astype(np.float16)\nlags = [1,2,3,4,5,6]\n\nall_data = create_lag(all_data, ['date_item_price_mean'],lags)\n\ndel group\ngc.collect()\n\nfor i in lags:\n    all_data['delta_item_price_lag_'+str(i)] = \\\n        (all_data['date_item_price_mean_lag_'+str(i)] - all_data['item_price_mean']) / all_data['item_price_mean']\n\ndelta_cols = [col  for col in all_data.columns.values if col.startswith('delta_item_price_lag_')]\ndate_item_price_cols = [col  for col in all_data.columns.values if col.startswith('date_item_price_mean_lag_')]\n\nall_data[delta_cols] = all_data[delta_cols].astype(np.float32) \nall_data['delta_item_price_lag'] = all_data[delta_cols].fillna(method='backfill', axis=1).iloc[:, 0]\n\nall_data['delta_item_price_lag'] = all_data['delta_item_price_lag'].fillna(0).astype(np.float16)\ncols_to_drop = delta_cols + date_item_price_cols +['date_item_price_mean', 'item_price_mean']\nall_data.drop(cols_to_drop, axis=1, inplace=True)\nall_data['delta_item_price_lag'].head()\n\ntime.time()-ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Extra features**"},{"metadata":{},"cell_type":"markdown","source":"Create number of days in month features"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['month'] = all_data['date_block_num'] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nall_data['days'] = all_data['month'].map(days).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of month since the last sale for shop/item and for just item. Stay calm, this will take some time to calculate."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ncache = {} # key is 'item_id shop_id', values is date_block_num\nall_data['item_shop_last_sale'] = -1\nall_data['item_shop_last_sale'] = all_data['item_shop_last_sale'].astype(np.int8)\nfor idx, row in all_data.iterrows():    \n    key = str(row['item_id'])+' '+str(row['shop_id'])\n    if key not in cache:\n        if row['target']!=0:\n            cache[key] = row['date_block_num']\n    else:\n        last_date_block_num = cache[key]\n        all_data.at[idx, 'item_shop_last_sale'] = row['date_block_num'] - last_date_block_num\n        cache[key] = row['date_block_num']         \n\ndel cache\ngc.collect()        \ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ncache = {}\nall_data['item_last_sale'] = -1\nall_data['item_last_sale'] = all_data['item_last_sale'].astype(np.int8)\nfor idx, row in all_data.iterrows():    \n    key = row['item_id']\n    if key not in cache:\n        if row['target']!=0:\n            cache[key] = row['date_block_num']\n    else:\n        last_date_block_num = cache[key]\n        if row['date_block_num']>last_date_block_num:\n            all_data.at[idx, 'item_last_sale'] = row['date_block_num'] - last_date_block_num\n            cache[key] = row['date_block_num']   \n\n            \ndel cache\ngc.collect()\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Monthes since first sale for shop/item and for item only"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nall_data['item_shop_first_sale'] = all_data['date_block_num'] - all_data.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nall_data['item_first_sale'] = all_data['date_block_num'] - all_data.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove old data"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = all_data[all_data['date_block_num'] > 11]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And After creating lags we have a lot of nans. So let's fill it zeros"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.fillna(0, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see we generate quite a lot of new features and it's is almost 900 MB. So at this point our jorney of eda and feature engineering is finished but new road of building a predictive model is opening."},{"metadata":{},"cell_type":"markdown","source":"Save it to pickle. I chose pickle instead of csv because pkl format loads faster."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.to_pickle('all_data.pkl')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}