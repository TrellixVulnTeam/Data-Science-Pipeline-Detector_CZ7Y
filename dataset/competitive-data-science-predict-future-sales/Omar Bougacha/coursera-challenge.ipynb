{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, I present my approach to solve the challenge of predicting future sales described in the Coursera course on How to win data science competitions. \n\nThis notebook is oganized as follows: \n1. Exploratory Data Analysis\n2. Data Preprocessing including feature engineering\n3. Modeling including feature selection and model hyperparameters optimization\n4. Ensembling\n5. Submitting the predictions"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Numpy: {np.__version__}')\nprint(f'Pandas: {pd.__version__}')\nprint(f'Matplotlib: {matplotlib.__version__}')\nprint(f'Seaborn: {sns.__version__}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the submitted version of my notebook I used the following versions of the libraries:\n* Numpy 1.18.5\n* Pandas 1.1.5\n* Matplotlib 3.2.1\n* Seaborn 0.10.0"},{"metadata":{},"cell_type":"markdown","source":"# 1. Exploratory Data Analysis (EDA)"},{"metadata":{},"cell_type":"markdown","source":"For this challenge, we have 6 provided csv-data files. These files are: \n* sales_train.csv contains the daily historical data from january 2013 to october 2015\n* test.csv contains the entries we are supposed to predict for november 2015\n* sample_submission.csv is an example of submission file.\n* items.csv presents supplemental information about the items/products.\n* item_categories.csv  presents the supplemental information about the items categories.\n* shops.csv contains the supplemental information about the shops.\n\nLet's load the data and investigate it. "},{"metadata":{},"cell_type":"markdown","source":"## a. Loading Data and Overall View"},{"metadata":{},"cell_type":"markdown","source":"### i. Train Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nprint('Train Sales Data')\ntrain.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Checking for data types"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Checking for missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Statistic Summary"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ii. Items Extra Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nprint('Items Data')\nitems.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Data Types"},{"metadata":{"trusted":true},"cell_type":"code","source":"items.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"items.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Statistical Summary"},{"metadata":{},"cell_type":"markdown","source":"Since data is categorical I will just present the number of unique values "},{"metadata":{"trusted":true},"cell_type":"code","source":"items.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### iii. Item Categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"it_cat = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nprint('Item Categories Data')\nit_cat.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No need for the data types investigation let's go directly into the missing data.\n* Missing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"it_cat.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Statistical Summary "},{"metadata":{"trusted":true},"cell_type":"code","source":"it_cat.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"it_cat.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this number of categories is equal to the one of the item's data. "},{"metadata":{},"cell_type":"markdown","source":"### iv. Shops Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"shop = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\nprint(\"Shops Data\")\nshop.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### v. Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\nprint('Test Data')\ntest.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Missing Values?"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Statistical Summary"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So for the test we are only going to predict the sales for a 42 shop and a 5100 item. Even though we have 60 shops and 22170 item in total. Let's check how many items and shops we have in the training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apperently, we have shops in the training data that we won't predict for the test data. **Maybe we should get rid of these shops.**"},{"metadata":{},"cell_type":"markdown","source":"### vi. Submission Sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')\nsub.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So our objective is to predict the count of items that would be sold per(shop and item)."},{"metadata":{},"cell_type":"markdown","source":"## b. Basic Data Investigation"},{"metadata":{},"cell_type":"markdown","source":"### i. Train Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's investigate the range of number of items sold per day (i.e., the target value)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.item_cnt_day.min(), train.item_cnt_day.max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So apparently we have some days when items are given back to the shop. While some items are sold by the thousands in a single day. "},{"metadata":{},"cell_type":"markdown","source":"* Let's see how many entries represents the back to shop transactions"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.item_cnt_day<0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have a 7356 back to shop transaction. "},{"metadata":{},"cell_type":"markdown","source":"At first glance, it seems like the shop with id 25 is the responsible for all these items returns. **Let's check that out and compute the total number of returned items per shop id.** "},{"metadata":{"trusted":true},"cell_type":"code","source":"return_per_shop = train[train.item_cnt_day<0].groupby('shop_id')['item_cnt_day'].sum()\nreturn_per_shop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well no, a lot of shops are involved in the return transactions. **Which ones have the highest number of returns?** "},{"metadata":{"trusted":true},"cell_type":"code","source":"return_per_shop.sort_values(ascending=True).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So here the list of the 10 shop that have the highest number of returns. **What about those with low returns?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"return_per_shop.sort_values(ascending=False).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And those are the shops with the lowest returns. "},{"metadata":{},"cell_type":"markdown","source":"**Does the number of returns depends on the item id?** Let's get the list of the 10 most returned items and the 10 less returned ones. "},{"metadata":{"trusted":true},"cell_type":"code","source":"return_per_item = train[train.item_cnt_day<0].groupby('item_id')['item_cnt_day'].sum()\nreturn_per_item","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"return_per_item.sort_values(ascending=True).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"return_per_item.sort_values(ascending=False).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how this return is doing in relation with time (monthly returns). "},{"metadata":{"trusted":true},"cell_type":"code","source":"returns_monthly = train[train.item_cnt_day<0].groupby('date_block_num')['item_cnt_day'].sum()\nreturns_monthly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"returns_monthly.plot()\nplt.xlabel('date_block_num')\nplt.ylabel('Number of returned Items')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the number of returns is quite seasonly and it is evolving in time (not stationary). "},{"metadata":{},"cell_type":"markdown","source":"**Q: Do we have entries with no sold items?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"no_sold = train[train.item_cnt_day==0.0]\nno_sold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No and this is quite logical. \n"},{"metadata":{},"cell_type":"markdown","source":"**Let's check how the total monthly sold items evolves in time!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sold_per_month = train.groupby('date_block_num')['item_cnt_day'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sold_per_month.plot()\nplt.xlabel('date_block_num')\nplt.ylabel('Number of sold Items')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are some observations we can derive from this graph: \n* First, it seems like the number of sold items is decreasing in time\n* also we have two pulses of increasing number of transactions around month number 11/12 and 23/24. \n* it seems like we have a seasonality in the data maybe related to a yearly evolution. "},{"metadata":{},"cell_type":"markdown","source":"**Let's check the evolution in time of the total count of sold items per month and per shop**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\ntmp_data = train.groupby(['date_block_num', 'shop_id'], as_index=False)['item_cnt_day'].sum()\nsns.lineplot(x='date_block_num', y='item_cnt_day', hue='shop_id', data = tmp_data)\nplt.xlabel('Date Month Block numbre')\nplt.ylabel('Number of Sold Items')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the figure is not very clear we can have a sens that most shops have the same shape in the sales as the total number of sales. We can also notice that some shops do not have data for all dates this means some shops have opened after a certain date. Let's check the first date of sales for each shop! "},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_data = train.groupby('shop_id')['date_block_num'].min()\ntmp_data[tmp_data>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Do these shops exist in the test set?**"},{"metadata":{},"cell_type":"markdown","source":"Yes and this is the list of the shops that are in the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test['shop_id'].isin(tmp_data[tmp_data>0].index)]['shop_id'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of these shops actually only started in the last period of time! This would effect our model. "},{"metadata":{},"cell_type":"markdown","source":"Does this also apply for items? do we have items that are not present in the training data? or that we do not have a lot of data ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_items = test['item_id'].unique()\ntest_items","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_data = train.groupby('item_id', as_index=False)['date_block_num'].min()\ntmp_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_data[tmp_data.item_id.isin(test_items)].sort_values(by='date_block_num', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes we also have new items. \n\nHow would we use this in our model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_items.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also have items that do not exist in the training set. "},{"metadata":{},"cell_type":"markdown","source":"**Do we have outliers?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='item_cnt_day', data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check which items are these that are sold above 1000 piece a day."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['item_cnt_day']>=1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items[items['item_id'].isin([20949, 11373])]['item_name'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think these are outliers and should be dropped. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='item_price', data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have an item priced over 300000 let's check this one. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['item_price']>=100000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items[items['item_id']==6066]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is also an outlier and should either be fixed or droped **Do we have other sales record of this item?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['item_id']==6066]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"actually it is a single record so let's drop it. "},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"## a. Drop Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train['item_cnt_day']<1000]\ntrain = train[train['item_price']<100000]\ntrain.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## b. Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"Here are my main ideas of feature engineering:\n* From the date find out if that day is holiday or weekend\n* Compute the number of holidays per month \n* get the month id\n* convert item category name \n* convert item name\n* get sales of previous month "},{"metadata":{"trusted":true},"cell_type":"code","source":"import holidays","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ru_holidays = holidays.Russia()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before getting if the date is a holiday we should redefine the date formula. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['new_date'] = train['date'].apply(lambda x: x.split('.')[1]+'/'+x.split('.')[0]+'/'+x.split('.')[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['is_holiday'] = train['new_date'].apply(lambda x: x in ru_holidays)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['date_block_num']==0]['new_date'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_data = train.groupby(['date_block_num', 'new_date'], as_index=False)['is_holiday'].sum()\ntmp_data['is_holiday'] = tmp_data['is_holiday']>0\nn_holidays = tmp_data.groupby(['date_block_num'])['is_holiday'].sum()\nn_holidays","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['new_date'] = pd.to_datetime(train['new_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['weekend'] = train['new_date'].apply(lambda x : x.weekday() in [5, 6])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_data = train.groupby(['date_block_num', 'date'], as_index=False)['weekend'].sum()\ntmp_data['weekend'] = tmp_data['weekend']>0\nn_weekends = tmp_data.groupby(['date_block_num'])['weekend'].sum()\nn_weekends","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['money'] = train['item_price'] * train['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(['date_block_num', 'shop_id'], as_index=False)['money'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['month'] = train['date'].apply(lambda x: x.split('.')[1])\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.shop_id == 0, \"shop_id\"] = 57\ntest.loc[test.shop_id == 0 , \"shop_id\"] = 57\ntrain.loc[train.shop_id == 1, \"shop_id\"] = 58\ntest.loc[test.shop_id == 1 , \"shop_id\"] = 58\ntrain.loc[train.shop_id == 11, \"shop_id\"] = 10\ntest.loc[test.shop_id == 11, \"shop_id\"] = 10\ntrain.loc[train.shop_id == 40, \"shop_id\"] = 39\ntest.loc[test.shop_id == 40, \"shop_id\"] = 39","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.groupby(['date_block_num', 'shop_id', 'item_id'], as_index=False)['item_cnt_day'].sum()\nX_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['Weekends'] = X_train['date_block_num'].map(n_weekends)\nX_train['Holidays'] = X_train['date_block_num'].map(n_holidays)\nX_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['month'] = X_train['date_block_num'].map(train.groupby(['date_block_num'])['month'].unique())\nX_train['month'] = X_train['month'].apply(lambda x : int(x[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's get features from the shops dataframe**"},{"metadata":{"trusted":true},"cell_type":"code","source":"shop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I googled some of the shops names and took a glance at some of the EDA notebooks. It seems like the shop name contains more information than just the name. The first word represents the city and the second represents the type of the shop. So let's get these features! "},{"metadata":{"trusted":true},"cell_type":"code","source":"shop.loc[ shop.shop_name == 'Сергиев Посад ТЦ \"7Я\"',\"shop_name\" ] = 'СергиевПосад ТЦ \"7Я\"'\nshop[\"city\"] = shop.shop_name.str.split(\" \").map( lambda x: x[0] )\nshop[\"type\"] = shop.shop_name.str.split(\" \").map( lambda x: x[1] )\nshop.loc[shop.city == \"!Якутск\", \"city\"] = \"Якутск\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems to me that we have X types of shops the ТЦ, ТРЦ, ТК, ТРК, MTPЦ, other categories I'm not sure they are really categories so I will just join them in a single category I call other. "},{"metadata":{"trusted":true},"cell_type":"code","source":"shop.loc[~shop.type.isin(['ТЦ', 'ТРЦ', 'ТК', 'ТРК', 'MTPЦ']),\"type\"] = 'other'\nshop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops = shop[['shop_id', 'city', 'type']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops[\"city\"] = LabelEncoder().fit_transform( shops.city )\nshops[\"type\"] = LabelEncoder().fit_transform( shops.type )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef clean_name(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9А-Яа-я]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\nitems[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n\nitems[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\nitems[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\nitems = items.fillna('0')\n\nitems[\"item_name\"] = items[\"item_name\"].apply(lambda x: clean_name(x))\nitems.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\nitems.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nitems.loc[ items.type == \"\", \"type\"] = \"mac\"\nitems.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\nitems.loc[ (items.type == 'pc' )| (items.type == 'pс') | (items.type == \"pc\"), \"type\" ] = \"pc\"\nitems.loc[ items.type == 'рs3' , \"type\"] = \"ps3\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group_sum = items.groupby([\"type\"], as_index=False).agg({\"item_id\": \"count\"})\ngroup_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = group_sum.loc[group_sum['item_id']<40,'type'].values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.name2 = items.name2.apply( lambda x: \"etc\" if (x in drop_cols) else x )\nitems = items.drop([\"type\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.name2 = LabelEncoder().fit_transform(items.name2)\nitems.name3 = LabelEncoder().fit_transform(items.name3)\n\nitems.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import product\nimport time\nts = time.time()\nmatrix = []\ncols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\nfor i in range(34):\n    sales = train[train.date_block_num == i]\n    matrix.append( np.array(list( product( [i], sales.shop_id.unique(), sales.item_id.unique() ) ), dtype = np.int16) )\n\nmatrix = pd.DataFrame( np.vstack(matrix), columns = cols )\nmatrix[\"date_block_num\"] = matrix[\"date_block_num\"].astype(np.int8)\nmatrix[\"shop_id\"] = matrix[\"shop_id\"].astype(np.int8)\nmatrix[\"item_id\"] = matrix[\"item_id\"].astype(np.int16)\nmatrix.sort_values( cols, inplace = True )\ntime.time()- ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"revenue\"] = train[\"item_cnt_day\"] * train[\"item_price\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = train.groupby( [\"date_block_num\", \"shop_id\", \"item_id\"] ).agg( {\"item_cnt_day\": [\"sum\"]} )\ngroup.columns = [\"item_cnt_month\"]\ngroup.reset_index( inplace = True)\nmatrix = pd.merge( matrix, group, on = cols, how = \"left\" )\nmatrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].fillna(0).clip(0,20).astype(np.float16)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"date_block_num\"] = 34\ntest[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\ntest[\"shop_id\"] = test.shop_id.astype(np.int8)\ntest[\"item_id\"] = test.item_id.astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\nmatrix = pd.concat([matrix, test.drop([\"ID\"],axis = 1)], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna( 0, inplace = True )\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = pd.merge( matrix, shops, on = [\"shop_id\"], how = \"left\" )\nmatrix = pd.merge(matrix, items, on = [\"item_id\"], how = \"left\")\nmatrix[\"city\"] = matrix[\"city\"].astype(np.int8)\nmatrix[\"type\"] = matrix[\"type\"].astype(np.int8)\nmatrix[\"item_category_id\"] = matrix[\"item_category_id\"].astype(np.int8)\nmatrix[\"name2\"] = matrix[\"name2\"].astype(np.int8)\nmatrix[\"name3\"] = matrix[\"name3\"].astype(np.int16)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature( df,lags, cols ):\n    for col in cols:\n        print(col)\n        tmp = df[[\"date_block_num\", \"shop_id\",\"item_id\",col ]]\n        for i in lags:\n            shifted = tmp.copy()\n            shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\"+str(i)]\n            shifted.date_block_num = shifted.date_block_num + i\n            df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\nmatrix = lag_feature( matrix, [1,2,3], [\"item_cnt_month\"] )\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby( [\"date_block_num\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1], [\"date_avg_item_cnt\"] )\nmatrix.drop( [\"date_avg_item_cnt\"], axis = 1, inplace = True )\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix.date_item_avg_item_cnt = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3], ['date_item_avg_item_cnt'])\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_shop_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1,2,3], [\"date_shop_avg_item_cnt\"] )\nmatrix.drop( [\"date_shop_avg_item_cnt\"], axis = 1, inplace = True )\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby( [\"date_block_num\",\"shop_id\",\"item_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_item_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\",\"item_id\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_shop_item_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1,2,3], [\"date_shop_item_avg_item_cnt\"] )\nmatrix.drop( [\"date_shop_item_avg_item_cnt\"], axis = 1, inplace = True )\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id'], how='left')\nmatrix.date_shop_subtype_avg_item_cnt = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_shop_subtype_avg_item_cnt'])\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_city_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', \"city\"], how='left')\nmatrix.date_city_avg_item_cnt = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_city_avg_item_cnt'])\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id', 'city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city'], how='left')\nmatrix.date_item_city_avg_item_cnt = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_item_city_avg_item_cnt'])\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = train.groupby( [\"item_id\"] ).agg({\"item_price\": [\"mean\"]})\ngroup.columns = [\"item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge( group, on = [\"item_id\"], how = \"left\" )\nmatrix[\"item_avg_item_price\"] = matrix.item_avg_item_price.astype(np.float16)\n\n\ngroup = train.groupby( [\"date_block_num\",\"item_id\"] ).agg( {\"item_price\": [\"mean\"]} )\ngroup.columns = [\"date_item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge(group, on = [\"date_block_num\",\"item_id\"], how = \"left\")\nmatrix[\"date_item_avg_item_price\"] = matrix.date_item_avg_item_price.astype(np.float16)\nlags = [1, 2, 3]\nmatrix = lag_feature( matrix, lags, [\"date_item_avg_item_price\"] )\nfor i in lags:\n    matrix[\"delta_price_lag_\" + str(i) ] = (matrix[\"date_item_avg_item_price_lag_\" + str(i)]- matrix[\"item_avg_item_price\"] )/ matrix[\"item_avg_item_price\"]\n\ndef select_trends(row) :\n    for i in lags:\n        if row[\"delta_price_lag_\" + str(i)]:\n            return row[\"delta_price_lag_\" + str(i)]\n    return 0\n\nmatrix[\"delta_price_lag\"] = matrix.apply(select_trends, axis = 1)\nmatrix[\"delta_price_lag\"] = matrix.delta_price_lag.astype( np.float16 )\nmatrix[\"delta_price_lag\"].fillna( 0 ,inplace = True)\n\nfeatures_to_drop = [\"item_avg_item_price\", \"date_item_avg_item_price\"]\nfor i in lags:\n    features_to_drop.append(\"date_item_avg_item_price_lag_\" + str(i) )\n    features_to_drop.append(\"delta_price_lag_\" + str(i) )\nmatrix.drop(features_to_drop, axis = 1, inplace = True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = train.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"revenue\": [\"sum\"] })\ngroup.columns = [\"date_shop_revenue\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge( group , on = [\"date_block_num\", \"shop_id\"], how = \"left\" )\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby([\"shop_id\"]).agg({ \"date_block_num\":[\"mean\"] })\ngroup.columns = [\"shop_avg_revenue\"]\ngroup.reset_index(inplace = True )\n\nmatrix = matrix.merge( group, on = [\"shop_id\"], how = \"left\" )\nmatrix[\"shop_avg_revenue\"] = matrix.shop_avg_revenue.astype(np.float32)\nmatrix[\"delta_revenue\"] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\nmatrix[\"delta_revenue\"] = matrix[\"delta_revenue\"]. astype(np.float32)\n\nmatrix = lag_feature(matrix, [1], [\"delta_revenue\"])\nmatrix[\"delta_revenue_lag_1\"] = matrix[\"delta_revenue_lag_1\"].astype(np.float32)\nmatrix.drop( [\"date_shop_revenue\", \"shop_avg_revenue\", \"delta_revenue\"] ,axis = 1, inplace = True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix[\"month\"] = matrix[\"date_block_num\"] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix[\"days\"] = matrix[\"month\"].map(days).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix[\"item_shop_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\",\"shop_id\"])[\"date_block_num\"].transform('min')\nmatrix[\"item_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\"])[\"date_block_num\"].transform('min')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix['Weekends'] = matrix['date_block_num'].map(n_weekends).fillna(10)\nmatrix['Holidays'] = matrix['date_block_num'].map(n_holidays).fillna(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ndata = matrix[matrix[\"date_block_num\"] > 3]\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBRegressor(\n    max_depth=9,\n    n_estimators=1000,\n    min_child_weight=1.5, \n    colsample_bytree=0.6, \n    subsample=0.7, \n    eta=0.01,\n#     tree_method='gpu_hist',\n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance\nfig, ax = plt.subplots(1,1,figsize=(10,15))\nplot_importance(booster=model, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}