{"cells":[{"metadata":{"toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-of-libraries\" data-toc-modified-id=\"Import-of-libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import of libraries</a></span></li><li><span><a href=\"#Some-global-variables\" data-toc-modified-id=\"Some-global-variables-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Some global variables</a></span></li><li><span><a href=\"#Some-helper-functions\" data-toc-modified-id=\"Some-helper-functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Some helper functions</a></span></li><li><span><a href=\"#Preprocessing-of-the-data-before-features-generation\" data-toc-modified-id=\"Preprocessing-of-the-data-before-features-generation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Preprocessing of the data before features generation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Correct-the-shop-names-and-id\" data-toc-modified-id=\"Correct-the-shop-names-and-id-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Correct the shop names and id</a></span></li><li><span><a href=\"#Generate-item_category_features\" data-toc-modified-id=\"Generate-item_category_features-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Generate item_category_features</a></span></li><li><span><a href=\"#Remove-the-huge-price-and-item-sales-outliers\" data-toc-modified-id=\"Remove-the-huge-price-and-item-sales-outliers-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Remove the huge price and item sales outliers</a></span></li><li><span><a href=\"#Generate-a-full-df-with-all-data-and-records\" data-toc-modified-id=\"Generate-a-full-df-with-all-data-and-records-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Generate a full df with all data and records</a></span></li><li><span><a href=\"#Create-a-groupby-df-with-all-the-sales-for-shop_id-and-item_id-grouped-by-months\" data-toc-modified-id=\"Create-a-groupby-df-with-all-the-sales-for-shop_id-and-item_id-grouped-by-months-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Create a groupby df with all the sales for shop_id and item_id grouped by months</a></span></li><li><span><a href=\"#Join-the-full_df-with-gb_df\" data-toc-modified-id=\"Join-the-full_df-with-gb_df-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Join the full_df with gb_df</a></span></li><li><span><a href=\"#Add-additional-features-to-our-full-sales-df\" data-toc-modified-id=\"Add-additional-features-to-our-full-sales-df-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Add additional features to our full sales df</a></span><ul class=\"toc-item\"><li><span><a href=\"#FeatureGenerator-class\" data-toc-modified-id=\"FeatureGenerator-class-4.7.1\"><span class=\"toc-item-num\">4.7.1&nbsp;&nbsp;</span>FeatureGenerator class</a></span></li><li><span><a href=\"#Generate-additional-features-as,-mean-and-total-sales-for-shop_id-,-item_id,-city-...-for-every-month\" data-toc-modified-id=\"Generate-additional-features-as,-mean-and-total-sales-for-shop_id-,-item_id,-city-...-for-every-month-4.7.2\"><span class=\"toc-item-num\">4.7.2&nbsp;&nbsp;</span>Generate additional features as, mean and total sales for shop_id , item_id, city ... for every month</a></span><ul class=\"toc-item\"><li><span><a href=\"#Date-features\" data-toc-modified-id=\"Date-features-4.7.2.1\"><span class=\"toc-item-num\">4.7.2.1&nbsp;&nbsp;</span>Date features</a></span></li><li><span><a href=\"#Date-and-shop_id-features\" data-toc-modified-id=\"Date-and-shop_id-features-4.7.2.2\"><span class=\"toc-item-num\">4.7.2.2&nbsp;&nbsp;</span>Date and shop_id features</a></span></li><li><span><a href=\"#Date-and-item_id-features\" data-toc-modified-id=\"Date-and-item_id-features-4.7.2.3\"><span class=\"toc-item-num\">4.7.2.3&nbsp;&nbsp;</span>Date and item_id features</a></span></li><li><span><a href=\"#Date-and-item_category-features\" data-toc-modified-id=\"Date-and-item_category-features-4.7.2.4\"><span class=\"toc-item-num\">4.7.2.4&nbsp;&nbsp;</span>Date and item_category features</a></span></li><li><span><a href=\"#Date-and-type_code-features\" data-toc-modified-id=\"Date-and-type_code-features-4.7.2.5\"><span class=\"toc-item-num\">4.7.2.5&nbsp;&nbsp;</span>Date and type_code features</a></span></li><li><span><a href=\"#Date-and-sub_type_code-features\" data-toc-modified-id=\"Date-and-sub_type_code-features-4.7.2.6\"><span class=\"toc-item-num\">4.7.2.6&nbsp;&nbsp;</span>Date and sub_type_code features</a></span></li><li><span><a href=\"#Date,-shop-and-type-features\" data-toc-modified-id=\"Date,-shop-and-type-features-4.7.2.7\"><span class=\"toc-item-num\">4.7.2.7&nbsp;&nbsp;</span>Date, shop and type features</a></span></li><li><span><a href=\"#Date,-shop-and-sub_type-features\" data-toc-modified-id=\"Date,-shop-and-sub_type-features-4.7.2.8\"><span class=\"toc-item-num\">4.7.2.8&nbsp;&nbsp;</span>Date, shop and sub_type features</a></span></li><li><span><a href=\"#Date-and-city-features\" data-toc-modified-id=\"Date-and-city-features-4.7.2.9\"><span class=\"toc-item-num\">4.7.2.9&nbsp;&nbsp;</span>Date and city features</a></span></li><li><span><a href=\"#Date,-city-and-item_id-features\" data-toc-modified-id=\"Date,-city-and-item_id-features-4.7.2.10\"><span class=\"toc-item-num\">4.7.2.10&nbsp;&nbsp;</span>Date, city and item_id features</a></span></li></ul></li><li><span><a href=\"#Shift-features-of-shop_id-and-item_id-sales\" data-toc-modified-id=\"Shift-features-of-shop_id-and-item_id-sales-4.7.3\"><span class=\"toc-item-num\">4.7.3&nbsp;&nbsp;</span>Shift features of shop_id and item_id sales</a></span></li><li><span><a href=\"#Shift-features-of-shop_id-and-item_id-price\" data-toc-modified-id=\"Shift-features-of-shop_id-and-item_id-price-4.7.4\"><span class=\"toc-item-num\">4.7.4&nbsp;&nbsp;</span>Shift features of shop_id and item_id price</a></span></li><li><span><a href=\"#Shift-features-of-shop_id-and-item_id-revenue\" data-toc-modified-id=\"Shift-features-of-shop_id-and-item_id-revenue-4.7.5\"><span class=\"toc-item-num\">4.7.5&nbsp;&nbsp;</span>Shift features of shop_id and item_id revenue</a></span></li><li><span><a href=\"#Datetime-features\" data-toc-modified-id=\"Datetime-features-4.7.6\"><span class=\"toc-item-num\">4.7.6&nbsp;&nbsp;</span>Datetime features</a></span></li><li><span><a href=\"#Adding-holiday-data\" data-toc-modified-id=\"Adding-holiday-data-4.7.7\"><span class=\"toc-item-num\">4.7.7&nbsp;&nbsp;</span>Adding holiday data</a></span></li><li><span><a href=\"#City-population-and-mean_income-per-city\" data-toc-modified-id=\"City-population-and-mean_income-per-city-4.7.8\"><span class=\"toc-item-num\">4.7.8&nbsp;&nbsp;</span>City population and mean_income per city</a></span></li></ul></li><li><span><a href=\"#Join-full-sales-df-with-all-the-features-generated\" data-toc-modified-id=\"Join-full-sales-df-with-all-the-features-generated-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Join full sales df with all the features generated</a></span></li></ul></li><li><span><a href=\"#Basic-model-train\" data-toc-modified-id=\"Basic-model-train-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Basic model train</a></span></li><li><span><a href=\"#Feature-importance\" data-toc-modified-id=\"Feature-importance-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Feature importance</a></span></li><li><span><a href=\"#Predict-and-model-evaluation\" data-toc-modified-id=\"Predict-and-model-evaluation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Predict and model evaluation</a></span></li><li><span><a href=\"#Homework\" data-toc-modified-id=\"Homework-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Homework</a></span>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import of libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the basic libraries we will use in this kernel\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nimport time\nimport datetime\nfrom datetime import datetime\nimport calendar\n\nfrom sklearn import metrics\nfrom math import sqrt\nimport gc\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport itertools\nimport warnings\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nwarnings.filterwarnings(\"ignore\") # specify to ignore warning messages","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some global variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Resample the sales by this parameter\nPERIOD = \"M\"\n# SHOPS = [8, 11, 14, 25, 28, 31, 33, 36, 37, 40, 41, 42, 44, 51, 52, 54, 59]\n\nSHOPS = [8, 14, 37, 41, 59]\n\n# this is help us change faster between Kaggle and local machine\nLOCAL = False\n\nif LOCAL:\n    PATH = os.getcwd()\nelse:\n    PATH = '../input/competitive-data-science-predict-future-sales/'\n    PATH_EXTERNAL = \"../input/group-by-df/\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some helper functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# prints the local files\ndef print_files():\n    \n    '''\n    Prints the files that are in the current working directory.\n    '''\n    \n    cwd = \"../input/competitive-data-science-predict-future-sales/\"\n    \n    for f, ff, fff in os.walk(cwd):\n        for file in fff:\n            if file.split(\".\")[1] in [\"pkl\", \"csv\"]:\n                print(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_files()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reduces the memory of a dataframe\ndef reduce_mem_usage(df, verbose = True):\n    \n    '''\n    Reduces the space that a DataFrame occupies in memory.\n\n    This function iterates over all columns in a df and downcasts them to lower type to save memory.\n    '''\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\\\n              .format(end_mem, 100 * (start_mem - end_mem) / start_mem))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing of the data before features generation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The idea of this section is very simple. We have seen in our EDA part that there are a lot of missing values.\nOur model will benefit a lot if we can supply it a training data, with the missing values being zero. This way, it can learn from more amount of data.\n\nIn order to do so, we must perform a cartesian operation over dates x shops x items_id to generate all the possible combinations of months x shops and x items sales.\n\nIn this kernel we will only generate this type of features for the items that are present in TEST only.\n\nThis will reduce the amount of calculations required. If you have enough memory, we can do this for all possible combinations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## load all the df we have\n#shops_df = pd.read_csv(os.path.join(PATH, \"shops.csv\"))\n#items_df = pd.read_csv(os.path.join(PATH, \"items.csv\"))\n#items_category_df = pd.read_csv(os.path.join(PATH, \"item_categories.csv\"))\n#sales_df = pd.read_csv(os.path.join(PATH, \"sales_train.csv\"))\n#test_df = pd.read_csv(os.path.join(PATH, \"test.csv\"))\n##sub = pd.read_csv(os.path.join(PATH, \"sample_submission.csv\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correct the shop names and id","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#\n#\n## we have seen in our EDA that we have some duplicate shops, let's correct them.\n#shops_df.loc[shops_df.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n#shops_df['city'] = shops_df['shop_name'].str.split(' ').map(lambda x: x[0])\n#shops_df.loc[shops_df.city == '!Якутск', 'city'] = 'Якутск'\n#shops_df['city_code'] = LabelEncoder().fit_transform(shops_df['city'])\n#shops_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, we have some duplicate shop names, let's manually clean them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#shops_df[shops_df[\"shop_id\"].isin([0, 57])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Якутск Орджоникидзе, 56\n#sales_df.loc[sales_df.shop_id == 0, 'shop_id'] = 57\n#test_df.loc[test_df.shop_id == 0, 'shop_id'] = 57\n#\n## Якутск ТЦ \"Центральный\"\n#sales_df.loc[sales_df.shop_id == 1, 'shop_id'] = 58\n#test_df.loc[test_df.shop_id == 1, 'shop_id'] = 58\n#\n## Жуковский ул. Чкалова 39м²\n#sales_df.loc[sales_df.shop_id == 10, 'shop_id'] = 11\n#test_df.loc[test_df.shop_id == 10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate item_category_features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#items_category_df['split'] = items_category_df['item_category_name'].str.split('-')\n#items_category_df['type'] = items_category_df['split'].map(lambda x: x[0].strip())\n#items_category_df['type_code'] = LabelEncoder().fit_transform(items_category_df['type'])\n#\n## if subtype is nan then type\n#items_category_df['subtype'] = items_category_df['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n#items_category_df['subtype_code'] = LabelEncoder().fit_transform(items_category_df['subtype'])\n#\n#items_category_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove the huge price and item sales outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#sales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## we have negative prices and some outlier\n## let's replace the data with the mean value and also filter all the outliers\n#mean = sales_df[(sales_df[\"shop_id\"] == 32) & (sales_df[\"item_id\"] == 2973) & (sales_df[\"date_block_num\"] == 4) & (sales_df[\"item_price\"] > 0)][\"item_price\"].mean()\n#sales_df.loc[sales_df.item_price < 0, 'item_price'] = mean\n#\n#sales_df = sales_df[sales_df[\"item_price\"] < np.percentile(sales_df[\"item_price\"], q = 100)]\n#sales_df = sales_df[sales_df[\"item_cnt_day\"] < np.percentile(sales_df[\"item_cnt_day\"], q = 100)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate a full df with all data and records","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#sales_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#type(sales_df[\"date\"].iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to datetime the date column\n# specify the format since otherwise it might give some problems\n#sales_df[\"date\"] = pd.to_datetime(sales_df[\"date\"], format = \"%d.%m.%Y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## max date in sales is 31.10.2015.\n## In the Kaggle competition we are asked to predict the sales for the next month\n## this means the sales of November\n#min_date = sales_df[\"date\"].min()\n#max_date_sales = sales_df[\"date\"].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#min_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#max_date_sales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#max_date_test = datetime(2015, 11, 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#date_range = pd.date_range(min_date, max_date_test, freq = \"D\")\n#date_range","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(date_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model will benefit a lot if we can train it with the highest granularity (daily sales).\n\nHowever, as we can see doing this on a local machine is almost impossible since we have more than 1.4 BILLION rows.\nIf we add 10 featrues (columns) this means that our total DataFrame will have more than 10.4 BILLIONS instances.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#shops = sorted(list(shops_df[\"shop_id\"].unique()))\n#\n## only items present in test\n#items = sorted(list(items_df[\"item_id\"].unique()))\n#\n#cartesian_product = pd.MultiIndex.from_product([date_range, shops, items], names=[\"date\", \"shop_id\", \"item_id\"])\n#len(cartesian_product)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cartesian_product","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a weekly range and see how many rows it will produce.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#date_range = pd.date_range(min_date, max_date_test, freq = \"W\")\n#date_range","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(date_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we try to build our model with all the shops and item_id on a weekly basis we have a total of 46 million rows.\nThis makes the modeling part on a local machine very difficult.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#shops = sorted(list(shops_df[\"shop_id\"].unique()))\n#\n## only items present in test\n#items = sorted(list(items_df[items_df[\"item_id\"].isin(test_df[\"item_id\"].unique())][\"item_id\"].unique()))\n#\n#cartesian_product = pd.MultiIndex.from_product([date_range, shops, items], names=[\"date\", \"shop_id\", \"item_id\"])\n#len(cartesian_product)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to replicate the Kaggle competition, we will create a smaller DataFrame with only selected shops and train the model on a Monthly basis:\n\n--> 5 shops with most sales\n\n--> 5 shops with the lowest sales\n\n--> 6 shops with middle sales\n\n# La nueva selección son 5 tiendas.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#PERIOD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#date_range = pd.date_range(min_date, max_date_sales, freq = PERIOD)\n#date_range","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(date_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.87 million rows, we CAN work with this on a local machine.\n\nWe have created monthly date_range, if we want to join this with our sales data, we must \"resample\" our data to a monthly date_range aswell.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## only items present in test\n#items = sorted(list(test_df[\"item_id\"].unique()))\n#\n#cartesian_product = pd.MultiIndex.from_product([date_range, SHOPS, items], names = [\"date\", \"shop_id\", \"item_id\"])\n#len(cartesian_product)\n#\n##Son 3 iterables en los cuales te dicen cuales son los items que quieres tener","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(date_range) * len(SHOPS) * len(items)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cartesian_product","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create a groupby df with all the sales for shop_id and item_id grouped by months","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will be working with a DataFrame resampled by Months. We must resample the sales_df.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#st = time.time()\n## set index\n#sales_df[\"revenue\"] = sales_df[\"item_cnt_day\"]*sales_df[\"item_price\"]\n#gb_df = sales_df.set_index(\"date\")\n## groupby shop_id and item_id\n#gb_df = gb_df.groupby([\"shop_id\", \"item_id\"])\n## resample the sales to a weekly basis\n#gb_df = gb_df.resample(PERIOD).agg({'item_cnt_day': np.sum, \"item_price\": np.mean, \"revenue\":np.sum})\n## convert to dataframe and save the full dataframe\n#gb_df.reset_index(inplace = True)\n## save the groupby dataframe\n#gb_df.to_pickle(os.path.join(PATH, \"GROUP_BY_DF.pkl\"))\n#et = time.time()\n#print(\"Total time in minutes to preprocess took {}\".format((et - st)/60))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## read the groupby dataframe\n#gb_df = pd.read_pickle(os.path.join(PATH_EXTERNAL, \"GROUP_BY_DF.pkl\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SHOPS","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#gb_df = gb_df[(gb_df['shop_id']==8) | (gb_df['shop_id']==14)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gb_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gb_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# cosas para hacer aqui tienes que imputarle el precio medio de cadda uno en los nulos o algo diferente para que no sea 0 ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#gb_df.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Join the full_df with gb_df","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now that we have the sales_df resampled by months, and we have created a cartesian product (all possible combinations of months, shop_id and item_id), let's merge the df.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df = pd.DataFrame(index = cartesian_product).reset_index()\n#full_df = pd.merge(full_df, gb_df, on = ['date','shop_id', \"item_id\"], how = 'left')\n#\n#full_df[\"item_cnt_day\"].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add additional features to our full sales df","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## add shops_df information\n#full_df = pd.merge(full_df, shops_df, on = \"shop_id\")\n#full_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## add items_df information\n#full_df = pd.merge(full_df, items_df, on = \"item_id\")\n#full_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## add items_category_df information\n#full_df = pd.merge(full_df, items_category_df, on = \"item_category_id\")\n#full_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## We will clip the value in this line.\n## This means that the values greater than 20, will become 20 and lesser than 20\n#full_df[\"item_cnt_day\"] = np.clip(full_df[\"item_cnt_day\"], 0, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reduce_mem_usage(full_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FeatureGenerator class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeatureGenerator(object):\n    \n    '''\n    This is a helper class that takes a df and a list of features and creates sum, mean, \n    lag features and variation (change over month) features.\n    \n    '''\n    \n    def __init__(self, full_df,  gb_list):\n        \n        '''\n        Constructor of the class.\n        gb_list is a list of columns that must be in full_df.\n        '''\n        \n        self.full_df = full_df\n        self.gb_list = gb_list\n        # joins the gb_list, this way we can dinamically create new columns\n        # [\"date, \"shop_id] --> date_shop_id\n        self.objective_column_name = \"_\".join(gb_list)\n\n    @staticmethod\n    def reduce_mem_usage(df, verbose = True):\n        \n        '''\n        Reduces the space that a DataFrame occupies in memory.\n        This is a static method of FeatureGenerator class (we can use it outside the class).\n        \n        This function iterates over all columns in a df and downcasts them to lower type to save memory.\n        '''\n        \n        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n        start_mem = df.memory_usage().sum() / 1024**2    \n        for col in df.columns:\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n        end_mem = df.memory_usage().sum() / 1024**2\n        if verbose:\n            print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\\\n                  .format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n            \n    def generate_gb_df(self):\n        \n        '''\n        This function thakes the full_df and creates a groupby df based on the gb_list.\n        It creates 2 columns: \n            1. A sum column for every date and gb_list\n            2. Mean columns for every_date and gb_list\n            \n        The resulting df (gb_df_) is assigned back to the FeatureGenerator class as an attribute.\n        '''\n\n        def my_agg(full_df_, args):\n            \n            '''\n            This function is used to perform multiple operations over a groupby df and returns a df\n            without multiindex.\n            '''\n            \n            names = {\n                '{}_sum'.format(args):  full_df_['item_cnt_day'].sum(),\n                '{}_mean'.format(args): full_df_['item_cnt_day'].mean()\n            }\n\n            return pd.Series(names, index = [key for key in names.keys()])\n        \n        # the args is used to pass additional argument to the apply function\n        gb_df_ = self.full_df.groupby(self.gb_list).apply(my_agg, args = (self.objective_column_name)).reset_index()\n\n        self.gb_df_ = gb_df_\n\n        \n    def return_gb_df(self):  \n        \n        '''\n        This function takes the gb_df_ created in the previous step (generate_gb_df) and creates additional features.\n        We create 4 lag features (values from the past).\n        And 6 variation features: 3 with absolute values and 3 with porcentual change.\n        '''\n        \n        def generate_shift_features(self, suffix):\n            \n            '''\n            This function is a helper function that takes the gb_df_ and a suffix (sum or mean) and creates the\n            additional features.\n            '''\n\n            # dinamically creates the features\n            # date_shop_id --> date_shop_id_sum if suffix is sum\n            # date_shop_id --> date_shop_id_mean if suffix is mean\n            name_ = self.objective_column_name + \"_\" + suffix\n\n            self.gb_df_['{}_shift_1'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(1))\n            \n            self.gb_df_['{}_shift_2'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(2))\n            \n            self.gb_df_['{}_shift_3'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(3))\n            \n            self.gb_df_['{}_shift_6'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(6))\n            \n            # notice taht in var_3 we use shift(4), we do this because we want to capture the variation of 3 months\n            # and not the variation of month - 3\n\n            self.gb_df_['{}_var_1'.format(name_)] = self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(1) - x.shift(2))\n            self.gb_df_['{}_var_2'.format(name_)] = self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(1) - x.shift(3))\n            self.gb_df_['{}_var_3'.format(name_)] = self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(1) - x.shift(4))\n\n            self.gb_df_['{}_var_pct_1'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(2))/x.shift(2))\n            \n            self.gb_df_['{}_var_pct_2'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(3))/x.shift(3))\n            \n            self.gb_df_['{}_var_pct_3'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(4))/x.shift(4))\n            \n            self.gb_df_.fillna(-1, inplace = True)\n\n            self.gb_df_.replace([np.inf, -np.inf], -1, inplace = True)\n        \n        # call the generate_shift_featues function with different suffix (sum and mean)\n        generate_shift_features(self, suffix = \"sum\")\n        generate_shift_features(self, suffix = \"mean\")\n        \n        FeatureGenerator.reduce_mem_usage(self.gb_df_)\n    \n        return self.gb_df_\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modificado\n\nclass FeatureGenerator(object):\n    \n    '''\n    This is a helper class that takes a df and a list of features and creates sum, mean, \n    lag features and variation (change over month) features.\n    \n    '''\n    \n    def __init__(self, full_df,  gb_list):\n        \n        '''\n        Constructor of the class.\n        gb_list is a list of columns that must be in full_df.\n        '''\n        \n        self.full_df = full_df\n        self.gb_list = gb_list\n        # joins the gb_list, this way we can dinamically create new columns\n        # [\"date, \"shop_id] --> date_shop_id\n        self.objective_column_name = \"_\".join(gb_list)\n\n    @staticmethod\n    def reduce_mem_usage(df, verbose = True):\n        \n        '''\n        Reduces the space that a DataFrame occupies in memory.\n        This is a static method of FeatureGenerator class (we can use it outside the class).\n        \n        This function iterates over all columns in a df and downcasts them to lower type to save memory.\n        '''\n        \n        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n        start_mem = df.memory_usage().sum() / 1024**2    \n        for col in df.columns:\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n        end_mem = df.memory_usage().sum() / 1024**2\n        if verbose:\n            print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\\\n                  .format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n            \n    def generate_gb_df(self):\n        \n        '''\n        This function thakes the full_df and creates a groupby df based on the gb_list.\n        It creates 2 columns: \n            1. A sum column for every date and gb_list\n            2. Mean columns for every_date and gb_list\n            \n        The resulting df (gb_df_) is assigned back to the FeatureGenerator class as an attribute.\n        '''\n\n        def my_agg(full_df_, args):\n            \n            '''\n            This function is used to perform multiple operations over a groupby df and returns a df\n            without multiindex.\n            '''\n            \n            names = {\n                '{}_sum'.format(args):  full_df_['sales'].sum(),\n                '{}_mean'.format(args): full_df_['sales'].mean()\n            }\n\n            return pd.Series(names, index = [key for key in names.keys()])\n        \n        # the args is used to pass additional argument to the apply function\n        gb_df_ = self.full_df.groupby(self.gb_list).apply(my_agg, args = (self.objective_column_name)).reset_index()\n\n        self.gb_df_ = gb_df_\n\n        \n    def return_gb_df(self):  \n        \n        '''\n        This function takes the gb_df_ created in the previous step (generate_gb_df) and creates additional features.\n        We create 4 lag features (values from the past).\n        And 6 variation features: 3 with absolute values and 3 with porcentual change.\n        '''\n        \n        def generate_shift_features(self, suffix):\n            \n            '''\n            This function is a helper function that takes the gb_df_ and a suffix (sum or mean) and creates the\n            additional features.\n            '''\n\n            # dinamically creates the features\n            # date_shop_id --> date_shop_id_sum if suffix is sum\n            # date_shop_id --> date_shop_id_mean if suffix is mean\n            name_ = self.objective_column_name + \"_\" + suffix\n\n            self.gb_df_['{}_shift_1'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(1))\n            \n            self.gb_df_['{}_shift_2'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(2))\n            \n            self.gb_df_['{}_shift_3'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(3))\n            \n            self.gb_df_['{}_shift_6'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(6))\n            \n            # notice taht in var_3 we use shift(4), we do this because we want to capture the variation of 3 months\n            # and not the variation of month - 3\n\n            self.gb_df_['{}_var_1'.format(name_)] = self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(1) - x.shift(2))\n            self.gb_df_['{}_var_2'.format(name_)] = self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(1) - x.shift(3))\n            self.gb_df_['{}_var_3'.format(name_)] = self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(1) - x.shift(4))\n\n            self.gb_df_['{}_var_pct_1'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(2))/x.shift(2))\n            \n            self.gb_df_['{}_var_pct_2'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(3))/x.shift(3))\n            \n            self.gb_df_['{}_var_pct_3'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(4))/x.shift(4))\n            \n            self.gb_df_.fillna(-1, inplace = True)\n\n            self.gb_df_.replace([np.inf, -np.inf], -1, inplace = True)\n        \n        # call the generate_shift_featues function with different suffix (sum and mean)\n        generate_shift_features(self, suffix = \"sum\")\n        generate_shift_features(self, suffix = \"mean\")\n        \n        FeatureGenerator.reduce_mem_usage(self.gb_df_)\n    \n        return self.gb_df_\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_shift_features(df, column_name, gb_list):\n    \n    df['{}_shift_1'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(1))\n    df['{}_shift_2'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(2))\n    df['{}_shift_3'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(3))\n    df['{}_shift_6'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(6))\n    \n    df['{}_var_1'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(1) - x.shift(2))\n    df['{}_var_2'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(1) - x.shift(3))\n    df['{}_var_3'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(1) - x.shift(4))\n    \n    df['{}_var_pct_1'.format(column_name)]\\\n    = df.groupby(gb_list)[column_name].transform(lambda x: (x.shift(1) - x.shift(2))/x.shift(2))\n    df['{}_var_pct_2'.format(column_name)]\\\n    = df.groupby(gb_list)[column_name].transform(lambda x: (x.shift(1) - x.shift(3))/x.shift(3))\n    df['{}_var_pct_3'.format(column_name)]\\\n    = df.groupby(gb_list)[column_name].transform(lambda x: (x.shift(1) - x.shift(4))/x.shift(4))\n    \n    df.fillna(-1, inplace = True)\n\n    df.replace([np.inf, -np.inf], -1, inplace = True)\n\n    reduce_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_shift_features_no_grouper(df, column_name):\n    \n    df['{}_shift_1'.format(column_name)] = df[column_name].shift(1)\n    df['{}_shift_2'.format(column_name)] = df[column_name].shift(2)\n    df['{}_shift_3'.format(column_name)] = df[column_name].shift(3)\n    df['{}_shift_4'.format(column_name)] = df[column_name].shift(4)\n    df['{}_shift_5'.format(column_name)] = df[column_name].shift(5)\n    df['{}_shift_6'.format(column_name)] = df[column_name].shift(6)\n    df['{}_shift_8'.format(column_name)] = df[column_name].shift(8)\n    df['{}_shift_12'.format(column_name)] = df[column_name].shift(12)\n    \n    df['{}_var_1'.format(column_name)] = df[column_name].shift(1) - df[column_name].shift(2)\n    df['{}_var_2'.format(column_name)] = df[column_name].shift(1) - df[column_name].shift(3)\n    df['{}_var_3'.format(column_name)] = df[column_name].shift(1) - df[column_name].shift(4)\n    \n    df['{}_var_pct_1'.format(column_name)]\\\n    = (df[column_name].shift(1) - df[column_name].shift(2))/df[column_name].shift(2)\n    df['{}_var_pct_2'.format(column_name)]\\\n    = (df[column_name].shift(1) - df[column_name].shift(3))/df[column_name].shift(3)\n    df['{}_var_pct_3'.format(column_name)]\\\n    = (df[column_name].shift(1) - df[column_name].shift(4))/df[column_name].shift(4)\n    \n    df.fillna(-1, inplace = True)\n\n    df.replace([np.inf, -np.inf], -1, inplace = True)\n\n    reduce_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"# Generate additional features as, mean and total sales for shop_id , item_id, city ... for every month","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Date features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#st = time.time()\n#\n#def my_agg(x):\n#    names = {\n#        'month_mean': x['item_cnt_day'].mean(),\n#        'month_sum':  x['item_cnt_day'].sum()\n#    }\n#\n#    return pd.Series(names, index=[key for key in names.keys()])\n#\n#month_sales_features = full_df.groupby([\"date\"]).apply(my_agg).reset_index()\n#\n#column_name = \"month_sum\"\n#generate_shift_features_no_grouper(month_sales_features, column_name = column_name)\n#\n#column_name = \"month_mean\"\n#generate_shift_features_no_grouper(month_sales_features, column_name = column_name)\n#\n#et = time.time()\n#\n#(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#month_sales_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Date and shop_id features","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#st = time.time()\n#\n#gb_list = [\"date\", \"shop_id\"]\n#\n#fe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n#\n#fe_generator.generate_gb_df()\n#\n#shop_sales_features = fe_generator.return_gb_df()\n#\n#et = time.time()\n#\n#(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shop_sales_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shop_sales_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shop_sales_features[shop_sales_features[\"shop_id\"] == 8].T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Date and item_id features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#st = time.time()\n#\n#gb_list = [\"date\", \"item_id\"]\n#\n#fe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n#\n#fe_generator.generate_gb_df()\n#\n#item_sales_features = fe_generator.return_gb_df()\n#\n#et = time.time()\n#\n#(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#item_sales_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#item_sales_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#item_sales_features[item_sales_features[\"item_id\"] == 30].T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Date and item_category features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#st = time.time()\n#\n#gb_list = [\"date\", \"item_category_id\"]\n#\n#fe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n#\n#fe_generator.generate_gb_df()\n#\n#month_item_category_features = fe_generator.return_gb_df()\n#\n#et = time.time()\n#\n#(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#month_item_category_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#month_item_category_features[month_item_category_features[\"item_category_id\"] == 2].T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Date and type_code features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#st = time.time()\n#\n#gb_list = [\"date\", \"type_code\"]\n#\n#fe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n#\n#fe_generator.generate_gb_df()\n#\n#month_type_code_features = fe_generator.return_gb_df()\n#\n#et = time.time()\n#\n#(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#month_type_code_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#month_type_code_features[month_type_code_features[\"type_code\"] == 1].T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Date and sub_type_code features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#st = time.time()\n#\n#gb_list = [\"date\", \"subtype_code\"]\n#\n#fe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n#\n#fe_generator.generate_gb_df()\n#\n#month_subtype_code_features = fe_generator.return_gb_df()\n#\n#et = time.time()\n#\n#(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#month_subtype_code_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#month_subtype_code_features[month_subtype_code_features[\"subtype_code\"] == 1].T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Date, shop and type features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#st = time.time()\n#\n#gb_list = [\"date\", \"shop_id\",\"type_code\"]\n#\n#fe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n#\n#fe_generator.generate_gb_df()\n#\n#month_shop_type_code_features = fe_generator.return_gb_df()\n#\n#et = time.time()\n#\n#(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#month_shop_type_code_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filter_1 = month_shop_type_code_features[\"shop_id\"] == 8\n#filter_2 = month_shop_type_code_features[\"type_code\"] == 1\n#month_shop_type_code_features[filter_1 & filter_2].T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Date, shop and sub_type features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#st = time.time()\n#\n#gb_list = [\"date\", \"shop_id\",\"subtype_code\"]\n#\n#fe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n#\n#fe_generator.generate_gb_df()\n#\n#month_shop_subtype_code_features = fe_generator.return_gb_df()\n#\n#et = time.time()\n#\n#(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#month_shop_subtype_code_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filter_1 = month_shop_subtype_code_features[\"shop_id\"] == 8\n#filter_2 = month_shop_subtype_code_features[\"subtype_code\"] == 1\n#month_shop_subtype_code_features[filter_1 & filter_2].T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Date and city features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#st = time.time()\n#\n#gb_list = [\"date\",\"city_code\"]\n#\n#fe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n#\n#fe_generator.generate_gb_df()\n#\n#month_city_features = fe_generator.return_gb_df()\n#\n#et = time.time()\n#\n#(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#month_city_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#month_city_features[month_city_features[\"city_code\"] == 4].T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Date, city and item_id features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#st = time.time()\n#\n#gb_list = [\"date\",\"city_code\",\"item_id\"]\n#\n#fe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n#\n#fe_generator.generate_gb_df()\n#\n#month_city_item_id_features = fe_generator.return_gb_df()\n#\n#et = time.time()\n#\n#(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#month_city_item_id_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filter_1 = month_city_item_id_features[\"city_code\"] == 4\n#filter_2 = month_city_item_id_features[\"item_id\"] == 30\n#month_city_item_id_features[filter_1 & filter_2].T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Shift features of shop_id and item_id sales","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#st = time.time()\n#\n#gb_list = [\"shop_id\", \"item_id\"]\n#\n#column_name = \"item_cnt_day\"\n#generate_shift_features(full_df, column_name = column_name, gb_list = gb_list)\n#\n#et = time.time()\n#\n#(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Shift features of shop_id and item_id price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#st = time.time()\n#\n#gb_list = [\"shop_id\", \"item_id\"]\n#\n#column_name = \"item_price\"\n#generate_shift_features(full_df, column_name = column_name, gb_list = gb_list)\n#\n#et = time.time()\n#\n#(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Shift features of shop_id and item_id revenue","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#st = time.time()\n#\n#gb_list = [\"shop_id\", \"item_id\"]\n#\n#column_name = \"revenue\"\n#generate_shift_features(full_df, column_name = column_name, gb_list = gb_list)\n#\n#et = time.time()\n#\n#(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Datetime features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df[\"year\"] = full_df[\"date\"].dt.year\n#full_df[\"month\"] = full_df[\"date\"].dt.month\n#full_df[\"days_in_month\"] = full_df[\"date\"].dt.days_in_month\n#full_df[\"quarter_start\"] = full_df[\"date\"].dt.is_quarter_start\n#full_df[\"quarter_end\"] = full_df[\"date\"].dt.days_in_month","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding holiday data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#holidays_next_month = {\n#    12:8,\n#    1:1,\n#    2:1,\n#    3:0,\n#    4:2,\n#    5:1,\n#    6:0,\n#    7:0,\n#    8:0,\n#    9:0,\n#    10:1,\n#    11:0\n#}\n#\n#holidays_this_month = {\n#    1:8,\n#    2:1,\n#    3:1,\n#    4:0,\n#    5:2,\n#    6:1,\n#    7:0,\n#    8:0,\n#    9:0,\n#    10:0,\n#    11:1,\n#    12:0\n#}\n#\n#full_df[\"holidays_next_month\"] = full_df[\"month\"].map(holidays_next_month)\n#full_df[\"holidays_this_month\"] = full_df[\"month\"].map(holidays_this_month)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def extract_number_weekends(test_month):\n#    saturdays = len([1 for i in calendar.monthcalendar(test_month.year, test_month.month) if i[5] != 0])\n#    sundays = len([1 for i in calendar.monthcalendar(test_month.year, test_month.month) if i[6] != 0])\n#    return saturdays + sundays\n#\n#full_df[\"total_weekend_days\"] = full_df[\"date\"].apply(extract_number_weekends)\n#\n#date_diff_df = full_df[full_df[\"item_cnt_day\"] > 0][[\"shop_id\", \"item_id\", \"date\", \"item_cnt_day\"]].groupby([\"shop_id\", \"item_id\"])\\\n#[\"date\"].diff().apply(lambda timedelta_: timedelta_.days).to_frame()\n#\n#date_diff_df.columns = [\"date_diff_sales\"]\n#\n#full_df = pd.merge(full_df, date_diff_df, how = \"left\", left_index=True, right_index=True)\n#\n#full_df.fillna(-1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### City population and mean_income per city","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#city_population = {\\\n#'Якутск':307911, \n#'Адыгея':141970,\n#'Балашиха':450771, \n#'Волжский':326055, \n#'Вологда':313012, \n#'Воронеж':1047549,\n#'Выездная':1228680, \n#'Жуковский':107560, \n#'Интернет-магазин':1228680, \n#'Казань':1257391, \n#'Калуга':341892,\n#'Коломна':140129,\n#'Красноярск':1083865, \n#'Курск':452976, \n#'Москва':12678079,\n#'Мытищи':205397, \n#'Н.Новгород':1252236,\n#'Новосибирск':1602915 , \n#'Омск':1178391, \n#'РостовНаДону':1125299, \n#'СПб':5398064, \n#'Самара':1156659,\n#'СергиевПосад':104579, \n#'Сургут':373940, \n#'Томск':572740, \n#'Тюмень':744554, \n#'Уфа':1115560, \n#'Химки':244668,\n#'Цифровой':1228680, \n#'Чехов':70548, \n#'Ярославль':608353\n#}\n#\n#city_income = {\\\n#'Якутск':70969, \n#'Адыгея':28842,\n#'Балашиха':54122, \n#'Волжский':31666, \n#'Вологда':38201, \n#'Воронеж':32504,\n#'Выездная':46158, \n#'Жуковский':54122, \n#'Интернет-магазин':46158, \n#'Казань':36139, \n#'Калуга':39776,\n#'Коломна':54122,\n#'Красноярск':48831, \n#'Курск':31391, \n#'Москва':91368,\n#'Мытищи':54122, \n#'Н.Новгород':31210,\n#'Новосибирск':37014 , \n#'Омск':34294, \n#'РостовНаДону':32067, \n#'СПб':61536, \n#'Самара':35218,\n#'СергиевПосад':54122, \n#'Сургут':73780, \n#'Томск':43235, \n#'Тюмень':72227, \n#'Уфа':35257, \n#'Химки':54122,\n#'Цифровой':46158, \n#'Чехов':54122, \n#'Ярославль':34675\n#}\n#\n#full_df[\"city_population\"] = full_df[\"city\"].map(city_population)\n#\n#full_df[\"city_income\"] = full_df[\"city\"].map(city_income)\n#\n#full_df[\"price_over_income\"] = full_df[\"item_price\"]/full_df[\"city_income\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df.rename(columns = {\"item_cnt_day\":\"sales\"}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Join full sales df with all the features generated","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df = pd.merge(full_df, shop_sales_features, on = [\"date\", \"shop_id\"], how = \"left\")\n#\n#\n#full_df = pd.merge(full_df, item_sales_features, on = [\"date\", \"item_id\"], how = \"left\")\n#\n#\n#full_df = pd.merge(full_df, month_sales_features, on = [\"date\"], how = \"left\")\n#\n#\n#full_df = pd.merge(full_df, month_item_category_features, on = [\"date\", \"item_category_id\"], how = \"left\")\n#\n#\n#full_df = pd.merge(full_df, month_type_code_features, on = [\"date\", \"type_code\"], how = \"left\")\n#\n#\n#full_df = pd.merge(full_df, month_subtype_code_features, on = [\"date\", \"subtype_code\"], how = \"left\")\n#\n#\n#full_df = pd.merge(full_df, month_shop_type_code_features, on = [\"date\", \"shop_id\", \"type_code\"], how = \"left\")\n#\n#\n#full_df = pd.merge(full_df, month_shop_subtype_code_features, on = [\"date\", \"shop_id\", \"subtype_code\"], how = \"left\")\n#\n#\n#full_df = pd.merge(full_df, month_city_features, on = [\"date\", \"city_code\"], how = \"left\")\n#\n#\n#full_df = pd.merge(full_df, month_city_item_id_features, on = [\"date\", \"city_code\", \"item_id\"], how = \"left\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## delete dfs with features\n#del shop_sales_features, item_sales_features, month_sales_features, month_item_category_features,\\\n#month_type_code_features, month_subtype_code_features, month_shop_type_code_features,\\\n#month_shop_subtype_code_features, month_city_features, month_city_item_id_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## delete all the previous df\n#del shops_df, items_df, items_category_df, sales_df, test_df, sub, cartesian_product, gb_df\n#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# st = time.time()\n\n# full_df.to_pickle(os.path.join(PATH, \"FULL_DF_ONLY_TEST_ALL_FEATURES.pkl\"))\n\n# et = time.time()\n\n# (et - st)/60","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic model train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"FULL_DF_PATH = \"../input/full-df-only-test-all-features/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df = pd.read_pickle(os.path.join(FULL_DF_PATH, \"FULL_DF_ONLY_TEST_ALL_FEATURES.pkl\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SHOPS = [8, 14, 37, 41, 59]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df = full_df[full_df[\"shop_id\"].isin(SHOPS)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete all the columns where lags features are - 1 (shift(6))\nfull_df = full_df[full_df[\"date\"] > np.datetime64(\"2013-06-30\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = [\n\n'revenue',\n'shop_name',\n'city',\n'item_name',\n'item_category_name',\n'split',\n'type',\n'subtype',\n    \n'date_shop_id_sum',\n'date_shop_id_mean',\n\n'date_item_id_sum',\n'date_item_id_mean',\n\n'month_mean',\n'month_sum',\n\n'date_item_category_id_sum',\n'date_item_category_id_mean',\n\n'date_type_code_sum',\n'date_type_code_mean',\n\n'date_subtype_code_sum',\n'date_subtype_code_mean',\n\n'date_shop_id_type_code_sum',\n'date_shop_id_type_code_mean',\n\n'date_shop_id_subtype_code_sum',\n'date_shop_id_subtype_code_mean',\n\n'date_city_code_sum',\n'date_city_code_mean',\n\n'date_city_code_item_id_sum',\n'date_city_code_item_id_mean'\n    \n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.drop(cols_to_drop, inplace = True, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data into train, validation and test dataset\ntrain_index = sorted(list(full_df[\"date\"].unique()))[:-2]\n\nvalida_index = [sorted(list(full_df[\"date\"].unique()))[-2]]\n\ntest_index = [sorted(list(full_df[\"date\"].unique()))[-1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = full_df[full_df[\"date\"].isin(train_index)].drop(['sales', \"date\"], axis=1)\nY_train = full_df[full_df[\"date\"].isin(train_index)]['sales']\n\nX_valida = full_df[full_df[\"date\"].isin(valida_index)].drop(['sales', \"date\"], axis=1)\nY_valida = full_df[full_df[\"date\"].isin(valida_index)]['sales']\n\nX_test = full_df[full_df[\"date\"].isin(test_index)].drop(['sales', \"date\"], axis = 1)\nY_test = full_df[full_df[\"date\"].isin(test_index)]['sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st = time.time()\n\nmodel = XGBRegressor(seed = 175)\n\nmodel_name = str(model).split(\"(\")[0]\n\nday = str(datetime.now()).split()[0].replace(\"-\", \"_\")\nhour = str(datetime.now()).split()[1].replace(\":\", \"_\").split(\".\")[0]\nt = str(day) + \"_\" + str(hour)\n\nmodel.fit(X_train, Y_train, eval_metric = \"rmse\", \n    eval_set = [(X_train, Y_train), (X_valida, Y_valida)], \n    verbose = True, \n    early_stopping_rounds = 10)\n\net = time.time()\n\nprint(\"Training took {} minutes!\".format((et - st)/60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pickle.dump(model, open(\"{}_{}.dat\".format(model_name, t), \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = pickle.load(open(\"{}_{}.dat\".format(model_name, t), \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = model.get_booster().get_score(importance_type = \"gain\")\n\nimportance = {k: v for k, v in sorted(importance.items(), key = lambda item: item[1])}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 30))\nplot_importance(model, importance_type = \"gain\", ax = ax)\nplt.savefig(\"{}_{}_plot_importance.png\".format(model_name, t))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict and model evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_valida_pred = model.predict(X_valida)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.r2_score(Y_valida, Y_valida_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_valida = sqrt(metrics.mean_squared_error(Y_valida, Y_valida_pred))\nrmse_valida","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test_predict = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test_predict.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test_predict.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_test = sqrt(metrics.mean_squared_error(Y_test, Y_test_predict))\nrmse_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perfect_rmse = sqrt(metrics.mean_squared_error(Y_test, Y_test))\nperfect_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Homework\n\nCreate new features to improve the model performance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the basic libraries we will use in this kernel\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nimport time\nimport datetime\nfrom datetime import datetime\nimport calendar\n\nfrom sklearn import metrics\nfrom math import sqrt\nimport gc\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport itertools\nimport warnings\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nwarnings.filterwarnings(\"ignore\") # specify to ignore warning messages","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FULL_DF_PATH = \"../input/full-df-only-test-all-features/\"\nfull_df = pd.read_pickle(os.path.join(FULL_DF_PATH, \"FULL_DF_ONLY_TEST_ALL_FEATURES.pkl\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SHOPS = [8, 14, 37, 41, 59]\nfull_df = full_df[full_df[\"shop_id\"].isin(SHOPS)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reduces the memory of a dataframe\ndef reduce_mem_usage(df, verbose = True):\n    \n    '''\n    Reduces the space that a DataFrame occupies in memory.\n\n    This function iterates over all columns in a df and downcasts them to lower type to save memory.\n    '''\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\\\n              .format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n\ndef generate_shift_features_no_grouper(df, column_name):\n    \n    df['{}_shift_1'.format(column_name)] = df[column_name].shift(1)\n    df['{}_shift_2'.format(column_name)] = df[column_name].shift(2)\n    df['{}_shift_3'.format(column_name)] = df[column_name].shift(3)\n    df['{}_shift_4'.format(column_name)] = df[column_name].shift(4)\n    df['{}_shift_5'.format(column_name)] = df[column_name].shift(5)\n    df['{}_shift_6'.format(column_name)] = df[column_name].shift(6)\n    df['{}_shift_8'.format(column_name)] = df[column_name].shift(8)\n    df['{}_shift_12'.format(column_name)] = df[column_name].shift(12)\n    \n    df['{}_var_1'.format(column_name)] = df[column_name].shift(1) - df[column_name].shift(2)\n    df['{}_var_2'.format(column_name)] = df[column_name].shift(1) - df[column_name].shift(3)\n    df['{}_var_3'.format(column_name)] = df[column_name].shift(1) - df[column_name].shift(4)\n    \n    df['{}_var_pct_1'.format(column_name)]\\\n    = (df[column_name].shift(1) - df[column_name].shift(2))/df[column_name].shift(2)\n    df['{}_var_pct_2'.format(column_name)]\\\n    = (df[column_name].shift(1) - df[column_name].shift(3))/df[column_name].shift(3)\n    df['{}_var_pct_3'.format(column_name)]\\\n    = (df[column_name].shift(1) - df[column_name].shift(4))/df[column_name].shift(4)\n    \n    df.fillna(-1, inplace = True)\n\n    df.replace([np.inf, -np.inf], -1, inplace = True)\n\n    reduce_mem_usage(df)\n    \ndef generate_shift_features(df, column_name, gb_list):\n    \n    df['{}_shift_1'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(1))\n    df['{}_shift_2'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(2))\n    df['{}_shift_3'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(3))\n    df['{}_shift_4'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(4))\n    df['{}_shift_5'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(5))\n    df['{}_shift_6'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(6))\n    \n    df['{}_var_1'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(1) - x.shift(2))\n    df['{}_var_2'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(1) - x.shift(3))\n    df['{}_var_2_3'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(2) - x.shift(3))\n    df['{}_var_3'.format(column_name)] = df.groupby(gb_list)[column_name].transform(lambda x: x.shift(1) - x.shift(4))\n    \n    df['{}_var_pct_1'.format(column_name)]\\\n    = df.groupby(gb_list)[column_name].transform(lambda x: (x.shift(1) - x.shift(2))/x.shift(2))\n    df['{}_var_pct_2'.format(column_name)]\\\n    = df.groupby(gb_list)[column_name].transform(lambda x: (x.shift(1) - x.shift(3))/x.shift(3))\n    df['{}_var_pct_3'.format(column_name)]\\\n    = df.groupby(gb_list)[column_name].transform(lambda x: (x.shift(1) - x.shift(4))/x.shift(4))\n    \n    df.fillna(-1, inplace = True)\n\n    df.replace([np.inf, -np.inf], -1, inplace = True)\n\n    reduce_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Mover sales solamente por shop_id","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"st = time.time()\n\ngenerate_shift_features(full_df, column_name = 'sales', gb_list = ['shop_id'])\n\net = time.time()\n\n(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Max and mean values for each shop_id & item_id shifts","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_max_shop_id_item_id = full_df.groupby(['shop_id','item_id'])['sales'].max()\ndf_mean_shop_id_item_id = full_df.groupby(['shop_id','item_id'])['sales'].mean()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_max_shop_id_item_id = df_max_shop_id_item_id.to_frame()\ndf_max_shop_id_item_id.columns=  ['max_shop_id_item_id']\n\ndf_mean_shop_id_item_id = df_mean_shop_id_item_id.to_frame()\ndf_mean_shop_id_item_id.columns=  ['mean_shop_id_item_id']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfull_df = pd.merge(full_df, df_max_shop_id_item_id, on = [\"shop_id\",'item_id'], how = \"left\")\n\nfull_df = pd.merge(full_df, df_mean_shop_id_item_id, on = [\"shop_id\",'item_id'], how = \"left\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st = time.time()\n\ngenerate_shift_features(full_df, column_name = 'max_shop_id_item_id', gb_list = [\"shop_id\", 'item_id'])\n\ngenerate_shift_features(full_df, column_name = 'mean_shop_id_item_id', gb_list = [\"shop_id\",'item_id'])\n\net = time.time()\n\n(et - st)/60\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.drop(['max_shop_id_item_id','mean_shop_id_item_id'],axis = 1 ,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Rolling windows mean by shop_id","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import the original df to calculate the mean of the sales by each month ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LOCAL = False\n\nif LOCAL:\n    PATH = os.getcwd()\nelse:\n    PATH = '../input/competitive-data-science-predict-future-sales/'\n    PATH_EXTERNAL = \"../input/group-by-df/\"\n\n\n# load all the df we have\nshops_df = pd.read_csv(os.path.join(PATH, \"shops.csv\"))\nitems_df = pd.read_csv(os.path.join(PATH, \"items.csv\"))\nitems_category_df = pd.read_csv(os.path.join(PATH, \"item_categories.csv\"))\nsales_df = pd.read_csv(os.path.join(PATH, \"sales_train.csv\"))\ntest_df = pd.read_csv(os.path.join(PATH, \"test.csv\"))\n#sub = pd.read_csv(os.path.join(PATH, \"sample_submission.csv\"))\n\n\n\n# we have seen in our EDA that we have some duplicate shops, let's correct them.\nshops_df.loc[shops_df.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops_df['city'] = shops_df['shop_name'].str.split(' ').map(lambda x: x[0])\nshops_df.loc[shops_df.city == '!Якутск', 'city'] = 'Якутск'\nshops_df['city_code'] = LabelEncoder().fit_transform(shops_df['city'])\nshops_df.head()\n\nshops_df[shops_df[\"shop_id\"].isin([0, 57])]\n\n# Якутск Орджоникидзе, 56\nsales_df.loc[sales_df.shop_id == 0, 'shop_id'] = 57\ntest_df.loc[test_df.shop_id == 0, 'shop_id'] = 57\n\n# Якутск ТЦ \"Центральный\"\nsales_df.loc[sales_df.shop_id == 1, 'shop_id'] = 58\ntest_df.loc[test_df.shop_id == 1, 'shop_id'] = 58\n\n# Жуковский ул. Чкалова 39м²\nsales_df.loc[sales_df.shop_id == 10, 'shop_id'] = 11\ntest_df.loc[test_df.shop_id == 10, 'shop_id'] = 11\n\nitems_category_df['split'] = items_category_df['item_category_name'].str.split('-')\nitems_category_df['type'] = items_category_df['split'].map(lambda x: x[0].strip())\nitems_category_df['type_code'] = LabelEncoder().fit_transform(items_category_df['type'])\n\n# if subtype is nan then type\nitems_category_df['subtype'] = items_category_df['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitems_category_df['subtype_code'] = LabelEncoder().fit_transform(items_category_df['subtype'])\n\nitems_category_df.head()\n\n# we have negative prices and some outlier\n# let's replace the data with the mean value and also filter all the outliers\nmean = sales_df[(sales_df[\"shop_id\"] == 32) & (sales_df[\"item_id\"] == 2973) & (sales_df[\"date_block_num\"] == 4) & (sales_df[\"item_price\"] > 0)][\"item_price\"].mean()\nsales_df.loc[sales_df.item_price < 0, 'item_price'] = mean\n\nsales_df = sales_df[sales_df[\"item_price\"] < np.percentile(sales_df[\"item_price\"], q = 100)]\nsales_df = sales_df[sales_df[\"item_cnt_day\"] < np.percentile(sales_df[\"item_cnt_day\"], q = 100)]\n\n# convert to datetime the date column\n# specify the format since otherwise it might give some problems\nsales_df[\"date\"] = pd.to_datetime(sales_df[\"date\"], format = \"%d.%m.%Y\")\n\n\n# Select only the shops inside \"SHOPS\"         SHOPS = [8, 14, 37, 41, 59]\nsales_df = sales_df[sales_df[\"shop_id\"].isin(SHOPS)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df['shop_id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df = sales_df.set_index(\"date\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df['sales'] = sales_df['item_price'] * sales_df['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df = sales_df.groupby(['shop_id','item_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df = sales_df.resample('M').agg({'sales':np.mean})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df.rename(columns = {'sales':'AVG_Monthly_sales'}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df_no_index = sales_df.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df_prueba = sales_df.copy(deep = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df_prueba.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sales_df_prueba\ndf_rolling_window_1 = sales_df_prueba['AVG_Monthly_sales'].rolling(window=2).mean()\ndf_rolling_window_2 = sales_df_prueba['AVG_Monthly_sales'].rolling(window=3).mean()\ndf_rolling_window_3 = sales_df_prueba['AVG_Monthly_sales'].rolling(window=6).mean()\ndf_rolling_window_4 = sales_df_prueba['AVG_Monthly_sales'].rolling(window=12).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rolling_window_1 = df_rolling_window_1.to_frame()\ndf_rolling_window_1.columns=  ['Mean_Sales_rolling_window_shift_1']\n\ndf_rolling_window_2 = df_rolling_window_2.to_frame()\ndf_rolling_window_2.columns=  ['Mean_Sales_rolling_window_three_months_shift_1']\n\ndf_rolling_window_3 = df_rolling_window_3.to_frame()\ndf_rolling_window_3.columns=  ['Mean_Sales_rolling_window_four_months_shift_1']\n\ndf_rolling_window_4 = df_rolling_window_4.to_frame()\ndf_rolling_window_4.columns=  ['Mean_Sales_rolling_window_five_months_shift_1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rolling_window_1 = df_rolling_window_1.shift(1)\n\ndf_rolling_window_2 = df_rolling_window_2.shift(1)\n\ndf_rolling_window_3 = df_rolling_window_3.shift(1)\n\ndf_rolling_window_4 = df_rolling_window_4.shift(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df = pd.merge(full_df, df_rolling_window_1, on = ['shop_id','item_id','date'], how = \"left\")\n\nfull_df = pd.merge(full_df, df_rolling_window_2, on = ['shop_id','item_id','date'], how = \"left\")\n\nfull_df = pd.merge(full_df, df_rolling_window_3, on = ['shop_id','item_id','date'], how = \"left\")\n\nfull_df = pd.merge(full_df, df_rolling_window_4, on = ['shop_id','item_id','date'], how = \"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.fillna(0,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As the column AVG_Monthly_sales and the rest are self explanatory, they are going to be deleted\n\n#full_df.drop(['AVG_Monthly_sales','AVG_2_Months','AVG_3_Months','AVG_6_Months','AVG_12_Months'],axis = 1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rolling window for other periods","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##Rolling window of the month\n#\n##Doing the feature with the shift(1) is no longer self-explanatory\n#\n#df_rolling_window = sales_df.groupby(['shop_id','date'])['AVG_Monthly_sales']\n#\n##two months\n#df_rolling_window_1 = df_rolling_window.rolling(window=2).mean()\n#df_rolling_window_2 = df_rolling_window.rolling(window=2).max()\n#\n##three months\n#df_rolling_window_3 = df_rolling_window.rolling(window=3).mean()\n#df_rolling_window_4 = df_rolling_window.rolling(window=3).max()\n#\n##six months\n#df_rolling_window_5 = df_rolling_window.rolling(window=6).mean()\n#df_rolling_window_6 = df_rolling_window.rolling(window=6).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_rolling_window_1 = df_rolling_window_1.to_frame()\n#df_rolling_window_1.columns=  ['Mean_Sales_rolling_window_shift_2_mean']\n#\n#df_rolling_window_2 = df_rolling_window_2.to_frame()\n#df_rolling_window_2.columns=  ['Mean_Sales_rolling_window_shift_2_max']\n#\n#\n#df_rolling_window_3 = df_rolling_window_3.to_frame()\n#df_rolling_window_3.columns=  ['Mean_Sales_rolling_window_shift_3_mean']\n#\n#df_rolling_window_4 = df_rolling_window_4.to_frame()\n#df_rolling_window_4.columns=  ['Mean_Sales_rolling_window_shift_3_max']\n#\n#\n#df_rolling_window_5 = df_rolling_window_5.to_frame()\n#df_rolling_window_5.columns=  ['Mean_Sales_rolling_window_shift_6_mean']\n#\n#df_rolling_window_6 = df_rolling_window_6.to_frame()\n#df_rolling_window_6.columns=  ['Mean_Sales_rolling_window_shift_6_max']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df = pd.merge(full_df, df_rolling_window_1, on = ['shop_id','date'], how = \"left\")\n\n#full_df.drop('key_0',axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#full_df = pd.merge(full_df, df_rolling_window_1, on = df_rolling_window_1.index.to_numpy(), how = \"left\")\n#\n#full_df.drop('key_0',axis=1,inplace = True)\n#\n#\n#full_df = pd.merge(full_df, df_rolling_window_2, on = df_rolling_window_2.index.to_numpy(), how = \"left\")\n#\n#full_df.drop('key_0',axis=1,inplace = True)\n#\n#\n#\n#full_df = pd.merge(full_df, df_rolling_window_3, on = df_rolling_window_3.index.to_numpy(), how = \"left\")\n#\n#full_df.drop('key_0',axis=1,inplace = True)\n#\n#\n#\n#full_df = pd.merge(full_df, df_rolling_window_4, on = df_rolling_window_4.index.to_numpy(), how = \"left\")\n#\n#full_df.drop('key_0',axis=1,inplace = True)\n#\n#\n#\n#full_df = pd.merge(full_df, df_rolling_window_5, on = df_rolling_window_5.index.to_numpy(), how = \"left\")\n#\n#full_df.drop('key_0',axis=1,inplace = True)\n#\n#\n#\n#full_df = pd.merge(full_df, df_rolling_window_6, on = df_rolling_window_6.index.to_numpy(), how = \"left\")\n#\n#full_df.drop('key_0',axis=1,inplace = True)\n#\n#full_df.fillna(-1,inplace = True)\n#\n#reduce_mem_usage(full_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = [\n\n'revenue',\n'shop_name',\n'city',\n'item_name',\n'item_category_name',\n'split',\n'type',\n'subtype',\n    \n'date_shop_id_sum',\n'date_shop_id_mean',\n\n'date_item_id_sum',\n'date_item_id_mean',\n\n'month_mean',\n'month_sum',\n\n'date_item_category_id_sum',\n'date_item_category_id_mean',\n\n'date_type_code_sum',\n'date_type_code_mean',\n\n'date_subtype_code_sum',\n'date_subtype_code_mean',\n\n'date_shop_id_type_code_sum',\n'date_shop_id_type_code_mean',\n\n'date_shop_id_subtype_code_sum',\n'date_shop_id_subtype_code_mean',\n\n'date_city_code_sum',\n'date_city_code_mean',\n\n'date_city_code_item_id_sum',\n'date_city_code_item_id_mean'\n    \n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.drop(cols_to_drop, inplace = True, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data into train, validation and test dataset\ntrain_index = sorted(list(full_df[\"date\"].unique()))[:-2]\n\nvalida_index = [sorted(list(full_df[\"date\"].unique()))[-2]]\n\ntest_index = [sorted(list(full_df[\"date\"].unique()))[-1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = full_df[full_df[\"date\"].isin(train_index)].drop(['sales', \"date\"], axis=1)\nY_train = full_df[full_df[\"date\"].isin(train_index)]['sales']\n\nX_valida = full_df[full_df[\"date\"].isin(valida_index)].drop(['sales', \"date\"], axis=1)\nY_valida = full_df[full_df[\"date\"].isin(valida_index)]['sales']\n\nX_test = full_df[full_df[\"date\"].isin(test_index)].drop(['sales', \"date\"], axis = 1)\nY_test = full_df[full_df[\"date\"].isin(test_index)]['sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st = time.time()\n\nmodel = XGBRegressor(seed = 175)\n\nmodel_name = str(model).split(\"(\")[0]\n\nday = str(datetime.now()).split()[0].replace(\"-\", \"_\")\nhour = str(datetime.now()).split()[1].replace(\":\", \"_\").split(\".\")[0]\nt = str(day) + \"_\" + str(hour)\n\nmodel.fit(X_train, Y_train, eval_metric = \"rmse\", \n    eval_set = [(X_train, Y_train), (X_valida, Y_valida)], \n    verbose = True, \n    early_stopping_rounds = 10)\n\net = time.time()\n\nprint(\"Training took {} minutes!\".format((et - st)/60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = model.get_booster().get_score(importance_type = \"gain\")\n\nimportance = {k: v for k, v in sorted(importance.items(), key = lambda item: item[1])}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 30))\nplot_importance(model, importance_type = \"gain\", ax = ax)\nplt.savefig(\"{}_{}_plot_importance.png\".format(model_name, t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_valida_pred = model.predict(X_valida)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.r2_score(Y_valida, Y_valida_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_valida = sqrt(metrics.mean_squared_error(Y_valida, Y_valida_pred))\nrmse_valida","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test_predict = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_test = sqrt(metrics.mean_squared_error(Y_test, Y_test_predict))\nrmse_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}