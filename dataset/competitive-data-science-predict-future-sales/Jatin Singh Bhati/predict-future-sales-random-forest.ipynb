{"cells":[{"metadata":{},"cell_type":"markdown","source":"Author: Jatin Singh Bhati\n\n**Prediction of Sales using Random Forest** \n\n**Loading Libraries and Data**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport nltk\nimport datetime\n\n\ntrain_df = pd.read_csv(\"../input/sales_train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nsubmission_df = pd.read_csv(\"../input/sample_submission.csv\")\nitems_df = pd.read_csv(\"../input/items.csv\")\nitem_categories_df = pd.read_csv(\"../input/item_categories.csv\")\nshops_df = pd.read_csv(\"../input/shops.csv\")\n\nprint(\"Shape of train data : {}, Shape of test data : {}\".format(train_df.shape, test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[a for a in train_df.columns if a not in test_df]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Text features for items_df**"},{"metadata":{"trusted":true},"cell_type":"code","source":"items_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_count = 25\nitems_df['item_name_length'] = items_df['item_name'].map(lambda x : len(x)) #Length of each item_name(including punctuation in the item_name)\nitems_df['item_name_word_count'] = items_df['item_name'].map(lambda x : len(x.split(' '))) #Number of words/group of characters seperated by a whitespace\ntfidf = sklearn.feature_extraction.text.TfidfVectorizer(max_features=feature_count) #tfidf = term frequency inverse document frequency\nitems_df_item_name_text_features = pd.DataFrame(tfidf.fit_transform(items_df['item_name']).toarray())\nprint(\"Shape of items_df_item_name_text_features : {}\".format(items_df_item_name_text_features.shape))\ncols = items_df_item_name_text_features.columns\nfor idx in range(feature_count):\n    items_df['item_name_tfidf_' + str(idx)] = items_df_item_name_text_features[cols[idx]]\nitems_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Text features for item_categories_df**"},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_count = 25\nitem_categories_df['item_categories_name_length'] = item_categories_df['item_category_name'].map(lambda x : len(x)) #Length of each item_category_name(including punctuation in the item_category_name)\nitem_categories_df['item_categories_name_word_count'] = item_categories_df['item_category_name'].map(lambda x : len(x.split(' '))) #Number of words/group of characters seperated by a whitespace\ntfidf = sklearn.feature_extraction.text.TfidfVectorizer(max_features=feature_count) #tfidf = term frequency inverse document frequency\nitem_categories_df_item_category_name_text_features = pd.DataFrame(tfidf.fit_transform(item_categories_df['item_category_name']).toarray())\ncols = item_categories_df_item_category_name_text_features.columns\nfor idx in range(feature_count):\n    item_categories_df['item_category_name_tfidf_' + str(idx)] = item_categories_df_item_category_name_text_features[cols[idx]]\nitem_categories_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Text features for shops_df**"},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_count = 25\nshops_df['shop_name_length'] = shops_df['shop_name'].map(lambda x : len(x)) #Length of each shop_name(including punctuation in the shop_name)\nshops_df['shop_name_word_count'] = shops_df['shop_name'].map(lambda x : len(x.split(' '))) #Number of words/group of characters seperated by a whitespace\ntfidf = sklearn.feature_extraction.text.TfidfVectorizer(max_features=feature_count) #tfidf = term frequency inverse document frequency\nshops_df_shop_name_text_features = pd.DataFrame(tfidf.fit_transform(shops_df['shop_name']).toarray())\ncols = shops_df_shop_name_text_features.columns\nfor idx in range(feature_count):\n    shops_df['shop_name_tfidf_' + str(idx)] = shops_df_shop_name_text_features[cols[idx]]\nshops_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#turn data into monthly data\ntrain_df['date'] = pd.to_datetime(train_df['date'], format='%d.%m.%Y')\ntrain_df['month'] = train_df['date'].dt.month\ntrain_df['year'] = train_df['date'].dt.year\ntrain_df = train_df.drop(['date', 'item_price'], axis=1)\ntrain_df = train_df.groupby([c for c in train_df.columns if c not in ['item_cnt_day']], as_index=False)[['item_cnt_day']].sum()\ntrain_df = train_df.rename(columns={'item_cnt_day':'item_cnt_month'})\n\n#Monthly mean\nshop_item_monthly_mean = train_df[['shop_id', 'item_id', 'item_cnt_month']].groupby(['shop_id', 'item_id'], as_index=False)[['item_cnt_month']].mean()\nshop_item_monthly_mean = shop_item_monthly_mean.rename(columns={'item_cnt_month':'item_cnt_month_mean'})\n\n#Add Mean Features\ntrain_df = pd.merge(train_df, shop_item_monthly_mean, how='left', on=['shop_id', 'item_id'])\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Last Month : Oct 2015\nshop_item_prev_month = train_df[train_df['date_block_num'] == 33][['shop_id', 'item_id', 'item_cnt_month']]\nshop_item_prev_month = shop_item_prev_month.rename(columns={'item_cnt_month':'item_cnt_prev_month'})\nshop_item_prev_month.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add the above previous month features\ntrain_df = pd.merge(train_df, shop_item_prev_month, how='left', on=['shop_id', 'item_id'])\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.where(pd.isnull(train_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.fillna(0.)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add Item, Category and Shop features\ntrain_df = pd.merge(train_df, items_df, how='left', on='item_id')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.merge(train_df, item_categories_df, how='left', on=['item_category_id'])\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.merge(train_df, shops_df, how='left', on=['shop_id'])\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Manipulate test data\ntest_df['month'] = 11\ntest_df['year'] = 2015\ntest_df['date_block_num'] = 34\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add mean features\nshop_item_monthly_mean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.merge(test_df, shop_item_monthly_mean, how='left', on=['shop_id', 'item_id'])\nprint(len(test_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"5320 in train_df.item_id.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"5233 in train_df.item_id.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add previous month features\ntest_df = pd.merge(test_df, shop_item_prev_month, how='left', on=['shop_id', 'item_id'])\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Items features\ntest_df = pd.merge(test_df, items_df, how='left', on='item_id')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Item Category features\ntest_df = pd.merge(test_df, item_categories_df, how='left', on='item_category_id')\n#Shops features\ntest_df = pd.merge(test_df, shops_df, how='left', on='shop_id')\ntest_df = test_df.fillna(0.)\ntest_df['item_cnt_month'] = 0.\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntrain_test_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\nprint(\"train_df.shape = {}, test_df.shape = {}, train_test_df.shape = {}\".format(train_df.shape, test_df.shape, train_test_df.shape))\nstores_hm = train_test_df.pivot_table(index='shop_id', columns='item_category_id', values='item_cnt_month', aggfunc='count', fill_value=0)\nprint(\"stores_hm.shape = {}\".format(stores_hm.shape))\nstores_hm.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heatmap of \"item_cnt_month\" in \"shop_id vs item_category_id\" in \"train_test_df\"\nfig, ax = plt.subplots(figsize=(15,15))\nsns.heatmap(stores_hm, ax=ax, cbar=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heatmap of \"item_cnt_month\" in \"shop_id vs item_category_id\" in \"train_df\"\nstores_hm = train_df.pivot_table(index='shop_id', columns='item_category_id', values='item_cnt_month', aggfunc='count', fill_value=0)\n_, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(stores_hm, ax=ax, cbar=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heatmap of \"item_cnt_month\" in \"shop_id vs item_category_id\" in \"test_df\"\nstores_hm = test_df.pivot_table(index='shop_id', columns='item_category_id', values='item_cnt_month', aggfunc='count', fill_value=0)\n_, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(stores_hm, ax=ax, cbar=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Label Encoding**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in ['shop_name', 'item_category_name', 'item_name']:\n    le = sklearn.preprocessing.LabelEncoder()\n    le.fit(list(train_df[c].unique()) + list(test_df[c].unique()))\n    train_df[c] = le.transform(train_df[c].astype(str))\n    test_df[c] = le.transform(test_df[c].astype(str))\n    print(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**shop_id and labels for shop_name are the same and hence redundant. Similarly in the case of item_name and item_category_name. One in each redundant pair can be dropped.**\n\n**These are not dropped for the time being, since variable importance takes care of it later in this notebook.**"},{"metadata":{},"cell_type":"markdown","source":" **Train & Predict Models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df['shop_id'], train_df['shop_name'])\nprint('*'*80)\ntest_df.head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_list = [c for c in train_df.columns if c not in 'item_cnt_month']\n#Validation hold out month is 33\nx1 = train_df[train_df['date_block_num'] < 33]\ny1 = np.log1p(x1['item_cnt_month'].clip(0., 20.))\nx1 = x1[feature_list]\nx2 = train_df[train_df['date_block_num'] == 33]\ny2 = np.log1p(x2['item_cnt_month'].clip(0., 20.))\nx2 = x2[feature_list]\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators=25, random_state=42, max_depth=15, n_jobs=-1)#use n_estimators=25, max_depth=15, random_state=42(for consistent results)\nrf.fit(x1, y1)\nprint(\"RMSE on Validation hold out month 33: {}\".format(np.sqrt(sklearn.metrics.mean_squared_error(y2.clip(0., 20.), rf.predict(x2).clip(0., 20.)))))\n\n#Full train\nrf.fit(train_df[feature_list], train_df['item_cnt_month'].clip(0., 20.))\nprint(\"Accuracy on training data without considering variable importances:{}\".format(round(rf.score(train_df[feature_list], train_df['item_cnt_month'].clip(0., 20.))*100, 2)))\n\n#predict\ntest_df['item_cnt_month'] = rf.predict(test_df[feature_list]).clip(0., 20.)\n\n#create submission file\ntest_df[['ID', 'item_cnt_month']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualization of a single decision tree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nimport pydot\nfrom IPython.display import Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#doesn't print. May be because its too large. Try with a smaller image.\n'''tree = rf.estimators_[3]\nexport_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n(graph, ) = pydot.graph_from_dot_file('tree.dot')\ngraph.write_png('tree.png')\nImage(filename='tree.png')'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To see the decision tree in action, visualizing with a smaller one for illustrative purposes\nrf_small = RandomForestRegressor(n_estimators=2, random_state=42, max_depth=3, n_jobs=-1)\nrf_small.fit(train_df[feature_list], train_df['item_cnt_month'].clip(0., 20.))\nsmall_tree = rf_small.estimators_[1]\nexport_graphviz(small_tree, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\ngraph.write_png('small_tree.png')\nImage(filename='small_tree.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Variable Importances**\n\n**In order to quantify the usefulness of all the variables in the entire random forest, we can look at the relative importances of the variables.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get numerical feature importances\nimportances = list(rf.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Simple bar plot of the feature importances to illustrate the disparities in the relative significance of the variables.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the style\nplt.style.use('fivethirtyeight')\n\n#set size\nfig = plt.figure(figsize=(25, 5))\nax = fig.add_subplot(111)\n\n# list of x locations for plotting\nx_values = list(range(len(importances)))\n\n# Make a bar chart\nax.bar(x_values, importances, orientation = 'vertical')\n\n# Tick labels for x axis\nplt.xticks(x_values, feature_list, rotation='vertical', fontsize=12)\n\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**New random forest with only ten most important variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_most_important = RandomForestRegressor(n_estimators=25, random_state=42, max_depth=15, n_jobs=-1)\n\n# Extract the ten most important features\nimportant_features = ['item_cnt_month_mean', 'date_block_num', 'item_cnt_prev_month', 'item_id', 'item_name', 'item_name_length', 'year', 'item_name_word_count', 'item_name_tfidf_21', 'item_category_name_tfidf_6']\n\n#Full train\nrf_most_important.fit(train_df[important_features], train_df['item_cnt_month'].clip(0., 20.))\nprint(\"Accuracy on training data considering variable importances:{}\".format(round(rf_most_important.score(train_df[important_features], train_df['item_cnt_month'].clip(0., 20.))*100, 2)))\n\n#predict\ntest_df['item_cnt_month'] = rf_most_important.predict(test_df[important_features]).clip(0., 20.)\n\n#create submission file\ntest_df[['ID', 'item_cnt_month']].to_csv('submission_variable_importance.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For the available data, new random forest with only ten most important variables works better than the one which considers all features.**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}