{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nimport time\nimport sys\nimport gc\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# always start with checking out the files!\n!ls ../input/*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import all of them \ntrain=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\ncats=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nitems=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nshops=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\npopgdp=pd.read_csv(\"../input/russia-populationgdppercapita-predictsales/russia_popgdp_predictsales.csv\")\n# set index to ID to avoid droping it later\ntest=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\").set_index('ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1001]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correct item price for negative price\ntrain.loc[train.item_price<0, 'item_price']=train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop duplicates from tain and test, all columns\ntrain=train.drop_duplicates(keep = 'last').reset_index(drop = True)\ntrain=train.drop_duplicates(subset=['date','date_block_num','shop_id','item_id','item_cnt_day'],keep = 'last').reset_index(drop = True)\n#Drop duplicates from columns\nprint(train[train.duplicated(['date','date_block_num','shop_id','item_id'])])\n#Correction 1\ntrain.loc[(train.date=='27.01.2013')&(train.date_block_num==0)&(train.shop_id==38)&(train.item_id==15702),'item_price']=train.loc[(train.date=='27.01.2013')&(train.date_block_num==0)&(train.shop_id==38)&(train.item_id==15702)].item_price.mean()\ntrain.loc[(train.date=='27.01.2013')&(train.date_block_num==0)&(train.shop_id==38)&(train.item_id==15702),'item_cnt_day']=train.loc[(train.date=='27.01.2013')&(train.date_block_num==0)&(train.shop_id==38)&(train.item_id==15702)].item_cnt_day.sum()\n#Correction 2\ntrain.loc[(train.date=='29.12.2013')&(train.date_block_num==11)&(train.shop_id==27)&(train.item_id==12133),'item_price']=train.loc[(train.date=='29.12.2013')&(train.date_block_num==11)&(train.shop_id==27)&(train.item_id==12133)].item_price.mean()\ntrain.loc[(train.date=='29.12.2013')&(train.date_block_num==11)&(train.shop_id==27)&(train.item_id==12133),'item_cnt_day']=train.loc[(train.date=='29.12.2013')&(train.date_block_num==11)&(train.shop_id==27)&(train.item_id==12133)].item_cnt_day.sum()\n#Correction 3\ntrain.loc[(train.date=='26.12.2014')&(train.date_block_num==23)&(train.shop_id==17)&(train.item_id==3424),'item_price']=train.loc[(train.date=='26.12.2014')&(train.date_block_num==23)&(train.shop_id==17)&(train.item_id==3424)].item_price.mean()\ntrain.loc[(train.date=='26.12.2014')&(train.date_block_num==23)&(train.shop_id==17)&(train.item_id==3424),'item_cnt_day']=train.loc[(train.date=='26.12.2014')&(train.date_block_num==23)&(train.shop_id==17)&(train.item_id==3424)].item_cnt_day.sum()\n#Correction 4\ntrain.loc[(train.date=='17.02.2015')&(train.date_block_num==25)&(train.shop_id==5)&(train.item_id==21619),'item_price']=train.loc[(train.date=='17.02.2015')&(train.date_block_num==25)&(train.shop_id==5)&(train.item_id==21619)].item_price.mean()\ntrain.loc[(train.date=='17.02.2015')&(train.date_block_num==25)&(train.shop_id==5)&(train.item_id==21619),'item_cnt_day']=train.loc[(train.date=='17.02.2015')&(train.date_block_num==25)&(train.shop_id==5)&(train.item_id==21619)].item_cnt_day.sum()\n#Drop duplicates again\ntrain=train.drop_duplicates(keep = 'last').reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shops df preprocessing\nshops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'#Change names with \" \"\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nlen(shops.city.unique())\npopgdp=popgdp.fillna(0)\nshops=pd.merge(shops, popgdp, on=['city'], how='left')\nshops = shops[['shop_id','city_code','population','GMP_per_capita']]\n#Category df preprocessing\ncats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n## if subtype is nan then type\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]\n#items df preprocessing\nitems.drop(['item_name'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identify the new products in test\nlen(list(set(test.item_id) - set(test.item_id).intersection(set(train.item_id)))), len(list(set(test.item_id))), len(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in train.date_block_num.unique():\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['revenue'] = train['item_price'] *  train['item_cnt_day']\nts = time.time()\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#adding test to matrix\ntest['date_block_num'] = len(train.date_block_num.unique())\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34th month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Target lags"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = lag_feature(matrix, [1,2], 'item_cnt_month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_type_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'city_code'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\nmatrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\nmatrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# matrix['month'] = matrix['date_block_num'] % 12\n\n# days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n# matrix['days'] = matrix['month'].map(days).astype(np.int8)\n\n# cache = {}\n# matrix['item_shop_last_sale'] = -1\n# matrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\n# for idx, row in matrix.iterrows():    \n#     key = str(row.item_id)+' '+str(row.shop_id)\n#     if key not in cache:\n#         if row.item_cnt_month!=0:\n#             cache[key] = row.date_block_num\n#     else:\n#         last_date_block_num = cache[key]\n#         matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n#         cache[key] = row.date_block_num ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cache = {}\n# matrix['item_last_sale'] = -1\n# matrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\n# for idx, row in matrix.iterrows():    \n#     key = row.item_id\n#     if key not in cache:\n#         if row.item_cnt_month!=0:\n#             cache[key] = row.date_block_num\n#     else:\n#         last_date_block_num = cache[key]\n#         if row.date_block_num>last_date_block_num:\n#             matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n#             cache[key] = row.date_block_num     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# matrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n# matrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#matrix = matrix[matrix.date_block_num > 11]\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix['population'] = matrix['population'].astype(np.int8)\nmatrix.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#del cache\ndel popgdp\ndel group\ndel items\ndel shops\ndel cats\ndel train\n# leave test for submission\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = matrix[matrix.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = matrix[matrix.date_block_num < 33]['item_cnt_month']\nX_valid = matrix[matrix.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = matrix[matrix.date_block_num == 33]['item_cnt_month']\nX_test = matrix[matrix.date_block_num == 34].drop(['item_cnt_month'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_features(model, (10,14))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}