{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport pickle\nfrom statsmodels.api import OLS\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\n\nitems = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\")\nitem_categories = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv\")\nsales_train = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv\")\nshops = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv\")\nsales_test = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/test.csv\")\nsales_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing outliers and removing them**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sns.boxplot(sales_train[\"item_cnt_day\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(sales_train[\"item_price\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train[\"item_cnt_day\"] = sales_train[\"item_cnt_day\"].clip(0,200)\nsales_train[\"item_price\"] = sales_train[\"item_price\"].clip(0,5000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Statistics and Visualizations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# item counts over months\nsales_train.groupby(\"date_block_num\")[\"item_cnt_day\"].sum().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# distribution of shop_item combinations in train and test set\n# only a few shops/items are in test and train set\n# no leakage can be identified, split seems random\n\ntrain_unique = sales_train.groupby([\"shop_id\", \"item_id\"]).size()\ntest_unique = sales_test.groupby([\"shop_id\", \"item_id\"]).size()\n\ntrain_unique = pd.DataFrame({\"in_train\":True}, index=train_unique.index)\ntest_unique = pd.DataFrame({\"in_test\":True}, index=test_unique.index)\n\ncombined = pd.merge(train_unique, test_unique, on=[\"shop_id\", \"item_id\"], how=\"outer\").fillna(False)\n\ncombined[\"in_both\"] = combined[\"in_train\"] & combined[\"in_test\"]\n\nnum_in_both = sum(combined[\"in_both\"] == True)\nnum_in_train = sum((combined[\"in_train\"] == True) & (combined[\"in_test\"] == False))\nnum_in_test = sum((combined[\"in_test\"] == True) & (combined[\"in_train\"] == False))\n\npd.DataFrame({\"in_both\":[num_in_both], \n              \"in_train\": [num_in_train],\n              \"in_test\": [num_in_test]}).T.plot.pie(subplots=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Process data into nice represenation of counts for each item, shop and date_block combination\nfrom itertools import product\n\nmatrix = []\nindex_cols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\nfor i in range(34):\n    sales = sales_train[sales_train.date_block_num == i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype=np.int16))\n\nmatrix = pd.DataFrame(np.vstack(matrix), columns=index_cols)\nmatrix[\"date_block_num\"] = matrix[\"date_block_num\"].astype(np.int8)\nmatrix[\"shop_id\"] = matrix[\"shop_id\"].astype(np.int8)\nmatrix[\"item_id\"] = matrix[\"item_id\"].astype(np.int16)\nmatrix.sort_values(index_cols, inplace = True )\nmatrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add target variable (item counts for each month) to data\nitem_counts = sales_train.groupby(index_cols).agg({\"item_cnt_day\": [\"sum\"]}).reset_index()\n\nmatrix = pd.merge(matrix, item_counts, on=index_cols, how=\"left\")\nmatrix.fillna(0, inplace=True)\nmatrix.rename(columns={matrix.columns[-1]: \"item_cnt_month\"}, inplace=True)\nmatrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].clip(0,20).astype(np.float16)\nmatrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add test data to matrix\nsales_test[\"date_block_num\"] = 34\nmatrix = pd.concat([matrix, sales_test.drop([\"ID\"], axis=1)], ignore_index=True, keys=index_cols)\nmatrix.fillna(0, inplace=True)\nmatrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# downgrade types\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Generation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lag features and mean encoding\n\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id', col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df\n\nlag_dates = [1,2,3,6,12]\n\nmatrix = lag_feature(matrix, lag_dates, \"item_cnt_month\")\nmatrix.fillna(0, inplace=True)\n\n# mean item count per month\nfeature = matrix.groupby(['date_block_num']).agg({'item_cnt_month': 'mean'})\nfeature.index = feature.index+1\nfeature.columns = [\"item_cnt_month_mean_lag_1\"]\nmatrix = pd.merge(matrix, feature, on=[\"date_block_num\"], how=\"left\")\n\n# item category\nmatrix = pd.merge(matrix, items.drop([\"item_name\"], axis=1), how=\"left\", on=\"item_id\")\n\n# mean count of item_id\nfeature = matrix.groupby(['date_block_num', \"item_id\"]).agg({'item_cnt_month': 'mean'}).reset_index()\nfeature\nfeature.rename({\"item_cnt_month\": \"means_cnt_item_id_lag_1\"}, axis=1, inplace=True)\nfeature[\"date_block_num\"] += 1\nfeature\nmatrix = pd.merge(matrix, feature, on=[\"date_block_num\", \"item_id\"], how=\"left\")\n\n# mean count of shop_id\nfeature = matrix.groupby(['date_block_num', \"shop_id\"]).agg({'item_cnt_month': 'mean'}).reset_index()\nfeature\nfeature.rename({\"item_cnt_month\": \"means_cnt_shop_id_lag_1\"}, axis=1, inplace=True)\nfeature[\"date_block_num\"] += 1\nfeature\nmatrix = pd.merge(matrix, feature, on=[\"date_block_num\", \"shop_id\"], how=\"left\")\n\n# number of counts for shop/item id combination per date_block_num\npiv = sales_train.pivot_table(values=\"item_price\", index=[\"shop_id\", \"item_id\"] ,columns=\"date_block_num\", aggfunc=\"mean\")\npiv = piv.fillna(method=\"ffill\", axis=1).fillna(method=\"bfill\", axis=1)\npiv_diff = piv.diff(axis=1)\npiv_diff.columns += 1\npiv_div = piv_diff.stack()\npiv_div.name = \"price_diff_lag_1\"\nmatrix = pd.merge(matrix, piv_div, on=[\"shop_id\", \"item_id\", \"date_block_num\"], how=\"left\")\n\nmatrix.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# textual and coordinate features from shop and category\n\nshops['city'] = shops['shop_name'].apply(lambda x: x.split()[0].lower())\nshops.loc[shops.city == '!якутск', 'city'] = 'якутск'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n\ncoords = dict()\ncoords['якутск'] = (62.028098, 129.732555, 4)\ncoords['адыгея'] = (44.609764, 40.100516, 3)\ncoords['балашиха'] = (55.8094500, 37.9580600, 1)\ncoords['волжский'] = (53.4305800, 50.1190000, 3)\ncoords['вологда'] = (59.2239000, 39.8839800, 2)\ncoords['воронеж'] = (51.6720400, 39.1843000, 3)\ncoords['выездная'] = (0, 0, 0)\ncoords['жуковский'] = (55.5952800, 38.1202800, 1)\ncoords['интернет-магазин'] = (0, 0, 0)\ncoords['казань'] = (55.7887400, 49.1221400, 4)\ncoords['калуга'] = (54.5293000, 36.2754200, 4)\ncoords['коломна'] = (55.0794400, 38.7783300, 4)\ncoords['красноярск'] = (56.0183900, 92.8671700, 4)\ncoords['курск'] = (51.7373300, 36.1873500, 3)\ncoords['москва'] = (55.7522200, 37.6155600, 1)\ncoords['мытищи'] = (55.9116300, 37.7307600, 1)\ncoords['н.новгород'] = (56.3286700, 44.0020500, 4)\ncoords['новосибирск'] = (55.0415000, 82.9346000, 4)\ncoords['омск'] = (54.9924400, 73.3685900, 4)\ncoords['ростовнадону'] = (47.2313500, 39.7232800, 3)\ncoords['спб'] = (59.9386300, 30.3141300, 2)\ncoords['самара'] = (53.2000700, 50.1500000, 4)\ncoords['сергиев'] = (56.3000000, 38.1333300, 4)\ncoords['сургут'] = (61.2500000, 73.4166700, 4)\ncoords['томск'] = (56.4977100, 84.9743700, 4)\ncoords['тюмень'] = (57.1522200, 65.5272200, 4)\ncoords['уфа'] = (54.7430600, 55.9677900, 4)\ncoords['химки'] = (55.8970400, 37.4296900, 1)\ncoords['цифровой'] = (0, 0, 0)\ncoords['чехов'] = (55.1477000, 37.4772800, 4)\ncoords['ярославль'] = (57.6298700, 39.8736800, 2) \n\nshops['city_coord_1'] = shops['city'].apply(lambda x: coords[x][0])\nshops['city_coord_2'] = shops['city'].apply(lambda x: coords[x][1])\nshops['country_part'] = shops['city'].apply(lambda x: coords[x][2])\n\nshops = shops[['shop_id', 'city_code', 'city_coord_1', 'city_coord_2', 'country_part']]\n\nmatrix = pd.merge(matrix, shops, on=[\"shop_id\"], how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# features about items and their category names (common category and category code)\n\nmap_dict = {\n            'Чистые носители (штучные)': 'Чистые носители',\n            'Чистые носители (шпиль)' : 'Чистые носители',\n            'PC ': 'Аксессуары',\n            'Служебные': 'Служебные '\n            }\n\nitems = pd.merge(items, item_categories, on='item_category_id')\n\nitems['item_category'] = items['item_category_name'].apply(lambda x: x.split('-')[0])\nitems['item_category'] = items['item_category'].apply(lambda x: map_dict[x] if x in map_dict.keys() else x)\nitems['item_category_common'] = LabelEncoder().fit_transform(items['item_category'])\n\nitems['item_category_code'] = LabelEncoder().fit_transform(items['item_category_name'])\nitems = items[['item_id', 'item_category_common', 'item_category_code']]\n\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# interaction features: \n# - is item new\n# - has it been bought in shop before\n\nfirst_item_block = matrix.groupby(['item_id'])['date_block_num'].min().reset_index()\nfirst_item_block['item_first_interaction'] = 1\n\nfirst_shop_item_buy_block = matrix[matrix['date_block_num'] > 0].groupby(['shop_id', 'item_id'])['date_block_num'].min().reset_index()\nfirst_shop_item_buy_block['first_date_block_num'] = first_shop_item_buy_block['date_block_num']\n\nmatrix = pd.merge(matrix, first_item_block[['item_id', 'date_block_num', 'item_first_interaction']], on=['item_id', 'date_block_num'], how='left')\nmatrix = pd.merge(matrix, first_shop_item_buy_block[['item_id', 'shop_id', 'first_date_block_num']], on=['item_id', 'shop_id'], how='left')\n\nmatrix['first_date_block_num'].fillna(100, inplace=True)\nmatrix['shop_item_sold_before'] = (matrix['first_date_block_num'] < matrix['date_block_num']).astype('int8')\nmatrix.drop(['first_date_block_num'], axis=1, inplace=True)\n\nmatrix['item_first_interaction'].fillna(0, inplace=True)\nmatrix['shop_item_sold_before'].fillna(0, inplace=True)\n \nmatrix['item_first_interaction'] = matrix['item_first_interaction'].astype('int8')  \nmatrix['shop_item_sold_before'] = matrix['shop_item_sold_before'].astype('int8') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove first 12 months and split into train, validation and test data\nmatrix = matrix[matrix[\"date_block_num\"] >= 12]\n\nX_train = matrix[matrix[\"date_block_num\"] < 33].drop([\"item_cnt_month\"], axis=1)\nX_val = matrix[matrix[\"date_block_num\"] == 33].drop([\"item_cnt_month\"], axis=1)\nX_test = matrix[matrix[\"date_block_num\"] == 34].drop([\"item_cnt_month\"], axis=1)\n\ny_train = matrix[matrix[\"date_block_num\"] < 33][\"item_cnt_month\"]\ny_val = matrix[matrix[\"date_block_num\"] == 33][\"item_cnt_month\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quick linear regression analysis on validation data\n\nresults = sm.OLS(y_val.to_numpy(), X_val.astype(float)).fit()\nresults.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Validation and Testing**\n\nUse XGBoost and LightGBM to for prediction and combine the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nxgb_model = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nxgb_model.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_val, y_val)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\npickle.dump(xgb_model, open(\"xgb_model.p\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_model = LGBMRegressor(\n    n_estimators=200,\n    learning_rate=0.03,\n    num_leaves=32,\n    colsample_bytree=0.9,\n    subsample=0.8,\n    max_depth=8,\n    reg_alpha=0.04,\n    reg_lambda=0.07,\n    min_split_gain=0.02,\n    min_child_weight=40,\n    seed=42\n)\n\nlgbm_model.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_val, y_val)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\npickle.dump(lgbm_model, open(\"lgbm_model.p\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb feature importance\ncols = X_val.columns\nplt.figure(figsize=(10,10))\nplt.barh(cols, xgb_model.feature_importances_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lgbm feature importance\n\ncols = X_val.columns\nplt.figure(figsize=(10,10))\nplt.barh(cols, lgbm_model.feature_importances_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict and combine using a linear combination of results\n\nxgb_preds = xgb_model.predict(X_test)\nlgbm_preds = lgbm_model.predict(X_test)\n\nfinal_preds = (xgb_preds + lgbm_preds) / 2\nfinal_preds = final_preds.clip(0,20)\nfinal_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"item_cnt_month\": final_preds})\nsubmission.index.name=\"ID\"\nsubmission\nsubmission.to_csv(\"submission.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}