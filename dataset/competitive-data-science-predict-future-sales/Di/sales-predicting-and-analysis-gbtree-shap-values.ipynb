{"cells":[{"metadata":{},"cell_type":"markdown","source":"1. Simple Visualization \n\n\n2. Data Cleaning\n\n\n3. Feature Engineering\n\n   3.1 Item Categories\n   \n   3.2 Shops\n   \n   3.3 Items\n   \n   3.4 Merge training and test data\n   \n   3.5 Sales Lag creation\n   \n   3.6 Price Trend Lag creation\n   \n   3.7 Time features creation\n   \n   \n4. Fine tuning and modeling \n   \n   4.1 XGBoost\n   \n   4.2 LightGBM\n   \n   \n5. Fit result plot and Analysis\n   "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport itertools\nimport gc\nimport tensorflow as tf\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport optuna.integration.lightgbm as lgb\nimport lightgbm as lgbm\nimport warnings\nimport shap\n\npd.set_option('display.max_columns',None)\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple Visualization"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"ic = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nitem = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nshop = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\ntrain = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv').set_index('ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop.shape[0],item.shape[0],ic.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"60 unique shops , 22170 unique items and 84 unique category of items"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.subplot(121)\nplt.boxplot(train['item_price'])\nplt.title('Item Price')\nplt.subplot(122)\nplt.boxplot(train['item_cnt_day'])\nplt.title('Item count Day')\nplt.tight_layout(pad=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the box plot, the outlier in item price and item sales number is obvious."},{"metadata":{},"cell_type":"markdown","source":"#  Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[(train['item_price']<10000) & (train['item_cnt_day']<1001)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negp = train[train['item_price']<=0]\nprint(negp)\nmedian = train[(train.shop_id==32)&(train.item_id==2973)&\n               (train.date_block_num==4)&(train.item_price>0)]['item_price'].median()\ntrain.loc[train.item_price<0,'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use median price to modify the data with negative prices"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.shop_id == 0,'shop_id']=57\ntest.loc[test.shop_id == 0,'shop_id']=57\ntrain.loc[train.shop_id == 1,'shop_id']=58\ntest.loc[test.shop_id == 1,'shop_id']=58\ntrain.loc[train.shop_id == 10,'shop_id']=11\ntest.loc[test.shop_id == 10,'shop_id']=11","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shop name of shop id 0 and 57, 1 and 58, 10 and 11 are the duplicated pairs. So that we should modify it in both training and test dataset"},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"## Item Categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"ic['split'] = ic['item_category_name'].apply(lambda x: x.split('-'))\nic['type'] = ic['split'].apply(lambda x:x[0].strip())\nic['type_code'] = LabelEncoder().fit_transform(ic['type'])\nic['subtype'] = ic['split'].map(lambda x: x[1].strip() if len(x)>1 else x[0].strip())\nic['subtype_code'] = LabelEncoder().fit_transform(ic['subtype'])\nic = ic[['item_category_id','type_code','subtype_code']]\nic.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Shops"},{"metadata":{"trusted":true},"cell_type":"code","source":"shop['split'] = shop['shop_name'].apply(lambda x: x.split(' '))\nshop['city'] = shop['split'].apply(lambda x: x[0].strip())\nshop['city_code'] = LabelEncoder().fit_transform(shop['city'])\nshop = shop[['shop_id','city_code']]\nshop.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Items"},{"metadata":{"trusted":true},"cell_type":"code","source":"item.drop(['item_name'],axis=1,inplace=True)\nitem.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merge training data and test data"},{"metadata":{},"cell_type":"markdown","source":"1. shops in training data, but not in test data\n2. items in test data, but not in training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(test['item_id'])) - len(set(train['item_id']).intersection(set(test['item_id'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"387 new items found in the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(train['shop_id'])) - len(set(train['shop_id']).intersection(set(test['shop_id'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"15 shops are not in the test data, but are found in training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train['shop_id'].isin(test.shop_id.unique())]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop training data with shops not in test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_match = []\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    train_match.append(np.array(list(itertools.product([i],sales.shop_id.unique(),\n                                        sales.item_id.unique())),dtype=np.int16))\n\ntrain_match = pd.DataFrame(np.vstack(train_match),columns =['date_block_num','shop_id','item_id'])\ntrain_match.sort_values(by = ['date_block_num','shop_id','item_id'],inplace=True)\ntrain_match.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use itertools.product to create all possible (shop,item) pairs in all months"},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped = train[['date_block_num','shop_id','item_id',\n                 'item_cnt_day']].groupby(['date_block_num','shop_id','item_id']).agg('sum')\ngrouped.columns = ['item_cnt_month']\ngrouped.reset_index(inplace = True)\ntrain_match = pd.merge(train_match,grouped,on=['date_block_num','shop_id','item_id'],how='left')\n\ntrain_match['item_cnt_month'] = (train_match['item_cnt_month']\n                                 .fillna(0).clip(0,20).astype(np.float16))\ntrain_match.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fill in the created (shop,item) pairs with zero and clip the item_cnt_month into (0,20)"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['date_block_num'] = 34\ntrain_match = pd.concat([train_match,test],ignore_index=True, axis=0,\n                        sort = False,keys = ['date_block_num','shop_id','item_id'])\ntrain_match.fillna(0,inplace=True)\ntrain_match.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Concanate training and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_match = pd.merge(train_match, item, on=['item_id'], how = 'left')\ntrain_match = pd.merge(train_match, ic, on=['item_category_id'], how = 'left')\ntrain_match = pd.merge(train_match, shop, on=['shop_id'], how='left')\ntrain_match.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Item sales feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_lag_feature(df,lags,col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for lag in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id',col+'_lag_'+str(lag)]\n        ##col value is the timestamp 'lag' (months/date_block_num) before \n        shifted['date_block_num'] += lag\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'],how = 'left')\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After adding the value of lag to date_block_num in shifted dataframe and concatenating it with original dataframe, the lag feature is added."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_match = create_lag_feature(train_match,[1,2,3,6,12],'item_cnt_month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped2 = train_match.groupby(['date_block_num']).agg({'item_cnt_month':['mean']})\ngrouped2.columns = ['avg_item_cnt_month']\ngrouped2.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_match = pd.merge(train_match, grouped2, on=['date_block_num'], how='left')\ntrain_match['avg_item_cnt_month'] = train_match['avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1],'avg_item_cnt_month')\ntrain_match.drop(['avg_item_cnt_month'],axis =1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped3 = train_match.groupby(['date_block_num','item_id']).agg({'item_cnt_month':['mean']})\ngrouped3.columns = ['item_avg_item_cnt_month']\ngrouped3.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped3, on=['date_block_num','item_id'], how ='left')\ntrain_match['item_avg_item_cnt_month'] = train_match['item_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1,2,3,6,12], 'item_avg_item_cnt_month')\ntrain_match.drop(['item_avg_item_cnt_month'], axis =1, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped4 = train_match.groupby(['date_block_num','shop_id']).agg({'item_cnt_month':['mean']})\ngrouped4.columns = ['shop_avg_item_cnt_month']\ngrouped4.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped4, on=['date_block_num','shop_id'], how ='left')\ntrain_match['shop_avg_item_cnt_month'] = train_match['shop_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1,2,3,6,12], 'shop_avg_item_cnt_month')\ntrain_match.drop(['shop_avg_item_cnt_month'], axis =1, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped5 = train_match.groupby(['date_block_num','item_category_id']).agg({'item_cnt_month':['mean']})\ngrouped5.columns = ['ic_avg_item_cnt_month']\ngrouped5.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped5, on=['date_block_num','item_category_id'], how ='left')\ntrain_match['ic_avg_item_cnt_month'] = train_match['ic_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'ic_avg_item_cnt_month')\ntrain_match.drop(['ic_avg_item_cnt_month'], axis =1, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped6 = train_match.groupby(['date_block_num','type_code']).agg({'item_cnt_month':['mean']})\ngrouped6.columns = ['type_avg_item_cnt_month']\ngrouped6.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped6, on=['date_block_num','type_code'], how ='left')\ntrain_match['type_avg_item_cnt_month'] = train_match['type_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'type_avg_item_cnt_month')\ntrain_match.drop(['type_avg_item_cnt_month'], axis =1, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped7 = train_match.groupby(['date_block_num','subtype_code']).agg({'item_cnt_month':['mean']})\ngrouped7.columns = ['subtype_avg_item_cnt_month']\ngrouped7.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped7, on=['date_block_num','subtype_code'], how ='left')\ntrain_match['subtype_avg_item_cnt_month'] = train_match['subtype_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'subtype_avg_item_cnt_month')\ntrain_match.drop(['subtype_avg_item_cnt_month'], axis =1, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped8 = train_match.groupby(['date_block_num','city_code']).agg({'item_cnt_month':['mean']})\ngrouped8.columns = ['city_avg_item_cnt_month']\ngrouped8.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped8, on=['date_block_num','city_code'], how ='left')\ntrain_match['city_avg_item_cnt_month'] = train_match['city_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'city_avg_item_cnt_month')\ntrain_match.drop(['city_avg_item_cnt_month'], axis =1, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped9 = train_match.groupby(['date_block_num','shop_id','item_category_id']).agg({'item_cnt_month':['mean']})\ngrouped9.columns = ['shop_ic_avg_item_cnt_month']\ngrouped9.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped9, on=['date_block_num','shop_id','item_category_id'], how ='left')\ntrain_match['shop_ic_avg_item_cnt_month'] = train_match['shop_ic_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'shop_ic_avg_item_cnt_month')\ntrain_match.drop(['shop_ic_avg_item_cnt_month'], axis =1, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped10 = train_match.groupby(['date_block_num','shop_id','type_code']).agg({'item_cnt_month':['mean']})\ngrouped10.columns = ['shop_type_avg_item_cnt_month']\ngrouped10.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped10, on=['date_block_num','shop_id','type_code'], how ='left')\ntrain_match['shop_type_avg_item_cnt_month'] = train_match['shop_type_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'shop_type_avg_item_cnt_month')\ntrain_match.drop(['shop_type_avg_item_cnt_month'], axis =1, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped11 = train_match.groupby(['date_block_num','shop_id','subtype_code']).agg({'item_cnt_month':['mean']})\ngrouped11.columns = ['shop_subtype_avg_item_cnt_month']\ngrouped11.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped11, on=['date_block_num','shop_id','subtype_code'], how ='left')\ntrain_match['shop_subtype_avg_item_cnt_month'] = train_match['shop_subtype_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'shop_subtype_avg_item_cnt_month')\ntrain_match.drop(['shop_subtype_avg_item_cnt_month'], axis =1, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped12 = train_match.groupby(['date_block_num','item_id','city_code']).agg({'item_cnt_month':['mean']})\ngrouped12.columns = ['item_city_avg_item_cnt_month']\ngrouped12.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped12, on=['date_block_num','item_id','city_code'], how ='left')\ntrain_match['item_city_avg_item_cnt_month'] = train_match['item_city_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'item_city_avg_item_cnt_month')\ntrain_match.drop(['item_city_avg_item_cnt_month'], axis =1, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_match.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_match.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Price Trend feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped13 = train.groupby(['item_id']).agg({'item_price':['mean']})\ngrouped13.columns = ['item_avg_item_price']\ngrouped13.reset_index(inplace = True)\ntrain_match = pd.merge(train_match, grouped13, on=['item_id'], how='left')\ntrain_match['item_avg_item_price'] = train_match['item_avg_item_price'].astype('float16')\n\ngrouped14 = train.groupby(['date_block_num','item_id']).agg({'item_price':['mean']})\ngrouped14.columns = ['item_avg_item_price_month']\ngrouped14.reset_index(inplace = True)\ntrain_match = pd.merge(train_match, grouped14, on=['date_block_num','item_id'], how='left')\ntrain_match['item_avg_item_price_month'] = train_match['item_avg_item_price_month'].astype('float16')\n\nall_lags = [1,2,3,4,5,6]\ntrain_match = create_lag_feature(train_match,all_lags,'item_avg_item_price_month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for l in all_lags:\n    train_match['delta_price_lag_'+str(l)]= (train_match['item_avg_item_price_month_lag_'+str(l)] - train_match['item_avg_item_price'])/train_match['item_avg_item_price']\n\ndef valid_trend(row):\n    for l in all_lags:\n        if row['delta_price_lag_'+str(l)]:\n            return row['delta_price_lag_'+str(l)]\n    return 0\n\ntrain_match['delta_price_lag'] = train_match.apply(valid_trend, axis=1)\ntrain_match['delta_price_lag'] = train_match['delta_price_lag'].astype('float16')\ntrain_match['delta_price_lag'].fillna(0, inplace =True)\n\nto_drop = ['item_avg_item_price','item_avg_item_price_month']\nfor l in all_lags:\n    to_drop += ['item_avg_item_price_month_lag_'+str(l)]\n    to_drop += ['delta_price_lag_'+str(l)]\n    \ntrain_match.drop(to_drop, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add price fluctuation rate within 6 months. If no fluctuation occurs, fill with zero. If fluctuation occurs, fill with the most recent price fluctuation."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_match.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time features creation"},{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-output":true},"cell_type":"code","source":"train_match['month'] = ((train_match['date_block_num']%12)+1).astype('int8')\ndof = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30,\n       7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\ntrain_match['day'] = train_match['month'].map(dof).astype('int8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"no leap years between 2013-2015"},{"metadata":{"trusted":true},"cell_type":"code","source":"sale_record = {}\ntrain_match['last_item_sale'] = -1\ntrain_match['last_item_sale'] = train_match['last_item_sale'].astype('int8')\nfor idx,row in train_match.iterrows():\n    key = row.item_id\n    if key not in sale_record.keys():\n        if row.item_cnt_month != 0:\n            sale_record[key] = row.date_block_num\n            \n    else:\n        last_date_block_num = sale_record[key]\n        if row.date_block_num > last_date_block_num:\n            train_match.at[idx,'last_item_sale'] = row.date_block_num - last_date_block_num\n            sale_record[key] = row.date_block_num            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sale_record = {}\ntrain_match['last_shop_item_sale'] = -1\ntrain_match['last_shop_item_sale'] = train_match['last_shop_item_sale'].astype('int8')\nfor idx,row in train_match.iterrows():\n    key = (row.item_id,row.shop_id)\n    if key not in sale_record.keys():\n        if row.item_cnt_month != 0:\n            sale_record[key] = row.date_block_num\n            \n    else:\n        last_date_block_num = sale_record[key]\n        if row.date_block_num > last_date_block_num:\n            train_match.at[idx,'last_shop_item_sale'] = row.date_block_num - last_date_block_num\n            sale_record[key] = row.date_block_num ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_match['first_item_shop_sale'] = train_match['date_block_num'] - train_match.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\ntrain_match['first_item_sale'] = train_match['date_block_num'] -train_match.groupby('item_id')['date_block_num'].transform('min')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"transform function works like agg('sum') and merge on (item_id,shop_id)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_match = train_match[train_match['date_block_num']>11]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"def fillna_lag(df):\n    for col in df.columns:\n        if '_lag_' in col and df[col].isnull().any():\n            if 'item_cnt' in col:\n                df[col].fillna(0, inplace = True)\n                \n    return df\n\ntrain_match = fillna_lag(train_match)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_match.to_pickle('training.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_match\ndel sale_record  \ndel item\ndel shop\ndel ic\ndel train\ndel grouped\ndel grouped2\ndel grouped3\ndel grouped4\ndel grouped5\ndel grouped6\ndel grouped7\ndel grouped8\ndel grouped9\ndel grouped10\ndel grouped11\ndel grouped12\ndel grouped13\ndel grouped14\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training = pd.read_pickle('../input/training-data-for-predicting-fututre-sales/training.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split train, test, valid data"},{"metadata":{"trusted":true},"cell_type":"code","source":"training.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = training[training.date_block_num<33].drop(['item_cnt_month'],axis=1)\ntrain_y = training[training.date_block_num<33]['item_cnt_month']\nvalid_X = training[training.date_block_num==33].drop(['item_cnt_month'],axis=1)\nvalid_y = training[training.date_block_num==33]['item_cnt_month']\ntest_X = training[training.date_block_num==34].drop(['item_cnt_month'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fine Tuning and Model Fitting"},{"metadata":{},"cell_type":"markdown","source":"## Xgboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth_range = {'max_depth':range(6,10,1)}\ncv1 = GridSearchCV(estimator = XGBRegressor(n_estimators = 1000,  \n                                            eta = 0.3,\n                                            subsample=0.8,\n                                            colsample_bytree=0.8,\n                                            seed = 0),\n                  param_grid = max_depth_range,\n                  scoring = 'neg_root_mean_squared_error')\n\ncv1.fit(train_x,train_y)\ncv1.cv_results_, cv1.best_score_, cv1.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GridSearchCV from Scikit-learn is a good tool to do cv and find the good parameters. However, it is too slow because of huge training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params = {'n_estimators':1500, \n             'max_depth':8, \n             'eta':0.1,\n             'subsample':0.8,\n             'colsample_bytree':0.6,\n             'min_child_weight':3,\n             'seed':98}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = XGBRegressor(**xgb_params)\nxgb_model.fit(train_X,train_y,\n             eval_metric = 'rmse',\n             eval_set = [(train_X, train_y), (valid_X, valid_y)],\n             early_stopping_rounds = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = xgb_model.predict(test_X).clip(0,20)\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv').set_index('ID')\nsubmission = pd.DataFrame({'ID':test.index, 'item_cnt_month':result})\nsubmission.to_csv('xgb_submission.csv',index =False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = ['shop_id', 'item_id', \n                'item_category_id', 'type_code', 'subtype_code', \n                'city_code','month']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"dtrain = lgb.Dataset(train_X, label=train_y, categorical_feature=cat_features, free_raw_data=False)\ndval = lgb.Dataset(valid_X, label=valid_y, categorical_feature=cat_features, free_raw_data=False)\nparams = {\"objective\": \"regression\",\"metric\": \"rmse\",\"verbosity\": -1,\"boosting_type\": \"gbdt\"}\nmodel = lgb.train(params, dtrain, valid_sets=[dval], verbose_eval=100, early_stopping_rounds=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use integrated fine tuning tools for LightGBM model in optuna"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_lgbm_params = model.params\nbest_score = model.best_score\nbest_lgbm_params,best_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_lgbm_params = {'objective': 'regression',\n                  'metric': 'rmse',\n                  'verbosity': 1,\n                  'boosting_type': 'gbdt',\n                  'lambda_l1': 4.7870372679959425,\n                  'lambda_l2': 2.1719574172416273e-06,\n                  'num_leaves': 235,\n                  'feature_fraction': 0.5479999999999999,\n                  'bagging_fraction': 1.0,\n                  'bagging_freq': 0,\n                  'min_child_samples': 20}\n\n#X = training[training.date_block_num<34].drop(['item_cnt_month'],axis=1)\n#y = training[training.date_block_num<34]['item_cnt_month']\n\nlgbm_model = lgbm.LGBMRegressor(**best_lgbm_params)\nlgbm_model.fit(train_X,train_y,\n             eval_metric = 'rmse',\n             eval_set = [(train_X, train_y), (valid_X, valid_y)],\n             early_stopping_rounds = 10, categorical_feature=cat_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = lgbm_model.predict(test_X,categorical_feature=cat_features).clip(0,20)\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv').set_index('ID')\nsubmission = pd.DataFrame({'ID':test.index, 'item_cnt_month':result})\nsubmission.to_csv('lgbm_submission.csv',index =False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit result plot and Explaination"},{"metadata":{},"cell_type":"markdown","source":"LightGBM has better performance so I use the fit result of lightgbm to plot the feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(10,8))\nlgbm.plot_importance(lgbm_model, height=0.4, max_num_features=20,\n                importance_type='gain', ax=ax)\nplt.yticks(fontsize=13)\nax.set_title('Feature Importance',fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature importance plot only shows how important the feature is during spliting. However, shap values plot can show negative or positive effects of the features on the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(lgbm_model)\nshap_values = explainer.shap_values(valid_X)\nshap.summary_plot(shap_values, valid_X, max_display=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.dependence_plot('item_cnt_month_lag_1', shap_values, \n                     valid_X, interaction_index=None, show=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.dependence_plot('shop_id', shap_values, \n                     valid_X, interaction_index=None, show=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.dependence_plot('first_item_sale', shap_values, \n                     valid_X, interaction_index=None, show=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. For most lag features, the larger the feature value, the more positive impacts on model predicting.\n2. For categorical features, both large and small feature value can have both positive and negative features impacts on model predicting.\n3. For the features which represent months diff towards first item sales records, the smaller the feature value, the more positive impacts on model predicting."},{"metadata":{},"cell_type":"markdown","source":"In conclusion, most recent sales record of an item will strongly improve the model predicting."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}