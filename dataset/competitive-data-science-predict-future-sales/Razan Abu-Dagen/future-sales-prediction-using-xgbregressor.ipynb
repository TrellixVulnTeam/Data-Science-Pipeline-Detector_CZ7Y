{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\nitem_categories = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\n\ntrain = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\ntest = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\n\nsample_submission = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading the Data\n\nprint('item_categories')\ndisplay(item_categories.head())\n\nprint('items')\ndisplay(items.head())\n\nprint('shops')\ndisplay(shops.head())\n\nprint('train')\ndisplay(train.head())\n\nprint('test')\ndisplay(test.head())\n\nprint('sample_submission')\ndisplay(sample_submission.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check Missing Values\n\nprint('train')\ndisplay(train.isnull().sum())\n\nprint('test')\ndisplay(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train')\ndisplay(train.describe(include='all'))\n\nprint('test')\ndisplay(test.describe(include='all'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop duplicates\n\nsubset = ['date','date_block_num','shop_id','item_id','item_cnt_day']\nprint(train.duplicated(subset=subset).value_counts())\ntrain.drop_duplicates(subset=subset, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check negative values in item_price\n\ntrain[train['item_price'] < 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop negative value in item_price\n\ntrain = train[train['item_price'] > 0]\n\n\ntrain = train[train['item_cnt_day'] > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(train['item_price']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(train['item_cnt_day']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_outliers(df, feature, percentile_high = .99):\n\n    #train size before dropping values\n    shape_init = df.shape[0]\n\n    max_value = df[feature].quantile(percentile_high)\n\n    print('dropping outliers...')\n    df = df[df[feature] < max_value]\n    \n    print(str(shape_init - df.shape[0]) + ' ' + feature + \n          ' values over ' + str(max_value) + ' have been removed' )\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop outliers in item_price feature\n\ntrain = drop_outliers(train, 'item_price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop outliers in item_cnt_day\n\ntrain = drop_outliers(train, 'item_cnt_day')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prices_shop_df = train[['shop_id','item_id','item_price']]\nprices_shop_df = prices_shop_df.groupby(['shop_id','item_id']).apply(lambda df: df['item_price'][-2:].mean())\nprices_shop_df = prices_shop_df.to_frame(name = 'item_price')\n\nprices_shop_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.merge(test, prices_shop_df, how='left', left_on=['shop_id','item_id'], right_on=['shop_id','item_id'])\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for missing values\n\ntest['item_price'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split content in date into month and year\ntrain['month'] = [date.split('.')[1] for date in train['date']]\ntrain['year'] = [date.split('.')[2] for date in train['date']]\n\n#drop date and date_block_num features\ntrain.drop(['date','date_block_num'], axis=1, inplace=True)\n\n#create month and year features fot test dataset\ntest['month'] = '11'\ntest['year'] = '2015'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#change item_cnt_day into item_cnt_month\ntrain_monthly = train.groupby(['year','month','shop_id','item_id'], as_index=False)[['item_cnt_day']].sum()\ntrain_monthly.rename(columns={'item_cnt_day': 'item_cnt_month'}, inplace=True)\n\ntrain_monthly = pd.merge(train_monthly, prices_shop_df, how='left', left_on=['shop_id','item_id'], right_on=['shop_id','item_id'])\n\ntrain_monthly.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_monthly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#REINDEX TEST DATASET\n\ntest = test.reindex(columns=['ID','year','month','shop_id','item_id','item_price'])\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EXPLORING ITEMS CATEGORY DATASET\n\n#extract main categories\nitem_categories['main_category'] = [x.split(' - ')[0] for x in item_categories['item_category_name']]\n\nsub_categories = []\nfor i in range(len(item_categories)):\n    try:\n        sub_categories.append(item_categories['item_category_name'][i].split(' - ')[1])\n        \n    except IndexError as e:\n        sub_categories.append('None')\n\nitem_categories['sub_category'] = sub_categories\n\nitem_categories.drop(['item_category_name'], axis=1, inplace=True)\n\nitem_categories.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EXPLORING ITEMS DATASET\n\n#merge with item_categories\nitems = pd.merge(items, item_categories, how='left')\n\nitems.drop(['item_name','item_category_id'], axis=1, inplace=True)\n\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge to train and test datasets\n\ntrain = pd.merge(train, items, how='left')\ntest = pd.merge(test, items, how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EXPLORING SHOPS DATASET\n\n\nfrom string import punctuation\n\n# replace all the punctuation in the shop_name columns\nshops[\"shop_name_cleaned\"] = shops[\"shop_name\"].apply(lambda s: \"\".join([x for x in s if x not in punctuation]))\n\nshops[\"shop_city\"] = shops[\"shop_name_cleaned\"].apply(lambda s: s.split()[0])\n\nshops[\"shop_type\"] = shops[\"shop_name_cleaned\"].apply(lambda s: s.split()[1])\n\nshops[\"shop_name\"] = shops[\"shop_name_cleaned\"].apply(lambda s: \" \".join(s.split()[2:]))\n\nshops.drop(['shop_name_cleaned'], axis=1, inplace=True)\n\nshops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge to train and test datasets\n\ntrain = pd.merge(train, shops, how='left')\ntest = pd.merge(test, shops, how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train')\ndisplay(train.head())\n\nprint('test')\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FILL MISSING VALUES IN ITEM_PRICE (by item categories)\n\n#fill missing values with median of each main_category and sub_category\ntest['item_price'] = test.groupby(['main_category','sub_category'])['item_price'].apply(lambda df: df.fillna(df.median()))\n\ntest['item_price'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill missing values with median of each sub_category\ntest['item_price'] = test.groupby(['sub_category'])['item_price'].apply(lambda df: df.fillna(df.median()))\n\ntest['item_price'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test['item_price'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill missing values with median of main_category and sub_category from train dataset\nfiller = train[(train['main_category'] == 'PC') & (train['sub_category'] == 'Гарнитуры/Наушники')]['item_price'].median()\n\ntest['item_price'].fillna(filler, inplace=True)\ntest['item_price'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['item_cnt_month'] = train['item_cnt_month'].clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_array = train['item_cnt_month']\ntrain.drop(['item_cnt_month'], axis=1, inplace=True)\n\ntest_id = test['ID']\ntest.drop(['ID'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['shop_id','item_id'], axis=1, inplace=True)\ntest.drop(['shop_id','item_id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reduce memory\ndowncast_dtypes(train)\ndowncast_dtypes(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for any missing data\nprint('missing data in the train dataset : ', train.isnull().any().sum())\nprint('missing data in the test dataset : ', test.isnull().any().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalityTest(data, alpha=0.05):\n\n    from scipy import stats\n    \n    statistic, p_value = stats.normaltest(data)\n\n    if p_value < alpha:  \n        is_normal_dist = False\n    else:\n        is_normal_dist = True\n    \n    return is_normal_dist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check normality of all numericaal features and transform it if not normal distributed\n\nfor feature in train.columns:\n    if (train[feature].dtype != 'object'):\n        if normalityTest(train[feature]) == False:\n            train[feature] = np.log1p(train[feature])\n            test[feature] = np.log1p(test[feature])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_array = np.log1p(target_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ENCODING\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nenc = OrdinalEncoder()\n\nX = enc.fit_transform(train)\ny = target_array\n\nX_predict = enc.fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SPLITTING THE DATA IN ORDER TO CREATE THE MODEL\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .1, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n\n#create a model\nmodel = XGBRegressor()\n\n#fitting\nmodel.fit(X_train, y_train, eval_metric=\"rmse\", eval_set=[(X_train, y_train), (X_test, y_test)], \n          verbose=True, early_stopping_rounds = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate Mean Squared Error\nfrom sklearn.metrics import mean_squared_error\n\nprint('MSE : ', mean_squared_error(y_test, model.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = model.predict(X_predict)\ny_predict = np.expm1(y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame({'ID': test_id, 'item_cnt_month': y_predict})\nresults.to_csv('my_submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}