{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nimport gc\nimport re\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Load Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"items_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nitem_categories_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\nsales_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\nshops_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\ntest_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **EDA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprcoess item_categories_df**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the type of the items and label them\nitem_categories_df['type'] = item_categories_df['item_category_name'].str.split('-').map(lambda x:x[0])\nitem_categories_df['type_code'] = LabelEncoder().fit_transform(item_categories_df['type'])\n\n# get the sub type of the items and label them\nitem_categories_df['sub_type'] = item_categories_df['item_category_name'].str.split('-').map(lambda x:x[1].strip() \n                                                                                             if len(x) > 1 else x[0].strip())\nitem_categories_df['sub_type_code'] = LabelEncoder().fit_transform(item_categories_df['sub_type'])\nitem_categories_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprcoess shops_df**"},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_df.loc[shops_df['shop_name']=='Сергиев Посад ТЦ \"7Я\"','shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops_df['city'] = shops_df['shop_name'].str.split(' ').map(lambda x:x[0])\nshops_df.loc[shops_df['city'] == '!Якутск','city']='Якутск'\nshops_df['city_code'] = LabelEncoder().fit_transform(shops_df['city'])\nshops_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprcoess sales_df**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace the negative item_price to mean value\nmean = sales_df[(sales_df[\"shop_id\"] == 32) & (sales_df[\"item_id\"] == 2973) & \n                (sales_df[\"date_block_num\"] == 4) & (sales_df[\"item_price\"] > 0)][\"item_price\"].mean()\nsales_df.loc[sales_df.item_price < 0, 'item_price'] = mean\n\n# clean the outliers\nsales_df = sales_df[sales_df[\"item_price\"] < np.percentile(sales_df[\"item_price\"], q = 100)]\nsales_df = sales_df[(sales_df[\"item_cnt_day\"] >= 0) & (sales_df[\"item_cnt_day\"] < np.percentile(sales_df[\"item_cnt_day\"], q = 100))]\n\n# change the format of date\nsales_df[\"date\"] = pd.to_datetime(sales_df[\"date\"], format = \"%d.%m.%Y\")\nsales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [(0, 57), (1, 58), (10, 11)]:\n    sales_df.loc[sales_df['shop_id'] == i[0], 'shop_id'] = i[1]\n    test_df.loc[test_df['shop_id'] == i[0], 'shop_id'] = i[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df['revenue'] = sales_df['item_cnt_day'] * sales_df['item_price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprcoess items_df**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def name_correction(x):\n    x = x.lower() #lower case\n    x = x.partition('[')[0] # partition by square brackets\n    x = x.partition('(')[0] # partition by curly brackets\n    x = re.sub('[^A-Za-z0-9А-Яа-я]+', ' ', x) # remove special characters\n    x = x.replace('  ', ' ') # replace double spaces with single spaces\n    x = x.strip() # remove leading and trailing white space\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split item names by first bracket\nitems_df['name1'], items_df['name2'] = items_df['item_name'].str.split('[', 1).str\nitems_df['name1'], items_df['name3'] = items_df['item_name'].str.split('(', 1).str\n\n# replace special characters and turn to lower case\nitems_df['name2'] = items_df['name2'].str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\nitems_df['name3'] = items_df['name3'].str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\n\n# fill nulls with '0'\nitems_df = items_df.fillna('0')\n\nitems_df['item_name'] = items_df['item_name'].apply(lambda x: name_correction(x))\n\n# return all characters except the last if name 2 is not \"0\" - the closing bracket\nitems_df['name2'] = items_df['name2'].apply(lambda x: x[:-1] if x != '0' else '0')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_df['type'] = (items_df['name2'].apply(lambda x: x[0:8] if x.split(' ')[0] == 'xbox' else x.split(' ')[0]))\n\nitems_df.loc[(items_df['type'] == 'x360')|(items_df['type'] == 'xbox360')|(items_df['type'] == 'xbox 360'),'type'] = 'xbox 360'\nitems_df.loc[items_df['type'] == '', 'type'] = 'mac'\nitems_df.type = (items_df['type'].apply(lambda x: x.replace(' ', '')))\nitems_df.loc[(items_df['type'] == 'pc' )|(items_df['type'] == 'pс')|(items_df['type'] == 'pс'),'type'] = 'pс'\n\nitems_df.loc[items_df['type'] == 'рs3' , 'type'] = 'рs3'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group_sum = (items_df.groupby('type').agg({'item_id': 'count'}).reset_index())\n\ndrop_cols = []\nfor categ in group_sum['type'].unique():\n    if group_sum.loc[(group_sum['type'] == categ), 'item_id'].values[0] <= 39:\n        drop_cols.append(categ)\n\nitems_df['name2'] = (items_df['name2'].apply(lambda x: 'other' if x in drop_cols else x))\nitems_df = items_df.drop(['type'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_df['name2'] = LabelEncoder().fit_transform(items_df['name2'])\nitems_df['name3'] = LabelEncoder().fit_transform(items_df['name3'])\n\nitems_df.drop(['item_name', 'name1'], axis=1, inplace=True)\nitems_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Preprocessing Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor month in sales_df['date_block_num'].unique():\n    cur_shops = sales_df.loc[sales_df['date_block_num'] == month, 'shop_id'].unique()\n    cur_items = sales_df.loc[sales_df['date_block_num'] == month, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [month]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates\ngb = sales_df.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'})\ngb.rename(columns ={'item_cnt_day':'item_cnt_month'},inplace = True)\ntrain_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n#Clip target values\ntrain_data['item_cnt_month'] = np.clip(train_data['item_cnt_month'],0,20)\ntrain_data.sort_values(index_cols, inplace = True)\n\n# Downcast dtypes from 64 to 32 bit to save memory\ntrain_data = downcast_dtypes(train_data)\ndel grid, gb \ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine test set by date_block_num\ntest_df.insert(loc=3, column='date_block_num', value=34)\ntrain_data = train_data.append(test_df.drop('ID', axis = 1)).fillna(0)\ntrain_data.head().append(train_data.tail())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Combine tables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# add shops_df\ntrain_data = pd.merge(train_data,shops_df.drop(['city','shop_name'], axis = 1), on=['shop_id'], how='left') \n\n# add items_df\ntrain_data = pd.merge(train_data, items_df, on=['item_id'], how='left') \n\n# add item_categories_df\ntrain_data = pd.merge(train_data, item_categories_df.drop(['item_category_name','type','sub_type'], axis = 1), on=['item_category_id'], how='left') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add month, days, and holidays\ntrain_data['month'] = train_data['date_block_num'] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\ntrain_data['days'] = train_data['month'].map(days)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Engineering**"},{"metadata":{},"cell_type":"markdown","source":"**Lag Feature**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_lag(df, lags, lag_col):\n    for i in lags:\n        shifted = df[['date_block_num','shop_id','item_id',lag_col]].copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', lag_col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = generate_lag(train_data, [1,2,3,6,12], 'item_cnt_month')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Part one: lag feature with date_block_num**"},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_data.groupby(['date_block_num','item_id'])['item_cnt_month'].mean().rename('date_item_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num', 'item_id'], how='left')\ntrain_data = generate_lag(train_data, [1,2,3,6,12], 'date_item_avg_cnt')\ntrain_data.drop(['date_item_avg_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_data.groupby(['date_block_num','shop_id'])['item_cnt_month'].mean().rename('date_shop_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num', 'shop_id'], how='left')\ntrain_data = generate_lag(train_data, [1,2,3,6,12], 'date_shop_avg_cnt')\ntrain_data.drop(['date_shop_avg_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_data.groupby(['date_block_num','item_category_id'])['item_cnt_month'].mean().rename('date_itemcat_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num','item_category_id'], how='left')\ntrain_data = generate_lag(train_data, [1,2], 'date_itemcat_avg_cnt')\ntrain_data.drop(['date_itemcat_avg_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_data.groupby(['date_block_num'])['item_cnt_month'].mean().rename('date_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num'], how='left')\ntrain_data = generate_lag(train_data, [1], 'date_avg_cnt')\ntrain_data.drop(['date_avg_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_data.groupby(['date_block_num','city_code'])['item_cnt_month'].mean().rename('date_city_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num', 'city_code'], how='left')\ntrain_data = generate_lag(train_data, [1], 'date_city_avg_cnt')\ntrain_data.drop(['date_city_avg_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_data.groupby(['date_block_num','city_code','item_id'])['item_cnt_month'].mean().rename('date_city_item_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num', 'city_code','item_id'], how='left')\ntrain_data = generate_lag(train_data, [1], 'date_city_item_avg_cnt')\ntrain_data.drop(['date_city_item_avg_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_data.groupby(['date_block_num','shop_id','item_id'])['item_cnt_month'].mean().rename('date_shop_item_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num', 'shop_id','item_id'], how='left')\ntrain_data = generate_lag(train_data, [1,2,3], 'date_shop_item_avg_cnt')\ntrain_data.drop(['date_shop_item_avg_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_data.groupby(['date_block_num','shop_id','sub_type_code'])['item_cnt_month'].mean().rename('date_shop_subtype_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num', 'shop_id','sub_type_code'], how='left')\ntrain_data = generate_lag(train_data, [1], 'date_shop_subtype_avg_cnt')\ntrain_data.drop(['date_shop_subtype_avg_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cleaning works\ntrain_data.fillna(0,inplace = True)\ntrain_data = downcast_dtypes(train_data)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Part two: lag feature with month**"},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_data.groupby(['month','item_id'])['item_cnt_month'].mean().rename('month_item_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['month', 'item_id'], how='left')\ntrain_data = generate_lag(train_data, [1,2,3], 'month_item_avg_cnt')\ntrain_data.drop(['month_item_avg_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_data.groupby(['month','shop_id'])['item_cnt_month'].mean().rename('month_shop_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['month', 'shop_id'], how='left')\ntrain_data = generate_lag(train_data, [1,2,3], 'month_shop_avg_cnt')\ntrain_data.drop(['month_shop_avg_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_data.groupby(['month'])['item_cnt_month'].mean().rename('month_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['month'], how='left')\ntrain_data = generate_lag(train_data, [1], 'month_avg_cnt')\ntrain_data.drop(['month_avg_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cleaning works\ntrain_data.fillna(0,inplace = True)\ntrain_data = downcast_dtypes(train_data)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Trend Feature**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# price\n# get the mean by item_id and item_price\ngroup = sales_df.groupby(['item_id'])['item_price'].mean().rename('item_avg_price').reset_index()\ntrain_data = pd.merge(train_data, group, on=['item_id'], how='left')\n\n# get the mean by date_block_num, item_id and item_price\ngroup = sales_df.groupby(['date_block_num','item_id'])['item_price'].mean().rename('date_item_avg_price').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num','item_id'], how='left')\n\n# calculate the trend of price and add lag\nlags = [1,2,3,4,5,6]\ntrain_data = generate_lag(train_data, lags, 'date_item_avg_price')\n\nfor i in lags:\n    train_data['trend_price_lag_'+str(i)] = \\\n        (train_data['date_item_avg_price_lag_'+str(i)] - train_data['item_avg_price']) / train_data['item_avg_price']\n    \ndef select_trend(row):\n    for i in lags:\n        if row['trend_price_lag_'+str(i)]:\n            return row['trend_price_lag_'+str(i)]\n    return 0\n    \ntrain_data['trend_price_lag'] = train_data.apply(select_trend, axis=1)\n\n# drop all the columns\nfetures_to_drop = ['item_avg_price', 'date_item_avg_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_price_lag_'+str(i)]\n    fetures_to_drop += ['trend_price_lag_'+str(i)]\n\ntrain_data.drop(fetures_to_drop, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the sum by date_block_num, shop_id and revenue\ngroup = sales_df.groupby(['date_block_num','shop_id',])['revenue'].sum().rename('sum_date_shops_revenue').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num','shop_id'], how='left')\n\n# get the mean by with shop_id and revenue\ngroup = group.groupby(['shop_id',])['sum_date_shops_revenue'].mean().rename('mean_shops_revenue').reset_index()\ntrain_data = pd.merge(train_data, group, on=['shop_id'], how='left')\n\n# calculate the trend of revenue and add lag\ntrain_data['trend_revenue'] = (train_data['sum_date_shops_revenue'] - train_data['mean_shops_revenue']) / train_data['mean_shops_revenue']\ntrain_data = generate_lag(train_data, [1], 'trend_revenue')\n\n# drop all the columns \ntrain_data.drop(['sum_date_shops_revenue'], axis=1, inplace=True)\ntrain_data.drop(['mean_shops_revenue'], axis=1, inplace=True)\ntrain_data.drop(['trend_revenue'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add the month of each shop and item first sale\ntrain_data['item_shop_first_sale'] = (\n    train_data['date_block_num'] - train_data.groupby(['item_id', 'shop_id'])['date_block_num'].transform('min')\n)\ntrain_data['item_first_sale'] = (\n    train_data['date_block_num'] - train_data.groupby(['item_id'])['date_block_num'].transform('min')\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cleaning works\ntrain_data.fillna(0,inplace = True)\ntrain_data = downcast_dtypes(train_data)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data[train_data['date_block_num'] > 3]\ntrain_data.head().append(train_data.tail())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **XGboost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_data[train_data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = train_data[train_data.date_block_num < 33]['item_cnt_month']\nX_valid = train_data[train_data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = train_data[train_data.date_block_num == 33]['item_cnt_month']\nX_test = train_data[train_data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del shops_df\ndel items_df\ndel item_categories_df\ndel sales_df\ndel train_data\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor, plot_importance\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.1,    \n    seed=42)\n\nmodel.fit(\n    X_train,\n    Y_train,\n    eval_metric=\"rmse\",\n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n    verbose=True,\n    early_stopping_rounds = 10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test).clip(0,20)\nsubmission['item_cnt_month'] = y_pred \nsubmission.to_csv('future_sales_pred.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10, 14))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}