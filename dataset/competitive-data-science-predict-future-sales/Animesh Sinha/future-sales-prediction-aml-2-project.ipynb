{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Additional Imports\nimport tensorflow as tf\nimport sklearn as sk\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading and Analyzing the Input Data"},{"metadata":{},"cell_type":"markdown","source":"## Loading the data into Pandas dataframes"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"INPUT_DIR = '../input'\ninput_item = pd.read_csv(os.path.join(INPUT_DIR, 'items.csv'))\ninput_test = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\ninput_train = pd.read_csv(os.path.join(INPUT_DIR, 'sales_train.csv'))\ninput_item_categories = pd.read_csv(os.path.join(INPUT_DIR, 'item_categories.csv'))\ninput_shops = pd.read_csv(os.path.join(INPUT_DIR, 'shops.csv'))\noutput_sample = pd.read_csv(os.path.join(INPUT_DIR, 'sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is what the problem statement says about the provided files. Let's now start the exploration.\n* sales_train.csv - the training set. Daily historical data from January 2013 to October 2015.\n* test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.\n* sample_submission.csv - a sample submission file in the correct format.\n* items.csv - supplemental information about the items/products.\n* item_categories.csv - supplemental information about the items categories.\n* shops.csv - supplemental information about the shops.\n\nThe Following is a list of the Data Fields and what they represent\n* **ID** - an Id that represents a (Shop, Item) tuple within the test set\n* **shop_id** - unique identifier of a shop\n* **item_id** - unique identifier of a product\n* **item_category_id** - unique identifier of item category\n* **item_cnt_day** - number of products sold. You are predicting a monthly amount of this measure\n* **item_price** - current price of an item\n* **date** - date in format dd/mm/yyyy\n* **date_block_num** - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n* **item_name** - name of item\n* **shop_name** - name of shop\n* **item_category_name** - name of item category"},{"metadata":{},"cell_type":"markdown","source":"## Looking through the files"},{"metadata":{},"cell_type":"markdown","source":"### Items and Categories"},{"metadata":{},"cell_type":"markdown","source":"This first file gives us a list of all the items, their names, ids, and corresponding **Category IDs**. There are a total of 84 item categories, some having way more items than the others. Some have about 5000 items, some have as few as 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_item.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Following are the unique Item Category IDs:\\n\", input_item['item_category_id'].unique(), \"(\" + str(len(input_item['item_category_id'].unique())) + \" categories)\")\n\nsns.countplot('item_category_id', data=input_item).set_title('Number of items in Each Category')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Test Set"},{"metadata":{},"cell_type":"markdown","source":"Following is the test data input, it's a list of all the items and the shops to which it belongs. All the **shops are equally sampled** for their items. Each *shop-item pair in the test set is unique*."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('shop_id', data=input_test).set_title('Number of items in Each Shop')\nplt.show()\nprint('There are a total of', len(input_test.index), 'Rows.')\nprint('It contains', len(input_test['item_id'].unique()), 'unique items.')\nprint('It contains', len(input_test['shop_id'].unique()), 'unique shops.')\nprint('It contains', len(input_test.groupby(['shop_id', 'item_id'])), 'unique shops-item pairs.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training Data"},{"metadata":{},"cell_type":"markdown","source":"Here is the training set for the sales data. It has, for each unique shop-item pair. This data **unlike the test set, is not uniformly sampled from all shops**. Also, this has **way more examples of low-price products than high-price products**."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are a total of', len(input_train.index), 'Rows.')\nprint('It contains', len(input_train['item_id'].unique()), 'unique items.')\nprint('It contains', len(input_train['shop_id'].unique()), 'unique shops.')\nprint('It contains', len(input_train['date_block_num'].unique()), 'date blocks.')\nprint('It contains', len(input_train.groupby(['shop_id', 'item_id'])), 'unique shops-item pairs.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('shop_id', data=input_train).set_title('Number of items in Each Shop')\nplt.show()\n_, plot_price_axes = plt.subplots(1,2, figsize=(15, 5))\nsns.distplot(input_train['item_price'], bins=100, ax=plot_price_axes[0]).set_title('Price of items (all prices)')\nsns.distplot(input_train['item_price'], bins=1000, ax=plot_price_axes[1]).set_title('Price of items (lower price)')\nplt.xlim(0, 10000)\nplt.show()\nsns.countplot('date_block_num', data=input_train).set_title('Number of items in Each Date-Block')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(100):\n    query = input_train.loc[input_train['item_id'] == i]\n    if (len(query) < 50): continue\n    x = pd.to_datetime(query['date'])\n    y = query['item_price']\n    sns.lineplot(x, y).set_title('Price with time')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(input_train['item_cnt_day'].unique())\ninput_train['item_cnt_day'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Item Category and Shops Names"},{"metadata":{},"cell_type":"markdown","source":"This is just a list of names, maybe we can further improve our results with some complex text processing to recognize classes of products, but this is mostly additional useless information. Nevertheless, good for fault finding when we are close to done."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_item_categories.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total Categories\", len(input_item_categories))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_shops.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of course, we have a single target a real number prediction for the expected number of sales of the product."},{"metadata":{"trusted":true},"cell_type":"code","source":"output_sample.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Insights"},{"metadata":{},"cell_type":"markdown","source":"## Understanding Each Category"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_categories = input_item['item_category_id'].values\ninput_train['item_category_id'] = input_train.apply(lambda x: temp_categories[x['item_id']], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"process_categories = pd.DataFrame(columns=('item_category_id', 'price_mean', 'items_count', 'price_min', 'price_quartile1', \n                                           'price_median', 'price_quartile3', 'price_max', 'std_dev'))\nfor i in range(len(input_item_categories)):\n    temp_query = input_train.loc[input_train['item_category_id'] == i]['item_price']\n    process_categories = process_categories.append(pd.Series([i, temp_query.mean(), temp_query.count(), temp_query.min(), temp_query.quantile(0.25), temp_query.median(), \n                                                              temp_query.quantile(0.75), temp_query.max(), temp_query.std()],\n                                                             index=process_categories.columns), ignore_index=True)\n\npd.set_option('display.max_rows', 100)\nprocess_categories.head(len(process_categories))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, plot_category_axes = plt.subplots(2,4, figsize=(20, 10))\nsns.barplot(x='item_category_id', y='price_mean', data=process_categories, ax=plot_category_axes[0][0]).set_title('Mean Price')\nsns.barplot(x='item_category_id', y='items_count', data=process_categories, ax=plot_category_axes[0][1]).set_title('Number of Samples')\nsns.barplot(x='item_category_id', y='price_min', data=process_categories, ax=plot_category_axes[0][2]).set_title('Min Price')\nsns.barplot(x='item_category_id', y='price_quartile1', data=process_categories, ax=plot_category_axes[0][3]).set_title('First Quartile Price')\nsns.barplot(x='item_category_id', y='price_median', data=process_categories, ax=plot_category_axes[1][0]).set_title('Median Price')\nsns.barplot(x='item_category_id', y='price_quartile3', data=process_categories, ax=plot_category_axes[1][1]).set_title('Third Quartile Price')\nsns.barplot(x='item_category_id', y='price_max', data=process_categories, ax=plot_category_axes[1][2]).set_title('Max Price')\nsns.barplot(x='item_category_id', y='std_dev', data=process_categories, ax=plot_category_axes[1][3]).set_title('Price Standard Deviation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline Classifier Model"},{"metadata":{},"cell_type":"markdown","source":"## Training and Validation Sets"},{"metadata":{},"cell_type":"markdown","source":"Here, we are splitting the data into the training set which is the first 32 months, and the validation set which is all the data from the last month."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_train = input_train[input_train['date_block_num'] < input_train['date_block_num'].max()] \ntemp_valid = input_train[input_train['date_block_num'] == input_train['date_block_num'].max()]\nx_train, y_train = temp_train[[col for col in temp_train.columns if 'item_cnt_day' not in col]], temp_train['item_cnt_day']\nx_valid, y_valid = temp_valid[[col for col in temp_valid.columns if 'item_cnt_day' not in col]], temp_valid['item_cnt_day']\n\nprint('The Shapes are:', x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)\nx_train = x_train.drop('date', axis=1)\nx_valid = x_valid.drop('date', axis=1)\nx_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z_train, p_train = x_train[[col for col in  x_train.columns if 'item_price' not in col]], x_train['item_price']\nz_valid, p_valid = x_valid[[col for col in  x_valid.columns if 'item_price' not in col]], x_valid['item_price']\nz_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier and Feature Importance"},{"metadata":{},"cell_type":"markdown","source":"We need to get some estimate of how important each of the features are to the result. So we train a bunch of Random Forest regressors and see the weights they output."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\ndef forest_classifier(x_val, y_val, importance=True):\n    # Build a forest and compute the feature importances\n    forest = RandomForestClassifier(n_estimators=3, random_state=0)\n\n    forest.fit(x_val, y_val)\n    importances = forest.feature_importances_\n    std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n    indices = np.argsort(importances)[::-1]\n\n    # Print the feature ranking\n    print(\"Feature ranking:\")\n    for f in range(x_val.shape[1]):\n        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n    # Plot the feature importances of the forest\n    plt.title(\"Feature importances\")\n    plt.bar(range(x_val.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n    plt.xticks(range(x_val.shape[1]), indices)\n    plt.xlim([-1, x_val.shape[1]])\n    plt.show()\n    \n    return forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\ndef forest_regressor(x_val, y_val, importance=True):\n    # Build a forest and compute the feature importances\n    forest = RandomForestRegressor(n_estimators=3, random_state=0)\n\n    forest.fit(x_val, y_val)\n    importances = forest.feature_importances_\n    std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n    indices = np.argsort(importances)[::-1]\n\n    # Print the feature ranking\n    print(\"Feature ranking:\")\n    for f in range(x_val.shape[1]):\n        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n    # Plot the feature importances of the forest\n    plt.title(\"Feature importances\")\n    plt.bar(range(x_val.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n    plt.xticks(range(x_val.shape[1]), indices)\n    plt.xlim([-1, x_val.shape[1]])\n    plt.show()\n    \n    return forest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Running the Regressors and Plotting Importance"},{"metadata":{},"cell_type":"markdown","source":"All the predictions are numberic. Forest-1 and Forest-2 are a combination, Forest-1 predicts the cost and Forest-2 predicts the count of sale. Forest-3 tries to make the prediction in one single step."},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_1 = forest_regressor(z_train, p_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_2 = forest_regressor(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_3 = forest_regressor(z_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_valid = z_valid.copy()\npredict_valid['predicted_price'] = forest_1.predict(z_valid)\npredict_valid['predicted_cnt_1'] = forest_2.predict(predict_valid)\npredict_valid['predicted_cnt_2'] = forest_3.predict(z_valid)\npredict_valid['actual_price'] = p_valid\npredict_valid['actual_cnt'] = y_valid\npredict_valid.head(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Final Output"},{"metadata":{},"cell_type":"markdown","source":"## In shape to make Predictions"},{"metadata":{},"cell_type":"markdown","source":"Just adding in the extra columns, date-block and item-category from the other files so that our Model can make it's predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(input_test.columns)\nprint(z_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i_test, x_test = input_test['ID'], input_test[[col for col in input_test.columns if 'ID' not in col]]\nx_test.insert(0, 'date_block_num', input_train['date_block_num'].max() + 1)\nx_test.insert(3, 'item_category_id', input_test.apply(lambda x: temp_categories[x.loc['item_id']], axis=1))\nx_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making the PREDICTIONS!!!"},{"metadata":{},"cell_type":"markdown","source":"For now, I am just outputting the **Random Forest Classifier** predictions. Will update to a better model in the future versions of the notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test['price'] = forest_1.predict(x_test)\nx_test['cnt'] = forest_2.predict(x_test)\nx_test.insert(0, 'ID', x_test.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get the Column and Print to File"},{"metadata":{},"cell_type":"markdown","source":"Output one final `submission.csv`. Job Complete!"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('submission.csv', 'w') as file:\n    file.write('ID,item_cnt_month' + '\\n')\n    for index, item in x_test.iterrows():\n        file.write(str(int(item['ID'])) + ',' + str(item['cnt']) + '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('submission.csv', 'r') as file:\n    for i in range(100):\n        print(next(file), end='')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}