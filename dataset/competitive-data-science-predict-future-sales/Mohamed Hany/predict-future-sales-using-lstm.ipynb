{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f63e065519f029cfc2c0e9580fb2e95a146ef0d5","id":"b_vMocjiU3TL","trusted":true},"cell_type":"code","source":"# Load the data \nos.listdir('../input')\nsales_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nitem_cat = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\nsample_submission = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')\ntest_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"cwV3adZWcbCu","outputId":"fb898009-aa2c-431a-cb61-3b93e08d2bc1","trusted":true},"cell_type":"code","source":"sales_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"RKxUSFfTcedO","outputId":"b2c3bde3-1a29-4260-ab25-06581330c80a","trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"DxuKV_dHcmr9","outputId":"576ccdad-6adc-484a-947e-d426c942d98f","trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f01c334c611aa85639105835ae6c12968e5f9ab","id":"JfNNsZmNU3TT","trusted":true},"cell_type":"code","source":"# Function that define all the EDA we need \ndef EDA(df):\n    print(\"HEAD OF THE DATA \")\n    print(df.head())\n    print(\"INFO\")\n    print(df.info())\n    print(\"Describe\")\n    print(df.describe())\n    print(\"Columns\")\n    print(df.columns)\n    print(\"Data Types\")\n    print(df.dtypes)\n    print(\"Missing Values\")\n    print(df.isnull().sum())\n    print(\"NULL values\")\n    print(df.isna().sum())\n    print(\"Shape Of Data\")\n    print(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40841bde3a83799519bf695f361a62fe03d9e4f1","id":"_Mn_AshwU3Ta","outputId":"d4dd2e43-32fe-446a-f760-fc1cc31458c6","trusted":true},"cell_type":"code","source":"#Litle bit of exploration of data\n\nprint(\"Sales Data __________________________________\")\nEDA(sales_data)\nprint(\"Test data_____________________________\")\nEDA(test_data)\nprint(\"Item Categories_____________________________\")\nEDA(item_cat)\nprint(\"Items______________________________________\")\nEDA(items)\nprint(\"Shops_______________________________\")\nEDA(shops)\nprint(\"Sample Submission___________________________________\")\nEDA(sample_submission)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa425faed80de06ea817c0bab179e47e33a1aa57","id":"6GBB1wNrU3Tl","trusted":true},"cell_type":"code","source":"#we can see that 'date' column in sales_data is an object but if we want to manipulate  it or want to work on it someway then we have convert it on datetime format\nsales_data['date'] = pd.to_datetime(sales_data['date'],format = '%d.%m.%Y')","execution_count":null,"outputs":[]},{"metadata":{"id":"XOfGqG9hZjTL","outputId":"dccd67f8-e70c-4e00-d24b-3caa39ce2b55","trusted":true},"cell_type":"code","source":"sales_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f919596e10dd4adafb8825f2c0d42b8c8ddd07a","id":"jyrPUzD_U3Ts","trusted":true},"cell_type":"code","source":"#now we will create a pivot tabel by going so we get our data in desired form \n#we want get total count value of an item over the whole month for a shop \n# That why we made shop_id and item_id our indices and date_block_num our column \n# the value we want is item_cnt_day and used sum as aggregating function \ndataset = sales_data.pivot_table(index = ['shop_id','item_id'],values = ['item_cnt_day'],columns = ['date_block_num'],fill_value = 0,aggfunc='sum')","execution_count":null,"outputs":[]},{"metadata":{"id":"bHH2m0GVZtrw","outputId":"10036bb1-7129-4b9c-9b7e-5b2b3a7155ea","trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ab7ea5328f337d2d90477215045bcdd5b3a289c","id":"6SSfr9nKU3T0","trusted":true},"cell_type":"code","source":"# lets reset our indices, so that data should be in way we can easily manipulate\ndataset.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efe0d2b799958be1ed8273ff21b641659671036b","id":"p4qk5D0lU3T_","outputId":"37a6f375-f167-4abc-df04-a05d78b57026","trusted":true},"cell_type":"code","source":"# lets check on our pivot table\ndataset.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d3f8cddfa21cd9b07ba1f6d6f243632050a3d07","id":"k_r2sTYDU3UG","outputId":"e3dfd764-f75f-4edf-a28c-c66a225a661b","trusted":true},"cell_type":"code","source":"# Now we will merge our pivot table with the test_data because we want to keep the data of items we have\n# predict\ndataset = pd.merge(test_data,dataset,on = ['item_id','shop_id'],how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"id":"XQ03jVGTaZXE","outputId":"0848b450-5166-4e47-8cd2-3fa1d506ebfe","trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e8e4c20cdeda9487557f5570461358d919bab72","id":"L-VTmReHU3UK","outputId":"5f6bb32d-9058-44cf-db97-6c8f7aeb3ddd","trusted":true},"cell_type":"code","source":"# lets fill all NaN values with 0\ndataset.fillna(0,inplace = True)\n# lets check our data now \ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e16316986055e62840caf762d2dc0cde3e49271","id":"suyy0Qe-U3UO","outputId":"bf4e4e03-b5ad-4a68-a4be-52c9611dc6dc","trusted":true},"cell_type":"code","source":"# we will drop shop_id and item_id because we do not need them\n# we are teaching our model how to generate the next sequence \ndataset.drop(['shop_id','item_id','ID'],inplace = True, axis = 1)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc7ca752bc9f265b71861a72499aec3dbcc28dfb","id":"skDsrXDOU3UU","outputId":"9b827468-4f0c-4fc6-d181-3f4b384af8a9","trusted":true},"cell_type":"code","source":"# X we will keep all columns execpt the last one \nX_train = np.expand_dims(dataset.values[:,:-1],axis = 2)\n# the last column is our label\ny_train = dataset.values[:,-1:]\n\n# for test we keep all the columns execpt the first one\nX_test = np.expand_dims(dataset.values[:,1:],axis = 2)\n\n# lets have a look on the shape \nprint(X_train.shape,y_train.shape,X_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dba3a0a291adce5f2146455fdb670996224aad70","id":"Ze2HVpp5U3Uc","trusted":true},"cell_type":"code","source":"# importing libraries required for our model\nfrom keras import optimizers\nfrom keras.utils import plot_model\nfrom keras.models import Sequential, Model\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten, Dropout\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6c04c796d4b30f7330d97ede3cdca92a7607a4c","id":"JvOqU0QOU3Ui","outputId":"7b57d5a1-a3b3-42a5-a7f8-8ae0d6515005","trusted":true},"cell_type":"code","source":"# DEFINE our model \nmodel_LSTM = Sequential()\nmodel_LSTM.add(LSTM(units =70,input_shape = (X_train.shape[1], X_train.shape[2])))\nmodel_LSTM.add(Dropout(0.4))\nmodel_LSTM.add(Dense(1))\n\nmodel_LSTM.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\nmodel_LSTM.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a25822391ef2ab412f5abdd2b317d9eec825e19b","scrolled":true,"id":"by7o6xEKU3Uq","outputId":"adc4fed9-a89f-48df-8246-f6e14d755e87","trusted":true},"cell_type":"code","source":"history_lstm = model_LSTM.fit(X_train,y_train,batch_size = 4000,epochs = 12)","execution_count":null,"outputs":[]},{"metadata":{"id":"1d-mFa9KjVYX","outputId":"7785e6ce-f9ba-4e3b-9084-6f93ee1755f3","trusted":true},"cell_type":"code","source":"# Plot the loss curves for training\nplt.plot(history_lstm.history['loss'], color='b', label=\"Training loss\")\nplt.legend(loc='best', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"z0rNBNEqtWbU"},"cell_type":"markdown","source":"### You can now import the predictions to the submission file "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}