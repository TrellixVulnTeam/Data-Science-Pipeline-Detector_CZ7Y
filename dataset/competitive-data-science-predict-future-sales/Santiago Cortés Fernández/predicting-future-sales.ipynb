{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"shops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\nshops.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\ncategories.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nitems.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ntraining_set.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\ntest.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Explorationg & EDA"},{"metadata":{},"cell_type":"markdown","source":"## Training Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,9))\nplt.subplots_adjust(hspace=.5)\n\nplt.subplot2grid((3,3), (0,0), colspan = 3)\ntraining_set['shop_id'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Shop ID Values in the Training Set (Normalized)')\n\nplt.subplot2grid((3,3), (1,0))\ntraining_set['item_id'].plot(kind='hist', alpha=0.7)\nplt.title('Item ID Histogram')\n\nplt.subplot2grid((3,3), (1,1))\ntraining_set['item_price'].plot(kind='hist', alpha=0.7, color='orange')\nplt.title('Item Price Histogram')\n\nplt.subplot2grid((3,3), (1,2))\ntraining_set['item_cnt_day'].plot(kind='hist', alpha=0.7, color='green')\nplt.title('Item Count Day Histogram')\n\nplt.subplot2grid((3,3), (2,0), colspan = 3)\ntraining_set['date_block_num'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Month (date_block_num) Values in the Training Set (Normalized)')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graphs above,\n\n* From the 60 different shop IDs, there is an uneven distribtution of these in the dataset. Four of these shops make around 25 percent of this dataset. These are shops.\n* The Item IDs seem to have variations in frequency, but it is no possible to make any further assumptions yet.\n* From the vast empty spaces in the histograms of 'item_price' and 'item_cnt_day', it is possible to argue that there are outliers in their distribution.\n* Plotting the individual months from January 2013 to October 2015, it is possible to see that the December months are the ones with a hgher amount of sales"},{"metadata":{},"cell_type":"markdown","source":"### Outliers by price and sales volume"},{"metadata":{},"cell_type":"markdown","source":"From the previous intuition regarding the outliers, using some boxplots it is possible to see that there are quite a few! Because of that, an empirical estimation is mabe (by looking at the boxplot) to identify the outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=training_set['item_cnt_day'])\nprint('Sale volume outliers:',training_set['item_id'][training_set['item_cnt_day']>=1000].unique())\n\nplt.figure(figsize=(10,4))\nplt.xlim(training_set['item_price'].min(), training_set['item_price'].max())\nsns.boxplot(x=training_set['item_price'])\nprint('Item price outliers:',training_set['item_id'][training_set['item_price']>=100000].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From that, I proceed to remove outliers from the training data set. Additionally, there is one price below zero (as seen when describing the data), so I change the value with the median"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set = training_set[training_set['item_price']<100000]\ntraining_set = training_set[training_set['item_cnt_day']<1001]\n\nmedian = training_set[(training_set['shop_id']==32)&(training_set['item_id']==2973)&(training_set['date_block_num']==4)&(training_set['item_price']>0)]['item_price'].median()\ntraining_set.loc[training_set['item_price']<0, 'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Revenue\nI believe that an interesting feature to have would be the revenue (total amount of money) from each transaction/sale"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set['revenue'] = training_set['item_price'] *  training_set['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Shops Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Additionally, by looking at the actual names and reading the community forums and notebooks, it was possible to determine that some shops have duplicated id/name.\n* 11 and 10\n* 1 and 58\n* 0 and 57"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set.loc[training_set.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n\ntraining_set.loc[training_set.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n\ntraining_set.loc[training_set.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Shop Names\nAfter using Google (and Google Maps) for a couple of minutes/hours, it was possible to understand that the structure of the sho_name is \"City\" - \"Type\" - \"Name\". With that, I use **Label Encoders** for encoding the city of each store"},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\nshops.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categories Dataset"},{"metadata":{},"cell_type":"markdown","source":"As done with the shops, I encode information about the categories, such as the name, type and subtype"},{"metadata":{"trusted":true},"cell_type":"code","source":"categories['split'] = categories['item_category_name'].str.split('-')\ncategories['type'] = categories['split'].map(lambda x: x[0].strip())\ncategories['type_code'] = LabelEncoder().fit_transform(categories['type'])\n\n# if subtype is nan then type\ncategories['subtype'] = categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncategories['subtype_code'] = LabelEncoder().fit_transform(categories['subtype'])\ncategories = categories[['item_category_id','type_code', 'subtype_code']]\n\ncategories.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Items Dataset\nFor now, I didn't find any good or relevant use to the items names, so I proceeded to remove them from the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"items.drop(['item_name'], axis=1, inplace=True)\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Dataset"},{"metadata":{},"cell_type":"markdown","source":"I just modify the types of the test data for future convenience. Additionally, I add the *date block* number from the month that the test data will be predcting"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feauture Engineering\nSince we have to alculate monthly sales, the creation and modification of feature consists in using and extending the information with have from each unique pair (item, shop) within the month (item, shop, month). This way train data will be similar to test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creation of Data structure for feauture engineering\nmatrix = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Generating the pairs\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = training_set[training_set['date_block_num']==i]\n    matrix.append(np.array(list(product([i], sales['shop_id'].unique(), sales['item_id'].unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the matrix generated, I proceed to generate new aggregated features"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_group = training_set.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ntemp_group.columns = ['item_cnt_month']\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month'].fillna(0).clip(0,20).astype(np.float16))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Likewise, I use the previous modification in the test dataset to add the month to predict to the matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Afterwards, using the EDA done to the other datasets, I proceed to join them with the feature-engineered datastructure"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, categories, on=['item_category_id'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Changing data types of the data structure to ease future processing\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean Encodings"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lags(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = lags(matrix, [1,2,3,6,12], 'item_cnt_month')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I create some additional aggregates for their posterior encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean number of sales per month\ntemp_group = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\n\n## Additional lags for the monthly average in count\nmatrix = lags(matrix, [1], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean quantities grouped by month and item\ntemp_group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_item_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\n\n## Additional Lags for the means that were created\nmatrix = lags(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Mean quantities by month and shop\ntemp_group = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_shop_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\n\n#As before, additional lags for the means are created\nmatrix = lags(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Means by month and the category Id\ntemp_group = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_cat_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\n\n## Same, additional Lags are created for the month-category means\nmatrix = lags(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Means by month, item and shop\ntemp_group = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = ['date_shop_cat_avg_item_cnt']\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\n\n#Creation of lags for the means of month, item and shop\nmatrix = lags(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Means using the month, shop, and shop type feature created before\ntemp_group = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = ['date_shop_type_avg_item_cnt']\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\n\n#Creating lags for... month, shop and the type\nmatrix = lags(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Likewise, creating of means with month, shop and the subtypes\ntemp_group = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = ['date_shop_subtype_avg_item_cnt']\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\n\n#Creating lags for... month, shop and the subtypes\nmatrix = lags(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_group = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_city_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num', 'city_code'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lags(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Means using month, item and the city encoding that was done previously\ntemp_group = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_item_city_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num', 'item_id', 'city_code'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\n\n# Lags with the month, item and encoded cities\nmatrix = lags(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Means with the month, and the type of category encodings done previously\ntemp_group = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_type_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num', 'type_code'], how='left')\nmatrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\n\n# Lags for the month and the encoded category types\nmatrix = lags(matrix, [1], 'date_type_avg_item_cnt')\nmatrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Just as before (what a surprise!), the means with month and the encoded subtypes of the categories\ntemp_group = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_subtype_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num', 'subtype_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\n\n# Lags for the months with encoded category subtypes\nmatrix = lags(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Additional Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Feature to relate the months along the dataset's years\nmatrix['month'] = matrix['date_block_num'] % 12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Feature to indicate the number of days per month\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final Preprocessing before training"},{"metadata":{},"cell_type":"markdown","source":"Because of the using 12 as lag value, it is necessary to drop those months. likewise, I remove the columns with this month's calculated values"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = matrix[matrix.date_block_num > 11]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Filling NAs from the lags\ndef process_nas(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = process_nas(matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ready to play! But firts, remove unnecesary information to avoid running out of space (happened a couple of times, unfortunately)"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.to_pickle('data.pkl')\ndel matrix\ndel group\ndel items\ndel shops\ndel cats\ndel train\n\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_pickle('data.pkl')\ndata = data[[\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code',\n    'subtype_code',\n    'item_cnt_month_lag_1',\n    'item_cnt_month_lag_2',\n    'item_cnt_month_lag_3',\n    'item_cnt_month_lag_6',\n    'item_cnt_month_lag_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_2',\n    'date_item_avg_item_cnt_lag_3',\n    'date_item_avg_item_cnt_lag_6',\n    'date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1',\n    'date_shop_avg_item_cnt_lag_2',\n    'date_shop_avg_item_cnt_lag_3',\n    'date_shop_avg_item_cnt_lag_6',\n    'date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    'month',\n    'days'\n]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Splitting\nValidation strategy is \n* 34 month for the test set\n* 33 month for the validation set \n* 13-33 months for the train."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\n## Remove unnecessary variables to allocate space\ndel data\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the training, the metaparameters where adjusted using the insights gathered during the course and documentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"## First Model\n\nmodel = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Second Moder\nrf_model = RandomForestRegressor(n_estimators=50, \n                                 max_depth=7, \n                                 random_state=0, \n                                 n_jobs=-1, \n                                 verbose=1)\n\nrf_model.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensembling Models"},{"metadata":{},"cell_type":"markdown","source":"To combine the 1st level model predictions, I'll use a simple linear regression (As I'm only feeding the model with predictions, it is not necessary a complex model)"},{"metadata":{},"cell_type":"markdown","source":"## Models Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predctions from XGB\nxgb_val_pred = model.predict(X_valid)\nxgb_test_pred = model.predict(X_test)\n\n## Predictions from RF\nrf_val_pred = rf_model.predict(X_valid)\nrf_test_pred = rf_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feauture Importance\nfig, ax = plt.subplots(1,1,figsize=(10, 14))\nplot_importance(booster=model, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Ensembling Architecture**\n\n1st level:\n* XGBM\n* Random forest\n\n2nd level:\n* Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset that will be the train set of the ensemble model.\nfirst_level = pd.DataFrame(xgb_val_pred, columns=['xgbm'])\nfirst_level['random_forest'] = rf_val_pred\nfirst_level['label'] = Y_valid.values\nfirst_level.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset that will be the test set of the ensemble model.\nfirst_level_test = pd.DataFrame(xgb_test_pred, columns=['xgbm'])\nfirst_level_test['random_forest'] = rf_test_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the model that will combine the other ones to hopefully make an overall better prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_model = LinearRegression(n_jobs=-1)\nfirst_level.drop('label', axis=1, inplace=True)\nmeta_model.fit(first_level, Y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_pred = meta_model.predict(first_level)\nfinal_predictions = meta_model.predict(first_level_test)\nprint('Train rmse:', np.sqrt(mean_squared_error(ensemble_pred, Y_valid)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": final_predictions.clip(0., 20.)\n})\nsubmission.to_csv('submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(xgb_val_pred, open('xgb_val.pickle', 'wb'))\npickle.dump(xgb_test_pred, open('xgb_test.pickle', 'wb'))\n\npickle.dump(rf_val_pred, open('rf_val.pickle', 'wb'))\npickle.dump(rf_test_pred, open('rf_test.pickle', 'wb'))\n\npickle.dump(final_predictions, open('ensemble_pred.pickle', 'wb'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}