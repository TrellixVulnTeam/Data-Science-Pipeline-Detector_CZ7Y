{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nsample_submission = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')\ntest = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\ntrain = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\nitem_categories = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\nprint('items shape:',items.shape)\nprint('sample_submission shape:',sample_submission.shape)\nprint('test shape:',test.shape)\nprint('train shape:',train.shape)\nprint('item categories shape:',item_categories.shape)\nprint('shops shape:',shops.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = test.copy()\nprueba = train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items['items_p_cat']= items.groupby(['item_category_id'])['item_id'].transform('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###### MANAGE DATABASES ##########","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prueba = prueba.drop(['date','date_block_num','shop_id','item_cnt_day'], axis=1)\nprueba = prueba.drop_duplicates(subset =\"item_id\",  keep='first') \ntest = pd.merge(test, prueba, on='item_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fechas(df):\n    dat = df[\"date\"].str.split(\".\", n = 3, expand = True) \n    df['day']= dat[0]\n    df['month']= dat[1].astype('int64')\n    df['year']= dat[2].astype('int64')-2012\n    df1 = df.drop(['date'], axis=1)\n    return df1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['date_block_num'] =34\ntest = test[['ID','date_block_num','shop_id','item_id','item_price']]\ntest['date']=\"30.11.2015\"\ntest= fechas(test)\ntrain= fechas(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train, items, on='item_id')\ntest = pd.merge(test, items, on='item_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def features_train(train):\n    train[\"item_name\"] = train[\"item_name\"].astype('category')\n    train[\"Item_name_cod\"] = train[\"item_name\"].cat.codes\n    \n    train['Item_date_count'] = train.groupby(['item_id','date_block_num'])['shop_id'].transform('count')\n    train['Shop_date_count'] = train.groupby(['shop_id','date_block_num'])['item_id'].transform('count')\n    train['Item_item_count'] = train.groupby(['item_id','date_block_num'])['item_id'].transform('count')\n    train['Shop_shop_count'] = train.groupby(['shop_id','date_block_num'])['shop_id'].transform('count')\n      \n    train['Item_month'] = train.groupby(['month'])['item_id'].transform('sum')\n    train['block_num'] = train.groupby(['date_block_num'])['item_id'].transform('sum')\n    train['Item_year'] = train.groupby(['year','item_id'])['item_id'].transform('sum')\n    train['Item_block'] = train.groupby(['item_id'])['date_block_num'].transform('sum')\n        \n    train['Item_date_sum'] = train.groupby(['item_id','date_block_num'])['item_id'].transform('sum')\n    train['Item_date_max'] = train.groupby(['item_id','date_block_num'])['item_id'].transform('max')\n    train['Item_date_min'] = train.groupby(['item_id','date_block_num'])['item_id'].transform('min')\n    \n    #train['Cat_date_mean'] = train.groupby(['item_category_id','date_block_num'])['item_id'].transform('mean')\n    train['Cat_date_max'] = train.groupby(['item_category_id','date_block_num'])['item_id'].transform('max')\n    train['Cat_date_min'] = train.groupby(['item_category_id','date_block_num'])['item_id'].transform('min')\n    \n    train['Price_p_cat_mean'] = train.groupby(['item_category_id'])['item_price'].transform('mean')\n    train['Price_p_cat_max'] = train.groupby(['item_category_id'])['item_price'].transform('max')\n    train['Price_p_cat_min'] = train.groupby(['item_category_id'])['item_price'].transform('min')\n    #train['Price_p_cat_std'] = train.groupby(['item_category_id'])['item_price'].transform('std')\n    \n    train['shop_by_item_by_year'] = train.groupby(['shop_id','year'])['item_id'].transform('count')\n    train['Year_shop_cat'] = train.groupby(['year','shop_id','item_category_id'])['item_id'].transform('count')\n    train['Month_shop_cat'] = train.groupby(['month','shop_id','item_category_id'])['item_id'].transform('count')\n    train['Block_shop_cat'] = train.groupby(['date_block_num','shop_id','item_category_id'])['item_id'].transform('count')\n    \n    train['pasta'] = train['item_cnt_day'] * train['item_price']\n    \n    train['number_one'] = train.groupby(['shop_id','date_block_num'])['item_cnt_day'].transform('sum')\n    train['number_two'] = train.groupby(['shop_id','item_id'])['item_cnt_day'].transform('sum')\n    #train['number_three'] = train.groupby(['item_id','date_block_num'])['item_cnt_day'].transform('sum')\n    train['number_three'] = train.groupby(['shop_id','month'])['item_cnt_day'].transform('sum')\n    train['number_four'] = train.groupby(['shop_id','item_id'])['pasta'].transform('sum')\n    train['number_five'] = train.groupby(['shop_id','item_id','date_block_num'])['pasta'].transform('sum')\n    #train['number_five'] = train.groupby(['item_id','year'])['item_cnt_day'].transform('sum') \n    \n    train = train.drop(['item_name','day','pasta'], axis=1)\n    return train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = features_train(train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train =train.drop(train[(train.item_cnt_day>=100)].index) # limitacion  item_cnt_day >=20\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"item_name\"] = test[\"item_name\"].astype('category')\ntest[\"Item_name_cod\"] = test[\"item_name\"].cat.codes\n\ntest['Item_date_count'] = test.groupby(['item_id','date_block_num'])['shop_id'].transform('count')\ntest['Shop_date_count'] = test.groupby(['shop_id','date_block_num'])['item_id'].transform('count')\ntest['Item_item_count'] = test.groupby(['item_id','date_block_num'])['item_id'].transform('count')\ntest['Shop_shop_count'] = test.groupby(['shop_id','date_block_num'])['shop_id'].transform('count')\n\ntest['Item_month'] = test.groupby(['month'])['item_id'].transform('sum')\ntest['block_num'] = test.groupby(['date_block_num'])['item_id'].transform('sum')\ntest['Item_year'] = test.groupby(['year','item_id'])['item_id'].transform('sum')\ntest['Item_block'] = test.groupby(['item_id'])['date_block_num'].transform('sum')\n\ntest['Item_date_sum'] = test.groupby(['item_id','date_block_num'])['item_id'].transform('sum')\ntest['Item_date_max'] = test.groupby(['item_id','date_block_num'])['item_id'].transform('max')\ntest['Item_date_min'] = test.groupby(['item_id','date_block_num'])['item_id'].transform('min')\n\n#test['Cat_date_mean'] = test.groupby(['item_category_id','date_block_num'])['item_id'].transform('mean')\ntest['Cat_date_max'] = test.groupby(['item_category_id','date_block_num'])['item_id'].transform('max')\ntest['Cat_date_min'] = test.groupby(['item_category_id','date_block_num'])['item_id'].transform('min')\n\ntest['uno'] = test.groupby(['item_category_id'])['item_price'].transform('mean')\n\ntest['item_price'].fillna(test['uno'], inplace=True)\n\n#test['item_price'].fillna(test['item_price'].mean(), inplace=True)\ntest['Price_p_cat_mean'] = test.groupby(['item_category_id'])['item_price'].transform('mean')\ntest['Price_p_cat_max'] = test.groupby(['item_category_id'])['item_price'].transform('max')\ntest['Price_p_cat_min'] = test.groupby(['item_category_id'])['item_price'].transform('min')\n#test['Price_p_cat_std'] = test.groupby(['item_category_id'])['item_price'].transform('std')\n\ntest['shop_by_item_by_year'] = test.groupby(['shop_id','year'])['item_id'].transform('count')\ntest['Year_shop_cat'] = test.groupby(['year','shop_id','item_category_id'])['item_id'].transform('count')\ntest['Month_shop_cat'] = test.groupby(['month','shop_id','item_category_id'])['item_id'].transform('count')\ntest['Block_shop_cat'] = test.groupby(['date_block_num','shop_id','item_category_id'])['item_id'].transform('count')\n\ntest.sort_values(\"ID\", inplace=True)\ntest = test.drop(['item_name','ID','day','uno'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## DEEP NEURAL NETWORK \n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization \nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\nfrom keras.layers.merge import Concatenate\nfrom keras.layers.core import Lambda, Flatten, Dense, Dropout\nfrom keras.initializers import glorot_uniform\nfrom keras.engine.topology import Layer\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom keras import regularizers\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def database_type(db_type):\n    \n    item_cnt_day= db_type[['item_cnt_day']].copy()\n    number_one = db_type[['number_one']].copy()\n    number_two = db_type[['number_two']].copy()\n    number_three = db_type[['number_three']].copy()\n    number_four = db_type[['number_four']].copy()\n    number_five = db_type[['number_five']].copy()\n\n    db_type = db_type.drop(['item_cnt_day', 'number_one','number_two','number_three','number_four','number_five'], axis=1)\n    #db_type = reduce_mem_usage(db_type)\n    \n    return [db_type,item_cnt_day, number_one, number_two, number_three, number_four, number_five]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv, val_csv = train_test_split(train, test_size = 0.1, random_state=42)\ntrain_csv = database_type(train_csv)\nval_csv = database_type(val_csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv[0].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nfilepath = \"weights1.best.hdf5\"\nbest_param =  ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', period=1)\n\ntrain = {'item_cnt_day': train_csv[1].values,         \n        'number_one': train_csv[2].values,\n        'number_two': train_csv[3].values,\n        'number_three': train_csv[4].values,\n        'number_four': train_csv[5].values,\n        'number_five': train_csv[6].values}\nvalidation = {'item_cnt_day': val_csv[1].values,         \n        'number_one': val_csv[2].values,\n        'number_two': val_csv[3].values,\n        'number_three': val_csv[4].values,\n        'number_four': val_csv[5].values,\n        'number_five': val_csv[6].values}\n\ndef model_sales(X):\n    X_input = Input(shape = (X.shape[1],))\n    \n    x0 = BatchNormalization()(X_input)\n\n    x1 = Dense(128, activation='relu')(x0)\n    x1 = Dense(64, activation='relu')(x1)\n    x1 = Dense(32, activation='relu')(x1)\n    x1 = Dense(16, activation='relu')(x1)\n    x1 = Dense(8, activation='relu')(x1)\n    output1  = Dense(1, activation='linear', name = 'number_one')(x1)\n     \n    x2 = concatenate([x0, output1])\n    x2 = Dense(128, activation='relu')(x2)\n    x2 = Dense(64, activation='relu')(x2)\n    x2 = Dense(32, activation='relu')(x2)\n    x2 = Dense(16, activation='relu')(x2)\n    x2 = Dense(8, activation='relu')(x2)\n    output2  = Dense(1, activation='linear', name = 'number_two')(x2)\n    \n    x3 = concatenate([x0, output1, output2])\n    x3 = Dense(128, activation='relu')(x3)\n    x3 = Dense(64, activation='relu')(x3)\n    x3 = Dense(32, activation='relu')(x3)\n    x3 = Dense(16, activation='relu')(x3)\n    x3 = Dense(8, activation='relu')(x3)\n    output3  = Dense(1, activation='linear', name = 'number_three')(x3)\n    \n    x4 = concatenate([x0, output1, output2, output3])\n    x4 = Dense(128, activation='relu')(x4)\n    x4 = Dense(64, activation='relu')(x4)\n    x4 = Dense(32, activation='relu')(x4)\n    x4 = Dense(16, activation='relu')(x4)\n    x4 = Dense(8, activation='relu')(x4)\n    output4  = Dense(1, activation='linear', name = 'number_four')(x4)\n    \n    x5 = concatenate([x0, output1, output2, output3, output4])\n    x5 = Dense(128, activation='relu')(x5)\n    x5 = Dense(64, activation='relu')(x5)\n    x5 = Dense(32, activation='relu')(x5)\n    x5 = Dense(16, activation='relu')(x5)\n    x5 = Dense(8, activation='relu')(x5)\n    output5  = Dense(1, activation='linear', name = 'number_five')(x5)\n    \n    x6 = concatenate([x0, output1, output2, output3, output4, output5])\n    x6 = Dense(128, activation='relu')(x6)\n    x6 = Dense(64, activation='relu')(x6)\n    x6 = Dense(32, activation='relu')(x6)\n    x6 = Dense(16, activation='relu')(x6)\n    x6 = Dense(8, activation='relu')(x6)\n    layerday  = Dense(1, activation='linear', name = 'item_cnt_day')(x6)\n\n    model= Model(inputs = [X_input] , outputs = [layerday])\n\n    model.summary()\n    return model\n    \nModel_sales = model_sales(train_csv[0])\nModel_sales.compile(loss='mean_squared_error', optimizer='Adam')\nhistory = Model_sales.fit(train_csv[0],train, validation_data=(val_csv[0],validation),epochs=30,verbose=1,batch_size = 256, callbacks=[best_param])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PREDICTION\n\nModel_sales.load_weights(\"weights1.best.hdf5\")\nModel_sales.compile(loss='mean_squared_error', optimizer='Adam')\nmodel_predict = Model_sales.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SUBMISSION:\n\ntest_pred['item_cnt_day'] = np.around(model_predict, decimals=0)\ntest_pred['item_cnt_month'] = test_pred.groupby(['item_id','shop_id'])['item_cnt_day'].transform('sum')\nsubmission = sample_submission.copy()\nsubmission['item_cnt_month'] = test_pred['item_cnt_month'].clip(0,20)\nsubmission.to_csv('submission_SALES.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CHECK. VALIDATION SET\nval_prueba = val_csv[0].copy()\n\n\nmodel_predict_val = Model_sales.predict(val_prueba)\nval_prueba['pred_item_cnt_day']= np.around(model_predict_val, decimals=0)\nval_prueba['pred_item_cnt_month']= val_prueba.groupby(['item_id','shop_id','date_block_num'])['pred_item_cnt_day'].transform('sum')\nval_prueba['item_cnt_day']=val_csv[1]\nval_prueba['item_cnt_month'] = val_prueba.groupby(['item_id','shop_id','date_block_num'])['item_cnt_day'].transform('sum').clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_prueba.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nval_prueba['diff'] = val_prueba['pred_item_cnt_month']- val_prueba['item_cnt_month']\nval_prueba['diff2'] = val_prueba['diff']**2\nsuma = val_prueba.diff2.sum()\nt = len(val_prueba)\nRmse = math.sqrt(suma/t)\nprint ('RMSE =', Rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ComplexNumber:\n    def __init__(self,r ,i ):\n        self.real = r\n        self.imag = i\n\n    def getData(self):\n        print(\"{0}+{1}j\".format(self.real,self.imag))\n    \n\n# Create a new ComplexNumber object\nc1 = ComplexNumber(2,3)\n\n# Call getData() function\n# Output: 2+3j\nc1.getData()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c2 = ComplexNumber(1,3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c2.getData()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del c2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c1.info","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}