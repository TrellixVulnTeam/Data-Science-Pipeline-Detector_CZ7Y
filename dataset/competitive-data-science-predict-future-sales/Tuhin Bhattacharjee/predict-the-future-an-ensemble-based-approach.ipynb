{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predict The Future - An Ensemble based approach\n\n### Introduction\nWelcome to an extensive Exploratory Data Analysis for the ongoing competition \"Predict The Future\". This notebook will grow over the coming days as I utilize this lockdown to improve my data-science skillset. In this notebook, we will be pre-processing the 4 different datasets and finally training an ensemble based boosting model of Decision Tree to predict the future demand of each item and shop combination from a Russian store. Also, before proceeding ahead with the notebook, I would like to express my gratitude and give due credit to \"Baek Kyun Shin\" whose existing notebook on LGBM and Feature engineering has been leveraged to a large extent for coming up with the code presented in this notebook. To know more about his code-base and analysis, please scroll down to the end of this noteboook. \n\nThe training data consists of four separate datasets which include\n1. Sales data (sales_train.csv)\n2. Item data  (items.csv)\n3. Shop data  (shops.csv)\n4. Item Category data (item_categories.csv)\n\nThe sales dataset contains the shop number as well as the item number and the date on which the said item was sold. It also includes the item price as well as the count of the items sold. This dataset will serve as our primary training set and we will use the other datasets as source of additional features to increase our prediction accuracy.\n\n### Load libraries","metadata":{"_uuid":"169a7501-88a1-4fbe-bf8a-bdaf6cda432e","_cell_guid":"8a112a3a-9029-4542-bd42-0f5f9a4574f7"}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Parameter Tuning ","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\ndata_path = '/kaggle/input/competitive-data-science-predict-future-sales/'\npd.set_option('float_format', '{:f}'.format)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Datasets","metadata":{}},{"cell_type":"code","source":"sales_train=pd.read_csv(data_path + 'sales_train.csv')\nshops=pd.read_csv(data_path + 'shops.csv')\nitems=pd.read_csv(data_path + 'items.csv')\nitem_categories=pd.read_csv(data_path + 'item_categories.csv')\ntest=pd.read_csv(data_path + 'test.csv')\nsubmission=pd.read_csv(data_path + 'sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the datasets all loaded into the dataframes, we are now all set to look under the hood to understand the basic structure of our data. The variable of interest is the column titled \"item_cnt_day\" which essentially will be the target variable for our predictive model. Also, we notice that the date format is not the regular machine-ready one and hence we need to convert it beforehand. We will be aggregating demand at a monthly level and hence it makes sense to assign a value of one to the day column so that the end result translates into the starting day of the month for each record in the sales table. We also go ahead and describe each of the columns in the sales dataset which throws up some interesting preliminary observations.","metadata":{}},{"cell_type":"code","source":"sales_train['date']=pd.to_datetime(sales_train['date'])\nsales_train['year']=sales_train['date'].dt.year\nsales_train['month']=sales_train['date'].dt.month\nsales_train['day']=sales_train['date'].dt.day\nsales_train['day']=1\n\nprint(sales_train.describe())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The describe output shows us that the minimum value for the number of items sold as well as the item price is below zero which is theoretically impractical. So this is an important observation which needs to be dealt with during data-cleaning. We also see that the item price and the number of items sold shoot up to astronomically high values which are especially above the 3rd quartile range of values. As such we realize that their are outliers or data-points which are much greater than the standard boundary of 1.5 * IQR and as such these need to be removed before proceeding ahead. After taking care of these sanity issues we go ahead with the plotting of the number of distinct items being sold in each shop.  ","metadata":{}},{"cell_type":"code","source":"sales_train = sales_train[sales_train['item_price'] > 0]\nsales_train = sales_train[sales_train['item_price'] < 50000]\nsales_train = sales_train[sales_train['item_cnt_day'] > 0]\nsales_train = sales_train[sales_train['item_cnt_day'] < 1000]\ndata = sales_train.groupby(['shop_id']).agg({'item_id':'nunique'}).reset_index()\n\nmpl.rc('font', size=6)\nfigure, ax=plt.subplots()\nfigure.set_size_inches(11,5)\ndata=data.reset_index()\n\nsns.barplot(x='shop_id', y='item_id', data=data)\nax.set(title='Distribution of items sold across different shops',xlabel='Shop Number',ylabel='Total Items Sold');\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot throws up certain interesting observations which need a further deep-dive to understand more about them. Some of them include:\n1. Shop number 11,20 and 36 seem to have an abnormally low number of items being sold\n2. Shop number 0,1,10 and 39 seem to be symmetric with shops 57,58,11 and 40 indicating that they might me related\n3. Shop number 25,31 and 54 seem to have exceptionally high number of items being sold\n\nTo understand more about the second observation, we go ahead and print the shop names for these shop id's and we realize that these shops are equivalent with just the city name concatenated at the end.","metadata":{}},{"cell_type":"code","source":"print(shops['shop_name'][0], '||', shops['shop_name'][57])\nprint(shops['shop_name'][1], '||', shops['shop_name'][58])\nprint(shops['shop_name'][10], '||', shops['shop_name'][11])\nprint(shops['shop_name'][39], '||', shops['shop_name'][40])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since these shops are the one and the same, we go ahead and replace the data for these shops with equivalent symmetric shop id's in both the training dataset as well as the test dataset. Also, we go ahead and create a separate column for the shop name in the shops dataset and categorically encode it so that it can act as a feature in our predictive model.","metadata":{}},{"cell_type":"code","source":"test.loc[test['shop_id'] == 0, 'shop_id'] = 57\ntest.loc[test['shop_id'] == 1, 'shop_id'] = 58\ntest.loc[test['shop_id'] == 10, 'shop_id'] = 11\ntest.loc[test['shop_id'] == 39, 'shop_id'] = 40\n\nshops['city'] = shops['shop_name'].apply(lambda x: x.split()[0])\nshops.loc[shops['city']=='!Якутск', 'city'] = 'Якутск'\nlabel_encoder = LabelEncoder()\nshops['city'] = label_encoder.fit_transform(shops['city'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preperation\nNext we go ahead with the aggregation of the sales dataset to get our item count at the year-month level. We also observe that the item price is unique at the year,month,shop_id and item_id level and as such we need to ensure that during aggregation the level of data remains fixed at this state. After aggregation, we go ahead and left join the sales dataset with the three other respective datasets to get all the features neccessary for our predictive model.The end result will be judged on the basis of RMSE and hence we need to ensure that we do a proper feature engineering to ensure the utmost accuracy. I will keep updating this section with more and more new features as and when i come across more such aspects.","metadata":{}},{"cell_type":"code","source":"#Data manipulation on the training dataset\ndata3 = sales_train.groupby(['year','month','date_block_num','shop_id','item_id']).agg({'item_price':'mean','item_cnt_day':'sum'}).reset_index()\ndata=pd.merge(data3,items,how='left', on='item_id')\ndata=pd.merge(data,item_categories,how='left',on='item_category_id')\ndata=pd.merge(data,shops,how='left',on='shop_id')\ndata['month']=data['date_block_num'].apply(lambda month: (month+1)%12)\ndata=data[['month','date_block_num','shop_id','item_id','item_category_id','city','item_price','item_cnt_day']]\n\n#Data manipulation on the testing dataset\ntest['date_block_num'] = 34\ntest['month']=11\nitem_price=data[['item_id','item_price']].groupby('item_id')['item_price'].mean().reset_index()\ntest=pd.merge(test,item_price,how='left',on='item_id')\ntest=pd.merge(test,items,how='left',on='item_id')\ntest=pd.merge(test,item_categories,how='left',on='item_category_id')\ntest=pd.merge(test,shops,how='left',on='shop_id')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Model Building\nFor the predictive model we will use the Light Gradient Boost Model which was originally developed by the researchers at Microsoft. We will have to break down our dataset into training and validation sets so that we can accordingly judge model accuracy. We will also set certain paramaters of the model which will enable us to retrain the model till an optimum RSME criteria is reached for the training and validation dataset. We have set the bagging fraction to 75% for optimum results and the number of leaf nodes will be pruned to 255 to ensure optimal accuracy.","metadata":{}},{"cell_type":"code","source":"target=['item_cnt_day']\nfeatures=['month','shop_id','item_id','item_category_id','city','item_price']\n\n\ndata1=data[data['month']<10]\ndata2=data[data['month']==10]\n\nx_train = data1[features].fillna(value=0)\ny_train=data1[target].fillna(value=0)\n\nx_valid=data2[features].fillna(value=0)\ny_valid=data2[target].fillna(value=0)\n\n\nparams = {'metric': 'rmse',\n          'num_leaves': 255,\n          'learning_rate': 0.005,\n          'feature_fraction': 0.75,\n          'bagging_fraction': 0.75,\n          'bagging_freq': 5,\n          'force_col_wise' : True,\n          'random_state': 10}\n\ncat_features=features\n\ndtrain=lgb.Dataset(x_train,y_train)\ndvalid=lgb.Dataset(x_valid,y_valid)\n\nlgb_model=lgb.train(params=params,\n                    train_set=dtrain,\n                    num_boost_round=1500,\n                    valid_sets=(dtrain, dvalid),\n                    early_stopping_rounds=150,\n                    categorical_feature=cat_features,\n                    verbose_eval=100)   \n\ntest1=test[features]\ntest1=test1.fillna(0)\ntest['preds']=lgb_model.predict(test1)\n\n\npreds=test[['ID','preds']]\npreds.columns=['ID','item_cnt_month']\npreds.to_csv('my_submission_lgbm_final.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I hope you guys had fun reading my notebook. This was my first attempt at a Kaggle notebook and hopefully there will be more to come in the future. I will periodically update this notebook with newer findings and optimized code as I get better and better in my journey towards becoming a data-scientist. \n\n### A huge shout-out to Baek Kyun Shin whose notebook has been a major source of inspiration for this code.\nHe is a consultant at KPC . A lot of the code used in this notebook has been sourced from his kernel titled \"(TOP 3.5%) LightGBM with Feature Engineering\".\n\nLink : https://www.kaggle.com/werooring/top-3-5-lightgbm-with-feature-engineering\n\nProfile Link : https://www.kaggle.com/werooring\n","metadata":{}}]}