{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nimport glob\nfrom functools import reduce\n\n\n%pip install deep_translator\nfrom deep_translator import GoogleTranslator as gt\n\n#text clustering library\n%pip install textpack\nfrom textpack import tp\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n\n\n#ML libraries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#figsizes for all plots\nfigsize = (18, 5)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:31:22.141342Z","iopub.execute_input":"2022-04-09T18:31:22.141728Z","iopub.status.idle":"2022-04-09T18:33:15.978121Z","shell.execute_reply.started":"2022-04-09T18:31:22.141633Z","shell.execute_reply":"2022-04-09T18:33:15.976995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading all files from the directory as DataFrames","metadata":{}},{"cell_type":"code","source":"import os\n#Loading all files in Kaggle kernel\nfilesList = []\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        \n        file = os.path.join(dirname, filename)    \n        fileExt = file[60:-4] #Get the name of the files following the current pattern\n\n        #read each file in their variable name\n        exec('{:s}_set = pd.read_csv(\"{:s}\")'.format(fileExt, file)) \n        \n        # -- Display the files that will be loaded and the varible names\n#         print('{:s}_set = pd.read_csv(\"{:s}\")'.format(fileExt, file))\n        \n\n        #append list of files\n        filesList.append('{:s}_set'.format(fileExt))\n\nfilesList","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:33:15.981198Z","iopub.execute_input":"2022-04-09T18:33:15.982206Z","iopub.status.idle":"2022-04-09T18:33:20.085884Z","shell.execute_reply.started":"2022-04-09T18:33:15.982158Z","shell.execute_reply":"2022-04-09T18:33:20.085287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Join data into consolidated datasets to work better\n\nI will join the data transforming an OLTP like schema to an OLAP like schema, to work better with our datasets","metadata":{}},{"cell_type":"code","source":"data = pd.merge(sales_train_set, items_set, on='item_id', how='left')\ndata = pd.merge(data, shops_set, on='shop_id', how='left')\ndata = pd.merge(data, item_categories_set, on='item_category_id', how='left')\n\n#Use date column as datetime type\nsales_train_set.date = pd.to_datetime(sales_train_set.date, format=\"%d.%m.%Y\")\ndata.date = pd.to_datetime(data.date, format=\"%d.%m.%Y\")\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:33:20.087067Z","iopub.execute_input":"2022-04-09T18:33:20.087439Z","iopub.status.idle":"2022-04-09T18:33:23.365528Z","shell.execute_reply.started":"2022-04-09T18:33:20.087408Z","shell.execute_reply":"2022-04-09T18:33:23.364377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA - Exploratory Data Analysis\n\nNow that I have the data all in one set, let's move on to the exploratory analysis step to better understand what's in our hands.\n\n### Translate the name of the stores to understand better\n\nI noticed that there are some store names that seem to be duplicates, as they are very similar. So I merged these stores into one ID.\n","metadata":{}},{"cell_type":"code","source":"shops_set['translated'] = shops_set.shop_name.apply(lambda x: gt(source='ru', target='en').translate(x))","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:33:23.36712Z","iopub.execute_input":"2022-04-09T18:33:23.367462Z","iopub.status.idle":"2022-04-09T18:34:11.067702Z","shell.execute_reply.started":"2022-04-09T18:33:23.367417Z","shell.execute_reply":"2022-04-09T18:34:11.066744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shops_set[[ 'shop_id', 'translated']].sort_values(by='translated')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:11.070841Z","iopub.execute_input":"2022-04-09T18:34:11.071196Z","iopub.status.idle":"2022-04-09T18:34:11.094029Z","shell.execute_reply.started":"2022-04-09T18:34:11.071154Z","shell.execute_reply":"2022-04-09T18:34:11.093174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Investigate if stores on shoppings have greater performance\n\nI didn't found any significant difference between store numbers, in general.","metadata":{}},{"cell_type":"code","source":"data = pd.merge(data, shops_set[['shop_id', 'translated']], on='shop_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:11.095258Z","iopub.execute_input":"2022-04-09T18:34:11.095502Z","iopub.status.idle":"2022-04-09T18:34:11.745393Z","shell.execute_reply.started":"2022-04-09T18:34:11.095471Z","shell.execute_reply":"2022-04-09T18:34:11.744395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.groupby(['translated']).sum()['item_cnt_day'].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:11.746832Z","iopub.execute_input":"2022-04-09T18:34:11.747768Z","iopub.status.idle":"2022-04-09T18:34:12.989052Z","shell.execute_reply.started":"2022-04-09T18:34:11.747719Z","shell.execute_reply":"2022-04-09T18:34:12.988058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#if there is shopping in 'translated', separate to analyse\nwordToAnalyse = 'shopping'\ngroup1 = data.loc[data['translated'].str.contains(wordToAnalyse, case=False)]\ngroup2 = data.loc[ ~ data['translated'].str.contains(wordToAnalyse, case=False)]\n\n#print each group mean\nprint('Difference between groups:', group1.item_cnt_day.mean() - group2.item_cnt_day.mean())\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:12.990308Z","iopub.execute_input":"2022-04-09T18:34:12.990586Z","iopub.status.idle":"2022-04-09T18:34:21.54492Z","shell.execute_reply.started":"2022-04-09T18:34:12.990554Z","shell.execute_reply":"2022-04-09T18:34:21.544264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After analysing the shops dataset, I noticed that some stores have similar names, so I will attribute the same ID for them.","metadata":{}},{"cell_type":"code","source":"data.shop_id = data.shop_id.replace({10:11, 0:57, 1:58})","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:21.54606Z","iopub.execute_input":"2022-04-09T18:34:21.546394Z","iopub.status.idle":"2022-04-09T18:34:21.590705Z","shell.execute_reply.started":"2022-04-09T18:34:21.546365Z","shell.execute_reply":"2022-04-09T18:34:21.589615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looking for nulls in data\n\nI found no null data, what is great.","metadata":{}},{"cell_type":"code","source":"#Find out if we have null data\n((data.isna().sum()/data.isna().count())*100).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:21.591905Z","iopub.execute_input":"2022-04-09T18:34:21.592171Z","iopub.status.idle":"2022-04-09T18:34:24.193818Z","shell.execute_reply.started":"2022-04-09T18:34:21.592119Z","shell.execute_reply":"2022-04-09T18:34:24.193021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The 'item_cnt_day' and 'price' features","metadata":{}},{"cell_type":"code","source":"print('item price description:')\nround(data.item_price.describe())","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:24.195205Z","iopub.execute_input":"2022-04-09T18:34:24.196006Z","iopub.status.idle":"2022-04-09T18:34:24.315452Z","shell.execute_reply.started":"2022-04-09T18:34:24.195951Z","shell.execute_reply":"2022-04-09T18:34:24.314509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('item count description:')\nround(data.item_cnt_day.describe())","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:24.317086Z","iopub.execute_input":"2022-04-09T18:34:24.3177Z","iopub.status.idle":"2022-04-09T18:34:24.40283Z","shell.execute_reply.started":"2022-04-09T18:34:24.317648Z","shell.execute_reply":"2022-04-09T18:34:24.401935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"perc1 = round(data[data['item_cnt_day'] < 0].count()['date'] / data['item_cnt_day'].sum(),5)\nperc2 = round(data[data['item_price'] < 0].count()['date'] / data['item_price'].sum(),5)\n\nprint('Proportion of negative values on item_cnt_day feature:', perc1*100, '%')\nprint('Proportion of negative values on item_price feature:', perc2*100, '%')\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:24.40396Z","iopub.execute_input":"2022-04-09T18:34:24.40419Z","iopub.status.idle":"2022-04-09T18:34:24.44644Z","shell.execute_reply.started":"2022-04-09T18:34:24.404162Z","shell.execute_reply":"2022-04-09T18:34:24.445467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are negative numbers on both variables and a lot of outliers, specially on [item_price]. The negative values could mean returned items, difference found on inventory, etc. \nFor the study I will handle outliers and change remaining negative values by the mode (in the [item_cnt_day]) or by 0 (in the [item_price] variable).","metadata":{}},{"cell_type":"code","source":"#distribution of Item cnt day considering outliers\nplt.figure(figsize=figsize)\nplt.title('Outliers of item_cnt_day before adjust')\nsns.boxplot(x=data['item_cnt_day'], color='red')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:24.450145Z","iopub.execute_input":"2022-04-09T18:34:24.450412Z","iopub.status.idle":"2022-04-09T18:34:25.912312Z","shell.execute_reply.started":"2022-04-09T18:34:24.450379Z","shell.execute_reply":"2022-04-09T18:34:25.911439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeOutliers(data, column, q1, q3):\n  #Detect the outliers using IQR technique\n\n  q1_value = data[column].quantile(q1)\n  q3_value = data[column].quantile(q3)\n  iqr = q3_value - q1_value\n  \n  #Upper and Lower Limits\n  upper = q3_value + 1.5 * iqr\n  lower = q1_value - 1.5 * iqr\n\n  print(\"Lower bound:\", lower)\n  print(\"Upper bound:\", upper)\n  \n  # new_df = data[data['Income'] > upper]\n  # new_df = data[data['Income']  < lower]\n\n  # Capping (if value is above ou below defined limit, it will be setted to the limit)\n  newColumn = column + '_Ol' \n\n  data[newColumn] = np.where(data[column] > upper, upper, \n                            np.where(data[column] < lower, lower,\n                            data[column]))\n\n  print(\"=\" *50)\n  print('New Column name is:', newColumn)\n\n  mode1 = data[newColumn].mode()\n\n  #If we have negative values, change for the mode.\n  data[newColumn] = data[newColumn].mask(data[newColumn] < 0, float(mode1))\n\n  #print result described\n  print(\"=\" *50)\n  print('item count description:')\n  print(round(data[newColumn].describe()))\n\n  # #distribution of Income without the greater outliers\n  # plt.figure(figsize=figsize)\n  # plt.title('Outliers of item_cnt_day after adjust')\n  # sns.boxplot(x=data['item_cnt_day_ol'], color='red')\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:25.913777Z","iopub.execute_input":"2022-04-09T18:34:25.914092Z","iopub.status.idle":"2022-04-09T18:34:25.924409Z","shell.execute_reply.started":"2022-04-09T18:34:25.914047Z","shell.execute_reply":"2022-04-09T18:34:25.923498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"removeOutliers(data, 'item_cnt_day', 0.01, 0.9)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:25.925576Z","iopub.execute_input":"2022-04-09T18:34:25.92597Z","iopub.status.idle":"2022-04-09T18:34:26.194352Z","shell.execute_reply.started":"2022-04-09T18:34:25.925939Z","shell.execute_reply":"2022-04-09T18:34:26.193371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill negative values from price with 0\ndata['item_price'] = data['item_price'].mask(data['item_price'] < 0, 0)\nround(data['item_price'].describe())","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:26.195473Z","iopub.execute_input":"2022-04-09T18:34:26.195687Z","iopub.status.idle":"2022-04-09T18:34:26.344368Z","shell.execute_reply.started":"2022-04-09T18:34:26.195659Z","shell.execute_reply":"2022-04-09T18:34:26.343326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot [item_cnt_day] over time to verify seasonality and trends ","metadata":{}},{"cell_type":"code","source":"# #adding month and year columns\n# sales_train_set['year'] = sales_train_set['date'].dt.year\n# sales_train_set['month'] = sales_train_set['date'].dt.month\n\n#item count over time\ndata.groupby(['date_block_num']).sum()['item_cnt_day'].plot(figsize=figsize, title='Item count sum over time \"months\"')\n\n#Same as below:\n# sales_train_set.groupby([sales_train_set.index.year, sales_train_set.index.month])['item_cnt_day'].sum().plot(figsize=figsize)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:26.345492Z","iopub.execute_input":"2022-04-09T18:34:26.345722Z","iopub.status.idle":"2022-04-09T18:34:27.062258Z","shell.execute_reply.started":"2022-04-09T18:34:26.345692Z","shell.execute_reply":"2022-04-09T18:34:27.06145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#item price over time\ndata.groupby(['date_block_num']).sum()['item_price'].plot(figsize=figsize, title='Item price sum over time \"months\"')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:27.063565Z","iopub.execute_input":"2022-04-09T18:34:27.063983Z","iopub.status.idle":"2022-04-09T18:34:27.609532Z","shell.execute_reply.started":"2022-04-09T18:34:27.063952Z","shell.execute_reply":"2022-04-09T18:34:27.608657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set date index to use the dataset in decomposing technique\ndata = data.set_index('date', inplace=False)\n\ndataTimeMean = pd.DataFrame(data.resample('M')['item_cnt_day'].sum()) #resample using months as parameter\n# dataTimeMean = dataTimeMean.fillna(0)\n# dataTimeMean.drop(dataTimeMean.tail(8).index, inplace=True) # Drop the last observations with 0 \n\n#Decompose Time Series\ndecompose = seasonal_decompose(dataTimeMean, extrapolate_trend=12)\n\n#Trend\nobs = decompose.observed\n#Trend\ntrend = decompose.trend\n#Seazonal\nseazon = decompose.seasonal\n#Error\nrandom = decompose.resid","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:27.610715Z","iopub.execute_input":"2022-04-09T18:34:27.61095Z","iopub.status.idle":"2022-04-09T18:34:29.828102Z","shell.execute_reply.started":"2022-04-09T18:34:27.610921Z","shell.execute_reply":"2022-04-09T18:34:29.826761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot\nfig, axes = plt.subplots(4, 1, figsize=(15,8), sharex=True)\nfig.suptitle('Decompose of the sum of Item Count over months')\n\nsns.lineplot(x=obs.index, y=obs, ax=axes[0], data=obs)\nsns.lineplot(x=trend.index, y=trend, ax=axes[1], data=trend)\nsns.lineplot(x=seazon.index, y=seazon, ax=axes[2], data=seazon)\nsns.lineplot(x=random.index, y=random, ax=axes[3], data=random)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:29.829818Z","iopub.execute_input":"2022-04-09T18:34:29.830194Z","iopub.status.idle":"2022-04-09T18:34:30.877595Z","shell.execute_reply.started":"2022-04-09T18:34:29.830144Z","shell.execute_reply":"2022-04-09T18:34:30.864704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Seazonality analysis (De-trending)\ndataTimeMean.diff(1).plot(figsize=figsize, title='Seazonality over months')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:30.878677Z","iopub.execute_input":"2022-04-09T18:34:30.879606Z","iopub.status.idle":"2022-04-09T18:34:31.246213Z","shell.execute_reply.started":"2022-04-09T18:34:30.879567Z","shell.execute_reply":"2022-04-09T18:34:31.245355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Seazonality of item counts\ndataTimeMean.item_cnt_day.diff(1).groupby(dataTimeMean.index.month).sum().plot(kind='bar', figsize=figsize, title='Seazonality of item count over months')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:31.247596Z","iopub.execute_input":"2022-04-09T18:34:31.247829Z","iopub.status.idle":"2022-04-09T18:34:31.511754Z","shell.execute_reply.started":"2022-04-09T18:34:31.2478Z","shell.execute_reply":"2022-04-09T18:34:31.510859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataTimeMean.groupby(dataTimeMean.index.month).mean().plot(kind='bar')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:31.513319Z","iopub.execute_input":"2022-04-09T18:34:31.513559Z","iopub.status.idle":"2022-04-09T18:34:31.797807Z","shell.execute_reply.started":"2022-04-09T18:34:31.513529Z","shell.execute_reply":"2022-04-09T18:34:31.796888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I then noticed a strong element of seasonality, indicating that in January the sum of items is always lower, after a large peak in December.\nI also observed a general downward trend in the number of items, as shown by the downward curve of the 'trend' chart. ","metadata":{}},{"cell_type":"markdown","source":"### Best stores classification\n\nI will define the best stores within the dataset","metadata":{}},{"cell_type":"code","source":"#item count by store\ndata.groupby(['shop_id']).sum()['item_cnt_day'].sort_values(ascending=False).plot(kind='bar', figsize=figsize, title='Item count by store')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:31.799254Z","iopub.execute_input":"2022-04-09T18:34:31.799493Z","iopub.status.idle":"2022-04-09T18:34:32.926694Z","shell.execute_reply.started":"2022-04-09T18:34:31.799464Z","shell.execute_reply":"2022-04-09T18:34:32.925745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I noticed that in the general analysis, stores 31, 25, 54 and 28 had the best performance in terms of the number of items sold. However, we know that the dataset has stores that were not operational during the entire period, so we will also evaluate the average sales taking into account the number of days that the store was operating (period between the first and last store registration in the dataset), which gives insight into which stores did best in the time they were in operation. ","metadata":{}},{"cell_type":"markdown","source":"#### Feature creation (operating days)","metadata":{}},{"cell_type":"code","source":"listShop = []\nlistDays = []\n\nfor i in range(len(shops_set.shop_id)):\n    a = data[data['shop_id'] == i].index.max()\n    b = data[data['shop_id'] == i].index.min()\n    opDays = (a-b).days\n\n    # print('Store {} had {} days operating'.format(i, opDays))\n    listShop.append(i) \n    listDays.append(opDays)\n\ndaysOps = pd.DataFrame({'shop_id': listShop,\n                        'opDays': listDays})\n\n#insert on new dataset the mean of item count considering the operation days\ndaysOps['meanByOpDays'] = data.groupby(['shop_id']).sum()['item_cnt_day'] / daysOps['opDays']\ndaysOps = daysOps.dropna(axis=0)\n\ndaysOps.sort_values(by='meanByOpDays', ascending=False).head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:32.928284Z","iopub.execute_input":"2022-04-09T18:34:32.928744Z","iopub.status.idle":"2022-04-09T18:34:34.208224Z","shell.execute_reply.started":"2022-04-09T18:34:32.928694Z","shell.execute_reply":"2022-04-09T18:34:34.207188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#item count by store (mean)\ndaysOps['meanByOpDays'].sort_values(ascending=False).plot(kind='bar', figsize=figsize, title='Item count by store operating days')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:34.209683Z","iopub.execute_input":"2022-04-09T18:34:34.209941Z","iopub.status.idle":"2022-04-09T18:34:35.283674Z","shell.execute_reply.started":"2022-04-09T18:34:34.209912Z","shell.execute_reply":"2022-04-09T18:34:35.282778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tests for predictions\n\nI'll start with predictions, testing linear regression algorithms","metadata":{}},{"cell_type":"markdown","source":"#### Cluster similar items using Text Pack","metadata":{}},{"cell_type":"code","source":"itemCluster = tp.TextPack(items_set, ['item_name'], match_threshold=0.1, ngram_remove=r'[,-./]', ngram_length=3)\nitemCluster.run(column_name='clustered')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:34:35.284947Z","iopub.execute_input":"2022-04-09T18:34:35.285207Z","iopub.status.idle":"2022-04-09T18:35:15.995903Z","shell.execute_reply.started":"2022-04-09T18:34:35.28517Z","shell.execute_reply":"2022-04-09T18:35:15.994963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Clustering reduced the items by text similatiry into {} categories'.format(items_set.clustered.nunique()))","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:15.997287Z","iopub.execute_input":"2022-04-09T18:35:15.997561Z","iopub.status.idle":"2022-04-09T18:35:16.008417Z","shell.execute_reply.started":"2022-04-09T18:35:15.997529Z","shell.execute_reply":"2022-04-09T18:35:16.007355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items_set['category'] = pd.factorize(items_set.clustered)[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:16.009909Z","iopub.execute_input":"2022-04-09T18:35:16.010749Z","iopub.status.idle":"2022-04-09T18:35:16.02361Z","shell.execute_reply.started":"2022-04-09T18:35:16.010701Z","shell.execute_reply":"2022-04-09T18:35:16.022455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items_set = items_set.drop('clustered', axis=1)\nitems_set","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:16.024827Z","iopub.execute_input":"2022-04-09T18:35:16.025099Z","iopub.status.idle":"2022-04-09T18:35:16.043949Z","shell.execute_reply.started":"2022-04-09T18:35:16.025057Z","shell.execute_reply":"2022-04-09T18:35:16.043348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#merge clustering result into merged data set\ndata = data.reset_index() #reset index to avoid losing date index\n\ndata = pd.merge(data, items_set[['category','item_id']], on='item_id', how='left')\n\ndata = data.set_index('date', inplace=False) #reset to date index again","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:16.044864Z","iopub.execute_input":"2022-04-09T18:35:16.045088Z","iopub.status.idle":"2022-04-09T18:35:18.060151Z","shell.execute_reply.started":"2022-04-09T18:35:16.04506Z","shell.execute_reply":"2022-04-09T18:35:18.059214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:18.061406Z","iopub.execute_input":"2022-04-09T18:35:18.061629Z","iopub.status.idle":"2022-04-09T18:35:18.081333Z","shell.execute_reply.started":"2022-04-09T18:35:18.061602Z","shell.execute_reply":"2022-04-09T18:35:18.080241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data[['date_block_num','shop_id','item_id','item_price','item_category_id','item_cnt_day_Ol', 'category']]\ndata","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:18.082899Z","iopub.execute_input":"2022-04-09T18:35:18.083323Z","iopub.status.idle":"2022-04-09T18:35:18.656871Z","shell.execute_reply.started":"2022-04-09T18:35:18.083278Z","shell.execute_reply":"2022-04-09T18:35:18.655907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Disregarding stores with more than X days inoperative","metadata":{}},{"cell_type":"code","source":"#find the shops with more than 30 days down, to set 0 for each item.\nlistShopsDown = []\n\nfor i in range(len(data.shop_id.unique())):\n    a = data[data['shop_id'] == i].index.max()\n    max = data.index.max()\n\n    \n    if (a - max).days < -30:\n        listShopsDown.append(i) \n    else:\n        pass\n\nprint('Shops to desconsider:', listShopsDown)\n\ndata_pred = data.query('shop_id != @listShopsDown') #remove stores from the data set therefore\ndata_pred","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:18.658095Z","iopub.execute_input":"2022-04-09T18:35:18.658436Z","iopub.status.idle":"2022-04-09T18:35:19.368139Z","shell.execute_reply.started":"2022-04-09T18:35:18.658341Z","shell.execute_reply":"2022-04-09T18:35:19.367394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Parcial auto correlation function (pac) to determine auto correlation between lags","metadata":{}},{"cell_type":"code","source":"# If we would use autoregression:\nlenOfDataToPredict = len(data_pred)\n\nif lenOfDataToPredict < 10:\n    lags=(lenOfDataToPredict/2)-1\nelse:\n    lags=10\n\nplot_pacf(data_pred.item_cnt_day_Ol, lags=lags)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:19.369386Z","iopub.execute_input":"2022-04-09T18:35:19.369629Z","iopub.status.idle":"2022-04-09T18:35:20.004975Z","shell.execute_reply.started":"2022-04-09T18:35:19.369596Z","shell.execute_reply":"2022-04-09T18:35:20.004023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature creation (lags and month of the year)","metadata":{}},{"cell_type":"code","source":"#sorting by items for lag creation\ndata_pred = data_pred.sort_values(by=['shop_id', 'item_id'])\n\n#creating features for autoregression technique\ndata_pred['x1'] = data_pred.item_cnt_day_Ol.shift(1)\ndata_pred['x2'] = data_pred.item_cnt_day_Ol.shift(2)\ndata_pred['x3'] = data_pred.item_cnt_day_Ol.shift(3)\n\n#feature engeneering\ndata_pred['month'] = data_pred.index.month\n# data_pred['week'] = data_pred.index.week\n# data_pred['day'] = data_pred.index.day\n\n\ndata_pred.dropna(axis=0, inplace=True)\n\ndata_pred.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:20.006296Z","iopub.execute_input":"2022-04-09T18:35:20.006903Z","iopub.status.idle":"2022-04-09T18:35:21.18671Z","shell.execute_reply.started":"2022-04-09T18:35:20.006868Z","shell.execute_reply":"2022-04-09T18:35:21.185766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dummies and normalization","metadata":{}},{"cell_type":"code","source":"#one hot encoding for the months\n# onehot = pd.get_dummies(data_pred.month, drop_first=True, prefix=\"m\")\n# data_pred = data_pred.drop('month', axis=1)\ndata_pred = pd.get_dummies(data_pred, columns=['month'], drop_first=True, prefix=\"m\")\ndata_pred = pd.get_dummies(data_pred, columns=['category'], drop_first=True, prefix=\"cat_\")\n\n#normalize item_price feature\ntoNormalizeData = data_pred['item_price']\n\nscaler = StandardScaler().fit(toNormalizeData.values.reshape(-1,1))\ntoNormalizeData = scaler.transform(toNormalizeData.values.reshape(-1,1))\n\n#Get the arrays generated back in the dataset\ndata_pred['item_price'] = toNormalizeData\n\ndata_pred","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:21.187997Z","iopub.execute_input":"2022-04-09T18:35:21.188275Z","iopub.status.idle":"2022-04-09T18:35:22.435033Z","shell.execute_reply.started":"2022-04-09T18:35:21.188241Z","shell.execute_reply":"2022-04-09T18:35:22.434214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#group to predict by month, not by day as we have daily data\ndata_pred = data_pred.groupby(['date_block_num', \n                                'shop_id',\n                                'item_id']).sum().reset_index()\n                                \n#remove aggregatiions on onehot encoded columns\nlistOfOHColumns = ['m_2','m_3','m_4','m_5','m_6','m_7','m_8','m_9','m_10','m_11','m_12']\n\nfor c in listOfOHColumns:\n    data_pred[c] = np.where(data_pred[c] >= 1, 1,\n                            data_pred[c])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:22.436584Z","iopub.execute_input":"2022-04-09T18:35:22.437049Z","iopub.status.idle":"2022-04-09T18:35:27.1472Z","shell.execute_reply.started":"2022-04-09T18:35:22.437004Z","shell.execute_reply":"2022-04-09T18:35:27.146182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Solve the summed item category ID issue\ntempItems = pd.merge(items_set, item_categories_set, on='item_category_id', how='left')\ntempItems = tempItems[['item_id','item_category_id']]\ntempItems","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:27.148482Z","iopub.execute_input":"2022-04-09T18:35:27.148703Z","iopub.status.idle":"2022-04-09T18:35:27.170406Z","shell.execute_reply.started":"2022-04-09T18:35:27.148675Z","shell.execute_reply":"2022-04-09T18:35:27.169787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_pred = data_pred.drop('item_category_id', axis=1)\ndata_pred = pd.merge(data_pred, tempItems, on='item_id', how='left')\ndata_pred = data_pred.drop('item_category_id', axis=1)\ndata_pred","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:27.175037Z","iopub.execute_input":"2022-04-09T18:35:27.175801Z","iopub.status.idle":"2022-04-09T18:35:27.859997Z","shell.execute_reply.started":"2022-04-09T18:35:27.17575Z","shell.execute_reply":"2022-04-09T18:35:27.859335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train and test split","metadata":{}},{"cell_type":"code","source":"randomState = 12\n\nX = data_pred.copy()\ny = X.pop('item_cnt_day_Ol')\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size = 0.2,\n                                                    random_state = 1\n                                                    )","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:27.861235Z","iopub.execute_input":"2022-04-09T18:35:27.861638Z","iopub.status.idle":"2022-04-09T18:35:28.47174Z","shell.execute_reply.started":"2022-04-09T18:35:27.861589Z","shell.execute_reply":"2022-04-09T18:35:28.47079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best performance model (LGBM)\n\nAfter testing few models using a Pipeline technique, I decided to focus on the LGBM model, as it has showed best scores","metadata":{}},{"cell_type":"code","source":"lgbm = LGBMRegressor(learning_rate= 0.2,\n                     max_depth= 20,\n                     num_leaves= 150,\n                     subsample= 0.4).fit(X_train, y_train)\n\npredictions_LGBM = np.around(lgbm.predict(X_test), decimals=1)\n\n\nprint('RMSE for lgbm was: \\n', mean_squared_error(y_test, predictions_LGBM, squared=False))\nprint('==============='*5)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:28.473097Z","iopub.execute_input":"2022-04-09T18:35:28.473447Z","iopub.status.idle":"2022-04-09T18:35:32.689462Z","shell.execute_reply.started":"2022-04-09T18:35:28.473404Z","shell.execute_reply":"2022-04-09T18:35:32.688657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_pred['pred_1'] = np.around(lgbm.predict(X), decimals=1)\ndata_pred[['item_cnt_day_Ol', 'pred_1']].plot(figsize=figsize, title='Plot with predictions and the actual data')\n\nprint('RMSE for lgbm on all data was: \\n', mean_squared_error(data_pred['item_cnt_day_Ol'], data_pred['pred_1'], squared=False))","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:32.690898Z","iopub.execute_input":"2022-04-09T18:35:32.691465Z","iopub.status.idle":"2022-04-09T18:35:38.900654Z","shell.execute_reply.started":"2022-04-09T18:35:32.691424Z","shell.execute_reply":"2022-04-09T18:35:38.899733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_pred.query('shop_id==2')[['item_cnt_day_Ol', 'pred_1']].plot(figsize=figsize, title='Plot with predictions and the actual data')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:38.902039Z","iopub.execute_input":"2022-04-09T18:35:38.902299Z","iopub.status.idle":"2022-04-09T18:35:39.213126Z","shell.execute_reply.started":"2022-04-09T18:35:38.902267Z","shell.execute_reply":"2022-04-09T18:35:39.212509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final model to run predictions:","metadata":{}},{"cell_type":"code","source":"lgbm = LGBMRegressor(learning_rate= 0.2,\n                     max_depth= 20,\n                     num_leaves= 150,\n                     subsample= 0.4).fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:39.214636Z","iopub.execute_input":"2022-04-09T18:35:39.215491Z","iopub.status.idle":"2022-04-09T18:35:44.178045Z","shell.execute_reply.started":"2022-04-09T18:35:39.215446Z","shell.execute_reply":"2022-04-09T18:35:44.177308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nI will insert in the data set that will be used for the predictions, the same variables created for the test set. These variables do not exist in the original data and therefore need to be added so that the model has the same capabilities for the final prediction. ","metadata":{}},{"cell_type":"code","source":"test_set_merge = pd.merge(test_set, data_pred[['item_id', 'shop_id', 'item_price', \n                                                'cat__1', 'cat__2', 'cat__3', 'cat__4', 'cat__5', 'cat__6', \n                                                'cat__7', 'cat__8', 'cat__9', 'cat__10', 'cat__11']], on=['item_id', 'shop_id'], how='left')\n\n#removing duplicates\ntest_set = pd.merge(test_set, test_set_merge.drop_duplicates(['ID']), on=['ID', 'item_id', 'shop_id'], how='left')\n\n#remove nans from items with no values on original data\ntest_set = test_set.fillna(0)\n\n#insert next month index\ntest_set['date_block_num'] = data_pred.date_block_num.max() + 1\n\n#drop ID\n# test_set = test_set.drop('ID', axis=1)\n\n#add month encoded columns to test data\ntest_set[['m_2','m_3','m_4','m_5','m_6','m_7','m_8','m_9','m_10','m_11','m_12']] = [0,0,0,0,0,0,0,0,0,1,0] #as the forecast will use the november as month\ntest_set[['x1', 'x2', 'x3']] = [None,None,None]\n\n#Vizualize\ntest_set.query('item_id == 5233 & shop_id == 5')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:44.181986Z","iopub.execute_input":"2022-04-09T18:35:44.184446Z","iopub.status.idle":"2022-04-09T18:35:44.773777Z","shell.execute_reply.started":"2022-04-09T18:35:44.184388Z","shell.execute_reply":"2022-04-09T18:35:44.773142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reorder columns to match the training dataset\ntest_set = test_set[X_test.columns]","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:44.774763Z","iopub.execute_input":"2022-04-09T18:35:44.775109Z","iopub.status.idle":"2022-04-09T18:35:44.800676Z","shell.execute_reply.started":"2022-04-09T18:35:44.775078Z","shell.execute_reply":"2022-04-09T18:35:44.799986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#concatenating the datasets, so I will be able to fill lags with actual data from previous months\ntemp_test = pd.concat([data_pred, test_set], keys=[\"x\", \"y\"])\ntemp_test","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:44.801664Z","iopub.execute_input":"2022-04-09T18:35:44.802287Z","iopub.status.idle":"2022-04-09T18:35:47.737877Z","shell.execute_reply.started":"2022-04-09T18:35:44.80225Z","shell.execute_reply":"2022-04-09T18:35:47.736862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#just to vizualize if the concat did it well\ntemp_test.sort_values(by=['shop_id', 'item_id', 'date_block_num']).head(20)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:47.739174Z","iopub.execute_input":"2022-04-09T18:35:47.73941Z","iopub.status.idle":"2022-04-09T18:35:48.75697Z","shell.execute_reply.started":"2022-04-09T18:35:47.739382Z","shell.execute_reply":"2022-04-09T18:35:48.756153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove aggregatiions on onehot encoded columns\nlistOfOHColumns = ['cat__1', 'cat__2', 'cat__3', 'cat__4', 'cat__5', 'cat__6', 'cat__7', 'cat__8', 'cat__9', 'cat__10', 'cat__11']\n\nfor c in listOfOHColumns:\n    temp_test[c] = np.where(temp_test[c] >= 1, 1,\n                            temp_test[c])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:48.758375Z","iopub.execute_input":"2022-04-09T18:35:48.758607Z","iopub.status.idle":"2022-04-09T18:35:48.886908Z","shell.execute_reply.started":"2022-04-09T18:35:48.758579Z","shell.execute_reply":"2022-04-09T18:35:48.88577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating features for autoregression technique\ntemp_test = temp_test.sort_values(by=['shop_id', 'item_id', 'date_block_num'])\n\ntemp_test['x1'] = temp_test['x1'].fillna(temp_test.item_cnt_day_Ol.shift(1))\ntemp_test['x2'] = temp_test['x2'].fillna(temp_test.item_cnt_day_Ol.shift(2))\ntemp_test['x3'] = temp_test['x3'].fillna(temp_test.item_cnt_day_Ol.shift(3))\n\n#fill residual NaNs with 1\ntemp_test = temp_test.loc['y'].fillna(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:48.888909Z","iopub.execute_input":"2022-04-09T18:35:48.889538Z","iopub.status.idle":"2022-04-09T18:35:50.425075Z","shell.execute_reply.started":"2022-04-09T18:35:48.889486Z","shell.execute_reply":"2022-04-09T18:35:50.424379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = temp_test[X.columns].sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:50.426161Z","iopub.execute_input":"2022-04-09T18:35:50.426852Z","iopub.status.idle":"2022-04-09T18:35:50.490062Z","shell.execute_reply.started":"2022-04-09T18:35:50.426813Z","shell.execute_reply":"2022-04-09T18:35:50.489253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction of the trained model using the given test dataset ","metadata":{}},{"cell_type":"code","source":"test_set['pred'] = np.around(lgbm.predict(test_set), decimals=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:50.491719Z","iopub.execute_input":"2022-04-09T18:35:50.492215Z","iopub.status.idle":"2022-04-09T18:35:50.865094Z","shell.execute_reply.started":"2022-04-09T18:35:50.492178Z","shell.execute_reply":"2022-04-09T18:35:50.864416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Items on test set that we do not have on train sets.\nitemsNotInTrain = test_set.item_id[~test_set.item_id.isin(data_pred.item_id)]\n\n# fill predictions to 0 for this items\ntest_set['pred'] = test_set.pred.where(test_set.item_id.isin(itemsNotInTrain), 0)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:50.866432Z","iopub.execute_input":"2022-04-09T18:35:50.866918Z","iopub.status.idle":"2022-04-09T18:35:50.891098Z","shell.execute_reply.started":"2022-04-09T18:35:50.866881Z","shell.execute_reply":"2022-04-09T18:35:50.890424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:50.892462Z","iopub.execute_input":"2022-04-09T18:35:50.892907Z","iopub.status.idle":"2022-04-09T18:35:50.962438Z","shell.execute_reply.started":"2022-04-09T18:35:50.892873Z","shell.execute_reply":"2022-04-09T18:35:50.961561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set['pred'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T18:35:50.963567Z","iopub.execute_input":"2022-04-09T18:35:50.963794Z","iopub.status.idle":"2022-04-09T18:35:50.978156Z","shell.execute_reply.started":"2022-04-09T18:35:50.963765Z","shell.execute_reply":"2022-04-09T18:35:50.977458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The final result for this notebook was a RMSE of 1.24015","metadata":{}},{"cell_type":"markdown","source":"# References\n\nhttps://github.com/lukewhyte/textpack\n\nhttps://www.kaggle.com/code/deinforcement/top-1-predict-future-sales-features-lightgbm/\n\nhttps://github.com/seatgeek/thefuzz","metadata":{}}]}