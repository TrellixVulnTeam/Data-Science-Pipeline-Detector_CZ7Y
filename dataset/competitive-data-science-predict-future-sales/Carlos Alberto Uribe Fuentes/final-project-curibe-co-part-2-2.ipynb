{"cells":[{"metadata":{},"cell_type":"markdown","source":"For memory issues on this platform, I have to split the Final Project in 2 parts:\n\nPart 1 \n* EDA\n* Features \n* Training models for validation\n\nPart 2 (This kernel)\n* Training models for predictions\n* Generating submission file"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load data of competition"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sales        = pd.read_csv(os.path.join(dirname, 'sales_train.csv'))\nitems        = pd.read_csv(os.path.join(dirname, 'items.csv'))\nitem_cat     = pd.read_csv(os.path.join(dirname, 'item_categories.csv'))\nshops        = pd.read_csv(os.path.join(dirname, 'shops.csv'))\ntest         = pd.read_csv(os.path.join(dirname, 'test.csv'))\nsubmission   = pd.read_csv(os.path.join(dirname, 'sample_submission.csv'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load Python libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nimport scipy\nimport seaborn\nimport gc\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nfrom tqdm import notebook\nfrom math import sqrt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\n\nfrom itertools import product\nimport joblib","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"List Versions used"},{"metadata":{"trusted":true},"cell_type":"code","source":"for p in [np, pd, scipy,sklearn, seaborn, lgb]:\n    print (p.__name__, p.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function to downsize types from 64 to 32 - took from luliu31415926 on github"},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function to calc RMSE"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_rmse(actual, predicted):\n    '''\n        Input: \n                actual, predicted: series object type\n        Output:\n                root mean squared error: float\n    '''\n    \n    # Select columns to downcast\n    mse = mean_squared_error(actual, predicted)\n    rmse = sqrt(mse)\n        \n    return rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**** PREDICTION STAGE ******"},{"metadata":{},"cell_type":"markdown","source":"I am going to append test data to the training data to do all features preprocessing and transformatinos, after I will split training from test data for training the model with all data available and then apply the prediction to the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_test = sales.copy()\ntest_grid = test[[\"shop_id\", \"item_id\"]]\ntest_grid[\"date_block_num\"] = 34\ntest_grid[\"month\"] = 11\ntest_grid[\"date\"] = test_grid[\"item_price\"] = test_grid[\"item_cnt_day\"] = 0\ntest_grid = test_grid.reindex(columns=sales.columns.values)\nsales_test = sales_test.append(test_grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the sequence it is the same as TRAINING VALIDATON STAGE - I omitted explaination."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in sales_test['date_block_num'].unique():\n    cur_shops = sales_test.loc[sales_test['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales_test.loc[sales_test['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates\ngb = sales_test.groupby(index_cols,as_index=False).item_cnt_day.sum()\ngb.columns = index_cols + [\"target\"]\n\n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n# Same as above but with shop-month aggregates\n\n#gb = sales_test.groupby(['shop_id', 'date_block_num'],as_index=False).item_cnt_day.sum()\n#gb.columns = [\"shop_id\", \"date_block_num\", \"target_shop\"]\n\ngb = sales_test.groupby(['shop_id', 'date_block_num'],as_index=False).agg({\"item_cnt_day\":\"sum\",\"item_price\":\"max\"})\ngb.columns = [\"shop_id\", \"date_block_num\", \"target_shop\", \"max_price\"]\n\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n# Same as above but with item-month aggregates\ngb = sales_test.groupby(['item_id', 'date_block_num'],as_index=False).item_cnt_day.sum()\ngb.columns = [\"item_id\", \"date_block_num\", \"target_item\"]\n\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\n# Downcast dtypes from 64 to 32 bit to save memory\nall_data = downcast_dtypes(all_data)\ndel grid, gb \ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of columns that we will use to create lags\ncols_to_rename = list(all_data.columns.difference(index_cols)) \n\nshift_range = [1, 2, 3, 4, 5, 12]\n\nfor month_shift in notebook.tqdm(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n\n    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\ndel train_shift\n\n# Don't use old data from previous month block\n#all_data = all_data[all_data['date_block_num'] >= 12] \n\n# I am going to use whole data available for testing\n\n# List of all lagged features\nfit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n\n#This line is needed with 2 ´date_block_num´  -- was used to fix error when you choose only 2 items witnin range\n#fit_cols += [col for col in all_data.columns if col[-2] in [str(item) for item in shift_range]]\n\n# We will drop these at fitting stage\nto_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n# Category for each item\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\nall_data = downcast_dtypes(all_data)\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seniority"},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_first_block = sales_test.groupby(\"shop_id\").date_block_num.min()\nall_data[\"shop_1st\"] = all_data[\"shop_id\"].map(shop_first_block)\nall_data[\"shop_1st\"] = all_data[\"date_block_num\"] - all_data[\"shop_1st\"]\n\nitem_first_block = sales_test.groupby(\"item_id\").date_block_num.min()\nall_data[\"item_1st\"] = all_data[\"item_id\"].map(item_first_block)\nall_data[\"item_1st\"] = all_data[\"date_block_num\"] - all_data[\"item_1st\"]\n\nall_data.shop_1st.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean Econding"},{"metadata":{},"cell_type":"markdown","source":"# Mean Encoding for item_id\ncumsum = all_data.groupby('item_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_id').cumcount()\nall_data['item_target_enc'] = cumsum / cumcnt\nall_data['item_target_enc'].fillna(0.3343, inplace=True) \n\n# Mean Encoding for shop_id\ncumsum = all_data.groupby('shop_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('shop_id').cumcount()\nall_data['shop_target_enc'] = cumsum / cumcnt\nall_data['shop_target_enc'].fillna(0.3343, inplace=True) \n\n# Mean Encoding for item_category_id\ncumsum = all_data.groupby('item_category_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_category_id').cumcount()\nall_data['category_target_enc'] = cumsum / cumcnt\nall_data['category_target_enc'].fillna(0.3343, inplace=True) \n\nall_data = all_data.drop([\"item_id\", \"shop_id\", \"item_category_id\"], axis=1)  # \"item_category_id\"\n\nall_data = downcast_dtypes(all_data)\ndel cumsum, cumcnt\ngc.collect();"},{"metadata":{},"cell_type":"markdown","source":"Month"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[\"month\"]=(all_data[\"date_block_num\"]+1)%12","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean Encoding for month\ncumsum = all_data.groupby('month')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('month').cumcount()\nall_data['month_enc'] = cumsum / cumcnt\nall_data['month_enc'].fillna(0.3343, inplace=True) \n\nall_data = all_data.drop([\"month\"], axis=1)\n\nall_data = downcast_dtypes(all_data)\ndel cumsum, cumcnt\ngc.collect();\n\nall_data.head(5)"},{"metadata":{},"cell_type":"markdown","source":"Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"dates = all_data['date_block_num']\n\nlast_block = dates.max()   #This will be our validation set\n\nprint('Test `date_block_num` is %d' % last_block)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates_train = dates[dates <  last_block]\ndates_test  = dates[dates == last_block]\n\nX_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\nX_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n\ny_train = all_data.loc[dates <  last_block, 'target'].values.clip(0,20)\ny_test =  all_data.loc[dates == last_block, 'target'].values.clip(0,20)\n\nprint(X_train.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just to ensure that index from test file and X_test set matchs for prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_compare = pd.DataFrame(data=X_test, columns=[\"ID\", \"shop_id\", \"item_id\"])\nX_test_compare.reset_index(inplace=True, drop=True)\nX_test_compare[\"ID\"]=X_test_compare.index\n\nprint(test.columns, test.shape, test.head())\nprint(X_test_compare.columns, X_test_compare.shape, X_test_compare.head())\nprint(\"Are the same\", (test == X_test_compare).value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First Level"},{"metadata":{},"cell_type":"markdown","source":"SGD Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = make_pipeline(StandardScaler(), SGDRegressor(loss=\"epsilon_insensitive\"))\nreg.fit(X_train.values, y_train)\npred_reg = reg.predict(X_test.values).clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"r2 train: \", r2_score(y_train, reg.predict(X_train.values).clip(0,20)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'pred_reg_model_l1.sav'\njoblib.dump(reg, filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LigthGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }\n\nlgb1 = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)    \npred_lgb = lgb1.predict(X_test).clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"r2 train: \", r2_score(y_train, lgb1.predict(X_train).clip(0,20)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'pred_lgb003_model_l1.sav'\njoblib.dump(lgb1, filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Level 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_level2 = np.c_[pred_reg, pred_lgb]\n\nprint(X_test_level2.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kfold range"},{"metadata":{"trusted":true},"cell_type":"code","source":"months_f=np.array([i for i in range(10,last_block)])\ndates_train_level2 = dates_train[dates_train.isin(months_f)]\n\n# That is how we get target for the 2nd level dataset\ny_train_level2 = y_train[dates_train.isin(months_f)]\n\nprint('shape of y_train_level2: {}'.format(y_train_level2.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kfold Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# And here we create 2nd level feature matrix, init it with zeros first\nX_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n\n# Now fill `X_train_level2` with metafeatures\nfor cur_block_num in notebook.tqdm(months_f):\n    \n    print(cur_block_num, end='')\n    \n    '''\n        1. Split `X_train` into parts\n           Remember, that corresponding dates are stored in `dates_train` \n        2. Fit linear regression \n        3. Fit LightGBM and put predictions          \n        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n           You can use `dates_train_level2` for it\n           Make sure the order of the meta-features is the same as in `X_test_level2`\n    '''      \n    \n    #  YOUR CODE GOES HERE\n    X_train_block = all_data.loc[dates < cur_block_num].drop(to_drop_cols, axis=1)\n    X_test_block = all_data.loc[dates == cur_block_num].drop(to_drop_cols, axis=1)\n    \n    y_train_block = all_data.loc[dates <  cur_block_num, 'target'].values.clip(0,20)\n    y_test_block = all_data.loc[dates == cur_block_num, 'target'].values.clip(0,20)\n    \n    print(':  X_train_block.shape={}'.format(X_train_block.shape), end='')\n    print(',  X_test_block.shape={}'.format(X_test_block.shape), end='')\n    print(',   Total Size={}'.format(X_train_block.shape[0] + X_test_block.shape[0]), end='')\n    print()\n    \n    reg.fit(X_train_block, y_train_block)\n    X_train_level2[dates_train_level2 == cur_block_num, 0] = reg.predict(X_test_block.values).clip(0,20)\n    \n    model = lgb.train(lgb_params, lgb.Dataset(X_train_block, label=y_train_block), 100)\n    X_train_level2[dates_train_level2 == cur_block_num, 1] = model.predict(X_test_block).clip(0,20)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_train_level2, y_train_level2)\n\nprint('Coefficient:            {}'.format(lr.coef_))\nprint('Normalized Coefficient: {}'.format(lr.coef_ / lr.coef_.sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_stacking_lr = lr.predict(np.vstack((pred_reg, pred_lgb)).T).clip(0,20)\ntest_preds_stacking_lr.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"r2 train: \", r2_score(y_train_level2, lr.predict(X_train_level2).clip(0,20)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'pred_stack_model_l2.sav'\njoblib.dump(lr, filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** GENERATING SUBMISSION FILE"},{"metadata":{"trusted":true},"cell_type":"code","source":"submissionstackinglr = submission.copy()\nsubmissionstackinglr[\"item_cnt_month\"]=test_preds_stacking_lr\nprint(submissionstackinglr.item_cnt_month.min(), submissionstackinglr.item_cnt_month.max())\nsubmissionstackinglr[\"item_cnt_month\"]\nprint(submissionstackinglr.item_cnt_month.min(), submissionstackinglr.item_cnt_month.max())\nprint(submissionstackinglr.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To generate submission file for download and send for assess."},{"metadata":{"trusted":true},"cell_type":"code","source":"submissionstackinglr.to_csv('cu_full_1_5_12_reg_lgb_stacked_lr_v4.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** END OF MODEL ***"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}