{"cells":[{"metadata":{},"cell_type":"markdown","source":"For memory issues on this platform, I have to split the Final Project in 2 parts:\n\nPart 1 (This kernel)\n* EDA\n* Features \n* Training models for validation\n\nPart 2\n* Training models for predictions\n* Generating submission file"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sales        = pd.read_csv(os.path.join(dirname, 'sales_train.csv'))\nitems        = pd.read_csv(os.path.join(dirname, 'items.csv'))\nitem_cat     = pd.read_csv(os.path.join(dirname, 'item_categories.csv'))\nshops        = pd.read_csv(os.path.join(dirname, 'shops.csv'))\ntest         = pd.read_csv(os.path.join(dirname, 'test.csv'))\nsubmission   = pd.read_csv(os.path.join(dirname, 'sample_submission.csv'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load Python libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nimport scipy\nimport seaborn\nimport gc\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nfrom tqdm import notebook\nfrom math import sqrt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\n\nfrom itertools import product\nimport joblib","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"List Versions used"},{"metadata":{"trusted":true},"cell_type":"code","source":"for p in [np, pd, scipy,sklearn, seaborn, lgb]:\n    print (p.__name__, p.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function to downsize types from 64 to 32 - took from luliu31415926 on github"},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function to calc RMSE"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_rmse(actual, predicted):\n    '''\n        Input: \n                actual, predicted: series object type\n        Output:\n                root mean squared error: float\n    '''\n    \n    # Select columns to downcast\n    mse = mean_squared_error(actual, predicted)\n    rmse = sqrt(mse)\n        \n    return rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**EDA STAGE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\nwarnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\nseaborn.set_style(\"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function to plot data - took from Cloistered Monkey"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_figure_and_axis(x_label, y_label, title, figsize=(10, 8)):\n    \"\"\"make a matplotlib figure\n\n    Args:\n     x_label (str): label for the x-axis\n     y_label (str): label for the y-axis\n     title (str): title for the plot\n     figsize: tuple of width, height\n    Returns:\n     tuple: figure, axis\n    \"\"\"\n    fig = plt.figure(figsize=figsize)\n    axe = fig.gca()\n    axe.set_xlabel(x_label)\n    axe.set_ylabel(y_label)\n    axe.set_title(title)\n    return fig, axe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Basic anlysis of data - complementing those posted by Kaggle Data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trasaction dataset \"sales\" and \"test\"\n\npd.options.display.float_format = '{:,.2f}'.format\nprint(\"Sales\\n\", sales.describe(), \"\\n\")\n\nprint(\"Test\\n\",test.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking for outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sales.item_price.value_counts())\nprint(sales[sales[\"item_price\"] == -1].item_price.value_counts())\nprint(sales[sales[\"item_price\"] == 307980].item_price.value_counts())\nitem1 = sales[sales[\"item_price\"] == -1].item_id.max()\nitem2 = sales[sales[\"item_price\"] == 307980].item_id.max()\n\nprint(\"Item: \", item1, \"\\n\", sales[sales[\"item_id\"] == item1].item_price.value_counts())\nprint(\"Item: \", item2, \"\\n\", sales[sales[\"item_id\"] == item2].item_price.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look for item_id with price outlier in test set, to determine if it is possible to drop"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test[test[\"item_id\"] == item2].item_id.count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confirm outlayers with plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.plot(sales.item_price, \".\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Actions, replace by mean in the first case and drop unique row for special item just to not distort de model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sales.loc[sales[\"item_price\"] == -1, [\"item_id\", \"item_price\"]]) \nsales.loc[sales[\"item_price\"] == -1, [\"item_price\"]] = sales[sales[\"item_id\"] == item1].item_price.mean()\nprint(sales.loc[sales[\"item_price\"] == -1, [\"item_id\"]]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales[sales.item_price != 307980]\nprint(sales[\"item_price\"].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.plot(sales.item_cnt_day, \".\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I don't take any action in this case"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(sales.date_block_num, sales.date_block_num.max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploring shop_id distribution beetwen sales and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(sales.shop_id, len(sales.shop_id.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(test.shop_id, len(test.shop_id.unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This distribution show that test data was sintetic, hardly any validation set sustracted from sales data will be aproximated to this."},{"metadata":{},"cell_type":"markdown","source":"Prepare data for specific analysis based on competition's target"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_test = sales.copy()\nsales_test[\"item_revenue\"] = sales_test.item_price*sales_test.item_cnt_day \n\nsales_test[\"day\"] = sales_test.date.str[0:2]\nsales_test[\"month\"] = sales_test.date.str[3:5]\nsales_test[\"year\"] = sales_test.date.str[6:10]\n\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\ngrid = [] \nfor block_num in sales_test['date_block_num'].unique():\n    cur_shops = sales_test.loc[sales_test['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales_test.loc[sales_test['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates\ngb = sales_test.groupby(index_cols,as_index=False).item_cnt_day.sum()\ngb.columns = index_cols + [\"month_qty\"]\n\n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\nall_data[\"month\"]=(all_data[\"date_block_num\"]+1)%12\nall_data[\"year\"]=(all_data[\"date_block_num\"]+1)//12\nall_data.loc[all_data[\"year\"] == 0, [\"year\"]] = 2013\nall_data.loc[all_data[\"year\"] == 1, [\"year\"]] = 2014\nall_data.loc[all_data[\"year\"] == 2, [\"year\"]] = 2015\n\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n\nall_data.head()\n\n# Downcast dtypes from 64 to 32 bit to save memory\nall_data = downcast_dtypes(all_data)\ndel grid, gb \ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look for taget variable trends in time"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = seaborn.catplot(x=\"date_block_num\", data=all_data, kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See item sold per month"},{"metadata":{"trusted":true},"cell_type":"code","source":"date_group = all_data.groupby(\"date_block_num\")\nsummed = date_group.sum()\nsummed = summed.reset_index()\n\ngrid = seaborn.relplot(x=\"date_block_num\", y=\"month_qty\", data=summed, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[\"Date\"] = all_data.year.apply(str) + \"-\" + all_data.month.apply(str)\nmonth_grouped = all_data.groupby(\"Date\")\nmonth_summed = month_grouped.sum().reset_index()\n\ntop_two = month_summed.sort_values(\"month_qty\", ascending=False)[:2]\n\ngrid = seaborn.relplot(x=\"Date\", y=\"month_qty\", data=month_summed, kind=\"line\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(top_two[[\"Date\", \"month_qty\"]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shop analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"group = all_data.groupby(\"shop_id\").sum().reset_index().sort_values(\"month_qty\")\n\ngrid = seaborn.relplot(x=\"shop_id\", y=\"month_qty\", data=group)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Item category analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"group = all_data.groupby(\"item_category_id\").sum().reset_index()\n\ngrid = seaborn.relplot(x=\"item_category_id\", y=\"month_qty\", data=group)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Category most frecuent sold"},{"metadata":{"trusted":true},"cell_type":"code","source":"category_group = all_data.groupby([\"Date\", \"item_category_id\"]).sum().reset_index()\nbiggest = category_group.iloc[category_group[\"month_qty\"].idxmax()]\n\nbiggest_category = category_group[category_group.item_category_id == biggest.item_category_id]\n\ngrid = seaborn.relplot(x=\"Date\", y=\"month_qty\", data=biggest_category, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Biggest category\\n\", biggest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again its confirms sales decreasing during the time"},{"metadata":{},"cell_type":"markdown","source":"*** TRAINING STAGE ***"},{"metadata":{},"cell_type":"markdown","source":"Here I used the code from \"Ensembling implementation\" programming assigment, from the course, to generate grid matrix of features.  We did some adjust to run the code and add item_price as a new feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_test = sales.copy()\n# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in sales_test['date_block_num'].unique():\n    cur_shops = sales_test.loc[sales_test['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales_test.loc[sales_test['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates\ngb = sales_test.groupby(index_cols,as_index=False).item_cnt_day.sum()\ngb.columns = index_cols + [\"target\"]\n\n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n# Same as above but with shop-month aggregates\n\n#gb = sales_test.groupby(['shop_id', 'date_block_num'],as_index=False).item_cnt_day.sum()\n#gb.columns = [\"shop_id\", \"date_block_num\", \"target_shop\"]\n\ngb = sales_test.groupby(['shop_id', 'date_block_num'],as_index=False).agg({\"item_cnt_day\":\"sum\",\"item_price\":\"max\"})\ngb.columns = [\"shop_id\", \"date_block_num\", \"target_shop\", \"max_price\"]\n\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n# Same as above but with item-month aggregates\ngb = sales_test.groupby(['item_id', 'date_block_num'],as_index=False).item_cnt_day.sum()\ngb.columns = [\"item_id\", \"date_block_num\", \"target_item\"]\n\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\n# Downcast dtypes from 64 to 32 bit to save memory\nall_data = downcast_dtypes(all_data)\ndel grid, gb \ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are going to add the most important features, lags for target, item, shop and price -- adapted from \"Ensembling implementation\" programming assigment.  I tested manually different lags ranges and at the end best metrics was obtained selecting 1:5 and 12 range."},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of columns that we will use to create lags\ncols_to_rename = list(all_data.columns.difference(index_cols)) \n\nshift_range = [1, 2, 3, 4, 5, 12]\n\nfor month_shift in notebook.tqdm(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n\n    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\ndel train_shift\n\n# Don't use old data from previous month block\n#all_data = all_data[all_data['date_block_num'] >= 12] \n\n# I am going to use whole data available for testing\n\n# List of all lagged features\nfit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n\n#This line is needed with 2 ´date_block_num´  -- was used to fix error when you choose only 2 items witnin range\n#fit_cols += [col for col in all_data.columns if col[-2] in [str(item) for item in shift_range]]\n\n# We will drop these at fitting stage\nto_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n# Category for each item\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n\nall_data = downcast_dtypes(all_data)\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are going to add new features related with first time when appear in training set item_id and shop_id. Then I substract from date_block_num in each row to mean seniority in training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_first_block = sales_test.groupby(\"shop_id\").date_block_num.min()\nall_data[\"shop_1st\"] = all_data[\"shop_id\"].map(shop_first_block)\nall_data[\"shop_1st\"] = all_data[\"date_block_num\"] - all_data[\"shop_1st\"]\n\nitem_first_block = sales_test.groupby(\"item_id\").date_block_num.min()\nall_data[\"item_1st\"] = all_data[\"item_id\"].map(item_first_block)\nall_data[\"item_1st\"] = all_data[\"date_block_num\"] - all_data[\"item_1st\"]\n\nall_data.shop_1st.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding Mean Encoding for shop_id, item_id and item_category_id.  \n\nIn this case I test applying expanding mean scheme for regualization purpose but model metrics was not favorable, then for final version was omitted."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Mean Encoding for item_id\ncumsum = all_data.groupby('item_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_id').cumcount()\nall_data['item_target_enc'] = cumsum / cumcnt\nall_data['item_target_enc'].fillna(0.3343, inplace=True) \n\n# Mean Encoding for shop_id\ncumsum = all_data.groupby('shop_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('shop_id').cumcount()\nall_data['shop_target_enc'] = cumsum / cumcnt\nall_data['shop_target_enc'].fillna(0.3343, inplace=True) \n\n# Mean Encoding for item_category_id\ncumsum = all_data.groupby('item_category_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_category_id').cumcount()\nall_data['category_target_enc'] = cumsum / cumcnt\nall_data['category_target_enc'].fillna(0.3343, inplace=True) \n\nall_data = all_data.drop([\"item_id\", \"shop_id\", \"item_category_id\"], axis=1)  # \"item_category_id\"\n\nall_data = downcast_dtypes(all_data)\ndel cumsum, cumcnt\ngc.collect();"},{"metadata":{},"cell_type":"markdown","source":"Now I am going to add month to grid based on date_block_num, this for include in this new feature seasonality that its represent the month in the year - this seasonality for sales was viewed in a histogram data."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[\"month\"]=(all_data[\"date_block_num\"]+1)%12\nall_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we must encoding month, the same as before, because this columns is likehood to categorical feature.  At the end was ommitted for model metrics results."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Mean Encoding for month\ncumsum = all_data.groupby('month')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('month').cumcount()\nall_data['month_enc'] = cumsum / cumcnt\nall_data['month_enc'].fillna(0.3343, inplace=True) \n\nall_data = all_data.drop([\"month\"], axis=1)\n\nall_data = downcast_dtypes(all_data)\ndel cumsum, cumcnt\ngc.collect();"},{"metadata":{},"cell_type":"markdown","source":"Split trasaction dataframes into train and test, taking in a count that this is a regression over the time then prediction are going to be done by month, then we reserve the last block/month to be the validation set (test set)."},{"metadata":{"trusted":true},"cell_type":"code","source":"dates = all_data['date_block_num']\n\nlast_block = dates.max()   #This will be our validation set\n\nprint('Test `date_block_num` is %d' % last_block)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we split the data as explained but we must delete from the trainset all columns related with our target to predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"dates_train = dates[dates <  last_block]\ndates_test  = dates[dates == last_block]\n\nX_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\nX_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n\ny_train = all_data.loc[dates <  last_block, 'target'].values.clip(0,20)\ny_test =  all_data.loc[dates == last_block, 'target'].values.clip(0,20)\n\nprint(X_train.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"APPLYING MODELS OF THE 1st LEVEL OF ENSEMBLING"},{"metadata":{},"cell_type":"markdown","source":"**Linear Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_train.values, y_train)\npred_lr = lr.predict(X_test.values).clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calc RMSE and R2 for model Linear Regression. this will be used manually to select best features conformation until obtain best R2."},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_lr = get_rmse(y_test, pred_lr)\nprint(\"rmse: \", rmse_lr)\nprint(\"r2 train: \", r2_score(y_train, lr.predict(X_train.values).clip(0,20)))\nprint(\"r2 test: \", r2_score(y_test, pred_lr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Serialize the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'lr_model_l1.sav'\njoblib.dump(lr, filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SGD Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = make_pipeline(StandardScaler(), SGDRegressor(loss=\"epsilon_insensitive\"))\nreg.fit(X_train.values, y_train)\npred_reg = reg.predict(X_test.values).clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calc RMSE for model SGD Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_reg = get_rmse(y_test, pred_reg)\nprint(\"rmse: \", rmse_reg)\nprint(\"r2 train: \", r2_score(y_train, reg.predict(X_train.values).clip(0,20)))\nprint(\"r2 test: \", r2_score(y_test, pred_reg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Serialize"},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'reg_model_l1.sav'\njoblib.dump(reg, filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We tested with whole types and parameters of SGDRegressor in order to obtain better performance than linear regression.\nI tried other models like SVR from svm and Logistic Regression but none could improve linear regression."},{"metadata":{},"cell_type":"markdown","source":"LightGBM Gradient Boosting with decision trees."},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }\n\nlgb1 = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)    \npred_lgb = lgb1.predict(X_test).clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calc RMSE an R2 for model Light GBM.  This metrics were the key for optimize learning rate and iterations, we test over 20 combinations to obtain better performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_lgb = get_rmse(y_test, pred_lgb)\nprint(\"rmse: \", rmse_lgb)\nprint(\"r2 train: \", r2_score(y_train, lgb1.predict(X_train.values).clip(0,20)))\nprint(\"r2 test: \", r2_score(y_test, pred_lgb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Serialize"},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'lgb003_model_l1.sav'\njoblib.dump(lgb1, filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"varimp = lgb1.feature_importance()\nnames = X_train.columns.values\nvar_array = pd.DataFrame(list(zip(names, varimp)))\nvar_array.columns=[\"names\", \"varimp\"]\nvar_array.plot(kind=\"bar\", x=\"names\", y=\"varimp\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can observe feature importance of model, IDs are most important, then season feature (month of the year), then lag 1 pack and seniority measure for shop and item."},{"metadata":{},"cell_type":"markdown","source":"BEGINS 2nd LEVEL FOR ENSEMBLING"},{"metadata":{},"cell_type":"markdown","source":"Test level 2 are taking directly from predictions of two models L1"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_level2 = np.c_[pred_reg, pred_lgb]\n\nprint(X_test_level2.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am going to use \"KFold scheme in time series\" to validate 2nd. level model from stacking procedure."},{"metadata":{"trusted":true},"cell_type":"code","source":"months_f=np.array([i for i in range(11,last_block)])\ndates_train_level2 = dates_train[dates_train.isin(months_f)]\n\n# That is how we get target for the 2nd level dataset\ny_train_level2 = y_train[dates_train.isin(months_f)]\n\nprint('shape of y_train_level2: {}'.format(y_train_level2.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, I used routine from programming assignment from course, I did some adjusts in params and code to be usable here."},{"metadata":{"trusted":true},"cell_type":"code","source":"# And here we create 2nd level feature matrix, init it with zeros first\nX_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n\n# Now fill `X_train_level2` with metafeatures\nfor cur_block_num in notebook.tqdm(months_f):\n    \n    print(cur_block_num, end='')\n    \n    '''\n        1. Split `X_train` into parts\n           Remember, that corresponding dates are stored in `dates_train` \n        2. Fit linear regression \n        3. Fit LightGBM and put predictions          \n        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n           You can use `dates_train_level2` for it\n           Make sure the order of the meta-features is the same as in `X_test_level2`\n    '''      \n    \n    #  YOUR CODE GOES HERE\n    X_train_block = all_data.loc[dates < cur_block_num].drop(to_drop_cols, axis=1)\n    X_test_block = all_data.loc[dates == cur_block_num].drop(to_drop_cols, axis=1)\n    \n    y_train_block = all_data.loc[dates <  cur_block_num, 'target'].values.clip(0,20)\n    y_test_block = all_data.loc[dates == cur_block_num, 'target'].values.clip(0,20)\n    \n    print(':  X_train_block.shape={}'.format(X_train_block.shape), end='')\n    print(',  X_test_block.shape={}'.format(X_test_block.shape), end='')\n    print(',   Total Size={}'.format(X_train_block.shape[0] + X_test_block.shape[0]), end='')\n    print()\n    \n    reg.fit(X_train_block, y_train_block)\n    X_train_level2[dates_train_level2 == cur_block_num, 0] = reg.predict(X_test_block.values).clip(0,20)\n    \n    model = lgb.train(lgb_params, lgb.Dataset(X_train_block, label=y_train_block), 100)\n    X_train_level2[dates_train_level2 == cur_block_num, 1] = model.predict(X_test_block).clip(0,20)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train 2nd level model, Linear regression in this case"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train_level2, y_train_level2)\n\nprint('Coefficient:            {}'.format(lr.coef_))\nprint('Normalized Coefficient: {}'.format(lr.coef_ / lr.coef_.sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_stacking_lr = lr.predict(np.vstack((pred_lr, pred_lgb)).T).clip(0,20)\ntest_preds_stacking_lr.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calc RMSE an R2 for this Stack Model - level 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_lr_stack = get_rmse(y_test, test_preds_stacking_lr)\nprint(\"rmse: \", rmse_lr_stack)\nprint(\"r2 train: \", r2_score(y_train_level2, lr.predict(X_train_level2).clip(0,20)))\nprint(\"r2 test: \", r2_score(y_test, test_preds_stacking_lr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Serialize"},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'stack_model_lr_l2.sav'\njoblib.dump(lr, filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** END OF VALIDATION MODEL ***"},{"metadata":{},"cell_type":"markdown","source":"Continue with \"Final Project CUribe.co Part 2/2\" kernel \n\nhttps://www.kaggle.com/curibe10/final-project-curibe-co-part-2-2"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}