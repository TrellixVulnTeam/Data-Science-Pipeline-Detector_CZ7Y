{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Jul 14 21:33:08 2021\n\n@author: yingyingliu\n\"\"\"\n\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nfrom itertools import product\nimport lightgbm as lgb\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\n\nsns.set(style=\"darkgrid\")\n\n%matplotlib inline \n\npd.set_option('display.max_rows', 600)\npd.set_option('display.max_columns', 50)\n\n# import lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom tqdm import tqdm_notebook\n\nfrom itertools import product\n\ndef downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df\n\n# Data Loading \npath = '../input/competitive-data-science-predict-future-sales/'\n\n#../input/competitive-data-science-predict-future-sales/item_categories.csv\ndf_train = pd.read_csv(path + 'sales_train.csv')\ndf_train.head()\ndf_test = pd.read_csv(path + 'test.csv')\n# df_test.head()\ndf_shops = pd.read_csv(path + 'shops.csv')\ndf_shops.head(15)\ndf_items = pd.read_csv(path + 'items.csv')\n# df_items.head()\ndf_items_categories = pd.read_csv(path + 'item_categories.csv')\n# df_items_categories.head()\ndf_submission = pd.read_csv(path + 'sample_submission.csv')\n# df_submission.head()\n\n# Number of NaNs for each columns\nprint('df_train shape', df_train.shape)\nprint('df_shops shape', df_train.shape)\ndf_shops.isnull().sum(axis=0).head()\ndf_train.isnull().sum(axis=0).head(15)\n\n\n# Delete outlier from df_train\ndf_train.describe()\n\nsns.boxplot(x = df_train.item_cnt_day)  # 1,000\nsns.boxplot(x = df_train.item_price)    # 100,000\n\ndf_train = df_train[(df_train.item_price < 100000) & (df_train.item_price > 0)]\ntrain = df_train[df_train.item_cnt_day < 1001]\n\n\n# detect same shop_names with different shop_ids\nprint(df_shops[df_shops.shop_id.isin([0, 57])]['shop_name'])  # 57 = 0\nprint(df_shops[df_shops.shop_id.isin([1, 58])]['shop_name'])\nprint(df_shops[df_shops.shop_id.isin([39, 40])]['shop_name'])\nprint(df_shops[df_shops.shop_id.isin([10, 11])]['shop_name'])\n\n\n# data cleaning #\n''' train '''\ndf_shops['shop_id'].nunique()\n\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ndf_test.loc[df_test.shop_id == 0, 'shop_id'] = 57\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ndf_test.loc[df_test.shop_id == 1, 'shop_id'] = 58\ntrain.loc[train.shop_id == 40, 'shop_id'] = 39\ndf_test.loc[df_test.shop_id == 40, 'shop_id'] = 39\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ndf_test.loc[df_test.shop_id == 10, 'shop_id'] = 11\n\n# train[\"revenue\"] = train[\"item_cnt_day\"] * train[\"item_price\"]\n\nu_df_test_id = df_test['shop_id'].unique()\ntrain = train[train['shop_id'].isin(u_df_test_id)]\ntrain['shop_id'].nunique()\n\n''' df_shops '''\ndf_shops[\"city\"] = df_shops.shop_name.str.split(\" \").map(lambda x: x[0])\ndf_shops[\"city\"]\ndf_shops.loc[df_shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\ndf_shops['category'] = df_shops.shop_name.str.split(\" \").map(lambda x: x[1])  # df_shop['shop_name'][1] = category\ndf_shops.head()\ndf_shops['category']\n\n# Only keep shop category if there are 5 or more shops of that category\ncategory = []    \nfor cat in df_shops.category.unique():\n    if len(df_shops[df_shops.category == cat]) >= 5:\n           category.append(cat)\ndf_shops.category = df_shops.category.apply(lambda x: x if (x in category) \n                                            else \"others\")\n\n\nlabel_encoder = LabelEncoder()\ndf_shops['city'] = label_encoder.fit_transform(df_shops['city'])\ndf_shops = df_shops.drop('shop_name', axis = 1)\ndf_shops['category'] = label_encoder.fit_transform(df_shops['category'])     \nshops = df_shops\nshops.head()\n\n\n'''df_items_categories'''\ndf_items_categories.head()\ndf_items_categories['type'] = df_items_categories['item_category_name'].apply(lambda x: x.split()[0]).astype(str)\ndf_items_categories['type'].value_counts()\ndf_items_categories.head()\n\ncategory = []\nfor cat in df_items_categories['type'].unique():\n    if len(df_items_categories[df_items_categories.type == cat]) >= 5:\n        category.append(cat)\ndf_items_categories.type = df_items_categories.type.apply(lambda x: x if (x in category) else 'etc')\n\ndf_items_categories.type = LabelEncoder().fit_transform(df_items_categories.type)\ndf_items_categories.head()\n\ncats = df_items_categories[['type', 'item_category_id']]\ncats.head()\n\n\n'''items'''\ndf_items.head()\ndf_items = df_items.drop('item_name', axis = 1)\n# Create the date the product was first sold as a feature\ndf_items['first_sale'] = train.groupby('item_id').agg({'date_block_num': 'min'})['date_block_num']\ndf_items[df_items['first_sale'].isna()]\ndf_items['first_sale'] = df_items['first_sale'].fillna(34)\ndf_items.head()\n# items = pd.merge(cats, df_items, on = 'item_category_id')\n# items = items.drop('item_category_name', axis = 1)\n\n\n\n# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n# For every month we create a grid from all shops/items combinations from that month\n\n#### import itertools from product\ngrid = []\nfor block_num in train['date_block_num'].unique():\n    cur_shops = train.loc[train['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = train.loc[train['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])), dtype = 'int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n\n# groupby('index_cols')? \ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': 'sum'})\ngroup = group.reset_index()\ngroup = group.rename(columns = {'item_cnt_day': 'item_cnt_month'})\n\nall_data = pd.merge(grid, group, on = index_cols, how = 'left').fillna(0)\n# all_data.avg_shop_price = all_data.avg_shop_price.fillna(0).astype(np.float16)\n\n\n\n# concate test data with training data which has been cleaned before\ndf_test.head()\ndf_test['date_block_num'] = 34\ndf_test['date_block_num'] = df_test['date_block_num'].astype(np.int8)\nall_data = pd.concat([all_data, df_test.drop('ID', axis = 1)],\n                     ignore_index = True,\n                     keys = index_cols)\n# Replace NaN with 0\nall_data = all_data.fillna(0)\n\n\n# concatenate shop, item, etc data\nall_data = pd.merge(all_data, shops, on = 'shop_id', how = 'left')\nall_data = pd.merge(all_data, df_items, on = 'item_id', how = 'left')\nall_data = pd.merge(all_data, cats, on = 'item_category_id', how = 'left')\n\n# feature summary\ndef resumetable(df):\n    print(f'Data Shape: {df.shape}')\n    summary = pd.DataFrame(df.dtypes, columns=['Dtypes'])\n    summary['Null'] = df.isnull().sum().values\n    summary['Uniques'] = df.nunique().values\n    summary['First_values'] = df.loc[0].values\n    summary['Second_values'] = df.loc[1].values\n    summary['Third_values'] = df.loc[2].values\n    \n    return summary\n\nresumetable(all_data)\n\n\n# Basic comprehension of cnt\nfigure, ax= plt.subplots() \nfigure.set_size_inches(11, 5)\n# Total item sales/ shop_id\ngroup_shop_sum = all_data.groupby('shop_id').agg({'item_cnt_month': 'sum'})\ngroup_shop_sum = group_shop_sum.reset_index()\n\ngroup_shop_sum = group_shop_sum[group_shop_sum['item_cnt_month'] > 10000]\n\nsns.barplot(x='shop_id', y='item_cnt_month', data=group_shop_sum)\nax.set(title='Distribution of total item counts by shop id',\n       xlabel='Shop ID', \n       ylabel='Total item counts')\nax.tick_params(axis='x', labelrotation=90)\n\n\n\n\n# Target Lags\ndef lag_feature( df,lags, cols ):\n    for col in cols:\n        print(col)\n        tmp = df[[\"date_block_num\", \"shop_id\", \"item_id\", col ]]\n        for i in lags:\n            shifted = tmp.copy()\n            shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\" + str(i)]\n            shifted.date_block_num = shifted.date_block_num + i\n            df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df\n\n\n # Add lag-'item_cnt_month' \n## all_data = lag_feature(all_data, [1,2,3], [\"avg_shop_price\"])   # Add lag-'avg_shop_price'\n## all_data = lag_feature(all_data, [1,2,3], ['avg_item_price'])   # Add lag-'avg_item_price'\nall_data = lag_feature(all_data, [1,2,3], [\"item_cnt_month\"])\n\n# mean encoder based on n_splits\n# previous month's avg_items\ngroup = all_data.groupby([\"date_block_num\"]).agg({\"item_cnt_month\": [\"mean\"]})\ngroup.columns = [\"date_avg_item\"]\ngroup.reset_index(inplace = True)\n\nall_data =pd.merge(all_data, group, on = ['date_block_num'], how = 'left')\nall_data.data_avg_item = all_data[\"date_avg_item\"].astype(np.float16)\n\nall_data = lag_feature(all_data, [1], [\"date_avg_item\"])\nall_data.drop(['date_avg_item'], axis = 1, inplace = True) # lag = 1 bacause of correlation\n\n\n# Add lag features for avg_item_month\ngroup = all_data.groupby([\"date_block_num\", \"item_id\"]).agg({\"item_cnt_month\": [\"mean\"]})\ngroup.columns = [\"avg_item_cnt_month\"]\ngroup.reset_index(inplace = True)\n\nall_data = pd.merge(all_data, group, on = ['date_block_num', 'item_id'], how = 'left')\nall_data[\"avg_item_cnt_month\"] = all_data[\"avg_item_cnt_month\"].astype(np.float16)\n\nall_data = lag_feature(all_data, [1,2,3], [\"avg_item_cnt_month\"])\nall_data.drop([\"avg_item_cnt_month\"], axis = 1, inplace = True)\n\n\n\n\n\n\n# add lag features for month&shop\ngroup = all_data.groupby([\"date_block_num\", \"shop_id\"]).agg({\"item_cnt_month\": [\"mean\"]})\ngroup.columns = [\"avg_date_shop_item\"]\ngroup.reset_index(inplace = True)\n\nall_data = pd.merge(all_data, group, on = ['date_block_num', 'shop_id'], how = 'left')\nall_data[\"avg_date_shop_item\"] = all_data[\"avg_date_shop_item\"].astype(np.float16)\n\nall_data = lag_feature(all_data, [1,2,3], [\"avg_date_shop_item\"])\nall_data.drop([\"avg_date_shop_item\"], axis = 1, inplace = True)\n\n\n\n\n\n\n# Add lag features for avg_item_month_cnt\ngroup = all_data.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg({\"item_cnt_month\": [\"mean\"]})\ngroup.columns = [\"date_shop_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nall_data = pd.merge(all_data, group, on = [\"date_block_num\", \"shop_id\", \"item_id\"], how = 'left')\nall_data[\"date_shop_avg_item_cnt\"] = all_data[\"date_shop_avg_item_cnt\"].astype(np.float16)\n\nall_data = lag_feature(all_data, [1], [\"date_shop_avg_item_cnt\"])\nall_data.drop([\"date_shop_avg_item_cnt\"], axis = 1, inplace = True)\n\n\n# avg_date_shop_item\ngroup = all_data.groupby([\"date_block_num\", \"city\"]).agg({\"item_cnt_month\": [\"mean\"]})\ngroup.columns = [\"date_city_avg_item\"]\ngroup.reset_index(inplace = True)\n\nall_data = pd.merge(all_data, group, on = ['date_block_num', 'city'], how = 'left')\nall_data[\"date_city_avg_item\"] = all_data[\"date_city_avg_item\"].astype(np.float16)\n\nall_data = lag_feature(all_data, [1], [\"date_city_avg_item\"])\nall_data.drop([\"date_city_avg_item\"], axis = 1, inplace = True)\n\n\n\n# revenue\nall_data[\"revenue\"] = train[\"item_cnt_day\"]*train[\"item_price\"]\n\ngroup = all_data.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nall_data = pd.merge(all_data, group, on=['date_block_num','shop_id'], how='left')\nall_data['date_shop_revenue'] = all_data['date_shop_revenue'].astype(np.float32)\n\n### shop_avg_revenue\ngroup = all_data.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nall_data = pd.merge(all_data, group, on=['shop_id'], how='left')\nall_data['shop_avg_revenue'] = all_data['shop_avg_revenue'].astype(np.float32)\n\n\n\n### delta_revenue denotes the difference between paticular avg_revenue and avg(all_shop_avg_revenue)\nall_data['delta_revenue'] = (all_data['date_shop_revenue'] - all_data['shop_avg_revenue']) / all_data['shop_avg_revenue']\nall_data['delta_revenue'] = all_data['delta_revenue'].astype(np.float32)\n\nall_data = lag_feature(all_data, [1], ['delta_revenue'])\nall_data.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\n\n\n### item based trends ###\ngroup = train.groupby( [\"item_id\"] ).agg({\"item_price\": [\"mean\"]})\ngroup.columns = [\"item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nall_data = all_data.merge( group, on = [\"item_id\"], how = \"left\" )\nall_data[\"item_avg_item_price\"] = all_data.item_avg_item_price.astype(np.float32)\n\n\ngroup = train.groupby( [\"date_block_num\",\"item_id\"] ).agg( {\"item_price\": [\"mean\"]} )\ngroup.columns = [\"date_item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nall_data = all_data.merge(group, on = [\"date_block_num\",\"item_id\"], how = \"left\")\nall_data[\"date_item_avg_item_price\"] = all_data.date_item_avg_item_price.astype(np.float32)\n\nlags = [1, 2, 3]\nall_data = lag_feature(all_data, lags, [\"date_item_avg_item_price\"] )\nfor i in lags:\n    all_data[\"delta_price_lag_\" + str(i) ] = (all_data[\"date_item_avg_item_price_lag_\" + \n                                                       str(i)]- all_data[\"item_avg_item_price\"] )/ all_data[\"item_avg_item_price\"]\n\n\ndef select_trends(row) :\n    for i in lags:\n        if row[\"delta_price_lag_\" + str(i)]:\n            return row[\"delta_price_lag_\" + str(i)]\n    return 0\n\n\nall_data[\"delta_price_lag\"] = all_data.apply(select_trends, axis = 1)\nall_data[\"delta_price_lag\"] = all_data.delta_price_lag.astype( np.float32)\nall_data[\"delta_price_lag\"].fillna(0 ,inplace = True)\n\nfeatures_to_drop = [\"item_avg_item_price\", \"date_item_avg_item_price\"]\nfor i in lags:\n    features_to_drop.append(\"date_item_avg_item_price_lag_\" + str(i) )\n    features_to_drop.append(\"delta_price_lag_\" + str(i) )\nall_data.drop(features_to_drop, axis = 1, inplace = True)\n\n\n# Special Feature\nall_data.month = all_data[\"date_block_num\"] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nall_data['days'] = all_data.month.map(days).astype(np.int8)\n\n\nall_data = all_data[all_data[\"date_block_num\"] > 3] # no lag_variables for first three months\nall_data.head().T\n\n# fill NA brought by lag_variables generation\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nall_data = fill_na(all_data)\n\nall_data.columns\nall_data.shape\nall_data.info\nall_data.describe\n\n\n# all_data = preprocessing.scale(all_data)   # no need to normalize data\n# all_data = pd.DataFrame(all_data)\n# all_data.describe\n\n# Training data \nX_train = all_data[all_data['date_block_num'] < 33].drop([\"item_cnt_month\"], axis = 1)\nY_train = all_data[all_data.date_block_num < 33]['item_cnt_month']\n# Validation\nX_valid = all_data[all_data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = all_data[all_data.date_block_num == 33]['item_cnt_month']\n\nX_test = all_data[all_data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\n\n\n# Xgboost\nmodel = XGBRegressor(\n    max_depth=6,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\nmodel.fit\n\n\nY_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": Y_test, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\n\nplot_features(model, (10,14))\n\n\n# training's rmse: 2.22276\tvalid_1's rmse: 1.67866","metadata":{"execution":{"iopub.status.busy":"2021-12-06T20:55:34.265293Z","iopub.execute_input":"2021-12-06T20:55:34.265588Z","iopub.status.idle":"2021-12-06T21:04:13.373938Z","shell.execute_reply.started":"2021-12-06T20:55:34.265558Z","shell.execute_reply":"2021-12-06T21:04:13.373006Z"},"trusted":true},"execution_count":null,"outputs":[]}]}