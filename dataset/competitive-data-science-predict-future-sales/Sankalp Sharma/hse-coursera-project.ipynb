{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport gc\nfrom tqdm import tqdm_notebook\nfrom itertools import product\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.svm import LinearSVR\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import AdaBoostRegressor,GradientBoostingRegressor,RandomForestRegressor,VotingRegressor,StackingRegressor\nfrom sklearn.model_selection import KFold,train_test_split\nimport lightgbm as lgb\nimport tensorflow as tf\nimport keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data=pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data=pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape,test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data['shop_id'].nunique(),test_data['shop_id'].nunique())\nl=train_data['shop_id'].unique()\nfor i in test_data['shop_id'].unique():\n    if i not in l:\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All shop_id for test data are present in training data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data['item_id'].nunique(),test_data['item_id'].nunique())\nl=train_data['item_id'].unique()\nc=0\nfor i in test_data['item_id'].unique():\n    if i not in l:\n        c+=1\nprint('test item id not in train',c)\nl=test_data['item_id'].unique()\nc=0\nfor i in train_data['item_id'].unique():\n    if i not in l:\n        c+=1\nprint('train item id not in test',c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Out of 5100 item_id in test data, 363 are not present in training data. We need to look at items.csv and item_categories.csv files.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items=pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nprint(items.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items['item_id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_dict={}\nfor i in range(items.shape[0]):\n    itid=items.loc[i,'item_id']\n    itcat=items.loc[i,'item_category_id']\n    item_dict[itid]=itcat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Downcasting to 16 bit integer\ntrain_data['item_cnt_day'] = train_data['item_cnt_day'].astype('int16')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do not have null values in data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['date_block_num'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['item_cnt_day'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(train_data['item_cnt_day'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that one item has -9 as item count which is clearly wrong. We will deal with it later.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['item_price'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(train_data['item_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see one item has negative price. We will replace it with its median value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,9))\nplt.subplots_adjust(hspace=.5)\n\nplt.subplot2grid((3,3), (0,0), colspan = 3)\ntrain_data['shop_id'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Shop ID Values in the Training Set (Normalized)')\n\nplt.subplot2grid((3,3), (1,0))\ntrain_data['item_id'].plot(kind='hist', alpha=0.7)\nplt.title('Item ID Histogram')\n\nplt.subplot2grid((3,3), (1,1))\ntrain_data['item_price'].plot(kind='hist', alpha=0.7, color='orange')\nplt.title('Item Price Histogram')\n\nplt.subplot2grid((3,3), (1,2))\ntrain_data['item_cnt_day'].plot(kind='hist', alpha=0.7, color='green')\nplt.title('Item Count Day Histogram')\n\nplt.subplot2grid((3,3), (2,0), colspan = 3)\ntrain_data['date_block_num'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Month (date_block_num) Values in the Training Set (Normalized)')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.percentile(train_data['item_cnt_day'],99.99))\nprint(np.percentile(train_data['item_price'],99.99))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting rid of item_cnt_day values above 100 and item_price above 30000 as they are outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data[train_data['item_cnt_day'] < 100]\ntrain_data = train_data[train_data['item_price'] < 30000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data['item_price']<0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will replace it with median.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"median = train_data[(train_data.shop_id==32)&(train_data.item_id==2973)&(train_data.date_block_num==4)&(train_data.item_price>0)].item_price.median()\ntrain_data.loc[train_data.item_price<0, 'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data['item_cnt_day']<0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These denote item returned, so we do not need to change these.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Some shops are duplicates (according to name), so fixing them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Якутск Орджоникидзе, 56\ntrain_data.loc[train_data.shop_id == 0, 'shop_id'] = 57\ntest_data.loc[test_data.shop_id == 0, 'shop_id'] = 57\n# Якутск ТЦ \"Центральный\"\ntrain_data.loc[train_data.shop_id == 1, 'shop_id'] = 58\ntest_data.loc[test_data.shop_id == 1, 'shop_id'] = 58\n# Жуковский ул. Чкалова 39м²\ntrain_data.loc[train_data.shop_id == 10, 'shop_id'] = 11\ntest_data.loc[test_data.shop_id == 10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Generation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories=pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\nshops=pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some Observations**\n\n* Each shop_name starts with the city name.\n* Each category contains type and subtype in its name.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\nitem_categories['split'] = item_categories['item_category_name'].str.split('-')\nitem_categories['type'] = item_categories['split'].map(lambda x: x[0].strip())\nitem_categories['type_code'] = LabelEncoder().fit_transform(item_categories['type'])\n# if subtype is nan then type\nitem_categories['subtype'] = item_categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitem_categories['subtype_code'] = LabelEncoder().fit_transform(item_categories['subtype'])\nitem_categories = item_categories[['item_category_id','type_code', 'subtype_code']]\n\nitems.drop(['item_name'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Aggregating data and getting monthly features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will also try to make test data similar to train data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train_data[train_data.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time()-ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['revenue'] = train_data['item_price'] *  train_data['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Monthly item count and revenue","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = train_data.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Appending test data to matrix to make test set similar to training set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['date_block_num'] = 34\ntest_data['date_block_num'] = test_data['date_block_num'].astype(np.int8)\ntest_data['shop_id'] = test_data['shop_id'].astype(np.int8)\ntest_data['item_id'] = test_data['item_id'].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = pd.concat([matrix, test_data], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding generated features from shops, items and categories.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, item_categories, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lag feature generation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean Encoded feature generation along with lag","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_type_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'city_code'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\nmatrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\nmatrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Month feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix['month'] = matrix['date_block_num'] % 12","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Since, maximum lag value is 12, we cannot use features for month<12. So, we will drop them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = matrix[matrix.date_block_num > 11]\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacement of null values produced during lag feature generation\nWe simply replace them with 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving the final data for training and freeing up the memory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.to_pickle('data.pkl')\ndel matrix\ndel group\ndel items\ndel shops\ndel item_categories\ndel train_data\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_pickle('data.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Separating data into train and test sets and clipping target variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del data\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_train.values, y_train)\npred_lr = lr.predict(X_valid.values).clip(0,20)\ntest_lr = lr.predict(X_test.values).clip(0,20)\nprint('Test R-squared for linreg is %f' % r2_score(y_valid, pred_lr))\nprint('Test rmse for linreg is %f' % mean_squared_error(y_valid, pred_lr,squared=False))\nnp.save('lr.npy',pred_lr)\nnp.save('lr_test.npy',test_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }\n\nmodel = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\npred_lgb = model.predict(X_valid).clip(0,20)\ntest_lgb = model.predict(X_test).clip(0,20)\nprint('Test R-squared for LightGBM is %f' % r2_score(y_valid, pred_lgb))\nprint('Test rmse for LightGBM is %f' % mean_squared_error(y_valid, pred_lgb,squared=False))\nnp.save('lgb.npy',pred_lgb)\nnp.save('lgb_test.npy',test_lgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = LinearSVR(verbose=1)\nsvc.fit(X_train, y_train)\npred_svc = svc.predict(X_valid).clip(0,20)\ntest_svc = svc.predict(X_test).clip(0,20)\nprint('Test R-squared for xgboost is %f' % r2_score(y_valid, pred_svc))\nprint('Test rmse for xgboost is %f' % mean_squared_error(y_valid, pred_svc,squared=False))\nnp.save('svc.npy',pred_svc)\nnp.save('svc_test.npy',test_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = RidgeCV()\nridge.fit(X_train, y_train)\npred_ridge = ridge.predict(X_valid).clip(0,20)\ntest_ridge = ridge.predict(X_test).clip(0,20)\nprint('Test R-squared for xgboost is %f' % r2_score(y_valid, pred_ridge))\nprint('Test rmse for xgboost is %f' % mean_squared_error(y_valid, pred_ridge,squared=False))\nnp.save('ridge.npy',pred_ridge)\nnp.save('ridge_test.npy',test_ridge)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBRegressor(max_depth=8,n_estimators=20,min_child_weight=300, colsample_bytree=0.8, subsample=0.8, eta=0.3)\nxgb.fit(X_train, y_train, eval_metric=\"rmse\", eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=True)\npred_xgb = xgb.predict(X_valid).clip(0,20)\ntest_xgb = xgb.predict(X_test).clip(0,20)\nprint('Test R-squared for xgboost is %f' % r2_score(y_valid, pred_xgb))\nprint('Test rmse for xgboost is %f' % mean_squared_error(y_valid, pred_xgb,squared=False))\nnp.save('xgb.npy',pred_xgb)\nnp.save('xgb_test.npy',test_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grad = GradientBoostingRegressor(criterion='mse',n_estimators=20,subsample=0.8,min_samples_split=100,max_depth=8,verbose=2)\ngrad.fit(X_train, y_train)\npred_grad = grad.predict(X_valid).clip(0,20)\ntest_grad = grad.predict(X_test).clip(0,20)\nprint('Test R-squared for gardient boost is %f' % r2_score(y_valid, pred_grad))\nprint('Test rmse for gradient boost is %f' % mean_squared_error(y_valid, pred_grad,squared=False))\nnp.save('grad.npy',pred_grad)\nnp.save('grad_test.npy',test_grad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand = RandomForestRegressor(n_estimators=50,min_samples_split=20,max_depth=8,verbose=2)\nrand.fit(X_train, y_train)\npred_rand = rand.predict(X_valid).clip(0,20)\ntest_rand = rand.predict(X_test).clip(0,20)\nprint('Test R-squared for random forest is %f' % r2_score(y_valid, pred_rand))\nprint('Test rmse for random forest is %f' % mean_squared_error(y_valid, pred_rand,squared=False))\nnp.save('rand.npy',pred_rand)\nnp.save('rand_test.npy',test_rand)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_model=Sequential()\nnn_model.add(Dense(1024,activation='relu'))\nnn_model.add(Dense(1024,activation='relu'))\nnn_model.add(Dense(256,activation='relu'))\nnn_model.add(Dense(256,activation='relu'))\nnn_model.add(Dense(1,activation='relu'))\nnn_model.compile(optimizer=Adam(lr=0.0000001),loss='mean_squared_error',metrics=['mean_squared_error'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_model.fit(X_train,y_train,epochs=2,batch_size=512,validation_data=(X_valid,y_valid))\npred_nn=nn_model.predict(X_valid).clip(0,20)\ntest_nn=nn_model.predict(X_test).clip(0,20)\nprint('Test R-squared for neural network is %f' % r2_score(y_valid, pred_nn))\nprint('Test rmse for neural network is %f' % mean_squared_error(y_valid, pred_nn,squared=False))\nnp.save('nn.npy',pred_nn)\nnp.save('nn_test.npy',test_nn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensembling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Loading data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lr=np.load('/kaggle/input/predictions/lr.npy')\npred_lgb=np.load('/kaggle/input/predictions/lgb.npy')\npred_svc=np.load('/kaggle/input/predictions/svc.npy')\npred_ridge=np.load('/kaggle/input/predictions/ridge.npy')\npred_xgb=np.load('/kaggle/input/predictions/xgb.npy')\npred_grad=np.load('/kaggle/input/predictions/grad.npy')\npred_rand=np.load('/kaggle/input/predictions/rand.npy')\npred_nn=np.load('/kaggle/input/predictions/nn.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_lr=np.load('/kaggle/input/predictions/lr_test.npy')\ntest_lgb=np.load('/kaggle/input/predictions/lgb_test.npy')\ntest_svc=np.load('/kaggle/input/predictions/svc_test.npy')\ntest_ridge=np.load('/kaggle/input/predictions/ridge_test.npy')\ntest_xgb=np.load('/kaggle/input/predictions/xgb_test.npy')\ntest_grad=np.load('/kaggle/input/predictions/grad_test.npy')\ntest_rand=np.load('/kaggle/input/predictions/rand_test.npy')\ntest_nn=np.load('/kaggle/input/predictions/nn_test.npy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating level 2 features. We will ensemble them using xgboost on validation set from above data with K-Fold validation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\ndata_level2=np.c_[pred_lr,pred_lgb,pred_svc,pred_ridge,pred_xgb,pred_grad,pred_rand,pred_nn]\ntest_level2=np.c_[test_lr,test_lgb,test_svc,test_ridge,test_xgb,test_grad,test_rand,test_nn]\ny_valid=np.array(y_valid)\nprint(data_level2.shape,y_valid.shape)\nprint(test_level2.shape,X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5)\ni=0\ntest=np.zeros(test_lgb.shape)\nfor train_index, val_index in kf.split(data_level2,y_valid):\n    X_train, X_val = data_level2[train_index], data_level2[val_index]\n    y_train, y_val = y_valid[train_index], y_valid[val_index]\n    lr = XGBRegressor(max_depth=5,n_estimators=50,min_child_weight=50, colsample_bytree=0.8, subsample=0.8, eta=0.3)\n    lr.fit(X_train, y_train,eval_metric=\"rmse\",eval_set=[(X_train, y_train), (X_val, y_val)], verbose=True)\n    val = lr.predict(X_val).clip(0,20)\n    test_lr = lr.predict(test_level2).clip(0,20)\n    i+=1\n    print('Iteration = %f' % i)\n    print('Test R-squared for linreg is %f' % r2_score(y_val, val))\n    print('Test rmse for linreg is %f' % mean_squared_error(y_val, val,squared=False))\ntest=test/5\nnp.save('test.npy',test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n    \"ID\": test_data.index, \n    \"item_cnt_month\": test\n})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}