{"cells":[{"metadata":{"_cell_guid":"9d6a6163-6545-4d07-9caa-9e1c972db380","_uuid":"91f6e9e76f634c5cea62b652ca20232a8beb71cb"},"cell_type":"markdown","source":"# Table of Contents\n___\n\n-[Introduction](#Introduction)\n\n-[Data Exploration](#Data-Exploration)\n\n-[Understanding russian using googletrans](#Understanding-russian-using-googletrans)\n\n-[Time series analysis](#Time-series-analysis)\n\n-[Sliding Window Method](#Sliding-Window-Method)\n   \n-[Conclusions](#Conclusions)"},{"metadata":{"_cell_guid":"7ce411e7-c484-4e5e-8c5b-61bf259558da","_uuid":"5716007ae83a0fb0f7e65096cdab517bbf203842"},"cell_type":"markdown","source":"## Introduction\n___\n\nIn this kernel, is to make an EDA (exploration data analysis), focusing on how can we get information after using `googletrans` library to translate all the russian words in this dataset. The translation is done two times: one with a smaller dataframe and another with a larger dataframe (the only difference is the time of execution). For the larger, there are a few additions to try to optimize reduce the translation time. We'll also see a time series decomposition using the `statsmodel` library. At last, we will see how to reshape a time series forecasting problem into a supervised learning prediction one using a method called Sliding Window. In particular, we will make use of linear regression model.\n\nFirst we import the libraries and get each dataset."},{"metadata":{"_cell_guid":"06b5f7f4-4df9-4e6e-bb6b-3894296f470d","collapsed":true,"_uuid":"facbbd93160e5b5c6861c9908dcb84921c5e48a8","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n%matplotlib inline \n#shows plot from matplotlib and seaborn in the Jupyter notebook","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cc863de3-40ea-4ee5-a1ab-ee4877ee64c1","_uuid":"bf2564bb1a14f77a7be25ab7152a7766850a2567","trusted":false,"collapsed":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\nitems = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nitem_categories = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"390b6276-2f6f-44f3-b5d8-f454b2dda1f8","_uuid":"13f96ea9a98f4cb57e437ac16ba010376a51a7be"},"cell_type":"markdown","source":"The main dataset doesn't tell us much, but we can marge this dataset with the other two, so we get more information."},{"metadata":{"_cell_guid":"fa6fed53-9e56-4b84-bdaa-41071bd5c072","_uuid":"0d044a96b6828a02a93ffa31529d4d91357ffa56","trusted":false,"collapsed":true},"cell_type":"code","source":"df = pd.merge(df, items, on=\"item_id\")\n\ndf = pd.merge(df, item_categories, on=\"item_category_id\")\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"08c5afa3-242b-4dd9-b37d-db5286e9539f","_uuid":"fa957ea95386cb719c9281cb61c5d9f23db16428"},"cell_type":"markdown","source":"Well, now it's much better! Some information is written in russian, so we will have to deal with it later. For now, let's see how the sales, that we should get from item_price, change as the time passes.\n\n## Data Exploration\n___\n\nThere is a catch in this dataset. The days are written in the form day-month-year, rather than the usual month-day-year very common in english-written datasets. (Thanks @anatoly for calling my attention to that!) We now group by date and sum over the `item_price` and extract day, month and year from the `date` column."},{"metadata":{"_cell_guid":"8772bb84-0b93-4a6c-92f7-a3e85947bacd","_uuid":"9c97dfcbe7a367405e6c488a6643ead598ddb12e","trusted":false,"collapsed":true},"cell_type":"code","source":"revenueByDate = pd.DataFrame(df.groupby('date', as_index=False)['item_price'].sum())\nrevenueByDate[\"day\"] = revenueByDate.date.str.extract(\"([0-9][0-9]).\", expand = False)\nrevenueByDate[\"month\"] = revenueByDate.date.str.extract(\".([0-9][0-9]).\", expand = False)\nrevenueByDate[\"year\"] = revenueByDate.date.str.extract(\".([0-9][0-9][0-9][0-9])\", expand =False)\nrevenueByDate.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"90174b7b-6a4e-4c50-8c64-9ae6a3c56749","_uuid":"5c318df73b3c2f524c4b3b37b1254f67e4d8db99","trusted":false,"collapsed":true},"cell_type":"code","source":"revenueByDate[\"date\"] = pd.to_datetime(revenueByDate[[\"year\", \"month\", \"day\"]])\n\nplt.plot( \"date\", \"item_price\", data = revenueByDate.sort_values(by=\"date\"))\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"575ccb65-e0b5-4334-a1bd-742b45e28d17","_uuid":"9cff36e7cbfe3cb4d0cc1fd7633759b48cb9411a"},"cell_type":"markdown","source":"Not as enlightening as I expected! The data is too noisy in this plot, as it accounts for the variation of each day's sales. It may be good if we make this plot monthly rather than daily, so we'll be able to see a less noisy graph and maybe get some intuition about our series.\n\nBut first, we will have to create a new date column so we will have only months and years."},{"metadata":{"_cell_guid":"7e94e793-b61f-4601-8295-5a3954cffbfb","_uuid":"4116f3ae707033e2a0cc7fd425ae1f508c0746a6","trusted":false,"collapsed":true},"cell_type":"code","source":"revenueByDate[\"day\"] = 1 #rewrite this column using ones so I can use the DatetimeIndex and aggregate sales by month\nrevenueByDate['monthlyDate'] = pd.to_datetime(revenueByDate[[\"year\", \"month\", \"day\"]])\n\nrevenueByDate.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d3b9de44-d232-4152-9736-19bb4f09b911","_uuid":"e00edd849bba47c75dbf79be07c6bc9294c5f2f1","trusted":false,"collapsed":true},"cell_type":"code","source":"revenueByMonth = pd.DataFrame(revenueByDate.groupby(\"monthlyDate\", as_index=False)[\"item_price\"].sum())\nrevenueByMonth.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"17867016-00dc-443b-8856-0c07c614e9a9","_uuid":"f71609dfb00bc929d01e93cefae10ce8f7e9814f"},"cell_type":"markdown","source":"Great! Now we have the total sales for each month. What does this series looks like?"},{"metadata":{"_cell_guid":"49bbc245-dba9-435b-a047-6f4f738e8643","_uuid":"9da7efaa601eb0f0bd01fc28074288d3685f438f","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.plot(\"monthlyDate\", \"item_price\", data = revenueByMonth.sort_values(by=\"monthlyDate\"))\nplt.xticks(rotation = 90);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"995671d5-b168-41e5-83d3-cf50f7ff8abb","_uuid":"000a284ad8c02d5c06fe395212f91d5537dad51d"},"cell_type":"markdown","source":"Wow! Now it is much more clear how the sales behaved during all the period of records. We can see two peaks in sales, one in Dec 2013 and another in Dec 2014. Those peaks can be confirmed using `nlargest` method as indicated below."},{"metadata":{"_cell_guid":"ef52c725-760c-4516-bc4c-9f0de338746e","_uuid":"57bc3d98ca4f93522992c4fb7b2e0538e30ed163","trusted":false,"collapsed":true},"cell_type":"code","source":"revenueByMonth[revenueByMonth[\"item_price\"] == revenueByMonth[\"item_price\"].max()]\n\n#gets the 2 highest item prices and their respective row index.\nrevenueByMonth[\"item_price\"].nlargest(2)\n\nrevenueByMonth.iloc[[11,23],:]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5daffd6a-6252-43bd-a35f-35674c2d7d1a","_uuid":"493dd2124f529f6861d4cbfdcec155360107ef98"},"cell_type":"markdown","source":"We've just seen how sales behaved through the years, but how do monthly revenue looks like? "},{"metadata":{"_cell_guid":"a47d85cd-a019-4cc0-aa36-5e8a6d0be84b","_uuid":"26ae5740990a95d0a8966f986943a4a270cc647f","trusted":false,"collapsed":true},"cell_type":"code","source":"monthlyRev = pd.DataFrame(revenueByDate.groupby([\"month\", \"year\"], as_index=False)[\"item_price\"].sum())\nmonthlyRev.head()\n\n\ng = sns.FacetGrid(data = monthlyRev.sort_values(by=\"month\"), hue = \"year\", size = 5, legend_out=True)\ng = g.map(plt.plot, \"month\", \"item_price\")\ng.add_legend()\ng;","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"882d496b-541a-4ff7-9032-f3c82848eb7a","_uuid":"b4448ec02666196a76e387af73791fc5bdc3f539"},"cell_type":"markdown","source":"2014's sales were higher than 2013's in all months, but as we've seen before, 2015 wasn't a good year for this store. Sales went lower than the 2 previous years just after January.\n\nBefore proceeding in our analysis, we should change the data type of some columns."},{"metadata":{"_cell_guid":"4444ae7f-a85c-498e-8c59-e807712cf758","_uuid":"c31faec6feb402703451618c5856eee11def8f68","trusted":false,"collapsed":true},"cell_type":"code","source":"df[\"item_id\"] = df[\"item_id\"].astype(\"category\")\ndf[\"item_name\"] = df[\"item_id\"].astype(\"category\")\ndf[\"item_category_id\"] = df[\"item_category_id\"].astype(\"category\")\ndf[\"item_category_name\"] = df[\"item_category_name\"].astype(\"str\")\n#apparently, if item_category_name is set as category, we cannot use googletrans in the next section\n\n#a nice way to visualize the content of a dataset and also its shape. It is built to be similar to str function in R.\ndef rstr(df): \n    return df.shape, df.apply(lambda x: [x.unique()])\n\nrstr(df)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6dd4f476-565d-41d8-8e6c-f7894c21db3c","_uuid":"88a4e409090510e6e7ed9e1f239e760d430a61b7"},"cell_type":"markdown","source":"We can create a new dataframe that has the total revenue over the years for each item category. I'll limit our dataframe to the 15 most rentable categories for analysis for simplicity."},{"metadata":{"_cell_guid":"ff3d5168-0755-4289-b9ca-cf03d83a671f","_uuid":"d9f6f0701408cf2a62f147ef2e1ce92b8eebe2a6","trusted":false,"collapsed":true},"cell_type":"code","source":"sales_by_category = df.groupby(\"item_category_name\", \n                             as_index = False)[\"item_price\"].sum().sort_values(by = \"item_price\")\n\ntop_sales = sales_by_category.nlargest(n=15, columns=[\"item_price\"])\n\ntop_sales.set_index(np.arange(0,len(top_sales),1))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ac15820e-17d3-44d9-95d2-344047605194","_uuid":"686c4e48d3cd95a5c8dcc7d32c804003af49ce71"},"cell_type":"markdown","source":"Oh look! I recognize some words! I know what an Xbox, PS, DVD, PC, Blu-ray and CD are! But... that's all. How can we get information if we don't understand what is being written? Fear not, young grasshopper! I'll guide you with the help of Google (yeah.. always them)."},{"metadata":{"_cell_guid":"5431d09d-2f92-451e-8674-11d2360b6272","_uuid":"b0ef827948b9d33f82c0d7ec8344741c73ee6c69"},"cell_type":"markdown","source":"## Understanding russian using googletrans\n___\n\nWe finally arrived in this section! I cannot understand russian, although I wish I could, and I think many people here are in the same situation. In order to know what we are dealing with, we should translate those category names. Here I'll use googletrans library in order to provide a fair translation of the content. As we all know, google translations still have a way to go in some languages (translation in Portuguese, my mother language, are terrible sometimes), but hopefully the translation will be good enough to at least provide a basic understanding of the categories.\n\nHere I'll loop through every row in top_sales dataframe and translate the categories while rewriting them. At the end, I'll use a barplot to visualize top_sales dataframe."},{"metadata":{"_cell_guid":"5e30a210-12de-403b-a316-85cc54468786","_uuid":"3c76adb46bf94c4c9c7f68b6c26c811ef8434169","trusted":false,"collapsed":true},"cell_type":"code","source":"#The code below is used to translate each row of \"item_category_name\"\n#However, this library needs to use the internet (to access Google)\n#in order to make its translation. Kaggle doesn't let kernels to access\n#the web, so in order to overcome this issue, I've uploaded top_sales\n#dataframe after the translation, the same you should get after running this \n#piece of commented code in your local machine\n'''\nfrom googletrans import Translator\n\ntranslator = Translator()\n\ni = 0\nfor row in top_sales[\"item_category_name\"]:\n    english_word = translator.translate(row)\n    top_sales.iloc[i,0] = english_word.text\n    i+=1\n'''\n\ntop_sales = pd.read_csv(\"../input/additional-data-for-competition/topsales.csv\")\n\nsns.barplot(y = \"item_category_name\", x = \"item_price\",\n             data = top_sales)\nplt.title(\"Sales for each one of the top 15 categories-products\")\nplt.xlabel(\"Sales\")\nplt.ylabel(\"Category-Product\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"556329de-186c-4a92-899a-bf374ea15544","_uuid":"2f1df794c6ce9d2ec6a034ed36171d3290b717f9"},"cell_type":"markdown","source":"It is clear that most sales are related to eletronics, especially videogames and consoles. Perhaps the 2015 drop on the previous plot was because of a sharp cut in the purchases of game-related products. We can check it in a moment.\n\nInstead of using the \"category - product\" format, I'll use only the category, so we can condense information a little bit."},{"metadata":{"_cell_guid":"799e5e41-d3bf-46f1-950d-e6d630d237e3","_uuid":"e1bcef05204da479f3ade0a03231dd5d0c8884d9","trusted":false,"collapsed":true},"cell_type":"code","source":"top_sales[\"item_category\"] = top_sales.item_category_name.str.extract('([A-Za-z\\ ]+)', expand=False) \n\ntop_sales.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7218dfb4-0300-4477-8d13-9859ff3e98a0","_uuid":"072a1eb5d6bd13ea2637776dad1637059e5b70b3"},"cell_type":"markdown","source":"It seems we got what we wanted. Now let's make the same plot as before:"},{"metadata":{"_cell_guid":"b2612676-bf88-48a4-8f90-6baca4871ff2","_uuid":"0bd357e2b50d87f4f56b8ccd09bb38ace2f24460","trusted":false,"collapsed":true},"cell_type":"code","source":"sns.barplot(y = \"item_category\", x = \"item_price\",\n             data = top_sales)\nplt.title(\"Sales for each one of the top 8 categories\")\nplt.xlabel(\"Sales\")\nplt.ylabel(\"Category-Product\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"67a36549-e3cb-41f5-a340-7ce965599b51","collapsed":true,"_uuid":"a09ac5f4bc2f9a113c345abaabde0e31a20a5687"},"cell_type":"markdown","source":"This plot says what we already figured out from the previous barplot. Most sales are game-related. \n\nNow we could make a time series plot over each category and see the increase or decrease in sales over the years. First, though, we have to make a similar translation for all the dataset.\n\nThis time, I'm using the counts of items that were sold, rather than the total price, so I won't consider possible changes in prices through, but only the amount of itens sold."},{"metadata":{"_cell_guid":"cf7e6751-5b7b-47c3-89c8-c16505170891","_uuid":"7f9e60603282bc2f0bc1d950fd7c47827ed7209b","trusted":false,"collapsed":true},"cell_type":"code","source":"dailyItensByCat = pd.DataFrame(df.groupby([\"item_category_name\", \"date\"], as_index=False)[\"item_cnt_day\"].sum())\n\ndailyItensByCat[\"month\"] = dailyItensByCat.date.str.extract(\".([0-9][0-9]).\", expand = False)\ndailyItensByCat[\"year\"] = dailyItensByCat.date.str.extract(\".([0-9][0-9][0-9][0-9])\", expand =False)\ndailyItensByCat[\"day\"] = 1 #create this column so I can use the DatetimeIndex\ndailyItensByCat['monthlyDate'] = pd.to_datetime(dailyItensByCat[[\"year\", \"month\", \"day\"]])\n\nmonthlyItensByCat = pd.DataFrame(dailyItensByCat.groupby([\"monthlyDate\", \"item_category_name\"],\n                                                         as_index = False)[\"item_cnt_day\"].sum())\nmonthlyItensByCat.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3e16685d-1f31-4599-a192-0b559d2757c5","_uuid":"5b66ae2a85ac971bb1fb2cf7fd40d51df0e8e7da","trusted":false,"collapsed":true},"cell_type":"code","source":"monthlyItensByCat = pd.DataFrame(dailyItensByCat.groupby([\"monthlyDate\", \"item_category_name\"],\n                                                         as_index = False)[\"item_cnt_day\"].sum())\n\nmonthlyItensByCat.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ae2705b9-7934-4374-85ca-e3ef49a92174","_uuid":"800a256bd920a5b8912f5c69485405ab7da8318d","trusted":false,"collapsed":true},"cell_type":"code","source":"monthlyItensByCat[\"item_category\"] = (\n    monthlyItensByCat.item_category_name.str.extract(r'((?i)[А-Яa-я\\ ]+)', expand=False))\n\nmonthlyItensByCat.head()\n\n#As a record: I've spent hours looking for a way to extract \n#cyrillic alphabet, but there is no simple answer on the web.\n#In the end, turned out the solution was simple indeed and achieved by \n#trial and error. I hope this will help someone when they need a way\n#to extract cyrillic alphabet words so they won't spend such a long time\n#looking for an answer :P \n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dbb1a645-c2cd-48cd-a10c-67e51437e58d","_uuid":"17317cf40f6f066ef601c9fe0ef082413063a6ee"},"cell_type":"markdown","source":"Again, we keep grouping. Now, we group by the new general category `item_category` and month."},{"metadata":{"_cell_guid":"b811d4d9-8975-406d-8138-c35a2089af1a","_uuid":"5c4c998d89c6847671bf0e899ac9ca6664008a27","trusted":false,"collapsed":true},"cell_type":"code","source":"countByCatByMonth = monthlyItensByCat.groupby([\"item_category\", \"monthlyDate\"], as_index=False)[\"item_cnt_day\"].sum()\ncountByCatByMonth['item_category_trans'] = None\ncountByCatByMonth.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"78271614-0e20-4866-9043-88efb6bc374c","_uuid":"29e16d1043b29a0ea6ab2bf8f3a4dcf07dc7c6e3"},"cell_type":"markdown","source":"Finally we can translate the new dataset. Here I've made a slight change in the code from before. Since googletrans uses the internet to identify the language and translate it (you can set the language in your code, by the way), checking everytime for the same word sounds very time consuming. So I've created a dictionary to store the words that were already translated and only get them rather them querying google translator again. "},{"metadata":{"_cell_guid":"6979295f-574a-45bc-8080-c8adb82ba472","_uuid":"150d14729633d2c4e20817baab9fed505a37e2ee","trusted":false,"collapsed":true},"cell_type":"code","source":"#Here again, we need internet access to perform the translations\n#and Kaggle doesn't let us. So I uploaded the exact dataframe that\n#should be generated after running the code below\n\n'''\nfrom googletrans import Translator\n\ncat_translator = Translator()\n\nobs = 0\nalready_translated = {'word': [], 'translation':[]}\n#creates a new column in countByCatByMonth\ncountByCatByMonth['item_category_trans'] = None\n\n#for each row...\nfor cat_name in countByCatByMonth[\"item_category\"]:\n    \n    #check if it has already been translated\n    if cat_name in already_translated['word']:\n        #if it is, get the index of the original word on the dictionary...\n        word_index = already_translated[\"word\"].index(cat_name)\n        #and use it to get the translated word\n        countByCatByMonth.iloc[obs,3] = already_translated[\"translation\"][word_index]\n        #if the word was not translated yet...\n    else:\n        try:\n            #translate it\n            english_word = cat_translator.translate(cat_name)\n            #append in dictionary for later use\n            already_translated['word'].append(cat_name)\n            already_translated['translation'].append(english_word.text)\n            #write the translated word into dataframe\n            countByCatByMonth.iloc[obs,3] = english_word.text\n    \n        except:\n            print (\"Error in row \"+ cat_name)\n    obs+=1\n'''\ncountByCatByMonth = pd.read_csv(\"../input/additional-data-for-competition/countbycatbymonth.csv\")\ncountByCatByMonth.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"452cbb0f-62d2-48d6-91ae-31893d68743d","_uuid":"fe24ed5f858ae6a75f94eada6da24cc49a9af973","trusted":false,"collapsed":true},"cell_type":"code","source":"g = sns.FacetGrid(data = countByCatByMonth.sort_values(by=\"monthlyDate\"), hue = \"item_category_trans\", legend_out=True, size = 8)\ng = (g.map(plt.plot, \"monthlyDate\", \"item_cnt_day\").set(xticks=[0, 5, 11, 17, 23, 29, 35],\n                                                        xticklabels=['2013-01-01', '2013-06-01', '2013-12-01', \n                                                                     '2014-06-01','2014-12-01', '2015-06-01','2015-12-01']))\ng.add_legend()\nplt.xlabel(\"Monthly Date\")\nplt.ylabel(\"Number of itens sold\");\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c989a148-8255-4e54-ad06-c07d75624b1d","_uuid":"59d6e12edf1993fc8d51da9241172f93b5375a4b"},"cell_type":"markdown","source":"What a pretty good looking plot! From it we can see that items from those most profitable categories had a cut in sells. People had stopped buying such products since the beginning of 2014, although some itens had a sharper decrease than others."},{"metadata":{"_cell_guid":"99105356-7d73-457a-b1e3-593d310d33df","_uuid":"af505bed9264ceb53034528888e36c607fcf4d41"},"cell_type":"markdown","source":"## Time series analysis\n___\n\nFirst I'll analyse the item_price series. To make our analysis, it is best to set the dates as index, so on the series decomposition they will show up in x-axis."},{"metadata":{"_cell_guid":"e7298474-913d-4fed-ae63-608655db8ca3","_uuid":"b5292b6bd0d37dd0372dcdfe62bc8dc90640f16b","trusted":false,"collapsed":true},"cell_type":"code","source":"ts = revenueByMonth.set_index(\"monthlyDate\")\nts.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e10e4d62-1e65-4e03-b95c-25226d551144","_uuid":"bfbe400514fa7b4cb33c4d3743738c766137666f"},"cell_type":"markdown","source":"First let's take a look at the decomposition of the item_price time series. We will split our time series plot in three components: trend, seasonality and residual error (the part of the time series that cannot be explained by the former two). After that, we plot each component and analyse what we got."},{"metadata":{"_cell_guid":"a5bfefd8-a019-4003-b80f-22155f35ae8c","_uuid":"21471b7ddcf4a42079c169296f4ece1edf57985e","trusted":false,"collapsed":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(ts)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey = 'row')\nax1.plot(ts)\nax1.set_title(\"original\")\nax2.plot(trend)\nax2.set_title(\"trend\")\nax3.plot(seasonal)\nax3.set_title(\"seasonal\")\n\nax4.plot(residual)\nax4.set_title(\"residual\")\nf.set_figheight(7)\nf.set_figwidth(10)\n\n#let me rotate the labels of x-axis. \n#Otherwise they get mixed and very hard to read.\nfor ax in f.axes:\n    plt.sca(ax)\n    plt.xticks(rotation = 45)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bbe85f45-0f26-4c85-a593-8850e6be2614","_uuid":"2426c3ac5ade847fabd953c6d19eb51ede9fb2e7"},"cell_type":"markdown","source":"In this decomposition, we can describe the behaviour of the most important features of our timeseries. We can clearly see that sales increase every december. When we decompose a time series, the algorithm will always try to find a seasonality. So in order to confirm if there is or not a real seasonality, we should see both residual and seasonal plots and compare the scales. Sometimes, the residuals magnitude is way above seasonal magnitude, which means that if there is a seasonal effect, it is negligible. Here, however, seasonality does indeed exists, as the order of magnitude is the same (1e7) and the max value of the seasonality effect in sales is about 4 times the highest residual. \n\nTrend goes up until July 2014 and then starts to drop and keep this way as by the end of the records. \n\nResidual plot show that there are two key points in time where neither trend nor seasonal effects can explain the sales - Dec 2013 and Dec 2014. In the former, sales were lower than expected by the trend at the time and the seasonal effect, while on the latter, sales were higher than expected.\n\nNow that we've seen how the item_price time series behave, let's make some predictions. Here I'll use a relative unorthodox method. Rather than ARIMA or ETS, we'll be using the...\n\n## Sliding Window Method\n\nThis method can be used to transform what would be a time series forecast into a supervised learning prediction. The key thing we want here is to use the  value of `item_price` on the previous date as the predictor variable to the current date. For simplicity, I'll call the previous `item_price` as `item_price_x`. "},{"metadata":{"_cell_guid":"7edfbaf7-e8e8-45ea-a03e-d916c75ad6b0","_uuid":"2ffa61637c41c850dc7dbc8af385fa47c77fda8e","trusted":false,"collapsed":true},"cell_type":"code","source":"revenueByMonth[\"item_price_x\"] = revenueByMonth[\"item_price\"].shift(1)\n\nslideRevenueByMonth = revenueByMonth.drop(columns = \"monthlyDate\")\n\nsns.lmplot(data = slideRevenueByMonth, x = \"item_price_x\", y = \"item_price\", ci=False);\n\nslideRevenueByMonth.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"07ee8ab4-5cbc-478a-9ddf-c01d8751fe47","_uuid":"f42d43335aa4cee54930fe4e86945201ddfbbda0"},"cell_type":"markdown","source":"Note that the first row of `item_price_x` is a NaN value, because there is no previous `item_price` for the first row.\n\nI'll also take the log of both columns so the data will be less spread out."},{"metadata":{"_cell_guid":"1a0c8799-83db-49ee-a90a-33ee199a1700","collapsed":true,"_uuid":"e23c9794fb2d2ff4de0fe14b1b312b796fc445ee","trusted":false},"cell_type":"code","source":"slideRevenueByMonth[\"log_itemprice\"] = np.log(slideRevenueByMonth[\"item_price\"])\n\nslideRevenueByMonth[\"log_itemprice_x\"] = np.log(slideRevenueByMonth[\"item_price_x\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"369f1948-7e6e-43a9-99a2-5264303157f3","_uuid":"628129c575182772fd9a63550b7e102985ddbb69","trusted":false,"collapsed":true},"cell_type":"code","source":"sns.lmplot(data = slideRevenueByMonth, x = \"log_itemprice_x\", y = \"log_itemprice\", ci=False);\n\nslideRevenueByMonth.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dc2ae738-0133-4b1f-a1a9-72dea4ec9dd0","_uuid":"c185f082cf8489869b8ab326dc023752a52ec963"},"cell_type":"markdown","source":"The data now seems better to work on.  I'll use the log values to make the predictions here.\n\nThe sliding window method is now done and we can proceed to the actual training. Let's create the training and test datasets first and then use Random Forest and Linear regressors.\n\n### First trials"},{"metadata":{"_cell_guid":"c80eb5da-e13f-4bfd-8b82-4e8ae5660c3c","collapsed":true,"_uuid":"b9729386dc76644c6bffbd703f72ff801d747e0f","trusted":false},"cell_type":"code","source":"xtrain = slideRevenueByMonth.iloc[1:20,3].values.reshape(-1,1)\nytrain = slideRevenueByMonth.iloc[1:20,2]\n\nxtest = slideRevenueByMonth.iloc[20:,3].values.reshape(-1,1)\nytest = slideRevenueByMonth.iloc[20:, 2]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"58468229-93d6-49ca-9980-6cc1ccdd84a1","_uuid":"deeba7330aeec8b459316ea7621c903bdf27830d","trusted":false,"collapsed":true},"cell_type":"code","source":"#Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf_reg = RandomForestRegressor(random_state = 42)\n\nrf_reg.fit(xtrain, ytrain)\n\nypred_rf = rf_reg.predict(xtest)\n\n#LinearRegressor\n\nfrom sklearn.linear_model import LinearRegression\n\nlinear = LinearRegression()\n\nlinear.fit(xtrain,ytrain)\n\nypred_linear = linear.predict(xtest)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d8c6f6c1-a029-4e38-a3b1-ce79835bb655","_uuid":"98609c11cc9f39afa764ab9475319d1e5aca7397"},"cell_type":"markdown","source":"Now that the predictions were done, we will see how the models performed against the testing set. The ideal scatterplot here should be a 45 degree line, so the predictions match perfectly the actual values. "},{"metadata":{"_cell_guid":"e804f012-c4bb-4f1c-988f-5479c4ce87f9","_uuid":"cc2b202b8e805762f63f6b85a0e6b4dbd5521ec3","trusted":false,"collapsed":true},"cell_type":"code","source":"#Just putting all predictions in one dataframe to get things more organized\ntesting = pd.concat([pd.DataFrame(ypred_linear),\n                     pd.DataFrame(ypred_rf),pd.DataFrame(ytest).reset_index().drop([\"index\"], axis = 1)], axis = 1)\n\ntesting.columns = [\"log_itemprice_pred_ln\",\"log_itemprice_pred_rf\", \"log_itemprice\"]\n\n#making the plot and also drawing a 45 degree line so we can see the ideal situation\ngrid = sns.JointGrid(y = testing.log_itemprice_pred_rf, x = testing.log_itemprice, space=0, size=6, ratio=50)\ngrid.plot_joint(plt.scatter, color=\"g\")\nplt.plot([17.8, 19.0], [17.8, 19.0], linewidth=2);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"34311954-2fa2-48ec-bee2-85afaa0e5f0a","_uuid":"5b4cafea1713aff4741f2f94819d0ac03a0e22bc"},"cell_type":"markdown","source":"Well... could be worse, right? Let's see how the linear model performed."},{"metadata":{"_cell_guid":"611e5865-bd7b-4ea5-8e36-4c68245b9abb","_uuid":"1e71b65f1ecb0505c36c811cb04f7424612fbca5","trusted":false,"collapsed":true},"cell_type":"code","source":"grid = sns.JointGrid(y = testing.log_itemprice_pred_ln, x = testing.log_itemprice, space=0, size=6, ratio=50)\ngrid.plot_joint(plt.scatter, color=\"g\")\nplt.plot([17.8, 19.0], [17.8, 19.0], linewidth=2);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0be459ee-77aa-4bbc-9633-95b7088f21de","_uuid":"ce202bc9af328f1258a7e2b93734aea0df0ec82a"},"cell_type":"markdown","source":"Doesn't look good either... In both models, low item prices (< 18.1) are predicted to be higher than they should be and high item prices are predicted to be lower. I thought on this for a while and maybe we could improve those results by creating a new variable. And that lead us to...\n\n### The UpOrDown variable\n\nWe know that if there is a rising short time trend, it is more likely that our next value will go up. Similarly, if we are looking at a descending trend, it is likely the next value will be lower. So I will take the same approach as before but this time, if the previous value was lower than the present one, we will create a column called \"up\", and a column called \"down\" otherwise and use them as predictors as well. Since in our predictions we won't know the current value to compute the \"UpOrDown\" variable, we will use as a predictor the PreviousUpOrDown. That is, was the sales value increasing or decreasing last time we saw them?\n\nThis could solve the issue that, for example, our model sees the value \"18\" and, based on its previous training, thinks the next value will be 19. However, during training the local trend was a rising one, and now is a descending one. "},{"metadata":{"_cell_guid":"c6e6c08c-3916-40bf-b140-7a52f2eb05d6","_uuid":"d913dfcef6ac3fa75c3e95c4314be1bb6fb288c1","trusted":false,"collapsed":true},"cell_type":"code","source":"slideRevenueByMonth[\"UpOrDown\"] = np.sign(slideRevenueByMonth[\"item_price\"] - slideRevenueByMonth[\"item_price_x\"])\nslideRevenueByMonth[\"PreviousUpOrDown\"] = slideRevenueByMonth[\"UpOrDown\"].shift(1)\n#drop two rows that have null values in UpOrDown and PreviousUpOrDown\nslideRevenueByMonth = slideRevenueByMonth.dropna()\nslideRevenueByMonth.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"12e58612-d1b0-4ee1-83b3-43cf2900e78b","_uuid":"0c229494a371f34ce4ac59f8f0a25474a6df6b7c","trusted":false,"collapsed":true},"cell_type":"code","source":"xtrain = slideRevenueByMonth.iloc[2:22,[3,5]]\nytrain = slideRevenueByMonth.iloc[2:22,2]\n\nxtest = slideRevenueByMonth.iloc[22:,[3,5]]\nytest = slideRevenueByMonth.iloc[22:, 2]\n\n##########Random Forest###########\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf_reg = RandomForestRegressor(random_state = 42)\n\nrf_reg.fit(xtrain, ytrain)\n\nypred_rf = rf_reg.predict(xtest)\n\n##########LinearRegressor###########\n\nfrom sklearn.linear_model import LinearRegression\n\nlinear = LinearRegression()\n\nlinear.fit(xtrain,ytrain)\n\nypred_linear = linear.predict(xtest)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3c0bfc3e-a120-42ae-8bc5-344a4ba41dc3","_uuid":"0e7162ed60bbd526ba15e06a3f7ec156c22a581d","trusted":false,"collapsed":true},"cell_type":"code","source":"#Creating the predictions dataframe\ntesting = pd.concat([pd.DataFrame(ypred_linear),\n                     pd.DataFrame(ypred_rf),pd.DataFrame(ytest).reset_index().drop([\"index\"], axis = 1)], axis = 1)\n\ntesting.columns = [\"log_itemprice_pred_ln\",\"log_itemprice_pred_rf\", \"log_itemprice\"]\n\ngrid = sns.JointGrid(y = testing.log_itemprice_pred_rf,x = testing.log_itemprice, space=0, size=6, ratio=50)\ngrid.plot_joint(plt.scatter, color=\"g\")\nplt.plot([17.8, 19.0], [17.8, 19.0], linewidth=2)\nplt.title(\"Random Forest x True\", fontsize = 20);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"63dacb7b-0705-4089-bc5e-54c0e8b1f9c7","_uuid":"e16f07e9f0bb9ee202804eefb2b6f3a951bab095","trusted":false,"collapsed":true},"cell_type":"code","source":"grid = sns.JointGrid(y = testing.log_itemprice_pred_ln,x = testing.log_itemprice, space=0, size=6, ratio=50)\ngrid.plot_joint(plt.scatter, color=\"g\")\nplt.plot([17.8, 19.0], [17.8, 19.0], linewidth=2)\nplt.title(\"Linear Regression x True\", fontsize = 20);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2c6c59e2-4367-4f3b-aa09-60dce8f2093f","_uuid":"cb46abd4e4f8ac6e46b6a33dc138bb877c0ed86e"},"cell_type":"markdown","source":"Random Forest didn't improve much, but Linear Regressor did a pretty amazing job here. It is clear that the results are biased, but now they are linear. If we manage to identify that bias, our predictions could be more accurate and thus allowing us to use Linear Regression to make forecasts. I believe that if we had access to more training data, the model would be better, as we only trained the algorithm with 16 data points and tested on the other rest. The fact that this method forces us drop two data points (one due to UpOrDown and another due to PreviousUpOrDown) makes even more important to have a greater number of training points for this technique to work at its best."},{"metadata":{"_cell_guid":"51a1afa1-0905-463d-8590-aca4f94bb67b","_uuid":"a2512a154d69bb4adce02504b6eae893e7e4893d"},"cell_type":"markdown","source":"## Conclusions\n___\n\nIn this notebook, we've seen how to deal with data in another language using `googletrans` package. Some exploratory analysis was done and it was seen that some categories were sold more frequently than others and that by mid-2014 sales had started dropping.  We've made a basic decomposition of the `item_price` time series and confirmed that there is indeed a seasonality (sales in December are increased by a lot).  At the end, we saw how to use the sliding window method to make forecasts using supervised machine learning techniques, especially Linear Regression. Further studies should be done on how to improve this method to make the predictions and use them to predict the number of items sold for all items in the dataset.\n\nI'm ending the notebook here but as soon as I learn more about time series, I'll come back and finish this notebook with a forecast. I'm planning to use an ARIMA model to make the it. If you liked, please upvote! Any suggestions to improve the notebook or doubts please tell me on the comments!"}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"anaconda-cloud":{}},"nbformat":4,"nbformat_minor":1}