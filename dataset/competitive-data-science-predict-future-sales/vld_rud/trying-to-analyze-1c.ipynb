{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\nfrom IPython.display import Image\n\nfrom sklearn.preprocessing import LabelEncoder \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading the CSV\nitems_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nitem_categories_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\nsales_train_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\nshops_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\n\ntest_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Printing the data for train"},{"metadata":{"trusted":true},"cell_type":"code","source":"#items\nprint(items_df.head())\nprint(items_df.columns)\nprint(items_df.dtypes)\nprint(items_df.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# item_categories\nprint(item_categories_df.head())\nprint(item_categories_df.columns)\nprint(item_categories_df.dtypes)\nprint(item_categories_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sales\nprint(sales_train_df.head())\nprint(sales_train_df.columns)\nprint(sales_train_df.dtypes)\nprint(sales_train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shops\nprint(shops_df.head())\nprint(shops_df.columns)\nprint(shops_df.dtypes)\nprint(shops_df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"printing test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# shops\nprint(test_df.head())\nprint(test_df.columns)\nprint(test_df.dtypes)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The scheme of database"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/database/database.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What should I do?**Data preprocessing:\n\n* 1) check the missing values\n* 2) drop any columns\n* 3) see the categorical values\n* 4) normalization the data\n* 5) feature scalling"},{"metadata":{},"cell_type":"markdown","source":"1. Check the missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the count of Nan values\nprint(\"#sales_train_df:\")\nprint(sales_train_df.isna().sum().sort_values(ascending=False))\nprint('')\nprint(\"#items_df:\")\nprint(items_df.isna().sum().sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"Ok. I have no NaN values. It's a perfect."},{"metadata":{},"cell_type":"markdown","source":"2. drop any columns\nFor these part, I must understand relationship between columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here I can initialize functions which can be used for plotting\ndef plot_plt_scatter(x, y, ymin=10000):\n    #plotting the scatter\n    plt.figure(figsize=(40, 30))\n    plt.scatter(x, y)\n    if ymin != False:\n        plt.ylim(0, ymin)\n    plt.title('')\n    plt.show()\n\n    \ndef plot_plt_histogram(set_x, bins, grid=False, height=70, width=100,\n                       need_xlim=False, need_ylim=False, xlim=1000, ylim=1000):\n    # plotting the histogram\n    plt.figure(figsize=(width, height))\n    plt.hist(set_x, bins=bins)\n    if need_xlim == True:\n        plt.xlim(0, xlim)\n    if need_ylim == True:\n        plt.ylim(0, ylim)\n    plt.title(\"Histogram\")\n    plt.grid(grid)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At first, let watch to shops_df first words. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# the shop_df has cities as a first word. \n#In Russian, geography has big value and give us more accurate prediction\nfull_name = shops_df['shop_name'].str.split(' ', n=0, expand=True)\ncities = full_name[0].str.split('.', n=0, expand=True)\ncities = cities[0]\ncities = cities.replace(['!Якутск'], 'Якутск')\n# we need to label_encode the cities\nlabel_encoder = LabelEncoder()\ncities_encoded = label_encoder.fit_transform(cities)\nshops_df['city_id'] = pd.Series(cities_encoded, index=shops_df.index)\n# joining the 'cities_id' column to the sales_train_df\nsales_train_df = sales_train_df.join(\n                  shops_df[['shop_id', 'city_id']].set_index('shop_id'), on='shop_id'\n                 )\n# the result of ths - classification sales by geography\nsales_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grouping cities\ngrouped_city_names = cities.groupby(cities, as_index=True).count()\ngrouped_city_by_count = sales_train_df[['city_id', 'item_id']].groupby(sales_train_df['city_id'],\n                                               as_index=True).count()\n#plotting the histogram for cities\nplot_plt_histogram(sales_train_df['city_id'], bins=cities_encoded.shape[0], grid=True, \n                   need_ylim=True, ylim=200000)\n#printing the result\nprint('median: ',grouped_city_by_count.median()[0],\n      'min: ', grouped_city_by_count.min()[0], 'max: ', grouped_city_by_count.max()[0],\n      \"std: \", grouped_city_by_count.std()[0], 'mean: ', grouped_city_by_count.mean()[0])\ngrouped_city_by_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's watch on items and what's their distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# groupping the item by frequency\ngrouped_item_id = sales_train_df['item_id'].groupby(sales_train_df['item_id'],\n                                  as_index=True).count()\n\n#printing the histogram of \nplot_plt_histogram(sales_train_df['item_id'], bins=items_df.shape[0],\n                   height=25, width=100,\n                   need_ylim=True, ylim=1000)\n# The result of this histogram is frequency of changing the price of item. \n# The high limit for y is 1000 because if it is without limit the plot will be unreadble\nprint(grouped_item_id)\nprint(grouped_item_id.sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we need to group our frequency\nplot_plt_histogram(grouped_item_id, bins=grouped_item_id.shape[0],\n                   height=40, width=100,\n                   need_ylim=True, ylim=800, need_xlim=True, xlim=1000)\n# printing the median of times of changing the price\nprint('median:',grouped_item_id.median(), 'mode:',grouped_item_id.mode()[0],\n      'min:', grouped_item_id.min(), 'max:', grouped_item_id.max(),\n      \"std:\", grouped_item_id.std(), 'mean:', grouped_item_id.mean())\ngrouped_item_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see, that half of items has more than 32 times of changing the price and half of items has less. So many items has no relationship to time.\nSo, we can classificate items to groups as 'rare', 'often', 'popular' as 0, 1, 2. 'Rare' is from 1 to 12, 'often' is from 13 to 365, 'popular' is from 366 to max."},{"metadata":{"trusted":true},"cell_type":"code","source":"# for working with grouped data we need to copy variable grouped_item_id\nfreq = pd.Series(grouped_item_id.copy().to_numpy())\n# create a dataframe for table of classification \nclassification_by_freq = pd.DataFrame()\nclassification_by_freq['item_id'] = freq.index\nclassification_by_freq['num_freq'] = freq\n# 'freq' column is our classifier\nclassification_by_freq['freq'] = 0\nclassification_by_freq['freq'][(classification_by_freq['num_freq'] >= 13) \\\n                               & (classification_by_freq['num_freq'] <=365)] = 1\nclassification_by_freq['freq'][(classification_by_freq['num_freq'] >= 366) \\\n                               & (classification_by_freq['num_freq'] <= classification_by_freq['num_freq'].max())] = 2\n# if I need to cut on more parts, write code to HERE \\/\n\n# printing the count of every class\nprint('count items:', \n      classification_by_freq['freq'].groupby(classification_by_freq['freq'], as_index=True).count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"#pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\nitem_id_grouped_by_mean = sales_train_df[['item_id', 'item_price']].groupby(sales_train_df['item_id'],\n                                  as_index=True).mean()\nitem_id_grouped_by_median = sales_train_df[['item_id', 'item_price']].groupby(sales_train_df['item_id'],\n                                  as_index=True).median()\n#print(item_id_grouped_by_mean.sort_values(by='item_price', ascending=False))\nplot_plt_scatter(item_id_grouped_by_mean['item_id'], item_id_grouped_by_mean['item_price'])\nplot_plt_scatter(item_id_grouped_by_median['item_id'], item_id_grouped_by_median['item_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_ser = sales_train_df[['date', 'item_price']][sales_train_df['item_id'] == 20949]\n#time_ser\ngrouped_time_series = time_ser.groupby('date', as_index=False).mean()\ngrouped_time_series.plot(x='date', y='item_price', figsize=(100, 30))\ngrouped_time_series\nprint()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n        ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}