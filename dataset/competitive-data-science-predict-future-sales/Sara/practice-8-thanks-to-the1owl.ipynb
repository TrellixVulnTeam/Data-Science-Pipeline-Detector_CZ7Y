{"cells":[{"metadata":{"_uuid":"79cf6af491297aa65276666d006a4e09d850442b"},"cell_type":"markdown","source":"# **Introduction**\n* My 8rd Practice\n* Main topic: Time series data, TF-IDF, catboost\n* **data description summary**\n    * time-series dataset consisting of daily sales data\n    * predict** total sales for every product and store** in the next month\n    * Submissions are evaluated by root mean squared error (RMSE)\n    * Data fields  \nID - an Id that represents a (Shop, Item) tuple within the test set  \nshop_id - unique identifier of a shop  \nitem_id - unique identifier of a product  \nitem_category_id - unique identifier of item category  \n**item_cnt_day** - number of products sold. You are predicting a monthly amount of this measure  \nitem_price - current price of an item  \ndate - date in format dd/mm/yyyy  \n**date_block_num **- a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33  \nitem_name - name of item  \nshop_name - name of shop  \nitem_category_name - name of item category  \n* This kernel includes lots of contents of [1owl's](https://www.kaggle.com/the1owl/playing-in-the-sandbox).\n* I'm trying to apply [SEMMA](https://en.wikipedia.org/wiki/SEMMA) in all of my practice.\n* I cited various sources with its original link. If there are some copyright problems, please let me know. \n"},{"metadata":{"_uuid":"94bd5c6dba8d6d5821cef64de1f7bcc7a2d95adc"},"cell_type":"markdown","source":"## 1. <span style=\"color:red\"> S</span>ample \n---\n*Data preprocessing / Data partition*"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# 필요 라이브러리 부르기\nimport numpy as np\nimport pandas as pd\nfrom sklearn import *\nimport nltk, datetime\n\n# 데이터 부르기\ntrain = pd.read_csv('../input/sales_train.csv')\ntest = pd.read_csv('../input/test.csv')\nsubmission = pd.read_csv('../input/sample_submission.csv')\nitems = pd.read_csv('../input/items.csv')\nitem_cats = pd.read_csv('../input/item_categories.csv')\nshops = pd.read_csv('../input/shops.csv')\nprint('train:', train.shape, 'test:', test.shape) #(행-1,열) ?!\nprint(submission.head())\nprint(items.head())\nprint(item_cats.head())\nshops.head()","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"[c for c in train.columns if c not in test.columns] # 이름하야 리스트 컴프리핸션","execution_count":24,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77f440d6be4730158a58a600dd7137c8be3eb7a3"},"cell_type":"code","source":"train.head()","execution_count":25,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecfee72b3e987be775beef13b6048fdd75b39e46"},"cell_type":"code","source":"test.head()","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"01d2e43366716ff75d85962da3b3f6358d2d0dc2"},"cell_type":"markdown","source":"### 변수추가\n1. Text Features  \n[문서내 빈도 X log n/1+전체문서에서 빈도 = tf-idf](https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/)\n2. Date Features\n"},{"metadata":{"trusted":true,"_uuid":"972fb4f5101623ba2620286b50fb4da07193eaec"},"cell_type":"code","source":"#Text Features 이름분리\nfeature_cnt = 25 # 요만큼만 뽑아줭\ntfidf = feature_extraction.text.TfidfVectorizer(max_features=feature_cnt)\nitems['item_name_len'] = items['item_name'].map(len) #Lenth of Item Description\nitems['item_name_wc'] = items['item_name'].map(lambda x: len(str(x).split(' '))) #Item Description Word Count\ntxtFeatures = pd.DataFrame(tfidf.fit_transform(items['item_name']).toarray())\ncols = txtFeatures.columns\nfor i in range(feature_cnt):\n    items['item_name_tfidf_' + str(i)] = txtFeatures[cols[i]]\nitems.head()","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b3e164b7bf81154c8bdfaac98c6bca116299343"},"cell_type":"code","source":"#Text Features 카테고리화\nfeature_cnt = 25\ntfidf = feature_extraction.text.TfidfVectorizer(max_features=feature_cnt)\nitem_cats['item_category_name_len'] = item_cats['item_category_name'].map(len)  #Lenth of Item Category Description\nitem_cats['item_category_name_wc'] = item_cats['item_category_name'].map(lambda x: len(str(x).split(' '))) #Item Category Description Word Count\ntxtFeatures = pd.DataFrame(tfidf.fit_transform(item_cats['item_category_name']).toarray())\ncols = txtFeatures.columns\nfor i in range(feature_cnt):\n    item_cats['item_category_name_tfidf_' + str(i)] = txtFeatures[cols[i]]\nitem_cats.head()","execution_count":29,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccd2b1177f979e341f81e80d51be763d4cf90644"},"cell_type":"code","source":"#Text Features 가게이름을 TFIDF\nfeature_cnt = 25\ntfidf = feature_extraction.text.TfidfVectorizer(max_features=feature_cnt)\nshops['shop_name_len'] = shops['shop_name'].map(len)  #Lenth of Shop Name\nshops['shop_name_wc'] = shops['shop_name'].map(lambda x: len(str(x).split(' '))) #Shop Name Word Count\ntxtFeatures = pd.DataFrame(tfidf.fit_transform(shops['shop_name']).toarray())\ncols = txtFeatures.columns\nfor i in range(feature_cnt):\n    shops['shop_name_tfidf_' + str(i)] = txtFeatures[cols[i]]\nshops.head()","execution_count":30,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1995a45153ca5a2f1e43ec257048401950fceb46"},"cell_type":"code","source":"#Make Monthly 날짜붙은거 보고 달달로 만듦\ntrain['date'] = pd.to_datetime(train['date'], format='%d.%m.%Y')\ntrain['month'] = train['date'].dt.month\ntrain['year'] = train['date'].dt.year\ntrain = train.drop(['date','item_price'], axis=1)\ntrain = train.groupby([c for c in train.columns if c not in ['item_cnt_day']], as_index=False)[['item_cnt_day']].sum()\ntrain = train.rename(columns={'item_cnt_day':'item_cnt_month'})\n\n#Monthly Mean 팔린 평균\nshop_item_monthly_mean = train[['shop_id','item_id','item_cnt_month']].groupby(['shop_id','item_id'], as_index=False)[['item_cnt_month']].mean()\nshop_item_monthly_mean = shop_item_monthly_mean.rename(columns={'item_cnt_month':'item_cnt_month_mean'})\n\n#Add Mean Feature 평균이들 합쳐주고\ntrain = pd.merge(train, shop_item_monthly_mean, how='left', on=['shop_id','item_id'])\n\n#Last Month (Oct 2015) 가장 최근 거 - 시계열이니까 하나의 변수로 중요하게 생각한 듯\nshop_item_prev_month = train[train['date_block_num']==33][['shop_id','item_id','item_cnt_month']]\nshop_item_prev_month = shop_item_prev_month.rename(columns={'item_cnt_month':'item_cnt_prev_month'})\nshop_item_prev_month.head()\n\n#Add Previous Month Feature 그전 거도 붙이고\ntrain = pd.merge(train, shop_item_prev_month, how='left', on=['shop_id','item_id']).fillna(0.)\n\n#Items features 하나씩 합쳐주고\ntrain = pd.merge(train, items, how='left', on='item_id')\n\n#Item Category features\ntrain = pd.merge(train, item_cats, how='left', on='item_category_id')\n\n#Shops features\ntrain = pd.merge(train, shops, how='left', on='shop_id')\ntrain.head()","execution_count":31,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bf2b5326c5dbdab6dcfef679226d26a480f92dc"},"cell_type":"code","source":"# 아니 test데이터를 왜 이렇게 하는 거지\ntest['month'] = 11\ntest['year'] = 2015\ntest['date_block_num'] = 34\n\n#Add Mean Feature\ntest = pd.merge(test, shop_item_monthly_mean, how='left', on=['shop_id','item_id']).fillna(0.)\n\n#Add Previous Month Feature\ntest = pd.merge(test, shop_item_prev_month, how='left', on=['shop_id','item_id']).fillna(0.)\n\n#Items features\ntest = pd.merge(test, items, how='left', on='item_id')\n\n#Item Category features\ntest = pd.merge(test, item_cats, how='left', on='item_category_id')\n\n#Shops features\ntest = pd.merge(test, shops, how='left', on='shop_id')\ntest['item_cnt_month'] = 0.\ntest.head()","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"52a13db22155c7947d8ed669913592ca7eb0ac9b"},"cell_type":"markdown","source":"## 2. <span style=\"color:red\"> E</span>xplore\n---\n*Summary /Visualization / Fitter Outliner /Correlation*"},{"metadata":{"trusted":true,"_uuid":"b6249ff98d424ef95bf532a5f1a9b526f94f0399"},"cell_type":"code","source":"from PIL import Image, ImageDraw, ImageFilter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# 카테고리&매장 별 월별 팔린거 (총)\ndf_all = pd.concat((train, test), axis=0, ignore_index=True)\nstores_hm = df_all.pivot_table(index='shop_id', columns='item_category_id', values='item_cnt_month', aggfunc='count', fill_value=0)\nfig, ax = plt.subplots(figsize=(10,10))\n_ = sns.heatmap(stores_hm,cmap=sns.color_palette(\"Blues\"),ax=ax)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e74b0b4d33bc045f60ffb5bd7e43577a758ba3b"},"cell_type":"code","source":"# 카테고리&매장 별 월별 팔린거 (테스트 데이터)\nstores_hm = test.pivot_table(index='shop_id', columns='item_category_id', values='item_cnt_month', aggfunc='count', fill_value=0)\nfig, ax = plt.subplots(figsize=(10,10))\n_ = sns.heatmap(stores_hm,cmap=sns.color_palette(\"Blues\"),ax=ax) # 역시 하늘색이 잘 보임","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2bb7ad10e5e801ddfd480c5212469568d6ff419d"},"cell_type":"markdown","source":"## 3. <span style=\"color:red\"> M</span>odify\n---\n*dimension reduction(PCA, SVD, NMF...)/unsupervised learning(K-means algorithm, EM algorithm…)/transformation of variable([Normalization](https://thebook.io/006723/ch09/02/01/), Scale transpos)*\n\n### Label Encoding\n다른 방식으로 접근 해보겠 -> 가중치 기반으로!"},{"metadata":{"trusted":true,"_uuid":"e8eac418a1e2a5863081aa104d0d0b74eb90dee5"},"cell_type":"code","source":"for c in ['shop_name','item_name','item_category_name']:\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(train[c].unique())+list(test[c].unique()))\n    train[c] = lbl.transform(train[c].astype(str))\n    test[c] = lbl.transform(test[c].astype(str))\n    print(c)\n    # 변수로 하나씩 카테고리화? (원핫인코딩)한 다음 된대로 찍은 듯","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"62f0cf85fa61cbde973964ddb7bdfaa828b4ec0a"},"cell_type":"markdown","source":"## 5. <span style=\"color:red\"> M</span>odeling (Main Point of this practice)\n---\n*Classification/Regression*"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f20f53616ff6da53d12228b627b38aadd65b8bfa"},"cell_type":"code","source":"col = [c for c in train.columns if c not in ['item_cnt_month']]\n\n#Validation Hold Out Month\nx1 = train[train['date_block_num']<33]\ny1 = np.log1p(x1['item_cnt_month'].clip(0.,20.))\nx1 = x1[col]\nx2 = train[train['date_block_num']==33]\ny2 = np.log1p(x2['item_cnt_month'].clip(0.,20.))\nx2 = x2[col]\n\nreg = ensemble.ExtraTreesRegressor(n_estimators=25, n_jobs=-1, max_depth=15, random_state=18)\nreg.fit(x1,y1)\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y2.clip(0.,20.),reg.predict(x2).clip(0.,20.))))\n\n#full train\nreg.fit(train[col],train['item_cnt_month'].clip(0.,20.))\ntest['item_cnt_month'] = reg.predict(test[col]).clip(0.,20.)\ntest[['ID','item_cnt_month']].to_csv('submission.csv', index=False)\n\n# RMSE: 0.275956686573","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c31a14a2a6b6752f4c8f154b4efbbaeb322bebf0"},"cell_type":"code","source":"import xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nfrom multiprocessing import *\n\n#XGBoost\ndef xgb_rmse(preds, y):\n    y = y.get_label()\n    score = np.sqrt(metrics.mean_squared_error(y.clip(0.,20.), preds.clip(0.,20.)))\n    return 'RMSE', score\n\nparams = {'eta': 0.2, 'max_depth': 4, 'objective': 'reg:linear', 'eval_metric': 'rmse', 'seed': 18, 'silent': True}\n#watchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\n#xgb_model = xgb.train(params, xgb.DMatrix(x1, y1), 100,  watchlist, verbose_eval=10, feval=xgb_rmse, maximize=False, early_stopping_rounds=20)\n#test['item_cnt_month'] = xgb_model.predict(xgb.DMatrix(test[col]), ntree_limit=xgb_model.best_ntree_limit)\n#test[['ID','item_cnt_month']].to_csv('xgb_submission.csv', index=False)\n\n#LightGBM\ndef lgb_rmse(preds, y):\n    y = np.array(list(y.get_label()))\n    score = np.sqrt(metrics.mean_squared_error(y.clip(0.,20.), preds.clip(0.,20.)))\n    return 'RMSE', score, False\n\nparams = {'learning_rate': 0.2, 'max_depth': 7, 'boosting': 'gbdt', 'objective': 'regression', 'metric': 'mse', 'is_training_metric': False, 'seed': 18}\n#lgb_model = lgb.train(params, lgb.Dataset(x1, label=y1), 100, lgb.Dataset(x2, label=y2), feval=lgb_rmse, verbose_eval=10, early_stopping_rounds=20)\n#test['item_cnt_month'] = lgb_model.predict(test[col], num_iteration=lgb_model.best_iteration)\n#test[['ID','item_cnt_month']].to_csv('lgb_submission.csv', index=False)\n\n#CatBoost\ncb_model = CatBoostRegressor(iterations=100, learning_rate=0.2, depth=7, loss_function='RMSE', eval_metric='RMSE', random_seed=18, od_type='Iter', od_wait=20) \ncb_model.fit(x1, y1, eval_set=(x2, y2), use_best_model=True, verbose=False)\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y2.clip(0.,20.), cb_model.predict(x2).clip(0.,20.))))\ntest['item_cnt_month'] += cb_model.predict(test[col])\ntest['item_cnt_month'] /= 2\ntest[['ID','item_cnt_month']].to_csv('cb_blend_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ea69fcfdf13f1639248d30c8ca7d253349ef5cfe"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1195afdadf915534adfc78d260a3d04fc280e8e3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}