{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering and Initial GBT Model\n\nThis notebook picks up from my first notebook in this competition: [EDA and Previous Value Benchmark](https://www.kaggle.com/noahfinberg/eda-previous-value-benchmark), which achieves a benchmark score of 1.16777. That notebook does basic Exploratory Data Analysis and establishes a previous value benchmark (basically using October 2015 target results as predictions for November 2015). Check out that notebook first if you don't already have a baseline.\n\nThis notebook does some basic feature engineering and trains an initial gradient boosted tree model.\n\n- Creates lag-based features\n- Creates mean-encoded features\n- Train GBT model\n\nI adapt https://www.kaggle.com/dlarionov/feature-engineering-xgboost to fit these three objectives. His notebook does a more comprehensive job with feature engineering, but I wanted to build a simpler initial model.\n\nThere is certainly more feature engineering to do, but I'm building out the model iteratively, applying what I learn from the Coursera course after watching each week's lectures. This notebook corresponds to knowledge gained up through Week 3 of the course. The previous notebook reflects Week 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# using same dependencies as https://www.kaggle.com/dlarionov/feature-engineering-xgboost\n\nimport numpy as np\n\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final project advice #3\n\n---\n\nYou can get a rather good score after creating some lag-based features like in advice from previous week and feeding them into gradient boosted trees model.\n\nApart from item/shop pair lags you can try adding lagged values of total shop or total item sales (which are essentially mean-encodings). All of that is going to add some new information."},{"metadata":{},"cell_type":"markdown","source":"> ## Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/test.csv\")\n# items: item_name, item_id, item_category_id\nitems = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\") \n# categories: item_category_name, item_category_id\ncategories = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv\")\n# train: date, date_block_num, shop_id, item_id, item_price, item_cnt_day**\ntrain = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv\")\n# shops: shop_name, shop_id\nshops = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean the Data\n\n* **Data Types**:\nThe first thing we should do is convert the date column in the training data from an \"object\" to a \"datetime\" dtype.\n\n* **Constant features**:\nThere doesn't appear to be any constant features (features with no variation across all rows)\n\n* **Duplicated features and rows**:\nAs we discovered from the Pandas Profiler, there is no missing data and there are no duplicates\n\n* **Handle Outliers**: Not covered in previous notebook."},{"metadata":{},"cell_type":"markdown","source":"### Outliers"},{"metadata":{},"cell_type":"markdown","source":"Again, see https://www.kaggle.com/dlarionov/feature-engineering-xgboost. I've used his walkthrough as a basis for this one."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day) # check item_cnt_day\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1) #set x-axis between min sales price and max sales price\nsns.boxplot(x=train.item_price)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove outliers. Price > 100,000 and item_cnt_day > 1500. There is also an item with a price below zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1500]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm going to impute negative prices with the global median price."},{"metadata":{"trusted":true},"cell_type":"code","source":"median = train['item_price'].median()\ntrain.loc[train.item_price<0, 'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocess Shop and Item Categories"},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/dlarionov/feature-engineering-xgboost makes a really interesting observation. Each shop name and each category name actual contains multiple useful pieces of information\n\n- Shop names begin with the city they are located in.\n- Category names have both type and subtype of store.\n\nI don't speak Russian so this wasn't immediately obvious to me :)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0]) # extract city from first part of shop name\nshops['city_code'] = LabelEncoder().fit_transform(shops['city']) # give each city a unique encoding\nshops = shops[['shop_id','city_code']] # no more string features -- all label encoded\n\ncategories['split'] = categories['item_category_name'].str.split('-') # splits on dash\ncategories['type'] = categories['split'].map(lambda x: x[0].strip()) # type is the first element in the split on category name\ncategories['type_code'] = LabelEncoder().fit_transform(categories['type']) # give each item category types a unique label\n\n# try to extract a subtype (some category names don't have them)\ncategories['subtype'] = categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip()) # use type is subtype doesn't exist\ncategories['subtype_code'] = LabelEncoder().fit_transform(categories['subtype'])\n\ncategories = categories[['item_category_id','type_code', 'subtype_code']] # no more string features -- all label encoded","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a dataframe - matrix - that represents every possible row in the dataset. In other words, we want every combination of shop_id and item_id pairs for each month."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    \n    sales = train[train.date_block_num==i] # all sales in given date block\n    \n    # matrix is basically an array of every possible tuple (date_block, shop_id, item_id) combination\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols) # turns matrix back into df    \n# downcast bit representations to save memory\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\n\n#sort matrix by date_block_num, then by shop_id, and finally by item_id\nmatrix.sort_values(cols,inplace=True)\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.head())\nprint(matrix.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a new feature \"revenue\" in the training set. Revenue is equal to the number of items sold per day times the price of those items."},{"metadata":{},"cell_type":"markdown","source":"### Preprocess Training Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target variable is the monthly count so we need to group the data by month and then sum across the item_cnt_day to add up each day's sales in the given month."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']}) # aggregates monthly sales per shop and item\ngroup.columns = ['item_cnt_month'] # sets column name\ngroup.reset_index(inplace=True) # resets index to get date_block_num, shop_id, and item_id as columns in the df again\n\n# match up grouped training data with our master matrix of all possible combos (most will have target of zero)\nmatrix = pd.merge(matrix, group, on=cols, how='left')\n\n# fill missing values, clip target between 0,20\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Append Test Set to Matrix\n\nWe need to append the test set (month 34) to the matrix in order to match things up correctly and use \"time tricks.\""},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ntest['date_block_num'] = 34 # represents Nov. 2015\n\n# downcasts to save memory\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)\n\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Append Additional Features to Matrix\n\nIn this case, we want to add the features we extracted earlier from the shops, items, and categories dataframes."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n# join on shops\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\n# join on items\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ts = time.time()\nmatrix = pd.merge(matrix, categories, on=['item_category_id'], how='left')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n# downcast to save memory\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generating Feature Lags\n\nhttps://www.kaggle.com/dlarionov/feature-engineering-xgboost wrote a nice function to lag features. I replicate it here and walk through how it works.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Takes a dataframe, an array of integers representing the magnitudes of all the different lags you want, and the column you want to lag on\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]] # when generating the lag we only need these 4 features\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)] # rename columns (this will generate len(lags) new columns)\n        \n        # change date block number by amount of lag shift\n        shifted['date_block_num'] += i\n        \n        # when we merge back into the original df, it will put enter the lag column at the shifted date_block_num\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mean Encodings\n\nThe below mean encodings are copied from https://www.kaggle.com/dlarionov/feature-engineering-xgboost.\n\nMean encodings provide a nice way to give your features in the model a baseline. Mean encoded features take the average target value for any given category. \n\nSo for example, if shop 1 tends to sell 5000 items per month on average and shop 2 only sells 2000 per month, our feature can now account for that. Some intuition: if we were given a row with shop 1, it'd be better for us to guess the target value at 5000 than at 2000. On average we'd be right most often."},{"metadata":{"trusted":true},"cell_type":"code","source":"# average items sold per month for each data block\n# this feature captures baseline for a given month estimate. \n# Of course we don't have mean encodings for month 34, but encodings for previous months may help, esp. if there is some seasonality in the data\n# E.g. maybe number of items sold in November 2014 is somewhat predictive of number of items sold in November 2015.\nts = time.time()\n\ngroup = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt') # creates a lag for every date block\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True) # no longer need the original encoding\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_type_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'city_code'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\nmatrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\nmatrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check out all the new lag and mean encoding features!\nmatrix.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/dlarionov/feature-engineering-xgboost covers trend features and other special features. That is out of the scope of this notebook. I'll return to these kinds of futures in a later notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the matrix so it's easier to reload later and in future notebooks\nmatrix.to_pickle('data.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost Model\n\nSince we're using a yearlong lag (lag of 12), we only need to really train our model on all months of data after the first 11. In all the previous months, a lag of 12 would mean all NaNs. We could still train our model on lags under 12, but we're guessing that they won't add more predictive power.\n\nIn general, lags create a ton of NaN values so we can fill all remaining NaNs with zeros."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = matrix[matrix.date_block_num > 11]\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check out all the features we currently have\nmatrix.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quick sanity check on what's in matrix\nmatrix.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the matrix so it's easier to reload later\nmatrix.to_pickle('data.pkl')\n# del matrix\n# del group\n# del items\n# del shops\n# del cats\n# del train\n# # leave test for submission\n# gc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reload the preprocessed data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_pickle('data.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pick which features we want.\ndata = data[\n    [\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n#     'ID',\n    'city_code',\n#     'item_name',\n    'item_category_id',\n    'type_code',\n    'subtype_code',\n    'item_cnt_month_lag_1', \n    'item_cnt_month_lag_2',\n    'item_cnt_month_lag_3',\n    'item_cnt_month_lag_6',\n    'item_cnt_month_lag_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_2',\n    'date_item_avg_item_cnt_lag_3',\n    'date_item_avg_item_cnt_lag_6',\n    'date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1',\n    'date_shop_avg_item_cnt_lag_2',\n    'date_shop_avg_item_cnt_lag_3',\n    'date_shop_avg_item_cnt_lag_6',\n    'date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    'date_shop_type_avg_item_cnt_lag_1',\n    'date_shop_subtype_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    'date_type_avg_item_cnt_lag_1',\n    'date_subtype_avg_item_cnt_lag_1']\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validation Approach\n\nWe will us month 34 as the test set. Month 33 can serve as the validation set for our model. We'll train the model on all other months. Before we submit, we should make sure to train the model on month 33 as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del data\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_features(model, (10,14))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}