{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Büyük Veri Final\n\n## Kaggle Üzerinde Proje Geliştirme\n\nMerve TAFRALI - 160202100","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Bu projedeki amacım veri setinde olan verilerden yararlanarak bir sonraki ayın satış tahminlerini gerçekleştirmektir. Öncelikle versetini tanıyabilmek adına veri görselleştirme adımlarını kullanacağım.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from math import ceil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.palplot(sns.husl_palette(10))\nfrom datetime import datetime, date\nfrom dateutil.relativedelta import relativedelta\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom math import ceil\nfrom keras.callbacks import LambdaCallback\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LSTM\nfrom keras.optimizers import RMSprop\n\n%matplotlib inline\n\n#dataları okuma\n\ntrain = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\nsample = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nitem_cats = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Veri Görselleştirme","execution_count":null},{"metadata":{},"cell_type":"raw","source":"Verileri fonksiyon tanımlayarak inceledim","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def bilgi(data):\n    print(\"-----İlk 5 Kayıt-----\")\n    print(data.head(5))\n    print(\"-----Bilgiler-----\")\n    print(data.info())\n    print(\"-----Data Türleri-----\")\n    print(data.dtypes)\n    print(\"-----Eksik Değerler-----\")\n    print(data.isnull().sum())\n    print(\"-----Null Değerler-----\")\n    print(data.isna().sum())\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilgi(train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilgi(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilgi(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilgi(items)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilgi(item_cats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilgi(shops)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Veri setinde biraz düzenleme yapmak için aşağıdaki kodları çalıştırdım.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#outliers\ntrain = train[train.item_price <= 100000]\ntrain = train[train.item_cnt_day <= 1000]\n\n\nmedian = train[(train.shop_id == 32) & (train.item_id == 2973) & (train.date_block_num == 4) & (train.item_price > 0)].item_price.median()\ntrain.loc[train.item_price < 0, 'item_price'] = median\n\ntest_shops = test.shop_id.unique()\ntrain = train[train.shop_id.isin(test_shops)]\ntest_items = test.item_id.unique()\ntrain = train[train.item_id.isin(test_items)]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"İkiliyen dataların çıakrılması için basit bir fonksiyon","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef drop_duplicate(data, subset):\n    print('Before drop shape:', data.shape)\n    before = data.shape[0]\n    data.drop_duplicates(subset,keep='first', inplace=True) \n    data.reset_index(drop=True, inplace=True)\n    print('After drop shape:', data.shape)\n    after = data.shape[0]\n    print('Total Duplicate:', before-after)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subset = ['date', 'date_block_num', 'shop_id', 'item_id','item_cnt_day']\ndrop_duplicate(train, subset = subset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(train['item_cnt_day'],train['item_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_cats.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kategoriler rusça olduğundan daha kolay ve daha rahat görselleştirmek için uygulanan adımlar,","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def r(x):\n    if 'PC' in x:\n        return 'PC'\n    elif 'Live!' in x:\n        return 'Live!'\n    elif 'CD' in x:\n        return 'CD'\n    elif 'PS2' in x:\n        return 'PS2'\n    elif 'PSVita' in x:\n        return 'PSVita'\n    elif 'Windows' in x:\n        return 'Windows'\n    elif 'DVD' in x:\n        return 'DVD'\n    elif 'MP3' in x:\n        return 'MP3'\n    elif 'PS3' in x:\n        return 'PS3'\n    elif 'PSP' in x:\n        return 'PSP'\n    elif 'PS4' in x:\n        return 'PS4'\n    elif 'PSVita' in x:\n        return 'PSVita'\n    elif 'XBOX 360' in x:\n        return 'XBOX 360'\n    elif 'XBOX ONE' in x:\n        return 'XBOX ONE'\n    elif 'Blu-Ray 3D' in x:\n        return 'Blu-Ray 3D'\n    elif 'Blu-Ray 4K' in x:\n        return 'Blu-Ray 4K'\n    \n    else:\n        return 'Others'\n    \n\n    \nitem_cats['item_category_name']=item_cats['item_category_name'].apply(r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(item_cats['item_category_name'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Amacımız aylık satış tahmini olduğundan dolayı her ay ki satışları görmek için bir görselleştirme adımı gerçekleştirilmelidir.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nax = sns.distplot(train.groupby('date_block_num').sum()['item_cnt_day'], color=\"r\")\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Date tipini değiştirmemiz gerekiyor, yukarıda yaptığımız analizleri tekrar uyguluyorum","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train2=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\",index_col='date',parse_dates=['date'])\ntrain2 = train2[train2.shop_id.isin(test_shops)]\ntest_items = test.item_id.unique()\ntrain2 = train2[train2.item_id.isin(test_items)]\ntrain2 = train2[train2.item_price <= 100000]\ntrain2 = train2[train2.item_cnt_day <= 1000]\nmedian = train2[(train2.shop_id == 32) & (train2.item_id == 2973) & (train2.date_block_num == 4) & (train2.item_price > 0)].item_price.median()\ntrain2.loc[train2.item_price < 0, 'item_price'] = median\n\n\ntrain2[\"item_price\"][:'2014-01-01'].plot(figsize=(16,10),legend=True,color='r')\ntrain2[\"item_price\"]['2014-01-01':'2015-01-01'].plot(figsize=(16,10),legend=True,color='b')\ntrain2[\"item_price\"]['2015-01-01':].plot(figsize=(16,10),legend=True,color='g')\nplt.xlabel(\"Dates\")\nplt.ylabel(\"Item price rise\")\nplt.title(\"Item pices vs Date\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Makine Öğrenmesinin Uygulanması","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Üç tür makine öğrenimi vardır\n\n1. Denetimli Makine Öğrenimi\n2. Denetimsiz Makine Öğrenimi\n3. Takviye Makinesi Öğrenimi\n\n**Denetimli Makine Öğrenimi**\n\nHem girdi hem de istenen çıktı verilerinin sağlandığı bir öğrenme türüdür. Girdi ve çıktı verileri, gelecekteki veri işleme için bir öğrenme temeli sağlamak üzere sınıflandırma için etiketlenir. Bu algoritma, belirli bir yordayıcı kümesinden (bağımsız değişkenler) tahmin edilmesi gereken bir hedef / sonuç değişkeninden (veya bağımlı değişken) oluşur. Bu değişkenler setini kullanarak, girişleri istenen çıkışlarla eşleştiren bir fonksiyon üretiyoruz. Eğitim süreci, model eğitim verileri üzerinde istenen bir doğruluk seviyesine ulaşana kadar devam eder.\n\n**Denetimsiz Makine Öğrenimi**\n\nGözetimsiz öğrenme, ne sınıflandırılmamış ne de etiketlenmemiş bilgileri kullanan ve algoritmanın rehberlik olmadan bu bilgiler üzerinde hareket etmesine izin veren bir eğitimdir. Denetimsiz öğrenmenin arkasındaki ana fikir, makineleri büyük miktarlarda çeşitli verilere maruz bırakmak ve öğrenmesine izin vermektir. ve verilerden çıkar. Ancak, makineler öncelikle verilerden öğrenilecek şekilde programlanmalıdır.\n\nGözetimsiz öğrenme sorunları ayrıca kümeleme ve ilişkilendirme sorunları olarak gruplandırılabilir.\n\n**Takviye Makinesi Öğrenimi - Semi-Supervised**\n\nVerisetinin bir kısmını denetimle bir kısmını da denetimsiz öğrenme yardımıyla eğiterek gerçekleştirilir.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\nfrom catboost import Pool\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport gc\nimport itertools\nimport xgboost\nfrom xgboost import plot_importance\nfrom xgboost import XGBRegressor\n\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bu fonksiyon https://www.kaggle.com/gemartin/load-data-reduce-memory-usage adresinde vardır","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n  \n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col]#.astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops = reduce_mem_usage(shops)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Veri setinde düzenlemer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\nitem_cats['split'] = item_cats['item_category_name'].str.split('-')\nitem_cats['type'] = item_cats['split'].map(lambda x: x[0].strip())\nitem_cats['type_code'] = LabelEncoder().fit_transform(item_cats['type'])\n\nitem_cats['subtype'] = item_cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitem_cats['subtype_code'] = LabelEncoder().fit_transform(item_cats['subtype'])\nitem_cats = item_cats[['item_category_id','type_code', 'subtype_code']]\n\nitems.drop(['item_name'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Veri setinin oluşturulması","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfull_data = []\nfor date_block_num in range(34):\n    all_shop_id = train2[train2.date_block_num==date_block_num].shop_id.unique()\n    all_item_id = train2[train2.date_block_num==date_block_num].item_id.unique()\n    full_data.append(np.array(list(itertools.product([date_block_num], all_shop_id, all_item_id))))\n\nfull_data = np.vstack(full_data)\nfull_data = pd.DataFrame(full_data, columns=[\"date_block_num\", \"shop_id\", \"item_id\"])\nfull_data.fillna(0, inplace=True)\nfull_data.sort_values([\"date_block_num\", \"shop_id\", \"item_id\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data = reduce_mem_usage(full_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nfull_data = pd.merge(full_data, group, on=['date_block_num','shop_id','item_id'], how='left')\nfull_data['item_cnt_month'] = (full_data['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20)\n                                .astype(np.float16))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data = pd.concat([full_data, test], ignore_index=True, sort=False, keys=['date_block_num','shop_id','item_id'])\nfull_data.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data.columns, item_cats.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data = pd.merge(full_data, shops, on=['shop_id'], how='left')\nfull_data = pd.merge(full_data, items, on=['item_id'], how='left')\nfull_data = pd.merge(full_data, item_cats, on=['item_category_id'], how='left')\nfull_data['city_code'] = full_data['city_code'].astype(np.int8)\nfull_data['item_category_id'] = full_data['item_category_id'].astype(np.int8)\nfull_data['type_code'] = full_data['type_code'].astype(np.int8)\nfull_data['subtype_code'] = full_data['subtype_code'].astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfull_data = lag_feature(full_data, [1,2,3,6,12], 'item_cnt_month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndate_item_group = full_data.groupby([\"date_block_num\", \"item_id\"]).agg({\"item_cnt_month\": [\"mean\"]})\ndate_item_group.columns = [\"date_item_avg_item_cnt\"]\ndate_item_group.reset_index(inplace=True)\nfull_data = pd.merge(full_data, date_item_group, on=[\"date_block_num\", \"item_id\"], how=\"left\")\nfull_data = lag_feature(full_data, [1, 2, 3, 6, 12], \"date_item_avg_item_cnt\")\nfull_data.drop([\"date_item_avg_item_cnt\"], axis=1, inplace=True)\n\ndate_item_group = full_data.groupby([\"date_block_num\", \"shop_id\"]).agg({\"item_cnt_month\": [\"mean\"]})\ndate_item_group.columns = [\"date_shop_avg_item_cnt\"]\ndate_item_group.reset_index(inplace=True)\nfull_data = pd.merge(full_data, date_item_group, on=[\"date_block_num\", \"shop_id\"], how=\"left\")\nfull_data = lag_feature(full_data, [1, 2, 3, 6, 12], \"date_shop_avg_item_cnt\")\nfull_data.drop([\"date_shop_avg_item_cnt\"], axis=1, inplace=True)\n\ndate_item_group = full_data.groupby([\"date_block_num\", \"item_id\", \"shop_id\"]).agg({\"item_cnt_month\": [\"mean\"]})\ndate_item_group.columns = [\"date_item_shop_avg_item_cnt\"]\ndate_item_group.reset_index(inplace=True)\nfull_data = pd.merge(full_data, date_item_group, on=[\"date_block_num\", \"item_id\", \"shop_id\"], how=\"left\")\nfull_data = lag_feature(full_data, [1, 2, 3], \"date_item_shop_avg_item_cnt\")\nfull_data.drop([\"date_item_shop_avg_item_cnt\"], axis=1, inplace=True)\n\ndate_shop_cat_group = full_data.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ndate_shop_cat_group.columns = ['date_shop_cat_avg_item_cnt']\ndate_shop_cat_group.reset_index(inplace=True)\nfull_data = pd.merge(full_data, date_shop_cat_group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nfull_data = lag_feature(full_data, [1], 'date_shop_cat_avg_item_cnt')\nfull_data.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\n\nitem_group = train.groupby([\"item_id\"]).agg({\"item_price\": [\"mean\"]})\nitem_group.columns = [\"item_avg_item_price\"]\nitem_group.reset_index(inplace=True)\nfull_data = pd.merge(full_data, item_group, on=[\"item_id\"], how=\"left\")\n\ndate_item_group = train.groupby([\"date_block_num\", \"item_id\"]).agg({\"item_price\": [\"mean\"]})\ndate_item_group.columns = [\"date_item_avg_item_price\"]\ndate_item_group.reset_index(inplace=True)\nfull_data = pd.merge(full_data, date_item_group, on=[\"date_block_num\", \"item_id\"], how=\"left\")\n\nlags = [1,2,3,4,5,6]\nfull_data = lag_feature(full_data, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    full_data['delta_price_lag_'+str(i)] = \\\n        (full_data['date_item_avg_item_price_lag_'+str(i)] - full_data['item_avg_item_price']) / full_data['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nfull_data['delta_price_lag'] = full_data.apply(select_trend, axis=1)\nfull_data['delta_price_lag'] = full_data['delta_price_lag'].astype(np.float16)\nfull_data['delta_price_lag'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nfull_data = pd.merge(full_data, group, on=['date_block_num','shop_id'], how='left')\nfull_data['date_shop_revenue'] = full_data['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nfull_data = pd.merge(full_data, group, on=['shop_id'], how='left')\nfull_data['shop_avg_revenue'] = full_data['shop_avg_revenue'].astype(np.float32)\n\nfull_data['delta_revenue'] = (full_data['date_shop_revenue'] - full_data['shop_avg_revenue']) / full_data['shop_avg_revenue']\nfull_data['delta_revenue'] = full_data['delta_revenue'].astype(np.float16)\n\nfull_data = lag_feature(full_data, [1], 'delta_revenue')\n\nfull_data.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data['month'] = full_data['date_block_num'] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nfull_data['days'] = full_data['month'].map(days).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data = full_data[full_data.date_block_num > 11]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nfull_data = fill_na(full_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data.drop([\"ID\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col = [\"date_block_num\", \"shop_id\", \"item_id\", \"item_category_id\", \"month\", \"days\"]\nfull_data[cat_col] = full_data[cat_col].astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = full_data[full_data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = full_data[full_data.date_block_num < 33]['item_cnt_month']\nX_valid = full_data[full_data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_valid = full_data[full_data.date_block_num == 33]['item_cnt_month']\nX_test = full_data[full_data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop([\"delta_price_lag_1\", \"delta_price_lag_2\", \"delta_price_lag_3\", \"delta_price_lag_4\", \"delta_price_lag_5\", \"delta_price_lag_6\"], axis=1, inplace=True)\nX_valid.drop([\"delta_price_lag_1\", \"delta_price_lag_2\", \"delta_price_lag_3\", \"delta_price_lag_4\", \"delta_price_lag_5\", \"delta_price_lag_6\"], axis=1, inplace=True)\nX_test.drop([\"delta_price_lag_1\", \"delta_price_lag_2\", \"delta_price_lag_3\", \"delta_price_lag_4\", \"delta_price_lag_5\", \"delta_price_lag_6\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feats = [i for i, c in enumerate(X_train.columns) if c in cat_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del full_data\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.to_csv(\"X_train.csv\", index=False)\nX_valid.to_csv(\"X_valid.csv\", index=False)\nX_test.to_csv(\"X_test.csv\", index=False)\n\ny_train.to_csv(\"y_train.csv\", index=False)\ny_valid.to_csv(\"y_valid.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nxgb_model = XGBRegressor(max_depth=10,\n                         n_estimators=1000, \n                         min_child_weight=1000,\n                         colsample_bytree=0.7, \n                         subsample=0.7,\n                         eta=0.3, \n                         seed=0)\nxgb_model.fit(X_train, \n              y_train, \n              eval_metric=\"rmse\", \n              eval_set=[(X_train, y_train), (X_valid, y_valid)], \n              verbose=20, \n              early_stopping_rounds=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (10, 14)\nplot_importance(xgb_model)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nxgb_train_pred = xgb_model.predict(X_train)\nxgb_val_pred = xgb_model.predict(X_valid)\nxgb_test_pred = xgb_model.predict(X_test)\n\nprint('Train rmse:', np.sqrt(mean_squared_error(y_train, xgb_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(y_valid, xgb_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model.save_model(\"xgboost\")\ndel xgb_model, xgb_train_pred\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(test['ID'], columns=['ID'])\nsubmission['item_cnt_month'] = xgb_test_pred.clip(0., 20.)\nsubmission.to_csv('submission_xgb.csv', index=False)\nsubmission.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LightGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel=LGBMRegressor(\n        n_estimators=200,\n        learning_rate=0.03,\n        num_leaves=42,\n        colsample_bytree=0.95,\n        subsample=0.8,\n        max_depth=9,\n        reg_alpha=0.03,\n        reg_lambda=0.07,\n        min_split_gain=0.02,\n        min_child_weight=40,\n        silent=False)\n\n\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nlgb_train_pred = model.predict(X_train)\nlgb_val_pred = model.predict(X_valid)\nlgb_test_pred = model.predict(X_test)\n\nprint('Train rmse:', np.sqrt(mean_squared_error(y_train, lgb_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(y_valid, lgb_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.booster_.save_model('lgb_classifier.txt') \ndel model, lgb_train_pred\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test_preds = 0.5 * lgb_test_pred + 0.5 * xgb_test_pred \nfinal_test_preds = final_test_preds.clip(0., 20.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(test['ID'], columns=['ID'])\nsubmission['item_cnt_month'] = final_test_preds\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}