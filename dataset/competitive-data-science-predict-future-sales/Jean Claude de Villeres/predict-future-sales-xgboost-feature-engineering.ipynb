{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Load modules and data\n\n[Predict Future Sales](https://www.kaggle.com/c/competitive-data-science-predict-future-sales) is one of the earlier contests I entered, a very simple regression model can be used but you will likely get a lower score compared to another model create using xgboost with good feature engineering, you can to this notebook as standalone. Credits to [dlarionov](https://www.kaggle.com/dlarionov/feature-engineering-xgboost) for straightforward feature engineering tips: ","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Check datasets\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Load our data\npath = \"/kaggle/input/competitive-data-science-predict-future-sales/\"\n\nitems = pd.read_csv(path + 'items.csv')\nshops = pd.read_csv(path + 'shops.csv')\ncats = pd.read_csv(path + 'item_categories.csv')\ntrain = pd.read_csv(path + 'sales_train.csv')\n\n# Set index to ID to avoid droping it later\ntest = pd.read_csv(path + 'test.csv').set_index('ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"items.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"cats.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check for outliers","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# There are items with strange prices and sales. After detailed exploration \n# I decided to remove items with price > 100000 and sales > 1001 (1000 is ok)\nplt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Let's look at the size of our training data\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Let us remove two rows of outlier data\ntrain = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1001]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Let us check any rows where item_price is < 0\ntrain.loc[train.item_price<0]\n# Found one! We should fix it because it will affect our predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Let us replace this with the average of two prices of the same item\n# Select the entry with same shop_id, item_id, and date_block_num with price > 0 \ntrain[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Compute the median of the two prices\nmedian = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\nmedian","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Substitute the negative price with the median\ntrain.loc[train.item_price<0, 'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check for duplicate shops","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"shops","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"shops.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"shops['shop_name'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"shops['shop_name'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As per above, we have 60 unique shop names, but upon visual inspection that is not the case and we have to set same shop_id for same shops","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Якутск Орджоникидзе, 56\n# Set shop_id to 57 because the name is duplicated\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# Якутск ТЦ \"Центральный\"\n# Set shop_id to 58 because the name is duplicated\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# Жуковский ул. Чкалова 39м²\n# Set shop_id to 11 because the name is duplicated\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# The below code return empty since we have replaced these shop_ids-\n# with the same id as the duplicated shops\ntrain.loc[(train.shop_id == 0) | (train.shop_id == 1) | (train.shop_id == 10)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing for shops, categories, and items ","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"shops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Apparently, the shop_name start with the city name\n# We will use this to create a label encoded feature called 'city_code'\nshops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"shops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"cats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# The categories contains the types as well as its subtype\n# (e.g., type: Аксессуары subtype: PS2)\ncats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n# if subtype is nan then type\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"cats.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Month Sales\n\nTest set is a product of some shops and some items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new compared to the train. Hence, for the most of the items in the test set target value should be zero. \n\nIn the other hand train set contains only pairs which were sold or returned in the past. Tha main idea is to calculate monthly sales and extend it with zero sales for each unique pair within the month. This way train data will be similar to test data.","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"test.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Let us compare test items and train items\n# We convert them to set first and then find set 'intersection'\ntest_item_list = set(test.item_id)\ntrain_item_list = set(train.item_id)\n\nintersection_item_list = test_item_list.intersection(train_item_list)\nlen(intersection_item_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Items in the test item which are not in the train list. \n# Playing with the idea of extending zero sales for such items?\nlen(list(test_item_list - intersection_item_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Returns number of seconds pass since Epoch, 1970\n# Create new matrix/data frame for monthly sales aggregates using train set\nimport time\nts = time.time()\n\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"for i in range(34):\n    # Get train set data and plug it into sales\n    sales = train[train.date_block_num == i]\n    # Get the unique shop and item id to plug into our monthly matrix data\n    # Dtype set to avoid downcasting/changing type after concatenation \n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"matrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"time.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"matrix.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aggregate train set by shop/item pairs to calculate target aggreagates, then clip(0,20) target value. This way train target will be similar to the test predictions.\n\nI use floats instead of ints for item_cnt_month to avoid downcasting it after concatination with the test set later. If it would be int16, after concatination with NaN values it becomes int64, but foat16 becomes float16 even with NaNs.\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Add new feature revenue which is product of item price and item count per day\n# We will use this to create a trend feature\ntrain['revenue'] = train['item_price'] *  train['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Note: We can manipulate our dataframe by grouping by a specific feature and aggregating data\n# Example shows grup of shops with total price\ntrain.groupby(['shop_id']).agg({'item_price': ['sum']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Shows aggregate amount of item count per month\n# This was done using date_block_num grouping as well as shop and item IDs, aggregate SUM\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True); group","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Combine our matrix data with the item_cnt_month\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0) # Label encoding\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test set processing\n\nTo use time tricks append test pairs to the matrix!","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:34:53.96757Z","start_time":"2020-05-03T10:34:53.933578Z"},"trusted":true},"cell_type":"code","source":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:34:54.056434Z","start_time":"2020-05-03T10:34:53.969209Z"},"trusted":true},"cell_type":"code","source":"matrix = pd.concat([matrix, test], ignore_index=True, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Shops, items, categories feature processing","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:35:02.10758Z","start_time":"2020-05-03T10:34:54.058182Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"matrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target Lags","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A lag features is a fancy name for a variable which contains data from prior time steps. If we have time-series data-, \nwe can convert it into rows. Every row contains data about one observation and includes all previous occurrences of that-\nobservation.","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:35:02.140555Z","start_time":"2020-05-03T10:35:02.108909Z"},"trusted":true},"cell_type":"code","source":"# We will create lag features, which will be used later for mean-encoding values. This is a key function\n# After applying lag features, we can see that on month 34. Values for lag 1, 2, 3, 6, and 12 as being used in our prediction!\n# The respective months refer to values value for month(item_cnt_mean), 33(0.568359), 32(2.511719), 31(2.833984), 28(1.977539), and 22(1.299805) \n# (i.e., from last year 12 months ago)\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:35:42.204919Z","start_time":"2020-05-03T10:35:02.142137Z"},"trusted":true},"cell_type":"code","source":"matrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:35:42.238651Z","start_time":"2020-05-03T10:35:42.206603Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:35:42.278267Z","start_time":"2020-05-03T10:35:42.240222Z"},"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:35:42.312669Z","start_time":"2020-05-03T10:35:42.279694Z"},"trusted":true},"cell_type":"code","source":"matrix.columns","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:35:58.848932Z","start_time":"2020-05-03T10:35:42.314253Z"},"trusted":true},"cell_type":"code","source":"matrix.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mean-encoded features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Average item count per month","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:35:59.835419Z","start_time":"2020-05-03T10:35:58.850287Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"# It seems like we will aggregate the item_cnt_month per date_block_num (monthly) column\nmatrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:36:14.338473Z","start_time":"2020-05-03T10:35:59.836941Z"},"trusted":true},"cell_type":"code","source":"# Same code as above, asign to group\n# We take the mean values for all items per month and add that as a lag feature \ngroup = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\n# This aggregate will be added as date_avg_item_cnt\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\n\n# Apply lag using date_avg_item_count column\n# This basically means that we will check the number of items last month when making predictions!\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\n# After apply lag feature, drop the original column\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:36:14.378133Z","start_time":"2020-05-03T10:36:14.340192Z"},"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average item count per month and item","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:36:14.430785Z","start_time":"2020-05-03T10:36:14.37954Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"matrix[matrix.item_id == 5037].groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\n# As shown in code output for item_id 5037, we have entries from month 20 to 34 (target month)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:36:14.657259Z","start_time":"2020-05-03T10:36:14.432145Z"},"trusted":true},"cell_type":"code","source":"# After applying lag features, we can see that on month 34. Values for lag 1, 2, 3, 6, and 12 as being used in our prediction!\n# The respective months refer to values value for month(item_cnt_mean), 33(0.568359), 32(2.511719), 31(2.833984), 28(1.977539), and 22(1.299805) \n# (i.e., from last year 12 months ago)\nmatrix[matrix.date_block_num == 34]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:37:05.568173Z","start_time":"2020-05-03T10:36:14.775246Z"},"trusted":true},"cell_type":"code","source":"# We do the same for all items and months as mentioned above\n# Take note that we are taking the mean values per month and using those as lag features\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average item count per month and shop","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:37:59.318818Z","start_time":"2020-05-03T10:37:05.571984Z"},"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average item count per month and item category","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:38:20.878084Z","start_time":"2020-05-03T10:37:59.320525Z"},"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average item count per month, shop and item category","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:38:43.708904Z","start_time":"2020-05-03T10:38:20.87926Z"},"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_type_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average item count per month, shop, and item sub-type","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:39:07.199133Z","start_time":"2020-05-03T10:38:43.71013Z"},"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average item count per month and city","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:39:30.158947Z","start_time":"2020-05-03T10:39:07.200502Z"},"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'city_code'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average item count per month, item, and city","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:40:01.450535Z","start_time":"2020-05-03T10:39:30.16026Z"},"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average item count per month and item type","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:40:27.211568Z","start_time":"2020-05-03T10:40:01.452142Z"},"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\nmatrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\nmatrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-05-03T10:40:51.83703Z","start_time":"2020-05-03T10:40:27.21298Z"},"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trends features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Price trends for the last six months","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:09:10.502797Z","end_time":"2020-07-13T06:20:02.072043Z"},"trusted":true},"cell_type":"code","source":"group = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\n# Compute delta price lag = (average monthly price per item lag - average price per item) / (average price per item)\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\n# https://stackoverflow.com/questions/31828240/first-non-null-value-per-row-from-a-list-of-pandas-columns/31828559\n# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n# Invalid dtype for backfill_2d [float16]\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:20:02.093175Z","end_time":"2020-07-13T06:20:02.224809Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:20:02.229647Z","end_time":"2020-07-13T06:20:02.368748Z"},"trusted":true},"cell_type":"code","source":"# Select dataset with 'delta' columns\nmatrix.filter(regex='delta')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Last month shop revenue trend","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:20:02.371218Z","end_time":"2020-07-13T06:20:44.683071Z"},"trusted":true},"cell_type":"code","source":"# Compute revenue (revenue = item count * item price)\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Special features\n\nNumber of days in a month. There are no leap years.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:20:44.684886Z","end_time":"2020-07-13T06:20:45.784454Z"},"trusted":true},"cell_type":"code","source":"# Make sure month is 1 to 12\nmatrix['month'] = matrix['date_block_num'] % 12\n\n# Days in a month, account for leap years\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Months since the last sale for each shop/item pair and for item only. I use programing approach.\n\nCreate HashTable with key equals to {shop_id,item_id} and value equals to date_block_num. Iterate data from the top. Foreach row if {row.shop_id,row.item_id} is not present in the table, then add it to the table and set its value to row.date_block_num. if HashTable contains key, then calculate the difference beteween cached value and row.date_block_num.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:20:45.787534Z","end_time":"2020-07-13T06:38:18.777946Z"},"trusted":true},"cell_type":"code","source":"cache = {}\nmatrix['item_shop_last_sale'] = -1\nmatrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = str(row.item_id)+' '+str(row.shop_id)\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n        cache[key] = row.date_block_num","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:38:18.796052Z","end_time":"2020-07-13T06:52:22.11667Z"},"trusted":true},"cell_type":"code","source":"cache = {}\nmatrix['item_last_sale'] = -1\nmatrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = row.item_id\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        if row.date_block_num>last_date_block_num:\n            matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n            cache[key] = row.date_block_num","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Months since the first sale for each shop/item pair and for item only.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:52:22.126529Z","end_time":"2020-07-13T06:52:26.353663Z"},"trusted":true},"cell_type":"code","source":"matrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final preparations\n\nBecause of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:52:26.355464Z","end_time":"2020-07-13T06:52:29.047097Z"},"trusted":true},"cell_type":"code","source":"matrix = matrix[matrix.date_block_num > 11]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Producing lags brings a lot of nulls.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:52:29.049524Z","end_time":"2020-07-13T06:53:21.360347Z"},"trusted":true},"cell_type":"code","source":"def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:53:21.361912Z","end_time":"2020-07-13T06:53:21.366902Z"},"trusted":true},"cell_type":"code","source":"matrix.columns","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:53:21.368711Z","end_time":"2020-07-13T06:53:21.386965Z"},"trusted":true},"cell_type":"code","source":"matrix.info()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:53:21.388702Z","end_time":"2020-07-13T06:53:24.543493Z"},"trusted":true},"cell_type":"code","source":"# Save our matrix\nmatrix.to_pickle('data.pkl')\n\n# # Delete unused variables\n# del matrix\n# del cache\n# del group\n# del items\n# del shops\n# del cats\n# del train\n\n# don't delete test and leave it for submission\n# garbage collection\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost\n\n## Load saved pickle data","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:53:24.545212Z","end_time":"2020-07-13T06:53:40.452686Z"},"trusted":true},"cell_type":"code","source":"data = pd.read_pickle('data.pkl')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:53:40.454759Z","end_time":"2020-07-13T06:53:40.459566Z"},"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Select perfect features","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:53:40.461479Z","end_time":"2020-07-13T06:53:41.409382Z"},"trusted":true},"cell_type":"code","source":"data = data[[\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code',\n    'subtype_code',\n    'item_cnt_month_lag_1',\n    'item_cnt_month_lag_2',\n    'item_cnt_month_lag_3',\n    'item_cnt_month_lag_6',\n    'item_cnt_month_lag_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_2',\n    'date_item_avg_item_cnt_lag_3',\n    'date_item_avg_item_cnt_lag_6',\n    'date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1',\n    'date_shop_avg_item_cnt_lag_2',\n    'date_shop_avg_item_cnt_lag_3',\n    'date_shop_avg_item_cnt_lag_6',\n    'date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    #'date_shop_cat_avg_item_cnt_lag_1',\n    #'date_shop_type_avg_item_cnt_lag_1',\n    #'date_shop_subtype_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    #'date_type_avg_item_cnt_lag_1',\n    #'date_subtype_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month',\n    'days',\n    'item_shop_last_sale',\n    'item_last_sale',\n    'item_shop_first_sale',\n    'item_first_sale',\n]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Validation strategy is 34 month for the test set, 33 month for the validation set and 13-33 months for the train.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:53:41.411069Z","end_time":"2020-07-13T06:53:44.630074Z"},"trusted":true},"cell_type":"code","source":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:53:44.631715Z","end_time":"2020-07-13T06:53:44.728117Z"},"trusted":true},"cell_type":"code","source":"del data\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit our XGBoost model and feed our train and validation data","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:53:44.730524Z","end_time":"2020-07-13T06:57:48.042234Z"},"trusted":true},"cell_type":"code","source":"ts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:57:48.045225Z","end_time":"2020-07-13T06:58:01.872077Z"},"trusted":true},"cell_type":"code","source":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-13T06:58:01.873614Z","end_time":"2020-07-13T06:58:02.502117Z"},"trusted":true},"cell_type":"code","source":"plot_features(model, (10,14))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}