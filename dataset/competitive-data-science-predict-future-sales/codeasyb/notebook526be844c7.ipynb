{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales = '/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv'\ntest = '/kaggle/input/competitive-data-science-predict-future-sales/test.csv'\nsample = '/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(sales)\nprint(df_train.shape)\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.drop(['date_block_num','item_price'], axis=1, inplace=True)\ndf_train['date'] = pd.to_datetime(df_train['date'], dayfirst=True)  \ndf_train['date'] = df_train['date'].apply(lambda x: x.strftime('%Y-%m'))\ndf_train.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_train.groupby(['date','shop_id','item_id']).sum()\ndf = df.pivot_table(index=['shop_id','item_id'], columns='date', \n                    values='item_cnt_day', fill_value=0)\ndf.reset_index(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.merge(df_test, df, on=['shop_id','item_id'], how='left')\n# drop ID so that we have only one column of index\ndf_test.drop(['ID', '2013-01'], axis=1, inplace=True) \ndf_test = df_test.fillna(0) # fill all NAN values to 0.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train = df['2015-10'].values \nX_train = df.drop(['2015-10'], axis = 1)\nX_test = df_test               # test data being compared","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Return a tuple representing the dimensionality of the DataFrame.\nprint('\\n-------------------------------')\nprint(\"Our Dataframes dimensionalities\")\nprint('-------------------------------')\nprint(\"Data DataFrame: {0}\\nTarget Values:  {1}\\nTest Dataframe: {2}\"\n                    .format( X_train.shape, Y_train.shape, X_test.shape))\nprint('-------------------------------\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('----------------------------------')\nprint(\"Starting the training phase\")\nprint('----------------------------------')\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split( X_train, Y_train, \n                                                     test_size=0.2, random_state=101)\nprint ('Train set: ', x_train.shape,  y_train.shape)\nprint ('Test set:  ', x_test.shape,  y_test.shape)\nprint('----------------------------------\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv|')\nprint('|           --------------------------              |')\nprint(\"|           Linear Regression accuracy              |\")\nprint('|           --------------------------              |')\nfrom sklearn.linear_model import LinearRegression\nLR = LinearRegression() # y=mx+b\nLR.fit(x_train,y_train)\n\nfrom sklearn.metrics import mean_squared_error\nLR_train_set = mean_squared_error(y_train, LR.predict(x_train))\nLR_test_set = mean_squared_error(y_test, LR.predict(x_test))\nLR_test_score = LR.score(x_train,y_train)\nprint('|      =====================================        |')\nif LR_train_set >= 10.0: \n    print('|     | Train set mse:  {:.14f}   |       |'.format(LR_train_set))\nelse: print('|     | Train set mse:  {:.14f}    |       |'.format(LR_train_set))\nprint('|     | Test set mse:   {:.14f}    |       |'.format(LR_test_set))\nprint('|     | Test set score: {:.14f}    |       |'.format(LR_test_score))\nprint('|      =====================================        |')\n\nprint('|       ---------------------------------           |')\nprint(\"|       Random Forest Regression accuracy           |\")\nprint('|       ---------------------------------           |')\nfrom sklearn.ensemble import RandomForestRegressor\nRFR = RandomForestRegressor(n_estimators = 10)\nRFR.fit(x_train,y_train)\n\nRFR_train_set = mean_squared_error(y_train, RFR.predict(x_train))\nRFR_test_set = mean_squared_error(y_test, RFR.predict(x_test))\nRFR_test_score = RFR.score(x_train,y_train)\nprint('|      =====================================        |')\nprint('|     | Train set mse:  {:.14f}    |       |'.format(RFR_train_set))\nprint('|     | Test set mse:   {:.14f}    |       |'.format(RFR_test_set))\nprint('|     | Test set score: {:.14f}    |       |'.format(RFR_test_score))\nprint('|      =====================================        |')\nprint('|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv|')\nprint('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('-------------------------------------------------')\nprint(\"Comparing LinearRegression with RandomForestRegressor.\")\nprint('-------------------------------------------------')\nif LR.score(x_train,y_train) < RFR.score(x_train,y_train):\n    print(\"LinearRegression is better than RandomForestRegressor\")\nif RFR.score(x_train,y_train) < LR.score(x_train,y_train):\n    print(\"RandomForestRegressor is better than LinearRegression\")\nprint('-------------------------------------------------')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission = pd.read_csv(sample)\nprediction = RFR.predict(X_test)\ndf_submission['item_cnt_month'] = prediction\ndf_submission.to_csv('prediction.csv', index=False)\nprint(\"Work Done.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}