{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"!!!!!!!!!!!!!!1****redo****!!!!!!!!!!!!!!!!!!!!!!\n\nI'm sure you're well aware of the value of accurate forecasts, but producing them isn't easy. In this document I'll try to outline various basic univariate time series forecasting methods in simple and easy to understand language, assuming you have a basic knowledge of statistics and python.\n\n**Time series data definition**: Data collected on the same metrics or same objects at regular time intervals. It could be stock market records or sales records.\n\n**Univariate Time Series Forecasting**: Only using the previous values in a time series to predict future values (not using any outside variables)."},{"metadata":{},"cell_type":"markdown","source":"# Data Handling"},{"metadata":{},"cell_type":"markdown","source":"### Importing Packages"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd, seaborn as sns\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.stattools import adfuller\nfrom pandas import Series\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading in Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"item_cats = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nsales_train = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\ntest_df = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inspecting the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('item cats')\nprint(item_cats.head())\nprint('items')\nprint(items.head())\nprint('sales train')\nprint(sales_train.head())\nprint('shops')\nprint(shops.head())\nprint('test df')\nprint(test_df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to change the date into a datetime variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.date = sales_train.date.apply(lambda x: datetime.datetime.strptime(x, '%d.%m.%Y'))\nsales_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a deeper look at our sales dataframe:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display\ndisplay(sales_train.head())\ndisplay(sales_train.shape)\ndisplay(sales_train.isnull().any())\ndisplay(sales_train.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nIn this cell we are having a look at the total sales for the company 1C.\nIt appears as though there is a downward trend and seasonality.\n\"\"\"\nts = sales_train.groupby(['date_block_num'])['item_cnt_day'].sum()\nts.astype(float)\n\nrolling_mean = ts.rolling(window = 12).mean() # rolling average of 12 months\nrolling_std = ts.rolling(window = 12).std() # rolling std of 12 months\n\nplt.figure(figsize=(16,8))\nplt.title('Total Sales of 1C')\nplt.xlabel('Month')\nplt.ylabel('Units Sold')\nplt.plot(ts, color = 'blue', label = 'Sales')\nplt.plot(rolling_mean, color = 'red', label = 'Rolling Mean')\nplt.plot(rolling_std, color = 'black', label = 'Rolling Std')\nplt.legend(loc = 'best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(ts.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time series Analysis\n\n## Stationarity\n\n**definition**: The statistical properties of a stationary time series do not change over time. i.e. 2 points in a time series are related to each other by only how far apart they are & not by the direction (each point is independent).\n\nEssentially, the mean, variance, and covariance should remain constant over time. If the data has a trend, it isn't stationary.\n\nThe reason it's important, without going into the math, is that many models rely on stationarity and assume that the data is too.\n\nYou can test for stationarity with the following tests:\n* Augmented Dicky Fuller (ADF)\n* KPSS\n* Philips-Perron (PP)\n\nFor our data I will be performing an ADF test."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nIn this cell we perform the ADF test to check for stationarity. The\nADF tests the null hypothesis that a unit root is present in the\ntime series. i.e. if the p-value is less than 5%, you can reject the\nnull hypothesis and assume that the data is stationary.\n\"\"\"\n\ndef adf_test(ts):\n    print('ADF test results:')\n    adf = adfuller(ts, autolag  = 'AIC')\n    adf_out = pd.Series(adf[0:4], index=['Test Statistic',\n                                        'p-value','#Lags Used',\n                                        'Number of Observations Used'])\n    for key, val in adf[4].items():\n        adf_out['Critical Value (%s)' %key] = val\n    print(adf_out)\n    \nadf_test(ts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The p-value is 14.3%, we therefore can't assume stationarity. "},{"metadata":{},"cell_type":"markdown","source":"## Differencing\n\n**definition**: Differencing is a transformation of a time series, taking the difference between consecutive terms in a series. It can be used to remove time dependency and stabilise the mean, reducing trends and seasonality.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def difference(df, interval=1):\n    diff = [] # Create empty list\n    for i in range(interval, len(df)): # Iterate over every lag\n        val = df[i] - df[i - interval] # Take the difference between consective terms\n        diff.append(val) # Add the new values to the end of the list\n    return Series(diff) # Return the differenced values as a time series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nBelow the original time series is plotted, the same as the plot above.\n\"\"\"\nts.astype(float)\nplt.figure(figsize=(16,16))\nplt.subplot(311)\nplt.title('Original')\nplt.xlabel('Month')\nplt.ylabel('Units Sold')\nplt.plot(ts) # Plot the original time series\n\n\"\"\"\nBelow the new differenced time series is plotted.\n\"\"\"\nnew_ts = difference(ts) # difference the time series\nplt.subplot(312)\nplt.title('Post-differencing')\nplt.xlabel('Month')\nplt.ylabel('Units Sold')\nplt.plot(new_ts)\nplt.plot()\n\n\"\"\"\nBelow the time series is de-seasonalised (assuming the seasonality\n12 months long)\n\"\"\"\nds_ts = difference(ts, interval = 12)\nplt.subplot(313)\nplt.title('After De-seasonalising')\nplt.xlabel('Month')\nplt.ylabel('Units Sold')\nplt.plot(ds_ts)\nplt.plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's test the differenced and deseasonalised series:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Differenced')\nadf_test(new_ts)\n\nprint('\\nDeseasonalised')\nadf_test(ds_ts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ADF test of the deseasonalised data is below 5%, we can therefore reject the null hypothesis and assume the deseasonalised series is stationary. "},{"metadata":{},"cell_type":"markdown","source":"### Considerations\nYou have to be careful not to over-difference the time series. An over-differenced series may still be stationary, but will affect the model parameters (settings).\n\nYou should aim to use the minimum necessary differences to achieve stationarity.\n\n**How do you know if a time series is over differenced?** Optimaly, the Autocorrelation Function (ACF) plot should reach 0 quickly, as seen below. If the first lag (the second pole on the PACF plot) is too far in the negative, then it is probably over-differenced."},{"metadata":{},"cell_type":"markdown","source":"Ok so let's have a look at an over-differenced series:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplot_acf(difference(difference(ts)));\nplt.title('2nd Order Differencing ACF')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As previously described, the first lag goes far into the negative, suggesting that it is over-differenced.\n\nThe deseasonalised series is a much better series to work on:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplot_acf(ds_ts);\nplt.title('Deseasonalised ACF')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Autocorrelation:** autocorrelation summarises the strength of a relationship with an observation in a time series with observations at previous steps.\n\nIn simpler terms: correlation is the strength of a relationship between 2 variables (-1 -> 1), because the correlation of the time series observations are calculated with values of the same series at prior time steps, this is called a serial correlation or *autocorrelation*.\n\n**How to read the above graph:** The ACF plot shows the lag value along the x-axis & the correlation on the y-axis (betweeen -1 and 1). By default the plot_acf function has a 95% confidence interval cone in light blue, suggesting that values outside of this cone are likely a correlation and not a statistical fluke."},{"metadata":{},"cell_type":"markdown","source":"# SARIMA Modeling"},{"metadata":{},"cell_type":"markdown","source":"Now that the time series is differenced, we can move on to building our models.\n\nSeasonal AutoRegressive Integrated Moving Average modeling is an old statistical model that combines a moving average (MA), an auto regressive (AR) model and a seasonal component.\n* MA: Assumes that the next value in the series is a function of the average of the previous n values.\n* AR: Assumes that the next value in the series is a function of the errors (difference in the mean) in the previous n values.\n\nPros:\n* Very effective; remains close to cutting edge performance\n* Simple to implement and not computationally intensive\n\nCons:\n* Not very intuitive\n* No way to build in our understanding about how our data works:\n    * random walk element\n    * external regressors"},{"metadata":{},"cell_type":"markdown","source":"## How does the SARIMA model work?\nThere are 3 important terms in ARIMA models: p, d & q\n* **p** is the order of the AR term\n* **q** is the order of the MA term\n* **d** is the number of times differencing is required to make the time series stationary\n* **s** the seasonal component is comprised of:\n    * P - The seasonal autoregressive order\n    * D - The seasonal difference order\n    * Q - The Seasonal moving average order\n    * m - The number of time steps in a single seasonal period\n\n**What do these terms mean?**\nThe AR part in ARIMA is a linear regression model that uses its own lags (previous time steps) as predictors. For a linear regression model to be effective you need the predictors to be independent of each other (not correlated), i.e. the time series needs to be stationary.\n\nA common and effective way to make a time series stationary is to difference it (subtract the previous value from the current value). Depending on how complex the series is you may need more than one differencing. **d** is the minimum number of differences needed for the data to be stationary, so if it is stationary by default; d = 0.\n\n**p** is the order of the AR term and refers to the number of lags (time steps) of Y (the dependent (the variable you're trying to forecast)) to be used as predictors.\n\n**q** is the order of the MA terms and refers to the number of lagged forecast errors that should go into the ARIMA model.\n\nAn ARIMA model is a model that is differenced at least once and combines the MA and AR terms.\n\npredicted Yt = Constant + linear combination of lags of Y (up to p lags) + linear combination of lagged forecast errors (up to q lags)\n\n[source: https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/ ]"},{"metadata":{},"cell_type":"markdown","source":"### Estimating the differencing term (d)\n\nIt is possible to use packages to estimate the number of differences required. We can use the function \"ndiffs()\" to perform a test of stationarity for different levels of d (and different tests) and estimate the number of differences required to make the time series stationary. As seen by the results below it doesn't always work, we know from the above tests that d is neither 1,2 or 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfrom pmdarima.arima.utils import ndiffs, nsdiffs\n\n# Normal Differencing:\n\n# ADF test\nd_adf = ndiffs(ts, test='adf') # = 1\n\n# KPSS test\nd_kpss = ndiffs(ts, test='kpss') # = 2\n\n# PP test\nd_pp = ndiffs(ts, test='pp') # = 0\n\nprint('Difference Estimations:\\nADF:%s KPSS:%s PP:%s' % (d_adf,d_kpss,d_pp))\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding the AR term (p)\nWe find p by analysing the Partial AutoCorrelation Function (PACF) plot.\n\n**PACF explanation:** Autocorrelation for an observation & another observation at a prior time step is comprised of both the direct correlation & indirect correlations. The indirect correlations are a linear function of the correlation of the observation with observations at intervening time steps.\n\nIt is these indirect correlations that the PACF seeks to remove. The correlation between point Y0 and Y1 will have seome inertia and affect points later on.\n\nIn short, the PACF kind of conveys the pure correlation between an observation and the series. That way you will know if the obsevation is needed in the AR term or not.\n\n**How do we find p?:** Any autocorrelation in a stationary time series can be fixed by adding enough AR terms. So we initially take the order of the AR term to be equal to the number of lags that cross the significance limit in the PACF plot.\n\nTime series analysis is a bit of an art, there isn't a set methodlogy that you have to follow, many people analyse the ACF and PACF plots to find certain patterns that may give away the right order, but it is also possible to systematically find the correct order, although it is rather computationally intensive."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nLooping over possible values of p and q and measuring their AIC.\n\nAIC can be thought of like mean squared error, it measures on average\nhow far off the prediction is from the actual result.\n'''\nimport statsmodels.api as sm\nimport warnings\n\nrng = range(5)\nbest_aic = np.inf\nbest_model = None\nbest_order = None\n\nwarnings.filterwarnings('ignore')\n\nfor i in rng:\n    for j in rng:\n        temp_model = sm.tsa.statespace.SARIMAX(ds_ts, order = (i, 0, j))\n        results = temp_model.fit()\n        temp_aic = results.aic\n        if temp_aic < best_aic:\n            best_aic = temp_aic\n            best_order = (i, 0, j)\n            best_model = temp_model\n\nprint('Best AIC: %s | Best order: %s' % (best_aic, best_order))\n\nwarnings.warn('Reinstating warnings')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nSo in the above code cell we determined that p & q were best set at 1.\nEarlier on with the ADF test we found that we needed to perform a seasonal difference with the interval set to 12.\n\nWe supplied the SARIMAX function with 3 parameters here; order, trend and seasonal order.\n* The order parameter is just a copy of the results above.\n* I chose the trend through trial and error, setting it to 't' gave me the best results.\n* The seasonal order is (P,D,Q,m) where m is the number of time steps, 12 in our case. We set d to 1 because we only need 1\n  seasonal difference and p & q are already used in the order parameter. We could supply seasonal P & Q but it's important\n  not to make the model too complex and cause overfitting.\n\"\"\"\nsarima_model = sm.tsa.statespace.SARIMAX(ts, order = (1,0,1),trend = 't', seasonal_order=(0,1,0,12))\nresults = sarima_model.fit()\nprint(results.aic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best practice is to split the data into a training and testing set prior to fitting the model to validate it's accuracy, however I do want to keep this brief."},{"metadata":{},"cell_type":"markdown","source":"## Forecasting Sales for 1C"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nWe'll predict from the 22nd month, 2 years into the future.\n'''\nfrom statsmodels.tsa.statespace.sarimax import SARIMAXResults\n\n\npreds = SARIMAXResults.predict(results, start = 33, end = 46)\n\n\nax = ts.plot(label = 'Observed')\npreds.plot(ax = ax, label = 'SARIMA forecast')\nplt.legend()\nplt.title('1C Sales')\nax.set_xlabel('Month')\nax.set_ylabel('Units Sold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prophet Forecasting\n\nIn February 2017 Facebook's Data Science team open sourced their forecasting library \"Prophet\". It's a highly optimised package to quickly perform forecasting on non-stationary data."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nBefore forecasting we need to add the dates back into the time-series\n'''\nts.index = pd.date_range(start = '2013-01-01', \n                         end = '2015-10-01', \n                         freq = 'MS')\nts = ts.reset_index()\nts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet # Import the package\n\n# Prophet requires you to name your columns the following:\nts.columns = ['ds','y']\nprophet_model = Prophet(yearly_seasonality = True) # As determined in stationarity testing\nprophet_model.fit(ts)\n\n# We'll predict 12 months into the future\n# 'MS' = month start\nfuture = prophet_model.make_future_dataframe(periods = 12, freq = 'MS')\nforecast = prophet_model.predict(future)\nforecast.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prophet_model.plot(forecast);\nplt.title('1C Sales - Prophet Forecast')\nplt.xlabel('Date')\nplt.ylabel('Units Sold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prophet_model.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = sales_train.groupby(['date_block_num'])['item_cnt_day'].sum()\nax = ts.plot(label = 'Observed')\npreds.plot(ax = ax, label = 'SARIMA forecast', alpha = 0.9, linestyle = '-')\nforecast.yhat[33:46].plot(ax = ax, label = 'Prophet forecast', alpha = 0.9, linestyle = '--')\n\nplt.legend()\nplt.title('1C Sales')\nax.set_xlabel('Month')\nax.set_ylabel('Units Sold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems as though SARIMA does a better job of generalising and appears to be the simpler model, although Prophet is much easier to implement."},{"metadata":{},"cell_type":"markdown","source":"# More Complex Forecasting (Competition Entry)\n\n\nWe have to take the sales_train data and preprocess it and transform it so we can train a model to predict the test_df (below).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(test_df.head())\ndisplay(sales_train.tail())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning the Data"},{"metadata":{},"cell_type":"markdown","source":"### Removing outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see below that there's significant outliers that must\n# be removed\n\n# Plotting\nplt.boxplot(sales_train.item_price)\n\n# Removing Outlier\nsales_train = sales_train[(sales_train.item_price < 300000)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting\nplt.boxplot(sales_train.item_cnt_day)\n\n# Removing outlier\nsales_train = sales_train[(sales_train.item_cnt_day < 1000)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking for Duplicates\n\nWe have 6 rows that are duplicated that we might have to address, however 6 rows in a dataset this big will unlikely make a material difference."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sales_train[sales_train.duplicated()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Downcasting the dataset\n\nWe can significantly reduce the size of the dataset by changing the datatypes of variables down from 64 bits to 16 & 32. This will make training our model much faster.\n\n[Source for below: [kyakovlev](https://www.kaggle.com/kyakovlev/1st-place-solution-part-1-hands-on-data)]"},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast(df):\n    # Identifies whether the column is a float or int\n    float_cols = [x for x in df if df[x].dtype == 'float64']\n    int_cols = [x for x in df if df[x].dtype in ['int64', 'int32']]\n    \n    # Downsized them to their 32 & 16 bit equivalent\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\nsales_train = downcast(sales_train)\nprint(sales_train.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inspecting the shops\n\nIt's somewhat difficult to see, given that it's in Russian, but some of the shop names are duplicated (e.g. shop 10 & 11). Maybe they've changed location or there's been an error, but it's probably a good idea to change it so it resembles the testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nDuplicated shops:\n0 = 57\n1 = 58\n10 = 11\n40 = 39\n'''\nshops","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_shops(df):\n    # Replace 0 with 57\n    df.loc[df.shop_id == 0, 'shop_id'] = 57\n    # Replace 1 with 58\n    df.loc[df.shop_id == 1, 'shop_id'] = 58\n    # Replace 10 with 11\n    df.loc[df.shop_id == 10, 'shop_id'] = 11\n    # Replace 40 with 39\n    df.loc[df.shop_id == 40, 'shop_id'] = 39\n    return df\n\n# Perform the same changes to training & testing set\nreplace_shops(sales_train)\nreplace_shops(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inspecting changes (no 0, 1, 10, or 40)\nsales_train['shop_id'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Adding in City name and Category of Shop**\n\nReading over the #1 notebook, we can add the city and shop type to the shops dataframe. I had to borrow this, given that I don't read Russian.\n\n[Source: [kyakovlev](https://www.kaggle.com/kyakovlev/1st-place-solution-part-1-hands-on-data)]\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Double spaces and removes special characters \nshops['shop_name'] = shops['shop_name'].apply(lambda x: x.lower()).str.replace('[^\\w\\s]', '').str.replace('\\d+','').str.strip()\n\n# Adds in the city name\nshops['shop_city'] = shops['shop_name'].str.partition(' ')[0]\n\n# Adds the shop category\nshops['shop_type'] = shops['shop_name'].apply(lambda x: 'мтрц' if 'мтрц' in x else 'трц' if 'трц' in x else 'трк' if 'трк' in x else 'тц' if 'тц' in x else 'тк' if 'тк' in x else 'NO_DATA')\nshops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nENCODING\n\nHere we're going to encode the shop_city & shop_type variables.\nIn short, the model doesn't understand what \"тц\" means, so we \nassign each category of shop type a number. So all \"тц\" shops could\nbe assigned the number 4, and when that number comes up the model knows\nthat it's in the same group as the other observations with the number 4.\n\"\"\"\nfrom sklearn.preprocessing import LabelEncoder\nshops['shop_city'] = LabelEncoder().fit_transform(shops.shop_city)\nshops['shop_type'] = LabelEncoder().fit_transform(shops.shop_type)\n\n\"\"\"\nWe don't need the shop_name, so we'll just remove it\n\"\"\"\nshops = shops[['shop_id','shop_city','shop_type']]\nshops.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inspecting the items\n\nI had to refer back to the #1 notebook on this, it's much simpler than using google translate on dozens of russian words. In this section we extract features from the item names (e.g. what they have in their brackets).\n\n[Source for below: [kyakovlev](https://www.kaggle.com/kyakovlev/1st-place-solution-part-1-hands-on-data)]"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(items.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re # importing regex to identify text with certain patterns\n\ndef rename(text):\n    text = text.lower() # convert to lower case\n    text = text.partition('[')[0] # Split at square bracket\n    text = text.partition('(')[0] # Split at regular bracket\n    text = re.sub('[^A-Za-z0-9А-Яа-я]+', '  ', text) # remove special characters (e.g. !)\n    text = text.replace('  ',' ') # replace double space with single\n    text = text.strip()\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the item name by the first bracket\nitems['name1'], items['name2'] = items.item_name.str.split('[',1).str\nitems[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n\n# Convert text to lowercase & remove special characters\nitems[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\nitems[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\n\n# impute empty cells with '0'\nitems = items.fillna('0')\n\n# Correct the item names,\n# See if needed #items[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n\n# Cuts off the last 2 characters of name2 unless it's 0\nitems.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"source: [dordotron85](https://www.kaggle.com/gordotron85/future-sales-xgboost-top-3)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pulls the item type from name (in square brackets)\nitems[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\n\n# ID's when the item is an xbox, mac, pc or playstation\nitems.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nitems.loc[ items.type == \"\", \"type\"] = \"mac\"\nitems.type = items.type.apply( lambda x: x.replace(\" \", \"\") )# Removes spaces\nitems.loc[ items.type == 'pc', \"type\" ] = \"pc\"\nitems.loc[ items.type == 'рs3' , \"type\"] = \"ps3\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group the dataset by type & count the number of each item id\ncat_counts = items.groupby(['type']).agg({'item_id':'count'})\ncat_counts = cat_counts.reset_index()\n\nbad_cats = []\n\n# Counts whether each category has at least 40 observations, if not it labels it as other\nfor cat in cat_counts.type.unique():\n    if cat_counts.loc[(cat_counts.type == cat), 'item_id'].values[0] < 40:\n        bad_cats.append(cat)\n\nitems.name2 = items.name2.apply(lambda x: 'other' if (x in bad_cats) else x)\nitems = items.drop(['type'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.name2 = LabelEncoder().fit_transform(items.name2)\nitems.name3 = LabelEncoder().fit_transform(items.name3)\n\nitems.drop(['item_name','name1'], axis = 1, inplace = True)\nitems.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}