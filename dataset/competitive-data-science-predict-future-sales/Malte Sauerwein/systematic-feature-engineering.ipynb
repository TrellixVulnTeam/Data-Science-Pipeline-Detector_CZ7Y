{"cells":[{"metadata":{"_cell_guid":"cd6f454b-0155-4a26-a0f7-c589a16d95a2","_uuid":"093f451f-7248-4287-ab41-fc7da1f0f147"},"cell_type":"markdown","source":"# PREDICT FUTURE SALES\n# PART I: Feature Engineering"},{"metadata":{"_cell_guid":"8b66dc77-b220-404b-be80-f651fb54486d","_uuid":"b3eba069-4de1-45f7-b960-98edbb23aa19"},"cell_type":"markdown","source":"## Description\nThis challenge serves as final project for the \"How to win a data science competition\" Coursera course.\nIn this competition you will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company. \nWe are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.\n\n## Evaluation\nSubmissions are evaluated by root mean squared error (RMSE). True target values are clipped into [0,20] range."},{"metadata":{},"cell_type":"markdown","source":"## Reference"},{"metadata":{"trusted":true},"cell_type":"code","source":"# useful links:\n# https://pbpython.com/pandas-qcut-cut.html\n# https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\n# https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n# https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"47608b8c-725b-4b9d-80d2-f6fabfe1583b","_uuid":"e17f11eb-ee72-46ce-99e3-7144cbccc48e"},"cell_type":"markdown","source":"# Setup"},{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"_cell_guid":"ff1b65a7-8092-4286-a348-d4f771ee843c","_uuid":"ed56cad8-f949-48f3-89a0-d1bc239927f0","scrolled":true,"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom itertools import product\nimport seaborn as sns\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nimport gc\nimport pickle\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nsns.set(context='talk')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH_TO_INPUT_FILES = '../input/competitive-data-science-predict-future-sales'\nDTYPES_FOR_COLUMN_NAMES = pd.Series({\n    r'avg_':'float16',\n    r'^date_block_num$':'uint8',\n    r'^item_id$':'uint16',\n    r'^shop_id$':'uint8',\n    r'item_cnt_month':'uint8',\n    r'^item_id_bucket$':'uint8',\n    r'^shop_city$':'uint8',\n    r'^item_category_id$':'uint8',\n    r'^item_category_group$':'uint8',\n    r'^item_first_sold$':'uint8',\n    r'^item_sold_before$':'uint8', \n    r'^item_sold_before_in_shop$':'uint8',\n    r'^month$':'uint8',\n    r'^year$':'uint16',\n    r'^daysinmonth$':'uint8',\n    r'^quarter$':'uint8',\n})\nRUSSIAN_REGEX_PATTERN = '[^а-яА-Я0-9]+'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_csv_from_input(file_name):\n    return pd.read_csv(os.path.join(PATH_TO_INPUT_FILES, file_name))\n\ndef print_df_info(df, name_df='DF'):\n    print('DATAFRAME:',name_df)\n    print(f'\\n-----------------{name_df} INFO-----------------\\n')\n    print(df.info())\n    print(f'\\n-----------------{name_df} STATISTICS-----------------\\n')\n    print(df.describe())\n    print(f'\\n-----------------{name_df} COUNT NAN VALUES-----------------\\n')\n    print(df.isna().sum())\n    print(f'\\n-----------------{name_df} SAMPLE-----------------\\n')\n    print(df.sample(5))\n\ndef drop_columns_in_df(df, cols):\n    for col in cols:\n        if col in df.columns:\n            df.drop(col, axis=1, inplace=True)\n\ndef print_columns_in_df(df):\n    for col in df.columns:\n        print(f\"'{col}',\")\n\ndef transform_dtypes_in_df(df, col_dtype_pairs=DTYPES_FOR_COLUMN_NAMES):\n    for (key,value) in col_dtype_pairs.items():\n        for col in df.columns:\n            if re.search(key, col):\n                df[col] = df[col].astype(value)\n    return df\n\ndef rel_value_counts_in_df(df):\n    return df.value_counts() / len(df)\n\ndef merge_rare_cat_values(df_col, threshold=0.005, replace_by='other'):\n    rel_value_counts = rel_value_counts_in_df(df_col)\n    rare_cat_values = rel_value_counts[rel_value_counts < threshold].index\n    return df_col.replace(rare_cat_values, replace_by)\n\ndef clean_word(word):\n    return re.sub(RUSSIAN_REGEX_PATTERN, '', word)\n\ndef transform_column(df_column, dtype):\n    if dtype:\n        df_column = df_column.astype(dtype)\n    return df_column\n\ndef fillna_column(df_column, fillna_value):\n    if fillna_value is not None:\n        df_column = df_column.fillna(fillna_value)\n    return df_column\n\ndef get_column_name_from_merge_cols(merge_cols, column_name=''):\n    column_name += '_'.join(merge_cols)\n    return column_name\n\ndef get_buckets_from_continuous_feature(df_col, n_bins):\n    '''\n    Create buckets from continuous features (binning).\n    qcut: equal distribution of the items in your bins\n    cut: define your own numeric bin ranges\n    '''\n    labels_bins = [(f'bin_{n_bin}') for n_bin in range(n_bins)]\n    return pd.cut(df_col, bins=n_bins, labels=labels_bins)\n\n# The following function is mainly copied from\n# https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\ndef reduce_mem_usage(props, float_type=np.float32, verbose=0):\n    if verbose:\n        start_mem_usg = props.memory_usage().sum() / 1024**2 \n        print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            if verbose:\n                # Print current column type\n                print(\"******************************\")\n                print(\"Column: \",col)\n                print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes float_type\n            else:\n                props[col] = props[col].astype(float_type)\n            \n            if verbose:\n                # Print new column type\n                print(\"dtype after: \",props[col].dtype)\n                print(\"******************************\")\n    if verbose:\n        # Print final result\n        print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n        mem_usg = props.memory_usage().sum() / 1024**2 \n        print(\"Memory usage is: \",mem_usg,\" MB\")\n        print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return props","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize the item_price and item_cnt_day"},{"metadata":{"_cell_guid":"3aa9fd69-c157-4dc1-a7da-56e02ca92e1e","_uuid":"ac68ea6a-176a-46ac-b1ee-036284101bdb","scrolled":true,"trusted":true},"cell_type":"code","source":"def plot_item_price_cnt_day(df_sales):\n    fig, (ax_price, ax_cnt_day) = plt.subplots(ncols=2, figsize=(18,5))\n    plot_item_price = sns.boxplot(df_sales['item_price'], ax=ax_price)\n    plot_item_cnt_day = sns.boxplot(df_sales['item_cnt_day'], ax=ax_cnt_day)   ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0669e221-9aa1-48e1-bccc-5160350f6b9d","_uuid":"691ce8d3-2126-4847-83ba-23844133d922"},"cell_type":"markdown","source":"## Delete outliers in training data"},{"metadata":{"_cell_guid":"9b72b95c-80d6-4a69-a847-0db1a583efe0","_uuid":"a76ecbd8-01da-4329-b911-69d94e1c1cd2","scrolled":true,"trusted":true},"cell_type":"code","source":"def delete_outliers_from_quantiles(df_sales):\n    '''\n    The item_price and the item_cnt_day are filtered according to the specified quantile,\n    such that outliers are deleted from the data set.\n    '''\n    # Define quantiles for data range.\n    quantiles_to_filter_outliers = [0.005, 0.995]\n    min_item_price, max_item_price = df_sales['item_price'].quantile(quantiles_to_filter_outliers)\n    min_item_cnt_day, max_item_cnt_day = df_sales['item_cnt_day'].quantile(quantiles_to_filter_outliers)\n\n    # df_sales_filtered corresponds to the cleaned data set.\n    filter_query = '(@min_item_price <= item_price <= @max_item_price)' \\\n                 + ' & (@min_item_cnt_day <= item_cnt_day <= @max_item_cnt_day)'\n    df_sales_filtered = df_sales.query(filter_query).copy()\n    return df_sales_filtered\n\ndef delete_outliers_from_absolute_values(df_sales):\n    df_sales_filtered = df_sales \\\n        .query('(item_price < 100000) & (item_price > 0) & (item_cnt_day < 1001)') \\\n        .copy()\n    return df_sales_filtered","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d9bdda56-3813-4911-ba7c-460d5481cbe4","_uuid":"2291fc15-47d5-4ac0-b3ed-ea4307c9b82f"},"cell_type":"markdown","source":"# Get monthly data"},{"metadata":{},"cell_type":"markdown","source":"## Create shop-item gird from train set"},{"metadata":{"_cell_guid":"ad6c6863-295a-494c-9a7b-60a84207a7fe","_uuid":"cd2d888f-21ce-4c19-a0f6-39a409e26232"},"cell_type":"markdown","source":"Up to now, only sold products are tracked in the training data set. Therefore, two possible data augmentation strategies can be followed\n1. each shop should contain all items that are sold in that specific month, i.e. the product portfolio changes every month\n2. each shop should contain all items that are sold in the whole period, i.e. the product portfolio is constant\n\nNext, we are following the first approach:"},{"metadata":{"_cell_guid":"8abf10d9-35eb-4631-af57-70cf50077c0a","_uuid":"6c91e22f-fe0f-46a3-8110-db5999d6b180","scrolled":true,"trusted":true},"cell_type":"code","source":"def create_shop_item_grid(df_sales):\n    '''\n    Create grid for shop-item pairs.\n    '''\n    matrix = []\n    merge_cols = ['date_block_num', 'item_id', 'shop_id']\n    date_block_nums = np.sort(df_sales['date_block_num'].unique())\n    for date_block_num in date_block_nums:\n        sales = df_sales.query('date_block_num == @date_block_num').copy()\n        matrix.append(\n            np.array(\n                list(\n                    product(\n                        [date_block_num],\n                        sales['item_id'].unique(),\n                        sales['shop_id'].unique(),\n                    )\n                )\n            )\n        )\n        \n    matrix = pd.DataFrame(np.vstack(matrix), columns=merge_cols, dtype=np.int16)\n    matrix.sort_values(merge_cols,inplace=True)\n\n    # get monthly sold items per shop_id and item_id\n    group = df_sales \\\n        .groupby(['date_block_num', 'item_id', 'shop_id']) \\\n        .agg({'item_cnt_day':'sum'}) \\\n        .rename(columns={'item_cnt_day':'item_cnt_month'}) \\\n        .reset_index()\n\n    # merge with shop_id-item_id pairs \n    matrix = pd.merge(\n        matrix,\n        group,\n        on=merge_cols,\n        how='left'\n    )\n\n    matrix['item_cnt_month'] = matrix['item_cnt_month'] \\\n        .fillna(0) \n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"232b4dde-946c-47d1-9437-7f96e233152e","_uuid":"c4e0245b-4f77-4235-87fc-a388ff2392de"},"cell_type":"markdown","source":"## Add unlabeled data to grid"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def concat_with_unlabeled_data(matrix, df_unlabeled, date_block_num=None):\n    if date_block_num is not None:\n        df_unlabeled['date_block_num'] = date_block_num\n    if 'item_cnt_month' not in df_unlabeled.columns:\n        df_unlabeled['item_cnt_month'] = 0.0\n    matrix = pd.concat([matrix, df_unlabeled], ignore_index=True, sort=False)\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e0630e4d-cd4e-474f-a33d-ca98cb9a6617","_uuid":"268438e9-b787-4c79-abf9-f56bf51aa046"},"cell_type":"markdown","source":"# Additional feature engineering"},{"metadata":{"_cell_guid":"b12dd2bc-b4b6-4c9a-b52e-4b10071af209","_uuid":"8b14e3ba-407a-4354-92e5-2d3b4d90d977"},"cell_type":"markdown","source":"## Add date-based features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_with_date_features(matrix, df_sales):\n    df = df_sales.copy()\n    df['date'] = pd.to_datetime(df['date'], format='%d.%m.%Y')\n    start_date = df['date'].min()\n    df_time = pd.DataFrame()\n    df_time['date'] = pd.date_range(start=start_date, periods=100, freq='M') \n    df_time['date_block_num'] = df_time.index            \n    df_time['month'] = df_time['date'].dt.month\n    df_time['year'] = df_time['date'].dt.year\n    df_time['daysinmonth'] = df_time['date'].dt.daysinmonth\n    df_time['quarter'] = df_time['date'].dt.quarter\n    df_time.drop('date', axis=1, inplace=True)\n    df_time = reduce_mem_usage(df_time, float_type=np.float16, verbose=0)\n    return pd.merge(matrix, df_time, on='date_block_num', how='left')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8c1889dc-090d-4422-bf4e-cdb6a37a5899","_uuid":"0e4a44b3-3774-4982-9d83-dd63cf6ddbc2"},"cell_type":"markdown","source":"## Add item-based features"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def merge_with_item_features(matrix, df_items, df_item_cat):\n    df_items_and_cats_info = pd.merge(df_items, df_item_cat, on='item_category_id', how='left')\n    df_items_and_cats_info['item_category_group'] = df_items_and_cats_info['item_category_name'] \\\n        .str.split('-').map(lambda x: x[0])\n    drop_columns_in_df(df_items_and_cats_info, ['item_category_name','item_name'])\n    matrix = pd.merge(matrix, df_items_and_cats_info, on=['item_id'], how='left')\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add shop-based features "},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_duplicated_shops(df, duplicated_shops):\n    for (shop_id, shop_id_duplicate) in duplicated_shops:\n        df.loc[df['shop_id'] == shop_id, 'shop_id'] = shop_id_duplicate\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_num_words_in_shop_name(df_shops):\n    n_words = df_shops['shop_name'].str.split(\" \").map(lambda x: len(x)).astype('category')\n    fig, ax = plt.subplots()\n    ax = n_words.hist()\n    ax.set_xlabel('number of words in shop_name')\n    plt.xticks(n_words.cat.categories);","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def merge_with_shop_features(matrix, df_shops):\n    df_shops['shop_city'] = df_shops['shop_name'].str.split(\" \").map(lambda x: x[0].lower()).apply(clean_word)\n    drop_columns_in_df(df_shops, ['shop_name'])\n    matrix = pd.merge(matrix, df_shops, on=['shop_id'], how='left')\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_duplicated_shops(df_shops, n_extra_info=1):\n    '''\n    The duplicated shop_ids are derived from the shop_city, shop_category and n_extra_infos.\n    The shop_city, the shop_category as well as extra_infos are extracted from\n    the shop_name. n_extra_info determines the number of additional infos. \n    '''\n    df = df_shops.copy()\n    extra_info_cols = [f'extra_info_{n}' for n in range(n_extra_info)]\n    cols = ['shop_city', 'shop_category'] + extra_info_cols\n    cols_iter = iter(cols)\n    for n in range(len(cols)):\n        df[next(cols_iter)] = df['shop_name'].str.split(' ').map(lambda x: x[n].lower() if (len(x) > n) else ' ').apply(clean_word)\n    duplicated_shops_bools = df \\\n        .duplicated(keep=False, subset=cols)                    \n    duplicated_shops = df[duplicated_shops_bools] \\\n        .groupby(cols)['shop_id'].apply(lambda x: list(x))       \n    return duplicated_shops","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add first-sold features"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def merge_with_first_sold_features(matrix):\n    group = matrix \\\n        .query('item_cnt_month > 0') \\\n        .groupby('item_id') \\\n        .agg({'date_block_num': 'min'}) \\\n        .reset_index()\n    group['item_first_sold'] = 1\n    group['date_block_num_item_first_sold'] = group['date_block_num'].astype(np.int8)\n    matrix = pd.merge(\n        matrix,\n        group.drop('date_block_num_item_first_sold',axis=1),\n        on=['item_id', 'date_block_num'],\n        how='left'\n    )\n    matrix = pd.merge(\n        matrix,\n        group.drop(['date_block_num', 'item_first_sold'],axis=1),\n        on=['item_id'], how='left'\n    )   \n    matrix['item_first_sold'].fillna(0, inplace=True) \n    matrix['date_block_num_item_first_sold'].fillna(100, inplace=True)\n    group = matrix \\\n        .query('item_cnt_month > 0') \\\n        .groupby(['shop_id','item_id']) \\\n        .agg({'date_block_num': 'min'}) \\\n        .rename(columns={'date_block_num':'date_block_num_item_first_sold_in_shop'}) \\\n        .reset_index()\n\n    matrix = pd.merge(matrix, group, on=['shop_id', 'item_id'], how='left')\n    matrix['item_sold_before'] = matrix.eval('date_block_num > date_block_num_item_first_sold').astype(np.int8)\n    matrix['item_sold_before_in_shop'] = matrix.eval('date_block_num > date_block_num_item_first_sold_in_shop').astype(np.int8)\n    matrix['item_first_sold'] = matrix.eval('date_block_num > date_block_num_item_first_sold_in_shop').astype(np.int8)\n    drop_columns_in_df(matrix, [\n        'date_block_num_item_first_sold',\n        'date_block_num_item_first_sold_in_shop',\n    ])\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ab3de026-5c0a-4830-8012-1d8f8f568bed","_uuid":"71232896-0fda-4e3c-8010-5067dd3306ef"},"cell_type":"markdown","source":"## Add price-related features\n"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"class PriceRelatedFeatures(object):\n    def __init__(self, df_sales):\n        self.df_sales = df_sales\n        \n    def merge_with_avg_price_wrt_merge_cols(self, matrix, merge_cols,\n                                            dtype=None, fillna_value=None):\n        '''\n        Average price with respect to merge_cols. \n        '''\n        col = get_column_name_from_merge_cols(merge_cols, column_name='avg_price_wrt_')                \n        group = self.df_sales \\\n                .groupby(merge_cols) \\\n                .agg({'item_price':'mean'}) \\\n                .rename(columns={'item_price': col}) \\\n                .reset_index() \n        matrix = pd.merge(matrix, group, on=merge_cols, how='left')\n        matrix[col] = fillna_column(matrix[col], fillna_value)\n        matrix[col] = transform_column(matrix[col], dtype)\n        return matrix\n\n    def merge_with_rel_price_wrt_merge_cols(self, matrix, merge_cols_1, merge_cols_2,\n                                            dtype=None, fillna_value=None):\n        '''\n        Relative average price: col_3 = (col_1 - col_2) / col_2\n        ''' \n        col_1 = get_column_name_from_merge_cols(merge_cols_1, column_name='avg_price_wrt_')\n        col_2 = get_column_name_from_merge_cols(merge_cols_2, column_name='avg_price_wrt_')\n        col_3 = get_column_name_from_merge_cols(merge_cols_2, column_name='rel_avg_price_')\n        matrix[col_3] = matrix.eval(f'({col_1} - {col_2}) / {col_2}')\n        matrix[col_3] = fillna_column(matrix[col_3], fillna_value)\n        matrix[col_3] = transform_column(matrix[col_3], dtype)\n        return matrix\n    \n    def merge_with_rel_avg_price_lag(self, matrix, merge_cols_1, merge_cols_2, lags=[1,2,3]):\n        matrix = self.merge_with_avg_price_wrt_merge_cols(matrix,\n                                                          merge_cols=merge_cols_1,\n                                                          dtype=np.float16, fillna_value=None)\n\n        matrix = self.merge_with_avg_price_wrt_merge_cols(matrix,\n                                                          merge_cols=merge_cols_2,\n                                                          dtype=np.float16, fillna_value=None)\n\n        matrix = self.merge_with_rel_price_wrt_merge_cols(matrix,\n                                                          merge_cols_1=merge_cols_1,\n                                                          merge_cols_2=merge_cols_2,\n                                                          dtype=np.float16,fillna_value=0)\n\n        matrix = lag_feature(matrix, lags,\n                             get_column_name_from_merge_cols(merge_cols_2, 'rel_avg_price_'),\n                             dtype=np.float16, fillna_value=0)\n        drop_columns_in_df(matrix,\n                           [get_column_name_from_merge_cols(merge_cols_1, 'avg_price_wrt_'),\n                            get_column_name_from_merge_cols(merge_cols_2, 'avg_price_wrt_'),\n                            get_column_name_from_merge_cols(merge_cols_2, 'rel_avg_price_')])\n        return matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add revenue-related features"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"class RevenueRelatedFeatures(object):\n    def __init__(self, df_sales):\n        self.df_sales = df_sales\n        self.df_sales['revenue'] = self.df_sales.eval('item_price * item_cnt_day')\n        \n    def merge_with_tot_revenue_wrt_date_block_num_shop_id(self, matrix,\n                                                          dtype=None, fillna_value=None):\n        merge_cols = ['date_block_num', 'shop_id']\n        col = 'tot_revenue_wrt_date_block_num_shop_id'\n        group = self.df_sales \\\n                .groupby(merge_cols) \\\n                .agg({'revenue': 'sum'}) \\\n                .rename(columns={'revenue': col}) \\\n                .reset_index()\n        matrix = pd.merge(matrix, group, on=merge_cols, how='left')\n        matrix[col] = fillna_column(matrix[col], fillna_value)\n        matrix[col] = transform_column(matrix[col], dtype)            \n        return matrix\n        \n    def merge_with_avg_tot_revenue_wrt_shop_id(self, matrix, \n                                               dtype=None, fillna_value=None,\n                                               drop_tot_revenue=True):\n        \n        if 'tot_revenue_wrt_date_block_num_shop_id' not in matrix.columns:\n            matrix = self.merge_with_tot_revenue_wrt_date_block_num_shop_id(matrix, dtype, fillna_value)\n        col = 'avg_tot_revenue_wrt_shop_id'\n        merge_cols = ['shop_id']\n        group = matrix \\\n            .groupby(merge_cols) \\\n            .agg({'tot_revenue_wrt_date_block_num_shop_id': 'mean'}) \\\n            .rename(columns={'tot_revenue_wrt_date_block_num_shop_id': col}) \\\n            .reset_index()\n        matrix = pd.merge(matrix, group, on=merge_cols, how='left')\n        matrix[col] = fillna_column(matrix[col], fillna_value)\n        matrix[col] = transform_column(matrix[col], dtype)\n        if drop_tot_revenue:\n            drop_columns_in_df(matrix, ['tot_revenue_wrt_date_block_num_shop_id'])\n        return matrix\n        \n    def merge_with_rel_shop_revenue(self, matrix,\n                                    dtype=None, fillna_value=None):\n        '''\n        col_1 = 'tot_revenue_wrt_date_block_num_shop_id'\n        col_2 = 'avg_tot_revenue_wrt_shop_id'\n        Relative shop revenue: col_3 = (col_1 - col_2) / col_2\n        '''\n        col_1 = 'tot_revenue_wrt_date_block_num_shop_id'\n        col_2 = 'avg_tot_revenue_wrt_shop_id'\n        col_3 = 'rel_avg_tot_revenue_wrt_shop_id'\n        matrix[col_3] = matrix.eval(f'({col_1} - {col_2}) / {col_2}')\n        matrix[col_3] = fillna_column(matrix[col_3], fillna_value)\n        matrix[col_3] = transform_column(matrix[col_3], dtype)\n        return matrix\n        \n    def merge_with_rel_avg_tot_shop_revenue_lag(self, matrix, lags=[1,2,3]):\n        col_1 = 'tot_revenue_wrt_date_block_num_shop_id'\n        col_2 = 'avg_tot_revenue_wrt_shop_id'\n        col_3 = 'rel_avg_tot_revenue_wrt_shop_id'\n        matrix = self.merge_with_avg_tot_revenue_wrt_shop_id(matrix, \n                                                             dtype=np.float16, fillna_value=None,\n                                                             drop_tot_revenue=False)\n        matrix = self.merge_with_rel_shop_revenue(matrix,dtype=np.float16, fillna_value=0)            \n        matrix = lag_feature(matrix, lags,col_3,\n                             dtype=np.float16, fillna_value=0)\n        drop_columns_in_df(matrix,[col_1, col_2, col_3])\n        return matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add monthly item count lags"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def merge_with_avg_item_cnt_wrt_merge_cols(matrix, merge_cols,\n                                           dtype=None, fillna_value=None):\n    '''\n    Average item count with respect to merge_cols\n    '''\n    col = get_column_name_from_merge_cols(merge_cols, column_name='avg_item_cnt_wrt_')\n    group = matrix \\\n            .groupby(merge_cols) \\\n            .agg({'item_cnt_month':'mean'}) \\\n            .rename(columns={'item_cnt_month': col}) \\\n            .reset_index()\n    matrix = pd.merge(matrix, group, on=merge_cols, how='left')\n    matrix[col] = fillna_column(matrix[col], fillna_value)\n    matrix[col] = transform_column(matrix[col], dtype)\n    return matrix\n\ndef merge_with_avg_item_cnt_lag(matrix, merge_cols, lags=[1,2,3]):\n    matrix = merge_with_avg_item_cnt_wrt_merge_cols(matrix,\n                                                    merge_cols=merge_cols,\n                                                    dtype=np.float16, fillna_value=None)\n \n    matrix = lag_feature(matrix, lags,\n                         get_column_name_from_merge_cols(merge_cols, 'avg_item_cnt_wrt_'),\n                         dtype=np.float16, fillna_value=0)\n    drop_columns_in_df(matrix, [get_column_name_from_merge_cols(merge_cols, 'avg_item_cnt_wrt_')])\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add item count for new items"},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_with_avg_item_cnt_of_new_items_wrt_merge_cols(matrix, merge_cols, \n                                                        dtype=None, fillna_value=None):\n    '''\n    Average item count of new items with respect to merge_cols\n    '''\n    col = get_column_name_from_merge_cols(merge_cols, column_name='avg_item_cnt_new_wrt_')\n    group = matrix \\\n        .query('item_first_sold == 1') \\\n        .groupby(merge_cols) \\\n        .agg({'item_cnt_month':'mean'}) \\\n        .rename(columns={'item_cnt_month': col}) \\\n        .reset_index()\n    matrix = pd.merge(matrix, group, on=merge_cols, how='left')\n    matrix[col] = fillna_column(matrix[col], fillna_value)\n    matrix[col] = transform_column(matrix[col], dtype)\n    return matrix\n\ndef merge_with_avg_item_cnt_of_new_items_lag(matrix, merge_cols, lags=[1,2,3]):\n    matrix = merge_with_avg_item_cnt_of_new_items_wrt_merge_cols(matrix,\n                                                                 merge_cols=merge_cols,\n                                                                 dtype=np.float16, fillna_value=None)\n\n    matrix = lag_feature(matrix, lags,\n                         get_column_name_from_merge_cols(merge_cols, 'avg_item_cnt_new_wrt_'),\n                         dtype=np.float16, fillna_value=0)\n    drop_columns_in_df(matrix, [get_column_name_from_merge_cols(merge_cols, 'avg_item_cnt_new_wrt_')])\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d2f644bf-e7a8-4d48-b61a-955856130302","_uuid":"7ec16ec1-faf6-4802-b067-4223d28d2bcd"},"cell_type":"markdown","source":"## Lag functions"},{"metadata":{"_cell_guid":"c40d4977-6fa3-4406-a649-14932bb5b1ee","_uuid":"351f5693-5c81-4836-a8f6-110d21be2ea5","scrolled":true,"trusted":true},"cell_type":"code","source":"def lag_feature(df, lags, col,\n                merge_cols=['date_block_num','shop_id','item_id'],\n                dtype=None, fillna_value=None):\n    '''\n    The function returns the lag of a feature and merges it to the \n    input dataframe.\n    '''\n    tmp = df[merge_cols + [col]].copy()\n    index_col = 'date_block_num'\n    for lag in lags:\n        shifted = tmp.copy()\n        col_lag = f'{col}_lag_{lag}'\n        shifted[index_col] = shifted[index_col].map(lambda x: x+lag)\n        shifted.rename(columns={col: col_lag}, inplace=True)\n        df = pd.merge(df, shifted, on=merge_cols, how='left')\n        df[col_lag] = fillna_column(df[col_lag], fillna_value)\n        df[col_lag] = transform_column(df[col_lag], dtype)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_avg_feature(df, lag_delta, col,\n                    merge_cols=['date_block_num','shop_id','item_id'],\n                    dtype=None, fillna_value=None):\n    '''\n    The function returns the mean value from the lag of a feature.\n    First, the lags of the feature are computed, while in a second step\n    the lag values are averaged row-wise. In the last step, the averaged\n    lag feature is merged to the input dataframe.\n    '''\n    tmp = df[merge_cols + [col]].copy()\n    agg_fun='mean'\n    index_col = 'date_block_num'\n    lag_avg_col = f'lag_avg_{value_col}'\n    for lag in np.arange(-lag_delta, lag_delta+1):\n        shifted = tmp.copy()\n        col_lag = f'{col}_lag_{lag}'\n        shifted[index_col] = shifted[index_col].map(lambda x: x+lag)\n        shifted.rename(columns={col: col_lag}, inplace=True)\n        df = pd.merge(df, shifted, on=merge_cols, how='left')\n        df[col_lag] = transform_column(df[col_lag], dtype)\n    pattern = rf'{col}_lag_'\n    lag_cols = [col for col in df.columns if re.search(pattern, col)]\n    df[lag_avg_col] = df[lag_cols].agg(agg_fun, axis=1)\n    df[lag_avg_col] = fillna_column(df[lag_avg_col], fillna_value)\n    df[lag_avg_col] = transform_column(df[lag_avg_col], dtype)\n    drop_columns_in_df(df, lag_cols)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Advanced lag feature motivated by kaggle kernal:\n# https://www.kaggle.com/uladzimirkapeika/feature-engineering-lightgbm-top-1\ndef lag_feature_adv(df, lags, col,\n                    shift = -1,\n                    merge_cols=['date_block_num','shop_id','item_id'],\n                    dtype=None, fillna_value=None):\n    '''\n    The function returns the lag of a feature with respect to a neighboring\n    item_id. The lag is finally merged with the input dataframe.\n    ''' \n    shift_col = 'item_id'\n    index_col = 'date_block_num'\n    tmp = df[merge_cols + [col]].copy()\n    tmp[shift_col] = tmp[shift_col].map(lambda x: x+shift)\n    for lag in lags:\n        col_lag_adv = f'{col}_adv_lag_{lag}'\n        shifted = tmp.copy()\n        shifted[index_col] = shifted[index_col].map(lambda x: x+lag)\n        shifted.rename(columns={col: col_lag_adv}, inplace=True)\n        df = pd.merge(df, shifted, on=merge_cols, how='left')\n        df[col_lag_adv] = fillna_column(df[col_lag_adv], fillna_value)\n        df[col_lag_adv] = transform_column(df[col_lag_adv], dtype)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Execute functions"},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import csv files\ndf_sales = read_csv_from_input('sales_train.csv')\ndf_item_cat = read_csv_from_input('item_categories.csv')\ndf_items = read_csv_from_input('items.csv')\ndf_shops = read_csv_from_input('shops.csv')\ndf_test = read_csv_from_input('test.csv')\n\n# preprocessing\ndf_sales['date'] = pd.to_datetime(df_sales['date'], format='%d.%m.%Y')\ndrop_columns_in_df(df_test, ['ID'])\n\n# remove duplicated shops\nduplicated_shops = get_duplicated_shops(df_shops, n_extra_info=1)\ndf_sales = remove_duplicated_shops(df_sales, duplicated_shops)\ndf_test = remove_duplicated_shops(df_test, duplicated_shops)\n\n# remove outliers \ndf_sales = delete_outliers_from_quantiles(df_sales)\n\n# monthly shop-item pairs\nmatrix = create_shop_item_grid(df_sales)\n\n# add test data\nmatrix = concat_with_unlabeled_data(matrix, df_test, date_block_num=34)\n\n# add bucket for item_id\nmatrix['item_id_bucket'] = get_buckets_from_continuous_feature(matrix['item_id'], n_bins=25).astype('object')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# shop features\nmatrix = merge_with_shop_features(matrix, df_shops)\n\n# item features\nmatrix = merge_with_item_features(matrix, df_items, df_item_cat)\nmatrix['item_category_group'] = merge_rare_cat_values(matrix['item_category_group'], threshold=0.005, replace_by='other')\n\n# categorical feature encoding\nencoder = {}\nfor cat_feature in ['shop_city', 'item_category_group', 'item_id_bucket']:\n    encoder[cat_feature] = LabelEncoder()\n    matrix[cat_feature] = encoder[cat_feature].fit_transform(matrix[cat_feature]).astype(np.int32)\n\n# first-sold features\nmatrix = merge_with_first_sold_features(matrix)\n\n# date features\nmatrix = merge_with_date_features(matrix, df_sales)\n\n# reduce size\nmatrix = transform_dtypes_in_df(matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Advanced feature engineering\n### Price- and revenue related features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# price-related features\nrel_avg_price_merge_cols_1 = [\n    ['date_block_num', 'item_id'],\n    ['date_block_num', 'item_category_id'],\n]\nrel_avg_price_merge_cols_2 = [\n    ['item_id'],\n    ['item_category_id'],\n]\npriceRelatedFeatures = PriceRelatedFeatures(merge_with_item_features(df_sales, df_items, df_item_cat))\nfor (m_1, m_2) in zip(rel_avg_price_merge_cols_1, rel_avg_price_merge_cols_2):\n    matrix = priceRelatedFeatures.merge_with_rel_avg_price_lag(matrix,\n                                                               merge_cols_1=m_1,\n                                                               merge_cols_2=m_2,\n                                                               lags=[1,2,3])\nmatrix = priceRelatedFeatures.merge_with_avg_price_wrt_merge_cols(matrix,\n                                                                  ['item_id'],\n                                                                  dtype=np.float16)\nmatrix = priceRelatedFeatures.merge_with_avg_price_wrt_merge_cols(matrix,\n                                                                  ['item_category_id'],\n                                                                  dtype=np.float16)\n# Fill NaN values in column with values from different column\nmatrix['avg_price_wrt_item_id'] = matrix['avg_price_wrt_item_id'].combine_first(matrix['item_category_id'])\n\n# revenue-realted features\nrevenueRelatedFeatures = RevenueRelatedFeatures(df_sales)\nmatrix = revenueRelatedFeatures.merge_with_rel_avg_tot_shop_revenue_lag(matrix, lags=[1,2,3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average item cnt features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# item cnt lag feature\nmatrix = lag_feature(matrix, [1, 2, 3], 'item_cnt_month', dtype=np.float16, fillna_value=0)\n\n# average item cnt lag\navg_item_cnt_merge_cols = [\n    ['date_block_num', 'item_id'],\n    ['date_block_num', 'item_id_bucket'],\n    ['date_block_num', 'item_id', 'shop_city'],\n    ['date_block_num', 'item_id', 'shop_id'],\n    ['date_block_num', 'item_category_id', 'shop_id'],\n    ['date_block_num', 'item_category_group', 'shop_id'],\n]\nfor m in avg_item_cnt_merge_cols:\n    matrix = merge_with_avg_item_cnt_lag(matrix, merge_cols=m, lags=[1,2,3])\n\n# average item cnt for new items\navg_item_cnt_of_new_items_merge_cols = [\n    ['date_block_num', 'item_category_id', 'shop_id'],\n    ['date_block_num', 'shop_id'],\n]\nfor m in avg_item_cnt_of_new_items_merge_cols:\n    matrix = merge_with_avg_item_cnt_of_new_items_lag(matrix, merge_cols=m, lags=[1,2,3])\n\n# advanced item cnt lag feature\nmatrix = lag_feature_adv(matrix, [1, 2, 3], 'item_cnt_month', dtype=np.float16, fillna_value=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Postprocessing\n* Reduce memory usage.\n* Delete missing data from dataframe.\n* Clip data to range.\n* Data set contains lagged features, such that the first months cannot be used as training data. "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"matrix.fillna(0, inplace=True)\nmatrix = transform_dtypes_in_df(matrix)\nmatrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0, 20)\nmatrix = matrix.query('date_block_num > 2').copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save data set and encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.to_pickle('data_after_feature_eng.pkl')\nwith open('cat_encoder.pkl', 'wb') as handle:\n    pickle.dump(encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.info()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}