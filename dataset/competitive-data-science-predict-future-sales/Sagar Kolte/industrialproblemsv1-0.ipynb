{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\nitems = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nmain_df = pd.merge(main_df,items,on='item_id')\nmain_df = main_df.drop(['item_id','item_name'],axis=1)\nmain_df_grouped = main_df.groupby(['date_block_num','item_category_id','shop_id'])[\"item_price\",\"item_cnt_day\"].agg({\"item_price\":\"mean\",\"item_cnt_day\":\"sum\"}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_prev_sales(shop_id,item_category_id,date_block_num,lag,df):\n    df = df[(df['shop_id']==shop_id)&(df['item_category_id']==item_category_id)&(df['date_block_num']==date_block_num-lag)]\n    #df = main_df.query('shop_id == @shop_id and item_id == @item_id and date_block_num==@date_block_num-@lag')\n    if len(df)==0:\n        return np.nan\n    else:\n        #print(len(df))\n        return np.sum(df['item_cnt_day'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df_grouped['month-1'] = main_df_grouped.apply(lambda row:get_prev_sales(row['shop_id'],row['item_category_id'],row['date_block_num'],1,df=main_df_grouped),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,6):\n    main_df_grouped['month-'+str(i)] = main_df_grouped.apply(lambda row:get_prev_sales(row['shop_id'],row['item_category_id'],row['date_block_num'],i,df=main_df_grouped),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df_grouped.to_csv('prepared_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df_grouped.dropna(subset=['month-1','month-2','month-3','month-4','month-5'],how='any')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM,Dense,Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model = Sequential()\nmy_model.add(LSTM(units = 1,input_shape = (5,1)))\n#my_model.add(Dropout(0.4))\nmy_model.add(Dense(1))\n\nmy_model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\nmy_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Colab Content Start"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas import read_csv\nimport math\nimport tensorflow as tf\nimport tensorflow.keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adapted from https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu\n\nPATH_TPU_WORKER = ''\n\ndef check_tpu():\n    \"\"\"\n    Detect TPU hardware and return the appopriate distribution strategy\n    \"\"\"\n    \n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n        print('Running on TPU: {}'.format(tpu.master()))\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n        tpu_strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\n    print(\"Num. replicas: {}\".format(tpu_strategy.num_replicas_in_sync))\n    \n    return tpu, tpu_strategy\n    \ntpu, tpu_strategy = check_tpu()\nPATH_TPU_WORKER = tpu.master()\nNUM_REPLICAS = tpu_strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/prepared-data/prepared_data.csv',engine='python')\ndata = data.dropna(subset=['month-1','month-2','month-3','month-4','month-5'],how='any')\ndata = data.drop(columns=['Unnamed: 0','date_block_num','item_price','item_category_id','shop_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = data.values\ndataset = dataset.astype('float32')\ndatasetX, datasetY = dataset[:,[1,2,3,4,5]],dataset[:,0]\n# normalize the dataset\nx_scaler = MinMaxScaler()\ny_scaler = MinMaxScaler()\ndatasetX = x_scaler.fit_transform(datasetX)\ndatasetY = y_scaler.fit_transform(datasetY.reshape(-1, 1))\n# split into train and test sets\ntrain_size = int(len(dataset) * 0.67)\ntest_size = len(dataset) - train_size\ntrainX, testX = datasetX[0:train_size,:], datasetX[train_size:len(datasetX),:]\ntrainY, testY = datasetY[0:train_size,:], datasetY[train_size:len(datasetY),:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape input to be [samples, time steps, features]\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create and fit the LSTM network\nmodel = Sequential()\nmodel.add(LSTM(4, input_shape=(1, 5)))\n#model.add(Dropout(0.4))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n# make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# invert predictions\ntrainPredict = y_scaler.inverse_transform(trainPredict)\ntrainY = y_scaler.inverse_transform(trainY)\ntestPredict = y_scaler.inverse_transform(testPredict)\ntestY = y_scaler.inverse_transform(testY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sqrt(((trainPredict-trainY)**2).sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Colab Content End"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}