{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos a crear todas las funciones que necesitamos para empezar a trabajar sobre el dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# make calendar maps\n!pip install calmap","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import calmap\n# Main libraries that we will use in this kernel\nimport datetime\nimport numpy as np\nimport pandas as pd\n\n# # garbage collector: free some memory is needed\nimport gc\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# pip install squarify (algorithm for treemap) if missing\nimport squarify\n\n# statistical package and some useful functions to analyze our timeseries\nfrom statsmodels.tsa.stattools import pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport statsmodels.tsa.stattools as stattools\n\nimport time\n\nfrom xgboost import XGBRegressor\nfrom string import punctuation\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\n\ndef print_files():\n    import os\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's correct the shops df and also generate a few more features\ndef fix_shops(shops):\n    '''\n    This function modifies the shops df inplace.\n    It correct's 3 shops that we have found to be 'duplicates'\n    and also creates a few more features: extracts the city and encodes it using LabelEncoder\n    '''\n    \n    d = {0:57, 1:58, 10:11, 23:24}\n    \n    # this 'tricks' allows you to map a series to a dictionary, but all values that are not in the dictionary won't be affected\n    # it's handy since if we blindly map the values, the missings values will be replaced with nan\n    shops[\"shop_id\"] = shops[\"shop_id\"].apply(lambda x: d[x] if x in d.keys() else x)\n    \n    # replace all the punctuation in the shop_name columns\n    shops[\"shop_name_cleaned\"] = shops[\"shop_name\"].apply(lambda s: \"\".join([x for x in s if x not in punctuation]))\n    \n    # extract the city name\n    shops[\"city\"] = shops[\"shop_name_cleaned\"].apply(lambda s: s.split()[0])\n    \n    # encode it using a simple LabelEncoder\n    shops[\"city_id\"] = LabelEncoder().fit_transform(shops['city'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a simple function that creates a global df with all joins and also shops corrections\ndef create_df():\n    '''\n    This is a helper function that creates the train df.\n    '''\n    # import all df\n    shops = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\")\n    fix_shops(shops) # fix the shops as we have seen before\n    \n    items_category = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv\")\n    items = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\")\n    sales = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv\")\n    \n    # fix shop_id in sales so that we can leater merge the df\n    d = {0:57, 1:58, 10:11, 23:24}\n    sales[\"shop_id\"] = sales[\"shop_id\"].apply(lambda x: d[x] if x in d.keys() else x)\n    \n    # create df by merging the previous dataframes\n    df = pd.merge(items, items_category, left_on = \"item_category_id\", right_on = \"item_category_id\")\n    df = pd.merge(sales, df, left_on = \"item_id\", right_on = \"item_id\")\n    df = pd.merge(df, shops, left_on = \"shop_id\", right_on = \"shop_id\")\n    \n    # convert to datetime and sort the values\n#     df[\"date\"] = pd.to_datetime(df[\"date\"], format = \"%d.%m.%Y\")\n    df.sort_values(by = [\"shop_id\", \"date\"], ascending = True, inplace = True)\n    \n    # reduce memory usage\n#     df = reduce_mem_usage(df)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = create_df()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['date']=pd.to_datetime(df['date'], format=\"%d.%m.%Y\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 1: Create a plot with the moving average of total sales (7 days) and the variation on the second axis.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Nos hacemos nuestro subconjunto de datos para pintar las medias moviles de los ultimos 7 dias y su variacion:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ma7d = df[[\"date\", \"item_cnt_day\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ponemos como indice el campo fecha (date)\ndf_ma7d.set_index(\"date\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ma7d.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como ya tenemos como índice el campo fecha, podemos hacer el \"resample\" de los datos y agrupamos por dias. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Como solo tengo una variable en este dataframe, podemos hacer el resample agrupando por dias asi:\ndf_ma7d = df_ma7d.resample(\"D\").sum()\n# Si tuvieramos mas variables en el dataframe, lo podriamos hacer asi: \n# df_ma7d = df_ma7d.resample(\"D\")['item_cnt_day'].sum().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vemos que han quedado agrupados por dia con el sumatorio de las ventas:\ndf_ma7d.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora ya podemos hacer el rolling para que nos haga las medias moviles en este caso a 7 días:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ma7d['MA7D']=df_ma7d['item_cnt_day'].rolling(window = 7).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vemos que han quedado agrupados por dia y su correspondiente MA:\ndf_ma7d.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vemos que los 6 primeros días el valor de la media movil es nulo, como debe de ser.\nAhora vamos a calcular su variacion:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate the intra day variation between total sales\ndf_ma7d[\"Variation\"] = df_ma7d[\"MA7D\"].diff()/df_ma7d[\"MA7D\"].shift(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vemos que han quedado agrupados por dia, su correspondiente MA7D y su variacion diaria:\ndf_ma7d.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Borro los NaN\ndf_ma7d.dropna(axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparamos la figura\nfig = plt.figure(figsize = (15, 10))\nax = fig.add_subplot(111)\n\n# pintamos las medias moviles a 7 dias\nplot1 = ax.plot(df_ma7d[\"MA7D\"], label = \"MA7D sales\", color = \"blue\", alpha = 0.5)\n\n# creamos un eje secundario y pintamos la variacion diaira de las medias moviles a 7 dias:\nax_bis = ax.twinx()\nplot2 = ax_bis.plot(df_ma7d[\"Variation\"], label = \"Intra - day variation MA7D\", color = \"red\", alpha = 0.5)\n\n# create a common legend for both plots\nlns = plot1 + plot2\nlabs = [l.get_label() for l in lns]\nax.legend(lns, labs, loc = \"upper left\")\n\n# añadimos el titulo\nax.set_title(\"Total MA7D sales and day variation\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 2: Create a decomposition plot for a city of weekly sales","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Vamos a buscar la ciudad con mas ventas de todas:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(['city_id']).size().sort_values(ascending=False).to_frame().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vemos que es la que tiene id 13, vamos a ver cual es:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[ df['city_id']==13]['city'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vemos que es Moscu. Vamos a crearnos nuestro dataframe con los datos que necesitamos:\nPara esto primero agrupamos por la ciudad, sobre este agrupamiento (Como ya tenemos por indice el campo fecha) hacemos un \"resample\" por semanas y sumamos. \nAsi tenemos los datos agrupamos por ciudades y semanas: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_timeindex_city=df.set_index('date').groupby('city_id').resample(\"W\")[\"item_cnt_day\"].sum().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_timeindex_city.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como me molesta el indice tupla de este dataframe,lo manipulo un poco para dejarlo solo con la fecha como indice y la ciudad como variable:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lo formateamos un poco:\ndf_timeindex_city.reset_index(inplace=True)\ndf_timeindex_city.set_index('date',inplace=True)\n# Vemos los datos para Moscu:\ndf_timeindex_city[df_timeindex_city['city_id']==13]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pintamos la descomposición de la serie, pero solo para Moscu: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# decompose the series using stats module\n# results in this case is a special class \n# whose attributes we can acess\nresult = seasonal_decompose(df_timeindex_city[df_timeindex_city['city_id']==13][\"item_cnt_day\"])\n\n# ----------------------------------------------------------------------------------------------------\n# instanciate the figure\n# make the subplots share teh x axis\nfig, axes = plt.subplots(ncols = 1, nrows = 4, sharex = True, figsize = (12,10))\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data\n# using this cool thread:\n# https://stackoverflow.com/questions/45184055/how-to-plot-multiple-seasonal-decompose-plots-in-one-figure\n# This allows us to have more control over the plots\n\n# plot the original data\nresult.observed.plot(ax = axes[0], legend = False)\naxes[0].set_ylabel('Observed')\naxes[0].set_title(\"Decomposition of a series\")\n\n# plot the trend\nresult.trend.plot(ax = axes[1], legend = False)\naxes[1].set_ylabel('Trend')\n\n# plot the seasonal part\nresult.seasonal.plot(ax = axes[2], legend = False)\naxes[2].set_ylabel('Seasonal')\n\n# plot the residual\nresult.resid.plot(ax = axes[3], legend = False)\naxes[3].set_ylabel('Residual')\n\n# ----------------------------------------------------------------------------------------------------\n# prettify the plot\n\n# get the xticks and the xticks labels\nxtick_location = df_timeindex_city.index.tolist()\n\n# set x_ticks\naxes[0].set_xticks(xtick_location);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 3: Create a treemap plot for item_category and the total combined sales","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Creamos nuestro dataframe solo con los datos que necesitamos. No filtramos por años, ya que no nos dices nada en el ejercicio.Asi que sacaremos los datos sobre las ventas totales de la serie historica disponible:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_items=df[['item_category_name', 'item_cnt_day']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_items.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Agrupamos por el  nombre de las diversas cateforias y asi ya tenemos las ventas totales por categoria.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_items=df_items.groupby(['item_category_name'])['item_cnt_day'].sum().sort_values(ascending=False).to_frame()\n# Cambiamos el nombre ,quitamos el dia y ponemos toital:\ndf_items.columns=['item_cnt_total']\ndf_items.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the x and y values\nmy_values = df_items[\"item_cnt_total\"]\nmy_pct = df_items[\"item_cnt_total\"]/df_items[\"item_cnt_total\"].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Retocamos la lista para que solo se generen etiquetas para las categorias que tienen un % mayor o igual al 1 en las ventas totales. si no es superior a 1, la etiqueta sera un nulo ('')","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['{} - Sales :{}k \\n {}% of total'.format(item, sales/1000, round(pct, 2)*100) \n          if (pct >= 0.01) else '' for item, sales, pct in zip(df_items.index, my_values, my_pct)]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pintamos usando los colores por defecto:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (30, 8))\nsquarify.plot(sizes = my_values, label = labels,  alpha = 0.8)\nplt.title(\"Sales by city item_category % over total sales\",fontsize = 23, fontweight = \"bold\")\n\nplt.axis('off')\nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lo pintamos de nuevo, usando la paleta azul que definiste en un notebook:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a color palette, mapped to the previous values\ncmap = matplotlib.cm.Blues\n\n# we want to normalize our values, otherwise a city will have the darkest collor and all the others will pale\nmini = min(my_values)\nmaxi= np.percentile(my_values, q = 99)\nnorm = matplotlib.colors.Normalize(vmin = mini, vmax = maxi)\ncolors = [cmap(norm(value)) for value in my_values]\n# instanciate the figure\nplt.figure(figsize = (30, 8))\n# we can pass colors but Moscow is way too big and most of the cities are pale blue\nsquarify.plot(sizes = my_values, label = labels,  alpha = 0.8, color  = colors)\n\n# Remove our axes, set a title and display the plot\nplt.title(\"Sales by city item_category % over total sales\", fontsize = 23, fontweight = \"bold\")\nplt.axis('off')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}