{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"The following notebook includes a XGBoost regression to predict total sales for every item and store in the following month (as proposed by the competition: https://www.kaggle.com/competitions/competitive-data-science-predict-future-sales/overview/description).\n\nWe begin with an **EDA**, followed by **data engineering** focused on pivoting and preparing the data as a timeseries able to be fed into our regression algorithm with both a training and testing period, and lastly we run our **XGBoost regression**, and apply our **prediction** to the testing data.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nfrom numpy import absolute\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom sklearn.model_selection import RandomizedSearchCV\nimport seaborn as sns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_categories = pd.read_csv ('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nitems = pd.read_csv ('../input/competitive-data-science-predict-future-sales/items.csv')\ntrain = pd.read_csv ('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nshops = pd.read_csv ('../input/competitive-data-science-predict-future-sales/shops.csv')\ntest = pd.read_csv ('../input/competitive-data-science-predict-future-sales/test.csv')\nsample_submission = pd.read_csv ('../input/competitive-data-science-predict-future-sales/sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"#Applying Basic EDA function\ndef EDA(df):\n    return 'First rows', df.head(3),\\\n    'Info', df.info,\\\n    'Describe', df.describe(),\\\n    'Missing Values', df.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets = [item_categories, items, train, shops, test, sample_submission]\n#we run the EDA function to all the provided datasets\nfor i in datasets:\n    print(EDA(i))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the \"Describe\" table of Train, the minimum values of \"item_price\" and \"item_cnt_day\" are negative which doesn't seem to make much logic when we are dealing with Sales. Regarding the maximum values of said features, they are much larger than the mean so we might have outliers here.","metadata":{}},{"cell_type":"code","source":"# plot item_price\nsns.violinplot(y = train['item_price']).set(title='item_price has a long tail in the upper values')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.sort_values(by=['item_price']).head(5),train.sort_values(by=['item_price']).tail(5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We clearly have outliers: The last Rows shown above clearly have unusually high \"item_price\". Row 484683 contains a negative item_price which is not realistic.","metadata":{}},{"cell_type":"code","source":"# plot item_cnt_day\nsns.violinplot(y = train['item_cnt_day']).set(title='item_cnt_day has a long tail in the upper values')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.sort_values(by=['item_cnt_day']).head(5),train.sort_values(by=['item_cnt_day']).tail(5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice outliers with very high \"item_cnt_day\".","metadata":{}},{"cell_type":"markdown","source":"## Data Engineering","metadata":{}},{"cell_type":"code","source":"#In order to deal with outliers we will apply a Z-SCORE for item_cnt_day and item_price, and remove scores outside 3 Z-Scores\n#we'll limit the Z score to |3| (will cover ~99.77% of area)\n\ntrain['Zscore_item_cnt_day'] = (train.item_cnt_day - train.item_cnt_day.mean())/train.item_cnt_day.std(ddof=0)\ntrain['Zscore_item_price'] = (train.item_price - train.item_price.mean())/train.item_price.std(ddof=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Based on our analysis above we remove Outliers and \"sketchy\" Rows\n\n#First we remove the 1 row with negative item_price\ntrain = train[train['item_price'] > 0]\n\n#removing outliers based on Z-score\ntrain = train[(train['Zscore_item_cnt_day']<3)&(train['Zscore_item_cnt_day']>-3)]\ntrain = train[(train['Zscore_item_price']<3)&(train['Zscore_item_price']>-3)]\n\n#removing Zscores now that the operation is finished\ntrain.drop(columns=['Zscore_item_cnt_day','Zscore_item_price'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#our estimation is restricted to november 2015, so it's convenient we organise data by months. \n#We are working with a timeseries so we'll use datetime dtype to make things easier \n\ntrain['date'] = pd.to_datetime(train['date'], format = \"%d.%m.%Y\" )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5) #updated date","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In summary, the purpose of this competition is to figure out the expected values of how much items are sold for a given shop within the period of november 2015. Hence, an easier way to frame the problem is by making the data into a **pivot table** in which given a shop, we have the count values of items that were sold over a month and organise those monthly counts by frequency. Therefore we group train data by \"shop_id\" and \"item_id\"","metadata":{}},{"cell_type":"code","source":"pt = pd.pivot_table(train, index = ['shop_id', 'item_id'], values = 'item_cnt_day', columns = ['date_block_num'], aggfunc = np.sum, fill_value = 0)\npt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we currently have MultiIndex from the pivot table we built\n#It's easier if we convert pt to a plain DataFrame by resetting the index with reset_index which removes the MultiIndex\npt.reset_index(inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pt.tail(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now that we have item data, we merge the pivot table with the test data, giving priority to the latter\n#as a result we keep the ordered date_block_num\ndf = pd.merge(test, pt, on=['shop_id', 'item_id'], how = 'left')\ndf.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#as seen above we are missing values\ndf.fillna(0, inplace=True)\ndf.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Since we completed all data engineering we finally split the data between test and train\n\n#training data\nX_train = df.drop(columns=['shop_id','item_id', 'ID', 33], axis=1) #firstly we don't need ids, not the shop & item ones, and drop the last month\ny_train = df[33]\n\n#for test we keep all the columns except the first one so we maintain the same time window as in training\nX_test = df.drop(columns=['shop_id','item_id', 'ID', 0], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#observing our split datasets\nprint('X TRAIN \\n', X_train.head(3))\nprint('Y TRAIN \\n', y_train.head(3))\nprint('X TEST \\n', X_test.head(3))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Naturally X_test has different column names than X_train since they are a month apart, but in order to fit the model, we'll change X_test to have the same column names as X_train (as if sliding an imaginary time window)","metadata":{}},{"cell_type":"code","source":"X_test.columns = X_train.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.head(5) #artificial column names addded","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost Regression","metadata":{}},{"cell_type":"code","source":"# create an xgboost regression model\nmodel = XGBRegressor() #first we try the model with default parameters, our relevant evaluation metric is rmse (default)\n# define model evaluation method\ncv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)\n# evaluate model\nscores = cross_val_score(model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean RMSE: %.3f (%.3f)' % (scores.mean(), scores.std()) )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter tunning\n\nUsing RandomizedSearchCV, we run a random search with a grid. Although relatively computer intensive to run, we aim to obtain better parameters aiming to improve the RMSE we've got with the default parameters.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nCODE BELOW TAKES UP TO 2 HOURS TO RUN, SKIP THIS CELL FOR RESULTS\n\nregressor = model\n\nhyperparameter_grid = {\n    'n_estimators': [100, 400, 800],\n    'max_depth': [3, 6, 9],\n    'learning_rate': [0.05, 0.1, 0.20],\n    'min_child_weight': [1, 10, 100]\n    }\n\n# Set up the random search with 4-fold cross validation\nrandom_cv = RandomizedSearchCV(estimator=regressor,\n            param_distributions=hyperparameter_grid,\n            cv=5, n_iter=50,\n            scoring = 'neg_root_mean_squared_error',n_jobs = 4,\n            verbose = 5, \n            return_train_score = True,\n            random_state=42)\n\nrandom_cv.fit(X_train,y_train)\n\nrandom_cv.best_estimator_\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IMPROVED xgboost regression model\nregressor = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=3,\n             min_child_weight=1, monotone_constraints='()',\n             n_estimators=400, n_jobs=8, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\n# define model evaluation method\ncv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)\n# evaluate model\nscores = cross_val_score(regressor, X_train, y_train, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean RMSE: %.3f (%.3f)' % (scores.mean(), scores.std()) )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our new Hyperparameters slightly improve the model's Mean RMSE - this means the standard deviation of the residuals (prediction errors) decreased. In simpler terms, we have a smaller average distance between the observed data values and the values predicted by our model.","metadata":{}},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"#Best Regressor\nBest_Regressor = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=3,\n             min_child_weight=1, monotone_constraints='()',\n             n_estimators=400, n_jobs=8, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\n#Given the improved model, we fit our training data\nBest_Regressor.fit(X_train, y_train)\n\n# make a prediction\nyhat = Best_Regressor.predict(X_test)\n\n# summarize prediction for total number of sales\nprint(yhat)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\n            \"ID\": np.arange(len(yhat)),\n            \"item_cnt_month\": yhat\n    })","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('../submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Brief Conclusion","metadata":{}},{"cell_type":"markdown","source":"In brief, we conducted an EDA to then clean and prepare the data for an XGBoost regressor. After doing so we ran a Randomized Search Cross Validation to obtain better hyperparameters for our final regressor model.\n\nIn this version of the notebook we remove outliers based on the Zscore: the result was barely any difference in the competition score, but with the in-sample mean RMSE improving from 4.028 (before treating outliers) to 0.96 (after) using the default XGBoost regressor... The fact the out-of-sample score remained almost the same while the in-sample score improved significantly might be a cause of overfiting. In case I ever dive into this notebook again, it might be a worthy pursuit to explore ways to improve the out-of-sample score.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}