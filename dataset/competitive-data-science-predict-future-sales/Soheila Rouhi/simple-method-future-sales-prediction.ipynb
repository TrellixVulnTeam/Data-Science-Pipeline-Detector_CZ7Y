{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.ensemble import RandomForestRegressor\n\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nitem_cat = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nsales = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')# parse_dates=['date']\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sales.info())\nprint('Number of missing values in dataset : ' + str(sales.isnull().sum().max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking into we can see that there is no missing values . ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Getting more info about the data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sales.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Negative item price does not make sense, let's look at those items:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_price = sales.item_price < 0\nprint(sales[neg_price])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I prefer to remove this observation:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.drop(sales[neg_price].index, axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I convert date column to a datetime object. Then, I can easily extract year, month and week day as a separate column. Here, since the goal is predicting monthly sales, I only use month and year information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['date'] = pd.to_datetime(sales['date'], format='%d.%m.%Y')\nsales['year'] = sales.date.dt.year\nsales['month'] = sales.date.dt.month\nprint(sales.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the time interval in which data is available:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Time range is between {sales.date.min()}, {sales.date.max()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot monthly total sales of company:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## plot monthly total sales of company:\nsales_total_month = sales.loc[:, ['date_block_num', 'item_cnt_day']].groupby('date_block_num').sum()\nsales_total_month.rename(columns ={'item_cnt_day':'total_cnt_month'},inplace=True)\nsales_total_month.plot()\nplt.axvline(x=0, color='red', linestyle='--')\nplt.axvline(x=12, color='red', linestyle='--')\nplt.axvline(x=24, color='red', linestyle='--')\nplt.axvline(x=36, color='red', linestyle='--')\nlegend_x = 1\nlegend_y = 0.5\nplt.legend(loc='center left', bbox_to_anchor=(legend_x, legend_y))\nplt.ylabel('Total number of sold items per month')\nplt.xlabel('Month block number')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the figure above red lines specify one-year period. The decreasing trend and yearly pattern can be seen very well. So, adding year and month as  separate columns was a good decision.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Plotting average item_price by month","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"price_average_month = sales.loc[:, ['date_block_num', 'item_price']].groupby('date_block_num').mean()\nprice_average_month.rename(columns ={'item_price':'monthly_average_item_price'},inplace=True)\nprice_average_month.plot()\nplt.axvline(x=0, color='red', linestyle='--')\nplt.axvline(x=12, color='red', linestyle='--')\nplt.axvline(x=24, color='red', linestyle='--')\nplt.axvline(x=36, color='red', linestyle='--')\nlegend_x = 1\nlegend_y = 0.5\nplt.legend(loc='center left', bbox_to_anchor=(legend_x, legend_y))\nplt.ylabel('Monthly average price of items')\nplt.xlabel('Month block number')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a clear increasing trend and a yearly pattern in the price of items.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Plot distribution of items per category","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,14))\nsns.countplot(x='item_category_id', data=items) ## the same result but easier\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that item category is important feature! let's add it to sales data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## add item_category to sales data:\nitems = items.drop(columns='item_name')\nsales_cat = pd.merge(sales, items, on=['item_id'], how='left')\nprint(sales_cat.head())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we want to know the total daily sold items per category:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## plot total items sold per category:\nsales_total_cat = sales_cat.loc[:, ['item_category_id', 'item_cnt_day']].groupby('item_category_id').sum()\nsales_total_cat.rename(columns={'item_cnt_day':'total_sold_items'}, inplace=True)\nprint(sales_total_cat.head())\nsales_total_cat.plot()\nplt.xlabel('Category ID')\nplt.ylabel('Total sold items')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking the above plot category_id seems to be an important feature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Plotting average item price per shop","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_price_per_shop = sales.loc[:, ['shop_id', 'item_price']].groupby('shop_id').mean()\nsales_price_per_shop.rename(columns={'item_price':'average_item_price'}, inplace=True)\nsales_price_per_shop.plot(kind='bar', figsize=(20, 14))\nplt.xlabel('shop_id')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting total sold items per day per shop","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_sold_items_per_shop = sales.loc[:, ['shop_id', 'item_cnt_day']].groupby('shop_id').sum()\nsales_sold_items_per_shop.rename(columns={'item_cnt_day':'total_sold_items'}, inplace=True)\nsales_sold_items_per_shop.plot(kind='bar', figsize=(20, 14))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's add 'ID' column to sales data: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## add 'ID' to sales_cat data:\nsales_id = pd.merge(sales_cat, test, on=['item_id', 'shop_id'], how='left')\nsales_id_useful = sales_id.dropna()\nprint(sales_id_useful.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we want to predict the monthly amount of sold items, let's convert daily number of sold items to monthly number:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### convert item_cnt_day to item_cnt_month\nsales_monthly = sales_id_useful.groupby(['ID','shop_id', 'item_id', 'item_category_id', 'date_block_num', 'year', 'month'])['item_cnt_day'].sum().rename('item_cnt_month').reset_index()\nsales_monthly_sort_id = sales_monthly.sort_values(['ID'], ascending=True).reset_index().drop(columns='index')\nsales_monthly_sort_id['ID'] = sales_monthly_sort_id['ID'].astype(int)\nsales_monthly_sort_id['month'] = sales_monthly_sort_id['month'].astype('object')\nsales_monthly_sort_id['year'] = sales_monthly_sort_id['year'].astype('object')\nprint(sales_monthly_sort_id.head(10))\nprint(sales_monthly_sort_id.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the test data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(test.shop_id, bins=60, kde=False)\nplt.show()\nprint(test.shop_id.nunique())\nprint(test.shop_id.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of items per class in test dataset is uniform; we need to predict sales for items in only 42 shops.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we add 'item_category_id', 'year' and 'month' as features to test dataset:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## ## add 'item_category_id' to test data:\ntest_new = pd.merge(test, items, on=['item_id'], how='left')\ntest_new['date_block_num'] = 34\ntest_new['year'] = '2015'\ntest_new['month'] = '11'\nprint(test_new.head())\nX_test = test_new.drop(columns='ID').values\nX = sales_monthly_sort_id.drop(columns=['ID', 'item_cnt_month']).values\ny = sales_monthly_sort_id[['item_cnt_month']]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we specified shop_id, item_id, item_category_id and date_block_num as features and everything is ready to train a model. I try both random forest and XGBoost.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### build a model\n############################################ Building Random Forest model  #############################################\nrf = RandomForestRegressor(n_estimators=1000, max_depth=10, min_samples_leaf=10, criterion='mse', random_state=42, n_jobs=1)\nmodel=rf.fit(X, np.ravel(y))\n\ny_pred_rf = model.predict(X_test)\nprint(type(y_pred_rf))\ntest_new['item_cnt_month'] = y_pred_rf\ntest_new.loc[:, ['ID', 'item_cnt_month']].to_csv('Submission_RF.csv', index=False)\nprint(test_new.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nxgb = XGBRegressor(max_depth=15, random_state=42, n_estimators=50, learning_rate=0.0001, booster='gbtree', objective='reg:squarederror', min_child_weight=100, silent=1, n_jobs=10)\n\nxgb.fit(X, y.values.ravel())\ny_pred = xgb.predict(X_test)\n\ntest_new['item_cnt_month'] = y_pred\nprint(y_pred)\ntest_new.loc[:, ['ID', 'item_cnt_month']].to_csv('Submission_XGBoost.csv', index=False)\nprint(test_new.head())\nfeatures = ['shop_id', 'item_id', 'item_category_id', 'date_block_num', 'year', 'month']\nfeat_imp = pd.Series(xgb.feature_importances_, index=features).sort_values(ascending=True)\nfeat_imp.plot(kind='barh', title='Feature Importances XGBoost')\nplt.ylabel('Feature Importance Score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* My public score is 1.23 by XGBoost.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As next steps in future I want to add more features like monthly price, montly lagged sales of items etc. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}