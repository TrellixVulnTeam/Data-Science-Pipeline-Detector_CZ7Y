{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Final Project Coursera"},{"metadata":{},"cell_type":"markdown","source":"The first step is to load all the required libraries and load raw data files into memory."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Validating packages versions**.\n\nI'm going to validate de version of the installed packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"for p in [np, pd, lgb]:\n    print (p.__name__, p.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ntest_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nitem_category = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\n\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic functions"},{"metadata":{},"cell_type":"markdown","source":"This fuction is part of one of the assignments to reduce the size of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is required to add lagged data as new features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature(all_data, list_lags, index_cols, cols_to_rename):\n    shift_range = list_lags\n\n    for month_shift in tqdm_notebook(shift_range):\n        train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n        train_shift = train_shift.rename(columns=foo)\n\n        all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\n    del train_shift\n    return all_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"Here is how data looks like. "},{"metadata":{},"cell_type":"markdown","source":"I have ploted sales of shops per month, in order to see the general behavior"},{"metadata":{"trusted":true},"cell_type":"code","source":"Monthly_sales = sales.groupby([\"date_block_num\", \"shop_id\"])['item_cnt_day'].sum().reset_index(name = 'item_cnt_month')\n\n\nfig, axs = plt.subplots(10, 6)\n\nfor i in range(60):\n  shop_sale_per_month = Monthly_sales.loc[Monthly_sales['shop_id']==i]\n  axs[i//6,i%6].tick_params(axis='both', which='both', bottom=False, top= False, labelbottom=False, right=False, left=False, labelleft=False)\n  axs[i//6,i%6].plot(shop_sale_per_month['date_block_num'], shop_sale_per_month['item_cnt_month'])\n\n\ndel Monthly_sales, shop_sale_per_month","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In general, sales have seasonal behaviour as expected. However, some shops show abnormal behaviour which turned out to be duplication issue and fixed as follows. Also, we removed the outliers from data."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.loc[sales.shop_id == 0, 'shop_id'] = 57\ntest_data.loc[test_data.shop_id == 0, 'shop_id'] = 57\n\nsales.loc[sales.shop_id == 1, 'shop_id'] = 58\ntest_data.loc[test_data.shop_id == 1, 'shop_id'] = 58\n\nsales.loc[sales.shop_id == 10, 'shop_id'] = 11\ntest_data.loc[test_data.shop_id == 10, 'shop_id'] = 11\n\n\n# remove the oulier\nsales = sales[sales.item_cnt_day<1001]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Leakage"},{"metadata":{},"cell_type":"markdown","source":"Finding the number of unique shop-item combinations that only exist in test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = pd.merge(test_data[['shop_id','item_id']],sales[['shop_id','item_id']], on=['shop_id','item_id'], how='left', indicator='Exist')\ntemp_var =  (temp_df['Exist']=='left_only').sum()\nprint('Number of unique shop-item combination in the test set that do not exist in the training set:',temp_var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, about 52% of combinations already exists in the training set. I'm going to use them if any leaked is found."},{"metadata":{"trusted":true},"cell_type":"code","source":"Leakage_Percentage = ((test_data.shape[0]-temp_var)/test_data.shape[0])*100\nprint('Percentage of shop-item combination in test data that are available in the training set:', Leakage_Percentage)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"The following code is based in assignments and it will be used in order to create all posible combinations of shop-items and fill out the target column."},{"metadata":{"trusted":true},"cell_type":"code","source":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales[sales['date_block_num']==block_num]['shop_id'].unique()\n    cur_items = sales[sales['date_block_num']==block_num]['item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n#turn the grid into pandas dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n#get aggregated values for (shop_id, item_id, month)\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':['sum']})\ngb.rename(columns = {'sum':'target'}, inplace = True) \n\n#fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n#join aggregated data to the grid\nall_data = pd.merge(grid,gb,how='left',on=index_cols).fillna(0)\n#sort the data\nall_data.sort_values(['date_block_num','shop_id','item_id'],inplace=True)\nall_data['target'] = all_data['target'].fillna(0).clip(0,20)\n\n# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12] \nall_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm going to add the following features:\n* City code from the shops csv\n* Category ID\n* The text data in item category gives some extra info that can be used such as the basket of commodities that an item belongs to"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding city_enc column\nshops['city'] = shops.shop_name.apply(lambda x: str.replace(x, '!', '')).apply(lambda x: x.split(' ')[0])\nshops['city_enc'] = LabelEncoder().fit_transform(shops['city'])\nshops_data = shops[['shop_id','city_enc']]\nall_data = pd.merge(all_data, shops_data, how='left', on=['shop_id'])\n\n# Adding item_category_id column\nall_data = pd.merge(all_data, items, how='left', on=['item_id'])\nall_data = all_data.drop('item_name',axis =1)\n\n# Adding basket_enc column\nitem_category['basket'] = item_category['item_category_name'].apply(lambda x: str(x).split(' ')[0])\nitem_category['basket_enc'] = LabelEncoder().fit_transform(item_category['basket'])\nitem_category = item_category[['item_category_id','basket_enc']]\nall_data = pd.merge(all_data, item_category, how='left', on=['item_category_id'])\nall_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I'm going to add the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.concat([all_data, test_data], ignore_index=True, sort=False, keys=['date_block_num','shop_id','item_id', 'city_enc', 'item_category_id', 'basket_enc', 'target'])\nall_data = downcast_dtypes(all_data)\nall_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean Encoding"},{"metadata":{},"cell_type":"markdown","source":"Here, I'm going to aggregate data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# shop-month aggregates\ngb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':['sum']})\ngb.rename(columns = {'sum':'target_shop'}, inplace = True)\n\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n# item-month aggregates\ngb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':['sum']})\ngb.rename(columns = {'sum':'target_item'}, inplace = True)\ngb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\nall_data = downcast_dtypes(all_data)\n\nall_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, I'm going to add lagged data. Based on my analysis using trial and error, I found just lagged data for previous 3 months has highest impact"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook\n\nindex_cols = ['shop_id', 'item_id', 'date_block_num', 'item_category_id', 'basket_enc', 'city_enc']\ncols_to_rename = list(all_data.columns.difference(index_cols)) \nlist_lags = [1, 2, 3]\nall_data = lag_feature(all_data, list_lags, index_cols, cols_to_rename)\nall_data = downcast_dtypes(all_data)\n\nall_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{},"cell_type":"markdown","source":"I'm going to train two LGBM and random forest model and later I'll stack them"},{"metadata":{"trusted":true},"cell_type":"code","source":"shift_range = list_lags\n\n# List of all lagged features\nfit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n# We will drop these at fitting stage\nto_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \ndel sales, grid\nto_drop_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = all_data[all_data.date_block_num < 33].drop(to_drop_cols, axis=1)\nY_train = all_data[all_data.date_block_num < 33]['target']\nX_valid = all_data[all_data.date_block_num == 33].drop(to_drop_cols, axis=1)\nY_valid = all_data[all_data.date_block_num == 33]['target']\nX_test = all_data[all_data.date_block_num == 34].drop(to_drop_cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X_train.append(X_valid)\nY = np.append(Y_train, Y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.05, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }\nlgb = lgb.train(lgb_params, lgb.Dataset(X, label=Y), 100)\n\npred_lgb_val = lgb.predict(X_valid)\n\nprint('Train mse is %f' % mean_squared_error(Y_train, lgb.predict(X_train)))\nprint('Val mse is %f' % mean_squared_error(Y_valid, pred_lgb_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(bootstrap=0.7, criterion='mse', max_depth=10,\n           max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           n_estimators=10, n_jobs=4, oob_score=False, random_state=None,\n           verbose=0, warm_start=False)\nrf.fit(X,Y)\n\npred_rf_val = rf.predict(X_valid)\nprint('Train mse is %f' % mean_squared_error(Y_train, rf.predict(X_train)))\nprint('Val mse is %f' % mean_squared_error(Y_valid, pred_rf_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking"},{"metadata":{},"cell_type":"markdown","source":"I'm going to stack two models in order to improve the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(pred_rf_val, pred_lgb_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val_level2 = np.c_[pred_rf_val, pred_lgb_val]\n\nlr = LinearRegression()\nlr.fit(X_val_level2, Y_valid)\npred_lr_val =  lr.predict(X_val_level2)\nprint('Test mse is %f' % mean_squared_error(Y_valid, pred_lr_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_val_level2, Y_valid)\npred_lr_val =  lr.predict(X_val_level2)\nprint('Test mse is %f' % mean_squared_error(Y_valid, pred_lr_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.to_csv('mycsvfile.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}