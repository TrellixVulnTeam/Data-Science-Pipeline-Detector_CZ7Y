{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Make The Data Being Used and Initialistation**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom xgboost import XGBRegressor\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing the data \nitem = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nshop = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\nsales_train = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\nitem_categories = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\ntestd = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\nsampl = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampl.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seeing teh basic structure of the dta in teh frame \ndata = [item,shop,sales_train,testd,item_categories,sampl]\nfor i in data:\n    print(i.info())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding Any Null values\nsales_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **EDA BEing done Very importnat**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some Anylysis Seeing Tools Imported\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution Of sales Vs Shop Analysis in details and seeing how shop perform\nsns.set(rc={'figure.figsize':(40, 40)})\nsns.set_context(\"talk\", font_scale=1)\nsales_month_shop_id = pd.DataFrame(sales_train.groupby(['shop_id']).sum().item_cnt_day).reset_index()\nsales_month_shop_id.columns = ['shop_id', 'sum_sales']\nsns.barplot(x ='shop_id', y='sum_sales', data=sales_month_shop_id, palette='Paired')\nplt.title('Distribution of sales per shop');\ndel sales_month_shop_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seeing Items with sales analysis\nsales_item_id = pd.DataFrame(sales_train.groupby(['item_id']).sum().item_cnt_day)\nplt.xlabel('item id')\nplt.ylabel('sales')\nplt.plot(sales_item_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seeing the real max item and its name and its other info\nanom_item = sales_item_id.item_cnt_day.argmax()\nprint(anom_item)\nitem[item['item_id'] == 20602]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  **Checking For Any Outliers IF Any**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will try to plot how does the items matches\nsns.set(style = \"dark\")\nplt.plot(sales_train['item_id'], sales_train['item_price'], '*', color='Green');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will see that there are not many outliers but a few that is very harmful in the algo","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train[sales_train['item_price'] > 50000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We See three but two are very close to most so can be ignored and can even be taken ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(item[item['item_id'] == 6066])\nprint(item[item['item_id'] == 11365])\nprint(item[item['item_id'] == 13199])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(item_categories[item_categories['item_category_id'] == 75])\nprint(item_categories[item_categories['item_category_id'] == 9])\nprint(item_categories[item_categories['item_category_id'] == 69])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(shop[shop['shop_id'] == 12])\nprint(shop[shop['shop_id'] == 25])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We Have Now seen how the outliers and what dtata they really hold","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_sub = sales_train\nsales_train_sub['month'] = pd.DatetimeIndex(sales_train_sub['date']).month\nsales_train_sub['year'] = pd.DatetimeIndex(sales_train_sub['date']).year\nsales_train_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_sales=sales_train_sub.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\"item_cnt_day\"].agg(item_cnt_day = 'sum')\n\nmonthly_sales['date_block_num'] = monthly_sales.index.get_level_values('date_block_num') \nmonthly_sales['shop_id'] = monthly_sales.index.get_level_values('shop_id') \nmonthly_sales['item_id'] = monthly_sales.index.get_level_values('item_id') \nmonthly_sales.reset_index(drop=True, inplace=True)\n\nmonthly_sales = monthly_sales.reindex(['date_block_num','shop_id','item_id','item_cnt_day'], axis=1)\nmonthly_sales.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,8))\nplt.subplots_adjust(hspace=.5)\n\nplt.subplot2grid((3,3), (0,0), colspan = 3)\ntestd['shop_id'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Shop ID Values in the Test Set (Normalized)')\n\nplt.subplot2grid((3,3), (1,0))\ntestd['item_id'].plot(kind='hist', alpha=0.7)\nplt.title('Item ID Histogram - Test Set')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Remove the outliers\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers\nsales_train = sales_train[sales_train.item_price <= 100000]\nsales_train = sales_train[sales_train.item_cnt_day <= 1000]\n\n# Adjusting negatice prices (change it for median values)\nmedian = sales_train[(sales_train.shop_id == 32) & (sales_train.item_id == 2973) & (sales_train.date_block_num == 4) & (sales_train.item_price > 0)].item_price.median()\nsales_train.loc[sales_train.item_price < 0, 'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Shop Data Preprossesing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Якутск Орджоникидзе, 56\nsales_train.loc[sales_train.shop_id == 0, 'shop_id'] = 57\ntestd.loc[testd.shop_id == 0, 'shop_id'] = 57\n# Якутск ТЦ \"Центральный\"\nsales_train.loc[sales_train.shop_id == 1, 'shop_id'] = 58\ntestd.loc[testd.shop_id == 1, 'shop_id'] = 58\n# Жуковский ул. Чкалова 39м²\nsales_train.loc[sales_train.shop_id == 10, 'shop_id'] = 11\ntestd.loc[testd.shop_id == 10, 'shop_id'] = 11\n# РостовНаДону ТРК \"Мегацентр Горизонт\"\nsales_train.loc[sales_train.shop_id == 39, 'shop_id'] = 40\ntestd.loc[testd.shop_id == 39, 'shop_id'] = 40","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop.shop_name.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop.loc[shop.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshop['shop_category'] = shop['shop_name'].str.split(' ').map(lambda x:x[1]).astype(str)\ncategories = ['Орджоникидзе,', 'ТЦ', 'ТРК', 'ТРЦ','ул.', 'Магазин', 'ТК', 'склад']\nshop.shop_category = shop.shop_category.apply(lambda x: x if (x in categories) else 'etc')\nshop.shop_category.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop.groupby(['shop_category']).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncategory = ['ТЦ', 'ТРК', 'ТРЦ', 'ТК']\nshop.shop_category = shop.shop_category.apply(lambda x: x if (x in category) else 'etc')\nprint('Category Distribution', shop.groupby(['shop_category']).sum())\n\nshop['shop_category_code'] = LabelEncoder().fit_transform(shop['shop_category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop['city'] = shop['shop_name'].str.split(' ').map(lambda x: x[0])\nshop.loc[shop.city == '!Якутск', 'city'] = 'Якутск'\nshop['city_code'] = LabelEncoder().fit_transform(shop['city'])\nshop = shop[['shop_id','city_code', 'shop_category_code']]\n\nshop.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprossing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(item_categories.item_category_name.unique()))\nitem_categories.item_category_name.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories['type'] = item_categories.item_category_name.apply(lambda x: x.split(' ')[0]).astype(str)\nitem_categories.loc[(item_categories.type == 'Игровые') | (item_categories.type == 'Аксессуары'), 'category'] = 'Игры'\nitem_categories.loc[item_categories.type == 'PC', 'category'] = 'Музыка'\ncategory = ['Игры', 'Карты', 'Кино', 'Книги','Музыка', 'Подарки', 'Программы', 'Служебные', 'Чистые', 'Аксессуары']\nitem_categories['type'] = item_categories.type.apply(lambda x: x if (x in category) else 'etc')\nprint(item_categories.groupby(['type']).sum())\nitem_categories['type_code'] = LabelEncoder().fit_transform(item_categories['type'])\n\n# if subtype is nan then type\nitem_categories['split'] = item_categories.item_category_name.apply(lambda x: x.split('-'))\nitem_categories['subtype'] = item_categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitem_categories['subtype_code'] = LabelEncoder().fit_transform(item_categories['subtype'])\nitem_categories = item_categories[['item_category_id','type_code', 'subtype_code']]\n\nitem_categories.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train['date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train['date'] = pd.to_datetime(sales_train['date'], format='%d.%m.%Y')\nsales_train['month'] = sales_train['date'].dt.month\nsales_train['year'] = sales_train['date'].dt.year\nsales_train = sales_train.drop(columns=['date'])\n\n# sales.head()\nto_append = testd[['shop_id', 'item_id']].copy()\n\nto_append['date_block_num'] = sales_train['date_block_num'].max() + 1\nto_append['year'] = 2015\nto_append['month'] = 11\nto_append['item_cnt_day'] = 0\nto_append['item_price'] = 0\n\nsales_train = pd.concat([sales_train, to_append], ignore_index=True, sort=False)\nsales_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Date Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"period = sales_train[['date_block_num', 'year', 'month']].drop_duplicates().reset_index(drop=True)\nperiod['days'] = period.apply(lambda r: monthrange(r.year, r.month)[1], axis=1)\n\nsales_train = sales_train.drop(columns=['month', 'year'])\n\nperiod.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import product\nindex_cols = ['date_block_num', 'shop_id', 'item_id']\ngrid = [] \nfor block_num in sales_train['date_block_num'].unique():\n    cur_shops = sales_train.loc[sales_train['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales_train.loc[sales_train['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[[block_num], cur_shops, cur_items])), dtype='int16'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols, dtype = np.int16)\ngrid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.merge(grid, shop, on='shop_id')\ndata = pd.merge(data, item, on='item_id')\ndata = pd.merge(data, item_categories, on='item_category_id')\ndata = pd.merge(data, period, on='date_block_num')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom calendar import monthrange\nfrom itertools import product\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = data[['date_block_num', 'year', 'month','days']]# 'item_price', 'item_cnt_day'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adjusting columns order\ndata = data[['date_block_num', 'year', 'month', 'days', 'city_code', 'shop_category_code', 'shop_id', 'item_category_id', 'type_code', 'subtype_code', 'item_id']] # 'item_price', 'item_cnt_day'\n\n# Downcasting values\nfor c in ['date_block_num', 'month', 'days', 'city_code', 'shop_category_code', 'shop_id', 'item_category_id', 'type_code', 'subtype_code']:\n    data[c] = data[c].astype(np.int8)\ndata['item_id'] = data['item_id'].astype(np.int16)\ndata['year'] = data['year'].astype(np.int16)\n\n# Remove unused and temporary datasets\ndel grid, shop, item, item_categories, to_append\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aux = sales_train\\\n.groupby(['date_block_num', 'shop_id', 'item_id'], as_index=False)\\\n.agg({'item_cnt_day' : 'sum', 'item_price' : 'mean'})\\\n.rename(columns= {'item_cnt_day' : 'item_cnt_month', 'item_price' : 'item_price_month'})\n\naux['item_cnt_month'] = aux['item_cnt_month'].astype(np.float16)\naux['item_price_month'] = aux['item_price_month'].astype(np.float16)\n\nmonth_summary = pd.merge(data, aux, how='left', on=['date_block_num', 'shop_id', 'item_id'])\\\n    .fillna(0.0).sort_values(by=['shop_id', 'item_id', 'date_block_num'])\n\ndel data, aux\n\nmonth_summary.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"checcking essentials\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Min: {} and Max: {} item_cnt_month values'.format(month_summary['item_cnt_month'].min(), month_summary['item_cnt_month'].max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_summary['item_cnt_month'] = month_summary['item_cnt_month'].clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def agg_by(month_summary, group_cols, new_col, target_col = 'item_cnt_month', agg_func = 'mean'):\n    aux = month_summary\\\n        .groupby(group_cols, as_index=False)\\\n        .agg({target_col : agg_func})\\\n        .rename(columns= {target_col : new_col})\n    aux[new_col] = aux[new_col].astype(np.float16)\n\n    return pd.merge(month_summary, aux, how='left', on=group_cols)\n\ndef lag_feature(df, col, lags=[1,2,3,6,12]):\n    tmp = df[['date_block_num','shop_id','item_id', col]]\n    for i in lags:\n        shifted = tmp.copy()\n        cols = ['date_block_num','shop_id','item_id', '{}_lag_{}'.format(col, i)]\n        shifted.columns = cols\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left').fillna(value={(cols[-1]) : 0.0})\n    return df\n\ndef agg_by_and_lag(month_summary, group_cols, new_col, lags=[1,2,3,6,12], target_col = 'item_cnt_month', agg_func = 'mean'):\n    tmp = agg_by(month_summary, group_cols, new_col, target_col, agg_func)\n    tmp = lag_feature(tmp, new_col, lags)\n    return tmp.drop(columns=[new_col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_summary = agg_by_and_lag(month_summary, ['date_block_num'], 'date_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_id'], 'date_item_avg_item_cnt', [1,2,3,6,12])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code'], 'date_city_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id'], 'date_shop_avg_item_cnt', [1,2,3,6,12])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_category_id'], 'date_cat_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'type_code'], 'date_type_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'subtype_code'], 'date_subtype_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code'], 'date_shop_category_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'item_category_id'], 'date_shop_cat_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'type_code'], 'date_shop_type_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'subtype_code'], 'date_shop_subtype_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code', 'subtype_code'], 'date_shop_category_subtype_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code', 'item_id'], 'date_item_city_avg_item_cnt', [1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_summary = agg_by_and_lag(month_summary, ['date_block_num'], 'date_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_id'], 'date_item_avg_item_price', [1,2,3,6,12], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code'], 'date_city_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id'], 'date_shop_avg_item_price', [1,2,3,6,12], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_category_id'], 'date_cat_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'type_code'], 'date_type_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'subtype_code'], 'date_subtype_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code'], 'date_shop_category_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'item_category_id'], 'date_shop_cat_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'type_code'], 'date_shop_type_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'subtype_code'], 'date_shop_subtype_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code', 'subtype_code'], 'date_shop_category_subtype_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code', 'item_id'], 'date_item_city_avg_item_price', [1], 'item_price_month')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"month_summary['item_shop_first_sale'] = month_summary['date_block_num'] - month_summary.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmonth_summary['item_first_sale'] = month_summary['date_block_num'] - month_summary.groupby('item_id')['date_block_num'].transform('min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_summary.to_pickle('month_summary.pkl')\nmonth_summary.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"month_summary = pd.read_pickle('month_summary.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_subsample(month_summary, target='item_cnt_month'):\n    X_test = month_summary[month_summary['date_block_num'] == 34]\n    X_test = X_test.drop(columns=[target])\n\n    X_val = month_summary[month_summary['date_block_num'] == 33]\n    y_val = X_val[target]\n    X_val = X_val.drop(columns=[target])\n\n    X_train = month_summary[(month_summary['date_block_num'] >= 12) & (month_summary['date_block_num'] < 33)]\n    y_train = X_train[target]\n    X_train = X_train.drop(columns=[target])\n\n    return X_train, y_train, X_val, y_val, X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train, X_val, y_val, X_test = generate_subsample(month_summary.drop(columns=['item_price_month']), 'item_cnt_month')\n\ndel month_summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Moodel\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_gbmodel(X_train, y_train, X_val, y_val):\n\n    RAND_SEED = 42\n\n    lgb_params = {'num_leaves': 2**8, 'max_depth': 19, 'max_bin': 107, #'n_estimators': 3747,\n              'bagging_freq': 1, 'bagging_fraction': 0.7135681370918421, \n              'feature_fraction': 0.49446461478601994, 'min_data_in_leaf': 2**8, # 88\n              'learning_rate': 0.015980721586917768, 'num_threads': 2, \n              'min_sum_hessian_in_leaf': 6,\n              'random_state' : RAND_SEED,\n              'bagging_seed' : RAND_SEED,\n              'boost_from_average' : 'true',\n              'boost' : 'gbdt',\n              'metric' : 'rmse',\n              'verbose' : 1}\n\n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_val = lgb.Dataset(X_val, label=y_val)\n\n    return lgb.train(lgb_params, lgb_train, \n                      num_boost_round=300,\n                      valid_sets=[lgb_train, lgb_val],\n                      early_stopping_rounds=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_old_item = train_gbmodel(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]).clip(0, 20), X_val, y_val.clip(0, 20))\ngbm_model = train_gbmodel(X_train, y_train, X_val, y_val)\n\ny_hat = gbm_model.predict(X_val).clip(0, 20)\nprint(np.sqrt(mean_squared_error(y_val.clip(0, 20), y_hat)))\n\nwith open('./gbm_model.pickle', 'wb') as handle:\n    pickle.dump(gbm_model, handle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = gbm_model.predict(X_test).clip(0, 20)\n\nresult = pd.merge(testd, X_test.assign(item_cnt_month=y_pred), how='left', on=['shop_id', 'item_id'])[['ID', 'item_cnt_month']]\nresult.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}