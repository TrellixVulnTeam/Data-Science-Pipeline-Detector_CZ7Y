{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom itertools import product\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgb\n\n\npd.set_option('display.max_columns', 30)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Datasets research**","metadata":{}},{"cell_type":"markdown","source":"**Dataset items**","metadata":{}},{"cell_type":"code","source":"items = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nitems.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Unique values of product names: {}'.format(items.item_name.unique()))\nprint()\nprint('Number of unique values of products: {}'.format(items.item_name.nunique()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def simple_hist(data, x, bins, title, xlabel, xmin, xmax):\n    plt.figure(figsize = (12, 8))\n    sns.set()\n    sns.distplot(data[x], color = 'lightcoral')\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.xlim(xmin, xmax)\n    plt.show()\n\nsimple_hist(items, 'item_category_id', 10, \n            'Distribution of item categories in the item dataframe', 'item_categories_id', -2, 85)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset item_categories**","metadata":{}},{"cell_type":"code","source":"item_categories = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nitem_categories.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_categories.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_categories.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Unique values of product identifiers: {}'.format(item_categories.item_category_id.unique()))\nprint()\nprint('Number of unique values of product identifiers: {}'.format(item_categories.item_category_id.nunique()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset shops**","metadata":{}},{"cell_type":"code","source":"shops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\nshops.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shops.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shops.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Unique meanings of store names: {}'.format(shops.shop_name.unique()))\nprint()\nprint('Number of unique store values: {}'.format(shops.shop_name.nunique()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset train**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simple_hist(train, 'shop_id', 10, \n            'Distribution of stores in the train dataframe', 'shop_id', -5, 65)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simple_hist(train, 'item_id', 30, \n            'Distribution of items in the train dataframe', 'item_id', -1000, 25000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simple_hist(train, 'item_price', 1000, \n            'Distribution of the price of items in the train dataframe', 'item_price', -100, 10000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 8))\nsns.boxplot(y=train['item_price'])\nplt.ylim(0, 10000)\nplt.grid()\nplt.title('Boxplot for the price of goods in the range from 0 to 10000 rubles')\nplt.ylabel('item_price')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the histogram is uninformative - it is better to look at the numbers\ntrain.item_cnt_day.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset test**","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simple_hist(test, 'shop_id', 10, \n            'Shops distributions in the test dataframe', 'id shops', 0, 70)\n# the distributions of the training and test datasets are different in this fic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simple_hist(test, 'item_id', 30, \n            'Distribution of items in the test dataframe', 'id items', 0, 25000)\n# the distributions of the training and test datasets are similar in this feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset submission**","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Leakage**\n\nAround 42% of training shop_id ~ item_id pairs are present in test set, but I don't use it.","metadata":{}},{"cell_type":"code","source":"df_temp = pd.Series(list(train[['item_id', 'shop_id']].itertuples(index=False, name=None)))\ntest_iter_temp = pd.Series(list(test[['item_id', 'shop_id']].itertuples(index=False, name=None)))\nprint(str(round(df_temp.isin(test_iter_temp).sum()/len(df_temp),2)*100)+'%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_shop_ids = test['shop_id'].unique()\n#test_item_ids = test['item_id'].unique()\n# Only shops that exist in test set.\n#train = train[train['shop_id'].isin(test_shop_ids)]\n# Only items that exist in test set.\n#train = train[train['item_id'].isin(test_item_ids)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Prepare and feature engineering data**","metadata":{}},{"cell_type":"code","source":"# remove outliers\ntrain = train[train.item_price < 100000]\ntrain = train[train.item_cnt_day <= 900]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Aggregate train data**\n\nSince the test data is generated with combination of shops and items, we have to restructure train data to match the test data generation.","metadata":{}},{"cell_type":"code","source":"# aggregate\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# compute all shops/items combinations\ngrid = []\nfor block_num in train['date_block_num'].unique():\n    cur_shops = train.loc[train['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = train.loc[train['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add sale for month\ntrain_merge = train.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_day':'sum'})\ntrain_merge.columns = ['item_cnt_month']\ntrain_merge.reset_index(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge grid and train\ntrain_merge = pd.merge(grid, train_merge, on = index_cols, how='left').fillna(0)\ntrain_merge['item_cnt_month'] = train_merge['item_cnt_month'].clip(0, 40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Merge train and item datasets**","metadata":{}},{"cell_type":"code","source":"items_prepare = pd.merge(items, item_categories, on='item_category_id')\ntrain_merge = pd.merge(train_merge, items_prepare, on = ['item_id'], how = 'left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Prepare test**","metadata":{}},{"cell_type":"code","source":"# prepare to concat with train\ntest_temp = test.copy()\ntest_temp['date_block_num'] = 34\ntest_temp.drop('ID', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge with items and item_category\ntest_temp = test_temp.merge(items, how='left', on='item_id')\ntest_temp = test_temp.merge(item_categories, how='left', on='item_category_id')\ntest_temp.drop('item_name', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat test and train dataframes\ntrain_merge = pd.concat([train_merge, test_temp], axis=0, ignore_index=True, keys=index_cols)\ntrain_merge.fillna(0, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Prepare items features**\n\nCategorization of products.","metadata":{}},{"cell_type":"code","source":"map_dict = {\n            'Чистые носители (штучные)': 'Чистые носители',\n            'Чистые носители (шпиль)' : 'Чистые носители',\n            'PC ': 'Аксессуары',\n            'Служебные': 'Служебные '\n            }\n# extract common categories\ntrain_merge['item_category'] = train_merge['item_category_name'].apply(lambda x: x.split('-')[0])\ntrain_merge['item_category'] = train_merge['item_category'].apply(lambda x: map_dict[x] if x in map_dict.keys() else x)\n# encoding common categories\ntrain_merge['item_category_common'] = LabelEncoder().fit_transform(train_merge['item_category'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Prepare shops features**\n\nExtract and encode the names of cities. Add new features - coordinates of cities and parts of the country.","metadata":{}},{"cell_type":"code","source":"# extract and encode cities\nshops['city'] = shops['shop_name'].apply(lambda x: x.split()[0].lower())\nshops.loc[shops.city == '!якутск', 'city'] = 'якутск'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n# add coordinates of cities\ncoords = dict()\ncoords['якутск'] = (62.028098, 129.732555, 4)\ncoords['адыгея'] = (44.609764, 40.100516, 3)\ncoords['балашиха'] = (55.8094500, 37.9580600, 1)\ncoords['волжский'] = (53.4305800, 50.1190000, 3)\ncoords['вологда'] = (59.2239000, 39.8839800, 2)\ncoords['воронеж'] = (51.6720400, 39.1843000, 3)\ncoords['выездная'] = (0, 0, 0)\ncoords['жуковский'] = (55.5952800, 38.1202800, 1)\ncoords['интернет-магазин'] = (0, 0, 0)\ncoords['казань'] = (55.7887400, 49.1221400, 4)\ncoords['калуга'] = (54.5293000, 36.2754200, 4)\ncoords['коломна'] = (55.0794400, 38.7783300, 4)\ncoords['красноярск'] = (56.0183900, 92.8671700, 4)\ncoords['курск'] = (51.7373300, 36.1873500, 3)\ncoords['москва'] = (55.7522200, 37.6155600, 1)\ncoords['мытищи'] = (55.9116300, 37.7307600, 1)\ncoords['н.новгород'] = (56.3286700, 44.0020500, 4)\ncoords['новосибирск'] = (55.0415000, 82.9346000, 4)\ncoords['омск'] = (54.9924400, 73.3685900, 4)\ncoords['ростовнадону'] = (47.2313500, 39.7232800, 3)\ncoords['спб'] = (59.9386300, 30.3141300, 2)\ncoords['самара'] = (53.2000700, 50.1500000, 4)\ncoords['сергиев'] = (56.3000000, 38.1333300, 4)\ncoords['сургут'] = (61.2500000, 73.4166700, 4)\ncoords['томск'] = (56.4977100, 84.9743700, 4)\ncoords['тюмень'] = (57.1522200, 65.5272200, 4)\ncoords['уфа'] = (54.7430600, 55.9677900, 4)\ncoords['химки'] = (55.8970400, 37.4296900, 1)\ncoords['цифровой'] = (0, 0, 0)\ncoords['чехов'] = (55.1477000, 37.4772800, 4)\ncoords['ярославль'] = (57.6298700, 39.8736800, 2) \n\nshops['city_coord_1'] = shops['city'].apply(lambda x: coords[x][0])\nshops['city_coord_2'] = shops['city'].apply(lambda x: coords[x][1])\nshops['country_part'] = shops['city'].apply(lambda x: coords[x][2])\n\nshops = shops[['shop_id', 'city_code', 'city_coord_1', 'city_coord_2', 'country_part']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_merge = pd.merge(train_merge, shops, on = ['shop_id'], how='left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_merge.drop(['item_name', 'item_category_name', 'item_category'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_merge.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Generate lag feature and mean encoding**","metadata":{}},{"cell_type":"code","source":"# define lag_feature\ndef lag_feature(data, lags, column):\n    temp = data[['date_block_num', 'shop_id', 'item_id', column]]\n    for lag in lags:\n        shifted = temp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', column + '_lag_' + str(lag)]\n        shifted['date_block_num'] += lag\n        data = pd.merge(data, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n        data[column+'_lag_'+str(lag)] = data[column+'_lag_'+str(lag)].astype('float16')\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add sales lags for last 3 months\ntrain_merge = lag_feature(train_merge, [1, 2, 3], 'item_cnt_month')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_merge.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# value reduction\ndef value_reduction(data):\n    for column in data.columns:\n        if data[column].dtype == 'float64':\n            data[column] = data[column].astype(np.float32)\n        if (data[column].dtype == 'int64' or data[column].dtype == 'int32') and (data[column].max() < 32767 and data[column].min() > -32768) and data[column].isnull().sum()==0:\n            data[column] = data[column].astype(np.int16)\n    return data\n\ntrain_merge = value_reduction(train_merge)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add mean encoding for items for last 3 month\nitem_id_target_mean = train_merge.groupby(['date_block_num','item_id'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_enc\"}, errors=\"raise\")\ntrain_merge = pd.merge(train_merge, item_id_target_mean, on=['date_block_num','item_id'], how='left')\n\ntrain_merge['item_target_enc'] = (train_merge['item_target_enc']\n                                .fillna(0)\n                                .astype(np.float16))\n\ntrain_merge = lag_feature(train_merge, [1, 2, 3], 'item_target_enc')\ntrain_merge.drop(['item_target_enc'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add target encoding for item/city for last 3 months\nitem_id_target_mean = train_merge.groupby(['date_block_num','item_id', 'city_code'])['item_cnt_month'].mean().reset_index().rename(columns={\n    \"item_cnt_month\": \"item_loc_target_enc\"}, errors=\"raise\")\ntrain_merge = pd.merge(train_merge, item_id_target_mean, on=['date_block_num','item_id', 'city_code'], how='left')\n\ntrain_merge['item_loc_target_enc'] = (train_merge['item_loc_target_enc']\n                                .fillna(0)\n                                .astype(np.float16))\n\ntrain_merge = lag_feature(train_merge, [1, 2, 3], 'item_loc_target_enc')\ntrain_merge.drop(['item_loc_target_enc'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add target encoding for item/shop for last 3 months \nitem_id_target_mean = train_merge.groupby(['date_block_num','item_id', 'shop_id'])['item_cnt_month'].mean().reset_index().rename(columns={\n    \"item_cnt_month\": \"item_shop_target_enc\"}, errors=\"raise\")\n\ntrain_merge = pd.merge(train_merge, item_id_target_mean, on=['date_block_num','item_id', 'shop_id'], how='left')\n\ntrain_merge['item_shop_target_enc'] = (train_merge['item_shop_target_enc']\n                                .fillna(0)\n                                .astype(np.float16))\n\ntrain_merge = lag_feature(train_merge, [1, 2, 3], 'item_shop_target_enc')\ntrain_merge.drop(['item_shop_target_enc'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# interaction features\nfirst_item_block = train_merge.groupby(['item_id'])['date_block_num'].min().reset_index()\nfirst_item_block['item_first_interaction'] = 1\n\nfirst_shop_item_buy_block = train_merge[train_merge['date_block_num'] > 0].groupby(['shop_id', 'item_id'])['date_block_num'].min().reset_index()\nfirst_shop_item_buy_block['first_date_block_num'] = first_shop_item_buy_block['date_block_num']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge train and new features\ntrain_merge = pd.merge(train_merge, first_item_block[['item_id', 'date_block_num', 'item_first_interaction']], on=['item_id', 'date_block_num'], how='left')\ntrain_merge = pd.merge(train_merge, first_shop_item_buy_block[['item_id', 'shop_id', 'first_date_block_num']], on=['item_id', 'shop_id'], how='left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fillna and change type\ntrain_merge['first_date_block_num'].fillna(100, inplace=True)\ntrain_merge['shop_item_sold_before'] = (train_merge['first_date_block_num'] < train_merge['date_block_num']).astype('int8')\ntrain_merge.drop(['first_date_block_num'], axis=1, inplace=True)\n\ntrain_merge['item_first_interaction'].fillna(0, inplace=True)\ntrain_merge['shop_item_sold_before'].fillna(0, inplace=True)\n \ntrain_merge['item_first_interaction'] = train_merge['item_first_interaction'].astype('int8')  \ntrain_merge['shop_item_sold_before'] = train_merge['shop_item_sold_before'].astype('int8') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add avg category for new features\nitem_id_target_mean = train_merge[train_merge['item_first_interaction'] == 1].groupby(['date_block_num','item_category_id'])['item_cnt_month'].mean().reset_index().rename(columns={'item_cnt_month': 'new_item_cat_avg'}, errors='raise')\n\ntrain_merge = pd.merge(train_merge, item_id_target_mean, on=['date_block_num','item_category_id'], how='left')\n\ntrain_merge['new_item_cat_avg'] = (train_merge['new_item_cat_avg']\n                                .fillna(0)\n                                .astype(np.float16))\n\ntrain_merge = lag_feature(train_merge, [1, 2, 3], 'new_item_cat_avg')\ntrain_merge.drop(['new_item_cat_avg'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_merge.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill Nan values to 0\ntrain_merge.fillna(0, inplace=True)\n# take data only after 3 since the most lag month interval is 3\ntrain_merge = train_merge[train_merge['date_block_num'] > 2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save finished dataset to pickle\ntrain_merge.to_pickle('train_merge.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split dataset \nX_train = train_merge[train_merge.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = train_merge[train_merge.date_block_num < 33]['item_cnt_month']\nX_valid = train_merge[train_merge.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_valid = train_merge[train_merge.date_block_num == 33]['item_cnt_month']\nX_test = train_merge[train_merge.date_block_num == 34].drop(['item_cnt_month'], axis=1)\nprint('Shape X_train: {}'.format(X_train.shape))\nprint()\nprint('Shape y_train: {}'.format(y_train.shape))\nprint()\nprint('Shape X_valid: {}'.format(X_valid.shape))\nprint()\nprint('Shape y_valid: {}'.format(y_valid.shape))\nprint()\nprint('Shape X_test: {}'.format(X_test.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Fit models**","metadata":{}},{"cell_type":"markdown","source":"**Catboost**","metadata":{}},{"cell_type":"code","source":"cat_features = ['country_part', \n                'item_category_common',\n                'item_category_id', \n                'city_code']\n\ncatboost = CatBoostRegressor(random_state=1, \n                             iterations=2000, verbose=200, depth = 4, \n                             learning_rate=0.01, l2_leaf_reg=7,\n                             max_leaves = 2047, min_data_in_leaf = 1,\n                             subsample = 0.7,\n                             loss_function='RMSE', eval_metric='RMSE',\n                             task_type='GPU',early_stopping_rounds=30,\n                             grow_policy='Lossguide', bootstrap_type='Poisson',\n                            cat_features=cat_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catboost.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save catboost model\npickle.dump(catboost, open('catboost.sav', 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_cb_train = catboost.predict(X_train)\npredict_cb_valid = catboost.predict(X_valid)\npredict_cb_test = catboost.predict(X_test)\nprint('Train rmse for Catboost:', np.sqrt(mean_squared_error(y_train, predict_cb_train)))\nprint('Validation rmse for Catboost:', np.sqrt(mean_squared_error(y_valid, predict_cb_valid)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LinearRegression**","metadata":{}},{"cell_type":"code","source":"lr_features = ['item_target_enc_lag_1', 'item_target_enc_lag_2',\n              'item_loc_target_enc_lag_1', 'item_loc_target_enc_lag_2', 'item_loc_target_enc_lag_3', \n               'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3']\nlr_train = X_train[lr_features]\nlr_val = X_valid[lr_features]\nlr_test = X_test[lr_features]\n\n\nlr_scaler = MinMaxScaler()\nlr_scaler.fit(lr_train)\nlr_train = lr_scaler.transform(lr_train)\nlr_valid = lr_scaler.transform(lr_val)\nlr_test = lr_scaler.transform(lr_test)\n\nlr_level1 = LinearRegression()\nlr_level1.fit(lr_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save linear regression model\npickle.dump(lr_level1, open('lr_level1.sav', 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_lr_train = lr_level1.predict(lr_train)\npredict_lr_valid = lr_level1.predict(lr_valid)\npredict_lr_test = lr_level1.predict(lr_test)\nprint('Train rmse for LinearRegression:', np.sqrt(mean_squared_error(y_train, predict_lr_train)))\nprint('Validation rmse for LinearRegression:', np.sqrt(mean_squared_error(y_valid, predict_lr_valid)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LightGBM**","metadata":{}},{"cell_type":"code","source":"# define build model function\ndef build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features):\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_val = lgb.Dataset(X_val, y_val)\n    model = lgb.train(params=params, train_set=lgb_train, valid_sets=(lgb_train, lgb_val), verbose_eval=50,\n                     categorical_feature=cat_features)\n    return model\n\n# define parameters\nparams = {\n    'objective': 'rmse',\n    'metric': 'rmse',\n    'num_leaves': 1023,\n    'min_data_in_leaf':10,\n    'feature_fraction': 0.7,\n    'learning_rate': 0.01,\n    'num_rounds': 2000,\n    'early_stopping_rounds': 30,\n    'seed': 1\n}\n# fit model\nlgb_model = build_lgb_model(params, X_train, X_valid, y_train, y_valid, cat_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save lightgbm model\npickle.dump(lgb_model, open('lgb_1.sav', 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_lgb_train = lgb_model.predict(X_train)\npredict_lgb_valid = lgb_model.predict(X_valid)\npredict_lgb_test = lgb_model.predict(X_test)\nprint('Train rmse for LightGBM:', np.sqrt(mean_squared_error(y_train, predict_lgb_train)))\nprint('Validation rmse for LightGBM:', np.sqrt(mean_squared_error(y_valid, predict_lgb_valid)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RandomForest**","metadata":{}},{"cell_type":"code","source":"rf = RandomForestRegressor(random_state = 1, max_depth=10, max_features='sqrt', min_samples_leaf=7,\n                      min_samples_split=11, n_estimators=75)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pickle.dump(rf, open('rf.sav', 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_rf_train = rf.predict(X_train)\npredict_rf_valid = rf.predict(X_valid)\npredict_rf_test = rf.predict(X_test)\nprint('Train rmse for RandomForest:', np.sqrt(mean_squared_error(y_train, predict_rf_train)))\nprint('Validation rmse for RandomForest:', np.sqrt(mean_squared_error(y_valid, predict_rf_valid)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ensemble model**","metadata":{}},{"cell_type":"markdown","source":"Ensemble architecture:\n1st level:\n* Catboost\n* XGBM\n* Random forest\n* Linear Regression\n\n2nd level:\n* Linear Regression","metadata":{}},{"cell_type":"code","source":"# dataset that will be the train set of the ensemble model\nfirst_level = pd.DataFrame(predict_cb_valid, columns=['catboost'])\nfirst_level['lightgbm'] = predict_lgb_valid\nfirst_level['random_forest'] = predict_rf_valid\nfirst_level['linear_regression'] = predict_lr_valid\nfirst_level['label'] = y_valid.values\nfirst_level.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset that will be the test set of the ensemble model\nfirst_level_test = pd.DataFrame(predict_cb_test, columns=['catboost'])\nfirst_level_test['lightgbm'] = predict_lgb_test\nfirst_level_test['random_forest'] = predict_rf_test\nfirst_level_test['linear_regression'] = predict_lr_test\nfirst_level_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_model = LinearRegression(n_jobs=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_first_level = first_level.drop('label', axis=1)\ny_first_level = first_level['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trained on validation set using the 1st level models predictions as features\nmeta_model.fit(X_first_level, y_first_level)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions on test set using the 1st level models predictions as feature\nensemble_pred_test = meta_model.predict(first_level_test).clip(0, 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save ensemble model\npickle.dump(meta_model, open('meta_model.sav', 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predict and submit task**","metadata":{}},{"cell_type":"code","source":"lgb_submission = pd.DataFrame({\n    'ID': test.index, \n    'item_cnt_month': lgb_model.predict(X_test).clip(0, 20)\n})\nlgb_submission.to_csv('lgb_submission.csv', index=False)\nprint(lgb_submission)\n\nensemble_submission = pd.DataFrame({\n    'ID': test.index, \n    'item_cnt_month': ensemble_pred_test\n})\nensemble_submission.to_csv('ensemble_submission.csv', index=False)\nprint(ensemble_submission)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Total score**\n* LightGBM model (public score) - 0.8981\n* LightGBM model (private score) - 0.9120\n* Ensemble model (public score) - 0.9050\n* Ensemble model (private score) - 0.9070","metadata":{}}]}