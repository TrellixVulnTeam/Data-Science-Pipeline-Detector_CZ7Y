{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport datetime\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier as xgb\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"##########################################################################################################\n##########################################   STEP 1: LOAD DATA   #########################################\n##########################################################################################################\n\n\nsales_train = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\", parse_dates=['date'], infer_datetime_format=False, dayfirst=True)\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\nitem_categories = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nitems = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\n\nprint(\"Sales_train\")\ndisplay(sales_train.head(10))\nprint(\"Test\")\ndisplay(test.head(10))\nprint(\"Item_categories\")\ndisplay(item_categories.head(10))\nprint(\"Items\")\ndisplay(items.head(10))\nprint(\"Shops\")\ndisplay(shops.head(1))\n\n# Auxiliar function to reduce data storage\ndef downcast_dtypes(df):\n    # Columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    return df\n\nall_data = sales_train\nall_data = downcast_dtypes(all_data)\ndisplay(all_data.head(10))\n\nprint(\"Train set size: \", len(sales_train))\nprint(\"Test set size: \", len(test))\nprint(\"Item categories set size: \", len(item_categories))\nprint(\"Items set size: \", len(items))\nprint(\"Shops set size: \", len(shops))\nprint(\"All data size: \", len(all_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##########################################################################################################\n######################################   STEP 2: DATA EXPLORATION   ######################################\n##########################################################################################################\n\n\n# Describe merged data to look for inusual values\ndisplay(all_data.describe())\n#print(\"Item_price outlier: \")\n#print(all_data.loc[all_data['item_price'].idxmax()])\n#print(\"\\nItem_cnt_day maximum: \")\n#print(all_data.loc[all_data['item_cnt_day'].idxmax()])\n\nf1, axes = plt.subplots(1, 2, figsize=(15,5))\nf1.subplots_adjust(hspace=0.4, wspace=0.2)\nsns.boxplot(x=all_data['item_price'], ax=axes[0])\nsns.boxplot(x=all_data['item_cnt_day'], ax=axes[1])\n#sns.boxplot(x=all_data['item_price'], y=all_data['item_cnt_day'], ax=axes[2])\n\n#print(shops['shop_name'].unique())\n\n# Conclusions: \n# 1 - There are negative prices and counts (errors, returns?)\n# 2 - Item_id = 6066 has an abnormal large price (item_price = 307980), and is only sold one time\n# 3 - 2 items have very large item_cnt_day when compared with the other products\n# 4 - Shop_name contains the shops' city names (Москва, Moscow). An additional feature can be obtained\n# 5 - Якутск city is expressed as Якутск and !Якутск. This could be fixed\n# 6 - Shop_id = 0 & 1 are the same than 57 & 58 but for фран (Google translator => fran maybe franchise). Shop_id = 10 & 11 are the same\n\n# Drop outliers\nall_data = all_data.drop(all_data[all_data['item_price']>250000].index)\nall_data = all_data.drop(all_data[all_data['item_cnt_day']>800].index)\n\n# Unify duplicated shops\n#all_data.loc[all_data['shop_id'] == 57,'shop_id'] = 0\n#all_data.loc[all_data['shop_id'] == 58,'shop_id'] = 1\n#all_data.loc[all_data['shop_id'] == 11,'shop_id'] = 10\n\n# An alternative for negative price outliers is to replace them with the median value for the impacted shops:\nall_data.loc[all_data['item_price'] < 0, 'item_price'] = all_data[(sales_train['shop_id'] == 32) & \n                                                                  (all_data['item_id'] == 2973) & \n                                                                  (all_data['date_block_num'] == 4) & \n                                                                  (all_data['item_price'] > 0)].item_price.median()\nprint(\"Raw data length: \",len(sales_train), \", post-outliers length: \", len(all_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##########################################################################################################\n######################################   STEP 3: MISSINGS CLEANING   #####################################\n##########################################################################################################\n\n\n# Missings count. Surprisingly enough, there are no missings!\nmissings_count = {col:all_data[col].isnull().sum() for col in all_data.columns}\nmissings = pd.DataFrame.from_dict(missings_count, orient='index')\nprint(missings.nlargest(30, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##########################################################################################################\n#####################################   STEP 4: FEATURE ENGINEERING   ####################################\n##########################################################################################################\n\ndef enrich_monthly_data(all_data, sales_train, items, item_categories, shops):\n    \n    # Extract year-month-day feats\n    all_data['year'] = all_data['date'].dt.year\n    all_data['month'] = all_data['date'].dt.month\n    all_data['day'] = all_data['date'].dt.day\n\n    # Split again data into train (date_block_num: 0-33) and test (date_block_num = 34) \n    all_data_train = all_data[all_data['date_block_num']<34]\n    all_data_test = all_data[all_data['date_block_num']==34]\n\n    # Aggregate monthly data and join with items file\n    monthly_data = all_data_train.groupby(['month', 'year', 'item_id','shop_id', 'date_block_num'])['item_cnt_day'].sum().reset_index()\n    monthly_data = monthly_data.join(items, on='item_id', rsuffix='_item')\n    monthly_data.drop(['item_id_item'], axis=1, inplace=True)\n\n    ## Add median item_price per item, shop and month\n    median_item_price = sales_train.groupby(['date_block_num', 'shop_id', 'item_id'],as_index=False).agg({'item_price':{'median_item_price':'median'}})\n    monthly_data = pd.merge(monthly_data, median_item_price, on=['item_id', 'shop_id', 'date_block_num'])\n    monthly_data = monthly_data.drop(['item_name'], axis=1)\n    monthly_data = monthly_data.rename(columns={monthly_data.columns[7]: \"item_price\"})\n\n    return monthly_data\n\nmonthly_data = enrich_monthly_data(all_data, sales_train, items, item_categories, shops)\n\n\ndef obtain_month_columns(monthly_data, test, shops, item_categories):\n    \n    # Create one column per year/month, with the item_cnt_day per item_id,shop_id\n    data_by_month_train = monthly_data.copy()\n    data_by_month_train['year/month'] = (monthly_data['year'].map(str)).str.cat(monthly_data['month'].map(str), sep='/')\n    data_by_month_train = data_by_month_train[['year/month','item_id','shop_id','item_cnt_day']]\n    data_by_month_train = data_by_month_train.pivot_table(index=['item_id','shop_id'], columns='year/month',values='item_cnt_day',fill_value=0).reset_index()\n\n    # Join test data\n    data_by_month = pd.merge(test, data_by_month_train, on=['item_id','shop_id'], how='left')\n    data_by_month = data_by_month.fillna(0)\n    \n    # Add item_categories\n    data_by_month = data_by_month.join(monthly_data['item_category_id'], on='item_id').join(item_categories, on='item_category_id', rsuffix='_item_categories')\n     \n    # Extract cities information from shop_name. Replace !Якутск by Якутск since it's the same city\n    data_by_month = data_by_month.join(shops, on='shop_id', rsuffix='_shops')\n    data_by_month['city'] = data_by_month['shop_name'].str.split(' ').map(lambda row: row[0])\n    data_by_month.loc[data_by_month.city == '!Якутск', 'city'] = 'Якутск'\n    data_by_month.drop(['shop_id_shops', 'shop_name'], axis=1, inplace=True)\n    \n    # Extract main category and subcategory from category name\n    categories_split = data_by_month['item_category_name'].str.split('-')\n    data_by_month['main_category'] = categories_split.map(lambda row: row[0].strip())\n    data_by_month['secondary_category'] = categories_split.map(lambda row: row[1].strip())\n\n    # Encode cities and categories\n    encoder = sklearn.preprocessing.LabelEncoder()\n    data_by_month['city_label'] = encoder.fit_transform(data_by_month['city'])\n    data_by_month['main_category_id'] = encoder.fit_transform(data_by_month['main_category'])\n    data_by_month['secondary_category_id'] = encoder.fit_transform(data_by_month['secondary_category'])\n    data_by_month.drop(['city', 'item_category_name', 'item_category_id_item_categories', 'main_category', 'secondary_category'], axis = 1, inplace = True)\n    \n    return data_by_month\n\ndata_by_month = obtain_month_columns(monthly_data, test, shops, item_categories)\ndata_by_month = downcast_dtypes(data_by_month)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_by_month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################################################\n###################################       STEP 5: DATASET PROCESSING        ###################################\n###############################################################################################################\n\n# Rename data\nX_test_full = data_by_month.copy()\n\nprint(len(X_test_full))\n\n# Break off test set from training data\nmonth_to_predict = '2015/10'\ny_train = data_by_month[month_to_predict]\nX_train_full = data_by_month.drop(labels=[month_to_predict], axis=1)\n\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 40 and \n                        X_train_full[cname].dtype not in ['int64', 'float64', 'int32', 'float32']]\nprint(\"Low cardinality columns: \", low_cardinality_cols)\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64', 'int32', 'float32']]\nprint(\"Numeric columns: \", numeric_cols)\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data if needed\nX_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define model with best MAE\n#model = xgb(colsample_bytree=0.7, learning_rate=.01, max_depth=5, min_child_weight=3, n_estimators=100, \n#                     nthread=1, objective='reg:squarederror', subsample=0.7, random_state=21, \n#                     early_stopping_rounds = 10, eval_set=[(X_valid, y_valid)], verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=LGBMRegressor(\n        n_estimators=200,\n        learning_rate=0.03,\n        num_leaves=32,\n        colsample_bytree=0.9497036,\n        subsample=0.8715623,\n        max_depth=8,\n        reg_alpha=0.04,\n        reg_lambda=0.073,\n        min_split_gain=0.0222415,\n        min_child_weight=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation accuracy for 3 folds\nscores = cross_val_score(model, X_train, y_train, cv=3)\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the test set predictions and clip values to the specified range\ny_pred = model.predict(X_test).clip(0., 20.)\n\n# make sure results are in the same order as the original test set\n(test[['shop_id','item_id']].values == X_test[['shop_id','item_id']]).all()\n\n# Create the submission file and submit!\npreds = pd.DataFrame(y_pred, columns=['item_cnt_month'])\npreds.to_csv('submission.csv',index_label='ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(test[['shop_id','item_id']].values))\nprint(len(X_test[['shop_id','item_id']]))\nprint(len(X_test_full))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define model with best MAE\nmodel = XGBRegressor(colsample_bytree=0.6, learning_rate=.01, max_depth=8, min_child_weight=3, n_estimators=5000, \n                     nthread=1, objective='reg:squarederror', subsample=0.6, random_state=21, \n                     early_stopping_rounds = 10, verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_pipeline = Pipeline(steps=[('model', model)])\nmy_pipeline.fit(X_train, y_train)\n\nprint(\"Training finished! Now let's predict test values.\")\n\npreds_test = my_pipeline.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimize_xgb(X_trian, y_train): \n    xgb1 = XGBRegressor()\n    parameters = {'nthread':[1], #when use hyperthread, xgboost may become slower\n                  'objective':['reg:linear'],\n                  'learning_rate': [.03, .02, .01, .0075, .005], #so called `eta` value\n                  'max_depth': [5, 6, 7],\n                  'min_child_weight': [3, 4, 5, 6],\n                  'subsample': [0.7],\n                  'colsample_bytree': [0.7],\n                  'n_estimators': [1000, 2500]}\n\n    xgb_grid = GridSearchCV(xgb1,\n                            parameters,\n                            cv = 2,\n                            n_jobs = 5,\n                            verbose=True)\n\n    xgb_grid.fit(X_train, y_train)\n\n    print(xgb_grid.best_score_)\n    print(xgb_grid.best_params_)\noptimize_xgb(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define model with best MAE\nmodel = XGBRegressor(colsample_bytree=0.7, learning_rate=.01, max_depth=6, min_child_weight=3, n_estimators=3000, \n                     nthread=1, objective='reg:squarederror', subsample=0.7, random_state=21, \n                     early_stopping_rounds = 10, eval_set=[(X_valid, y_valid)], verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the test set predictions and clip values to the specified range\ny_pred = preds_test.clip(0., 20.)\n\n# make sure results are in the same order as the original test set\n(test[['shop_id','item_id']].values == X_test[['shop_id','item_id']]).all()\n\n# Create the submission file and submit!\npreds = pd.DataFrame(y_pred, columns=['item_cnt_month'])\npreds.to_csv('submission.csv',index_label='ID')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}