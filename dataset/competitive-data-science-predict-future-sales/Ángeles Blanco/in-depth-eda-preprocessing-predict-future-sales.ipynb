{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Predict Future Sales**\n## *Exploratory Data Analysis and Preprocessing*","metadata":{}},{"cell_type":"markdown","source":"This challenge is marked as final project for the [\"How to win a data science competition\"](https://www.coursera.org/learn/competitive-data-science/home/welcome) Coursera course.\n\nIn [this competition](https://www.kaggle.com/c/competitive-data-science-predict-future-sales) we will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms, [1C Company](http://1c.ru/eng/title.htm). \n\nThey are asking to **predict total sales for every product and store in the next month**. I have previously translated and processed their datasets in my notebook [[Translation/Text Processing] Predict Future Sales](https://www.kaggle.com/tymecd/translation-text-processing-predict-future-sales?scriptVersionId=84610744). They also tell us that submissions are evaluated by root mean squared error (RMSE) and that true **target values are clipped into [0,20] range**.\n\nIn this notebook we will continue with **understanding and preparing the data** for the subsequent modeling part. The competition is marked as **_Playground_** type, and I can see that it is indeed a playground. There are **endless possibilities** with these datasets. We'll start with first approaches, and add more explorations and studies in following versions.","metadata":{}},{"cell_type":"markdown","source":"### Tasks covered\n- [x] Process Russian texts and create new variables\n- [x] Perform some EDA and data mining for deeper understanding, and prepare data\n- [ ] Create model and evaluate results","metadata":{}},{"cell_type":"markdown","source":"### Content\n* [1. Libraries](#1.-Libraries).\n* [2. Datasets profiling and description](#2.-Dataset-profiling-and-description).\n    + [2.1. Items, categories and shops catalogues](#2.1.-Items,-categories-and-shops-catalogues)\n    + [2.2. Sales and test sets](#2.2.-Train-and-test-sets)\n* [3. Preprocessing](#3.-Preprocessing).\n    + [3.1. Outliers](#3.1.-Outliers)\n        + [3.1.1. Item price](#3.1.1.-Item-price)\n        + [3.1.2. Item count](#3.1.2.-Item-count)\n    + [3.2. Cartesian product](#3.2.-Cartesian-product)\n        + [3.2.1. Reducing memory usage](#3.2.1.-Reducing-memory-usage)\n        + [3.2.2. Imputing new items](#3.2.2.-Imputing-new-items)\n    + [3.3. Feature engineering](#3.3.-Feature-engineering)\n        + [3.3.1. Shops time-series clustering](#3.3.1.-Shops-time-series-clustering)\n        + [3.3.2. Items RFM (Recency, Frequency and Monetary value)](#3.3.2.-Items-RFM-(Recency,-Frequency-and-Monetary-value))\n        + [3.3.3. Mean-based, trends, and other features](#3.3.3.-Mean-based,-trends,-and-other-features)\n    + [3.4. Quantitative variable transformations](#3.4.-Quantitative-variable-transformations)\n    + [3.5. Encoding categorical variables](#3.5.-Encoding-categorical-variables)\n    + [3.6. Dividing dataset](#3.6.-Dividing-dataset)\n* [4. Visualization summary](#4.-Visualization-summary).\n    \n* [Saving files](#Saving-files).","metadata":{}},{"cell_type":"markdown","source":"## 1. Libraries","metadata":{}},{"cell_type":"code","source":"! pip install tslearn","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:46:49.48555Z","iopub.execute_input":"2022-03-07T16:46:49.485864Z","iopub.status.idle":"2022-03-07T16:47:00.293973Z","shell.execute_reply.started":"2022-03-07T16:46:49.485788Z","shell.execute_reply":"2022-03-07T16:47:00.293166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# system\nimport os\nimport warnings\n\n# preprocessing\nimport pandas as pd\nimport numpy as np\nfrom itertools import product\nfrom calendar import monthrange\nfrom tslearn.preprocessing import TimeSeriesScalerMeanVariance\nfrom sklearn import preprocessing\n\n# data mining\nimport pandas_profiling as pf\nfrom tslearn.clustering import TimeSeriesKMeans, silhouette_score\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom collections import Counter\n\n# visualization\nimport ipywidgets as widgets\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_context('notebook', rc={\"axes.titlesize\":14, \"axes.labelsize\":13})\nsns.set_style('white')\n#alt_palette = [\"#111425\", \"#554946\", \"#006DE4\", \"#438BD0\", \"#AD997A\", \"#00565C\", \n#               \"#EA9C39\", \"#AD6B3E\"]\n#sns.set_palette(alt_palette)\nsns.palplot(sns.color_palette()) #default","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-03-07T16:47:00.295917Z","iopub.execute_input":"2022-03-07T16:47:00.296194Z","iopub.status.idle":"2022-03-07T16:47:04.026667Z","shell.execute_reply.started":"2022-03-07T16:47:00.296146Z","shell.execute_reply":"2022-03-07T16:47:04.025813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Datasets profiling and description\n\nNow, let's dive into the data! We start by loading the competition's datasets and inspecting their basic properties. The first step must always be to perform some EDA, not only to understand the data but also to start detecting possible problems.","metadata":{}},{"cell_type":"markdown","source":"#### File descriptions\n* `sales_train.csv` - the training set. Daily historical data from January 2013 to October 2015.\n* `test.csv` - the test set. We need to forecast the sales for these shops and products for November 2015.\n* `items.csv` - supplemental information about the items/products.\n* `item_categories.csv`  - supplemental information about the items categories.\n* `shops.csv`- supplemental information about the shops.\n\n#### Original data fields\n* `ID` - an Id that represents a (Shop, Item) tuple within the test set\n* `shop_id` - unique identifier of a shop\n* `item_id` - unique identifier of a product\n* `item_category_id` - unique identifier of item category\n* `item_cnt_day` - number of products sold. We are predicting a monthly amount of this measure\n* `item_price` - current price of an item\n* `date` - date in format dd/mm/yyyy\n* `date_block_num` - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n* `item_name` - name of item\n* `shop_name` - name of shop\n* `item_category_name` - name of item category","metadata":{}},{"cell_type":"code","source":"folder = 'input/competitive-data-science-predict-future-sales'\npath = f'../{folder}'\nfiles = os.listdir(path)\nprint(files)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:47:04.031062Z","iopub.execute_input":"2022-03-07T16:47:04.03141Z","iopub.status.idle":"2022-03-07T16:47:04.042698Z","shell.execute_reply.started":"2022-03-07T16:47:04.031369Z","shell.execute_reply":"2022-03-07T16:47:04.04196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# original datasets\nitems_original = pd.read_csv(path + '/' + files[0])\ncategories_original = pd.read_csv(path + '/' + files[2])\nshops_original = pd.read_csv(path + '/' + files[-2])\n\n# pretranslated datasets\nitems = pd.read_csv('../input/items-english/items_english.csv')\ncategories = pd.read_csv('../input/categories-english/categories_english.csv')\nshops = pd.read_csv('../input/shops-english/shops_english.csv')\n\n# sales and test\ntrain = pd.read_csv(path + '/' + files[-3])\ntest = pd.read_csv(path + '/' + files[-1])","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:47:04.047842Z","iopub.execute_input":"2022-03-07T16:47:04.049326Z","iopub.status.idle":"2022-03-07T16:47:06.545257Z","shell.execute_reply.started":"2022-03-07T16:47:04.049281Z","shell.execute_reply":"2022-03-07T16:47:06.544513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1. Items, categories and shops catalogues","metadata":{}},{"cell_type":"markdown","source":"The three catalogues have neither missing data nor any primary duplicates, so that's great. Nevertheless, after inspecting a bit the shops dataset, one can observe that there are three overlapping shops, with similar but different names. It's then corroborated by computing sales by each shop ID, so we will correct those IDs when preprocessing.\n\nThe main (potential) inconvenient is that all **names were given in Russian**, and, at least for now, that's not a language I'm fluent in. ðŸ˜… As I stated in a previous notebook, names may contain a lot of information about items and shops, and they will surely provide major insight towards creating a model for predicting sales. We've **already cleaned and translated all names to English**. We've also created new variables with provisional IDs, so we'll use those processed datasets. You can check the code for doing so with the link to my notebook provided in the introduction above, [[Translation/Text Processing] Predict Future Sales](https://www.kaggle.com/tymecd/translation-text-processing-predict-future-sales?scriptVersionId=84610744).","metadata":{}},{"cell_type":"code","source":"for df in [items, categories, shops]:\n    print('\\nOriginal dataset:')\n    if df is items:\n        display(items_original.head(3))\n    elif df is categories:\n        display(categories_original.head(3))\n    else:\n        display(shops_original.head(3))\n    print('Processed dataset:')\n    display(df.head(3))\n    print(df.shape)\n    print('\\nMissing values: \\n', df.isna().sum())\n    print('\\nUnique values: \\n', df.nunique())","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:47:06.547401Z","iopub.execute_input":"2022-03-07T16:47:06.547651Z","iopub.status.idle":"2022-03-07T16:47:06.616764Z","shell.execute_reply.started":"2022-03-07T16:47:06.547618Z","shell.execute_reply":"2022-03-07T16:47:06.615951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now join items and categories, and continue with inspecting the sales and test sets before aggregating all the info. We will also seize this cell to group here some categories that we've seen in further inspections after merging with sales data that have low quantities of items.","metadata":{}},{"cell_type":"code","source":"# grouping some categories with low quantity of items before computing new features\ncategories['category_name'] = (categories['category_name']\n                               .apply(lambda x: \n                                      'other games' if x in ['android games', 'mac games'] else \n                                      ('blank media' if 'blank media' in x else \n                                       ('service' if 'tickets' in x else x))))\ncategories['subcategory_name'] = (categories['subcategory_name']\n                                  .apply(lambda x: \n                                         'tickets' if 'tickets' in x else\n                                         ('blank media' if 'blank media' in x else x)))\n\n# overriding provisional IDs\ncategories['category_id'] = preprocessing.LabelEncoder().fit_transform(categories.category_name\n                                                                           .values).astype('int8')\ncategories['subcategory_id'] = preprocessing.LabelEncoder().fit_transform(categories.subcategory_name\n                                                                              .values).astype('int8')\n\nitems_categories = items.merge(categories, on='item_category_id', how='left')\nitems_categories.sample(5)\nprint('New number of categories: ', categories.category_id.nunique(), \n      '\\nNew number of subcategories: ', categories.subcategory_id.nunique())","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:47:06.618274Z","iopub.execute_input":"2022-03-07T16:47:06.61853Z","iopub.status.idle":"2022-03-07T16:47:06.647164Z","shell.execute_reply.started":"2022-03-07T16:47:06.618496Z","shell.execute_reply":"2022-03-07T16:47:06.646457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2. Sales and test sets","metadata":{}},{"cell_type":"markdown","source":"We can see that there are no missing data in both datasets, but we're lacking `item_price` column in the **test set**, as it merely consists of the **cartesian product of 42 shops and 5100 items** in November 2015, with 214200 total shop-item pairs. Also, there are more shops in the training set than there exist in the test set, and quantile ranges and standard deviation from mean give us a hint for the presence of outliers in `item_price` and `item_count` variables. We'll see them in detail in a subsequent report. After doing some more research, we'll start by creating a **new dataset** adding all the supplemental information, and later **build train, validation and test sets from it**.","metadata":{}},{"cell_type":"code","source":"it = 0\nfor df in [train, test]:\n    if it == 0:\n        print('----- Sales (Train) -----')\n    else:\n        print('----- Test -----')\n    display(df.head());\n    print(df.info(show_counts=True))\n    display(df.describe());\n    print('Unique values: \\n', df.nunique())\n    \n    it +=1","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:47:06.648577Z","iopub.execute_input":"2022-03-07T16:47:06.649039Z","iopub.status.idle":"2022-03-07T16:47:07.665977Z","shell.execute_reply.started":"2022-03-07T16:47:06.649002Z","shell.execute_reply":"2022-03-07T16:47:07.665141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This next profile report gives us quick and **further insight** into the variables. We see that we have 6 duplicate rows, and **histograms** tell us that there are some shops and items that have had much higher sales than others. Also, **outliers** are indeed present in both `item_price` and `item_cnt_day` variables, making them also highly skewed, with some of them looking like accounting or annotation errors. We'll have to take a closer look at those when preprocessing data.","metadata":{}},{"cell_type":"code","source":"# profile report with pandas profiling\npf.ProfileReport(train)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:47:07.667578Z","iopub.execute_input":"2022-03-07T16:47:07.667834Z","iopub.status.idle":"2022-03-07T16:48:22.68307Z","shell.execute_reply.started":"2022-03-07T16:47:07.667798Z","shell.execute_reply":"2022-03-07T16:48:22.682263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Total shops are not constant over time, nor are total items, as it can be seen in the following graph. To reflect test data, we'll also create a cartesian product of shops and items (mostly active, as we'll see later) every month for the train set, so this variation over time won't be such a cumbersome problem. Notwithstanding, we'll eliminate products and shops that seem to no longer be active. Let's dig deeper and inspect those included in the test set (i.e. November 2015) vs those not included.","metadata":{}},{"cell_type":"code","source":"# plot number of unique shops and items over time\nfig, ax1 = plt.subplots(figsize=(7,4))\n\nax1.set_xlabel('Month block')\nax1.set_ylabel('# of shops')\nax1.plot(train.set_index('date').groupby('date_block_num').nunique()[['shop_id']], \n         color='darkgreen', label='Shops')\nax1.legend(loc='upper left')\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\nax2.set_ylabel('# of items')  # we already handled the x-label with ax1\nax2.plot(train.set_index('date').groupby('date_block_num').nunique()[['item_id']], \n         color='lightgreen', alpha=.9, label='Items')\nax2.legend(loc='upper right')\nax2.set_title('Unique shops and items')\n\nfig.tight_layout()  # otherwise the right y-label is slightly clipped\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:48:22.685887Z","iopub.execute_input":"2022-03-07T16:48:22.686142Z","iopub.status.idle":"2022-03-07T16:48:25.633556Z","shell.execute_reply.started":"2022-03-07T16:48:22.686108Z","shell.execute_reply":"2022-03-07T16:48:25.632884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that shops included in the test set behave disparately than those not included. **In-test shops (ITS)**, as we'll call them, follow a **quasi-constant trend over time**, tilting upwards, with an oscilating 80%-100% remaining active; although the number of items sold is decreasing. **Out-of-test shops (OOTS)**, on the contrary, **behave very differently**, with a **decreasing trend** over time from 60% of active shops to a small 10%. Number of items sold, also decreases sharply. In the second graph, we can see that sales ratios have also different magnitudes. Thus, this confirms our necessity to remove these seemingly inactive shops from the sales dataset prior to a model construction, as they could add unnecessary bias. However, we won't simply eliminate those not present in the test set, since this would be a case of incorporating information from the future. We'll use inference for this.\n\nðŸ”¬ One **possible line of investigation** that is derived from here can be to divide the dataset into groups, maybe one for active shops and another for inactive shops -- either by means of a clustering algorithm or a rule-based segmentation incorporating their trends -- and develop separate models for each group. It could be a case of **stacking a classification algorithm with a regressor**. This could also help in the detection of shops that won't have very much success, or that are decreasing their revenue.","metadata":{}},{"cell_type":"code","source":"# plot percentage of unique shops and number of items over time\nfig, (ax1, ax3) = plt.subplots(1, 2, figsize=(15,4))\nax1.set_xlabel('Month block')\nax1.set_ylabel('% of active shops')\nax1.plot(train[~train.shop_id.isin(test.shop_id.unique())]\n         .set_index('date').groupby('date_block_num').nunique()[['shop_id']]*100/\n         len(train[~train.shop_id.isin(test.shop_id.unique())].shop_id.unique()), \n         color='darkblue', label='Out-of-test shops (OOTS)')\nax1.legend(loc='lower left')\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\nax2.set_ylabel('# of items')  # we already handled the x-label with ax1\nax2.plot(train[~train.shop_id.isin(test.shop_id.unique())]\n         .set_index('date').groupby('date_block_num').nunique()[['item_id']], \n         color='lightgrey', alpha=.8, label='OOTS Items')\nax2.legend(loc='upper right')\n\nax1.plot(train[train.shop_id.isin(test.shop_id.unique())]\n         .set_index('date').groupby('date_block_num').nunique()[['shop_id']]*100/len(test.shop_id.unique()), \n         color='red', label='In-test shops (ITS)')\nax1.legend(loc='lower left')\n\nax2.plot(train[train.shop_id.isin(test.shop_id.unique())]\n         .set_index('date').groupby('date_block_num').nunique()[['item_id']], \n         color='pink', alpha=.8, label='ITS Items')\nax2.legend(loc='upper right')\nax2.set_title('Active shops and unique items')\n\nfig.tight_layout()  # otherwise the right y-label is slightly clipped\n\nax3.plot(train[~train.shop_id.isin(test.shop_id.unique())]\n         .set_index('date').groupby('date_block_num').sum()[['item_cnt_day']]/\n         len(train[~train.shop_id.isin(test.shop_id.unique())].shop_id.unique()), \n         color='blue', linestyle='dashed', label='OOTS Sales Ratio')\n\nax3.plot(train[train.shop_id.isin(test.shop_id.unique())]\n         .set_index('date').groupby('date_block_num').sum()[['item_cnt_day']]/len(test.shop_id.unique()), \n         color='orange', linestyle='dashed', label='ITS Sales Ratio')\nax3.legend(loc='upper right')\nax3.set_xlabel('Month block')\nax3.set_ylabel('# of sales')\nax3.set_title('Total sales per shop')\n\nfig.tight_layout()  # otherwise the right y-label is slightly clipped\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:48:25.634855Z","iopub.execute_input":"2022-03-07T16:48:25.63528Z","iopub.status.idle":"2022-03-07T16:48:29.310004Z","shell.execute_reply.started":"2022-03-07T16:48:25.635242Z","shell.execute_reply":"2022-03-07T16:48:29.30934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These next number figures tell us even more things. There are **363 items for which we won't have any item supplemental information**, so we will have to infer it from closer items (we'll see later how). Even though there are no new shops, when computing shop-item pairs, we're left with a total of 102796 new shop-item pairs. From those, 15246 new pairs are due to new items, and the rest is simply due to the cartesian product. So, the only thing we have to do is to **infer the information for those items**, as we have everything we need for shops.","metadata":{}},{"cell_type":"code","source":"print('Number of shops not included in train (new shops): ', \n      test[~test.shop_id.isin(train.shop_id.unique())].shop_id.nunique())\nprint('Number of items not included in train (new items): ',\n      test[~test.item_id.isin(train.item_id.unique())].item_id.nunique())\nprint('\\nNumber of shop/item pairs to predict: ', \n      len(test.groupby(['shop_id', 'item_id']).count()))\nmerged_df = (train[['shop_id', 'item_id']].drop_duplicates()\n             .merge(test, on=['shop_id', 'item_id'], how='right', indicator=True))\nprint('Number of new shop/item pairs: ', \n      len(merged_df[merged_df['_merge'] == 'right_only']))\nprint('    Number of new shop/item pairs due to new items: ', \n      len(merged_df[(merged_df['_merge'] == 'right_only') & (~merged_df.item_id.isin(train.item_id.unique()))]))\nprint('Number of shop/item pairs in both sets: ', \n      len(merged_df[merged_df['_merge'] == 'both']))","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:48:29.311341Z","iopub.execute_input":"2022-03-07T16:48:29.311694Z","iopub.status.idle":"2022-03-07T16:48:29.891032Z","shell.execute_reply.started":"2022-03-07T16:48:29.31166Z","shell.execute_reply":"2022-03-07T16:48:29.890134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will also pay attention to **items that seem to be outdated** when evaluating our model. Depending on the item, it would be anomalous to have a non-zero prediction for them. Besides, it depends on our definition of outdated, but there are items that have not even been sold for the last 2 years. If the model does not capture them, we could add some sort of \"business rule\". Also, there are 4 items in train that have not been sold in any month of the whole dataset and are not present in test, so we'll just delete them.\n\nðŸ”¬ There's a simple **customer segmentation model** in retail sales called **RFM**, which stands for Recency, Frequency, and Monetary value, that could be helpful in this project. We'll explain it later when performing feature engineering, but it basically consists of executing a segmentation of customers based on the recency of their purchases, the frequency with which they buy, and the money they spend. We could make an **inverse analogy with items** to add interesting features for a model.","metadata":{}},{"cell_type":"code","source":"def pivot_df(df, index, values, columns, agg=np.sum, fill=0):\n    \"\"\"Pivot dataframe and arrange levels.\n    \n    Parameters\n    ----------\n    df: pandas dataframe\n    index: list\n        Index for pivoted df.\n    values: list\n        Column to aggregate.\n    columns: str\n        Column with levels to expand as new columns.\n    agg: numpy function\n        Function for aggregation.\n    fill: object\n        Value for filling empty values.\n    \"\"\"\n    pivoted = df.pivot_table(index=index, values=values, \n                             columns=columns, aggfunc=agg, fill_value=fill).reset_index()\n    pivoted.columns = pivoted.columns.droplevel()\n    pivoted = pivoted.set_index('').rename_axis(index[0], axis=1)\n    \n    return pivoted\n\nsales_by_item_id = pivot_df(train, ['item_id'], ['item_cnt_day'], 'date_block_num')\n\n# calculating \"outdated items\"\nfor blocknum in [0, 10, 22, 28]:\n    \n    outdated_items = sales_by_item_id[sales_by_item_id.iloc[:, blocknum:].sum(axis=1) == 0]\n\n    print(f'Number of items that have not been sold for the past {len(outdated_items.columns) - blocknum} months (outdated items): ',\n          len(outdated_items))\n    print('Number of these outdated items included in test set: ',\n          test[test.item_id.isin(outdated_items.index)].item_id.nunique(), '\\n')","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:48:29.894957Z","iopub.execute_input":"2022-03-07T16:48:29.895292Z","iopub.status.idle":"2022-03-07T16:48:30.442823Z","shell.execute_reply.started":"2022-03-07T16:48:29.895256Z","shell.execute_reply":"2022-03-07T16:48:30.442097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the table below we can see a sample of 5 shops with sales lower than that of the mean for each month. Some are very recent. Shop 36, in particular, has just been added in October 2015, the previous month of test -- which will be used for validation. For this kind of recent shops, we'll maybe have to include the prediction of some other model, possibly based on similarity scores, or use the help of clustering algorithms, as we said earlier. On the other hand, shops with decreasing trends to zero are not present in the test set (OOTS), as the test set only consists of active shops in November 2015.","metadata":{}},{"cell_type":"code","source":"# computing sales by shop\nsales_by_shop_id = pivot_df(train, ['shop_id'], ['item_cnt_day'], 'date_block_num')\n\n# sales lower than mean for month\nsales_by_shop_id[sales_by_shop_id < sales_by_shop_id.mean(axis=1).mean()].dropna().tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:48:30.443977Z","iopub.execute_input":"2022-03-07T16:48:30.44426Z","iopub.status.idle":"2022-03-07T16:48:30.687879Z","shell.execute_reply.started":"2022-03-07T16:48:30.444223Z","shell.execute_reply":"2022-03-07T16:48:30.687069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following graphs show sales by shop over time wrapped by the categories we created when processing their name. For the first three types, we can see a similar behaviour, with major sales around the last quarter of each year and spikes in the months of December, coinciding with shopping for New Year's and Christmas Eve in Russia. We can see that shops deemed special and online behave very differently. The only sales in special shops are during this high-demand season, being possibly prepared for these events, and online shops show an incresing demand over time. ","metadata":{}},{"cell_type":"code","source":"long_sales_by_shop_id = (sales_by_shop_id.reset_index()\n                   .melt(id_vars='', var_name='date_block_month', value_name='sales')\n                   .merge(shops[['shop_id', 'shop_type_name']], right_on='shop_id', left_on='', how='left'))\n                          \ng = sns.FacetGrid(long_sales_by_shop_id,\n                   col = \"shop_type_name\", col_wrap = 5, aspect = 1.2, sharey = False, sharex = True, despine = False);\ng.map_dataframe(sns.lineplot, x='date_block_month', \n                y='sales', hue='shop_id').set_axis_labels(\"Month number\", \"# of sales\");\ng.set_titles(\"Shop type: {col_name}\");","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:48:30.691979Z","iopub.execute_input":"2022-03-07T16:48:30.692171Z","iopub.status.idle":"2022-03-07T16:48:32.566644Z","shell.execute_reply.started":"2022-03-07T16:48:30.692147Z","shell.execute_reply":"2022-03-07T16:48:32.565941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All of this means that our shop-item sales time series are, as expected, **heteroskedastic and non-stationary, with strong seasonal components and varying trends**, being both trend-stationary and difference-stationary. As a further remark, it therefore follows that our series could be made stationary after de-trending and then taking differences between seasons, but we won't be testing for a definitive answer, as the methods we'll use to forecast do not assume this property of stationarity and the complexity of our data would hinder accurate computations. Nonetheless, we'll **engineer features to account for these components**. \n\nðŸ’¡ In addition, our data shows a **clear hierarchical structure**, in which the lower levels (items) are nested within the higher-level groups (shops). However, not only are our series hierarchical, but also **grouped**, as we can have multiple levels of detail: item category, price range, etc; thus, our disaggregating factors are both nested and crossed. As such, having this grouped structure in mind when performing feature engineering and modelling will be paramount. ","metadata":{}},{"cell_type":"markdown","source":"In this section we won't be modifying our datasets, as our aim was to merely introduce them and compute basic statistics. In the following section, we'll **continue to the core task of preprocessing our data**. Let's go!","metadata":{}},{"cell_type":"markdown","source":"## 3. Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Hands-on work!","metadata":{}},{"cell_type":"markdown","source":"In order to make better visualizations and derive greater insights in this section, we'll merge all the information together in the next cell. Nonetheless, this first conglomerate won't be our final dataset. As we stated earlier, we'll make a cartesian product of items and shops for the trainning set to reflect the testing set.","metadata":{}},{"cell_type":"code","source":"sales_pre = train.merge(items_categories, on='item_id', how='left').merge(shops, on='shop_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:48:32.567702Z","iopub.execute_input":"2022-03-07T16:48:32.568072Z","iopub.status.idle":"2022-03-07T16:48:33.615652Z","shell.execute_reply.started":"2022-03-07T16:48:32.568036Z","shell.execute_reply":"2022-03-07T16:48:33.614862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we've seen earlier, our first step will be to update shop IDs, remove items with no sales at all and duplicates in the sales dataset. We have to act as having no information from the future, i.e. from the test set or validation set (which we'll create later with October 2015), so we cannot simply remove shops and items that are not present in test. For a simple mark, we'll compute activity based on the last information in September 2015.","metadata":{}},{"cell_type":"code","source":"sales = sales_pre.copy()\n\n# assigning current shops to old ones\nfor df in [sales, test]:\n    df['shop_id'] = np.where(df.shop_id == 0, 57, \n                             np.where(df.shop_id == 1, 58, \n                                      np.where(df.shop_id == 11, 10, df.shop_id)))\n\n# eliminating completely outdated items\nsales = sales[~sales.item_id.isin(sales_by_item_id.sum(axis=1) == 0)]\n\n# eliminating duplicates\nsales = sales.drop_duplicates()\n\n# adding datetime format date for easier processing\nsales['datetime_date'] = pd.to_datetime(sales.date, format=\"%d.%m.%Y\")\ndel sales['date']","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:48:33.61712Z","iopub.execute_input":"2022-03-07T16:48:33.617394Z","iopub.status.idle":"2022-03-07T16:48:38.553751Z","shell.execute_reply.started":"2022-03-07T16:48:33.617359Z","shell.execute_reply":"2022-03-07T16:48:38.552896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1. Outliers","metadata":{}},{"cell_type":"markdown","source":"In the previous report, we saw that variables `item_price` and `item_cnt_day` are heavily right-skewed. Let's visualize them better and inspect them. Most parametric statistics, such as means, are highly sensitive to outliers, and they can really distort an analysis if we don't deal with them well. However, outliers may be legitimate observations and hide lots of insights, so it's important to investigate their nature before deciding what to do with them. What you'll see here is a first approach on dealing with these seemingly anomalous observations.","metadata":{}},{"cell_type":"markdown","source":"#### 3.1.1. Item price","metadata":{}},{"cell_type":"markdown","source":"The maximum value in item price is 2 orders of magnitude above the highest value of the interquartile range for all sold items, as can be seen on the introduction. Taking a closer look at it, we can see that it belongs to a single item named _\"radmin 3 522 persons\"_ bought once. Radmin is a remote access software product, and operates through licenses. One can buy buckets of hundreds of licenses, and it looks like a bucket for 522 persons was bought here. If we look for more Radmin products in the dataset, we can find another item called _\"radmin 3 1 person\"_ for a price of RUB 1299, which corroborates our hypotheses. Also, probably, buckets have discounts over the total price. We will eliminate this item from the dataset to yield more accurate predictions.","metadata":{}},{"cell_type":"code","source":"higher = sales[sales.item_price > sales.item_price.quantile(0.999)] # highest 0.1%\nlower = sales[sales.item_price < sales.item_price.quantile(0.001)] # lowest 0.1%\n# alternatively one can compute the z scores; depends on the necessity\n\ndisplay(sales[sales.item_name.str.contains('radmin')][['shop_city_name', 'shop_type_name',\n                                               'item_name', 'item_id','item_price']].drop_duplicates(['item_name', \n                                                                                                      'item_price']))\n\n# removing outlier with maximum price\nsales = sales[sales.item_id != 6066]","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:48:38.555154Z","iopub.execute_input":"2022-03-07T16:48:38.55543Z","iopub.status.idle":"2022-03-07T16:48:41.312867Z","shell.execute_reply.started":"2022-03-07T16:48:38.555395Z","shell.execute_reply":"2022-03-07T16:48:41.312129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the next two subplots we can see that the distributions of item prices widely differ among categories, being _game consoles_ the one containing the biggest number of expensive items, and having the highest mean (marked as a little cyan triangle in the graphs). For now, we'll remove items above the maximum price in this category, which amount to various orders of magnitude below the 0.0001% of our data and they are mere marginal outliers. We'll see how the rest affect the results of the model, and then, we'll decide upon different procedures, maybe deleting outliers by category. Nonetheless, we'll mark the top 1% for later usage.\n\nðŸ”¬ As our variable spans various orders of magnitude, a logarithmic transformation when preparing data for model creation can surely be beneficial. For example, _gifts_ category is the one with most price variance, spanning 6 magnitude orders, and this can be better discovered by the second graph.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,4))\nplt.subplot(121)\nboxplot_sales = sales.reset_index()\nax = sns.boxplot(data = boxplot_sales, \n                 x='category_name', y='item_price', showfliers=True, showmeans=True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\")\nplt.xlabel('Item category'); plt.ylabel('Item price [RUB]');\nplt.title('Price distribution amongst categories');\n\nplt.subplot(122)\nboxplot_sales = sales.reset_index()\nax = sns.boxplot(data = boxplot_sales, \n                 x='category_name', y=np.log10(abs(boxplot_sales.item_price)), showfliers=True, showmeans=True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\")\nplt.xlabel('Item category'); plt.ylabel('Log10(Item price)');\nplt.title('Price magnitude orders amongst categories');\nplt.tight_layout();\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-07T16:48:41.314214Z","iopub.execute_input":"2022-03-07T16:48:41.314473Z","iopub.status.idle":"2022-03-07T16:48:48.495691Z","shell.execute_reply.started":"2022-03-07T16:48:41.31444Z","shell.execute_reply":"2022-03-07T16:48:48.494344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# deleting outliers above max price for game consoles category\nsales = sales[sales.item_price < max(sales[sales.category_name == 'game consoles'].item_price)]\n\n# marking the highest 1%\nsales['highest_price'] = np.where(sales.item_id.isin(higher.item_id.unique()), 'highest', 'regular')","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:48:48.497501Z","iopub.execute_input":"2022-03-07T16:48:48.497885Z","iopub.status.idle":"2022-03-07T16:48:49.820461Z","shell.execute_reply.started":"2022-03-07T16:48:48.497846Z","shell.execute_reply":"2022-03-07T16:48:49.819684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Regarding the lowest prices, we can see that there are items with values even below RUB 1, which is currently 1 cent of EUR after conversion, and that do not make any sense. If we compare them to similar items, for example, some _stuffed toys_\nthat can be seen in the next table, we can observe that these low priced items are undoubtedly due to errors. We'll impute them with the median of their subcategory.","metadata":{}},{"cell_type":"code","source":"# lowest 1% of prices\ndisplay(lower.drop_duplicates(['item_price'])[['item_id','item_name', 'category_name',\n                                               'subcategory_name', 'item_price']].sort_values(by='item_price').head(3))\n\n# sample for some stuffed toys\ndisplay(sales[(sales.item_name.str.contains('cm')) & \n              (sales.subcategory_name == 'stuffed toys')].sample(3)[['item_id','item_name', 'category_name',\n                                                             'subcategory_name', 'item_price']])","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:48:49.821889Z","iopub.execute_input":"2022-03-07T16:48:49.822141Z","iopub.status.idle":"2022-03-07T16:48:52.458711Z","shell.execute_reply.started":"2022-03-07T16:48:49.822106Z","shell.execute_reply":"2022-03-07T16:48:52.458002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imputing with median of category for values below RUB 1\nsales['item_price'] = np.where(sales.item_price <= 1, np.nan, sales.item_price)\nsales['item_price'] = sales.groupby(\"subcategory_name\")['item_price'].transform(lambda x: x.fillna(x.median()))","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:48:52.460084Z","iopub.execute_input":"2022-03-07T16:48:52.460472Z","iopub.status.idle":"2022-03-07T16:48:53.187065Z","shell.execute_reply.started":"2022-03-07T16:48:52.460436Z","shell.execute_reply":"2022-03-07T16:48:53.18633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.2. Item count","metadata":{}},{"cell_type":"markdown","source":"The first approach we'll take for the number of purchases in a day for an item will be to remove those higher than 10e3 counts in one order, as they look influential, and leave the ones below zero untouched, as we'll suppose that they're returned items, and a net amount will be computed when aggregating -- ideally, we would ask the provider about this (they could be systemic errors, for example). Analogously to `item_price`, we'll mark the highest 0.1%, but after aggregating by month. \n\nWhen evaluating the model, we'll see how our approach behaves and maybe think about more elaborate ways of dealing with outliers. It's also important to note that this variable will be clipped in the range (0,20) when making predictions, and that's why we're being cautelous for now with its anomalous values.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,4))\nplt.subplot(131)\nboxplot_sales = sales.reset_index()\nax = sns.boxplot(data = boxplot_sales, \n                 x='category_name', y='item_cnt_day', showfliers=True, showmeans=True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\")\nplt.xlabel('Item category'); plt.ylabel('Number of sales');\nplt.title('Sales amongst categories');\n\nplt.subplot(132)\nboxplot_sales = sales.reset_index()\nax = sns.boxplot(data = boxplot_sales, \n                 x='shop_type_name', y=boxplot_sales.item_cnt_day, showfliers=True, showmeans=True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\")\nplt.xlabel('Shop type'); ax.set(ylabel=None)\nplt.title('Sales amongst shop types');\n\nplt.subplot(133)\nboxplot_sales = sales.reset_index()\nax = sns.boxplot(data = boxplot_sales, \n                 x='shop_city_name', y='item_cnt_day', showfliers=True, showmeans=True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\")\nplt.xlabel('City'); ax.set(ylabel=None)\nplt.title('Sales amongst cities');\nplt.tight_layout();\nplt.show()\n\n# removing those above 10\nsales = sales[sales.item_cnt_day <= 10e3]","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:48:53.188367Z","iopub.execute_input":"2022-03-07T16:48:53.188599Z","iopub.status.idle":"2022-03-07T16:49:04.391635Z","shell.execute_reply.started":"2022-03-07T16:48:53.188568Z","shell.execute_reply":"2022-03-07T16:49:04.390732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, note the difference between these two lists. The first is not the number of sales, but the number of times the category is repeated across the dataset. _PC games_ is the greatest in total sales, and appearing third in the first list means that it has more bulk orders, and therefore, more outliers, as can be seen in the previous graphs.","metadata":{}},{"cell_type":"code","source":"print('Top 5 categories (value counts):\\n', sales.category_name.value_counts()[:5], '\\n')\nprint('Top 5 categories (sales):\\n',\n      sales.groupby('category_name')['item_cnt_day'].sum().sort_values(ascending=False)[:5])","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:49:04.393031Z","iopub.execute_input":"2022-03-07T16:49:04.393387Z","iopub.status.idle":"2022-03-07T16:49:04.916324Z","shell.execute_reply.started":"2022-03-07T16:49:04.393348Z","shell.execute_reply":"2022-03-07T16:49:04.914901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. Cartesian product","metadata":{}},{"cell_type":"markdown","source":"We will now create the cartesian product of shops and items for each month so as to have a 0 every time an item in each unique shop-item pair has not been sold within that month, and join it with our previous dataframes. This way train data will be analogous to test data. Also, we'll seize this subsection to reduce memory usage.","metadata":{}},{"cell_type":"code","source":"## aggregation\n# calculating revenue before aggregation\nsales['revenue'] = sales['item_cnt_day'] * sales['item_price']\n\n# changing dates to first day of month before aggregation and adding year and month\nsales['date'] = sales.datetime_date.apply(lambda x: x.replace(day=1))\nsales['year'] = sales.datetime_date.apply(lambda x: x.year)\nsales['month'] = sales.datetime_date.apply(lambda x: x.month)\n\n# creating dict for aggregation\ndic = {}\nfor key in sales.columns:\n    vals = {key: 'sum' if key in ['item_cnt_day', 'revenue'] else 'last'}\n    dic.update(vals)\ndel dic['date_block_num'], dic['shop_id'], dic['item_id']\n\n# grouping by month, shop and item\ngrouped_sales = (sales\n                 .groupby(['date_block_num', 'shop_id', 'item_id'])\n                 .agg(dic)).reset_index()\n\n# adjusting negative item counts even after aggregation\ngrouped_sales['item_cnt_day'] = np.where(grouped_sales.item_cnt_day < 0, 0, grouped_sales.item_cnt_day)\ngrouped_sales['revenue'] = np.where(grouped_sales.item_cnt_day == 0, 0, grouped_sales.revenue)\n\n## product of shops x items within each month\n# creating cartesian product for new dataset\ndf = [] \nfor block_num in grouped_sales['date_block_num'].unique():\n    active_shops = grouped_sales[grouped_sales['date_block_num'] == block_num].shop_id.unique()\n    active_items = grouped_sales[grouped_sales['date_block_num'] == block_num].item_id.unique()\n    df.append(np.array(list(product(*[active_shops, active_items, [block_num]]))))\n\ndata = pd.DataFrame(np.vstack(df), columns=['shop_id', 'item_id', 'date_block_num'])\n\n# adding test product\ntest['date_block_num'] = 34\ndel test['ID']\ndata = pd.concat([data, test], ignore_index=True, sort=False)\ndata.count()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:49:04.917802Z","iopub.execute_input":"2022-03-07T16:49:04.918052Z","iopub.status.idle":"2022-03-07T16:50:24.07571Z","shell.execute_reply.started":"2022-03-07T16:49:04.918017Z","shell.execute_reply":"2022-03-07T16:50:24.075044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n## adding information\n# merging on item, shop and month together\ndata_second = (data.merge(grouped_sales[['shop_id', 'item_id', 'date_block_num',\n                                         'item_price', 'item_cnt_day', 'revenue']].drop_duplicates(), \n                                on=['shop_id', 'item_id', 'date_block_num'], how='left'))\n\n# imputing values for shop-item pairs not in sales data for corresponding month but with item information\n# revenue and count will be simply 0, whereas item price will be a median of item prices for each item ID\nvalues = {'item_cnt_day': 0, 'revenue': 0, 'item_price': np.nan}\ndata_second.fillna(value=values, inplace=True)\n# imputing price\ndata_second = data_second.merge(sales.groupby(['date_block_num', 'item_id'])['item_price'].median().reset_index(),\n                                on=['date_block_num', 'item_id'], how='left')\ndata_second['item_price'] = np.where(data_second.item_price_x.isna(), \n                                     data_second.item_price_y, data_second.item_price_x)\ndel data_second['item_price_x'], data_second['item_price_y']\n# filling month 34\ndata_second['item_price'] = data_second['item_price'].fillna(data_second.groupby(['item_id'])['item_price'].ffill())\n\n# merging on item, shop and month separately\nitems_cols = list(items_categories.columns)\nitems_cols.remove('item_category_id')\nitems_cols.append('highest_price')\nshops_cols = list(shops.columns)\n\ndata_third = (data_second\n              .merge(grouped_sales[items_cols].drop_duplicates(), on='item_id', how='left')\n              .merge(grouped_sales[shops_cols].drop_duplicates(), on='shop_id', how='left')\n              .merge(grouped_sales[['year', 'month', 'date', 'date_block_num']].drop_duplicates(), \n                     on='date_block_num', how='left')\n              .rename(columns={'item_cnt_day': 'item_cnt_month',\n                               'item_name_en_number_stopwords': 'item_stopwords',\n                               'item_name_en_number_nouns': 'item_nouns',\n                               'item_name_en_number_words': 'item_words'}))\n\n# filling month 34\ndata_third['year'] = data_third['year'].fillna(2015).apply(int).astype('int16')\ndata_third['month'] = data_third['month'].fillna(11).apply(int).astype('int8')\ndata_third['date'].fillna(pd.to_datetime(\"01.11.2015\", format=\"%d.%m.%Y\"), inplace=True)\n\n# marking highest 0.1% sales count for after use\ndata_third['highest_count'] = np.where(data_third.item_cnt_month >= \n                                       data_third.query('item_cnt_month > 0').item_cnt_month.quantile(0.999),\n                                       'highest', 'regular')\n# saving memory\ndel data_second, data, grouped_sales, sales","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:50:24.07709Z","iopub.execute_input":"2022-03-07T16:50:24.077354Z","iopub.status.idle":"2022-03-07T16:51:01.951975Z","shell.execute_reply.started":"2022-03-07T16:50:24.077319Z","shell.execute_reply":"2022-03-07T16:51:01.950471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, everything is well according to figures. Previously, we saw that the total number of new shop-item pairs due to the 363 new items incorporated in November 2015 was 15246, and, after the previous operations, we would expect to see that number of observations without any item information, as total observations (11127604) - non-null observations (11112358) = 15246. The proportion is significant (~7% of total samples in the test set). Thus we will have to **impute these values** before continuing. There are various modules that offer several imputation alternatives but, as our case is special, we'll take a straightforward approach, filling them with the median or the mode for **similar groups based on our informed variables**.\n\nðŸ”¬ When imputing multiple variables with frequent missing values, it is convenient to perform a **sensibility analysis** to explore whether our imputation system is introducing bias in the data. As our missing values correspond to the test set, we'll have to take this into account when evaluating our model.","metadata":{}},{"cell_type":"code","source":"data_third.info(null_counts=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:51:01.953472Z","iopub.execute_input":"2022-03-07T16:51:01.953732Z","iopub.status.idle":"2022-03-07T16:51:09.551839Z","shell.execute_reply.started":"2022-03-07T16:51:01.953697Z","shell.execute_reply":"2022-03-07T16:51:09.551083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2.1. Reducing memory usage","metadata":{}},{"cell_type":"markdown","source":"We will be _downcasting_ our data types in this subsection in order to reduce memory usage and speed up processes.","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(data):\n    \"\"\" Iterate through all the columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \n    Parameters\n    ----------\n    data: Pandas dataframe\n    \"\"\"\n    start_mem = data.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in data.columns:\n        col_type = data[col].dtype\n        if str(col_type).startswith('int'):\n            if max(data[col] > 30000):\n                new_type = 'int32'\n            elif max(data[col] > 100):\n                new_type = 'int16'\n            else:\n                new_type = 'int8'\n            data[col] = data[col].astype(new_type)\n        elif str(col_type).startswith('float'):\n            if max(data[col] > 6e4):\n                new_type = 'float32'\n            else:\n                new_type = 'float16'\n            data[col] = data[col].astype(new_type)\n        elif str(col_type) == 'object':\n            data[col] = data[col].astype('category')\n\n    end_mem = data.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:51:09.553217Z","iopub.execute_input":"2022-03-07T16:51:09.553624Z","iopub.status.idle":"2022-03-07T16:51:09.563202Z","shell.execute_reply.started":"2022-03-07T16:51:09.553587Z","shell.execute_reply":"2022-03-07T16:51:09.562372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nopt_data = reduce_mem_usage(data_third)\ndel data_third","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:51:09.565967Z","iopub.execute_input":"2022-03-07T16:51:09.566259Z","iopub.status.idle":"2022-03-07T16:51:50.733376Z","shell.execute_reply.started":"2022-03-07T16:51:09.566221Z","shell.execute_reply":"2022-03-07T16:51:50.732517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2.2. Imputing new items","metadata":{}},{"cell_type":"markdown","source":"As stated previously, we'll impute values for new items based on the columns where we can extract some information. We'll use buckets of aproximately twenty items based on `item_id` and compute the median to assign it to new items. Our assumption, as we have observed, is that similar items have neighbour IDs. After doing some testing, I've decided that we won't group by other columns -- such as shop type -- because of neighbouring complexity. The more columns we add, the more difficult it will be to find adjacent neighbours with ID buckets.","metadata":{}},{"cell_type":"code","source":"%%time\n\nitems_cols = [col for col in items_cols if col not in['item_name_en_number_words',\n                                                  'item_name_en_number_stopwords',\n                                                  'item_name_en_number_nouns']]\nitems_cols.extend(['item_stopwords', 'item_words', 'item_nouns', 'item_price'])\n\ndef basicImputing(df):\n    \"\"\"Function for imputing new items based on median or mode.\n    \n    Parameters\n    ----------\n    df: pandas dataframe\n    \"\"\"\n    # this will take the neighbours in 1000 buckets, aprox. 22 neighbours in each bucket given total items\n    df['neighbours'] = pd.cut(df.item_id, 1000)\n    \n    # imputing with median for neighbours\n    # neighbour ids are similar\n    for col in items_cols:\n        if df[col].isnull().values.any():\n            if str(df[col].dtype) == 'category': # if col is categorical, we take the mode of the group\n                df[col] = (df.groupby(['neighbours'])[col]\n                           .transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else np.nan)))\n            else:\n                df[col] = (df.groupby(['neighbours'])[col]\n                           .transform(lambda x: x.fillna(x.median())))\n        else:\n            pass\n    \n    return df\n\nopt_data = basicImputing(opt_data)\nprint('Missing values left: ', opt_data.isna().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:51:50.734599Z","iopub.execute_input":"2022-03-07T16:51:50.73508Z","iopub.status.idle":"2022-03-07T16:52:21.076083Z","shell.execute_reply.started":"2022-03-07T16:51:50.735033Z","shell.execute_reply":"2022-03-07T16:52:21.075346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 Feature engineering","metadata":{}},{"cell_type":"markdown","source":"This is yet another important step in every data science problem. A model is only as good as the data it is fed, right? Since we're dealing with time series, things get trickier. When computing new features, we have to pay attention not to add any new information in a past moment that wouldn't be available at that time. Every computation that involves **calculations along the time axis** has to be **performed on a time cross-section basis or on expanding time windows** to **avoid _data leakage_**. Models with data leakage lead to good performances in the trainning set, even in the test set, but result in poor implementations in production, which, in the end, is all that matters.","metadata":{}},{"cell_type":"markdown","source":"#### 3.3.1. Shops time-series clustering","metadata":{}},{"cell_type":"markdown","source":"Clustering is the task of grouping together similar objects; hence, it heavily depends on the notion of similarity one relies on. **Time series clustering allow us to cluster series based on its shape along the time axis**, and we'll use it here to find possible groups of shops based on their sales behaviour. We'll find the similarities within Euclidean space using the **Euclidean distance**, so it will take into account amplitudes --i.e., sales quantities --, but will not be invariant to time shifts. There are other more complex metrics specifically designed for time series, like DTW (Dynamic Time Warping), that do take into account the series phase, but for our problem, I do not deem them necessary. As a first approach, we'll be using the simple **_k_-means algorithm adapted to time series**. As a further remark, since this configuration is very sensitive to outliers, we'll be removing the ones in _highest count_ to compute the clusters.\n\nThe different clusters are indeed interesting, as, for example, the algorithm managed to differentiate the special shops category in cluster 6 that we visualized in section 2; so, if we wouldn't have arranged the _category_ feature, we could have sort of infer its existence from here. Silhouette score is positive but not very high, and number of shops, i.e., cardinality, is not even amongst clusters, as expected, but they're still insightful. Nonetheless, we won't be using this feature in our model to avoid data leakage, since we're using the whole time range to compute the groups. This is just for analysis purposes.\n\nðŸ”¬ In a future version, after having performed some feature engineering, we may try to cluster our whole dataset (not just the shops behaviour) using a static photo in time (cross-section) of the each available month, with a more robust technique such as PAM (Partitioning Around Medoids), which is similar to _k_-means but uses medoids instead of centroids, and visualize them with t-SNE in a low-dimensional space.","metadata":{}},{"cell_type":"code","source":"# euclidean k-means\nprint(\"Euclidean time series k-means:\")\n\n# only train info\nopt_train = opt_data.query('date_block_num < 33')\n\n# removing highest outliers \nopt_data_clust = opt_train.query('highest_count == \"regular\"')\n\n# computing new sales by shop\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    opt_data_clust['item_cnt_month'] = opt_data_clust.item_cnt_month.astype('int16')\n    \nsales_by_shop_id = pivot_df(opt_data_clust, ['shop_id'], ['item_cnt_month'], 'date_block_num')\n\n# shifting one month so as to use previous sales data every month\nshop_shifted_sales = sales_by_shop_id.shift(1, axis=1).dropna(axis=1)\n\n# standardizing series (mean 0, std 1)\nX_train = shop_shifted_sales.reset_index().drop([''], axis=1)\nscaled = TimeSeriesScalerMeanVariance(mu=0., std=1.).fit_transform(X_train.values)\ndata_train = pd.DataFrame(np.squeeze(scaled), columns=X_train.columns, index=X_train.index)\ndf = data_train.values\n\n# computing methods for cluster selection\nsil_scores = {}\ninertias = [] \nfor n_clusters in range(2, 9):\n    km = TimeSeriesKMeans(n_clusters=n_clusters, verbose=False, random_state=0, metric=\"euclidean\")\n    y_pred = km.fit_predict(df)\n    sil_scores[n_clusters] = silhouette_score(df, y_pred, metric=\"euclidean\")\n    inertias.append(km.inertia_)\n\n# plotting methods\nfig, ax1 = plt.subplots()\nax1.set_xlabel('Number of clusters')\nax1.set_ylabel('Inertia')\nax1.plot(range(2,9), inertias, \n         color='b', label='Elbow method (inertia)')\nax1.legend(loc='upper left')\nax2 = ax1.twinx() \nax2.set_xlabel('Number of clusters')\nax2.set_ylabel('Silhouette score')\nax2.plot(range(2,9), list(sil_scores.values()), \n         color='r', label='Silhouette score')\nax2.legend(loc='upper right')\nfig.tight_layout()  # otherwise the right y-label is slightly clipped\n\n# getting number of clusters that maximizes silhouette score\nsil_scores.pop(2) # dropping option with \nn_clusters = max(sil_scores, key=sil_scores.get)\n\nplt.axvline(x=n_clusters, color='k', ls='--', alpha=.3);\nplt.text(0.67, 0.75,'Optimal $k$', transform=plt.gca().transAxes);\nplt.show();\n\n# performing definite clustering\n# random seed is very important for k-means, as a bad one may lead to slower convergence and bias\nkm = TimeSeriesKMeans(n_clusters=n_clusters, verbose=True, random_state=0, metric=\"euclidean\")\ny_pred = km.fit_predict(df)\n\n# plotting and computing tot ss\nplt.figure(figsize=(15,10))\nplt.title(\"Euclidean $k$-means\");\ntot_ss = {}\nfor yi in range(n_clusters):\n    ss = []\n    plt.subplot(3, 3, yi + 1)\n    for xx in df[y_pred == yi]:\n        ss.append((xx - km.cluster_centers_.squeeze()[yi])**2)\n        tot_ss[yi] = ss\n        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n    tot_ss[yi] = np.mean(sum(tot_ss[yi]))\n    plt.plot(km.cluster_centers_[yi].ravel(), \"c-\")\n    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1), transform=plt.gca().transAxes);\n    plt.xlabel('Month'); plt.ylabel('Standardized sales')","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:52:21.080134Z","iopub.execute_input":"2022-03-07T16:52:21.080409Z","iopub.status.idle":"2022-03-07T16:52:26.613613Z","shell.execute_reply.started":"2022-03-07T16:52:21.080372Z","shell.execute_reply":"2022-03-07T16:52:26.612921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding the clusters as a new variable (but remember that they won't be used in the model)\nshop_clusters = {}\nit = 0\nfor shop in shop_shifted_sales.index:\n    shop_clusters[shop] = y_pred[it] + 1\n    it += 1\n\n# mapping shops\n# NAs will be due to new shops in the months 33 and 34, i.e., shop 36 that we saw earlier in month 33\n# we'll simply fill them with 0\n# if we were dealing with cross-sectional data, we would assign them to their nearest centroid\n# but since we're dealing with growing time series data and euclidean distance, length wouldn't be the same and\n# computations couldn't be performed. Whole new clusters would have to be calculated\nopt_data['shop_group'] = opt_data.shop_id.map(shop_clusters).fillna(0).apply(int).astype('int8')","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:52:26.615005Z","iopub.execute_input":"2022-03-07T16:52:26.615273Z","iopub.status.idle":"2022-03-07T16:52:32.680624Z","shell.execute_reply.started":"2022-03-07T16:52:26.615239Z","shell.execute_reply":"2022-03-07T16:52:32.679869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We're also computing a mean of total sum of squared distances in each cluster versus the number of shops in each cluster, i.e., magnitude versus cardinality, as another means of quantifying cluster quality. A higher cluster cardinality tends to result in a higher cluster magnitude, which intuitively makes sense. When cardinality doesn't correlate with magnitude relative to the other clusters, it may indicate that the cluster is anomalous. Our clusters are well in line with this dependency.","metadata":{}},{"cell_type":"code","source":"cluster_quality = pd.DataFrame()\ncluster_quality['cardinality'] = opt_data.query('shop_group != 0').groupby(['shop_group'])['shop_id'].nunique()\ncluster_quality['magnitude'] = list(tot_ss.values())\n\nsns.regplot(data=cluster_quality, x='cardinality', y='magnitude');\nplt.xlabel('Cardinality'); plt.ylabel('Magnitude');\nplt.title('Cluster quality');","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:52:32.682063Z","iopub.execute_input":"2022-03-07T16:52:32.68232Z","iopub.status.idle":"2022-03-07T16:52:35.012465Z","shell.execute_reply.started":"2022-03-07T16:52:32.682286Z","shell.execute_reply":"2022-03-07T16:52:35.011739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we wanted to see how our OOT shops are distributed among the shape clusters, and from the following piece of code, we can see that they mostly belong to clusters 2 and 5, the ones exhibiting a predominant downwards trend.","metadata":{}},{"cell_type":"code","source":"lists_shops = opt_data.groupby('shop_group')['shop_id'].apply(set).to_dict()\ntest_shops_clusters = []\nfor shop in list(set(train.shop_id) - set(test.shop_id)): # iterate over OOT\n    for cluster in lists_shops.keys(): # iterate over clusters\n        if shop in lists_shops[cluster]:\n            test_shops_clusters.append(cluster)\nprint(Counter(np.sort(test_shops_clusters)))","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:52:35.0136Z","iopub.execute_input":"2022-03-07T16:52:35.014447Z","iopub.status.idle":"2022-03-07T16:52:36.975917Z","shell.execute_reply.started":"2022-03-07T16:52:35.014402Z","shell.execute_reply":"2022-03-07T16:52:36.97516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.2. Items RFM (Recency, Frequency and Monetary value)","metadata":{}},{"cell_type":"markdown","source":"As stated in section 2, here we will make an **analogy to the RFM customer segmentation model, but for items**. The original model is used to segment customers into high-value customers, medium-value customers or low-value customers, and similarly many others. We'll compute the RFM segments in an expanding time window, being recency the time span between the present in the window and the last time the item was sold; calculating frequency as the ratio between the number of months the item was sold and the total window span in months; and revenue acting as the monetary value. Segment 111 will be that of highest value, 444 the lowest one, and so on, like sticking together ordinal encodings of each variable involved.","metadata":{}},{"cell_type":"markdown","source":"First, let's clear our dataset of inactive shops and items.","metadata":{}},{"cell_type":"code","source":"with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    # transforming type before pivoting\n    opt_data['item_cnt_month'] = opt_data.item_cnt_month.astype('int16')\n\n# pivoting df for computing new sales by item id\nsales_by_item_id = pivot_df(opt_data, ['item_id'], ['item_cnt_month'], 'date_block_num')\n\n# shifting one month so as to use previous sales data every month\nitem_shifted_sales = sales_by_item_id.shift(1, axis=1).dropna(axis=1)\n\n# dropping inactive shops to reduce variance, as stated in section 1\n# assumption is that if a shop does not have any sales during the previous month before val, \n# it will be inactive unless it's special. This way we won't use information from test, i.e. the future,\n# by simply dropping the ones that are not included (OOTS). \n# We have to make sure, however, not to eliminate any shop nor item in test that falls in this assumption, \n# since we're deriving the new test set from this dataframe\nprev_length = len(opt_data.query('date_block_num < 33'))\nopt_data = opt_data[(opt_data.shop_id\n                    .isin(shop_shifted_sales[shop_shifted_sales.loc[:,[32]].sum(axis=1) > 0].index)) |\n                    (opt_data.shop_type_name == 'special') | \n                    (opt_data.shop_id.isin(test.shop_id.unique()) & (opt_data.date_block_num == 34))]\n\nshops_not_included = list(set(train.shop_id.unique()) - \n                          set(opt_data.query('date_block_num < 33').shop_id.unique()))\nprint('\\nNumber of shops not included in model: ', len(shops_not_included))\n\n# dropping outdated items for more than 2 years, as stated in section 1 (unless they're on test set)\n# assumption is that if an item of the type that we're dealing with in this dataset has no sales during \n# the last 2 years in the whole Russia and had had previous sales, it is therefore out of sales catalogue. \n# Ideally assumptions would be discussed with Business department\nopt_data = opt_data[(opt_data.item_id\n                    .isin(item_shifted_sales[item_shifted_sales.iloc[:,11:].sum(axis=1) > 0].index)) |\n                    ((opt_data.item_id.isin(test.item_id.unique())) & (opt_data.date_block_num == 34))]\n\nitems_not_included = list(set(train.item_id.unique()) - \n                          set(opt_data.query('date_block_num < 33').item_id.unique()))\nprint('Number of items not included in model: ', len(items_not_included))\n\nprint('Number of train samples decreased by {}%'.format(\n      round((prev_length - len(opt_data.query('date_block_num < 33')))*100 / prev_length)))","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:52:36.977112Z","iopub.execute_input":"2022-03-07T16:52:36.978025Z","iopub.status.idle":"2022-03-07T16:52:45.934829Z","shell.execute_reply.started":"2022-03-07T16:52:36.977984Z","shell.execute_reply":"2022-03-07T16:52:45.934067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we'll compute the RFM segments.","metadata":{}},{"cell_type":"code","source":"%%time\n\n# pivoting df for computing new sales by item id after dropping inactive ones\nsales_by_item_id = pivot_df(opt_data, ['item_id'], ['item_cnt_month'], 'date_block_num')\n\n# shifting one month so as to use previous sales data every month\nitem_shifted_sales = sales_by_item_id.shift(1, axis=1).dropna(axis=1)\nitem_shifted_sales = item_shifted_sales.applymap(lambda x: 1 if x > 0 else 0) # binary sales\n\n# functions for computing score or quartile belonging (ordinal encoding)\ndef RScore(x,p,d):\n    \"\"\"Computes recency score.\n    \n    Parameters\n    ----------\n    x: int or float\n        Value.\n    p: string\n        Dictionary key.\n    d: float\n        Quantile.\n    \"\"\"\n    if x <= d[p][0.25]:\n        return 1\n    elif x <= d[p][0.50]:\n        return 2\n    elif x <= d[p][0.75]: \n        return 3\n    else:\n        return 4\n\ndef FMScore(x,p,d):\n    \"\"\"Computes frequency and monetary scores.\n    \n    Parameters\n    ----------\n    x: int or float\n        Value.\n    p: string\n        Dictionary key.\n    d: float\n        Quantile.\n    \"\"\"\n    if x <= d[p][0.25]:\n        return 4\n    elif x <= d[p][0.50]:\n        return 3\n    elif x <= d[p][0.75]: \n        return 2\n    else:\n        return 1\n\n    \n# instanciating rfm_df\nrfm_df = pd.DataFrame([])\n\n# computing rfm segments in expanding date blocks\nfor date_block in range(1, len(item_shifted_sales.columns) + 1):\n\n    # expanding temp dfs\n    temp_shifted_sales = item_shifted_sales.loc[:,[col for col in item_shifted_sales.columns if col <= date_block]]\n    pre_temp = opt_data.query(f'date_block_num <= {date_block}')\n\n    # computing recency\n    df_recency = (temp_shifted_sales.reset_index().melt(id_vars='', var_name='date_block_num', \n                  value_name='active_last_month').rename(columns={'': 'item_id'}).query('active_last_month > 0')\n                  .groupby(['item_id'])['date_block_num'].max().reset_index())\n    df_recency['recency'] = (pre_temp.date_block_num.max() - df_recency.date_block_num) + 1\n    del pre_temp\n    \n    # computing frequency\n    temp_shifted_sales['frequency'] = temp_shifted_sales.sum(axis=1)/len(temp_shifted_sales.columns)\n\n    # adding columns to cross-section temp df\n    temp = opt_data.query(f'date_block_num == {date_block}')\n    temp = temp.merge(temp_shifted_sales.reset_index()[['', 'frequency']], \n                      left_on=['item_id'], right_on='', how='left')\n    temp = temp.merge(df_recency[['item_id', 'recency']].drop_duplicates(), on=['item_id'], how='left')\n    del temp_shifted_sales, temp[''] # '' is just the item_id when reseting index in shifted df\n\n    # computing quartile df \n    quantiles = temp[['item_id', 'recency','frequency', 'item_price']].quantile(q=[0.25,0.5,0.75])\n    quantiles = quantiles.to_dict()\n\n    # segmenting with quartile df\n    temp['r_quartile'] = temp['recency'].apply(RScore, args=('recency',quantiles,))\n    temp['f_quartile'] = temp['frequency'].apply(FMScore, args=('frequency',quantiles,))\n    temp['m_quartile'] = temp['item_price'].apply(FMScore, args=('item_price',quantiles,))\n\n    # computing total score\n    temp['rfm'] = (temp['r_quartile'].apply(str) + temp['f_quartile'].apply(str) + temp['m_quartile'].apply(str))\n\n    rfm_df = pd.concat([rfm_df, temp])\n    del temp\n\n# adding rfm computation to our dataset\n# first month (date block 0) will be empty, but we'll drop it after the next subsection\nopt_data = pd.concat([opt_data.query('date_block_num == 0'), rfm_df])\n\n# filling empty block with -1 to change data type\nopt_data.loc[:, ['recency', 'rfm', 'frequency',\n             'r_quartile', 'f_quartile', 'm_quartile']] = (opt_data.loc[:, ['recency', 'rfm', 'frequency',\n                                                                        'r_quartile', 'f_quartile', 'm_quartile']]\n                                                           .fillna(-1).applymap(int).astype('int16'))\n# saving memory\ndel quantiles, rfm_df, sales_by_item_id","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:52:45.936146Z","iopub.execute_input":"2022-03-07T16:52:45.936422Z","iopub.status.idle":"2022-03-07T16:55:08.736306Z","shell.execute_reply.started":"2022-03-07T16:52:45.936386Z","shell.execute_reply":"2022-03-07T16:55:08.735552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Samples of RFM segments and their mean:')\ndisplay(opt_data.groupby('rfm')[['recency', 'frequency', 'item_price']].mean().sample(3))","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:55:08.737505Z","iopub.execute_input":"2022-03-07T16:55:08.738082Z","iopub.status.idle":"2022-03-07T16:55:09.146594Z","shell.execute_reply.started":"2022-03-07T16:55:08.738034Z","shell.execute_reply":"2022-03-07T16:55:09.145912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.3. Mean-based, trends, and other features","metadata":{}},{"cell_type":"markdown","source":"There are countless possibilities, so for this version we'll compute some logical features that may be important based on what we've seen so far and just loop over for improving results. One could also compute ACF and PACF values to derive the autocorrelation behaviour of aggregated values of our target variable, or pair-wise correlations with the predictors and the dependent variable to get more insights and center the calculations. We'll be **clipping the target variable before computations so as to diminish bias due to extreme values**.","metadata":{}},{"cell_type":"code","source":"%%capture\n# optimizing df again\nopt_data = reduce_mem_usage(opt_data)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:55:09.147803Z","iopub.execute_input":"2022-03-07T16:55:09.148127Z","iopub.status.idle":"2022-03-07T16:55:49.332707Z","shell.execute_reply.started":"2022-03-07T16:55:09.148086Z","shell.execute_reply":"2022-03-07T16:55:49.331963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# general for computing any agg operation and generate a column\ndef generate_agg_features(df, grouping_cols, col, operation):\n    \"\"\"Generates new features with transform operations over a group.\n    \n    Parameters\n    ----------\n    df: pandas dataframe\n    grouping_cols: list of str\n    col: str\n        Column to part from for creating new feature.\n    operation: lambda function or str\n    \"\"\"\n    return df.groupby(grouping_cols)[col].transform(operation)\n\n# we need to use this function for lagging features, as months are not continuous within shop-item pairs\ndef generate_lag_features(df, lag, col):\n    \"\"\"Generates lagged features over date block num.\n    \n    Parameters\n    ----------\n    df: pandas dataframe\n    lag: int\n    col: str\n    \"\"\"\n    temp = df[['date_block_num','shop_id','item_id', col]]\n    shifted = temp.copy()\n    shifted.columns = ['date_block_num','shop_id','item_id', col +'_lag'+ str(lag)]\n    shifted['date_block_num'] += lag\n    df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    if 'cnt_month' in col:\n        df[col +'_lag'+ str(lag)] = df[col +'_lag'+ str(lag)].fillna(0).apply(int).astype('int16')\n    return df     \n\n# clipping target variable before executing operations\nopt_data['unclipped_item_cnt_month'] = opt_data['item_cnt_month']\nopt_data['item_cnt_month'] = opt_data['item_cnt_month'].clip(0,20)\n\n# generating lags for item count\nfor lag in [1,2,3,6,12]:\n    opt_data = generate_lag_features(opt_data, lag, 'item_cnt_month')\n\n## time\n# days in month to correct for monthly variation\nopt_data['days_in_month'] = opt_data[['year', 'month']].apply(lambda x: monthrange(x['year'], x['month'])[1], axis=1)\n\n# dummy to account for seasonality\nopt_data['is_in_season'] = opt_data.month.apply(lambda x: 1 if x == 12 else 0)\nopt_data['was_in_season'] = opt_data.month.apply(lambda x: 1 if x == 11 else 0)\n\n## price and revenue magnitude order\nopt_data['log_item_price'] = np.log10(opt_data.item_price)\nopt_data['log_revenue'] = opt_data.revenue.apply(lambda x: np.log10(x) if x != 0 else 0).fillna(0)\nopt_data['price_segment'] = opt_data.item_price.apply(lambda x: round(np.log10(x))).astype('int8')\n\n# lagging revenue\nopt_data = generate_lag_features(opt_data, 1, 'log_revenue')\ndel opt_data['log_revenue']\n    \n## unique objects within some groups\nopt_data['shops_in_city'] = generate_agg_features(opt_data, ['shop_city_id', 'date_block_num'],'shop_id', \n                                                  'nunique').fillna(0).astype('int16')\nopt_data['items_in_shop'] = generate_agg_features(opt_data, ['shop_id', 'date_block_num'], 'item_id', \n                                                  'nunique').fillna(0).astype('int16')\nopt_data['items_in_cat'] = generate_agg_features(opt_data, ['category_id', 'date_block_num'], 'item_id', \n                                                 'nunique').fillna(0).astype('int16')\n# averaged by superior hierarchy\nopt_data['shops_in_city'] = (opt_data['shops_in_city'] / \n                             opt_data.groupby('date_block_num')['shop_city_id'].transform('nunique')).fillna(0).astype('int16')\nopt_data['items_in_shop'] = (opt_data['items_in_shop'] / \n                             opt_data.groupby('date_block_num')['shop_id'].transform('nunique')).fillna(0).astype('int16')\nopt_data['items_in_cat'] = (opt_data['items_in_cat'] / \n                            opt_data.groupby('date_block_num')['category_id'].transform('nunique')).fillna(0).astype('int16')\n\n# lagging these features\nfor col in ['shops_in_city', 'items_in_cat', 'items_in_shop']:\n    opt_data = generate_lag_features(opt_data, 1, col)\n    opt_data[col + '_lag1'] = opt_data[col + '_lag1'].fillna(0).astype('int16')\n    del opt_data[col]\n\n## trend\n# ideally we would use rolling functions over a date index, but I've found them to be extremely slow for this project\nfor lag in [3,6,12]:\n    opt_data[f'mean_item_cnt_month_lags1_{lag}'] = (pd.DataFrame(opt_data.item_cnt_month_lag1,\n                                                                 opt_data[f'item_cnt_month_lag{lag}'])\n                                                    ).reset_index().mean(axis=1).fillna(0).astype('float16')","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:55:49.333878Z","iopub.execute_input":"2022-03-07T16:55:49.33413Z","iopub.status.idle":"2022-03-07T17:03:30.866708Z","shell.execute_reply.started":"2022-03-07T16:55:49.334099Z","shell.execute_reply":"2022-03-07T17:03:30.865167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# this function will help us on computing operations to avoid data leakage\n# also tried with an expanding .rolling() window over datetime date, but computation was too expensive\ndef compute_expanding(df, grouping_cols, operation, col, new_col, drop_zeros=False):\n    \"\"\"Function to compute operations by looping over an expanding dataframe.\n    \n    Parameters\n    ----------\n    df: pandas daframe\n    grouping_cols: list of str\n    operation: lambda function or str\n    col: str\n    new_col: str\n    drop_zeros: bool\n        Whether to drop rows where sales are zero.\n    \"\"\"\n    new_df = pd.DataFrame([])\n    for date_block in range(1, len(item_shifted_sales.columns) + 1):\n        query_temp = df.query(f'date_block_num < {date_block}')\n        cross = df.query(f'date_block_num == {date_block}')\n        if drop_zeros: \n            query_temp = query_temp.query('item_cnt_month > 0')\n        temp = query_temp.copy()\n        temp.loc[:, new_col] = temp.groupby(grouping_cols)[col].transform(operation)\n        temp.loc[:,'date_block_num'] = date_block\n        cross = cross.merge(temp[[new_col, 'date_block_num'] + grouping_cols].drop_duplicates(), \n                            on=['date_block_num'] + grouping_cols, how='left')\n        new_df = pd.concat([new_df, cross])  \n    new_df[new_col] = new_df[new_col].fillna(0).apply(int).astype('int16')\n    return new_df  \n\n## time\n# first month of sales\nopt_data = compute_expanding(opt_data, ['item_id'], 'min', 'date_block_num', 'item_release', True)\nopt_data = compute_expanding(opt_data, ['subcategory_id'], 'min', 'date_block_num', 'subcat_release', True)\nopt_data = compute_expanding(opt_data, ['shop_id'], 'min', 'date_block_num', 'shop_opening', True)\nopt_data['item_age'] = opt_data['date_block_num'] - opt_data['item_release']\n\n# last month of sales\nopt_data = compute_expanding(opt_data, ['item_id'], 'max', 'date_block_num', 'item_last_sale', True)\nopt_data = compute_expanding(opt_data, ['shop_id'], 'max', 'date_block_num', 'shop_last_sale', True)\n    \n# part of the following code would be better summarised by lambda functions inside transform, but \n# computation time increases exponentially. This way is so much more efficient\n## totals averaged by time span\nopt_data = compute_expanding(opt_data, ['shop_id', 'item_id'], 'sum', 'item_cnt_month', 'total_avg_shop_item_sales')\nopt_data = compute_expanding(opt_data, ['shop_id', 'category_id'], 'sum', 'item_cnt_month', 'total_avg_shop_cat_sales')\nopt_data = compute_expanding(opt_data, ['shop_id', 'subcategory_id'], 'sum', 'item_cnt_month', 'total_avg_shop_subcat_sales')\nopt_data['total_avg_shop_item_sales'] = opt_data['total_avg_shop_item_sales']/opt_data.date_block_num\nopt_data['total_avg_shop_cat_sales'] = opt_data['total_avg_shop_cat_sales']/opt_data.date_block_num\nopt_data['total_avg_shop_subcat_sales'] = opt_data['total_avg_shop_subcat_sales']/opt_data.date_block_num\n\n## price\n# price diff within shop-item life and amongst shops\nopt_data = compute_expanding(opt_data, ['shop_id', 'item_id'], 'max', 'item_price', 'max_price')\nopt_data = compute_expanding(opt_data, ['shop_id', 'item_id'], 'min', 'item_price', 'min_price')\nopt_data['price_diff'] = (opt_data.max_price - opt_data.min_price) / opt_data.max_price\ndel opt_data['min_price'], opt_data['max_price']","metadata":{"execution":{"iopub.status.busy":"2022-03-07T17:03:30.867964Z","iopub.execute_input":"2022-03-07T17:03:30.868242Z","iopub.status.idle":"2022-03-07T17:15:11.891625Z","shell.execute_reply.started":"2022-03-07T17:03:30.868205Z","shell.execute_reply":"2022-03-07T17:15:11.890803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, so we've added quite a few variables. Let's continue.","metadata":{}},{"cell_type":"markdown","source":"### 3.4. Quantitative variable transformations","metadata":{}},{"cell_type":"markdown","source":"Several parametric statistics and models -- chiefly linear models -- assume certain properties in the input data such as similar variable scales or Gaussian (normal) distributions. Sometimes these properties can be achieved by applying some simple transformations to the original data, such as standardization (for making comparable scales) and Box-Cox or Yeo-Johnson transformations (for matching dispersion and normalizing data). Notwithstanding, given the nature of our data and the problem posed we'll probably use tree-based methods for making predictions, which are robust to differences in dispersion and scale, so we won't be needing many operations in this section, since these algorithms are invariant to monotonic transformations of the data. If we consider using a stacking of various other algorithms, we'll transform the data on the corresponding modelling notebook. On the other hand, most algorithms will at least benefit from standardization, but one has to also consider if the original units are more suitable than the standardized ones, and such is our case at first.\n\nWe'll simply remove `item_price` and `revenue` and leave their log-transforms, as it is their order of magnitude what really interests us.","metadata":{}},{"cell_type":"code","source":"# normalization of item price by log-transform, example with qq-plot\nfig = plt.figure()\nax1 = fig.add_subplot(211)\nx = opt_data['item_price'].dropna()\nprob = stats.probplot(x, dist=stats.norm, plot=ax1)\nax1.set_xlabel('')\nax1.set_title('Probplot against normal distribution')\n\nax2 = fig.add_subplot(212)\nprob = stats.probplot(np.log10(x), dist=stats.norm, plot=ax2)\nax2.set_title('Probplot after log-transformation')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T17:15:11.893228Z","iopub.execute_input":"2022-03-07T17:15:11.893742Z","iopub.status.idle":"2022-03-07T17:15:43.981243Z","shell.execute_reply.started":"2022-03-07T17:15:11.893703Z","shell.execute_reply":"2022-03-07T17:15:43.980537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del opt_data['item_price'], opt_data['revenue']","metadata":{"execution":{"iopub.status.busy":"2022-03-07T17:15:43.985788Z","iopub.execute_input":"2022-03-07T17:15:43.98626Z","iopub.status.idle":"2022-03-07T17:15:43.992842Z","shell.execute_reply.started":"2022-03-07T17:15:43.986231Z","shell.execute_reply":"2022-03-07T17:15:43.992032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5. Encoding categorical variables","metadata":{}},{"cell_type":"markdown","source":"As said, we'll be using tree-based algorithms for our model. Some of them may handle categorical and text variables internally, such as CatBoost, but others, such as XGBoost do not. We'll prepare a numerical representation for every categorical feature to also have greater control of the outcome, and compare results and performance in the next notebook. Also, most of our categorical features possess high-cardinality, so we'll probably treat them as a numerical input when modelling.","metadata":{}},{"cell_type":"code","source":"opt_data[[col for col in opt_data.columns if str(opt_data[col].dtypes) == 'category']].head(3)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T17:15:43.994128Z","iopub.execute_input":"2022-03-07T17:15:43.994397Z","iopub.status.idle":"2022-03-07T17:15:44.64545Z","shell.execute_reply.started":"2022-03-07T17:15:43.994361Z","shell.execute_reply":"2022-03-07T17:15:44.644726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the `item_name` variable, we'll use a **bag of words** created with an expanding time window over date blocks, and apply quartiles, such as with the RFM segmentation, to every name based on the count of its tokens. We'll always assign the maximum quartile found in all the tokens within the item name. We'll also leave the provisional ordinal ID created in the Text Processing notebook.","metadata":{}},{"cell_type":"code","source":"%%time\ndef apply_bag_of_words(row, bow):\n    \"\"\"Function for returning the max frequency found in a sentence with a previously created BoW.\n    \n    Parameters\n    ----------\n    row: pandas row\n    bow: pandas df\n        Bag of words in dataframe form.\n    \"\"\"\n    list_count = []\n    for token in row.split():\n        if not token.isdigit():\n            try:\n                list_count.append(bow.loc[token,:]['word_quartile'])\n            except KeyError:\n                list_count.append(0)\n        else: \n            list_count.append(0)\n    return max(list_count)\n\ndef expanding_bow_quartile(df, new_col='item_name_quartile'):\n    \"\"\"Function for creating a new feature segments assigned from the quartiles of a distribution \n    of word counts from a BoW. Has a dependency on function apply_bag_of_words().\n    \n    Parameters\n    ----------\n    df: pandas dataframe\n    new_col: str\n    \"\"\"\n    new_df = pd.DataFrame([])\n    for date_block in range(2, len(item_shifted_sales.columns) + 1):\n        query_temp = df.query(f'date_block_num < {date_block}')\n        cross = df.query(f'date_block_num == {date_block}')\n        cross_copy = cross.copy()\n        \n        dict_names = Counter([token for x in np.array(query_temp.item_name).flatten() \n                              for token in x.split() if not token.isdigit()])\n        bag_of_words = pd.DataFrame.from_dict(dict_names, orient='index').rename(columns={0:'count'})\n        quantiles = bag_of_words[['count']].quantile(q=[0.25,0.5,0.75])\n        bag_of_words['word_quartile'] = bag_of_words['count'].apply(RScore, args=('count',quantiles,))\n        \n        cross_copy.loc[:, new_col] = cross_copy.item_name.apply(lambda x: apply_bag_of_words(x, bag_of_words))\n        new_df = pd.concat([new_df, cross_copy])\n    \n    del bag_of_words, query_temp, cross, cross_copy\n    \n    new_df[new_col] = new_df[new_col].fillna(0).apply(int).astype('int8')\n    return new_df \n\nen_data = expanding_bow_quartile(opt_data)\ndel opt_data\n\ndisplay(en_data[['item_name', 'item_name_quartile']].sample(5))\ndisplay(en_data.groupby('item_name_quartile').nunique()[['item_id']])","metadata":{"execution":{"iopub.status.busy":"2022-03-07T17:15:44.647801Z","iopub.execute_input":"2022-03-07T17:15:44.648077Z","iopub.status.idle":"2022-03-07T17:27:22.227211Z","shell.execute_reply.started":"2022-03-07T17:15:44.648041Z","shell.execute_reply":"2022-03-07T17:27:22.226432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the `highest_price` and `highest_count` variables, we'll use one-hot encoding since they have only two categories each. For the rest of categorical variables, we have deemed that a mean-encoding from the first lag of target will be best. As said, most of our variables have many categories, so using one-hot or binary encoding (which reduces the number of newly created variables but still increments them) would create a very sparse matrix and increment computation time for the tree-based algorithms. Also, another simple encoding such as frequency encoding would not add much information since our data for every month is a cartensian product of shops and items, and the non-normality of our target variable difficults the use of some other encoding methods. You can check this useful Python library for [more types of encoding](https://contrib.scikit-learn.org/category_encoders/). However, as always, having time data adds to the complexity of the task.","metadata":{}},{"cell_type":"code","source":"%%time\n# one hot\nen_data['highest_price'] = en_data.highest_price.apply(lambda x: 1 if x=='highest' else 0)\nen_data['highest_count'] = en_data.highest_count.apply(lambda x: 1 if x=='highest' else 0)\n\n# mean-encoding and lagging\nfor col in ['item_id', 'shop_id', 'category_id', 'subcategory_id', 'shop_city_id', 'shop_type_id', \n            'rfm', 'price_segment', 'neighbours', 'month']:\n    if any(string in col for string in ['category', 'rfm', 'item_id', 'category_id', 'subcategory_id']):\n        en_data['mean_' + col + '_sales'] = generate_agg_features(en_data, ['shop_id', col, 'date_block_num'], \n                                                                  'item_cnt_month', \n                                                                  'mean').fillna(0).astype('float16')\n        en_data['mean_' + col + '_all_sales'] = generate_agg_features(en_data, [col, 'date_block_num'], \n                                                                      'item_cnt_month', \n                                                                      'mean').fillna(0).astype('float16')\n        en_data = generate_lag_features(en_data, 1, 'mean_' + col + '_sales')\n        en_data = generate_lag_features(en_data, 2, 'mean_' + col + '_sales')\n        del en_data['mean_' + col + '_sales']\n    else:\n        en_data['mean_' + col + '_all_sales'] = generate_agg_features(en_data, [col, 'date_block_num'], \n                                                                      'item_cnt_month', \n                                                                      'mean').fillna(0).astype('float16')\n    en_data = generate_lag_features(en_data, 1, 'mean_' + col + '_all_sales')\n    en_data = generate_lag_features(en_data, 2, 'mean_' + col + '_all_sales')\n    del en_data['mean_' + col + '_all_sales']\n    \n\n# ordinal ID for neighbours \nen_data['neighbours_id'] = preprocessing.LabelEncoder().fit_transform(en_data.neighbours.values).astype('int16')\ndel en_data['neighbours']\n\n# leaving only months from date block 2 to remove first nas\nen_data = en_data.query('date_block_num > 1')\n\ndisplay(en_data.sample(5))","metadata":{"execution":{"iopub.status.busy":"2022-03-07T17:27:22.229884Z","iopub.execute_input":"2022-03-07T17:27:22.230084Z","iopub.status.idle":"2022-03-07T17:36:24.732067Z","shell.execute_reply.started":"2022-03-07T17:27:22.230059Z","shell.execute_reply":"2022-03-07T17:36:24.731385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.6. Dividing dataset","metadata":{}},{"cell_type":"markdown","source":"It's due time to divide our dataset into train, validation and test sets. Always remind that this is a practice that should be performed just before preprocessing, unless it's a case like ours, where we need to create time-based features. We'll use November 2015 as test, as set by the competition, and use October 2015 as validation.","metadata":{}},{"cell_type":"code","source":"# we'll fill the remaining nas with 0\nen_data[[col for col in en_data.columns if \n         str(en_data[col].dtype) != 'category']] = en_data[[col for col in en_data.columns if \n                                                            str(en_data[col].dtype) != 'category']].fillna(0)\n\ntrain = en_data.query('date_block_num < 33')\nval = en_data.query('date_block_num == 33')\ntest = en_data.query('date_block_num == 34')\n\nprint('Train, val and test percentages: {:.1f}%, {:.1f}%, {:.1f}%'.format(len(train)*100/len(en_data), \n                                                                          len(val)*100/len(en_data),\n                                                                          len(test)*100/len(en_data)))\nprint('Total number of variables: ', len(train.columns)) # though not all of them will go to the final model","metadata":{"execution":{"iopub.status.busy":"2022-03-07T17:36:24.733292Z","iopub.execute_input":"2022-03-07T17:36:24.733779Z","iopub.status.idle":"2022-03-07T17:37:26.296013Z","shell.execute_reply.started":"2022-03-07T17:36:24.733741Z","shell.execute_reply":"2022-03-07T17:37:26.295222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Visualization summary","metadata":{}},{"cell_type":"markdown","source":"As the name of this section suggests, here we've developed a small interactive app for visualizing all features as a final exploration of our engineered train dataset. It's filtered by city for performance purposes. Enjoy! ðŸ”Ž","metadata":{}},{"cell_type":"code","source":"vis_data = en_data.copy()\ndel en_data\n\nvis_data.drop(['item_name', 'date'], axis=1, inplace=True) # dropping for vis purposes due to extremely high cardinality\nvis_data[['rfm', 'r_quartile', 'f_quartile', 'm_quartile',\n          'shop_group', 'price_segment', 'highest_price', 'highest_count',\n          'was_in_season', 'is_in_season', 'year', 'month', 'days_in_month']] = vis_data[\n        ['rfm', 'r_quartile', 'f_quartile', 'm_quartile', \n         'shop_group', 'price_segment', 'highest_price', 'highest_count',\n         'was_in_season', 'is_in_season', 'year', 'month', 'days_in_month']].astype('category')\n\n# vis buttons\npredictors = widgets.Dropdown(options=sorted([col for col in vis_data.columns]),\n                              value='log_item_price', description='Predictor:', disabled=False)\ncities = widgets.Dropdown(options=sorted(list(vis_data.shop_city_name.unique())),\n                          value='moscow', description='City:', disabled=False)\ndate_blocks = widgets.IntRangeSlider(value=[27, 32], min=0, max=34, step=1, description='Date block:')\nclipped = widgets.RadioButtons(options=['item_cnt_month', 'unclipped_item_cnt_month'], value='item_cnt_month',\n                               description='Target:', disabled=False)\n\ndef plot_variables(predictors, cities, date_blocks, clipped):\n    \"\"\"Plots various variables against target with interactive widgets.\n    \n    Parameters\n    ----------\n    predictors: ipywidget\n        Independent variables.\n    cities: ipywidget\n    date_blocks: ipywidget\n    clipped: ipywidget\n        Enables the choice for the target variable to be clipped or not.\n    \"\"\"\n    if str(vis_data[predictors].dtype) == 'category':\n        plt.figure(figsize=(15,4));\n        ax = sns.boxplot(data=vis_data[(vis_data.shop_city_name == cities) & \n                                       (vis_data.date_block_num.isin([*range(date_blocks[0], date_blocks[1])]))], \n                                        x=predictors, y=clipped);\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\")\n        plt.title(f'Sales vs {predictors}');\n        plt.tight_layout();\n        plt.show()\n    else:\n        plt.figure(figsize=(15,4));\n        sns.scatterplot(data=vis_data[(vis_data.shop_city_name == cities) & \n                                       (vis_data.date_block_num.isin([*range(date_blocks[0], date_blocks[1])]))], \n                        x=predictors, y=clipped, hue='category_id', legend=None); \n        #we don't care about the values of category id here, so we remove the legend accounting also for speed purposes\n        plt.title(f'Sales vs {predictors}');\n    \nv1 = widgets.VBox([date_blocks, cities])  \nv2 = widgets.VBox([predictors, clipped])\nui = widgets.HBox([v1, v2])\nout = widgets.interactive_output(plot_variables, {'predictors':predictors, 'cities':cities, \n                                                  'date_blocks':date_blocks, 'clipped':clipped})\ndisplay(ui, out)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T17:37:26.297195Z","iopub.execute_input":"2022-03-07T17:37:26.297436Z","iopub.status.idle":"2022-03-07T17:37:33.685677Z","shell.execute_reply.started":"2022-03-07T17:37:26.297399Z","shell.execute_reply":"2022-03-07T17:37:33.684696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving files","metadata":{}},{"cell_type":"markdown","source":"Awesome! âœ¨ Let's save the results.","metadata":{}},{"cell_type":"code","source":"#train.to_csv('train.csv', index=False)\n#val.to_csv('val.csv', index=False)\n#test.to_csv('test.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T17:37:33.68719Z","iopub.execute_input":"2022-03-07T17:37:33.687483Z","iopub.status.idle":"2022-03-07T17:37:44.846843Z","shell.execute_reply.started":"2022-03-07T17:37:33.687446Z","shell.execute_reply":"2022-03-07T17:37:44.846077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## ðŸ’¡ **Stay tuned for the next part: [XGBoost/Model/Performance] Predict Future Sales**\n\nAlso, if you have any question or comment to add, please, feel welcome to do so!","metadata":{}}]}