{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Predict Future Sales**\n## *Translation/Text Processing*","metadata":{}},{"cell_type":"markdown","source":"This challenge is marked as final project for the [\"How to win a data science competition\"](#https://www.coursera.org/learn/competitive-data-science/home/welcome) Coursera course.\n\nIn [this competition](#https://www.kaggle.com/c/competitive-data-science-predict-future-sales) we will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms, [1C Company](#http://1c.ru/eng/title.htm). They are asking to **predict total sales for every product and store in the next month**. They provide various files with supplemental information, but texts are in Russian.\n\nNames may contain a lot of information about items and shops, and they will surely provide major insight towards creating a model for predicting sales. In order to get the maximum value, in this notebook we will **translate Russian names for items, categories and shops**; we'll remove punctuation and stopwords, find named entities, create new columns with information of interest and generate new files.","metadata":{}},{"cell_type":"markdown","source":"### Tasks covered\n- [x] Process Russian texts and create new variables\n- [ ] Perform some EDA and data mining for deeper understanding, and prepare data\n- [ ] Create model and evaluate results\n\n### Content\n* [Libraries](#Libraries).\n* [Data loading](#Data-loading).\n* [Translation](#Translation).\n* [Preprocessing](#Preprocessing).\n    + [Categories](#Categories).\n    + [Shops](#Shops).\n    + [Items](#Items).\n* [Saving files](#Saving-files).","metadata":{}},{"cell_type":"markdown","source":"## Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom textblob import TextBlob\nimport re\nimport string\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\nimport spacy\n#load the spacy english core \nspacy_nlp = spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:40:44.447757Z","iopub.status.busy":"2022-01-07T22:40:44.447118Z","iopub.status.idle":"2022-01-07T22:40:56.166457Z","shell.execute_reply":"2022-01-07T22:40:56.165408Z","shell.execute_reply.started":"2022-01-07T22:40:44.447591Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data loading","metadata":{}},{"cell_type":"code","source":"folder = 'input/competitive-data-science-predict-future-sales'\npath = f'../{folder}'\nfiles = os.listdir(path)\nprint(files)","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:40:56.168927Z","iopub.status.busy":"2022-01-07T22:40:56.168569Z","iopub.status.idle":"2022-01-07T22:40:56.175702Z","shell.execute_reply":"2022-01-07T22:40:56.174614Z","shell.execute_reply.started":"2022-01-07T22:40:56.168879Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items = pd.read_csv(path + '/' + files[0]) # beware of using your own indexes if you edit this notebook\ncategories = pd.read_csv(path + '/' + files[2])\nshops = pd.read_csv(path + '/' + files[-2])","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:40:56.177888Z","iopub.status.busy":"2022-01-07T22:40:56.177515Z","iopub.status.idle":"2022-01-07T22:40:56.289374Z","shell.execute_reply":"2022-01-07T22:40:56.288249Z","shell.execute_reply.started":"2022-01-07T22:40:56.177842Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Translation","metadata":{}},{"cell_type":"markdown","source":"We will be using `TextBlob` for this task, which is a straightforward Text Processing library that I've already used in the past and works really well.","metadata":{}},{"cell_type":"code","source":"def translateText(text, from_lang, to_lang):\n    \"\"\"\n    Function for translating text.\n    It first translates and removes non-ascii characters. \n    If the first try is not succesful, it removes punctuation before translating, \n    as it may cause bad request http errors (probably due to an unresolved bug). If this is again\n    not successful, it returns NA.\n    \n    Parameters\n    ----------\n    text: str\n        Text for translating.\n    from_lang: str\n        Original language.\n    to_lang: str\n        Desired language.\n    \"\"\"\n    \n    try:\n        # Translating.\n        text = str(TextBlob(text).translate(from_lang=from_lang, to=to_lang))\n        # Removing non-ascii characters.\n        text = re.sub('[^\\x00-\\x7F]+', '', text) \n        \n    except Exception: # some punctuation symbols currently cause a bad request error, we'll remove them and try again\n        text = text.translate(str.maketrans('', '', string.punctuation))\n        \n        try:\n            # Translating.\n            text = str(TextBlob(text).translate(from_lang=from_lang, to=to_lang))\n            # Removing non-ascii characters.\n            text = re.sub('[^\\x00-\\x7F]+', '', text)\n            \n        except Exception: \n            text = np.nan # we'll return NA in case of any other error so we can detect it later\n    \n    return text","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:40:56.291504Z","iopub.status.busy":"2022-01-07T22:40:56.29127Z","iopub.status.idle":"2022-01-07T22:40:56.301288Z","shell.execute_reply":"2022-01-07T22:40:56.30007Z","shell.execute_reply.started":"2022-01-07T22:40:56.291474Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# applying translation for shops and categories\ncategories['category_name_en'] = categories.item_category_name.apply(lambda x: translateText(x, 'ru', 'en'))\nshops['shop_name_en'] = shops.shop_name.apply(lambda x: translateText(x, 'ru', 'en'))","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:40:56.303179Z","iopub.status.busy":"2022-01-07T22:40:56.302707Z","iopub.status.idle":"2022-01-07T22:41:28.106095Z","shell.execute_reply":"2022-01-07T22:41:28.10496Z","shell.execute_reply.started":"2022-01-07T22:40:56.303132Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"info alert-block alert-info\">\n    ðŸ’» I have created this notebook in my local machine and already executed it. For speeding up the running of this notebook in Kaggle, I will be using my locally translated files.\n</div>","metadata":{}},{"cell_type":"code","source":"# pretranslated files in local\nshops = pd.read_csv('../input/new-shops/new_shops.csv')\ncategories = pd.read_csv('../input/new-categories/new_categories.csv')","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:41:28.107575Z","iopub.status.busy":"2022-01-07T22:41:28.107332Z","iopub.status.idle":"2022-01-07T22:41:28.139028Z","shell.execute_reply":"2022-01-07T22:41:28.137886Z","shell.execute_reply.started":"2022-01-07T22:41:28.107543Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Translation for shops and categories is correct; there are no missing values. Great!","metadata":{}},{"cell_type":"code","source":"display(shops.head(3))\nprint(shops.shape)\nprint('\\nUntranslated values: \\n', shops.shop_name_en.isna().sum())\ndisplay(categories.head(3))\nprint(categories.shape)\nprint('\\nUntranslated values: \\n', categories.category_name_en.isna().sum())","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:41:28.141313Z","iopub.status.busy":"2022-01-07T22:41:28.140278Z","iopub.status.idle":"2022-01-07T22:41:28.171037Z","shell.execute_reply":"2022-01-07T22:41:28.1697Z","shell.execute_reply.started":"2022-01-07T22:41:28.141263Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">\n    <b>WARNING:</b> Translation for item names requires ~2 hours to complete running.\n    <br><em>Executed with MSI PS42 Modern 8RC model.</em>\n</div>","metadata":{}},{"cell_type":"code","source":"%%time\n# translating items\n# Uncomment the following line for a try, but beware of executing this cell with enough time to let it run, as it is a highly cpu and memory consuming task\n\n#items['item_name_en'] = items.item_name.apply(lambda x: translateText(x, 'ru', 'en'))","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:41:28.172741Z","iopub.status.busy":"2022-01-07T22:41:28.172376Z","iopub.status.idle":"2022-01-07T22:41:28.179439Z","shell.execute_reply":"2022-01-07T22:41:28.178385Z","shell.execute_reply.started":"2022-01-07T22:41:28.172708Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 321 unstranslated items, which is frankly well given the total number of observations, but let's dig deeper into them to see why they weren't translated.","metadata":{}},{"cell_type":"code","source":"# pretranslated files in local\nitems = pd.read_csv('../input/new-items/new_items.csv').drop(['Unnamed: 0'], axis=1)","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:41:28.181316Z","iopub.status.busy":"2022-01-07T22:41:28.180851Z","iopub.status.idle":"2022-01-07T22:41:28.31282Z","shell.execute_reply":"2022-01-07T22:41:28.311642Z","shell.execute_reply.started":"2022-01-07T22:41:28.181258Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(items.head())\nprint(items.shape)\nprint('\\nUntranslated values: \\n', items.item_name_en.isna().sum())","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:41:28.317515Z","iopub.status.busy":"2022-01-07T22:41:28.317229Z","iopub.status.idle":"2022-01-07T22:41:28.336129Z","shell.execute_reply":"2022-01-07T22:41:28.335112Z","shell.execute_reply.started":"2022-01-07T22:41:28.317484Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detectLanguage(text):\n    \"\"\"\n    Detects language in a text.\n    \n    Parameters\n    ----------\n    text: str\n    \"\"\"\n    try:\n        text = TextBlob(text).detect_language()\n    except Exception:\n        pass\n    \n    return text","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:41:28.338492Z","iopub.status.busy":"2022-01-07T22:41:28.337795Z","iopub.status.idle":"2022-01-07T22:41:28.344745Z","shell.execute_reply":"2022-01-07T22:41:28.343607Z","shell.execute_reply.started":"2022-01-07T22:41:28.338445Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items['language'] = items[items.item_name_en.isna()].item_name.apply(lambda x: detectLanguage(x))","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:41:28.347185Z","iopub.status.busy":"2022-01-07T22:41:28.346591Z","iopub.status.idle":"2022-01-07T22:43:13.86772Z","shell.execute_reply":"2022-01-07T22:43:13.866751Z","shell.execute_reply.started":"2022-01-07T22:41:28.347137Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the table below we can see that all **untranslated items are due to them being already in English** or with a **numeric name**. This is also fantastic news! We'll use their original name. (Due to the small length of some of the strings, we can also see that the algorithm does what it can at detecting the language. It's important to note that these **language detection algorithms work best with bigger strings of text**, as they also tend to use stopwords as a primary source of knowledge.)","metadata":{}},{"cell_type":"code","source":"display(items[(items.item_name_en.isna()) & (items.language != 'en')][['item_name', 'language']])\n\nitems['item_name_en'] = items.item_name_en.fillna(items.item_name)\ndel items['language']","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:43:13.869469Z","iopub.status.busy":"2022-01-07T22:43:13.869206Z","iopub.status.idle":"2022-01-07T22:43:13.899946Z","shell.execute_reply":"2022-01-07T22:43:13.898878Z","shell.execute_reply.started":"2022-01-07T22:43:13.869438Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have now translated all our datasets! Let's get rid of the names in Russian.","metadata":{}},{"cell_type":"code","source":"del shops['shop_name'], categories['item_category_name'], items['item_name']","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:43:13.901852Z","iopub.status.busy":"2022-01-07T22:43:13.901438Z","iopub.status.idle":"2022-01-07T22:43:13.910292Z","shell.execute_reply":"2022-01-07T22:43:13.909436Z","shell.execute_reply.started":"2022-01-07T22:43:13.9018Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Categories","metadata":{}},{"cell_type":"markdown","source":"From the category values below, we can infer that every category name is comprised of a general category plus a subcategory, or just a single category. Let's extract them to create 2 new variables.","metadata":{}},{"cell_type":"code","source":"# wholesome view of categories\npd.DataFrame(categories.category_name_en.values.reshape(-1, 12))","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:43:13.912171Z","iopub.status.busy":"2022-01-07T22:43:13.911791Z","iopub.status.idle":"2022-01-07T22:43:13.937009Z","shell.execute_reply":"2022-01-07T22:43:13.936129Z","shell.execute_reply.started":"2022-01-07T22:43:13.91214Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories['category_name'] = categories.category_name_en.apply(lambda x: x.split(' - ')[0])\ncategories['subcategory_name'] = categories.category_name_en.apply(lambda x: x.split(' - ')[-1])\n\ncategories.head()","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:43:13.938642Z","iopub.status.busy":"2022-01-07T22:43:13.93842Z","iopub.status.idle":"2022-01-07T22:43:13.954566Z","shell.execute_reply":"2022-01-07T22:43:13.95372Z","shell.execute_reply.started":"2022-01-07T22:43:13.938613Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll also normalise names by putting them in lowercase format, as they differ in some categories; we'll leave only alphanumeric characters and spaces; and we'll also manually correct two categories. Lastly, we'll create temporal variable IDs for these new groups with Label Encoding.","metadata":{}},{"cell_type":"code","source":"def onlyAlphaNumeric(text):\n    \"\"\"\n    Puts to lowercase format and removes non-alphanumeric characters except spaces.\n    \n    Parameters\n    ----------\n    text: str\n    \"\"\"\n    text = text.lower()\n    text = re.sub(r'</i>|<b>|<i>|</b>', '', text) # also deleting some html notation\n    text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n    \n    return text","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:43:13.956362Z","iopub.status.busy":"2022-01-07T22:43:13.955848Z","iopub.status.idle":"2022-01-07T22:43:13.97158Z","shell.execute_reply":"2022-01-07T22:43:13.970718Z","shell.execute_reply.started":"2022-01-07T22:43:13.956313Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_columns = [col for col in categories.columns if 'name' in col]\ncategories[cat_columns] = categories[cat_columns].applymap(lambda x: onlyAlphaNumeric(x))\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    categories[32:35]['category_name'] = 'payment cards'\n    categories.iloc[32]['subcategory_name'] = 'cinema music games'\n    categories.iloc[34]['subcategory_name'] = 'number'\n    \ncategories['category_id'] = LabelEncoder().fit_transform(categories.category_name.values)\ncategories['subcategory_id'] = LabelEncoder().fit_transform(categories.subcategory_name.values)\n\ndisplay(categories.head())\nprint('Unique values: \\n', categories.nunique())","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:43:13.973536Z","iopub.status.busy":"2022-01-07T22:43:13.973106Z","iopub.status.idle":"2022-01-07T22:43:14.002646Z","shell.execute_reply":"2022-01-07T22:43:14.001755Z","shell.execute_reply.started":"2022-01-07T22:43:13.973503Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del categories['category_name_en']","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:43:14.00465Z","iopub.status.busy":"2022-01-07T22:43:14.004383Z","iopub.status.idle":"2022-01-07T22:43:14.010104Z","shell.execute_reply":"2022-01-07T22:43:14.008971Z","shell.execute_reply.started":"2022-01-07T22:43:14.004618Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Shops","metadata":{}},{"cell_type":"markdown","source":"For the shops dataset, we'll perform the same text processing operations. In this case, though, **we're creating city** (as we can see that it tends to be the first token of the name) and **shop type variables** (as it is sometimes specified within). With a **little bit of research**, one may **comprehend the different types of shopping centers included**. For example, a TC (from the Russian Ð¢Ð¦) is a common shopping center, whereas a SEC (or TRK, depending on the translation, from the Russian Ð¢Ð Ð¦ or Ð¢Ð Ðš) is a shopping and entertainment complex, including cinemas and other leisure activities.","metadata":{}},{"cell_type":"code","source":"shops_original_en = shops.copy()\n\nshops.head()","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:43:14.01203Z","iopub.status.busy":"2022-01-07T22:43:14.011693Z","iopub.status.idle":"2022-01-07T22:43:14.028762Z","shell.execute_reply":"2022-01-07T22:43:14.027698Z","shell.execute_reply.started":"2022-01-07T22:43:14.011984Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# only alphanumeric strings\n# we could also eliminate 'quot' words, as they come from quotations before the translation,\n# but for our matter there is no necessity in doing so\ncat_columns = [col for col in shops.columns if 'name' in col]\nshops[cat_columns] = shops[cat_columns].applymap(lambda x: onlyAlphaNumeric(x))\n\n# also eliminating spelled quotations that may be left\nshops[cat_columns] = shops[cat_columns].applymap(lambda x: re.sub('quot', '', x))\n\n# cities\ndef takeCityName(text):\n    # we iterate over the name to find the city with a restriction of word-length\n    for i in range(0, len(text.split())):\n        if ((text.split()[i] != '') and (len(text.split()[i]) > 2)):\n            city = text.split()[i]\n            break\n                   \n    return city\n        \nshops['shop_city_name'] = shops.shop_name_en.apply(lambda x: takeCityName(x))\nshops['shop_city_name'] = np.where(shops['shop_city_name'] == 'digital', 'online', shops['shop_city_name'])\n\n# shop types\n#\nshops['shop_type_name'] = shops.shop_name_en.apply(lambda x: \n                                                   'megacenter' if re.findall(r'trk|xl|sec', x)\n                                                   else 'online' if 'online' in x\n                                                   else 'special' if re.findall(r'outbound|sale', x)\n                                                   else 'center' if re.findall(r'tc |shopping center', x)\n                                                   else 'unspecified')\n\n# ids\nshops['shop_city_id'] = LabelEncoder().fit_transform(shops.shop_city_name.values)\nshops['shop_type_id'] = LabelEncoder().fit_transform(shops.shop_type_name.values)   \n\ndisplay(shops.head())\nprint('Unique values: \\n', shops.nunique())","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:43:14.032466Z","iopub.status.busy":"2022-01-07T22:43:14.032088Z","iopub.status.idle":"2022-01-07T22:43:14.064113Z","shell.execute_reply":"2022-01-07T22:43:14.062651Z","shell.execute_reply.started":"2022-01-07T22:43:14.032419Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del shops['shop_name_en']","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:43:14.066795Z","iopub.status.busy":"2022-01-07T22:43:14.065949Z","iopub.status.idle":"2022-01-07T22:43:14.07393Z","shell.execute_reply":"2022-01-07T22:43:14.072964Z","shell.execute_reply.started":"2022-01-07T22:43:14.066745Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alternatively, for detecting cities, one could make use of **Named Entity Recognition** algorithms like the ones incorporated in `spaCy` library, but, as it's shown below, they're mostly unreliable for our task, even though it's one of the most powerful NLP libraries, with industrial strength, excelling at large-scale information processing.","metadata":{}},{"cell_type":"code","source":"with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    for shop in shops_original_en.shop_name_en[15:23]:\n        doc = spacy_nlp(shop)\n        #the whole text with fancy entities location and type of entity\n        spacy.displacy.render(doc, style='ent', jupyter=True)","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:43:14.076238Z","iopub.status.busy":"2022-01-07T22:43:14.07537Z","iopub.status.idle":"2022-01-07T22:43:14.175685Z","shell.execute_reply":"2022-01-07T22:43:14.174688Z","shell.execute_reply.started":"2022-01-07T22:43:14.07619Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Items","metadata":{}},{"cell_type":"markdown","source":"Now, let's process items dataframe. In the previous cases, we've mostly had only nouns within the name, but for item names, we may also find adverbs, determinants and other so called **stopwords**, i.e., the most common words in a language. We'll be removing them in order to solely keep **nouns**-- which, in essence, **contain the whole meaning** of the name--, and other useful parts-of-speech or content words. We won't create a new ID for this new column, as even though we manage to reduce some cardinality with this process, it is still utterly high and there is no use for a meaningless ID. Also, we'll add 3 new columns containing a count of total words in the name, number of nouns and number of stopwords.\n\nWe'll utilize the prepared name for yielding more insights during the EDA phase. I'll soon upload a notebook for it.","metadata":{}},{"cell_type":"code","source":"# printing stopwords examples with nltk\nstopwords_en = nltk.corpus.stopwords.words(\"english\")\nprint('10 examples of stopwords: ', stopwords_en[:10])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items.tail()","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:43:14.177005Z","iopub.status.busy":"2022-01-07T22:43:14.176782Z","iopub.status.idle":"2022-01-07T22:43:14.187944Z","shell.execute_reply":"2022-01-07T22:43:14.186776Z","shell.execute_reply.started":"2022-01-07T22:43:14.176974Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def addTextMetrics(df, column):\n    \"\"\"Function for adding text metrics like length of name, number of nouns, etc. to a Dataframe.\n    \n    Parameters\n    ----------\n    df: pandas dataframe\n    column: str\n        Column of df for deriving text metrics from.\n    \n    Notes\n    -----\n    It uses a preloaded spacy language core, spacy_nlp.\n    \"\"\"      \n    \n    # adding basic name metrics\n    df[f'{column}_number_words'] = df[column].apply(lambda x: len(spacy_nlp(x)))\n    df[f'{column}_number_stopwords'] = df[column].apply(lambda x: len([token for token in spacy_nlp(x) \n                                                                       if token.is_stop]))\n    df[f'{column}_number_nouns'] = df[column].apply(lambda x: len([token for token in spacy_nlp(x) \n                                                                   if token.pos_ == 'NOUN']))\n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeStopWords(text, content=False, outliers=False):\n    \"\"\" Removing stopwords and more non-useful words.\n    \n    Parameters\n    ----------\n    text: str\n    content: bool\n        If True, leaves only content words like nouns, adjectives, verbs and adverbs (also numbers).\n    outliers: bool\n         If True, removes what I call outliers in word length, for percentiles 5 and 95.\n         \n    Notes\n    -----\n    It uses a preloaded spacy language core, spacy_nlp.\n    \"\"\"      \n        \n    # Transforming to spacy.doc.Doc object for easier processing.\n    doc = spacy_nlp(text)\n    \n    if not content:\n        # removing stop_words with Spacy\n        tokens = [token.text for token in doc if not token.is_stop]\n\n        # if we've stripped all tokens by removing stopwords or we're left with just one word, we take the original name\n        if (len(tokens) < 2) or (not tokens):\n            tokens = [token.text for token in doc]\n    else:\n        # leaving only content words\n        tokens = [token.text for token in doc if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'VERB', 'NUM']]\n        if (len(tokens) < 2) or (not tokens): # if this is met, we take any remaining alpha characters\n            tokens = [token.text for token in doc if token.is_alpha]\n    \n    if outliers:\n        lengths = [len(token) for token in tokens]\n        words = []\n        for word in tokens:          \n            # Removing outliers in word-length:\n            if ((len(word.text) >= numpy.percentile(lengths,5)) and\n               (len(word.text) <= numpy.percentile(lengths,95))):\n                   words.append(word.text)  \n    \n        text = ' '.join(words) # return in string format\n    else:\n        text = ' '.join(tokens)\n \n    return text","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:43:14.190123Z","iopub.status.busy":"2022-01-07T22:43:14.189761Z","iopub.status.idle":"2022-01-07T22:43:14.204708Z","shell.execute_reply":"2022-01-07T22:43:14.203844Z","shell.execute_reply.started":"2022-01-07T22:43:14.19008Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">\n    <b>WARNING:</b> This next cell requires ~10 minutes to complete running.\n    <br><em>Executed with MSI PS42 Modern 8RC model.</em>\n</div>","metadata":{}},{"cell_type":"code","source":"%%time\n# only alphanumeric strings\ncat_columns = [col for col in items.columns if 'name' in col]\nitems[cat_columns] = items[cat_columns].applymap(lambda x: onlyAlphaNumeric(x))\n\n# also eliminating spelled quotations that may be left\nitems[cat_columns] = items[cat_columns].applymap(lambda x: re.sub('quot', '', x))\n\n# adding 3 new columns\nitems = addTextMetrics(items, 'item_name_en')\n\n# only removing stopwords and other non content words. \n# Outliers in word length are not necessary to be removed for this task.\nitems['item_name'] = items.item_name_en.apply(lambda x: removeStopWords(x, content=True))\n\n# we'll fill the remaining blank names with their original English name\nitems['item_name'] = np.where(items.item_name == '', items.item_name_en, items.item_name)","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:43:14.206616Z","iopub.status.busy":"2022-01-07T22:43:14.205909Z","iopub.status.idle":"2022-01-07T22:46:05.232853Z","shell.execute_reply":"2022-01-07T22:46:05.232138Z","shell.execute_reply.started":"2022-01-07T22:43:14.206565Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(items.tail())\nprint('Unique values: \\n', items.nunique())","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:46:05.23442Z","iopub.status.busy":"2022-01-07T22:46:05.234044Z","iopub.status.idle":"2022-01-07T22:46:05.287865Z","shell.execute_reply":"2022-01-07T22:46:05.286817Z","shell.execute_reply.started":"2022-01-07T22:46:05.234387Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del items['item_name_en']","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:46:05.291884Z","iopub.status.busy":"2022-01-07T22:46:05.291605Z","iopub.status.idle":"2022-01-07T22:46:05.296973Z","shell.execute_reply":"2022-01-07T22:46:05.296025Z","shell.execute_reply.started":"2022-01-07T22:46:05.291854Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving files","metadata":{}},{"cell_type":"markdown","source":"Awesome! âœ¨ Let's save the results.","metadata":{}},{"cell_type":"code","source":"#items.to_csv(path + '/items_english.csv', index=False) # default comma-separated\n#categories.to_csv(path + '/categories_english.csv', index=False)\n#shops.to_csv(path + '/shops_english.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2022-01-07T22:46:05.298736Z","iopub.status.busy":"2022-01-07T22:46:05.298465Z","iopub.status.idle":"2022-01-07T22:46:05.307029Z","shell.execute_reply":"2022-01-07T22:46:05.306266Z","shell.execute_reply.started":"2022-01-07T22:46:05.298701Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## ðŸ’¡ **Stay tuned for the next part: [In-depth EDA] Predict Future Sales**\n\nAlso, if you have any question or comment to add, please, feel welcome to do so!","metadata":{}}]}