{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style='background-color: #6c3d75; align:center;color:white;border: 4px solid ; text-align:center;margin: 5px;padding:5px;font-size:20px'>Sales Price Prediction Challenge - Step 2 - Analyze data</div>\n<div style=\"text-align: center;\">\n<img height=\"150\" width=\"800\"  src=\"https://sp-ao.shortpixel.ai/client/q_glossy,ret_img,w_1000/https://www.leadsquared.com/wp-content/uploads/2019/02/banner-4.png\" alt=\"C1 Technologies\"></div>","metadata":{}},{"cell_type":"markdown","source":"<h4 style=\"color:red;text-align:center\"> * * * Please upvote if you like this kernel. * * *</h4>  \n  <h4>This is step 2, Please visit <a href='https://www.kaggle.com/zenstat/notebook-for-beginners-step-1-load-the-data' > step 1 kernal code </a>to understand the basics of loading data. </h4><h4>In this kernal, we will work on second step of the anaysis which is to analyze and understand all variables. We will learn <br><br>\n    * merge and join <br><br>\n    * data quality checks - missing data, outliers <br><br>\n    * data transformation - MVI - missing value imputations<br><br>\n    We will load the data same as in <a href='https://www.kaggle.com/zenstat/notebook-for-beginners-step-1-load-the-data' >step 1 kernel</a> Be sure to check it out if you haven't.\n</h4>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\npd.options.display.float_format = '{:.2f}'.format","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:58:56.24948Z","iopub.execute_input":"2021-11-05T15:58:56.249879Z","iopub.status.idle":"2021-11-05T15:58:56.2785Z","shell.execute_reply.started":"2021-11-05T15:58:56.249783Z","shell.execute_reply":"2021-11-05T15:58:56.277675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load items data \nitems = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\")\n#load items category data\nitemscat = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv\")\n#load shops data\nshops = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\")\n#load sales train data  \ntrain = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv\")\n#load sales test data  \ntest = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:58:56.280172Z","iopub.execute_input":"2021-11-05T15:58:56.280714Z","iopub.status.idle":"2021-11-05T15:58:59.370236Z","shell.execute_reply.started":"2021-11-05T15:58:56.280679Z","shell.execute_reply":"2021-11-05T15:58:59.369138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>lets quickly take a look at individual datasets. ","metadata":{}},{"cell_type":"code","source":"print(\"************************** items table **************************\\n\", items.head(1),\"\\n******************************************************************************\\n\")\nprint(\"************************** items category table **************************\\n\",itemscat.head(1),\"\\n******************************************************************************\\n\")\nprint(\"************************** shops table **************************\\n\",shops.head(1),\"\\n******************************************************************************\\n\")\nprint(\"************************** train table **************************\\n\",train.head(1),\"\\n******************************************************************************\\n\")\nprint(\"************************** test table **************************\\n\",test.head(1),\"\\n******************************************************************************\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:58:59.372419Z","iopub.execute_input":"2021-11-05T15:58:59.372695Z","iopub.status.idle":"2021-11-05T15:58:59.398968Z","shell.execute_reply.started":"2021-11-05T15:58:59.372663Z","shell.execute_reply":"2021-11-05T15:58:59.397688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Before we begin data analysis, we first need a complete dataset that contains all variables we have for prediction. \n <h4>Here the datasets we are given are scattered and needs to be merged. So lets merge all datasets one by one and create a final dataset by merging all variables.","metadata":{}},{"cell_type":"code","source":"train_itms= train.join(items, on='item_id', rsuffix='_')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:58:59.401277Z","iopub.execute_input":"2021-11-05T15:58:59.401612Z","iopub.status.idle":"2021-11-05T15:58:59.870856Z","shell.execute_reply.started":"2021-11-05T15:58:59.401583Z","shell.execute_reply":"2021-11-05T15:58:59.869777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_itms_shps = train_itms.join(shops, on='shop_id', rsuffix='_')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:58:59.873247Z","iopub.execute_input":"2021-11-05T15:58:59.873677Z","iopub.status.idle":"2021-11-05T15:59:00.330997Z","shell.execute_reply.started":"2021-11-05T15:58:59.873632Z","shell.execute_reply":"2021-11-05T15:59:00.329823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_itms_shps_cat = train_itms_shps.join(itemscat, on='item_category_id', rsuffix='_')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:00.332489Z","iopub.execute_input":"2021-11-05T15:59:00.332817Z","iopub.status.idle":"2021-11-05T15:59:00.703577Z","shell.execute_reply.started":"2021-11-05T15:59:00.332778Z","shell.execute_reply":"2021-11-05T15:59:00.702619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop duplicate columns \ntrain_itms_shps_cat = train_itms_shps.join(itemscat, on='item_category_id', rsuffix='_')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:00.704553Z","iopub.execute_input":"2021-11-05T15:59:00.704807Z","iopub.status.idle":"2021-11-05T15:59:01.114192Z","shell.execute_reply.started":"2021-11-05T15:59:00.70478Z","shell.execute_reply":"2021-11-05T15:59:01.113181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_itms_shps_cat.drop([\"item_id_\",\"shop_id_\",\"item_category_id_\" ], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:01.11541Z","iopub.execute_input":"2021-11-05T15:59:01.115629Z","iopub.status.idle":"2021-11-05T15:59:01.642274Z","shell.execute_reply.started":"2021-11-05T15:59:01.115598Z","shell.execute_reply":"2021-11-05T15:59:01.641251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Here is a cool thing about python. You can do all above merge steps in one line","metadata":{}},{"cell_type":"code","source":"train_data=train.join(items, on='item_id', rsuffix='_').join(shops, on='shop_id', rsuffix='_').join(itemscat, on='item_category_id', rsuffix='_').drop([\"item_id_\",\"shop_id_\",\"item_category_id_\" ], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:01.644199Z","iopub.execute_input":"2021-11-05T15:59:01.644731Z","iopub.status.idle":"2021-11-05T15:59:03.264796Z","shell.execute_reply.started":"2021-11-05T15:59:01.64468Z","shell.execute_reply":"2021-11-05T15:59:03.263801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Also, let's change data type for date field same way as first kernel.","metadata":{}},{"cell_type":"code","source":"# lets convert date data type to date field.\ntrain_data[\"date\"]=pd.to_datetime(train_data[\"date\"])\n#create year and year-month field so we can summarise the data by dates\ntrain_data[\"date_yyyy\"]=pd.to_datetime(train_data[\"date\"]).dt.strftime('%Y') \ntrain_data[\"date_yyyymm\"]=pd.to_datetime(train_data[\"date\"]).dt.strftime('%Y-%m') ","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:03.266327Z","iopub.execute_input":"2021-11-05T15:59:03.266545Z","iopub.status.idle":"2021-11-05T15:59:46.478558Z","shell.execute_reply.started":"2021-11-05T15:59:03.26652Z","shell.execute_reply":"2021-11-05T15:59:46.477013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>How to check if there any duplicates in the datasets? ","metadata":{}},{"cell_type":"code","source":"#all duplcates will be stored in duplicate dataframe\nduplicate = train_data[train_data.duplicated()]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:46.481808Z","iopub.execute_input":"2021-11-05T15:59:46.482088Z","iopub.status.idle":"2021-11-05T15:59:50.503059Z","shell.execute_reply.started":"2021-11-05T15:59:46.482053Z","shell.execute_reply":"2021-11-05T15:59:50.502236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Check duplicate dataset and you will find 6 records that are duplicate. Below is how we can find and then get rid of duplicates","metadata":{}},{"cell_type":"code","source":"#we will get rid of the duplicates using \ntrain_final= train_data.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:50.504249Z","iopub.execute_input":"2021-11-05T15:59:50.504494Z","iopub.status.idle":"2021-11-05T15:59:54.022011Z","shell.execute_reply.started":"2021-11-05T15:59:50.504469Z","shell.execute_reply":"2021-11-05T15:59:54.020773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_final is the data without any duplicates\ntrain_final.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:54.02347Z","iopub.execute_input":"2021-11-05T15:59:54.023707Z","iopub.status.idle":"2021-11-05T15:59:54.033159Z","shell.execute_reply.started":"2021-11-05T15:59:54.023675Z","shell.execute_reply":"2021-11-05T15:59:54.032115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Are there any outliers in the dataset","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.boxplot(x=train_final['item_price'])","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:54.034938Z","iopub.execute_input":"2021-11-05T15:59:54.035301Z","iopub.status.idle":"2021-11-05T15:59:56.290117Z","shell.execute_reply.started":"2021-11-05T15:59:54.035243Z","shell.execute_reply":"2021-11-05T15:59:56.289095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>if you look carefully at item price column, there is a single item price which is $307,980. while mostly item price are less than 50k. there are only 3 items greater than 50k.\n<br><br>Same way, the count seem to have some exceptionally high values. this time we will treat them both. \n","metadata":{}},{"cell_type":"code","source":"#train_final[train_final[\"item_count\"]>40000].head(30).reset_index() \ntrain_final[train_final[\"item_cnt_day\"]>40000].head(30).reset_index() ","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:56.291578Z","iopub.execute_input":"2021-11-05T15:59:56.292047Z","iopub.status.idle":"2021-11-05T15:59:56.316261Z","shell.execute_reply.started":"2021-11-05T15:59:56.292016Z","shell.execute_reply":"2021-11-05T15:59:56.315637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=train_final['item_cnt_day'])","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:56.317282Z","iopub.execute_input":"2021-11-05T15:59:56.318118Z","iopub.status.idle":"2021-11-05T15:59:57.71975Z","shell.execute_reply.started":"2021-11-05T15:59:56.318083Z","shell.execute_reply":"2021-11-05T15:59:57.718879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_final[train_final[\"item_cnt_day\"]>500].head(30).reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:57.720961Z","iopub.execute_input":"2021-11-05T15:59:57.721225Z","iopub.status.idle":"2021-11-05T15:59:57.748394Z","shell.execute_reply.started":"2021-11-05T15:59:57.721196Z","shell.execute_reply":"2021-11-05T15:59:57.747381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Here we will treat the outliers and will cap them around 99th percentile. \n\n<h3><span style=\"color:red\"> Please be warned before doing this. </span> ALWAYS ALWAYS study outliers, they give insight in to process, business and outliers provide good information that will help in building statistical models aka ML algos.","metadata":{}},{"cell_type":"code","source":"#WIP, \ntrain_final['item_cnt_day'] = np.where(train_final['item_cnt_day']>400 , 400,train_final['item_cnt_day'] )\n\n#cap item count >500 to 500 \n#cap item price >40000 to 40000 \ntrain_final['item_price'] = np.where(train_final['item_price']>40000 , 40000,train_final['item_price'] )\n\n#another way to do same in python, try . there are tons of other ways!!! \n#train_final['item_cnt_day'] = [400 if x > 400 else x for x in train_final['item_cnt_day']]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:57.749795Z","iopub.execute_input":"2021-11-05T15:59:57.750709Z","iopub.status.idle":"2021-11-05T15:59:57.813772Z","shell.execute_reply.started":"2021-11-05T15:59:57.750673Z","shell.execute_reply":"2021-11-05T15:59:57.812635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=train_final['item_cnt_day'])","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:57.815254Z","iopub.execute_input":"2021-11-05T15:59:57.815842Z","iopub.status.idle":"2021-11-05T15:59:59.372466Z","shell.execute_reply.started":"2021-11-05T15:59:57.815801Z","shell.execute_reply":"2021-11-05T15:59:59.3714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=train_final['item_price'])","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:59:59.373949Z","iopub.execute_input":"2021-11-05T15:59:59.374188Z","iopub.status.idle":"2021-11-05T16:00:00.61635Z","shell.execute_reply.started":"2021-11-05T15:59:59.374162Z","shell.execute_reply":"2021-11-05T16:00:00.615234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>How to check if there any missing value in any of the variable? ","metadata":{}},{"cell_type":"code","source":"train_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T16:00:00.617673Z","iopub.execute_input":"2021-11-05T16:00:00.617885Z","iopub.status.idle":"2021-11-05T16:00:02.249045Z","shell.execute_reply.started":"2021-11-05T16:00:00.617861Z","shell.execute_reply":"2021-11-05T16:00:02.247977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>All above variables are showing zero's that means there are no variables with missing values. This is a good thing, <span style=\"color:red\"> We are in luck, we can skip missing value treatment step !!!</h5><h5>lets summarize all sales by date variable to see if we can learn something. ","metadata":{}},{"cell_type":"code","source":"train_year = train_data.groupby(['date_yyyy'])[\"item_price\",\"item_cnt_day\"].sum().reset_index().sort_values('date_yyyy', ascending=True)\ntrain_year.T","metadata":{"execution":{"iopub.status.busy":"2021-11-05T16:00:02.250211Z","iopub.execute_input":"2021-11-05T16:00:02.250447Z","iopub.status.idle":"2021-11-05T16:00:02.586532Z","shell.execute_reply.started":"2021-11-05T16:00:02.250423Z","shell.execute_reply":"2021-11-05T16:00:02.585396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_month = train_data.groupby(['date_yyyymm'])[\"item_price\",\"item_cnt_day\"].sum().reset_index().sort_values('date_yyyymm', ascending=True)\ntrain_month.T","metadata":{"execution":{"iopub.status.busy":"2021-11-05T16:00:02.588499Z","iopub.execute_input":"2021-11-05T16:00:02.588829Z","iopub.status.idle":"2021-11-05T16:00:02.973534Z","shell.execute_reply.started":"2021-11-05T16:00:02.588788Z","shell.execute_reply":"2021-11-05T16:00:02.972492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>but this doesnt look very intuitive, a visual is the easiest way to share the story. <br><br>\nLets display the same details on bar plot and see if it looks better. First lets look at yearly trend of total sales","metadata":{}},{"cell_type":"code","source":"import seaborn as sns  #import seaborn\nsns.set(rc={'figure.figsize':(5.7,5.27)})\nimport matplotlib.pylab as plt \nplt.xticks(rotation=45)\nsns.barplot(x='date_yyyy', y='item_price', data=train_year)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T16:00:02.975454Z","iopub.execute_input":"2021-11-05T16:00:02.9758Z","iopub.status.idle":"2021-11-05T16:00:03.221894Z","shell.execute_reply.started":"2021-11-05T16:00:02.975757Z","shell.execute_reply":"2021-11-05T16:00:03.220857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>We can clearly see 2014 has been the best year in terms of sales. Lets deep dive further and look in to monthly trend","metadata":{}},{"cell_type":"code","source":"#Historic sales items per day\nsns.set(rc={'figure.figsize':(20,6)})\nimport matplotlib.pylab as plt\nplt.xticks(rotation=45)\nsns.barplot(x='date_yyyymm', y='item_price', data=train_month)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T16:00:03.223211Z","iopub.execute_input":"2021-11-05T16:00:03.223738Z","iopub.status.idle":"2021-11-05T16:00:03.94365Z","shell.execute_reply.started":"2021-11-05T16:00:03.223709Z","shell.execute_reply":"2021-11-05T16:00:03.942588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>As you can see, there are two peaks in the data and they are present in last two months of each year. <BR><BR>This is because of festival season and people are buying a lot in the last couple of months. this is also attributed to a lot of discounts that are provided during festival season. \n<H5> does sales item show the same trend. Lets see ","metadata":{}},{"cell_type":"code","source":"#Historic sales items count per day\nsns.set(rc={'figure.figsize':(20,6)})\nimport matplotlib.pylab as plt\nplt.xticks(rotation=45)\nsns.barplot(x='date_yyyymm', y='item_cnt_day', data=train_month)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T16:00:03.944914Z","iopub.execute_input":"2021-11-05T16:00:03.945189Z","iopub.status.idle":"2021-11-05T16:00:04.762521Z","shell.execute_reply.started":"2021-11-05T16:00:03.945159Z","shell.execute_reply":"2021-11-05T16:00:04.761446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"color:red;text-align:center\"> * * *  to be continued * * * </h3>","metadata":{}}]}