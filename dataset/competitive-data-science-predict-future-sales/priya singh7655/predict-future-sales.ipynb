{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Loading Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\nsales_train = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\ntest = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')\nitem_cat = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('ITEMS')\ndisplay(items.head())\n\nprint('ITEM CATEGORIES')\ndisplay(item_cat.head())\n\nprint('SHOPS')\ndisplay(shops.head())\n\nprint('SALES TRAIN')\ndisplay(sales_train.head())\n\nprint('TEST')\ndisplay(test.head())\n\nprint('SAMPLE SUBMISSION')\ndisplay(sample_submission.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exploring Sample train dataset\nsales_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking missing values for sales train and test dataset\nprint('Sales train')\ndisplay(sales_train.isnull().sum())\n\nprint('test')\ndisplay(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*No Missing values found*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Sales train')\ndisplay(sales_train.describe())\n\nprint('Test')\ndisplay(test.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Sales train dataset contains negative values in feature item_price and  item_cnt_day.*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Exploring the Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing duplicates from Sales train dataset\nsubset = ['date','date_block_num','shop_id','item_id','item_cnt_day']\nsales_train.duplicated(subset=subset).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.drop_duplicates(subset=subset,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for negative values in item_price and item_cnt_day and dropping them as it contains only 1 negative value.\nprint('Negative values in item_price')\ndisplay(sales_train[sales_train['item_price']<0])\n\nprint('Negative values in item_cnt_day')\ndisplay(sales_train[sales_train['item_cnt_day']<0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = sales_train[sales_train['item_price']>0]\nsales_train = sales_train[sales_train['item_cnt_day']>0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Data Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Outliers for item price and item_cnt_day\nsns.boxplot(sales_train['item_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(sales_train['item_cnt_day'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping Ouliers\ndef drop_out(df,feature,high_percentile = .99):\n    df_shape = df.shape[0]         #sales train df size before dropping\n    max_val = df[feature].quantile(high_percentile)      #Percentile value\n    print('Dropping Outliers for ... {}'.format(feature))\n    df = df[df[feature] < max_val]\n    print(str(df_shape - df.shape[0]) + ' ' + feature + ' values over ' + str(max_val) + ' have been removed' )\n    return df\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping outliers for item_price\nsales_train = drop_out(sales_train,'item_price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping outliers for item_cnt_day\nsales_train = drop_out(sales_train,'item_cnt_day')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new dataframe with item_price feature group by shop_id and item_id to get price for each item per shop.\n# We can use this dataframe to create item_price feature for the test dataset.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_price_df = sales_train[['shop_id','item_id','item_price']]\nshop_price_df = shop_price_df.groupby(['shop_id','item_id']).apply(lambda df: df['item_price'][-2:].mean())\nshop_price_df = shop_price_df.to_frame(name='item_price')\nshop_price_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge this dataframe with test dataframe to create item_price feature in test dataset.\ntest = pd.merge(test,shop_price_df,how='left', left_on=['shop_id','item_id'], right_on=['shop_id','item_id'])\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check null values\ntest['item_price'].isnull().sum()      ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Item price in test dataset contains null values.fill this by creating more features from item_categories","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split date feature to month and year from sales_train dataset\nsales_train['month'] = [date.split('.')[1] for date in sales_train['date']]\nsales_train['year'] = [date.split('.')[2] for date in sales_train['date']]\n\nsales_train.drop(['date','date_block_num'],axis=1,inplace=True)\n\n#create month and year features fot test dataset\ntest['month'] = 11\ntest['year'] = 2015","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#change item_cnt_day to item_cnt_month \nsales_train_monthly = sales_train.groupby(['year','month','shop_id','item_id'],\n                                          as_index=False)[['item_cnt_day']].sum()\nsales_train_monthly.rename(columns={'item_cnt_day':'item_cnt_month'},inplace=True)\n\nsales_train_monthly = pd.merge(sales_train_monthly,shop_price_df,how='left',\n                               left_on=['shop_id','item_id'],right_on=['shop_id','item_id'])\nsales_train_monthly.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = sales_train_monthly\nsales_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.reindex(columns=['ID','year','month','shop_id','item_id','item_price'])\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring other Dataset\n* 1. Item_categories","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extracting main categories \nitem_cat['main categories'] = [x.split('-')[0] for x in item_cat['item_category_name']]\n\n# Some items dont have sub category for them we will use None as a sub category\nsub_cat=[]\nfor i in range(len(item_cat)):\n    try:\n        sub_cat.append(item_cat['item_category_name'][i].split('-')[1])\n    except IndexError as e:\n        sub_cat.append('None')\n\nitem_cat['sub categories'] = sub_cat\nitem_cat.drop(['item_category_name'],axis=1,inplace=True)\nitem_cat.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 2. Items dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.merge(items,item_cat,how='left')\n\n#drop item_name and item_category_id\nitems.drop(['item_name','item_category_id'],axis=1,inplace=True)\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge items to test and sales_train dataset\nsales_train = pd.merge(sales_train,items,how='left')\ntest = pd.merge(test,items,how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 3. Shops dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from string import punctuation\n\n#replace all the punctuations from shop_name feature\nshops['shop_name_cleaned'] = shops['shop_name'].apply(lambda s:''.join([x for x in s if x not in punctuation]))\n\n#Extract shop city name\nshops['shop_city'] = shops['shop_name_cleaned'].apply(lambda s:s.split()[0])\n\n#Extract shop type\nshops['shop_type'] = shops['shop_name_cleaned'].apply(lambda s:s.split()[1])\n\n#Extract shop name\nshops['shop_name'] = shops['shop_name_cleaned'].apply(lambda s:s.split()[2:])\n\n#Dropping shop_name_cleaned\nshops.drop(['shop_name_cleaned'],axis=1,inplace=True)\n\nshops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merging sales_train and test dataset with shops dataset\nsales_train = pd.merge(sales_train,shops,how='left')\ntest = pd.merge(test,shops,how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('SALES TRAIN')\ndisplay(sales_train.head())\n\nprint('TEST')\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"* Fill Missing values in item_price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill missing values with median of each main_category and sub_category\ntest['item_price'] = test.groupby(['main categories',\n                                   'sub categories'])['item_price'].apply(lambda df: df.fillna(df.median()))\ntest['item_price'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill missing values with median of each sub category\ntest['item_price'] = test.groupby(['sub categories'])['item_price'].apply(lambda df : df.fillna(df.median()))\ntest['item_price'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test['item_price'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* All the null item_price data contains same main categories (PC) and sub catogories (Гарнитуры/Наушники) value. This main and sub categories are not present in test dataset but are present in sales_train dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill missing values with median of main_category and sub_category from sales_train dataset\n\nfiller = sales_train[(sales_train['main categories'] == 'PC') & (sales_train['sub categories'] == 'Гарнитуры/Наушники')]['item_price'].median()\nfiller = 0\ntest['item_price'].fillna(filler, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['item_price'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clipping the item_cnt_month value to [0,20] range\nsales_train['item_cnt_month'] = sales_train['item_cnt_month'].clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining a target array and dropping it from sales_train dataset\ntarget_aaray = sales_train['item_cnt_month']\nsales_train.drop(['item_cnt_month'],axis=1,inplace=True)\n\n#Dropping ID column from test dataset\ntest_id = test['ID']\ntest.drop(['ID'],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop shop_id and item_id from sales_train and test dataset\nsales_train.drop(['shop_id','item_id'],axis=1,inplace=True)\ntest.drop(['shop_id','item_id'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reducing memory usage\ndef downcast_dtypes(df):\n    # Selecting columns to downcast\n    flt_cols = [c for c in df if df[c].dtype == 'float64']\n    int_cols = [d for d in df if df[d].dtype == 'int64']\n    \n    #Downcasting\n    df[flt_cols] = df[flt_cols].astype(np.float16)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"downcast_dtypes(sales_train)\ndowncast_dtypes(test)\n\nprint('SALES TRAIN')\ndisplay(sales_train.head())\n\nprint('TEST')\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for null values in test and sales_train dataset\nprint('Missing data in sales train: ',sales_train.isnull().any().sum())\nprint('Missing data in test: ',test.isnull().any().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normality test function\ndef norm_test(data,alpha=0.05):\n    from scipy import stats\n    statistic,p_val = stats.normaltest(data)\n    \n    #null hypothesis: array comes from a normal distribution\n    if p_val < alpha:  \n        #The null hypothesis can be rejected\n        is_normal_dist = False\n    else:\n        #The null hypothesis cannot be rejected\n        is_normal_dist = True\n    \n    return is_normal_dist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check normality of all numerical features and change them to normal distribution\nfor i in sales_train.columns:\n    if sales_train[i].dtype != 'object':\n        if norm_test(sales_train[i]) == False:\n            sales_train[i] = np.log1p(sales_train[i])\n            test[i] = np.log1p(test[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_aaray = np.log1p(target_aaray)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ENCODING\nord_enc = OrdinalEncoder()\narr = sales_train.to_numpy()\n#x = sales_train.to_numpy(dtype=str)\nprint(arr)\nX = ord_enc.fit_transform(arr)\nY = target_aaray\n\n#X_predict = ord_enc.fit_transform(test.to_numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Train test split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(sales_train,target_aaray,test_size=0.1,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model creation \n* We will use XGBRegressor model to predict total sales for every product and store","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a model\nxg_model = XGBRegressor()\n\n#Fitting model\nxg_model.fit(x_train, y_train, eval_metric=\"rmse\", eval_set=[(x_train, y_train), (x_test, y_test)], verbose=True, early_stopping_rounds = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}