{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_cat=pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\nitem = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nsales_train=pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\ntest = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# -I.DATA UNDERSTANDING-"},{"metadata":{"trusted":true},"cell_type":"code","source":"item_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train[\"date\"]=pd.to_datetime(sales_train[\"date\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\nsales_train['day'] = sales_train['date'].dt.day\nsales_train['month'] = sales_train[\"date\"].dt.month\nsales_train['year'] = sales_train[\"date\"].dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Montly Sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_sales = sales_train.groupby([\"date_block_num\",\"shop_id\",\"item_id\"]).agg({\"item_price\":\"mean\",\"item_cnt_day\":\"sum\"})\n\nmonthly_sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Allover Daily Sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"allover_sales_by_date = sales_train.groupby([\"date\"])[\"item_cnt_day\"].sum()\nallover_sales_by_date.plot(kind=\"line\",\n                     xlabel=\"Days\",\n                     ylabel=\"Sales\",\n                     title= \"Allover Sales by Date\",\n                     figsize=(26,8));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# -II.DATA ANALYSIS-"},{"metadata":{},"cell_type":"markdown","source":"# 1. Types of Time-Series\n\nTime-series are of generally two types:\n\n    * Additive Time-Series: Additive time-series is time-series where components (trend, seasonality, noise) are added to generate time series.\n    \n        Time-Series = trend + seasonality + noise\n        \n    * Multiplicative Time-Series: Multiplicative time-series is time-series where components (trend, seasonality, noise) are multiplied to generate time series. One can notice an increase in the amplitude of seasonality in multiplicative time-series.\n    \n        Time-Series = trend * seasonality * noise\n\n\n\n### MoM Total Daily Sales (continous / time series)"},{"metadata":{"trusted":true},"cell_type":"code","source":"mom_sales =  sales_train.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\n\nmom_sales.plot(kind=\"line\",\n                     xlabel=\"Days\",\n                     ylabel=\"Sales\",\n                     title= \"Month-over-Month Total Daily Sales\",\n                     figsize=(16,8));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Trend, Seasonality and Stationary\n\n\n## a.Trend:\n\n    The trends represent an increase or decrease in time-series value over time. If we notice that the value of measurement over time is increasing or decreasing then we can say that it has an upward or downward trend.\n    \n    How to remove trend from time-series data?\n\n    There are various ways to de-trend a time series. We have explained a few below.\n\n    1. Log Transformation.\n    2. Power Transformation.\n    3. Local Smoothing - Applying moving window functions to time-series data.\n    4. Differencing a time-series.\n    5. Linear Regression.\n    \n    \n## b.Seasonality:  \n\n    The seasonality represents variations in measured value which repeats over the same time interval regularly. If we notice that particular variations in value are happening every week, month, quarter or half-yearly then we can say that time series has some kind of seasonality.\n    \n    How to remove seasonality from time-series data?\n    \n    Average de-trended values.\n    Differencing a time-series.\n    Use the loess method.\n\n    (There are various ways to remove seasonality. The task of removing seasonality is a bit complicated. We have explained a few ways below to remove seasonality.)\n\n    \n    \n## c.Stationary \n    \n    If there is a upward or downward trend, or multiplative waving condition, or irregular waving trend in time series, it points to a Non-Stationary time serie. \n    To proceed the analysis, we need to convert it to Stationary timeseries.\n    \n    There are multiple tests that can be used to check stationarity as well.\n\n    1. ADF( Augmented Dicky Fuller Test)\n    2. KPSS\n    3. PP (Phillips-Perron test)\n\n    \n\n\n\n- Let's check the trend seasonality and stationarity of our time serie visualizing its mean and standard deviation \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(121)\nmom_sales.rolling(window = 12).mean().plot(figsize=(25,5), \n                                                 color=\"tab:blue\", \n                                                 title=\"Rolling Mean Over 12 Month Period\", \n                                                 legend = True);\nplt.subplot(122)\nmom_sales.rolling(window = 12).std().plot(color=\"tab:orange\", \n                                                title=\"Rolling Variance Over 12 Month Period\", \n                                                legend=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is seen that there is a descending additive trend in mean whereas ascending and increasing multivative trend in variance."},{"metadata":{},"cell_type":"markdown","source":"# 3. Decompose Time Series To Its Components\n\n    Normally there is 2 decompose model as \"multiplicative\" model and \"additive\" model. We will prefer additive model assessing no multiplative condition in the serie."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ndecompose_result = seasonal_decompose(mom_sales, freq=12, model=\"additive\")\ntrend = decompose_result.trend\nseasonal = decompose_result.seasonal\nresidual = decompose_result.resid\n\ndecompose_result.plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Stationary Testing with Dicky-Fuller "},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\ndef test_stationarity(timeseries):\n    \n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['1.Test Statistic','2.p-value','3.Lags Used','4.Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['5.Critical Value (%s)'%key] = value\n    print (dfoutput)\n\ntest_stationarity(mom_sales)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    We can interpret above results based on p-values of result.\n    1. p-value > 0.05 - This implies that time-series is non-stationary.\n    2. p-value <=0.05 - This implies that time-series is stationary\n                \n    Since P-value is greater than 0.05, our time-series is not stationary. It has time-dependent components present that we need to remove.                "},{"metadata":{},"cell_type":"markdown","source":"# -III.DATA PREPROCESSING-"},{"metadata":{},"cell_type":"markdown","source":"## Remove Trend and Seasonality\n\n    There are various ways like differencing, power transformation, log transformation, etc. to remove trends from data as we have discussed above. \n    \n    We'll practice differencing."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a differencing series to remove trend\n\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return pd.Series(diff)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts=sales_train.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(16,16))\nplt.subplot(311)\nplt.title('Original Time Series')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts)\n\nplt.subplot(312)\nplt.title('After Removing Trend')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts)\nplt.plot(new_ts)\nplt.plot()\n\nplt.subplot(313)\nplt.title('After Removing Seasonality')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts,12)       # assuming the seasonality is 12 months long\nplt.plot(new_ts)\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now testing the stationarity again after removing trend and seasonality\n\ntest_stationarity(new_ts)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Now after the transformations, our p-value for the Dickey Fuller Test is within 5 %. So we can assume Stationarity of the series."},{"metadata":{},"cell_type":"markdown","source":"# -IV.PREDICTIVE ANALYSIS / FORECASTING- "},{"metadata":{},"cell_type":"markdown","source":"## AR, MA and ARMA (Autoregressive Moving Average) models:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\ndef tsplot(y, lags=None, figsize=(10, 8), style='bmh',title=''):\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        #mpl.rcParams['font.family'] = 'Ubuntu Mono'\n        layout = (3, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        qq_ax = plt.subplot2grid(layout, (2, 0))\n        pp_ax = plt.subplot2grid(layout, (2, 1))\n        \n        y.plot(ax=ts_ax)\n        ts_ax.set_title(title)\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n        sm.qqplot(y, line='s', ax=qq_ax)\n        qq_ax.set_title('QQ Plot')        \n        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)\n\n        plt.tight_layout()\n    return ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simulate an AR(1) process with alpha = 0.6\nnp.random.seed(1)\nn_samples = int(1000)\na = 0.6\nx = w = np.random.normal(size=n_samples)\n\nfor t in range(n_samples):\n    x[t] = a*x[t-1] + w[t]\nlimit=12    \n_ = tsplot(x, lags=limit,title=\"AR(1)process\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AR(1) process -- has ACF tailing out and PACF cutting off at lag=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simulate an AR(2) process\n\nn = int(1000)\nalphas = np.array([.444, .333])\nbetas = np.array([0.])\n\n# Python requires us to specify the zero-lag value which is 1\n# Also note that the alphas for the AR model must be negated\n# We also set the betas for the MA equal to 0 for an AR(p) model\n# For more information see the examples at statsmodels.org\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\nar2 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n) \n_ = tsplot(ar2, lags=12,title=\"AR(2) process\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AR(2) process -- has ACF tailing out and PACF cutting off at lag=2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simulate an MA(1) process\nn = int(1000)\n# set the AR(p) alphas equal to 0\nalphas = np.array([0.])\nbetas = np.array([0.8])\n# add zero-lag and negate alphas\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\nma1 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n) \nlimit=12\n_ = tsplot(ma1, lags=limit,title=\"MA(1) process\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MA(1) process -- has ACF cut off at lag=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simulate MA(2) process with betas 0.6, 0.4\nn = int(1000)\nalphas = np.array([0.])\nbetas = np.array([0.6, 0.4])\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\nma3 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n)\n_ = tsplot(ma3, lags=12,title=\"MA(2) process\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MA(2) process -- has ACF cut off at lag=2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simulate an ARMA(2, 2) model with alphas=[0.5,-0.25] and betas=[0.5,-0.3]\nmax_lag = 12\n\nn = int(5000) # lots of samples to help estimates\nburn = int(n/10) # number of samples to discard before fit\n\nalphas = np.array([0.8, -0.65])\nbetas = np.array([0.5, -0.7])\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\narma22 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n, burnin=burn)\n_ = tsplot(arma22, lags=max_lag,title=\"ARMA(2,2) process\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Its not very clear/straight-forward. Let's use a systematic approach to finding the order of AR and MA processes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# pick best order by aic \n# smallest aic value wins\nbest_aic = np.inf \nbest_order = None\nbest_mdl = None\n\nrng = range(5)\nfor i in rng:\n    for j in rng:\n        try:\n            tmp_mdl = smt.ARMA(arma22, order=(i, j)).fit(method='mle', trend='nc')\n            tmp_aic = tmp_mdl.aic\n            if tmp_aic < best_aic:\n                best_aic = tmp_aic\n                best_order = (i, j)\n                best_mdl = tmp_mdl\n        except: continue\n\n\nprint('aic: {:6.5f} | order: {}'.format(best_aic, best_order))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We've correctly identified the order of the simulated process as ARMA(2,2)\n\n     Lets use it for the sales time-series."},{"metadata":{"trusted":true},"cell_type":"code","source":"# pick best order by aic \n# smallest aic value wins\nbest_aic = np.inf \nbest_order = None\nbest_mdl = None\n\nrng = range(5)\nfor i in rng:\n    for j in rng:\n        try:\n            tmp_mdl = smt.ARMA(new_ts.values, order=(i, j)).fit(method='mle', trend='nc')\n            tmp_aic = tmp_mdl.aic\n            if tmp_aic < best_aic:\n                best_aic = tmp_aic\n                best_order = (i, j)\n                best_mdl = tmp_mdl\n        except: continue\n\n\nprint('aic: {:6.5f} | order: {}'.format(best_aic, best_order))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simply use best_mdl.predict() to predict the next values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding the dates to the Time-series as index\nts=sales_train.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.index=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nts=ts.reset_index()\nts.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prophet:\n\n    Recently open-sourced by Facebook research. It's a very promising tool, that is often a very handy and quick solution to the frustrating flatline.\n    \n    Sure, one could argue that with proper pre-processing and carefully tuning the parameters the above graph would not happen.\n\n    But the truth is that most of us don't either have the patience or the expertise to make it happen.\n\n    Also, there is the fact that in most practical scenarios- there is often a lot of time-series that needs to be predicted. Eg: This competition. It requires us to predict the next month sales for the Store - item level combinations which could be in the thousands.(ie) predict 1000s of parameters!\n\n    Another neat functionality is that it follows the typical sklearn syntax.\n\n    At its core, the Prophet procedure is an additive regression model with four main components:\n\n    A piecewise linear or logistic growth curve trend. Prophet automatically detects changes in trends by selecting changepoints from the data.\n    A yearly seasonal component modeled using Fourier series.\n    A weekly seasonal component using dummy variables.\n    A user-provided list of important holidays.\n\n   Resources for learning more about prophet:\n\n   https://www.youtube.com/watch?v=95-HMzxsghY \\\n   https://facebook.github.io/prophet/docs/quick_start.html#python-api \\\n   https://research.fb.com/prophet-forecasting-at-scale/ \\\n   https://blog.exploratory.io/is-prophet-better-than-arima-for-forecasting-time-series-fa9ae08a5851\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet\n#prophet reqiures a pandas df at the below config \n# ( date column named as DS and the value column as Y)\nts.columns=['ds','y']\nmodel = Prophet( yearly_seasonality=True) #instantiate Prophet with only yearly seasonality as our data is monthly \nmodel.fit(ts) #fit the model with your dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict for five months in the furure and MS - month start is the frequency\nfuture = model.make_future_dataframe(periods = 5, freq = 'MS')  \n# now lets make the forecasts\nforecast = model.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.plot(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    The trend and seasonality from Prophet look similar to the ones that we had earlier using the traditional methods."},{"metadata":{},"cell_type":"markdown","source":"## Hierarchical time series:"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_sales=sales_train.groupby(['date_block_num'])[\"item_cnt_day\"].sum()\ndates=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\n\ntotal_sales.index=dates\ntotal_sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Middle out:\n\n    Let's predict for the store level"},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_shop_sales=sales_train.groupby([\"date_block_num\",\"shop_id\"])[\"item_cnt_day\"].sum()\n# get the shops to the columns\nmonthly_shop_sales=monthly_shop_sales.unstack(level=1)\nmonthly_shop_sales=monthly_shop_sales.fillna(0)\nmonthly_shop_sales.index=dates\nmonthly_shop_sales=monthly_shop_sales.reset_index()\nmonthly_shop_sales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nstart_time=time.time()\n\n# Calculating the base forecasts using prophet\n# From HTSprophet pachage -- https://github.com/CollinRooney12/htsprophet/blob/master/htsprophet/hts.py\nforecastsDict = {}\nfor node in range(len(monthly_shop_sales)):\n    # take the date-column and the col to be forecasted\n    nodeToForecast = pd.concat([monthly_shop_sales.iloc[:,0], monthly_shop_sales.iloc[:, node+1]], axis = 1)\n#     print(nodeToForecast.head())  # just to check\n# rename for prophet compatability\n    nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[0] : 'ds'})\n    nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[1] : 'y'})\n    growth = 'linear'\n    m = Prophet(growth, yearly_seasonality=True)\n    m.fit(nodeToForecast)\n    future = m.make_future_dataframe(periods = 1, freq = 'MS')\n    forecastsDict[node] = m.predict(future)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions = np.zeros([len(forecastsDict[0].yhat),1]) \nnCols = len(list(forecastsDict.keys()))+1\nfor key in range(0, nCols-1):\n    f1 = np.array(forecastsDict[key].yhat)\n    f2 = f1[:, np.newaxis]\n    if key==0:\n        predictions=f2.copy()\n       # print(predictions.shape)\n    else:\n       predictions = np.concatenate((predictions, f2), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction=predictions[-1]\nprediction","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}