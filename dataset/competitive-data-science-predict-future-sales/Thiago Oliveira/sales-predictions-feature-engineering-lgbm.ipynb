{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Loading Initial Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom calendar import monthrange\nfrom itertools import product\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"shops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\nitems = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\ncatgs = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\nsales = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\n\ntestd = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\nsampl = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA: Search for Outliers\n\nSearch for NaN values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sales.isna().sum(), '\\n')\nprint(testd.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No NaN values found, look for data distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=sales.item_cnt_day)\n\nprint('Item count day - Min: {}, Max: {}'.format(sales.item_cnt_day.min(), sales.item_cnt_day.max()))\n\nplt.figure(figsize=(10,4))\nplt.xlim(sales.item_price.min(), sales.item_price.max()*1.1)\nsns.boxplot(x=sales.item_price)\n\nprint('Item price - Min: {}, Max: {}'.format(sales.item_price.min(), sales.item_price.max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen on the graphs, there are some high outliers on prices and item count. \n\nAlso there's some negative values on prices and count. Nagative values are expected on count values (devolution cases), but not expected on prices.\n\nLet's remove the highest outliers and change the strange price values for a common value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers\nsales = sales[sales.item_price <= 100000]\nsales = sales[sales.item_cnt_day <= 1000]\n\n# Adjusting negatice prices (change it for median values)\nmedian = sales[(sales.shop_id == 32) & (sales.item_id == 2973) & (sales.date_block_num == 4) & (sales.item_price > 0)].item_price.median()\nsales.loc[sales.item_price < 0, 'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Shops dataset preprocessing\n\nSince I speak no Russian, I took advantage of other people work to help extract these features. Great part of this code was extracted from [this notebook](https://www.kaggle.com/karell/xgb-baseline-advanced-feature-engineering).\n\nSeveral shops are duplicates of each other (according to its name). Fix sales and testd set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Якутск Орджоникидзе, 56\nsales.loc[sales.shop_id == 0, 'shop_id'] = 57\ntestd.loc[testd.shop_id == 0, 'shop_id'] = 57\n# Якутск ТЦ \"Центральный\"\nsales.loc[sales.shop_id == 1, 'shop_id'] = 58\ntestd.loc[testd.shop_id == 1, 'shop_id'] = 58\n# Жуковский ул. Чкалова 39м²\nsales.loc[sales.shop_id == 10, 'shop_id'] = 11\ntestd.loc[testd.shop_id == 10, 'shop_id'] = 11\n# РостовНаДону ТРК \"Мегацентр Горизонт\"\nsales.loc[sales.shop_id == 39, 'shop_id'] = 40\ntestd.loc[testd.shop_id == 39, 'shop_id'] = 40","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.shop_name.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's categorize shops in ['Орджоникидзе,' 'ТЦ' 'ТРК' 'ТРЦ', 'ул.' 'Магазин' 'ТК' 'склад' ]\nThen transform other values to 'etc'"},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops['shop_category'] = shops['shop_name'].str.split(' ').map(lambda x:x[1]).astype(str)\ncategories = ['Орджоникидзе,', 'ТЦ', 'ТРК', 'ТРЦ','ул.', 'Магазин', 'ТК', 'склад']\nshops.shop_category = shops.shop_category.apply(lambda x: x if (x in categories) else 'etc')\nshops.shop_category.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.groupby(['shop_category']).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, some categories have small values. So we reduce categories 9 to 5.\n['Орджоникидзе,', 'ТЦ', 'ТРК', 'ТРЦ','ул.', 'Магазин', 'ТК', 'склад', 'etc'] => ['ТЦ', 'ТРК', 'ТРЦ', 'ТК', 'etc']**"},{"metadata":{"trusted":true},"cell_type":"code","source":"category = ['ТЦ', 'ТРК', 'ТРЦ', 'ТК']\nshops.shop_category = shops.shop_category.apply(lambda x: x if (x in category) else 'etc')\nprint('Category Distribution', shops.groupby(['shop_category']).sum())\n\nshops['shop_category_code'] = LabelEncoder().fit_transform(shops['shop_category'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract City name information from the Shop name"},{"metadata":{"trusted":true},"cell_type":"code","source":"shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code', 'shop_category_code']]\n\nshops.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Categories dataset preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(catgs.item_category_name.unique()))\ncatgs.item_category_name.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We think that category 'Игровые консоли' and 'Аксессуары' are same as 'Игры'.\nSo, we transform the two features to 'Игры'\nAlso, PC - Гарнитуры/Наушники and change to Музыка - Гарнитуры/Наушники"},{"metadata":{"trusted":true},"cell_type":"code","source":"catgs['type'] = catgs.item_category_name.apply(lambda x: x.split(' ')[0]).astype(str)\ncatgs.loc[(catgs.type == 'Игровые') | (catgs.type == 'Аксессуары'), 'category'] = 'Игры'\ncatgs.loc[catgs.type == 'PC', 'category'] = 'Музыка'\ncategory = ['Игры', 'Карты', 'Кино', 'Книги','Музыка', 'Подарки', 'Программы', 'Служебные', 'Чистые', 'Аксессуары']\ncatgs['type'] = catgs.type.apply(lambda x: x if (x in category) else 'etc')\nprint(catgs.groupby(['type']).sum())\ncatgs['type_code'] = LabelEncoder().fit_transform(catgs['type'])\n\n# if subtype is nan then type\ncatgs['split'] = catgs.item_category_name.apply(lambda x: x.split('-'))\ncatgs['subtype'] = catgs['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncatgs['subtype_code'] = LabelEncoder().fit_transform(catgs['subtype'])\ncatgs = catgs[['item_category_id','type_code', 'subtype_code']]\n\ncatgs.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Append train and test data\n\nConcatenate train (sales) and test (testd) data. Also add manually some missing information on the test data like: date_block_num, year, month, item_cnt_day, item_price.\n\n`item_price` is a missing information and `item_cnt_day` is part of the information we're trying to predict (in fact we're looking for `item_cnt_month`. `item_cnt_month` is the sum of `item_cnt_day` of a given shop and a given item on a month). For now we're gonna fill these values with 0.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['date'] = pd.to_datetime(sales['date'], format='%d.%m.%Y')\nsales['month'] = sales['date'].dt.month\nsales['year'] = sales['date'].dt.year\nsales = sales.drop(columns=['date'])\n\n# sales.head()\nto_append = testd[['shop_id', 'item_id']].copy()\n\nto_append['date_block_num'] = sales['date_block_num'].max() + 1\nto_append['year'] = 2015\nto_append['month'] = 11\nto_append['item_cnt_day'] = 0\nto_append['item_price'] = 0\n\nsales = pd.concat([sales, to_append], ignore_index=True, sort=False)\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Date dataset preprocessing\n\nLet's remove all date data (except `date_block_num`) from sales and store it on `period`."},{"metadata":{"trusted":true},"cell_type":"code","source":"period = sales[['date_block_num', 'year', 'month']].drop_duplicates().reset_index(drop=True)\nperiod['days'] = period.apply(lambda r: monthrange(r.year, r.month)[1], axis=1)\n\nsales = sales.drop(columns=['month', 'year'])\n\nperiod.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's group and summarize the sales dataset. The new dataset (agg_sales) will contain the mean price and total number of items shops and items for every single month.\n\nAfter grouping the sales dataset, it's time to convert shop_id and item_id into columns using a pivot function (shop_item_sales)."},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_sales = sales.groupby(['date_block_num','shop_id','item_id'], as_index=False).agg({'item_price' : np.mean, 'item_cnt_day' : np.sum})\\\n    .rename(columns={'item_cnt_day' : 'item_cnt_month'})\n\nshop_item_sales = pd.pivot_table(agg_sales, values='item_price', index=['date_block_num'],\n                    columns=['shop_id', 'item_id'], fill_value=np.nan)\nshop_item_sales","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen on this data set, there's a lot of NaN information. For instance, shop_id=2 and item_id=30, this column show that the price of this shop/item has changed during, but also show some missing data during the course of time.\n\nThe intuition of the next steps is that once a price is defined it will be the same until it is explicitly changed, and the missing data (between this period) means that no item was sold (then item count will be zero).\n\nThat been said, let's use the fillna function to copy the actual value of a price to future periods until a new value is defined."},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_item_sales = shop_item_sales.fillna(method='ffill')\nshop_item_sales","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be observed, on the same situation of shop_id=2 and item_id=30, almost all prices with NaN values are now filled with the most recent defined price, the exception are the first rows, since there are no previous information and no item sold we can't infer this information.\n\nNow if the missing information defined, it's time to change the dataset to it's original format (\"unpivot\" the table)."},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_sales_future = shop_item_sales.stack().stack().reset_index().rename(columns={0 : 'item_price'})\n\nprint('agg_sales shape: ', agg_sales.shape, '\\n')\nprint('agg_sales_future shape: ',agg_sales_future.shape)\n\nagg_sales_future.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since a lot of missing data was infered, the agg_sales_future have much more rows than the original agg_sales data set, however the agg_sales data set still have one extra column, the item_cnt_month.\n\nThe next step will join this two tables and fulfill all missing data (on item_cnt_month column) with zero (due the assumtion that no items were sold in this cases)."},{"metadata":{"trusted":true},"cell_type":"code","source":"month_summary = pd.merge(agg_sales_future, agg_sales.drop(columns='item_price'), how='left', on=['date_block_num', 'shop_id', 'item_id'])\\\n            .fillna(0.0)\\\n            .rename(columns= {'item_price' : 'item_price_month'})\\\n            .sort_values(by=['shop_id', 'item_id', 'date_block_num'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's add more dimensional data (`date_block_num`, `year`, `month`, `days`, `city_code`, `shop_category_code`, `shop_id`, `item_category_id`, `type_code`, `subtype_code`, `item_id`) to the above dataset. To achieve this goal let's join it with the other pre precessed datasets.\n\nAlso, let's downcast this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join dimensional data\nmonth_summary = pd.merge(month_summary, shops, on='shop_id')\nmonth_summary = pd.merge(month_summary, items, on='item_id')\nmonth_summary = pd.merge(month_summary, catgs, on='item_category_id')\nmonth_summary = pd.merge(month_summary, period, on='date_block_num')\n\n# Adjusting columns order\nmonth_summary = month_summary[['date_block_num', 'year', 'month', 'days', 'city_code', 'shop_category_code', 'shop_id', 'item_category_id', \n                               'type_code', 'subtype_code', 'item_id', 'item_price_month', 'item_cnt_month']]\n\n# Downcasting values\nfor c in ['date_block_num', 'month', 'days', 'city_code', 'shop_category_code', 'shop_id', 'item_category_id', 'type_code', 'subtype_code']:\n    month_summary[c] = month_summary[c].astype(np.int8)\nmonth_summary['item_id'] = month_summary['item_id'].astype(np.int16)\nmonth_summary['year'] = month_summary['year'].astype(np.int16)\nmonth_summary['item_cnt_month'] = month_summary['item_cnt_month'].astype(np.float16)\nmonth_summary['item_price_month'] = month_summary['item_price_month'].astype(np.float16)\n\n# Remove unused and temporary datasets\ndel shops, items, catgs, to_append, shop_item_sales, agg_sales, agg_sales_future\n\nmonth_summary.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Min: {} and Max: {} item_cnt_month values'.format(month_summary['item_cnt_month'].min(), month_summary['item_cnt_month'].max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As stated on the problem [evaluation section](https://www.kaggle.com/c/competitive-data-science-predict-future-sales/overview/evaluation):\n\n    Submissions are evaluated by root mean squared error (RMSE). True target values are clipped into [0,20] range.\n    \nthen, let's **clip(0,20)** target (`item_cnt_month`) values. This way train target will be similar to the test predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"month_summary['item_cnt_month'] = month_summary['item_cnt_month'].clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean Encoded Features\n\nThis section is focused on the generation of new features (measures) based on the existing ones. For instance, we can create a generalization feature that calculate the mean of `item_cnt_month` for every shop on a specific month and add this as a new feature `date_shop_avg_item_cnt`. This technique can be used with other dimensions (`item_id`, `item_category_id`, `city_code`) or a combination of features (`shop_id` + `item_category_id`).\n\nThis is a powerful technique to help generalize the prediction capabilities of a model. However, our test data does not contain any `item_price` or `item_cnt_month` (in fact, we're trying to predict this one) data. That been said, we can't count on any existing or generated actual feature, **but we can** still count on existing or generated features of **past data**. This means that, for instance, we can use the last 12 months prices of an `item_id` or last 3 months of any feature combination (`shop_id` + `item_category_id`).\n\nTo achieve this goal let's define the `agg_by_and_lag` function, it will generate mean encoded features based on an informed `group_cols` list of columns and \"lagging\" the data N months informed on the `"},{"metadata":{"trusted":true},"cell_type":"code","source":"def agg_by(month_summary, group_cols, new_col, target_col = 'item_cnt_month', agg_func = 'mean'):\n    aux = month_summary\\\n        .groupby(group_cols, as_index=False)\\\n        .agg({target_col : agg_func})\\\n        .rename(columns= {target_col : new_col})\n    aux[new_col] = aux[new_col].astype(np.float16)\n\n    return pd.merge(month_summary, aux, how='left', on=group_cols)\n\ndef lag_feature(df, col, lags=[1,2,3,6,12]):\n    tmp = df[['date_block_num','shop_id','item_id', col]]\n    for i in lags:\n        shifted = tmp.copy()\n        cols = ['date_block_num','shop_id','item_id', '{}_lag_{}'.format(col, i)]\n        shifted.columns = cols\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left').fillna(value={(cols[-1]) : 0.0})\n    return df\n\ndef agg_by_and_lag(month_summary, group_cols, new_col, lags=[1,2,3,6,12], target_col = 'item_cnt_month', agg_func = 'mean'):\n    tmp = agg_by(month_summary, group_cols, new_col, target_col, agg_func)\n    tmp = lag_feature(tmp, new_col, lags)\n    return tmp.drop(columns=[new_col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean encode and lag `item_cnt_month` data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# date_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num'], 'date_avg_item_cnt', [1])\n\n# date_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_id'], 'date_item_avg_item_cnt', [1,2,3,6,12])\n\n# date_city_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code'], 'date_city_avg_item_cnt', [1])\n\n# date_shop_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id'], 'date_shop_avg_item_cnt', [1,2,3,6,12])\n\n# date_cat_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_category_id'], 'date_cat_avg_item_cnt', [1])\n\n# date_type_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'type_code'], 'date_type_avg_item_cnt', [1])\n\n# date_subtype_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'subtype_code'], 'date_subtype_avg_item_cnt', [1])\n\n# date_shop_category_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code'], 'date_shop_category_avg_item_cnt', [1])\n\n# date_shop_cat_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'item_category_id'], 'date_shop_cat_avg_item_cnt', [1])\n\n# date_shop_type_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'type_code'], 'date_shop_type_avg_item_cnt', [1])\n\n# date_shop_subtype_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'subtype_code'], 'date_shop_subtype_avg_item_cnt', [1])\n\n# date_shop_category_subtype_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code', 'subtype_code'], 'date_shop_category_subtype_avg_item_cnt', [1])\n\n# date_item_city_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code', 'item_id'], 'date_item_city_avg_item_cnt', [1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean encode and lag `item_price_month` data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# date_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num'], 'date_avg_item_price', [1], 'item_price_month')\n\n# date_item_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_id'], 'date_item_avg_item_price', [1,2,3,6,12], 'item_price_month')\n\n# date_city_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code'], 'date_city_avg_item_price', [1], 'item_price_month')\n\n# date_shop_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id'], 'date_shop_avg_item_price', [1,2,3,6,12], 'item_price_month')\n\n# date_cat_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_category_id'], 'date_cat_avg_item_price', [1], 'item_price_month')\n\n# date_type_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'type_code'], 'date_type_avg_item_price', [1], 'item_price_month')\n\n# date_subtype_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'subtype_code'], 'date_subtype_avg_item_price', [1], 'item_price_month')\n\n# date_shop_category_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code'], 'date_shop_category_avg_item_price', [1], 'item_price_month')\n\n# date_shop_cat_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'item_category_id'], 'date_shop_cat_avg_item_price', [1], 'item_price_month')\n\n# date_shop_type_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'type_code'], 'date_shop_type_avg_item_price', [1], 'item_price_month')\n\n# date_shop_subtype_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'subtype_code'], 'date_shop_subtype_avg_item_price', [1], 'item_price_month')\n\n# date_shop_category_subtype_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code', 'subtype_code'], 'date_shop_category_subtype_avg_item_price', [1], 'item_price_month')\n\n# date_item_city_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code', 'item_id'], 'date_item_city_avg_item_price', [1], 'item_price_month')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extra features\n\nLet's extract some extra features, like the difference in months between and actual sell and the first time it happens."},{"metadata":{"trusted":true},"cell_type":"code","source":"month_summary['item_shop_first_sale'] = month_summary['date_block_num'] - month_summary.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmonth_summary['item_first_sale'] = month_summary['date_block_num'] - month_summary.groupby('item_id')['date_block_num'].transform('min')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dataset is now ready, let's summarize it."},{"metadata":{"trusted":true},"cell_type":"code","source":"month_summary.to_pickle('month_summary.pkl')\nmonth_summary.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split Data\n\nLet's split the generated data into train, validation and test data.\n\nFor test data we will take the last month (34), this is the month we must predict the `item_cnt_month`.\n\nFor the validation data we will use the last month in the original training set (33).\n\nAnd for the training data we will use all data between month 12 (since we have lagged some features in 12 months) and 32."},{"metadata":{"trusted":true},"cell_type":"code","source":"month_summary = pd.read_pickle('month_summary.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_subsample(month_summary, target='item_cnt_month'):\n    X_test = month_summary[month_summary['date_block_num'] == 34]\n    X_test = X_test.drop(columns=[target])\n\n    X_val = month_summary[month_summary['date_block_num'] == 33]\n    y_val = X_val[target]\n    X_val = X_val.drop(columns=[target])\n\n    X_train = month_summary[(month_summary['date_block_num'] >= 12) & (month_summary['date_block_num'] < 33)]\n    y_train = X_train[target]\n    X_train = X_train.drop(columns=[target])\n\n    return X_train, y_train, X_val, y_val, X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train, X_val, y_val, X_test = generate_subsample(month_summary.drop(columns=['item_price_month']), 'item_cnt_month')\n\ndel month_summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Model\n\nLets use the train and validation data to train a simples lightgbm model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_gbmodel(X_train, y_train, X_val, y_val):\n\n    RAND_SEED = 42\n\n    lgb_params = {'num_leaves': 2**8, 'max_depth': 19, 'max_bin': 107, #'n_estimators': 3747,\n              'bagging_freq': 1, 'bagging_fraction': 0.7135681370918421, \n              'feature_fraction': 0.49446461478601994, 'min_data_in_leaf': 2**8, # 88\n              'learning_rate': 0.015980721586917768, 'num_threads': 2, \n              'min_sum_hessian_in_leaf': 6,\n              'random_state' : RAND_SEED,\n              'bagging_seed' : RAND_SEED,\n              'boost_from_average' : 'true',\n              'boost' : 'gbdt',\n              'metric' : 'rmse',\n              'verbose' : 1}\n\n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_val = lgb.Dataset(X_val, label=y_val)\n\n    return lgb.train(lgb_params, lgb_train, \n                      num_boost_round=300,\n                      valid_sets=[lgb_train, lgb_val],\n                      early_stopping_rounds=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_old_item = train_gbmodel(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]).clip(0, 20), X_val, y_val.clip(0, 20))\ngbm_model = train_gbmodel(X_train, y_train, X_val, y_val)\n\ny_hat = gbm_model.predict(X_val).clip(0, 20)\nprint(np.sqrt(mean_squared_error(y_val.clip(0, 20), y_hat)))\n\nwith open('./gbm_model.pickle', 'wb') as handle:\n    pickle.dump(gbm_model, handle)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the trained model, let's finally use it to predict the `item_cnt_month` of the test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = gbm_model.predict(X_test).clip(0, 20)\n\nresult = pd.merge(testd, X_test.assign(item_cnt_month=y_pred), how='left', on=['shop_id', 'item_id'])[['ID', 'item_cnt_month']]\nresult.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}