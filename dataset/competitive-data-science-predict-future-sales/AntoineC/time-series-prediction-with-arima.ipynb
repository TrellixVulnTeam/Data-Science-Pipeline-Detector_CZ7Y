{"cells":[{"metadata":{"id":"A9V_9MYUg8WJ"},"cell_type":"markdown","source":"<img src=\"https://img.freepik.com/free-vector/financial-forecast-illustration-flat-tiny-economical-persons-concept_126608-1324.jpg?size=626&ext=jpg&ga=GA1.2.1775781678.1609891200\"></img>"},{"metadata":{"id":"-VDTjD9-hr1T"},"cell_type":"markdown","source":"<h1>ðŸ’°Predict Future Sales CompetitonðŸ’°: time series prediction with ARIMA</h1>"},{"metadata":{},"cell_type":"markdown","source":"This is my second challenge on Kaggle, but my first for almost five years! When I joined, I had (still have) some very big issues in understanding not only the rules of submission, but also the data and ... what everything mean?\n\nSo, as I go along I will try to bring some clear understanding and also point to some fruitful discussions. Ok, here we go!"},{"metadata":{"id":"LfxkA8m7ljGD"},"cell_type":"markdown","source":"## 1. Competition Outline"},{"metadata":{},"cell_type":"markdown","source":"In this competition we work with time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company. \n\nWe are asked to predict total sales for every product and store in the next month.\n\nBut what is time series prediction?"},{"metadata":{"id":"bU6rq8ojs5gV"},"cell_type":"markdown","source":">  *Time series forecasting is the use of a model to predict future values based on previously observed values.* "},{"metadata":{"id":"GnTfj_HKOoNZ"},"cell_type":"markdown","source":"## 2. Data "},{"metadata":{},"cell_type":"markdown","source":"All right, so what data are we provided for which we need to predict the future values?"},{"metadata":{"id":"YhfdhrKgOlKW"},"cell_type":"markdown","source":"- `sales_train.csv` - the training set. Daily historical data from January 2013 to October 2015. It has 1034 unique values `item_id` that were sold at a given `shop_id` at `date_block_num` time.\n- test.csv - the test set. You need to forecast the sales for these shops and products for November 2015. 214k `item_id` sales to predict at `shop_id` for next `date_block_num` time.\n- `sample_submission.csv` - a sample submission file in the correct format.\n- items.csv - supplemental information about the items/products. 22170\nunique values, with the categories.\n- item_categories.csv  - supplemental information about the items categories. 84\nunique values.\n- shops.csv- supplemental information about the shops. The name and sometimes the categories: TPK|Ð¢Ð¦|Ð¢Ðš|Ð¢Ð Ð¦|ÐœÐ¢Ð Ð¦|Ð¢Ð¦"},{"metadata":{"id":"8e-OoJ_6t5Fs"},"cell_type":"markdown","source":"## 3. Libraries ðŸ“š"},{"metadata":{},"cell_type":"markdown","source":"Ok, that sounds good. But what libraries do we need to do the prediction?"},{"metadata":{"id":"pNEYoZPi8W-J","trusted":true},"cell_type":"code","source":"# Basic packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random as rd # generating random numbers\nimport datetime # manipulating date formats\n# Viz\nimport matplotlib.pyplot as plt # basic plotting\nimport seaborn as sns # for prettier plots","execution_count":null,"outputs":[]},{"metadata":{"id":"N61ujKE_JcBs","outputId":"4d4e1f88-7f11-4156-e4d3-4ca146247c22","trusted":true},"cell_type":"code","source":"# time-series prediction packages\nfrom statsmodels.tsa.statespace import sarimax as smt # sarimax algorithm for actual predictions\nfrom statsmodels.graphics.tsaplots import plot_pacf # partial auto-correlation plotting tool for stationarity test\nfrom statsmodels.graphics.tsaplots import plot_acf # auto-correlation plotting tool for stationarity test\nfrom statsmodels.tsa.arima_process import ArmaProcess # arma process for simulation ","execution_count":null,"outputs":[]},{"metadata":{"id":"MmOrenQg_-bY","outputId":"d7df7eac-da61-49b8-c0a8-4b70b9b9215c","trusted":true},"cell_type":"code","source":"sales = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\n\nimport warnings\n\n# settings\nwarnings.filterwarnings('ignore')\n\n\nitem_cat = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nitems = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nsub = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sample_submission.csv\")\nshop = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"lNfkE-nvuOrD"},"cell_type":"markdown","source":"## 3. Example: time series forecasting with ARMA"},{"metadata":{"id":"8FUshQoNKT96"},"cell_type":"markdown","source":"I didn't knew how to model time series so I started by learning about that. It seems that one of the very common model for that are Auto-Regressive Moving Average processes. You must know what are random walk, moving average process (MA) and autoregressive models (AR) before tackling this notebook where we mix everything up with ARMA model. \n\nThere are notebooks about that there:\n\n- [Understanding the random walk and the moving average](https://towardsdatascience.com/how-to-model-time-series-in-python-9983ebbf82cf)\n- [A hands-on tutorial on AR(p) process for time series analysis in Python](https://towardsdatascience.com/time-series-forecasting-with-autoregressive-processes-ba629717401)\n- [Understand and implement ARMA and ARIMA models for time series forcasting Python](https://towardsdatascience.com/advanced-time-series-analysis-with-arma-and-arima-a7d9b589ed6d)"},{"metadata":{},"cell_type":"markdown","source":"Ok, now that we are wise, let's do a little example on how to predict a financial stock with ARMA before we dive into our competition"},{"metadata":{"id":"GYWI7C095wva"},"cell_type":"markdown","source":"### ARMA\n\nan ARMA(p,q) is simply the combination of both Moving Average process and Auto Regressive process into a single equation. $y_t$ value is equal to:\n\n$$y_t = \\overbrace{c + \\theta_1 \\epsilon_{t-1} + \\theta_{t-2} + ... + \\theta_q \\epsilon_{t-q}}^{Moving-Average(q)} + \\\\ \\underbrace{\\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_q y_{t-p}}_{Auto-regression(p)}$$"},{"metadata":{"id":"6RhPTekl6ba2"},"cell_type":"markdown","source":"Here would be a simulated ARMA model:"},{"metadata":{"id":"eBAJ69TtZEJ_","trusted":true},"cell_type":"code","source":"ar1 = np.array([1, 0.33])\nma1 = np.array([1, 0.9])\nsimulated_ARMA_data = ArmaProcess(ar1, ma1).generate_sample(nsample=10000)","execution_count":null,"outputs":[]},{"metadata":{"id":"c4xlyyS0a2gx","outputId":"aec90931-236e-495d-d2a1-74dc3e5f35a4","trusted":true},"cell_type":"code","source":"plt.figure(figsize=[15, 7.5]); # Set dimensions for figure\nplt.plot(simulated_ARMA_data)\nplt.title(\"Simulated ARMA(1,1) Process\")\nplt.xlim([0, 200])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"fea02AiD-R3h"},"cell_type":"markdown","source":"The first (and most important) step in fitting an ARIMA model is the determination of the order of differencing needed to stationarize the series."},{"metadata":{"id":"O7L2pREP-lz3"},"cell_type":"markdown","source":"Differencing is a method of transforming a time series dataset. It can be used to remove the series dependence on time, so-called temporal dependence."},{"metadata":{"id":"16hjMSVS_gHV"},"cell_type":"markdown","source":"Normally, the correct amount of differencing is the lowest order of differencing that yields a time series which fluctuates around a well-defined mean value and whose autocorrelation function (ACF) plot decays fairly rapidly to zero, either from above or below. If the series still exhibits a long-term trend, or otherwise lacks a tendency to return to its mean value, or if its autocorrelations are are positive out to a high number of lags (e.g., 10 or more), then it needs a higher order of differencing"},{"metadata":{"id":"COf-riSObjxL"},"cell_type":"markdown","source":"So let's take a look at the ACF and PACF plots:"},{"metadata":{"id":"fNd3EhXxbezQ","outputId":"0432de08-5814-425f-abaa-de1a0ec74b99","trusted":true},"cell_type":"code","source":"plot_pacf(simulated_ARMA_data);\nplot_acf(simulated_ARMA_data);","execution_count":null,"outputs":[]},{"metadata":{"id":"O7auRHC0boES"},"cell_type":"markdown","source":"As you can see, we cannot infer the order of the ARMA process by looking at these plots. In fact, looking closely, we can see some sinusoidal shape in both ACF and PACF functions. This suggests that both processes are in play."},{"metadata":{"id":"oX7-5ZbcbsOD"},"cell_type":"markdown","source":"Therefore, how can we make sure that we choose the right order for both the AR(p) and MA(q) processes?\n\nWe will need try different combinations of orders, fit an ARIMA model with those orders, and use a criterion for order selection.\n\nThis brings us to the topic of Akaikeâ€™s Information Criterion or AIC."},{"metadata":{"id":"CZn6Lulfbzzf"},"cell_type":"markdown","source":"### Akaike Information Criterion"},{"metadata":{"id":"i4qMJrqHb33D"},"cell_type":"markdown","source":"This criterion is useful for selecting the order (p,d,q) of an ARIMA model. The AIC is expressed as:\n\n$$AIC = -2\\log(L) + 2k$$"},{"metadata":{"id":"slq5jTF5cYLp"},"cell_type":"markdown","source":"Where L is the likelihood of the data and k is the number of parameters.\nIn practice, we select the model with the lowest AIC compared to other models."},{"metadata":{"id":"f0jXlgJQ9ecn"},"cell_type":"markdown","source":"It is important to note that the AIC cannot be used to select the order of differencing (d). Differencing the data will the change the likelihood (L) of the data. The AIC of models with different orders of differencing are therefore not comparable.\n\nAlso, notice that since we select the model with the lowest AIC, more parameters will increase the AIC score and thus penalize the model. While a model with more parameters could perform better, the AIC is used to find the model with the least number of parameters that will still give good results.\nA final note on AIC is that it can only be used relative to other models. A small AIC value is not a guarantee that the model will have a good performance on unsee data, or that its SSE will be small."},{"metadata":{"id":"m3SbCYctvcRF"},"cell_type":"markdown","source":"# SARIMAX"},{"metadata":{"id":"OsTgAq--cxre"},"cell_type":"markdown","source":"Letâ€™s use Microoft stock and model the time series with an ARIMA(p,d,q) model."},{"metadata":{"id":"iAT0ioMknkNT","outputId":"3d2b8fb1-ea48-4449-e077-d1828cc9cbcf","trusted":true},"cell_type":"code","source":"! pip install yfinance","execution_count":null,"outputs":[]},{"metadata":{"id":"-D7Q7ghwveHe","trusted":true},"cell_type":"code","source":"import yfinance as yf\n\nmsft = yf.Ticker(\"MSFT\")\n\n# get historical market data\nhist = msft.history(period=\"5y\")","execution_count":null,"outputs":[]},{"metadata":{"id":"UiUt6ajGzI9X","trusted":true},"cell_type":"code","source":"df_settle = hist['Close'].resample('MS').ffill().dropna()","execution_count":null,"outputs":[]},{"metadata":{"id":"psXxlASeEQYg","outputId":"7798b178-8049-4aaf-d57f-d46a5c6f83ec","trusted":true},"cell_type":"code","source":"df_settle.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"6lftTwLYyVv4","outputId":"328787db-cd53-4060-b691-b41768cce55b","trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\nresult = adfuller(df_settle)\nprint('ADF result', result[0])\nprint('p-value = ', result[1])\n\ncritical_values = result[4]\n\nfor key, value, in critical_values.items():\n  print(\"critical values (%s): %.3f\" % (key, value))","execution_count":null,"outputs":[]},{"metadata":{"id":"u3064UN1-f4g"},"cell_type":"markdown","source":"Here, the p-value is larger than 0.05, meaning the we cannot reject the null hypothesis stating that the time series is non-stationary.\nTherefore, we must apply some transformation and some differencing to remove the trend and remove the change in variance."},{"metadata":{"id":"UbBWxMTp7EX-"},"cell_type":"markdown","source":"### Finding model parameters by grid search"},{"metadata":{"id":"5Slr3nc2aO4N"},"cell_type":"markdown","source":"We could try to find the model parameters by detrending with a log-difference `np.log(df_settle)` and differenciating `df_settle.diff(seasonality)` and then run the Augmented Dickey-Fuller test again to see if we have a stationary time series."},{"metadata":{"id":"UzfFnnB0b0g1"},"cell_type":"markdown","source":"Although these plots can give us a rough idea of the processes in play, it is better to test multiple scenarios and choose the model that yield the lowest AIC."},{"metadata":{"id":"r6qqF_Qgr1Dk"},"cell_type":"markdown","source":"Therefore grid searching (p, d, q, s), allows to feed the data as it is without any transformation since SARIMAX will do the transformation for you under the hood."},{"metadata":{"id":"0yerROAwU9sf"},"cell_type":"markdown","source":"We just need to find the model minimizing the AIC"},{"metadata":{"id":"a7dI2yP12yGh","trusted":true},"cell_type":"code","source":"import itertools\nimport warnings\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n# from statsmodels.api.tsa.statespace import SARIMAX\n\nwarnings.filterwarnings(\"ignore\")\n\ndef arima_grid_search(dataframe, s):\n  p = d = q = range(2)\n  param_combinations = list(itertools.product(p, d, q))\n\n  lowest_aic, pdq, pdqs = None, None, None\n\n  total_iterations = 0\n  for order in param_combinations:\n    for (p, d, q) in param_combinations:\n      seasonal_order = (p, d, q, s)\n      total_iterations +=1\n      try:\n        model = SARIMAX(df_settle, order=order,\n                        seasonal_order = seasonal_order,\n                        enforce_stationarity=False,\n                        enforce_invertibility=False,\n                        disp=False\n                      )\n        model_result = model.fit(maxiter=200, disp=False)\n\n        if not lowest_aic or model_result.aic < lowest_aic:\n          lowest_aic = model_result.aic\n          pdq, pdqs = order, seasonal_order\n\n      except Exception as ex:\n        continue\n\n  return lowest_aic, pdq, pdqs","execution_count":null,"outputs":[]},{"metadata":{"id":"q6UyiMgj8gfd","trusted":true},"cell_type":"code","source":"lowest_aic, order, seasonal_order = arima_grid_search(df_settle, 12)","execution_count":null,"outputs":[]},{"metadata":{"id":"FGkT5GAe8thF","outputId":"10495cea-6ef8-4460-8594-593e14b45f30","trusted":true},"cell_type":"code","source":"print('ARIMA{}x{}'.format(order, seasonal_order))\nprint('Lowest AIC: %.3f' % (lowest_aic))","execution_count":null,"outputs":[]},{"metadata":{"id":"DRLF4QdZswD4"},"cell_type":"markdown","source":"Therefore, this suggests are ARIMA model with an AR(1) process and a MA(0)."},{"metadata":{"id":"CcSnYsBrt5c3"},"cell_type":"markdown","source":"The order of differencing (d) process is 1. But it's not related to the AIC, it has been found by the grid search model itself."},{"metadata":{"id":"Lq_FNRZC8_Jz"},"cell_type":"markdown","source":"## fitting the SARIMAX model"},{"metadata":{"id":"X3tBBo0N9CwV","trusted":true},"cell_type":"code","source":"model = SARIMAX(\n    df_settle, \n    order=order,\n    seasonal_order = seasonal_order,\n    enforce_stationarity=False,\n    enforce_invertibility=False,\n    disp=False\n)\n\nmodel_results = model.fit(maxiter=200, disp=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"VTGbVjrNslhv"},"cell_type":"markdown","source":"Now, we can print a summary of the best model, which an ARIMA (1,1,0)."},{"metadata":{"id":"7CbPqcWM_OrK","outputId":"0347895e-26a1-4bd6-970e-0463a33a7677","trusted":true},"cell_type":"code","source":"print(model_results.summary())","execution_count":null,"outputs":[]},{"metadata":{"id":"CmcqsNB7_Trc","outputId":"8b142caa-4c2b-43d8-aaa5-5627bb7f4927","trusted":true},"cell_type":"code","source":"model_results.plot_diagnostics(figsize=(12,8));","execution_count":null,"outputs":[]},{"metadata":{"id":"V6GcdLL5aFcG"},"cell_type":"markdown","source":"From the normal Q-Q plot, we can see that we almost have a straight line, which suggest no systematic departure from normality. Also, the correlogram on the bottom right suggests that there is no autocorrelation in the residuals, and so they are effectively white noise."},{"metadata":{"id":"WBf1H4w8_lzu","outputId":"6eb6f5e7-739d-4100-ab68-41a7421eeded","trusted":true},"cell_type":"code","source":"model_results.resid.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"IG75kLg9_pL8"},"cell_type":"markdown","source":"## Predicting the model"},{"metadata":{"id":"AcUsyylJc0at"},"cell_type":"markdown","source":"We are ready to plot the predictions of our model and forecast into the future:"},{"metadata":{"id":"fVF_Cf8J_rQ-","trusted":true},"cell_type":"code","source":"n = len(df_settle.index)\nprediction = model_results.get_prediction(\n    start=n-14*5, #changed from 12\n    end=n+5\n)\n\nprediction_ci = prediction.conf_int()","execution_count":null,"outputs":[]},{"metadata":{"id":"VFAnuXpK_-fe","outputId":"7b370c92-9c41-41a7-fc83-a6dbcb7451c0","trusted":true},"cell_type":"code","source":"prediction_ci.head(3)","execution_count":null,"outputs":[]},{"metadata":{"id":"qd1iKbg5ACYu","outputId":"788f1edd-795a-41e0-dcac-526497d18568","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nax = df_settle['2008':].plot(label='actual')\nprediction_ci.plot(\n    ax=ax, style=['--', '--'],\n    label='predicted/forecasted')\n\nci_index = prediction_ci.index\nlower_ci = prediction_ci.iloc[:, 0]\nupper_ci = prediction_ci.iloc[:, 1]\n\nax.fill_between(ci_index, lower_ci, upper_ci,\n                color='r', alpha= .1)\n\nax.set_xlabel('Time (years)')\nax.set_ylabel('Prices')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we know how to do time-series prediction! We are all set and ready for our competition."},{"metadata":{"id":"zWp4b7mrucTT"},"cell_type":"markdown","source":"## 4. Exploratory Data Analysis"},{"metadata":{"id":"0l7b-huic72z"},"cell_type":"markdown","source":"First, let's visit the dataset `sales_train.csv` that we talked about previously."},{"metadata":{"id":"UlxOdBR9_mTs","outputId":"02ffa91f-1fda-4b52-c963-c2f6314bede0","trusted":true},"cell_type":"code","source":"#formating dates as a date object\nsales.date = sales.date.apply(lambda x: datetime.datetime.strptime(x, \"%d.%m.%Y\"))\n# check\nprint(sales.info())","execution_count":null,"outputs":[]},{"metadata":{"id":"vkAYSFrKdQ_8"},"cell_type":"markdown","source":"So we have ~2M sales of items in the period we were given."},{"metadata":{"id":"hF3tDWmPdcHq"},"cell_type":"markdown","source":"How do they look like?"},{"metadata":{"id":"-0p7qj2WGKuV","trusted":true},"cell_type":"code","source":"sales_monthly = sales.groupby(\n    [\"date_block_num\", \"shop_id\", \"item_id\"])[\"date\",\"item_price\",\n                                              \"item_cnt_day\"].agg({\n        \"date\":[\"min\",\"max\"],\n        \"item_price\":\"mean\",\n        \"item_cnt_day\":\"sum\"})","execution_count":null,"outputs":[]},{"metadata":{"id":"acdeV0xcG-BF","outputId":"8a8e2117-381a-405f-b26b-690f6dc2a89d","trusted":true},"cell_type":"code","source":"sales_monthly.head(20)","execution_count":null,"outputs":[]},{"metadata":{"id":"yyL6V7EKH9XF","outputId":"12fcb0d8-117d-44c7-bc26-38ac992d764c","trusted":true},"cell_type":"code","source":"# number of items per cat \nitems.head()\nx = items.groupby(['item_category_id']).count() # but count is in column item_id ?\nx = x.sort_values(by='item_id',ascending=False)\nx=x.iloc[0:10].reset_index()\nx\n# plot\nplt.figure(figsize=(8,4))\nax=sns.barplot(x.item_category_id, x.item_id, alpha=0.8)\nplt.title(\"Items per Category\")\nplt.ylabel(\"# of items\", fontsize=12)\nplt.xlabel(\"Category\", fontsize=12)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"vUXG-NQ_diMp"},"cell_type":"markdown","source":"The sales by category seem to be unbalanced."},{"metadata":{"id":"oZSHM3nY3YHN"},"cell_type":"markdown","source":"First let's compute the total sales per month and plot that data.\n\n"},{"metadata":{"id":"nG7AGL0Zx2e_","trusted":true},"cell_type":"code","source":"ts = sales.groupby(['date_block_num'])['item_cnt_day'].sum()\n# ts = sales.groupby(['date_block_num','shop_id'])['item_cnt_day'].sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"OroIJ252_5kM","outputId":"8a23266d-28af-496a-ace6-e348ba5d5b45","trusted":true},"cell_type":"code","source":"out = sales.pivot_table(index='shop_id', \n                        columns='date_block_num',\n                        values='item_cnt_day',\n                        aggfunc='sum')\nout = out.fillna(out.mean())\nout.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"uhHQtslBzQlq","outputId":"0489ea7b-e18a-4e1d-a4f3-2d5770dddaeb","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.plot(ts)\nplt.title(\"Total sales of the company\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"# sales\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"j3yW7QFJODei","outputId":"ce830fe3-d27b-4590-f079-6c3ddb08f9a3","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nfor i,row in out.iterrows():\n  plt.scatter(out.columns, row)\nplt.title(\"Total sales of the company\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"# sales\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"lo4B4wZO3upo","outputId":"1775a1c9-0103-4f0e-8147-6484f43d08f9","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.plot(ts.rolling(window=12,center=False).mean(), label = \"rolling mean\")\nplt.plot(ts.rolling(window=12, center=False).std(), label = \"rolling std\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"1rN8c2xe5kZS"},"cell_type":"markdown","source":"There is clearly a seasonality and a trend.\n\nLet's check that with a quick decomposition into Trend, seasonality and residuals.\n\n"},{"metadata":{"id":"uo7A6PHo4-lJ","outputId":"c4dc3424-4965-4a07-93f2-a640fe7f01b1","trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nres = sm.tsa.seasonal_decompose(ts.values, freq=12, model=\"multiplicative\")\nfig=res.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"dN6i5VvF6N_X","outputId":"e9b07b74-d604-4321-c261-27c2768c931c","trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nres = sm.tsa.seasonal_decompose(ts.values, freq=12, model=\"addidtive\")\nfig=res.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"IWCxxt5kAYDs"},"cell_type":"markdown","source":"Now I need to predict at the (shop,item_level)"},{"metadata":{"id":"8VK8t-xOh9L7","outputId":"5b123721-decd-48f0-983f-47341545a6f0","trusted":true},"cell_type":"code","source":"import statsmodels.api as smt\nimport statsmodels\nimport scipy.stats as scs\nfrom pandas import Series\n\nts = sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\n\ndef difference(dataset, interval=1):\n  diff = list()\n  for i in range(interval, len(dataset)):\n    value = dataset[i] - dataset[i - interval]\n    diff.append(value)\n  return Series(diff)\n\nnew_ts = difference(ts, 12)\n\n\ndef tsplot(y, lags=None, figsize=(10, 8), style='bmh',title=''):\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        #mpl.rcParams['font.family'] = 'Ubuntu Mono'\n        layout = (3, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        qq_ax = plt.subplot2grid(layout, (2, 0))\n        pp_ax = plt.subplot2grid(layout, (2, 1))\n        \n        y.plot(ax=ts_ax)\n        ts_ax.set_title(title)\n        statsmodels.graphics.tsaplots.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n        statsmodels.graphics.tsaplots.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n        smt.qqplot(y, line='s', ax=qq_ax)\n        qq_ax.set_title('QQ Plot')        \n        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)\n\n        plt.tight_layout()\n    return \n\nmax_lag = 12\n_ = tsplot(ts.values, lags=max_lag,title=\"My De-trend and De-seasonalized values process\");","execution_count":null,"outputs":[]},{"metadata":{"id":"B6Fj5M3aE19d"},"cell_type":"markdown","source":"# ARMA"},{"metadata":{"id":"LbCfXUXFfdcZ"},"cell_type":"markdown","source":"We are now going to buil our model to predict the future sales for the company."},{"metadata":{"id":"ydC3U_j3fkVR"},"cell_type":"markdown","source":"We first need to create a multi-index dataframe with `(\"date_block_num\", \"shop_id\", \"item_id\")` as index in order to easily find the number of time an `item_id` was sold at month `date_block_num`."},{"metadata":{"id":"Mu_9xmwxwlii","trusted":true},"cell_type":"code","source":"sales_monthly = sales.groupby(\n    [\"date_block_num\", \"shop_id\", \"item_id\"])[\"date\", \"item_price\",\n                                              \"item_cnt_day\"].agg({\n    \"date\": [\"min\", \"max\"],\n    \"item_price\": \"mean\",\n    \"item_cnt_day\": \"sum\"})","execution_count":null,"outputs":[]},{"metadata":{"id":"O_SCcjOyf_Y2","outputId":"11e06b0b-93ac-43e2-fa74-b2556ee9a373","trusted":true},"cell_type":"code","source":"sales_monthly.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"_qR4Q3uLgkiF"},"cell_type":"markdown","source":"Each `\"shop_id\", \"item_id\"` is a time series for which we will find and create a SARIMAX model."},{"metadata":{"id":"Fmv_nZnrg1Tl"},"cell_type":"markdown","source":"We now find these couples iterating through `test` and creating the related SARIMAX model if we have enough data (at least 33 month of sales)"},{"metadata":{"id":"LfssG6vRCc1B","outputId":"3fb83045-89f5-4d7c-d2b7-229ee9546331","trusted":true},"cell_type":"code","source":"import more_itertools as mit\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n\narray = []\n\nfor i, row in test.iterrows():\n   \n    try:\n        # We get all the dates/indexes in order to fill the blanks of the time series with 0s later on \n        # We have a KeyError issue at row['shop_id']:  5  row['item_id']:  5003 which I don't have in my local notebook\n        existing_indexes = [x[0] \n                            for x in sales_monthly.loc[pd.IndexSlice[:, \n                            [row['shop_id']], [row['item_id']]], :].index]\n        # We multiply the price of the item by the number of this kind of item sold\n        ts = pd.DataFrame(sales_monthly.loc[pd.IndexSlice[:, # We have a key error here\n                      [row['shop_id']], [row['item_id']]], :]['item_price'].values *\n                      sales_monthly.loc[pd.IndexSlice[:, \n                      [row['shop_id']], [row['item_id']]], :]['item_cnt_day'].values).T.iloc[0]\n        ts_values = list(ts.values)\n        if ts.values != [] and len(ts.values) > 4:\n          # if this item isn't sold every month, we need to fill the gaps in the \n          # sellings list\n          if len(ts.values<3):\n            all_indexes = list(range(33))\n            insert_at_indexes = set(all_indexes) - set(existing_indexes)\n            insert_at_indexes = [list(group) \n                        for group in mit.consecutive_groups(insert_at_indexes)][1:]\n            insert_at_indexes = [item for sublist in insert_at_indexes for item in sublist]\n            # we only take the last one \n            for insert_at in insert_at_indexes:\n              ts_values[insert_at:insert_at] = [0.]\n          best_aic = np.inf\n          best_order = None\n          best_model = None\n\n          # we need to test different orders, but let's have a go with that ...\n          ranges = range(1, 5)\n          for difference in ranges:\n              tmp_model = SARIMAX(ts_values, order=(0, 1, 0), trend='t').fit()\n              tmp_aic = tmp_model.aic\n              if tmp_aic < best_aic:\n                  best_aic = tmp_aic\n                  best_difference = difference\n                  best_model = tmp_model\n          if best_model is not None:\n              y_hat = best_model.forecast()[0]\n              if y_hat < 0:\n                  y_hat = 0.5\n          else:\n              y_hat = 0.5\n        else:\n            y_hat = 0.5\n    except KeyError:\n        y_hat = 0.5\n    d = {'id': row['ID'], 'item_cnt_month': y_hat}\n    array.append(d)\n\ndf = pd.DataFrame(array)\ndf.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Allright ! We are all done, let's have a look at what we predicted."},{"metadata":{"id":"yffn4J_5_y5I","outputId":"37825298-83d2-41f3-a288-2b73b9cbf3d2","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"H5-dX-6cAxsF","trusted":true},"cell_type":"code","source":"df.to_csv(\"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"8ihetodbA3om","trusted":true},"cell_type":"code","source":" df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{"id":"9aR8iA6edICF"},"cell_type":"markdown","source":"And our score is ... 3556.23468 whereas the sample submission was 1.23646."},{"metadata":{"id":"ClbJ1TtfexV9"},"cell_type":"markdown","source":"As we are calculating the root mean square error, the the differences between values (sample or population values) predicted by a model or an estimator and the values observed, this is not very good. "},{"metadata":{"id":"W6zFT_5ve0aV"},"cell_type":"markdown","source":"We probably need to\n- get the right orders.\n- do further data cleaning:\n- Maybe the (shop,item) couples which don't generate any revenues anymore are not detected despite.\n- Maybe ARIMA isn't a good model for this case and we should rather use a RNN model?"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}