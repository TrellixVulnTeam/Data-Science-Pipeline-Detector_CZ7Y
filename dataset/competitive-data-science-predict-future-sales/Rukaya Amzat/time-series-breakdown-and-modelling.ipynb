{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-14T09:32:52.062259Z","iopub.execute_input":"2021-06-14T09:32:52.062686Z","iopub.status.idle":"2021-06-14T09:32:52.078419Z","shell.execute_reply.started":"2021-06-14T09:32:52.062596Z","shell.execute_reply":"2021-06-14T09:32:52.077549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n\nTime series data definition: Data collected on the same metrics or same objects at regular time intervals. It could be stock market records or sales records.","metadata":{}},{"cell_type":"markdown","source":"### Import Libraries","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pandas import Series\n\n\n# ignore warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\n\nfrom statsmodels.tsa.stattools import adfuller","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:52.079792Z","iopub.execute_input":"2021-06-14T09:32:52.080189Z","iopub.status.idle":"2021-06-14T09:32:53.066113Z","shell.execute_reply.started":"2021-06-14T09:32:52.080159Z","shell.execute_reply":"2021-06-14T09:32:53.065076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data path\npath = '/kaggle/input/competitive-data-science-predict-future-sales/'\n\n#Load\ntrain = pd.read_csv(path + 'sales_train.csv')\ntest = pd.read_csv(path + 'test.csv')\nsample_submission = pd.read_csv(path + 'sample_submission.csv')\nitems = pd.read_csv(path + 'items.csv')\nitem_categories = pd.read_csv(path + 'item_categories.csv')\nshops = pd.read_csv(path + 'shops.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:53.068111Z","iopub.execute_input":"2021-06-14T09:32:53.068477Z","iopub.status.idle":"2021-06-14T09:32:55.994537Z","shell.execute_reply.started":"2021-06-14T09:32:53.068446Z","shell.execute_reply":"2021-06-14T09:32:55.993666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inspect the data","metadata":{}},{"cell_type":"code","source":"#check the data\nprint(\"************** TRAIN **************\")\nprint(train.describe())\nprint(train.head())","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:55.995807Z","iopub.execute_input":"2021-06-14T09:32:55.996088Z","iopub.status.idle":"2021-06-14T09:32:56.536139Z","shell.execute_reply.started":"2021-06-14T09:32:55.996061Z","shell.execute_reply":"2021-06-14T09:32:56.53486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look at the test data\nprint(\"************** TEST**************\")\nprint(test.describe())\nprint(test.head())","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:56.537771Z","iopub.execute_input":"2021-06-14T09:32:56.538192Z","iopub.status.idle":"2021-06-14T09:32:56.575161Z","shell.execute_reply.started":"2021-06-14T09:32:56.538149Z","shell.execute_reply":"2021-06-14T09:32:56.574216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data fields\nID - an Id that represents a (Shop, Item) tuple within the test set\n\nshop_id - unique identifier of a shop\n\nitem_id - unique identifier of a product\n\nitem_category_id - unique identifier of item category\n\nitem_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n\nitem_price - current price of an item\n\ndate - date in format dd/mm/yyyy\n\ndate_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n\nitem_name - name of item\n\nshop_name - name of shop\n\nitem_category_name - name of item category","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:56.576345Z","iopub.execute_input":"2021-06-14T09:32:56.576627Z","iopub.status.idle":"2021-06-14T09:32:56.869118Z","shell.execute_reply.started":"2021-06-14T09:32:56.576599Z","shell.execute_reply":"2021-06-14T09:32:56.868158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:56.871147Z","iopub.execute_input":"2021-06-14T09:32:56.871439Z","iopub.status.idle":"2021-06-14T09:32:56.88462Z","shell.execute_reply.started":"2021-06-14T09:32:56.871412Z","shell.execute_reply":"2021-06-14T09:32:56.883573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Simplify Time Series Data\nWe will start by simplifying the input data a bit to explore data types. To do so, we will look at item price and item cnt by Date . This allows us to look a Time Series dataset with multiple time series. ","metadata":{}},{"cell_type":"code","source":"#grouping sales per day\nbase = train.groupby(['date'])['item_cnt_day'].sum().reset_index()\nbase.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:56.886791Z","iopub.execute_input":"2021-06-14T09:32:56.887222Z","iopub.status.idle":"2021-06-14T09:32:57.187728Z","shell.execute_reply.started":"2021-06-14T09:32:56.887178Z","shell.execute_reply":"2021-06-14T09:32:57.186803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#grouping sales per month\nbase_m = train.groupby(['date_block_num'])['item_cnt_day'].sum().reset_index()\nbase_m.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:57.189104Z","iopub.execute_input":"2021-06-14T09:32:57.18939Z","iopub.status.idle":"2021-06-14T09:32:57.251621Z","shell.execute_reply.started":"2021-06-14T09:32:57.18936Z","shell.execute_reply":"2021-06-14T09:32:57.250888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#grouping sales per shop\nbase_s = train.groupby(['shop_id'])['item_cnt_day'].sum().reset_index()\nbase_s.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:57.25269Z","iopub.execute_input":"2021-06-14T09:32:57.253095Z","iopub.status.idle":"2021-06-14T09:32:57.314473Z","shell.execute_reply.started":"2021-06-14T09:32:57.253052Z","shell.execute_reply":"2021-06-14T09:32:57.313443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating rolling averages for the base grouped data\nb_rolling_mean = base.rolling(window = 12).mean() # rolling average \nb_rolling_std = base.rolling(window = 12).std() # rolling std \n\n#creating rolling averages for the month base grouped data\nm_rolling_mean = base_m.rolling(window = 12).mean() # rolling average of 12 months\nm_rolling_std = base_m.rolling(window = 12).std() # rolling std of 12 months\n\n#creating rolling averages for the shoped grouped sales\ns_rolling_mean = base_s.rolling(window = 12).mean() # rolling average \ns_rolling_std = base_s.rolling(window = 12).std() # rolling std \n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:57.315775Z","iopub.execute_input":"2021-06-14T09:32:57.316142Z","iopub.status.idle":"2021-06-14T09:32:57.328971Z","shell.execute_reply.started":"2021-06-14T09:32:57.316112Z","shell.execute_reply":"2021-06-14T09:32:57.328048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualizing sales per day\nbase.plot(color = 'blue', label = 'Sales',figsize=(16,8), title= 'Sales per day', xlabel='date', ylabel='Items sold')\nplt.plot(b_rolling_mean, color = 'red', label = 'Rolling Mean')\nplt.plot(b_rolling_std, color = 'black', label = 'Rolling Std')\nplt.legend(loc = 'best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:57.330692Z","iopub.execute_input":"2021-06-14T09:32:57.331007Z","iopub.status.idle":"2021-06-14T09:32:57.623373Z","shell.execute_reply.started":"2021-06-14T09:32:57.33098Z","shell.execute_reply":"2021-06-14T09:32:57.622026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualizing sales per month count\nplt.figure(figsize=(16,8))\nplt.title('Total Sales per month')\nplt.xlabel('Month')\nplt.ylabel('Units Sold')\nplt.plot(base_m, color = 'blue', label = 'Sales')\nplt.plot(m_rolling_mean, color = 'red', label = 'Rolling Mean')\nplt.plot(m_rolling_std, color = 'black', label = 'Rolling Std')\nplt.legend(loc = 'best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:57.624741Z","iopub.execute_input":"2021-06-14T09:32:57.625148Z","iopub.status.idle":"2021-06-14T09:32:57.857564Z","shell.execute_reply.started":"2021-06-14T09:32:57.625106Z","shell.execute_reply":"2021-06-14T09:32:57.856589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualizing sales per shop\nplt.figure(figsize=(16,8))\nplt.title('Sales per shop')\nplt.xlabel('shop id')\nplt.ylabel('Items sold')\nplt.plot(base_s, color = 'blue', label = 'Sales')\nplt.plot(s_rolling_mean, color = 'red', label = 'Rolling Mean')\nplt.plot(s_rolling_std, color = 'black', label = 'Rolling Std')\nplt.legend(loc = 'best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:57.858719Z","iopub.execute_input":"2021-06-14T09:32:57.859005Z","iopub.status.idle":"2021-06-14T09:32:58.086591Z","shell.execute_reply.started":"2021-06-14T09:32:57.858977Z","shell.execute_reply":"2021-06-14T09:32:58.085859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Time Series Visualizations\nThere are a number of packages to help analyze Time Series data and create relevant plots. One example is __[statsmodels](https://www.statsmodels.org/stable/graphics.html#time-series-plots)__, which includes a number of methods for plotting Time Series-specific visualizations:\n- __[plot_acf](https://www.statsmodels.org/stable/generated/statsmodels.graphics.tsaplots.plot_acf.html#statsmodels.graphics.tsaplots.plot_acf)__: Plot of the Autocorrelation Function\n- __[plot_pacf](https://www.statsmodels.org/stable/generated/statsmodels.graphics.tsaplots.plot_pacf.html#statsmodels.graphics.tsaplots.plot_pacf)__: Plot of the Partial Autocorrelation Function\n- __[month_plot](https://www.statsmodels.org/stable/generated/statsmodels.graphics.tsaplots.month_plot.html#statsmodels.graphics.tsaplots.month_plot)__: Seasonal Plot for Monthly Data\n- __[quarter_plot](https://www.statsmodels.org/stable/generated/statsmodels.graphics.tsaplots.quarter_plot.html#statsmodels.graphics.tsaplots.quarter_plot)__: Seasonal Plot for Quarterly Data","metadata":{}},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nprint('Monthly data Autocorrelation Plots')\n# Autocorrelation and Partial Autocorrelation Functions for Daily Data\n\nacf_plot = plot_acf(base_m['item_cnt_day'], lags= 15, title='Autocorrelation in monthly Sales Data')\n\npacf_plot = plot_pacf(base_m['item_cnt_day'], lags= 15, title='Partial Autocorrelation in monthly Sales Data')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:58.08772Z","iopub.execute_input":"2021-06-14T09:32:58.088102Z","iopub.status.idle":"2021-06-14T09:32:58.59395Z","shell.execute_reply.started":"2021-06-14T09:32:58.088069Z","shell.execute_reply":"2021-06-14T09:32:58.592969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_new = base.copy()\nbase_new.index = base.date\nbase_new.index = pd.to_datetime(base_new.index)\nbase_new= base_new.drop(['date'], axis = 1)\nbase_new","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:58.595161Z","iopub.execute_input":"2021-06-14T09:32:58.595461Z","iopub.status.idle":"2021-06-14T09:32:58.61337Z","shell.execute_reply.started":"2021-06-14T09:32:58.595431Z","shell.execute_reply":"2021-06-14T09:32:58.612442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stationary Time Series\nIn order for time series data to be stationary, the data must exhibit four properties over time:\n1. constant mean\n2. constant variance\n3. constant autocorrelation structure\n4. no periodic component\n\nMean, variance, and periodic component (aka seasonality) should be familiar to you. Autocorrelation may not be. Autocorrelation simply means that the current time series measurement is correlated with a past measurement. For example, today's stock price is often highly correlated with yesterday's price.","metadata":{}},{"cell_type":"markdown","source":"Perhaps the easiest way to check for constant mean and variance is to chop up the data into separate chunks, calculate statistics for each chunk, and compare. It's not the most rigorous method but it gives you a good sense of whether your data is approximately stationary.","metadata":{}},{"cell_type":"code","source":"# split data into 22 equal chunks\nchunks = np.split(base_new, indices_or_sections= 22)\nprint(''''''''''mean of each chunk''''''''')\nprint(np.mean(chunks, axis = 1))\n\nprint('''''''''''variance of each chunk''''''''')\nprint(np.var(chunks, axis = 1))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:58.614521Z","iopub.execute_input":"2021-06-14T09:32:58.614814Z","iopub.status.idle":"2021-06-14T09:32:58.634322Z","shell.execute_reply.started":"2021-06-14T09:32:58.614785Z","shell.execute_reply":"2021-06-14T09:32:58.63326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the mean of each chunk is close to being constant while the variance chnages significantly, hence we can say the data is NON Stationary.\n\n\n> Note: We do expect some fluctuation in values. It's highly unlikely that the either the mean or variace will be exactly the same from chunk to chunk, but it should be close. \n\nIf you wanted to get even more sophisticated, you could run a statistical test to determine if the difference in means or the difference in variances is statistically significant.","metadata":{}},{"cell_type":"markdown","source":"### Augmented Dickey-Fuller Test\nThis is a statistical procedure to suss out whether a time series is stationary or not. We won't go into all the nitty gritty details but here's what you need to know:\n1. **Null hypothesis:** the series is nonstationary.\n2. **Alternative hypothesis:** the series is stationary.\n\nLike any statistical test you should set a significance level or threshold that determines whether you should accept or reject the null. \n> The value 0.05 is common but depends upons numerous factors.\n\n\n#### Stationary Data & ADF","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\na = train.groupby(['date_block_num'])['item_cnt_day'].sum()\na.astype(float)\n\nadf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(a, regression='c')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:58.635665Z","iopub.execute_input":"2021-06-14T09:32:58.635996Z","iopub.status.idle":"2021-06-14T09:32:58.714778Z","shell.execute_reply.started":"2021-06-14T09:32:58.635967Z","shell.execute_reply":"2021-06-14T09:32:58.713519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('''''''adf value''''''')\nprint(adf)\n\nprint('''''''p value''''''')\nprint(pvalue)\n\nprint('''''''nobs''''''')\nprint(nobs)\n\nprint('''''''critical value''''''')\nprint(critical_values)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:58.71947Z","iopub.execute_input":"2021-06-14T09:32:58.719759Z","iopub.status.idle":"2021-06-14T09:32:58.727434Z","shell.execute_reply.started":"2021-06-14T09:32:58.719731Z","shell.execute_reply":"2021-06-14T09:32:58.726223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, **adf** is the value of the test statistic. The more negative the value, the more confident we can be that the series is stationary. Here we see a value of -2.39. That may not mean anything to you just yet but the **pvalue** should\n\nNext, **pvalue** is interpreted like any p-value. Once we set a threshold, we can compare this p-value to that threshold. Either we reject or fail to reject the null. Here **pvalue** is 0.14 which is greater than the threshold so we fail to reject the null that this data is nonstationary. Hence, data is non stationary.\n\nThe variable **nobs** is simply the number of observations in the time series, in this case 33.\n\nFinally, the **critical_values** variable provides test statistic threholds for common significant levels. .","metadata":{}},{"cell_type":"markdown","source":" ## Common Nonstationary-to-Stationary Transformations\n\n###  Remove Changing Variance w/Log Transformation\nThis trick works well when you're dealing with heteroscedastic data. Let's plot that again to remind you what that looks like.","metadata":{}},{"cell_type":"markdown","source":"#### Log Transformation\n\nWe can apply a log transformation. However, we cannot take the log of nonpositive values. The way we can get around this is by adding a constant to all values to make them positive. ","metadata":{}},{"cell_type":"code","source":"#create the log of the monthly sales\nlog_new = np.log(a)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:45:04.906229Z","iopub.execute_input":"2021-06-14T09:45:04.90661Z","iopub.status.idle":"2021-06-14T09:45:04.911036Z","shell.execute_reply.started":"2021-06-14T09:45:04.90656Z","shell.execute_reply":"2021-06-14T09:45:04.909904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_sequence_plot(x, y, title, xlabel=\"time\", ylabel=\"sales\"):\n    plt.plot(x, y, 'k-')\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.grid(alpha=0.3);\n    \nrun_sequence_plot(a.index, log_new, title = 'log transformed data')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:45:06.075368Z","iopub.execute_input":"2021-06-14T09:45:06.075763Z","iopub.status.idle":"2021-06-14T09:45:06.353673Z","shell.execute_reply.started":"2021-06-14T09:45:06.075734Z","shell.execute_reply":"2021-06-14T09:45:06.352816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#carry out the stationarity test on the log transformed data\n\nadf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(log_new, regression='c')\n\nprint('''''''adf value''''''')\nprint(adf)\n\nprint('''''''p value''''''')\nprint(pvalue)\n\nprint('''''''nobs''''''')\nprint(nobs)\n\nprint('''''''critical value''''''')\nprint(critical_values)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:45:07.94841Z","iopub.execute_input":"2021-06-14T09:45:07.948762Z","iopub.status.idle":"2021-06-14T09:45:07.96407Z","shell.execute_reply.started":"2021-06-14T09:45:07.948734Z","shell.execute_reply":"2021-06-14T09:45:07.962899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The p value is still above the threshold. Hence the log transformation didnt work to remove the non stationarity on this data. We can then try differencing.\n\nTurns out we can transform this series into stationary by applying what's called a differece. It's a fancy term that simply means you're going to subtract a past value from a current value. An example will make this clear.\n\nWe know *lagged* was created with a lag of one. So let's subtract $O_{t-1}$ from $O_{t}$ where $O_{t}$ is the observed data at time *t* and $O_{t-1}$ is the observed data at *t-1*.","metadata":{}},{"cell_type":"code","source":"def difference(data, interval=1):\n    diff = [] # Create empty list\n    for i in range(interval, len(data)): # Iterate over every lag\n        val = data[i] - data[i - interval] # Take the difference between consective terms\n        diff.append(val) # Add the new values to the end of the list\n    return Series(diff) # Return the differenced values as a time series","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:45:13.215216Z","iopub.execute_input":"2021-06-14T09:45:13.215703Z","iopub.status.idle":"2021-06-14T09:45:13.220891Z","shell.execute_reply.started":"2021-06-14T09:45:13.215671Z","shell.execute_reply":"2021-06-14T09:45:13.219735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_a = difference(a) # difference the time series\n\n#plot original data\nplt.figure(figsize=(16,16))\nplt.subplot(211)\nplt.title('Original')\nplt.xlabel('Month')\nplt.ylabel('Units Sold')\nplt.plot(a) \n\n# plot the differenced data\nplt.subplot(212)\nplt.title('differenced data')\nplt.xlabel('Month')\nplt.ylabel('Units Sold')\nplt.plot(new_a)\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:45:14.478212Z","iopub.execute_input":"2021-06-14T09:45:14.478571Z","iopub.status.idle":"2021-06-14T09:45:14.799749Z","shell.execute_reply.started":"2021-06-14T09:45:14.478539Z","shell.execute_reply":"2021-06-14T09:45:14.798816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#carry out the stationarity test on the differenced data\n\nadf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(new_a, regression='c')\n\nprint('''''''adf value''''''')\nprint(adf)\n\nprint('''''''p value''''''')\nprint(pvalue)\n\nprint('''''''nobs''''''')\nprint(nobs)\n\nprint('''''''critical value''''''')\nprint(critical_values)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:45:28.666084Z","iopub.execute_input":"2021-06-14T09:45:28.666427Z","iopub.status.idle":"2021-06-14T09:45:28.684169Z","shell.execute_reply.started":"2021-06-14T09:45:28.666399Z","shell.execute_reply":"2021-06-14T09:45:28.682949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the adf value is quite negative and the p value is very insignificant(0), hence we reject the null and differencing has transformed our data into STATIONARY","metadata":{}},{"cell_type":"code","source":"plot_acf(difference(difference(a)));\nplt.title('Differencing ACF')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:59.271163Z","iopub.execute_input":"2021-06-14T09:32:59.271454Z","iopub.status.idle":"2021-06-14T09:32:59.441801Z","shell.execute_reply.started":"2021-06-14T09:32:59.271425Z","shell.execute_reply":"2021-06-14T09:32:59.440881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is Smooothing?\nAny data collection process is subject to noise. Oftentimes this noise can obscure useful patterns. Smoothing is a well-known and oft used technique to extract those patterns. \n\nSmoothing comes in two flavors:\n1. Simple \n2. Exponential \n\n\n###  Exponential Smoothing\nExponential smoothing is a way to weight observations differently. Specifically, recent observations are weighted moreso than more distant ones.\n\nThere are three key exponential smoothing techniques you need to be aware of:\n1. Single Exponential Smoothing - no trend or seasonality\n2. Double Exponential Smoothing - captures trend\n3. Triple Exponential Smoothing - captures trend & seasonality\n\n#### Single Exponential\nThis method is useful if your data lacks trend and seasonality and you want to approximately track patterns in your data. Furthermore, this method removes the lag associated with the moving average techniques discussed above. \n\n#### Double Exponential \nShould your data exhibit a trend, you'll want to use this smoothing method. It has all the benefits of Single Exponential with the ability to pickup on trend. \n\n#### Triple Exponential\nShould your data exhibit trend and seasonality, you'll want to use this smoothing method. It has all the benefits of Double Exponential with the ability to pickup on seasonality. ","metadata":{}},{"cell_type":"code","source":"#split the data\n#define test size\ntest_size = 10\n# train sets\ntrain_1 = new_a[:-test_size]\ntest_1 = new_a[-test_size:]","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:59.443134Z","iopub.execute_input":"2021-06-14T09:32:59.443502Z","iopub.status.idle":"2021-06-14T09:32:59.448041Z","shell.execute_reply.started":"2021-06-14T09:32:59.44347Z","shell.execute_reply":"2021-06-14T09:32:59.447025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.api import ExponentialSmoothing\n\nmodel_1 = ExponentialSmoothing(train_1,\n                              trend= None,\n                              seasonal=None,\n                              seasonal_periods= None).fit(optimized=True)\n\npreds_1 = model_1.forecast(len(test_1))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:59.449539Z","iopub.execute_input":"2021-06-14T09:32:59.449959Z","iopub.status.idle":"2021-06-14T09:32:59.693707Z","shell.execute_reply.started":"2021-06-14T09:32:59.449918Z","shell.execute_reply":"2021-06-14T09:32:59.692978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(train_1.index, train_1, 'b--', label=\"train\")\nplt.plot(test_1.index, test_1, color='orange', linestyle=\"--\", label=\"test\")\nplt.plot(test_1.index, preds_1, 'r--', label=\"predictions\")\nplt.legend(loc='upper left')\nplt.title(\"Triple Exponential Smoothing\")\nplt.grid(alpha=0.3);","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:59.694565Z","iopub.execute_input":"2021-06-14T09:32:59.694808Z","iopub.status.idle":"2021-06-14T09:32:59.8665Z","shell.execute_reply.started":"2021-06-14T09:32:59.694783Z","shell.execute_reply":"2021-06-14T09:32:59.865514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we are determining our ARIMA model we will come across the following standard inputs:\n- order(p,d,q):\n    - p is number of AR terms\n    - d is number of times that we would difference our data\n    - q is number of MA terms\n    \nWhen we work with SARIMA models 'S' refers to 'seasonal' and we have the additional standard inputs:\n- seasonal order(p,d,q):\n    - p is number of AR terms in regards to seasonal lag\n    - d is number of times that we would difference our seasonal lag (as seen above)\n    - q is number of MA terms in regards to seasonal lag\n    - s is number of periods in a season","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sm\nimport warnings\n\nrng = range(5)\nbest_aic = np.inf\nbest_model = None\nbest_order = None\n\nwarnings.filterwarnings('ignore')\n\nfor i in rng:\n    for j in rng:\n        temp_model = sm.tsa.statespace.SARIMAX(train_1, order = (i, 0, j))\n        results = temp_model.fit()\n        temp_aic = results.aic\n        if temp_aic < best_aic:\n            best_aic = temp_aic\n            best_order = (i, 0, j)\n            best_model = temp_model\n\nprint('Best AIC: %s | Best order: %s' % (best_aic, best_order))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:32:59.867671Z","iopub.execute_input":"2021-06-14T09:32:59.867986Z","iopub.status.idle":"2021-06-14T09:33:04.039545Z","shell.execute_reply.started":"2021-06-14T09:32:59.867958Z","shell.execute_reply":"2021-06-14T09:33:04.03838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# fit SARIMA monthly based on helper plots\nsar = SARIMAX(train_1, order=(0,0,1), \n                seasonal_order=(0,1,0,12), \n                trend='t').fit() \n                               \nsar.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:33:04.044777Z","iopub.execute_input":"2021-06-14T09:33:04.047481Z","iopub.status.idle":"2021-06-14T09:33:04.172447Z","shell.execute_reply.started":"2021-06-14T09:33:04.04742Z","shell.execute_reply":"2021-06-14T09:33:04.171385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sar_preds = sar.forecast(len(test_1)) #forecst on the kept test data\n\n#plot the graphs to visualize the performance\nplt.plot(train_1.index, train_1, 'b--', label=\"train\")\nplt.plot(test_1.index, test_1, color='orange', linestyle=\"--\", label=\"test\")\nplt.plot(test_1.index, sar_preds, 'r--', label=\"predictions\")\nplt.legend(loc='upper left')\nplt.title(\"SARIMA modelling\")\nplt.grid(alpha=0.3);","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:33:04.175567Z","iopub.execute_input":"2021-06-14T09:33:04.175901Z","iopub.status.idle":"2021-06-14T09:33:04.358386Z","shell.execute_reply.started":"2021-06-14T09:33:04.175868Z","shell.execute_reply":"2021-06-14T09:33:04.357366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the predictions made were quite close to the original test data.","metadata":{}},{"cell_type":"code","source":"#lets use the model to forecast into the future (24 months)\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAXResults\n\nsarima = sm.tsa.statespace.SARIMAX(new_a, order = (0,0,1),trend = 't', seasonal_order=(0,1,0,12))\nresult = sarima.fit()\npreds = SARIMAXResults.predict(result, start = 33, end = 46)\n\n\n#plot to visualize\nax = new_a.plot(label = 'Observed')\npreds.plot(ax = ax, label = 'SARIMA_forecast')\nplt.legend()\nplt.title('Sales forecast')\nax.set_xlabel('Month')\nax.set_ylabel('Units Sold')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:46:33.256319Z","iopub.execute_input":"2021-06-14T09:46:33.256966Z","iopub.status.idle":"2021-06-14T09:46:33.527094Z","shell.execute_reply.started":"2021-06-14T09:46:33.256928Z","shell.execute_reply":"2021-06-14T09:46:33.526201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prophet Model\n\nThe models revolves around two main observations in the practice of creating a variety of business forecasts:\n- Completely automatic forecasting techniques can be brittle and they are often too inflexible to incorporate useful assumptions or heuristics.\n- Analysts who can produce high quality forecasts are quite rare because forecasting is a specialized data science skill requiring substantial experience.\n\nProphet is an general additive model that includes a number of highly advanced, intelligent [forecasting methods]\n\n\n- For trend, a piecewise linear or logistic growth curve trend is used. \n    - Prophet automatically detects changes in trends by selecting changepoints from the data.\n- For seasonalities, different seasonality components are modeled using Fourier series.\n- One can either use fb provided list or incorporate their own holidays into model.\n","metadata":{}},{"cell_type":"code","source":"# prophet model requires that we reset the date as a column instead of being an index\nbase_n = base_new.reset_index()\nbase_n.head()\nprint(base_n.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:42:04.793501Z","iopub.execute_input":"2021-06-14T09:42:04.793918Z","iopub.status.idle":"2021-06-14T09:42:04.800468Z","shell.execute_reply.started":"2021-06-14T09:42:04.793877Z","shell.execute_reply":"2021-06-14T09:42:04.799461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fbprophet import Prophet # Import the package\n\n# Prophet requires you to name your columns the following:\nbase_n.columns = ['ds','y']\nprophet_model = Prophet(yearly_seasonality = True) # As determined in stationarity testing\nprophet_model.fit(base_n)\n\n# We'll predict 12 months into the future\n# 'MS' = month start\nfuture = prophet_model.make_future_dataframe(periods = 12, freq = 'MS')\nforecast = prophet_model.predict(future)\nforecast.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:46:45.165506Z","iopub.execute_input":"2021-06-14T09:46:45.165928Z","iopub.status.idle":"2021-06-14T09:46:49.472466Z","shell.execute_reply.started":"2021-06-14T09:46:45.165889Z","shell.execute_reply":"2021-06-14T09:46:49.471723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot forecast\nprophet_model.plot(forecast);","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:46:57.456201Z","iopub.execute_input":"2021-06-14T09:46:57.456723Z","iopub.status.idle":"2021-06-14T09:46:57.732957Z","shell.execute_reply.started":"2021-06-14T09:46:57.456673Z","shell.execute_reply":"2021-06-14T09:46:57.732022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot individual components of forecast: trend, weekly/yearly seasonality,\nprophet_model.plot_components(forecast);","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:46:59.588743Z","iopub.execute_input":"2021-06-14T09:46:59.589265Z","iopub.status.idle":"2021-06-14T09:47:00.277648Z","shell.execute_reply.started":"2021-06-14T09:46:59.589233Z","shell.execute_reply":"2021-06-14T09:47:00.276949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = new_a.plot(label = 'Observed')\npreds.plot(ax = ax, label = 'SARIMA forecast', alpha = 0.9, linestyle = '-')\nforecast.yhat[33:46].plot(ax = ax, label = 'Prophet forecast', alpha = 0.9, linestyle = '-')\n\nplt.legend()\nplt.title('Sales')\nax.set_xlabel('Month')\nax.set_ylabel('Units Sold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:47:53.845309Z","iopub.execute_input":"2021-06-14T09:47:53.845894Z","iopub.status.idle":"2021-06-14T09:47:54.05064Z","shell.execute_reply.started":"2021-06-14T09:47:53.845821Z","shell.execute_reply":"2021-06-14T09:47:54.049961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems the SARIMA MODEL did a better job at generalising.","metadata":{}}]}