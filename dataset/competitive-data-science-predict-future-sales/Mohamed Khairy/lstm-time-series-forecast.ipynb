{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom dateutil.relativedelta import relativedelta\nfrom IPython.display import display\nimport os\nfrom datetime import datetime\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN, LSTM, Activation, Dropout\nimport math\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport random\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.preprocessing.sequence import TimeseriesGenerator","metadata":{"execution":{"iopub.status.busy":"2022-04-10T03:28:42.71593Z","iopub.execute_input":"2022-04-10T03:28:42.716305Z","iopub.status.idle":"2022-04-10T03:28:50.297598Z","shell.execute_reply.started":"2022-04-10T03:28:42.716265Z","shell.execute_reply":"2022-04-10T03:28:50.29689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\ntrain = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nsample_sub = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T03:34:40.148319Z","iopub.execute_input":"2022-04-10T03:34:40.148652Z","iopub.status.idle":"2022-04-10T03:34:42.207951Z","shell.execute_reply.started":"2022-04-10T03:34:40.148616Z","shell.execute_reply":"2022-04-10T03:34:42.206922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T03:34:53.13763Z","iopub.execute_input":"2022-04-10T03:34:53.138075Z","iopub.status.idle":"2022-04-10T03:34:53.147997Z","shell.execute_reply.started":"2022-04-10T03:34:53.138041Z","shell.execute_reply":"2022-04-10T03:34:53.146979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T03:35:03.111224Z","iopub.execute_input":"2022-04-10T03:35:03.111775Z","iopub.status.idle":"2022-04-10T03:35:03.122149Z","shell.execute_reply.started":"2022-04-10T03:35:03.111713Z","shell.execute_reply":"2022-04-10T03:35:03.121224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T03:28:54.902374Z","iopub.execute_input":"2022-04-10T03:28:54.90325Z","iopub.status.idle":"2022-04-10T03:28:54.914303Z","shell.execute_reply.started":"2022-04-10T03:28:54.903191Z","shell.execute_reply":"2022-04-10T03:28:54.913598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub = sample_sub.drop('item_cnt_month',axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T03:28:55.613692Z","iopub.execute_input":"2022-04-10T03:28:55.613992Z","iopub.status.idle":"2022-04-10T03:28:55.625003Z","shell.execute_reply.started":"2022-04-10T03:28:55.613956Z","shell.execute_reply":"2022-04-10T03:28:55.623978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T03:28:59.130773Z","iopub.execute_input":"2022-04-10T03:28:59.131055Z","iopub.status.idle":"2022-04-10T03:28:59.141165Z","shell.execute_reply.started":"2022-04-10T03:28:59.131028Z","shell.execute_reply":"2022-04-10T03:28:59.140561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T03:32:55.989961Z","iopub.execute_input":"2022-04-10T03:32:55.990475Z","iopub.status.idle":"2022-04-10T03:32:56.001224Z","shell.execute_reply.started":"2022-04-10T03:32:55.990427Z","shell.execute_reply":"2022-04-10T03:32:56.000337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = train.merge(items[['item_category_id', 'item_id']], how='left', on='item_id')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T03:35:21.429759Z","iopub.execute_input":"2022-04-10T03:35:21.430266Z","iopub.status.idle":"2022-04-10T03:35:21.84954Z","shell.execute_reply.started":"2022-04-10T03:35:21.43022Z","shell.execute_reply":"2022-04-10T03:35:21.848707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finding the top sellers shops.\nplt.figure(figsize=(19,8))\nsns.countplot(data['shop_id'],palette='viridis')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lst = [6, 25, 31, 42, 54, 57, 27, 28] # the top sellers shop names\nshops[shops['shop_id'].isin([6, 25, 31, 42, 54, 57, 27, 28])]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items_count = pd.DataFrame(data['item_id'].value_counts()) # finding the top sellers items","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_items = items_count[items_count['item_id']>5000]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(19,8)) \nsns.barplot(x=top_items.index, y=\"item_id\", data=top_items, palette = \"Blues\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(top_items.index)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items[items['item_id'].isin([20949,  5822, 17717,  2808,  4181,  7856,  3732,  2308,  4870,\n        3734,  1855,  5821, 16787,  6675,  7894,  2445])] # top items name","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items_category_count = pd.DataFrame(data['item_category_id'].value_counts()) # finding the top sellers items category","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_items_category = items_category_count[items_category_count['item_category_id']>15000]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(19,8)) \nsns.barplot(x=top_items_category.index, y=\"item_category_id\", data=top_items_category, palette = \"Blues\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(top_items_category.index)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat[cat['item_category_id'].isin([40, 30, 55, 19, 37, 23, 28, 20, 63, 65, 72, 38, 75, 67, 64, 70, 41,\n       57, 21, 71, 69, 43, 62,  3, 22, 49, 35, 31, 25,  6,  2])] # top items category name","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Data cleaning, outliers , time series trend &cyclical","metadata":{}},{"cell_type":"code","source":"# reviewing the outlier of items prices\nax = sns.boxplot(data['item_price']) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hence will set the maximum for the item price 100,000, as 300,000 cannot even be due to plausible anomalies.\ndata = data[data['item_price']<100000]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reviewing the outlier of the number of products sold\nax = sns.boxplot(data['item_cnt_day']) \n# we can see that we have some negative values which has no meaning, the  other outliers can be due to plausible anomalies,as i am going to use LSTM it's not necessary to remove it.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data['item_cnt_day']>700]\n# checked if that day with high sales is kind of cyclical event every year.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing the negative numbers and zero values in number of products sold.\ndata[data['item_cnt_day']<1].count()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data[data['item_cnt_day']>0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe().T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# price had -1 value \n\ndata = data[data['item_price']>0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation between features\nplt.figure(figsize=(10,10))\n\nsns.heatmap(data.corr(),cmap='viridis',annot=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = data.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.pivot_table('item_cnt_day', index='date_block_num', columns='item_category_id', aggfunc='sum').plot(figsize=(19,8))\nplt.legend(title=\"item_category_id\", fontsize=10, title_fontsize=15, loc=(1.01, 0.01), ncol=3)\nplt.ylabel('Total category per date block');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.pivot_table('item_cnt_day', index='date_block_num', columns='item_category_id', aggfunc='max').plot(figsize=(19,8))\nplt.legend(title=\"item_category_id\", fontsize=10, title_fontsize=15, loc=(1.01, 0.01), ncol=3)\nplt.ylabel('Total category per date block');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cnt_grouped = df.groupby('date_block_num')['item_cnt_day'].sum()\ndf_cnt = pd.DataFrame(df_cnt_grouped)\nplt.figure(figsize = (15,5))\ndf_cnt['item_cnt_day'].plot() # sales trend going down over the time\n# block 11, 23 is December last 2 years was high season","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# as per Kaggle Data Description the training set. Daily historical data from January 2013 to October 2015.\n# lets check\nprint(df['date'].max())\nprint(df['date'].min())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fixing the same format\ndf['date'] = [datetime.strptime(i, \"%d.%m.%Y\") for i in df['date']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['date'].max())\nprint(df['date'].min())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Downsize the dataset","metadata":{}},{"cell_type":"code","source":"def downsizing(old_data, item_category_col, item_col, date_block_col, date_col, shop_col, sales_col, new_sales_avg):\n    \n    \"\"\"function will adjust the data by item_category and getting the average of the sales,\n       depending on the total number inside each category has been sold in the same date and same shop\"\"\"\n    def agg_d(k):\n        return k[0]\n    # for downsizing the data will create dictionary for item categories.\n    di_item_cat = old_data.groupby(item_category_col)[item_col].apply(lambda g: g.values.tolist()).to_dict()\n    \n    # remove the duplicted values in the dictionary.\n    item_category_dict = {a:list(set(b)) for a, b in di_item_cat.items()}\n    \n    # group by items categories, and shop ID.\n    df_adj = df.groupby([date_block_col, shop_col, item_category_col]).agg({sales_col:np.sum, item_col: 'count', date_col:lambda x : agg_d(list(x))})\n    df_adj[new_sales_avg] = df_adj[sales_col] / df_adj[item_col]\n    df_adj = df_adj.reset_index()\n    return df_adj","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_adj = downsizing(df, 'item_category_id', 'item_id', 'date_block_num', 'date', 'shop_id', 'item_cnt_day', 'item_cnt_day_avg')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_adj.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def blocks_shrinker(data_f, date, size):\n    \n    \"\"\"function will balance the date blocks and downsize the blocks rows as you will mention \n       data_f: is the data frame name\n       date: date blocks column\n       size: the size you want each date block to be\"\"\"\n    \n    # how many rows inside each month\n    class_size = data_f[date].value_counts().sort_values()\n    adj_size = class_size.iloc[0] - size\n    adj_index_size = class_size.index[0]\n    \n    # down sizing the data and blancing\n    if size <= class_size.iloc[0]:\n        data_f = data_f.drop(data_f[data_f[date]==adj_index_size].index[:adj_size])\n\n        try_df = data_f.groupby(date)\n\n        try_df = try_df.apply(lambda x: x.sample(try_df.size().min()).reset_index(drop=True))\n\n        try_df = try_df.drop(date, axis=1)\n        try_df = try_df.reset_index()\n        try_df = try_df.drop('level_1', axis=1)\n        return try_df\n    else:\n        return \" The size you enter {0} while the smallest date block is {1}\".format(size, class_size.iloc[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try_df = blocks_shrinker(df_adj, 'date_block_num', 500)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_data = try_df.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_data.describe().T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare the train data for LSTM ","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()\nnew_data[['scaled_shop_id', 'scaled_item_category_id']] = scaler.fit_transform(new_data[['shop_id', 'item_category_id']])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_data = new_data.set_index('date').sort_index()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = new_data[['scaled_shop_id', 'scaled_item_category_id', 'item_cnt_day_avg']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the test data for LSTM ","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_test = test.merge(items[['item_category_id', 'item_id']], how='left', on='item_id')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_conv = all_test.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_scaled_dic(full_train_df, item, scaled_item):\n    \n    \"\"\" to get dictionary from scaled values from training data set,\n            to scale the test dataset with the same STD & mean\"\"\"\n    \n    \n    item_dic = full_train_df.groupby(item)[scaled_item].apply(lambda g: g.values.tolist()).to_dict()\n    item_converter = {a:list(set(b)) for a, b in item_dic.items()}\n    \n    return item_converter","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_conv = test_conv.replace({\"item_category_id\": get_scaled_dic(new_data, 'item_category_id', 'scaled_item_category_id')})\ntest_conv = test_conv.replace({\"shop_id\": get_scaled_dic(new_data, 'shop_id', 'scaled_shop_id')})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_conv =test_conv.drop('item_id', axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = test_conv.set_index('ID')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_data[['scaled_shop_id', 'scaled_item_category_id']]\ny_train = train_data['item_cnt_day_avg']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lstm(train_X_init, train_y_init, n_step, b_size, epoch, verbose=1):\n    \n    \n    \n    train_data_gen = tf.keras.preprocessing.sequence.TimeseriesGenerator(X_train, y_train, length=n_step,\n                                                                         batch_size=b_size)\n  \n    \n    model = Sequential() \n\n    model.add(LSTM(128, input_shape=(n_step,2)))\n    \n    model.add(Dense(128, activation='softmax'))\n    model.add(Dense(64, activation='softmax'))\n    \n    model.add(Dense(1))\n\n    # define the loss function / optimization strategy, and fit\n    # the model with the desired number of passes over the data (epochs) \n    model.compile(loss='mean_squared_error', optimizer='adam')\n    model.fit_generator(train_data_gen, epochs=epoch, verbose=1)\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_step = 500\nb_size = 64\nepoch = 1\n\nmodel = get_lstm(X_train, y_train, n_step, b_size, epoch)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_gen(test_X_init):\n    \n    generator_test_zeros = tf.keras.preprocessing.sequence.TimeseriesGenerator(test_X_init, np.zeros(len(test_X_init)), \n                                                                           length=n_step, batch_size=b_size)\n    \n    return model.predict(generator_test_zeros, verbose=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator_pred = predict_gen(test_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator_pred.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sub_df(pred_array, yhat, submission_df):\n    \n    \"\"\"pred_array: model output\n       yhat ('str'): the name of yhat column\n       submission_df: the submission data frame \"\"\"\n    \n    rr = pd.DataFrame(pred_array, columns=[yhat])\n    \n    submission = pd.merge(submission_df, rr, left_index=True, right_index=True, how='left')\n    \n    submission[yhat] = submission[yhat].interpolate()\n    \n    return submission","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = sub_df(generator_pred, 'tem_cnt_month', sample_sub)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}