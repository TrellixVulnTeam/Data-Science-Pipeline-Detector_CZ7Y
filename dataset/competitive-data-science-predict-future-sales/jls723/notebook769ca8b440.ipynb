{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Loading all needed libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom itertools import product\nimport xgboost\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb\nimport calendar\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom math import ceil\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading Data\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\nsales = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nitem_cats = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped = pd.DataFrame(sales.groupby(['shop_id', 'date_block_num'])['item_cnt_day'].sum().reset_index())\nfig, axes = plt.subplots(nrows=5, ncols=2, sharex=True, sharey=True, figsize=(16,20))\nnum_graph = 10\nid_per_graph = ceil(grouped.shop_id.max() / num_graph)\ncount = 0\nfor i in range(5):\n    for j in range(2):\n        sns.pointplot(x='date_block_num', y='item_cnt_day', hue='shop_id', data=grouped[np.logical_and(count*id_per_graph <= grouped['shop_id'], grouped['shop_id'] < (count+1)*id_per_graph)], ax=axes[i][j])\n        count += 1\n    \ntrain = sales.copy()\ntrain = train.set_index('item_id').join(items.set_index('item_id')).drop('item_name', axis=1).reset_index()\ntrain['month'] = train.date.apply(lambda x: datetime.strptime(x, '%d.%m.%Y').strftime('%m'))\ntrain['year'] = train.date.apply(lambda x: datetime.strptime(x, '%d.%m.%Y').strftime('%Y'))\n\nfig, axes = plt.subplots(nrows=5, ncols=2, sharex=True, sharey=True, figsize=(16,20))\nnum_graph = 10\nid_per_graph = ceil(train.item_category_id.max() / num_graph)\ncount = 0\nfor i in range(5):\n    for j in range(2):\n        sns.pointplot(x='month', y='item_cnt_day', hue='item_category_id', \n                      data=train[np.logical_and(count*id_per_graph <= train['item_category_id'], train['item_category_id'] < (count+1)*id_per_graph)], \n                      ax=axes[i][j])\n        count += 1\n        \nfig, axes = plt.subplots(nrows=5, ncols=2, sharex=True, sharey=True, figsize=(16,20))\nnum_graph = 10\nid_per_graph = ceil(train.item_category_id.max() / num_graph)\ncount = 0\nfor i in range(5):\n    for j in range(2):\n        sns.pointplot(x='date_block_num', y='item_cnt_day', hue='item_category_id', \n                      data=train[np.logical_and(count*id_per_graph <= train['item_category_id'], train['item_category_id'] < (count+1)*id_per_graph)], \n                      ax=axes[i][j])\n        count += 1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking for item_cnt_day outliers\nsns.boxplot(x=sales.item_cnt_day)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking for item_price outliers\nsns.boxplot(x=sales.item_price)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing all tuples with item_price over 100,000 or negative price\ntrain = sales[(sales.item_price < 100000) & (sales.item_price > 0)]\n#Removing all tuples with more than 1000 units sold per day.\ntrain = train[sales.item_cnt_day < 1001]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Some shops have multiple shop ids\nprint(shops[shops.shop_id.isin([0, 57])]['shop_name'])\nprint(shops[shops.shop_id.isin([1, 58])]['shop_name'])\nprint(shops[shops.shop_id.isin([40, 39])]['shop_name'])\nprint(shops[shops.shop_id.isin([10, 11])]['shop_name'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ensuring shops with multiple shop ids a single shop id.\n#Specifically, one shop has ids 0 and 57, another has ids 1 and 58, and another has 40 and 39.\n#To correct, we set all tuples with shop id 0 to shop id 57, all tuples with shop id 1 to shop id 58,\n#and all tuples with shop id 40 to 39.\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n\ntrain.loc[train.shop_id == 40, 'shop_id'] = 39\ntest.loc[test.shop_id == 40, 'shop_id'] = 39\n\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We now create our basic training dataframe, which we will build on.\n#The columns we our dataframe will start with is shop_id, item_id, and date_block_num.\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n#To build the basic dataframe, we do the following:\n#For each block_num in our dataset (i.e., for each month for which we have some sale tuple):\n    #Find the unique shop ids of all tuples in our training set with that block_num (month)\n    #Find the unique item id of all tuples in our training set with that block_num (month)\n    #Calculate the Cartesian product of those unique shop ids, unique item ids, and the block_num.\n    #Add the resulting set of tuples to the dataframe we're building.\ndf = [] \nfor block_num in train['date_block_num'].unique():\n    cur_shops = train.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = train.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    df.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n    \n\n#Turn the list that we've created into an actual dataframe.\ndf = pd.DataFrame(np.vstack(df), columns = index_cols,dtype=np.int32)\n\n#Get a count of the the number of items sold for each date_block_num, shop_id, item_id combination in our data set.\n#This effectively gives us the sales per month for a given shop_id, item_id pair.\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\n#Add these monthly sales to our dataframe by joining on the common columns. By the way, the added column for monthly sales\n#is called 'item_cnt_month'\ndf = pd.merge(df, group, on=index_cols, how='left')\n#Fill any null cells in the item_cnt_month column as 0s, but also set the max possible value as 20 and the lowest possible\n#value as 0.\ndf['item_cnt_month'] = (df['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20)\n                                .astype(np.float16))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Add the test dataset to our dataframe so that we can make our predictions.\ntest['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)\ndf = pd.concat([df, test], ignore_index=True, sort=False, keys=index_cols)\n#Zero out predictions.\ndf.fillna(0, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the city each shop is located by splitting the shop_name category on spaces and getting the first val in the\n#resulting list.\nshops['city'] = shops['shop_name'].apply(lambda x: x.split()[0].lower())\n#Eliminate the one anomaly of the city that starts with the exclamation point.\nshops.loc[shops.city == '!якутск', 'city'] = 'якутск'\n#Encode the city names and add a column to our dataframe to reflect each city's code. This code will be used in later\n#analysis.\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n\n#Hard-code a dict that maps each city used in our dataset to its coordinates and part of the country lies in.\ncoords = dict()\ncoords['якутск'] = (62.028098, 129.732555, 4)\ncoords['адыгея'] = (44.609764, 40.100516, 3)\ncoords['балашиха'] = (55.8094500, 37.9580600, 1)\ncoords['волжский'] = (53.4305800, 50.1190000, 3)\ncoords['вологда'] = (59.2239000, 39.8839800, 2)\ncoords['воронеж'] = (51.6720400, 39.1843000, 3)\ncoords['выездная'] = (0, 0, 0)\ncoords['жуковский'] = (55.5952800, 38.1202800, 1)\ncoords['интернет-магазин'] = (0, 0, 0)\ncoords['казань'] = (55.7887400, 49.1221400, 4)\ncoords['калуга'] = (54.5293000, 36.2754200, 4)\ncoords['коломна'] = (55.0794400, 38.7783300, 4)\ncoords['красноярск'] = (56.0183900, 92.8671700, 4)\ncoords['курск'] = (51.7373300, 36.1873500, 3)\ncoords['москва'] = (55.7522200, 37.6155600, 1)\ncoords['мытищи'] = (55.9116300, 37.7307600, 1)\ncoords['н.новгород'] = (56.3286700, 44.0020500, 4)\ncoords['новосибирск'] = (55.0415000, 82.9346000, 4)\ncoords['омск'] = (54.9924400, 73.3685900, 4)\ncoords['ростовнадону'] = (47.2313500, 39.7232800, 3)\ncoords['спб'] = (59.9386300, 30.3141300, 2)\ncoords['самара'] = (53.2000700, 50.1500000, 4)\ncoords['сергиев'] = (56.3000000, 38.1333300, 4)\ncoords['сургут'] = (61.2500000, 73.4166700, 4)\ncoords['томск'] = (56.4977100, 84.9743700, 4)\ncoords['тюмень'] = (57.1522200, 65.5272200, 4)\ncoords['уфа'] = (54.7430600, 55.9677900, 4)\ncoords['химки'] = (55.8970400, 37.4296900, 1)\ncoords['цифровой'] = (0, 0, 0)\ncoords['чехов'] = (55.1477000, 37.4772800, 4)\ncoords['ярославль'] = (57.6298700, 39.8736800, 2) \n\n#Add on columns representing the the city of each tuples latitude, longitude, and which part of the country it's in.\nshops['city_coord_1'] = shops['city'].apply(lambda x: coords[x][0])\nshops['city_coord_2'] = shops['city'].apply(lambda x: coords[x][1])\nshops['country_part'] = shops['city'].apply(lambda x: coords[x][2])\n\nshops = shops[['shop_id', 'city_code', 'city_coord_1', 'city_coord_2', 'country_part']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Merge our dataframe with the augmented shops dataframe we have that includes each shop's city code, latitude, longitude\n#and part of the country it is in. Join them on the shop id.\ndf = pd.merge(df, shops, on=['shop_id'], how='left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a dict of common categories.\nmap_dict = {\n            'Чистые носители (штучные)': 'Чистые носители',\n            'Чистые носители (шпиль)' : 'Чистые носители',\n            'PC ': 'Аксессуары',\n            'Служебные': 'Служебные '\n            }\n\n#Merge items and item_cats on item_category id.\nitems = pd.merge(items, item_cats, on='item_category_id')\n\n#Augment items by creating a column called item 'item_category' that is the provided item_category that precedes its subcategory\n#in the item_category_name column. Item category should be the value that category maps to if its in the dict, and just what\n#it is otherwise. Finally, encode these item categories for later analysis.\nitems['item_category'] = items['item_category_name'].apply(lambda x: x.split('-')[0])\nitems['item_category'] = items['item_category'].apply(lambda x: map_dict[x] if x in map_dict.keys() else x)\nitems['item_category_common'] = LabelEncoder().fit_transform(items['item_category'])\n\n#Also encode the full item_category_name of each item.\nitems['item_category_code'] = LabelEncoder().fit_transform(items['item_category_name'])\nitems = items[['item_id', 'item_category_common', 'item_category_code']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Merge our dataframe with the augmented items dataframe we have that includes each item's item id, common category \n#code, and given category code. Join them on the item id.\ndf = pd.merge(df, items, on=['item_id'], how='left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Helper method to find the number of Saturdays in a month, the number of days in a month, and the month as an integer\n#of each date_block_num in our dataset.\ndef count_days(date_block_num):\n    year = 2013 + date_block_num // 12\n    month = 1 + date_block_num % 12\n    weeknd_count = len([1 for i in calendar.monthcalendar(year, month) if i[6] != 0])\n    days_in_month = calendar.monthrange(year, month)[1]\n    return weeknd_count, days_in_month, month\n\n#Create a dict that maps date_block_nums in our dataset to the number of Saturdays in that month, the number of days \n#in that month, and the date_block_num as a month as an integer using our count_days method.\nmap_dict = {i: count_days(i) for i in range(35)}\n\n#For each tuple, find the number of Saturdays in the month in which that sale was made (number of saturdays will be\n#thought of as the number of weekends in a relaxed sense). Store these values in a 'weekend_count' column.\ndf['weeknd_count'] = df['date_block_num'].apply(lambda x: map_dict[x][0])\n#For each tuple, find the number of days in the month in which that sale was made. Store these values in a \n#'days_in_month' column.\ndf['days_in_month'] = df['date_block_num'].apply(lambda x: map_dict[x][1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find the first month for each each item that that item was available.\nfirst_item_block = df.groupby(['item_id'])['date_block_num'].min().reset_index()\nfirst_item_block['item_first_interaction'] = 1\n\n#Find the first month for each item in a shop that that item was sold.\nfirst_shop_item_buy_block = df[df['item_cnt_month'] > 0].groupby(['shop_id', 'item_id'])['date_block_num'].min().reset_index()\nfirst_shop_item_buy_block['first_date_block_num'] = first_shop_item_buy_block['date_block_num']\n\n#Merge our datasets.\ndf = pd.merge(df, first_item_block[['item_id', 'date_block_num', 'item_first_interaction']], on=['item_id', 'date_block_num'], how='left')\n#Merge our datasets.\ndf = pd.merge(df, first_shop_item_buy_block[['item_id', 'shop_id', 'first_date_block_num']], on=['item_id', 'shop_id'], how='left')\n#Add a mask to indicate for each tuple if the item was sold in the first month that the item \n#was sold in that shop or not.\ndf['first_date_block_num'].fillna(100, inplace=True)\ndf['shop_item_sold_before'] = (df['first_date_block_num'] < df['date_block_num']).astype('int8')\ndf.drop(['first_date_block_num'], axis=1, inplace=True)\n\n#Add a mask to indicate for each tuple if the item was sold in the first month the item was\n#available or not.\ndf['item_first_interaction'].fillna(0, inplace=True)\ndf['shop_item_sold_before'].fillna(0, inplace=True)\ndf['item_first_interaction'] = df['item_first_interaction'].astype('int8')  \ndf['shop_item_sold_before'] = df['shop_item_sold_before'].astype('int8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Add lag features to our dataframe.\n#Helper method to add a lag feature to our dataframe. Given a dataframe, a number of time steps we would like\n#to look at, and a column to focus on, we can add a lag feature to our dataframe.\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n        df[col+'_lag_'+str(i)] = df[col+'_lag_'+str(i)].astype('float16')\n    return df\n\n#The first lag feature we would like to add is the item_cnt_month feature at 1, 2, and 3 time steps.\ndf = lag_feature(df, [1, 2, 3], 'item_cnt_month')\ndf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the average price of a given item sold at a given shop in a given month.\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\ngroup = train.groupby(index_cols)['item_price'].mean().reset_index().rename(columns={\"item_price\": \"avg_shop_price\"}, errors=\"raise\")\n#Merge the datasets so that we now have for each tuple the average price of the item sold in that shop during that month\n#as specified by the tuple.\ndf = pd.merge(df, group, on=index_cols, how='left')\n\n#For any null values, fill them in with 0s.\ndf['avg_shop_price'] = (df['avg_shop_price']\n                                .fillna(0)\n                                .astype(np.float16))\n\n#Get the average price of a given item in a given month.\nindex_cols = ['item_id', 'date_block_num']\ngroup = train.groupby(['date_block_num','item_id'])['item_price'].mean().reset_index().rename(columns={\"item_price\": \"avg_item_price\"}, errors=\"raise\")\n\n\n#Merge the datasets so that we now have for each tuple the average price of that item during that month across all shops.\ndf = pd.merge(df, group, on=index_cols, how='left')\n#For any null values, fill them in with 0s.\ndf['avg_item_price'] = (df['avg_item_price']\n                                .fillna(0)\n                                .astype(np.float16))\n\n#Find how much the the average price of an item for a shop in a given month deviates from the average price of that same\n#item across all shops as a percent of the average price of that item across all shops for that month.\ndf['item_shop_price_avg'] = (df['avg_shop_price'] - df['avg_item_price']) / df['avg_item_price']\n#For any null values, fill them in with 0s.\ndf['item_shop_price_avg'].fillna(0, inplace=True)\n\n#Add this price difference as a lag feature and get rid of the columns we added to compute it.\ndf = lag_feature(df, [1, 2, 3], 'item_shop_price_avg')\ndf.drop(['avg_shop_price', 'avg_item_price', 'item_shop_price_avg'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#In order to elicit a relationship between month-item pairs and the amount sold, we need to reduce the pairs to some\n#meaningful numerical value. To do this, we map each month-item pair to the average number of units sold that month.\nitem_id_target_mean = df.groupby(['date_block_num','item_id'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_enc\"}, errors=\"raise\")\n\n#We add this mapping to our data set, joining our tuples on the date_block_num and item_id columns\ndf = pd.merge(df, item_id_target_mean, on=['date_block_num','item_id'], how='left')\n\n#For any null values, fill them in with 0s.\ndf['item_target_enc'] = (df['item_target_enc']\n                                .fillna(0)\n                                .astype(np.float16))\n#Add this column as a lag feature and get rid of the columns we added to compute it.\ndf = lag_feature(df, [1, 2, 3], 'item_target_enc')\ndf.drop(['item_target_enc'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Likewise, in order to elicit a relationship between month-item-city pairs and the amount sold, we need to reduce the pairs to some\n#meaningful numerical value. To do this, we map each month-item-city pair to the average number of units sold that month in that city.\nitem_id_target_mean = df.groupby(['date_block_num','item_id', 'city_code'])['item_cnt_month'].mean().reset_index().rename(columns={\n    \"item_cnt_month\": \"item_loc_target_enc\"}, errors=\"raise\")\n#We add this mapping to our data set, joining our tuples on the date_block_num, item_id, and city_code columns\ndf = pd.merge(df, item_id_target_mean, on=['date_block_num','item_id', 'city_code'], how='left')\n\n#For any null values, fill them in with 0s.\ndf['item_loc_target_enc'] = (df['item_loc_target_enc']\n                                .fillna(0)\n                                .astype(np.float16))\n\n#Add this column as a lag feature and get rid of the columns we added to compute it.\ndf = lag_feature(df, [1, 2, 3], 'item_loc_target_enc')\ndf.drop(['item_loc_target_enc'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finally, in order to elicit a relationship between month-item-shop pairs and the amount sold, we need to reduce the pairs to some\n#meaningful numerical value. To do this, we map each month-item-shop pair to the average number of units sold that month in that shop.\nitem_id_target_mean = df.groupby(['date_block_num','item_id', 'shop_id'])['item_cnt_month'].mean().reset_index().rename(columns={\n    \"item_cnt_month\": \"item_shop_target_enc\"}, errors=\"raise\")\n\n#We add this mapping to our data set, joining our tuples on the date_block_num, item_id, and city_code columns\ndf = pd.merge(df, item_id_target_mean, on=['date_block_num','item_id', 'shop_id'], how='left')\n\n#For any null values, fill them in with 0s.\ndf['item_shop_target_enc'] = (df['item_shop_target_enc']\n                                .fillna(0)\n                                .astype(np.float16))\n\n#Add this column as a lag feature and get rid of the columns we added to compute it.\ndf = lag_feature(df, [1, 2, 3], 'item_shop_target_enc')\ndf.drop(['item_shop_target_enc'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now, we want to see how the newness of an item affects sales. To see this, we get tuples corresponding to a first-month \n#sale for an item (meaning the item was sold in the first month it was available), and find the the average number of units\n#sold by item category (We must do it by category because looking by item is too specific. We want to predict how this might\n#impact sales for a new item in the same category.).\nitem_id_target_mean = df[df['item_first_interaction'] == 1].groupby(['date_block_num','item_category_code'])['item_cnt_month'].mean().reset_index().rename(columns={\n    \"item_cnt_month\": \"new_item_cat_avg\"}, errors=\"raise\")\n\n#We add this column to our dataset, once again joining on date_block_num and item_category_code\ndf = pd.merge(df, item_id_target_mean, on=['date_block_num','item_category_code'], how='left')\n\n#For any null values, fill them in with 0s.\ndf['new_item_cat_avg'] = (df['new_item_cat_avg']\n                                .fillna(0)\n                                .astype(np.float16))\n\n#Add this column as a lag feature and get rid of the columns we added to compute it.\ndf = lag_feature(df, [1, 2, 3], 'new_item_cat_avg')\ndf.drop(['new_item_cat_avg'], axis=1, inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now, we want to see how the newness of an item within a category affects sales for specific stores. To see this, \n#we again get tuples corresponding to a first-month sales for an item, and find the the average number of units\n#sold by item category and shop id.\nitem_id_target_mean = df[df['item_first_interaction'] == 1].groupby(['date_block_num','item_category_code', 'shop_id'])['item_cnt_month'].mean().reset_index().rename(columns={\n    \"item_cnt_month\": \"new_item_shop_cat_avg\"}, errors=\"raise\")\n\n#We add this column to our dataset, once again joining on date_block_num and item_category_code\ndf = pd.merge(df, item_id_target_mean, on=['date_block_num','item_category_code', 'shop_id'], how='left')\n\n#For any null values, fill them in with 0s.\ndf['new_item_shop_cat_avg'] = (df['new_item_shop_cat_avg']\n                                .fillna(0)\n                                .astype(np.float16))\n\n#Add this column as a lag feature and get rid of the columns we added to compute it.\ndf = lag_feature(df, [1, 2, 3], 'new_item_shop_cat_avg')\ndf.drop(['new_item_shop_cat_avg'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We must remove the first tree months of data since we don't have complete data for those tuples (some of the lag\n#features will be missing).\ndf.fillna(0, inplace=True)\ndf = df[(df['date_block_num'] > 2)]\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Save dataset\ndf.drop(['ID'], axis=1, inplace=True, errors='ignore')\ndf.to_pickle('df.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training Time\ndf = pd.read_pickle('df.pkl')\n\n#Create a dataframe containing only tuples of sales that occurred before the 33rd month, but drop item_cnt_month from it\n#These will be our inputs.\nX_train = df[df.date_block_num < 33].drop(['item_cnt_month'], axis=1)\n\n#Create a dataframe containing only tuples of sales that occurred before the 33rd month, but only keep \n#item_cnt_month in it. These will be our outputs.\nY_train = df[df.date_block_num < 33]['item_cnt_month']\n\n#Finally, we separate the 33rd month in the same way, but use it for evaluation.\nX_valid = df[df.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = df[df.date_block_num == 33]['item_cnt_month']\nX_test = df[df.date_block_num == 34].drop(['item_cnt_month'], axis=1)\ndel df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get all of our features in a list.\nfeature_name = X_train.columns.tolist()\n\n\n#Parameters specific to our lgb.train() function\nparams = {\n    'objective': 'mse',\n    'metric': 'rmse',\n    'num_leaves': 2 ** 7 - 1,\n    'learning_rate': 0.005,\n    'feature_fraction': 0.75,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 5,\n    'seed': 1,\n    'verbose': 1\n}\n\nfeature_name_indexes = [ \n                        'country_part', \n                        'item_category_common',\n                        'item_category_code', \n                        'city_code',\n]\n\n#Turning the dataset into an lgb dataset.\nlgb_train = lgb.Dataset(X_train[feature_name], Y_train)\n#Turning the dataset into an lgb dataset for evaluation.\nlgb_eval = lgb.Dataset(X_valid[feature_name], Y_valid, reference=lgb_train)\n\nevals_result = {}\n#Building our model\ngbm = lgb.train(\n        params, \n        lgb_train,\n        num_boost_round=3000,\n        valid_sets=(lgb_train, lgb_eval), \n        feature_name = feature_name,\n        categorical_feature = feature_name_indexes,\n        verbose_eval=5, \n        evals_result = evals_result,\n        early_stopping_rounds = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot of each feature's importance\nlgb.plot_importance(\n    gbm, \n    max_num_features=50, \n    importance_type='gain', \n    figsize=(12,8));\n\n#Making our predictions on the test set and outputting results to a csv\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\nY_test = gbm.predict(X_test[feature_name]).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('gbm_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}