{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''Universal data analysis laptop. It only takes one line to replace - reading your data file'''\n# This is a sample Python script.\nimport pandas as pd\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n#we will split the train set into train and test data in future sections\ndata_raw = pd.read_csv('../input/number-of-arrests-for-drug-abuse-violations/arrests_usa_drug.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal passes by reference vs values, so we use the copy function: https://stackoverflow.com/questions/46327494/python-pandas-dataframe-copydeep-false-vs-copydeep-true-vs\ndata1 = data_raw.copy(deep = True)\n\n#preview data\n\nprint(\"\\n ----------Top-5- Record----------\")\nprint(data_raw.head(5))  #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html\n# print(data_raw.tail(5)) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html\n# print(data_raw.sample(10)) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\nprint(\"\\n -----------Information-----------\")\nprint(data_raw.info())  #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html\nprint(\"\\n -----------Data Types-----------\")\nprint(data_raw.dtypes)\nprint(\"\\n ----------Missing value-----------\")\nprint(data_raw.isnull().sum())\nprint(\"\\n ----------Null value-----------\")\nprint(data_raw.isna().sum())\nprint(\"\\n ----------Shape of Data----------\")\nprint(data_raw.shape)\nprint(\"\\n ----------Number of duplicates----------\")\nprint('Number of duplicates:', len(data_raw[data_raw.duplicated()]))\n\n# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n\nmissing_values_data = missing_values_table(data1)\nprint(\"\\n ----------Missing values----------\")\nprint(missing_values_data.head(30))\n\nprint(\"\\n ----------Number of types----------\")\n# Number of each type of column\nprint(data1.dtypes.value_counts())\n\nprint(\"\\n ----------Number of uniques----------\")\n# Let's now look at the number of unique entries in each of the object (categorical) columns.\nprint(data1.select_dtypes('object').apply(pd.Series.nunique, axis = 0))\n\nprint(\"\\n ----------Describe of tables----------\")\nprint(data_raw.describe(include = 'all'))\n\n\n#preview data again\nprint(data1.corr())","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.51773,"end_time":"2022-02-05T18:01:02.59295","exception":false,"start_time":"2022-02-05T18:01:01.07522","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T05:32:24.782046Z","iopub.execute_input":"2022-02-06T05:32:24.782357Z","iopub.status.idle":"2022-02-06T05:32:26.108407Z","shell.execute_reply.started":"2022-02-06T05:32:24.782274Z","shell.execute_reply":"2022-02-06T05:32:26.107685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(data1.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()\n\ndata1.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8); # ; avoid having the matplotlib verbose informations","metadata":{"papermill":{"duration":12.718624,"end_time":"2022-02-05T18:01:15.317893","exception":false,"start_time":"2022-02-05T18:01:02.599269","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T05:32:26.110177Z","iopub.execute_input":"2022-02-06T05:32:26.110434Z","iopub.status.idle":"2022-02-06T05:32:31.774457Z","shell.execute_reply.started":"2022-02-06T05:32:26.110401Z","shell.execute_reply":"2022-02-06T05:32:31.773832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nsns.pairplot(data1, size = 2.5)\nplt.show();","metadata":{"papermill":{"duration":429.315938,"end_time":"2022-02-05T18:08:24.652005","exception":false,"start_time":"2022-02-05T18:01:15.336067","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T05:32:31.775417Z","iopub.execute_input":"2022-02-06T05:32:31.775745Z","iopub.status.idle":"2022-02-06T05:33:20.971819Z","shell.execute_reply.started":"2022-02-06T05:32:31.775716Z","shell.execute_reply":"2022-02-06T05:33:20.970834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ReferencesÂ¶\nI hope you liked this code, I also prepared more interesting laptops for this competition and I will be glad to share them with you:\n\n1. [Data ScienceTutorial for Beginners](https://www.kaggle.com/andrej0marinchenko/data-sciencetutorial-for-beginners-predict-fs)\n2. [Step by Step for Beginners](https://www.kaggle.com/andrej0marinchenko/future-sales-step-by-step-for-beginners)\n3. [Future sales with automated ensembling](https://www.kaggle.com/andrej0marinchenko/future-sales-with-automated-ensembling)\n4. [Predict Future Sales LightGBM framework](https://www.kaggle.com/andrej0marinchenko/predict-future-sales-lightgbm-framework)\n5. [universal notebook for data analysis](https://www.kaggle.com/andrej0marinchenko/universal-notebook-for-data-analysis)","metadata":{}}]}