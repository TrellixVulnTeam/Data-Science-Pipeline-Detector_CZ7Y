{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import required libraries and get data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Library importer\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime as dt\nimport gc\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\nitems = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nitem_cats = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\ntest = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I'm going to use a memory reduction function that downcasts my dataframe columns.**\n\n**First things first! Don't forget to convert the date column to date format.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['date'] = train['date'].apply(lambda x: dt.strptime(x, '%d.%m.%Y'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Downcasting: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast(df):\n    cols = df.dtypes.index.tolist()\n    types = df.dtypes.values.tolist()\n    for i,t in enumerate(types):\n        if 'int' in str(t):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n        elif 'float' in str(t):\n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n        elif t == np.object:\n            if cols[i] == 'date':\n                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n            else:\n                df[cols[i]] = df[cols[i]].astype('category')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = downcast(train)\ntest = downcast(test)\nshops = downcast(shops)\nitems = downcast(items)\nitem_cats = downcast(item_cats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing Outliers:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Let's remove outliers first. I referred to this kernel for outlier removal: [https://www.kaggle.com/karell/xgb-baseline-advanced-feature-engineering](https://www.kaggle.com/karell/xgb-baseline-advanced-feature-engineering)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train.item_cnt_day)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train.item_price)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Eliminate all item prices below zero, and set all negative item_cnt_day values to 0.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train.item_price > 0].reset_index(drop=True)\ntrain[train.item_cnt_day <= 0].item_cnt_day.unique()\ntrain.loc[train.item_cnt_day < 1, 'item_cnt_day'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Several shops are duplicates of each other (according to their names). Fix train and test set.\nWe add 40 to 39.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Якутск Орджоникидзе, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# Якутск ТЦ \"Центральный\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# Жуковский ул. Чкалова 39м²\ntrain.loc[train.shop_id == 11, 'shop_id'] = 10\ntest.loc[test.shop_id == 11, 'shop_id'] = 10\n\ntrain.loc[train.shop_id == 40, 'shop_id'] = 39\ntest.loc[test.shop_id == 40, 'shop_id'] = 39","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get all possible data:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**We only have two columns in our test set, shop_id and item_id.**\n\n**This is not enough for us to make predictions. So, we need to put more features.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_test = pd.merge(pd.merge(pd.merge(test, items),shops),item_cats)\nnew_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will do the same thing for the train dataset.**\n\n**We need to get some common categories between them.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train = pd.merge(pd.merge(pd.merge(train, items),shops),item_cats)\nnew_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, sales from different shops and different items don't necessarily affect each other. We need to find a relation between them.**\n\n**Finding how different item categories affect the features is a good first step to understanding the data.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization and Analysis:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"aggr = new_train.groupby(['item_category_id']).agg({'item_price':'sum'})\naggr = aggr.reset_index()\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.plot(aggr['item_category_id'], aggr['item_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aggr = new_train.groupby(['item_category_id']).agg({'item_cnt_day':'mean'})\naggr = aggr.reset_index()\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.plot(aggr['item_category_id'], aggr['item_cnt_day'], color='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It looks like the item prices and items sold per day vary heavily by category. There are some spikes, and it shows a parabolic trend. If you look closely, you'll see the parabola.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**We mustn't forget that we need to find a monthly estimate of the number of items sold.**\n\n**Let's find out the relations between item_cnt_day, item_price and date_block_num.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"aggr = train.groupby(['date_block_num']).agg({'item_cnt_day':'mean'})\naggr = aggr.reset_index()\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.plot(aggr['date_block_num'], aggr['item_cnt_day'], color='brown')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aggr = train.groupby(['date_block_num']).agg({'item_price':'mean'})\naggr = aggr.reset_index()\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.plot(aggr['date_block_num'], aggr['item_price'], color='green')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**By looking closely, we can see some linear relationship in the above two graphs.**\n\n**There are many spikes, but the general trend seems to be an increase.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"aggr = train.groupby(['date_block_num']).agg({'item_price':'mean'})\naggr = aggr.reset_index()\n\nsns.scatterplot(data=aggr, x='date_block_num', y='item_price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style='ticks')\n\naggr = train.drop(columns = ['date','shop_id','item_id'])\nsns.pairplot(aggr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The above graphs are just to see what the data looks like when plotted against each other.**\n\n**For example, the bottom most, extreme left graph plots date_block_num(x) against item_cnt_day(y). It shows a slightly linear relationship.**\n\n**Pairplots are basically scatterplot matrices. They save the time and effort of plotting each one against another.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Delete unwanted dataframes\n\ndel new_train\ndel test\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We're going to do some group-bys to get our monthly sales.**\n\n**First groupby: To get the duration feature!**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"aggr = train.groupby(['shop_id','item_id','date_block_num'])['item_price','item_cnt_day','date'].agg({'item_price':['mean'], 'item_cnt_day':['sum'], 'date':['min','max']})\n\naggr = aggr.reset_index()\naggr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aggr['duration'] = (aggr['date']['max'] - aggr['date']['min']).dt.days.astype(np.int16)\naggr['duration'] += 1\naggr['item_cnt_day_sum'] = aggr['item_cnt_day']['sum']\naggr['item_price_mean'] = aggr['item_price']['mean']\n\naggr = aggr.drop(columns = ['date', 'item_cnt_day', 'item_price'])\n\naggr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Second groupby: To get our actual monthly estimates!**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_sales = aggr.groupby(['shop_id', 'item_id']).agg({'item_cnt_day_sum':['sum'], 'item_price_mean':['mean'], 'duration':['sum']})\n\nmonthly_sales = monthly_sales.reset_index()\nmonthly_sales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_sales['item_cnt_month'] = monthly_sales['item_cnt_day_sum']['sum']/30\nmonthly_sales['item_price_month'] = monthly_sales['item_price_mean']['mean']\nmonthly_sales['sales_duration'] = monthly_sales['duration']['sum']\n\nmonthly_sales = monthly_sales.drop(columns = ['item_cnt_day_sum','item_price_mean','duration'])\nmonthly_sales","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There we go! We now have our actual monthly estimates. We need to predict item_cnt_month for the test set.**\n\n**But first merge with shop, items, and item_cats to get remaining features!**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Delete unwanted dataframes\ndel aggr\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**When merging two dataframes of different levels, some column names will become tuples. Like this: (shop_id, )**\n\n**To solve this issue, we create the following function. It checks whether the column name is a string or not. If it's not a string, it changes it to a string.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def colnamecheck(df):\n    cols = []\n    \n    for x in df.columns:\n        if type(x)!= str:\n            cols.append(''.join(x))\n        else:\n            cols.append(x)\n            \n    df.columns = cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(monthly_sales, items, on='item_id')\ndf = df.drop(columns = ['item_id'])\n\ncolnamecheck(df)\n\ndf1 = pd.merge(df, shops, on='shop_id')\ndf2 = pd.merge(df1, item_cats, on='item_category_id')\n\nmonthly_sales = df2\n\ndel df, df1, df2\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def EncodeColumn(df, old_col, new_col):\n    \n    enc = LabelEncoder()\n    \n    df[new_col] = enc.fit_transform(df[old_col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EncodeColumn(monthly_sales, 'item_name', 'item_name_enc')\nEncodeColumn(new_test, 'item_name', 'item_name_enc')\n\nEncodeColumn(monthly_sales, 'shop_name', 'shop_name_enc')\nEncodeColumn(new_test, 'shop_name', 'shop_name_enc')\n\nEncodeColumn(monthly_sales, 'item_category_name', 'item_category_name_enc')\nEncodeColumn(new_test, 'item_category_name', 'item_category_name_enc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_sales = monthly_sales.drop(columns = ['item_name'])\nmonthly_sales = monthly_sales.drop(columns = ['shop_name'])\nmonthly_sales = monthly_sales.drop(columns = ['item_category_name'])\n\nnew_test = new_test.drop(columns = ['item_name'])\nnew_test = new_test.drop(columns = ['shop_name'])\nnew_test = new_test.drop(columns = ['item_category_name'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**We will be using XGBoost to predict item_price_month first, then sales_duration, and finally item_cnt_month.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBRegressor(\n    learning_rate=0.01,\n    max_depth=3,\n    n_estimators=1000, \n    colsample_bytree=0.8, \n    subsample=0.8,     \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = monthly_sales.drop(columns = ['item_price_month', 'sales_duration', 'item_cnt_month'])\ny = monthly_sales['item_price_month']\n\nxgb.fit(X, y)\n\npreds = xgb.predict(new_test.drop(columns=['ID']))\nnew_test['item_price_month'] = preds\nmonthly_sales = monthly_sales.drop(columns = ['item_price_month'])\nmonthly_sales['item_price_month'] = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = monthly_sales.drop(columns = ['sales_duration', 'item_cnt_month'])\ny = monthly_sales['sales_duration']\n\nxgb.fit(X, y)\n\npreds = xgb.predict(new_test.drop(columns=['ID']))\nnew_test['sales_duration'] = preds\nmonthly_sales = monthly_sales.drop(columns = ['sales_duration'])\nmonthly_sales['sales_duration'] = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = monthly_sales.drop(columns = ['item_cnt_month'])\ny = monthly_sales['item_cnt_month']\n\nxgb.fit(X, y)\n\npreds = xgb.predict(new_test.drop(columns=['ID']))\nnew_test['item_cnt_month'] = preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing Submission:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.DataFrame({'ID':new_test['ID'], 'item_cnt_month':new_test['item_cnt_month']})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We don't want values more than 1 in our predictions. So, we round all predictions greater than 1 to 1.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}