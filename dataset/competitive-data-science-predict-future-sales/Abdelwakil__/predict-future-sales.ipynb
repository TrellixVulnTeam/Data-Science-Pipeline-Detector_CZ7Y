{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom IPython.display import Image\nimport os\n!ls ../input/\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **The goal of this notebook was to do something different from the others and to show my abilities to approach a problem and my skills in Python.**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport optuna\nimport joblib\n\nfrom optuna.samplers import TPESampler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\nitem_categories = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Check if there is NaN values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"number_errors=sales_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Creating the Dataset**\n   **The goal of the competition is to predict the future sales for November 2015. At a first glance, we will predict this for October 2015 in order to find the best model possible. We have to create our Dataset. Firstly, we need to compute the sum for each month for each product and shop. We need to be careful and to compute, therefore, the mean of the item price for each month. Also, we have to clip the value between 0 and 20 because the true target values are between those values (constraint from the competition rules). In order to create a model based on the results of the prediction of October 2015, I choose to add for each month the couple (shop_id, item_id) if it does not exist. Moreover, I have to fill the columns \"item_name\" and \"item_price\". For \"item_name\", I will use Label Encoder because there is to many items and OneHotEncoder is too heavy for my computer in this case. For \"item_price\", I will use SimpleImputer (mean) and adding a column describing that we have filled the NaN values.**"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"X = sales_train.groupby([\"date_block_num\",\"shop_id\",\"item_id\"]).agg({\"item_price\":\"mean\",\"item_cnt_day\":\"sum\"})\nX.reset_index(inplace = True)\nX.rename(columns = {\"item_cnt_day\":\"item_cnt_month\"}, inplace = True)\nX = X.merge(items, how = \"left\", on = [\"item_id\"])\nlist_month = list(X[\"date_block_num\"].unique())\nX.set_index(\"date_block_num\", inplace = True)\nX[\"item_cnt_month\"] = X[\"item_cnt_month\"].clip(0,20)\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# For example, here's several helpful packages to load\n\n​\n\nimport numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n​\n\n# Input data files are available in the read-only \"../input/\" directory\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n​\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n\n    for filename in filenames:\n\n        print(os.path.join(dirname, filename))\n\nfrom IPython.display import Image\n\nimport os\n\n!ls ../input/\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nThe goal of this notebook was to do something different from the others and to show my abilities to approach a problem and my skills in Python.\n\nimport numpy as np\n\nimport pandas as pd\n\nimport lightgbm as lgb\n\nimport optuna\n\nimport joblib\n\n​\n\nfrom optuna.samplers import TPESampler\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.impute import SimpleImputer\n\nsales_train = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\n\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\n\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\n\nitem_categories = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\n\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n\n​\n\n\nX_test = X.loc[33,:]\nY_test = X_test.loc[:,\"item_cnt_month\"]\nX_test.drop(\"item_cnt_month\",axis = 1, inplace = True)\nX_oct_2015_concat = X.loc[33,[\"shop_id\", \"item_id\", \"item_name\", \"item_category_id\"]]\nX_oct_2015_concat.reset_index(inplace = True)\nX.reset_index(inplace = True)\nX_new = pd.DataFrame()\nfor i in list_month:\n    X_new = pd.concat([X_new,X[(X[\"date_block_num\"] == int(i))]], axis = 0) #I create the missing couples shop_id, item_id of October 2015 for each month \n    X_oct_2015_concat[\"date_block_num\"].replace(33, int(i), inplace = True) \n    X_new = pd.concat([X_new,X_oct_2015_concat], axis = 0)\nX_new.drop_duplicates(subset = [\"shop_id\",\"item_id\",\"date_block_num\"], inplace = True)\nX_new.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Label Encoder**\n"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"categ_var=(X_new.dtypes==\"object\")\ncateg_var=list(categ_var[categ_var].index)\nlabel_encoder=LabelEncoder()\nfor categ_col in categ_var:\n    X_new[categ_col]=label_encoder.fit_transform(X_new[categ_col])\n    X_test[categ_col]=label_encoder.transform(X_test[categ_col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Missing Values**"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"X_new[\"item_cnt_month\"].fillna(0, inplace = True) #on met 0 si nan pour le nombre de ventes\nX_new[\"item_price_\" + 'missed'] = X_new[\"item_price\"].isnull()\nX_test[\"item_price_\" + 'missed'] = X_test[\"item_price\"].isnull()\nmy_imputer = SimpleImputer()\nX_train = pd.DataFrame(my_imputer.fit_transform(X_new))\nX_train.columns = X_new.columns\nX_train.set_index(\"date_block_num\", inplace = True)\nX_train.drop(labels = 33, inplace = True)\nY_train = X_train[\"item_cnt_month\"]\nX_train.drop([\"item_cnt_month\",\"item_price_missed\"], axis = 1, inplace = True)\nX_test.drop(\"item_price_missed\", axis = 1, inplace = True)\nprint(Y_train, Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Modelling**\n**I will use Optuna in order to find the best parameters for my model. I choose LightGBM because it is faster than XGBoost and the accuracy are nearly the same.**"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def objective(trial):\n    bagging_freq =  trial.suggest_int('bagging_freq',1,10),\n    min_data_in_leaf =  trial.suggest_int('min_data_in_leaf',50,100),\n    max_depth = trial.suggest_int('max_depth',1,50),\n    learning_rate = trial.suggest_loguniform('learning_rate',0.001,0.1),\n    num_leaves = trial.suggest_int('num_leaves',70,200),\n    num_threads = trial.suggest_int('num_threads',1,50),\n    min_sum_hessian_in_leaf = trial.suggest_int('min_sum_hessian_in_leaf',1,50),\n    lambda_l1=trial.suggest_uniform('lambda_l1',0,5)\n    \n    model = lgb.LGBMRegressor(\n        verbosity = 1,\n        bagging_seed = 0,\n        boost_from_average = 'true',\n        boost = 'gbdt',\n        metric = 'auc',\n        lambda_l1=lambda_l1,\n        bagging_freq = bagging_freq ,\n        min_data_in_leaf = min_data_in_leaf,\n        max_depth = max_depth,\n        learning_rate = learning_rate,\n        num_leaves = num_leaves,\n        num_threads = num_threads,\n        min_sum_hessian_in_leaf = min_sum_hessian_in_leaf)\n\n    model.fit(X_train,Y_train)\n    pred=model.predict(X_test)\n    score_mae=mean_squared_error(Y_test, pred)\n    return np.sqrt(score_mae)\n\nsampler = TPESampler(seed = 10)\nstudy = optuna.create_study(direction = \"minimize\", sampler = sampler)\nstudy.optimize(objective, n_trials = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joblib.dump(study.best_params, \"new.joblib\")\nbest_params=joblib.load('new.joblib')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Train the model with the best parameters**"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model = lgb.LGBMRegressor(\n        verbosity = 1,\n        bagging_seed = 0,\n        boost_from_average = 'true',\n        boost = 'gbdt',\n        metric = 'mse',\n        **best_params,\n        n_estimators=1000)\nmodel.fit(X_train,Y_train,eval_set=[(X_train,Y_train),(X_test,Y_test)])\npred=model.predict(X_test)\nscore_mae=np.sqrt(mean_squared_error(Y_test,pred))\nprint(score_mae)\nlgb.plot_metric(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**We can conclude that the model is underfitting a little but the root mean squared error are quiet good = 2.1576773388831154 knowing that our model is simple. In fact, the best scores for this competition are near 1 when they try to predict for October 2015 and it was because they have done more complex model. The goal of this notebook was to do something different from the others and to show my abilities to approach a problem and my skills in Python.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/resultspredictfuturesales/Capture.PNG\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}