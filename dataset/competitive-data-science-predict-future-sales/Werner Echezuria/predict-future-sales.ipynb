{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n!pip install -i https://test.pypi.org/simple/  litemort==0.1.7\nfrom LiteMORT import *\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nimport time\nimport sys\nimport gc\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"item_categories = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nitems = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nsales_train = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\nsample_submission = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sample_submission.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train['item_price'].max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=sales_train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(sales_train.item_price.min(), sales_train.item_price.max()*1.1)\nsns.boxplot(x=sales_train.item_price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = sales_train[sales_train.item_price<100000]\nsales_train = sales_train[sales_train.item_cnt_day<1001]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is one item with price below zero. Fill it with median."},{"metadata":{"trusted":true},"cell_type":"code","source":"median = sales_train[(sales_train.shop_id==32)&(sales_train.item_id==2973)&(sales_train.date_block_num==4)&(sales_train.item_price>0)].item_price.median()\nsales_train.loc[sales_train.item_price<0, 'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Several shops are duplicates of each other (according to its name). Fix train and test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Якутск Орджоникидзе, 56\nsales_train.loc[sales_train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# Якутск ТЦ \"Центральный\"\nsales_train.loc[sales_train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# Жуковский ул. Чкалова 39м²\nsales_train.loc[sales_train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\nitem_categories['split'] = item_categories['item_category_name'].str.split('-')\nitem_categories['type'] = item_categories['split'].map(lambda x: x[0].strip())\nitem_categories['type_code'] = LabelEncoder().fit_transform(item_categories['type'])\n# if subtype is nan then type\nitem_categories['subtype'] = item_categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitem_categories['subtype_code'] = LabelEncoder().fit_transform(item_categories['subtype'])\nitem_categories = item_categories[['item_category_id','type_code', 'subtype_code']]\n\nitems = pd.merge(items, item_categories, on=['item_category_id'], how='left')\nsales_train = pd.merge(sales_train, shops, on=['shop_id'], how='left')\nsales_train = pd.merge(sales_train, items, on=['item_id'], how='left')\ntest = pd.merge(test, shops, on=['shop_id'], how='left')\ntest = pd.merge(test, items, on=['item_id'], how='left')\n\nsales_train.drop(['item_name'], axis=1, inplace=True)\ntest.drop(['item_name'], axis=1, inplace=True)\ndel shops\ndel item_categories\ndel items\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first thing I thought was grouping the sales_train products by month and shop_id, considering that date_block_num is a number representing month. I decided to drop the date column, does not seems important, because the date is for a day and I need for month.\n\nAnd it looks like I need to multiply item_price by item_cnt_day to get total sales."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = sales_train.drop(['date'], axis = 1)\ngrouped_train = train.groupby(['date_block_num', 'shop_id', 'item_id', 'item_price'], \n                              as_index=False).sum()\ngrouped_train['total_sales'] = grouped_train['item_price'] * grouped_train['item_cnt_day']\nsales_train.shape, grouped_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sales_train\ndel train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_in_test = grouped_train[grouped_train['item_id'].isin(test['item_id'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del grouped_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\", \"int16\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int8)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"downcast_train = downcast_dtypes(train_in_test)\ndowncast_test = downcast_dtypes(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_in_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"downcast_train.shape, downcast_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"downcast_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_stats = downcast_train.describe()\ntrain_stats = train_stats.transpose()\ndef norm(x):\n  return (x - train_stats['mean']) / train_stats['std']\nnormed_train_data = norm(downcast_train)\nnormed_test_data = norm(downcast_test)\nnormed_test_data=normed_test_data.drop(['ID', 'date_block_num', 'item_cnt_day', 'item_price', 'total_sales'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normed_train_data['date_block_num'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normed_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normed_test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = normed_train_data[normed_train_data.date_block_num < 1.4].drop(['item_price', 'item_cnt_day', 'total_sales'], axis=1)\nY_train = normed_train_data[normed_train_data.date_block_num < 1.4]['total_sales']\nX_valid = normed_train_data[normed_train_data.date_block_num > 1.4].drop(['item_price', 'item_cnt_day', 'total_sales'], axis=1)\nY_valid = normed_train_data[normed_train_data.date_block_num > 1.4]['total_sales']\nX_test = normed_test_data[['shop_id', 'item_id', 'city_code', 'item_category_id', 'type_code', 'subtype_code']]\nX_test.insert(0, 'date_block_num', 1.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del downcast_train\ndel downcast_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.fillna(0, inplace=True)\nY_train.fillna(0, inplace=True)\nX_test.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params={'num_leaves': 550,   \n        'n_estimators':1000,\n        'early_stopping_rounds':20,\n        'feature_fraction': 1,     \n        'bagging_fraction': 1,\n        'max_bin': 512,\n        'max_depth': 10,\n        'min_child_weight': 300,    #'min_data_in_leaf': 300,\n        'learning_rate': 0.1,\n        'objective': 'regression',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': {'rmse'}\n}\n\nprint(f\"Call LiteMORT... \")    \nt0=time.time()\nmodel = LiteMORT(params).fit(X_train,Y_train,eval_set=[(X_valid, Y_valid)])\nprint(f\"LiteMORT......OK time={time.time()-t0:.4g} model={model}\")\n\n#Y_pred = model.predict(X_valid).clip(0, 20)\n#score = np.sqrt(mean_squared_error(Y_pred, Y_valid))\n#Y_test = model.predict(X_test).clip(0, 20)\n#print(f\"score={score}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = XGBRegressor(\n#    max_depth=8,\n#    n_estimators=1000,\n#    min_child_weight=300,\n#    colsample_bytree=0.8, \n#    subsample=0.8, \n#    eta=0.3,    \n#    seed=42)\n\n#model.fit(\n#    X_train, \n#    Y_train, \n#    eval_metric=\"rmse\", \n#    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n#    verbose=True, \n#    early_stopping_rounds = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nY_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\nscore = np.sqrt(mean_squared_error(Y_pred, Y_valid))\nprint(score)\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}