{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \nfrom pandas import DataFrame\n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\nimport category_encoders as ce\n\n# Algorithms and models:\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Data categorization/encoding:\n\nimport category_encoders as ce ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-16T00:37:25.864393Z","iopub.execute_input":"2021-09-16T00:37:25.864724Z","iopub.status.idle":"2021-09-16T00:37:25.877418Z","shell.execute_reply.started":"2021-09-16T00:37:25.864695Z","shell.execute_reply":"2021-09-16T00:37:25.876436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '/kaggle/input/competitive-data-science-predict-future-sales/'\n\ntrain_df  = pd.read_csv(data_path + 'sales_train.csv')\ntest_df = pd.read_csv(data_path + 'test.csv')\n\ntrain_df.info()\ntest_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:25.878802Z","iopub.execute_input":"2021-09-16T00:37:25.879412Z","iopub.status.idle":"2021-09-16T00:37:27.228226Z","shell.execute_reply.started":"2021-09-16T00:37:25.879376Z","shell.execute_reply":"2021-09-16T00:37:27.227114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:27.229549Z","iopub.execute_input":"2021-09-16T00:37:27.229849Z","iopub.status.idle":"2021-09-16T00:37:27.241798Z","shell.execute_reply.started":"2021-09-16T00:37:27.229809Z","shell.execute_reply":"2021-09-16T00:37:27.240872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:27.242977Z","iopub.execute_input":"2021-09-16T00:37:27.243283Z","iopub.status.idle":"2021-09-16T00:37:27.257999Z","shell.execute_reply.started":"2021-09-16T00:37:27.243245Z","shell.execute_reply":"2021-09-16T00:37:27.257122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We put the date feature from train_df in the proper format:\n\ntrain_df['date']  =  pd.to_datetime(train_df['date'], format = '%d.%m.%Y')\n\n# Let's check if there are missing values in train_df:\n\ntotal = train_df.isnull().sum().sort_values(ascending=False)\npercent_1 = train_df.isnull().sum()/train_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:27.260032Z","iopub.execute_input":"2021-09-16T00:37:27.260309Z","iopub.status.idle":"2021-09-16T00:37:27.845182Z","shell.execute_reply.started":"2021-09-16T00:37:27.260278Z","shell.execute_reply":"2021-09-16T00:37:27.84434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are no missing values, let's look for outliers:\n\nplt.scatter(train_df['item_price'].unique(),train_df.groupby(['item_price']).sum()['item_cnt_day'])","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:27.846289Z","iopub.execute_input":"2021-09-16T00:37:27.846626Z","iopub.status.idle":"2021-09-16T00:37:28.374345Z","shell.execute_reply.started":"2021-09-16T00:37:27.846589Z","shell.execute_reply":"2021-09-16T00:37:28.37379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there is one outlier, we remove it\n\ntrain_df.drop(train_df[(train_df['item_price'] >200000)].index, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:28.375545Z","iopub.execute_input":"2021-09-16T00:37:28.37596Z","iopub.status.idle":"2021-09-16T00:37:28.52407Z","shell.execute_reply.started":"2021-09-16T00:37:28.375897Z","shell.execute_reply":"2021-09-16T00:37:28.523256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We check if there are nonsense item prices (minor than 0)\n\ntrain_df[(train_df['item_price']<0)]","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:28.525228Z","iopub.execute_input":"2021-09-16T00:37:28.525454Z","iopub.status.idle":"2021-09-16T00:37:28.543093Z","shell.execute_reply.started":"2021-09-16T00:37:28.52543Z","shell.execute_reply":"2021-09-16T00:37:28.54175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We remove this outlier too \n\ntrain_df.drop(train_df[train_df['item_price'] < 0].index, inplace = True)\n\n# Sold items cannot be negative, we remove these outliers too\n\ntrain_df.drop(train_df[train_df['item_cnt_day'] < 0].index, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:28.545959Z","iopub.execute_input":"2021-09-16T00:37:28.546234Z","iopub.status.idle":"2021-09-16T00:37:29.079498Z","shell.execute_reply.started":"2021-09-16T00:37:28.546199Z","shell.execute_reply":"2021-09-16T00:37:29.078745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:29.081118Z","iopub.execute_input":"2021-09-16T00:37:29.081708Z","iopub.status.idle":"2021-09-16T00:37:29.119996Z","shell.execute_reply.started":"2021-09-16T00:37:29.081667Z","shell.execute_reply":"2021-09-16T00:37:29.119188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:29.121269Z","iopub.execute_input":"2021-09-16T00:37:29.121483Z","iopub.status.idle":"2021-09-16T00:37:29.137795Z","shell.execute_reply.started":"2021-09-16T00:37:29.121459Z","shell.execute_reply":"2021-09-16T00:37:29.136994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some feature engineering:\n\ntrain_df['year'] = train_df['date'].dt.year\ntrain_df['month'] = train_df['date'].dt.month\ntrain_df['day'] = train_df['date'].dt.day\ntrain_df['year'] = train_df['year'].replace({2013:1,2014:2,2015:3})\n\n# We add categories: \n\nitems_df = pd.read_csv(data_path + \"items.csv\")\ntrain_df = pd.merge(train_df, items_df[['item_id','item_category_id']], how='left', on=['item_id'])\n\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:29.138929Z","iopub.execute_input":"2021-09-16T00:37:29.139264Z","iopub.status.idle":"2021-09-16T00:37:30.485731Z","shell.execute_reply.started":"2021-09-16T00:37:29.139234Z","shell.execute_reply":"2021-09-16T00:37:30.484969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df[['date','year','month','day','date_block_num','shop_id','item_id','item_price','item_category_id','item_cnt_day']]","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:30.486861Z","iopub.execute_input":"2021-09-16T00:37:30.487096Z","iopub.status.idle":"2021-09-16T00:37:30.712243Z","shell.execute_reply.started":"2021-09-16T00:37:30.48707Z","shell.execute_reply":"2021-09-16T00:37:30.711572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:30.714475Z","iopub.execute_input":"2021-09-16T00:37:30.715206Z","iopub.status.idle":"2021-09-16T00:37:30.721889Z","shell.execute_reply.started":"2021-09-16T00:37:30.715146Z","shell.execute_reply":"2021-09-16T00:37:30.721123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"Shop_id\" is an integer number, but actually it is just a label. We should encode it in a more logical way.\n# For this we are going to encode it based on the item_price:\n\nencoder=ce.TargetEncoder(cols='shop_id') \ntrain_df['shop_id'] = encoder.fit_transform(train_df['shop_id'],train_df['item_price'])","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:30.723375Z","iopub.execute_input":"2021-09-16T00:37:30.723679Z","iopub.status.idle":"2021-09-16T00:37:31.414652Z","shell.execute_reply.started":"2021-09-16T00:37:30.723641Z","shell.execute_reply":"2021-09-16T00:37:31.414007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same thing for the item_id:\n\nencoder=ce.TargetEncoder(cols='item_id') \ntrain_df['item_id'] = encoder.fit_transform(train_df['item_id'],train_df['item_price'])\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:31.415907Z","iopub.execute_input":"2021-09-16T00:37:31.416206Z","iopub.status.idle":"2021-09-16T00:37:32.248543Z","shell.execute_reply.started":"2021-09-16T00:37:31.416175Z","shell.execute_reply":"2021-09-16T00:37:32.247714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train_df seems to be ready. Let's work with test_df:\n# We need to add \"year\", \"month\", \"date_block_num\" and \"item_price\" for it to be consistent with the features from \"train_df\".\n# \"Year\" is clear (2015), month is clear (11), \"date_block_num\" is clear (34).\n\ntest_df.insert(0,'year',2015,True)\ntest_df.insert(1,'month',11,True)\ntest_df.insert(2,'date_block_num',34,True)\ntest_df.drop(['ID'], axis=1, inplace=True)\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:32.249923Z","iopub.execute_input":"2021-09-16T00:37:32.25013Z","iopub.status.idle":"2021-09-16T00:37:32.27306Z","shell.execute_reply.started":"2021-09-16T00:37:32.250107Z","shell.execute_reply":"2021-09-16T00:37:32.272186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For \"item_price\" we will start by adding the last price of the \"item_id\". It will not fill all the rows, but at least part of it:\n\nitem_price=dict(train_df.groupby('item_id')['item_price'].last('1D').reset_index().values)\ntest_df['item_price']=test_df['item_id'].map(item_price)\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:32.274313Z","iopub.execute_input":"2021-09-16T00:37:32.27535Z","iopub.status.idle":"2021-09-16T00:37:32.429468Z","shell.execute_reply.started":"2021-09-16T00:37:32.275304Z","shell.execute_reply":"2021-09-16T00:37:32.428573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are still NaN values that we will average. For this, we can set the item_price with the price of this item in other shops\n\ntest_df['item_price'].fillna(train_df.groupby(['item_id']).mean()['item_price'],inplace=True)\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:32.430591Z","iopub.execute_input":"2021-09-16T00:37:32.431761Z","iopub.status.idle":"2021-09-16T00:37:32.929561Z","shell.execute_reply.started":"2021-09-16T00:37:32.431713Z","shell.execute_reply":"2021-09-16T00:37:32.928958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:32.930451Z","iopub.execute_input":"2021-09-16T00:37:32.931371Z","iopub.status.idle":"2021-09-16T00:37:32.941288Z","shell.execute_reply.started":"2021-09-16T00:37:32.931332Z","shell.execute_reply":"2021-09-16T00:37:32.940243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are still nan values in the price from \"test_df\", corresponding to items that whether are new or have been never sold. \n# We can fill these NaN values with the average of the products from his category. For this:\n\n# 1) We first read the items.csv file\n\nitems_df = pd.read_csv(data_path + \"items.csv\")\nitems_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:32.942215Z","iopub.execute_input":"2021-09-16T00:37:32.942606Z","iopub.status.idle":"2021-09-16T00:37:32.999608Z","shell.execute_reply.started":"2021-09-16T00:37:32.942574Z","shell.execute_reply":"2021-09-16T00:37:32.998607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2) We add the category to each item from the test:\n\ntest_df = pd.merge(test_df, items_df[['item_id','item_category_id']], how='left', on=['item_id'])\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:33.000979Z","iopub.execute_input":"2021-09-16T00:37:33.001231Z","iopub.status.idle":"2021-09-16T00:37:33.040625Z","shell.execute_reply.started":"2021-09-16T00:37:33.001203Z","shell.execute_reply":"2021-09-16T00:37:33.039964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We average the price of every category in a new dataframe:\n\ncategory_prices_df = train_df[['item_category_id','item_price']].groupby(['item_category_id']).mean().reset_index()\ncategory_prices_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:33.04167Z","iopub.execute_input":"2021-09-16T00:37:33.042018Z","iopub.status.idle":"2021-09-16T00:37:33.143608Z","shell.execute_reply.started":"2021-09-16T00:37:33.041991Z","shell.execute_reply":"2021-09-16T00:37:33.142928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# And we merge it with the NaN values from \"test_df\"\n\ntest_df = pd.merge(test_df, category_prices_df, how='left', on=['item_category_id'])\ntest_df['item_price_x'].fillna(test_df['item_price_y'], inplace=True)\ntest_df.drop('item_price_y', axis=1, inplace=True)\ntest_df=test_df.rename(columns = {'item_price_x':'item_price'})\ntest_df['year']=3\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:33.144838Z","iopub.execute_input":"2021-09-16T00:37:33.145196Z","iopub.status.idle":"2021-09-16T00:37:33.201863Z","shell.execute_reply.started":"2021-09-16T00:37:33.145168Z","shell.execute_reply":"2021-09-16T00:37:33.201265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finally we confirm that there are no NaN values anymore in test_df\n\ntest_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:33.20279Z","iopub.execute_input":"2021-09-16T00:37:33.203151Z","iopub.status.idle":"2021-09-16T00:37:33.214403Z","shell.execute_reply.started":"2021-09-16T00:37:33.203125Z","shell.execute_reply":"2021-09-16T00:37:33.213425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:33.215584Z","iopub.execute_input":"2021-09-16T00:37:33.215942Z","iopub.status.idle":"2021-09-16T00:37:33.235759Z","shell.execute_reply.started":"2021-09-16T00:37:33.215891Z","shell.execute_reply":"2021-09-16T00:37:33.23513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:33.236874Z","iopub.execute_input":"2021-09-16T00:37:33.23721Z","iopub.status.idle":"2021-09-16T00:37:33.260182Z","shell.execute_reply.started":"2021-09-16T00:37:33.237182Z","shell.execute_reply":"2021-09-16T00:37:33.259195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the feature \"shop_id\" needs to be encoded yet, to be consistent with \"train_df\"\n\nencoder=ce.TargetEncoder(cols='shop_id')\ntest_df['shop_id'] = encoder.fit_transform(test_df['shop_id'],test_df['item_price'])\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:33.261516Z","iopub.execute_input":"2021-09-16T00:37:33.261972Z","iopub.status.idle":"2021-09-16T00:37:33.348617Z","shell.execute_reply.started":"2021-09-16T00:37:33.261927Z","shell.execute_reply":"2021-09-16T00:37:33.34799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same thing for \"item_id\":\n\nencoder=ce.TargetEncoder(cols='item_id')\ntest_df['item_id'] = encoder.fit_transform(test_df['item_id'],test_df['item_price'])\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:33.349851Z","iopub.execute_input":"2021-09-16T00:37:33.350308Z","iopub.status.idle":"2021-09-16T00:37:33.441965Z","shell.execute_reply.started":"2021-09-16T00:37:33.350274Z","shell.execute_reply":"2021-09-16T00:37:33.441066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_df.drop(['date','day','item_cnt_day'], axis=1)\n\n# The target values from \"train_df\" are too broad. We will normalize it using log:\n\ny = np.log(train_df['item_cnt_day'])\ny = y.fillna(y.median())\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T00:37:33.443305Z","iopub.execute_input":"2021-09-16T00:37:33.443727Z","iopub.status.idle":"2021-09-16T00:37:34.233955Z","shell.execute_reply.started":"2021-09-16T00:37:33.443689Z","shell.execute_reply":"2021-09-16T00:37:34.233162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HYPERPARAMETER OPTIMIZATION FOR XGBOOST:\n\n# Hyperparameters I tried with cross validation by means of XGBRegressor:\n\n# 'max_depth': [4,6,10],\n# 'learning_rate': [0.01, 0.1, 1],\n# 'n_estimators': [600, 1500, 2000],\n# 'colsample_bytree': [0.5,0.7,1],\n# 'tree_method': ['gpu_hist'],\n# 'alpha': [0,0.01,0.1],\n# 'gamma': [0, 0.1, 0.5],\n# 'eta': [0.01,0.1,0.2],\n# 'min_child_weight': [1,2,6]\n\n\n# The best hyperparameters turned out to be: Best parameters: {'alpha': 0, 'colsample_bytree': 0.5, 'eta': 0.01, 'gamma': 0.1, \n# 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 1500, 'tree_method': 'gpu_hist'}\n\n\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\n\nparams = { 'max_depth': [4],\n           'learning_rate': [0.1],\n           'n_estimators': [1500],\n           'colsample_bytree': [0.5],\n           'tree_method': ['gpu_hist'],\n           'alpha': [0],\n           'gamma': [0.1],\n           'eta': [0.01],\n           'min_child_weight': [1]\n            }\nxgbr = xgb.XGBRegressor(seed = 20)\nclf = GridSearchCV(estimator=xgbr, \n                   param_grid=params,\n                   scoring='neg_mean_squared_error', \n                   verbose=3)\nclf.fit(X, y)\nprint(\"Best parameters:\", clf.best_params_)\nprint(\"Lowest RMSE: \", (-clf.best_score_)**(1/2.0))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fitting 5 folds for each of 1 candidates, totalling 5 fits\n[CV] alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist \n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[CV]  alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist, score=-0.079, total=  58.4s\n[CV] alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist \n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   58.4s remaining:    0.0s\n[CV]  alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist, score=-0.096, total=  55.4s\n[CV] alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist \n[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.9min remaining:    0.0s\n[CV]  alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist, score=-0.080, total=  52.7s\n[CV] alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist \n[CV]  alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist, score=-0.093, total=  55.1s\n[CV] alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist \n[CV]  alpha=0, colsample_bytree=0.5, eta=0.01, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=1500, tree_method=gpu_hist, score=-0.087, total= 1.0min\n[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.7min finished\nBest parameters: {'alpha': 0, 'colsample_bytree': 0.5, 'eta': 0.01, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 1500, 'tree_method': 'gpu_hist'}\nLowest RMSE:  0.2946685995050872","metadata":{}},{"cell_type":"code","source":"results_df = clf.predict(test_df)\n\n# We undo the normalized results by means of the log:\n\nresults_df = pow(10, results_df)\n\ndf = pd.DataFrame(data=results_df, columns=[\"item_cnt_month\"])\ndf['ID'] = np.arange(len(df))\ndf.to_csv('final_file', sep=',', index=False, index_label=False )","metadata":{},"execution_count":null,"outputs":[]}]}