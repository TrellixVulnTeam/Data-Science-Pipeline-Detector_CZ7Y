{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#!pip install chart_studio\n#import chart_studio.plotly as py\n#import chart_studio.plotly as go\n#import plotly.plotly as py\n#import plotly.graph_objs as go\n#from plotly.offline import init_notebook_mode, iplot\n\nimport catboost\nfrom catboost import Pool\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sample = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nitem_categories = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\ntrain = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv', parse_dates=['date'])\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train.join(items, on='item_id', rsuffix='_').join(shops, on='shop_id', rsuffix='_').join(item_categories, on='item_category_id', rsuffix='_').drop(['item_id_', 'shop_id_', 'item_category_id_'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_shops = test.shop_id.unique()\ntrain_data = train_data[train_data.shop_id.isin(test_shops)]\ntest_items = test.item_id.unique()\ntrain_data = train_data[train_data.item_id.isin(test_items)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Features = ['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly = train_data[['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'item_cnt_day']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Result= []\nfor col in Features:\n    dp = train_data.filter(like=col)\n    Result.append(dp)\n    df = pd.concat((Result),axis=1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by month in this case \"date_block_num\" and aggregate features.\ntrain_monthly = train_monthly.sort_values('date').groupby(['date_block_num', 'shop_id', 'item_category_id', 'item_id'], as_index=False)\ntrain_monthly = train_monthly.agg({'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']})\n# Rename features.\ntrain_monthly.columns = ['date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'mean_item_price', 'item_cnt', 'mean_item_cnt', 'transactions']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a data set with all the possible combinations of ['date_block_num','shop_id','item_id'] so we won't have missing records.\nshop_ids = train_monthly['shop_id'].unique()\nitem_ids = train_monthly['item_id'].unique()\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge the train set with the complete set (missing records will be filled with 0).\ntrain_monthly = pd.merge(empty_df, train_monthly, on=['date_block_num','shop_id','item_id'], how='left')\ntrain_monthly.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract time based features.\ntrain_monthly['year'] = train_monthly['date_block_num'].apply(lambda x: ((x//12) + 2013))\ntrain_monthly['month'] = train_monthly['date_block_num'].apply(lambda x: (x % 12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping data for EDA.\ngp_month_mean = train_monthly.groupby(['month'], as_index=False)['item_cnt'].mean()\ngp_month_sum = train_monthly.groupby(['month'], as_index=False)['item_cnt'].sum()\ngp_category_mean = train_monthly.groupby(['item_category_id'], as_index=False)['item_cnt'].mean()\ngp_category_sum = train_monthly.groupby(['item_category_id'], as_index=False)['item_cnt'].sum()\ngp_shop_mean = train_monthly.groupby(['shop_id'], as_index=False)['item_cnt'].mean()\ngp_shop_sum = train_monthly.groupby(['shop_id'], as_index=False)['item_cnt'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly = train_monthly.query('item_cnt >= 0 and item_cnt <= 20 and item_price < 400000')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"def series_to_supervised(data, window=1, lag=1, dropnan=True):\n    cols, names = list(), list()\n    # Input sequence (t-n, ... t-1)\n    for i in range(window, 0, -1):\n        cols.append(data.shift(i))\n        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n    # Current timestep (t=0)\n    cols.append(data)\n    names += [('%s(t)' % (col)) for col in data.columns]\n    # Target timestep (t=lag)\n    cols.append(data.shift(-lag))\n    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n    # Put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # Drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"lag_size = (test['date'].max().date() - train['date'].max().date()).days\nprint('Max date from train set: %s' % train['date'].max().date())\nprint('Max date from test set: %s' % test['date'].max().date())\nprint('Forecast lag size', lag_size)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"window = 29\nlag = lag_size\nseries = series_to_supervised(train_monthly.drop('date', axis=1), window=window, lag=lag)\nseries.head()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly['item_cnt_month'] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_id'])['item_cnt'].shift(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly['item_price_unit'] = train_monthly['item_price'] // train_monthly['item_cnt']\ntrain_monthly['item_price_unit'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gp_item_price = train_monthly.sort_values('date_block_num').groupby(['item_id'], as_index=False).agg({'item_price':[np.min, np.max]})\ngp_item_price.columns = ['item_id', 'hist_min_item_price', 'hist_max_item_price']\n\ntrain_monthly = pd.merge(train_monthly, gp_item_price, on='item_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly['price_increase'] = train_monthly['item_price'] - train_monthly['hist_min_item_price']\ntrain_monthly['price_decrease'] = train_monthly['hist_max_item_price'] - train_monthly['item_price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Min value\nf_min = lambda x: x.rolling(window=3, min_periods=1).min()\n# Max value\nf_max = lambda x: x.rolling(window=3, min_periods=1).max()\n# Mean value\nf_mean = lambda x: x.rolling(window=3, min_periods=1).mean()\n# Standard deviation\nf_std = lambda x: x.rolling(window=3, min_periods=1).std()\n\nfunction_list = [f_min, f_max, f_mean, f_std]\nfunction_name = ['min', 'max', 'mean', 'std']\n\nfor i in range(len(function_list)):\n    train_monthly[('item_cnt_%s' % function_name[i])] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].apply(function_list[i])\n\n# Fill the empty std features with 0\ntrain_monthly['item_cnt_std'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_list = [1, 2, 3]\n\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly[ft_name] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].shift(lag)\n    # Fill the empty shifted features with 0\n    train_monthly[ft_name].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly['item_trend'] = train_monthly['item_cnt']\n\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly['item_trend'] -= train_monthly[ft_name]\n\ntrain_monthly['item_trend'] /= len(lag_list) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = train_monthly.query('date_block_num >= 3 and date_block_num < 28').copy()\nvalidation_set = train_monthly.query('date_block_num >= 28 and date_block_num < 33').copy()\ntest_set = train_monthly.query('date_block_num == 33').copy()\n\ntrain_set.dropna(subset=['item_cnt_month'], inplace=True)\nvalidation_set.dropna(subset=['item_cnt_month'], inplace=True)\n\ntrain_set.dropna(inplace=True)\nvalidation_set.dropna(inplace=True)\n\nprint('Train set records:', train_set.shape[0])\nprint('Validation set records:', validation_set.shape[0])\nprint('Test set records:', test_set.shape[0])\n\nprint('Train set records: %s (%.f%% of complete data)' % (train_set.shape[0], ((train_set.shape[0]/train_monthly.shape[0])*100)))\nprint('Validation set records: %s (%.f%% of complete data)' % (validation_set.shape[0], ((validation_set.shape[0]/train_monthly.shape[0])*100)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shop mean encoding.\ngp_shop_mean = train_set.groupby(['shop_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_mean.columns = ['shop_mean']\ngp_shop_mean.reset_index(inplace=True)\n# Item mean encoding.\ngp_item_mean = train_set.groupby(['item_id']).agg({'item_cnt_month': ['mean']})\ngp_item_mean.columns = ['item_mean']\ngp_item_mean.reset_index(inplace=True)\n# Shop with item mean encoding.\ngp_shop_item_mean = train_set.groupby(['shop_id', 'item_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_item_mean.columns = ['shop_item_mean']\ngp_shop_item_mean.reset_index(inplace=True)\n# Year mean encoding.\ngp_year_mean = train_set.groupby(['year']).agg({'item_cnt_month': ['mean']})\ngp_year_mean.columns = ['year_mean']\ngp_year_mean.reset_index(inplace=True)\n# Month mean encoding.\ngp_month_mean = train_set.groupby(['month']).agg({'item_cnt_month': ['mean']})\ngp_month_mean.columns = ['month_mean']\ngp_month_mean.reset_index(inplace=True)\n\n# Add meand encoding features to train set.\ntrain_set = pd.merge(train_set, gp_shop_mean, on=['shop_id'], how='left')\ntrain_set = pd.merge(train_set, gp_item_mean, on=['item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_year_mean, on=['year'], how='left')\ntrain_set = pd.merge(train_set, gp_month_mean, on=['month'], how='left')\n# Add meand encoding features to validation set.\nvalidation_set = pd.merge(validation_set, gp_shop_mean, on=['shop_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_item_mean, on=['item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_year_mean, on=['year'], how='left')\nvalidation_set = pd.merge(validation_set, gp_month_mean, on=['month'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create train and validation sets and labels. \nX_train = train_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\nY_train = train_set['item_cnt_month'].astype(int)\nX_validation = validation_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\nY_validation = validation_set['item_cnt_month'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Integer features (used by catboost model).\nint_features = ['shop_id', 'item_id', 'year', 'month']\n\nX_train[int_features] = X_train[int_features].astype('int32')\nX_validation[int_features] = X_validation[int_features].astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"latest_records = pd.concat([train_set, validation_set]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\nX_test = pd.merge(test, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])\nX_test['year'] = 2015\nX_test['month'] = 9\nX_test.drop('item_cnt_month', axis=1, inplace=True)\nX_test[int_features] = X_test[int_features].astype('int32')\nX_test = X_test[X_train.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sets = [X_train, X_validation, X_test]\n\n# This was taking too long.\n# Replace missing values with the median of each item.\n# for dataset in sets:\n#     for item_id in dataset['item_id'].unique():\n#         for column in dataset.columns:\n#             item_median = dataset[(dataset['item_id'] == item_id)][column].median()\n#             dataset.loc[(dataset[column].isnull()) & (dataset['item_id'] == item_id), column] = item_median\n\n# Replace missing values with the median of each shop.            \nfor dataset in sets:\n    for shop_id in dataset['shop_id'].unique():\n        for column in dataset.columns:\n            shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n            dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median\n            \n# Fill remaining missing values on test set with mean.\nX_test.fillna(X_test.mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'm dropping \"item_category_id\", we don't have it on test set and would be a little <hard to create categories for items that exist only on test set.\nX_train.drop(['item_category_id'], axis=1, inplace=True)\nX_validation.drop(['item_category_id'], axis=1, inplace=True)\nX_test.drop(['item_category_id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cat_features = [0, 1, 7, 8]\n\ncatboost_model = CatBoostRegressor(\n    iterations=500,\n    max_ctr_complexity=4,\n    random_seed=0,\n    od_type='Iter',\n    od_wait=25,\n    verbose=50,\n    depth=4\n)\n\ncatboost_model.fit(\n    X_train, Y_train,\n    #cat_features=cat_features,\n    eval_set=(X_validation, Y_validation)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_score = pd.DataFrame(list(zip(X_train.dtypes.index, catboost_model.get_feature_importance(Pool(X_train, label=Y_train)))), columns=['Feature','Score']) #, cat_features=cat_features\nfeature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\n\nplt.rcParams[\"figure.figsize\"] = (19, 6)\nax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\nax.set_title(\"Catboost Feature Importance Ranking\", fontsize = 14)\nax.set_xlabel('')\nrects = ax.patches\nlabels = feature_score['Score'].round(2)\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 0.35, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catboost_train_pred = catboost_model.predict(X_train)\ncatboost_val_pred = catboost_model.predict(X_validation)\ncatboost_test_pred = catboost_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, catboost_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, catboost_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_performance_sc_plot(predictions, labels, title):\n    # Get min and max values of the predictions and labels.\n    min_val = max(max(predictions), max(labels))\n    max_val = min(min(predictions), min(labels))\n    # Create dataframe with predicitons and labels.\n    performance_df = pd.DataFrame({\"Label\":labels})\n    performance_df[\"Prediction\"] = predictions\n    # Plot data\n    sns.jointplot(y=\"Label\", x=\"Prediction\", data=performance_df, kind=\"reg\", height=7)\n    plt.plot([min_val, max_val], [min_val, max_val], 'm--')\n    plt.title(title, fontsize=9)\n    plt.show()\n    \n# model_performance_sc_plot(catboost_train_pred, Y_train, 'Train')\n#model_performance_sc_plot(catboost_val_pred, Y_validation, 'Validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use only part of features on XGBoost.\nxgb_features = ['item_cnt','item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', \n                'item_cnt_shifted2', 'item_cnt_shifted3', 'shop_mean', \n                'shop_item_mean', 'item_trend', 'mean_item_cnt']\nxgb_train = X_train[xgb_features]\nxgb_val = X_validation[xgb_features]\nxgb_test = X_test[xgb_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = XGBRegressor(max_depth=8, \n                         n_estimators=500, \n                         min_child_weight=1000,  \n                         colsample_bytree=0.7, \n                         subsample=0.7, \n                         eta=0.3, \n                         seed=0)\nxgb_model.fit(xgb_train, \n              Y_train, \n              eval_metric=\"rmse\", \n              eval_set=[(xgb_train, Y_train), (xgb_val, Y_validation)], \n              verbose=20, \n              early_stopping_rounds=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (15, 6)\nplot_importance(xgb_model)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_train_pred = xgb_model.predict(xgb_train)\nxgb_val_pred = xgb_model.predict(xgb_val)\nxgb_test_pred = xgb_model.predict(xgb_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_performance_sc_plot(xgb_train_pred, Y_train, 'Train')\n#model_performance_sc_plot(xgb_val_pred, Y_validation, 'Validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use only part of features on random forest.\nrf_features = ['shop_id', 'item_id', 'item_cnt', 'transactions', 'year',\n               'item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', \n               'shop_mean', 'item_mean', 'item_trend', 'mean_item_cnt']\nrf_train = X_train[rf_features]\nrf_val = X_validation[rf_features]\nrf_test = X_test[rf_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestRegressor(n_estimators=50, max_depth=7, random_state=0, n_jobs=-1)\nrf_model.fit(rf_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_train_pred = rf_model.predict(rf_train)\nrf_val_pred = rf_model.predict(rf_val)\nrf_test_pred = rf_model.predict(rf_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_performance_sc_plot(rf_train_pred, Y_train, 'Train')\n#model_performance_sc_plot(rf_val_pred, Y_validation, 'Validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use only part of features on linear Regression.\nlr_features = ['item_cnt', 'item_cnt_shifted1', 'item_trend', 'mean_item_cnt', 'shop_mean']\nlr_train = X_train[lr_features]\nlr_val = X_validation[lr_features]\nlr_test = X_test[lr_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_scaler = MinMaxScaler()\nlr_scaler.fit(lr_train)\nlr_train = lr_scaler.transform(lr_train)\nlr_val = lr_scaler.transform(lr_val)\nlr_test = lr_scaler.transform(lr_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model = LinearRegression(n_jobs=-1)\nlr_model.fit(lr_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_train_pred = lr_model.predict(lr_train)\nlr_val_pred = lr_model.predict(lr_val)\nlr_test_pred = lr_model.predict(lr_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, lr_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, lr_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_performance_sc_plot(lr_train_pred, Y_train, 'Train')\n#model_performance_sc_plot(lr_val_pred, Y_validation, 'Validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use only part of features on KNN.\nknn_features = ['item_cnt', 'item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1',\n                'item_cnt_shifted2', 'shop_mean', 'shop_item_mean', \n                'item_trend', 'mean_item_cnt']\n\n# Subsample train set (using the whole data was taking too long).\nX_train_sampled = X_train[:100000]\nY_train_sampled = Y_train[:100000]\n\nknn_train = X_train_sampled[knn_features]\nknn_val = X_validation[knn_features]\nknn_test = X_test[knn_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_scaler = MinMaxScaler()\nknn_scaler.fit(knn_train)\nknn_train = knn_scaler.transform(knn_train)\nknn_val = knn_scaler.transform(knn_val)\nknn_test = knn_scaler.transform(knn_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_model = KNeighborsRegressor(n_neighbors=9, leaf_size=13, n_jobs=-1)\nknn_model.fit(knn_train, Y_train_sampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_train_pred = knn_model.predict(knn_train)\nknn_val_pred = knn_model.predict(knn_val)\nknn_test_pred = knn_model.predict(knn_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train_sampled, knn_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, knn_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_performance_sc_plot(knn_train_pred, Y_train_sampled, 'Train')\n#model_performance_sc_plot(knn_val_pred, Y_validation, 'Validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset that will be the train set of the ensemble model.\nfirst_level = pd.DataFrame(catboost_val_pred, columns=['catboost'])\nfirst_level['xgbm'] = xgb_val_pred\nfirst_level['random_forest'] = rf_val_pred\nfirst_level['linear_regression'] = lr_val_pred\nfirst_level['knn'] = knn_val_pred\nfirst_level['label'] = Y_validation.values\nfirst_level.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset that will be the test set of the ensemble model.\nfirst_level_test = pd.DataFrame(catboost_test_pred, columns=['catboost'])\nfirst_level_test['xgbm'] = xgb_test_pred\nfirst_level_test['random_forest'] = rf_test_pred\nfirst_level_test['linear_regression'] = lr_test_pred\nfirst_level_test['knn'] = knn_test_pred\nfirst_level_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_model = LinearRegression(n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop label from dataset.\nfirst_level.drop('label', axis=1, inplace=True)\nmeta_model.fit(first_level, Y_validation)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_pred = meta_model.predict(first_level)\nfinal_predictions = meta_model.predict(first_level_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train rmse:', np.sqrt(mean_squared_error(ensemble_pred, Y_validation)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_performance_sc_plot(ensemble_pred, Y_validation, 'Validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_df = pd.DataFrame(test['ID'], columns=['ID'])\nprediction_df['item_cnt_month'] = final_predictions.clip(0., 20.)\nprediction_df.to_csv('submission.csv', index=False)\nprediction_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}