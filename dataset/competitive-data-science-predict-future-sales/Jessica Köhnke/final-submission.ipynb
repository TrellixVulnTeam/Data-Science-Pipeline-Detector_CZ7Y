{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport gc\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport pickle\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.cluster import AgglomerativeClustering\nimport Levenshtein\nimport xgboost as xgb\nfrom scipy.spatial.distance import squareform\nfrom scipy.spatial.distance import pdist\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gc.set_debug(gc.DEBUG_LEAK)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def import_data():\n    \"\"\"Import all data from csv files\"\"\"\n    sales = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\n    item_cat = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\n    items = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\n    # sub_sample = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sample_submission.csv\")\n    shops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\n    test = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\n    return sales, item_cat, items, shops, test\n\n\ndef downcast_dtypes(df):\n    \"\"\"Downcast float columns to float32 and int columns to int16\"\"\"\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SALES, ITEM_CAT, ITEMS, SHOPS, TEST = import_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take into account, that some items appear to be the same despite of having slightly different names. Special characters are deleted from the item names and item_id is replaced by item_lab."},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_process_item_data(df):\n    \"\"\"create new item id 'item_lab' to account for duplicates. Of 22k items there are roughly\n    100 duplicates\"\"\"\n    df['name_pre'] = df.item_name.str.replace(r'[\\*\\!\\./,]', '')\n    df['name_pre'] = df.name_pre.str.lower()\n    # df['name_pre'] = df.name_pre.str.replace(r'\\((.*?)\\)', '')  # delete all () brackets\n    # df['name_pre'] = df.name_pre.str.replace(r'\\[(.*?)\\]', '')  # delete all [] brackets\n    df['name_pre'] = df.name_pre.str.replace(r'd$', '').str.strip()\n    df['item_lab'] = df.name_pre.factorize(sort=True)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_process_item_data(ITEMS)\nSALES = SALES.merge(ITEMS[['item_id', 'item_category_id', 'item_lab']], how='left', on='item_id')\n# SALES.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop all thats not needed for aggr, merge again later\nSALES.drop(['item_id', 'date', 'item_category_id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Deal with missing values, aggregate the item count per day to item count per month and add zeros for missing shop / item pairs. \n\nIn the test set are 5100 items * 42 shops = 214200 pairs which suggests, that entries with item_cnt_month == 0 are also included. The train set however does not contain any zeros."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clip_fillna_prices(df):\n    # clip item prices with 99 percentile\n    quantile_99 = df.item_price.quantile(0.99)\n    df.item_price.clip(upper=quantile_99, inplace=True)\n\n    # Fix single item price == -1 value with mean of same month, shop and item\n    idx = df[df.item_price < 0].index.tolist()\n    df.at[idx[0], 'item_price'] = (2499. + 1249.) / 2.\n\n\ndef aggregate_item_count(df):\n    # Aggregate item count to sum per month and item price to mean\n    train = df.groupby(['date_block_num', 'shop_id', 'item_lab']).agg(\n        item_price=('item_price', 'mean'), item_cnt_month=('item_cnt_day', 'sum')\n    ).reset_index()\n    \n    # clip item_cnt_month to min/max in test set\n    train.item_cnt_month.clip(lower=0, upper=20, inplace=True)\n    \n    return train\n\n\ndef add_missing_item_shop_pairs(df, price):\n    # set item count for missing shop / item pairs per month to zero\n    df.set_index(['date_block_num', 'shop_id', 'item_lab'], inplace=True)\n    idx = []\n    for month in df.index.unique('date_block_num'):\n        shops_unique = df.loc[month].index.unique('shop_id')\n        items_unique = df.loc[month].index.unique('item_lab')\n        idx.append(pd.MultiIndex.from_product([[month], shops_unique, items_unique], names=['date_block_num', 'shop_id', 'item_lab']))\n\n    idx = idx[0].append(idx[1:])\n    df = df.reindex(idx, fill_value=0.)\n\n    # fill missing item prices with mean\n    df = df.reset_index()\n    \n    df = pd.merge(df, price, how='left', on='item_lab', suffixes=('', '_'))\n    df['item_price'] = np.where(df.item_price > 0, df.item_price, df.item_price_)\n    del df['item_price_']\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clip_fillna_prices(SALES)\nPRICE = SALES[['item_lab', 'item_price']][SALES.item_price > 0].groupby('item_lab').agg('mean')\nMATRIX = aggregate_item_count(SALES)\nMATRIX = add_missing_item_shop_pairs(MATRIX, PRICE)\n\nMATRIX = MATRIX.merge(ITEMS[['item_category_id', 'item_lab']].drop_duplicates(), how='left', on='item_lab')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MATRIX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Include test set into matrix, so that feature generation will be consistent over train and test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST['date_block_num'] = 34\nTEST.drop('ID', axis=1, inplace=True)\nTEST['item_cnt_month'] = np.nan\n\nTEST = TEST.merge(ITEMS[['item_id', 'item_lab', 'item_category_id']], how='left', on='item_id')\nTEST = pd.merge(TEST.drop('item_id', axis=1), PRICE, how='left', on='item_lab')\n\nMATRIX = MATRIX.append(TEST, ignore_index=True)\nMATRIX['item_category_id'] = MATRIX.item_category_id.astype('int16')\n# MATRIX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fill missing item prices with mean values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fillna_means(df, col):\n    \"\"\"Fill with means of item_lab, if not available, use means of category\"\"\"\n    \n    # df[col] = df.groupby('item_id')[col].transform(lambda x: x.fillna(x.mean()))\n    df[col] = df.groupby('item_lab')[col].transform(lambda x: x.fillna(x.mean()))\n    df[col] = df.groupby('item_category_id')[col].transform(lambda x: x.fillna(x.mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fillna_means(MATRIX, 'item_price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do some preprocessing of item category data. Category name is split into category 1 and 2 names."},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_process_item_cat_data(df):\n    \"\"\"Do pre processing of item_cat data frame\"\"\"\n    \n    # split name at '-' to seperate to categories and sub-categories\n    cat = df.item_category_name.str.split('-', n=1, expand=True)\n    df['cat1'] = cat[0].str.strip().str.lower()\n    df['cat2'] = cat[1].str.strip().str.lower()\n\n    df.cat1.fillna('', inplace=True)\n    df.cat2.fillna('', inplace=True)\n    \n    df['cat1_lab'] = df.cat1.factorize(sort=True)[0]\n    df['cat2_lab'] = df.cat2.factorize(sort=True)[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Levenshteining, the category names are clustered into groups with similar category names. The new features cat1_lev and cat2_lev are created. They contain labels of the clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_string_correlation(array):\n    def dist(x, y):\n        lev = Levenshtein.distance(x[0],y[0])\n        m = np.mean([len(x[0]), len(y[0])])\n        return lev / m\n    cor = pdist(array.reshape(-1, 1), dist)\n    return squareform(cor)\n\n\ndef agglomerative_clustering(distance_matrix, num_clusters=None, threshold=None):\n    model = AgglomerativeClustering(n_clusters=num_clusters, affinity='precomputed', linkage='average', distance_threshold=threshold)\n    model.fit(distance_matrix)\n    return model.labels_\n\n\ndef add_levenshtein_feature(df, cols):\n    for col in cols:\n        categories = df[col].unique()\n        corr = get_string_correlation(categories)\n        labels = agglomerative_clustering(corr, threshold=0.5)\n        features = pd.DataFrame(np.array([categories, labels]).T, columns=[col, col + '_lev'])\n        df = df.merge(features, how='left', on=col)\n        df[col + '_lev'] = df[col + '_lev'].astype('int32')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_process_item_cat_data(ITEM_CAT)\nITEM_CAT = add_levenshtein_feature(ITEM_CAT, ['cat1', 'cat2'])\nMATRIX = MATRIX.merge(ITEM_CAT[['item_category_id', 'cat1_lab', 'cat2_lab', 'cat1_lev', 'cat2_lev']], how='left',\n                      on='item_category_id')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MATRIX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preproessing of shop data. Features city_id and shopping_center are created."},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_process_shop_data(df):\n    split = df.shop_name.str.split(n=1, expand=True)\n    df['city'] = split[0].str.lower().str.strip()\n    df['city_id'] = df.city.factorize(sort=True)[0]\n    df['shopping_center'] = split[1].str.lower().str.contains('ТЦ'.lower())\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_process_shop_data(SHOPS)\nMATRIX = MATRIX.merge(SHOPS[['shop_id', 'city_id', 'shopping_center']], how='left', on='shop_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The shop revenue per month is added. Since the shop revenue will not be able for the 34th month, only a shop revenue lag feature will be used"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_shop_revenue(df):\n    # tmp = df.loc[:, ['shop_id', 'date_block_num', 'item_price', 'item_cnt_month']]\n    df['revenue'] = df.item_price * df.item_cnt_month\n    df['shop_revenue_month'] = df.groupby(['shop_id', 'date_block_num'])['revenue'].transform('sum')\n    del df['revenue']\n\n# Delete this feature later on, only use lag feature (since revenue will not be available for test set)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_shop_revenue(MATRIX)\n# MATRIX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add lag features for 1, 2 and 12 months."},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature(df, lags, col):\n    \"\"\"Add lag features based on date_block_num to test and train df\"\"\"\n    tmp = df[['date_block_num', 'shop_id', 'item_lab', col]]\n    \n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num', 'shop_id', 'item_lab', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        \n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_lab'], how='left')\n        df[col+'_lag_'+str(i)].fillna(0, inplace=True)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MATRIX = lag_feature(MATRIX, (1, 2, 12), 'item_cnt_month')\nMATRIX = lag_feature(MATRIX, (1, 2, 12), 'shop_revenue_month')\ndel MATRIX['shop_revenue_month']\n# MATRIX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add month"},{"metadata":{"trusted":true},"cell_type":"code","source":"MATRIX['month'] = MATRIX.date_block_num.mod(12)\n# MATRIX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add some advanced features"},{"metadata":{},"cell_type":"markdown","source":"Time since item was first released"},{"metadata":{"trusted":true},"cell_type":"code","source":"MATRIX['release'] = MATRIX.item_lab.map(\n    MATRIX[MATRIX.item_cnt_month > 0].groupby(['item_lab']).agg(release=('date_block_num', 'min')).loc[:,'release']\n)\nMATRIX['since_release'] = MATRIX.date_block_num - MATRIX.release\n\n# substitute nans with 0 (since_release == nan -> release == nan -> item not seen before)\nMATRIX['since_release'] = MATRIX.since_release.fillna(0)\n\n# delete all entries where since_release is negative (item not released yet)\n# those entries stem from the added item/shop pairs with item_cnt_month == 0\nMATRIX = MATRIX[MATRIX.since_release >= 0]\ndel MATRIX['release']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add target mean as feature. To get the correct value for the test set, use leaderbord probing:\n(https://www.kaggle.com/c/competitive-data-science-predict-future-sales/discussion/79142)\n\n* N=214200\n* Predict 0: mse_0 = 1.25011 ** 2\n* Predict 1: mse_1 = 1.41241 ** 2\n\nsum_y_test = (mse_1 - mse_0 - 1) / -2\n\nmean_y_test = sum_y_true / N\n\nmean_y_test = 0.28394"},{"metadata":{"trusted":true},"cell_type":"code","source":"MATRIX['mean_item_cnt_month'] = MATRIX.groupby('date_block_num')['item_cnt_month'].transform('mean')\nMATRIX[MATRIX.item_cnt_month == 34].mean_item_cnt_month = 0.28394\n# MATRIX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split data into train, validation and test set. Since the 12 month lag features are only available starting in date_block_num == 12, the previous data is not used for training.\nValidation is done with month number 33, the test set is month number 34."},{"metadata":{"trusted":true},"cell_type":"code","source":"IDX_TRAIN = (MATRIX.date_block_num > 11) & (MATRIX.date_block_num < 33)\nIDX_VAL = MATRIX.date_block_num == 33\nIDX_TEST = MATRIX.date_block_num == 34","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Max count of item in whole data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_max_cnt(df):\n    max_cnt = df.loc[IDX_TRAIN, ['item_lab', 'shop_id', 'item_cnt_month']].groupby(['item_lab', 'shop_id']).agg(\n        max_cnt=('item_cnt_month', 'max')).reset_index()\n    df = df.merge(max_cnt, how='left', on=['item_lab', 'shop_id'])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MATRIX = add_max_cnt(MATRIX)\nfillna_means(MATRIX, 'max_cnt')\n\nIDX_TRAIN = (MATRIX.date_block_num > 11) & (MATRIX.date_block_num < 33)\nIDX_VAL = MATRIX.date_block_num == 33\nIDX_TEST = MATRIX.date_block_num == 34","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MATRIX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MATRIX.loc[IDX_TRAIN, 'max_cnt'].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add mean encoding with K-Fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mean_encodings_kfold(df, idx_train, idx_val, idx_test, feature_names, target_name, folds=5):\n    \n    skf = StratifiedKFold(n_splits=folds, shuffle=True)\n    global_mean = df.loc[idx_train, target_name].mean()\n    \n    for feature_name in feature_names:\n        print(feature_name)\n        \n        df.loc[:, feature_name + '_mean'] = np.nan\n        data = df.loc[idx_train, [feature_name, feature_name + '_mean', target_name]]\n\n        for idx_1, idx_2 in skf.split(data[[feature_name]], data[feature_name]):\n            # use means from set 1 for mean encoding of set 2\n            x_1 = data.iloc[idx_1].loc[:, [feature_name, feature_name + '_mean', target_name]]\n            x_2 = data.iloc[idx_2].loc[:, [feature_name, feature_name + '_mean', target_name]]\n            means = x_1.groupby(feature_name).agg(mean_target=(target_name, 'mean')).loc[:, 'mean_target']\n            x_2[feature_name + '_mean'] = x_2[feature_name].map(means)\n            data.update(x_2)\n\n        data[feature_name + '_mean'].fillna(global_mean, inplace=True)\n        df.update(data)\n    \n    # use means of complete training set for mean encoding of both validation and test set\n    for feature_name in feature_names:\n        x_1 = df.loc[idx_train, [feature_name, target_name]]\n        x_2 = df.loc[np.logical_or(idx_val, idx_test), [feature_name, target_name]]\n        means = x_1.groupby(feature_name).agg(mean_target=(target_name, 'mean')).loc[:, 'mean_target']\n        x_2[feature_name + '_mean'] = x_2[feature_name].map(means)\n        df.update(x_2)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MATRIX = get_mean_encodings_kfold(\n    MATRIX, IDX_TRAIN, IDX_VAL, IDX_TEST,\n    ['shop_id', 'city_id', 'item_lab', 'cat1_lab', 'cat2_lab', 'cat1_lev', 'cat2_lev', 'month'],\n    'item_cnt_month')\n\n# MATRIX[IDX_TRAIN].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Downcast types to save some RAM."},{"metadata":{"trusted":true},"cell_type":"code","source":"MATRIX = downcast_dtypes(MATRIX)\nMATRIX.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define train, validation and test matrices and target vectors."},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_X = MATRIX[IDX_TRAIN].drop('item_cnt_month', axis=1)\nTRAIN_Y = MATRIX[IDX_TRAIN].item_cnt_month\nVAL_X = MATRIX[IDX_VAL].drop('item_cnt_month', axis=1)\nVAL_Y = MATRIX[IDX_VAL].item_cnt_month\nTEST_X = MATRIX[IDX_TEST].drop('item_cnt_month', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del SALES, ITEM_CAT, ITEMS, SHOPS, TEST","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use XGBRegressor for predictions. Hyperparameters were slowly adapted over several runs:\n\n* n_estimators: started with 50, increased to 100\n* max_depth: started with 3, increased to 5\n* learning_rate: started with 0.1, decreased to 0.08"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n# fit xgb regressor\nregressor = xgb.XGBRegressor(n_estimators = 100,\n                             learning_rate = 0.08,\n                             max_depth = 5,\n                             subsample = 0.8,\n                             colsample_bytree = 0.8,\n                             n_jobs = 8\n                            )\nreg = regressor.fit(\n    X=TRAIN_X,\n    y=TRAIN_Y,\n    eval_metric='rmse',\n    eval_set=[(TRAIN_X, TRAIN_Y), (VAL_X, VAL_Y)],\n    verbose=True,\n    early_stopping_rounds=8\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save model"},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(reg, open(\"xgb.pickle.dat\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot RMSE for train and validation set"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"plt.plot(reg.evals_result()['validation_0']['rmse'], label='validation_0')\nplt.plot(reg.evals_result()['validation_1']['rmse'], label='validation_1')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check importance of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = pd.Series(reg.feature_importances_, index=TRAIN_X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create submission file. Some merging needs to be done since the item_id was changed to item_lab."},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\nPRED = TEST_X[['item_lab', 'shop_id']]\nPRED['item_cnt_month'] = reg.predict(TEST_X)\nPRED.drop_duplicates(inplace=True)\nprint(TEST.shape, PRED.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ITEMS = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\npre_process_item_data(ITEMS)\nTEST = TEST.merge(ITEMS[['item_id', 'item_lab']], how='left', on='item_id')\nprint(TEST.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST = TEST.merge(PRED, how='left', on=['item_lab', 'shop_id'])\n# TEST.drop_duplicates(ignore_index=True, inplace=True)\nTEST.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST.item_cnt_month.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST.item_cnt_month.clip(lower=0, upper=20, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST[['ID', 'item_cnt_month']].sort_values('ID').to_csv('submission.csv', index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}