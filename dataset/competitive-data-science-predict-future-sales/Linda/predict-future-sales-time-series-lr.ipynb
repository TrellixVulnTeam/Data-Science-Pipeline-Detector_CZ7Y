{"cells":[{"metadata":{},"cell_type":"markdown","source":"> # Predict Future Sales"},{"metadata":{},"cell_type":"markdown","source":"> ## Introduction"},{"metadata":{},"cell_type":"markdown","source":"It is a Dataset from kaggle with 5 csv files of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company. The sales data is from January 2013 - October 2015, splittet mainly into sold products and shops.\n\nMy challenge is to predict future sales in a time series.\n\n(The challenge of the kaggle competion will be to predict total sales for every product and store in the next month for the test set and to create a robust model that can handle monthly, slightly changes in the list of shops and products.)"},{"metadata":{},"cell_type":"markdown","source":"### Agenda"},{"metadata":{},"cell_type":"markdown","source":"1. Libraries \n\n2. Exploratory Data Analysis\n\n3. Predict Future Sales\n\n    3.1. Analysing historical data\n    \n    3.2. Test Stationarity with Augmented Dicky Fuller Test\n    \n    3.3. Forecast Time Series with ARIMA\n    \n    3.4. Fitting the SARIMAX model\n    \n    3.5. Validating forecasts\n    \n    3.6. Visualization of the forecast   \n    \n4. Focusing on certain shops\n\n    4.1. Offline Shops: sold products per day\n    \n    4.2. Online Shop: sold products per day\n    \n        4.2.1. Analysing historical data\n    \n        4.2.2 Test Stationarity with Augmented Dicky Fuller Test\n    \n        4.2.3. Forecast Time Series with ARIMA\n    \n        4.2.4. Fitting the SARIMAX model\n    \n        4.2.5. Validating forecasts\n    \n        4.2.6. Visualization of the forecast \n        \n    4.3. Extra Analysis\n"},{"metadata":{},"cell_type":"markdown","source":"### 1. Libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n#from googletrans import Translator\n\nfrom itertools import product\nimport itertools\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport matplotlib\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\nfrom statsmodels.tsa.stattools import adfuller,pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.graphics.gofplots import qqplot\nfrom statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n\nfrom pylab import rcParams\nmatplotlib.rcParams['axes.labelsize'] = 14\nmatplotlib.rcParams['xtick.labelsize'] = 12\nmatplotlib.rcParams['ytick.labelsize'] = 12\nmatplotlib.rcParams['text.color'] = 'k'\n\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\nfrom pandas.plotting import autocorrelation_plot\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Translate (item name, item category and) shop name into english"},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"\nimport re\ntranslator = Translator()\ntranslate = [\"item_name\",\"item_category_name\",\"shop_name\"]\nshops = pd.read_csv(\"shops.csv\")\nshops_lst = list(shops.shop_name.unique())\nshops[\"shop_name_en\"] = shops[\"shop_name\"].apply(translator.translate, src = \"ru\", dest = \"en\").apply(getattr, args=('text',))\n\nshops = shops.drop(columns = {\"shop_name\"})\nshop_lst = list(shops.shop_name_en)\nlist_of_list_shops = [re.findall(r'[a-zA-Z]+', i) for i in shop_lst]\nshops[\"City\"] = [list_of_list_shops[i][0] +\" \"+ list_of_list_shops[i][1] \n                 if ((list_of_list_shops[i][0] == \"St\") |(list_of_list_shops[i][0] == \"Itinerant\") | \n                                                                             (list_of_list_shops[i][0] ==\"Digital\"))\n                 else list_of_list_shops[i][0] + \" \"+ list_of_list_shops[i][1] +\" \"+ list_of_list_shops[i][2] \n                 if (list_of_list_shops[i][0] == \"Shop\")\n                 else list_of_list_shops[i][0] for i in range(len(list_of_list_shops))]\n\nshops.to_csv(\"shops_new.csv\", sep = \";\")\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately items and items_category are too big to translate, they are also not neccessary."},{"metadata":{},"cell_type":"markdown","source":"#### Import datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/test.csv\")\nitem_categories = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv\")\nsales_train = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv\")\nitems = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\")\nshops = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transform date to datetime"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.date = pd.to_datetime(sales_train.date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_after11 = sales_train[(sales_train[\"date\"] >= \"2015-11-01\")]\nsales_train_after11.date.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = sales_train[(sales_train[\"date\"] < \"2015-11-01\")]\nsales_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Data fields\n\nID - an Id that represents a (Shop, Item) tuple within the test set\n\nshop_id - unique identifier of a shop\n\nitem_id - unique identifier of a product\n\nitem_category_id - unique identifier of item category\n\nitem_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n\nitem_price - current price of an item\n\ndate - date in format dd/mm/yyyy\n\ndate_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n\nitem_name - name of item\n\nshop_name - name of shop\n\nitem_category_name - name of item category"},{"metadata":{},"cell_type":"markdown","source":"#### Merge Datasets\n"},{"metadata":{},"cell_type":"markdown","source":"Merge datasets to have dataset with more informations, columns.\n\nitem_categories.item_category_id = items.item_category_id\n\nitems.item_id = sales_train.item_id\n\nshops.shop_id = sales_train.shop_id\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.merge(items, item_categories, on = \"item_category_id\")\nitems.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = pd.merge(sales_train, items, on = \"item_id\")\nsales_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = pd.merge(sales_train, shops, on = \"shop_id\")\nsales_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = sales_train[(sales_train[\"date\"] < \"2015-11-01\")]\nsales_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bestseller Products"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_per_product = sales_train.groupby(\"item_name\", as_index=False).agg({\"item_cnt_day\":\"sum\"}).sort_values(by = \"item_cnt_day\", ascending = False)[0:10]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_per_product","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.barplot(x = \"item_cnt_day\", y = \"item_name\", data = sales_per_product)\nplt.figure(figsize=(20,10))\nplt.tight_layout()\n#sns.set_style(\"whitegrid\")\nax.set_title(\"Bestseller\",y= 1.1, fontsize=18, weight = \"semibold\")\nax.set_xlabel(\"# of products\", fontsize = 18, weight = \"semibold\")\nax.set_ylabel(\"Products\", fontsize = 18, weight = \"semibold\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Shop with the highest amount of sold products"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_per_shop = sales_train.groupby(by = \"shop_name\", as_index=False).agg({\"item_cnt_day\":\"sum\"}).sort_values(by = \"item_cnt_day\",ascending = False)[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.barplot(x = \"item_cnt_day\", y = \"shop_name\", data = sales_per_shop, palette=\"gist_heat\")\nsns.set_style(\"whitegrid\")\n\nax.set_title(\"Shops sort by amount of sold products\",y= 1.1, fontsize=20, weight = \"semibold\")\nax.set_xlabel(\"Amount of products\", fontsize = 18, weight = \"semibold\")\nax.set_ylabel(\"shops\", fontsize = 18, weight = \"semibold\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train[\"revenue\"] = sales_train[\"item_cnt_day\"]*sales_train[\"item_price\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Prediction Future Sales"},{"metadata":{},"cell_type":"markdown","source":"Analyse the data based on the sold products (item_cnt_day) per day."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.date.min(), sales_train.date.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales_train.groupby('date')['item_cnt_day'].sum().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales.set_index('date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = sales['item_cnt_day'].resample('MS').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y[\"2015\":]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot historical data about all sold products per day\ny.plot(figsize=(15, 6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefficients, residuals, _, _, _ = np.polyfit(range(len(y.index)),y,1,full=True)\nmse = residuals[0]/(len(y.index))\nnrmse = np.sqrt(mse)/(y.max() - y.min())\nprint('Slope ' + str(coefficients[0]))\nprint('NRMSE: ' + str(nrmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(y[33]-y[0])/y[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.1. Analysing historical data"},{"metadata":{},"cell_type":"markdown","source":"Analysis regarding observation, trend, seasonality and residuals"},{"metadata":{"trusted":true},"cell_type":"code","source":"rcParams['figure.figsize'] = 18, 8\ndecomposition = sm.tsa.seasonal_decompose(y, freq=12, model='additive')\nfig = decomposition.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Results"},{"metadata":{},"cell_type":"markdown","source":"The data is from 2013-01-01 till 2015-10-31, you can see seasonality over a year, with a peak at the end of the year. The trend goes down."},{"metadata":{},"cell_type":"markdown","source":"#### 3.2. Test Stationarity with Augmented Dicky Fuller Test"},{"metadata":{},"cell_type":"markdown","source":"Augmented Dicky Fuller Test is to check the stationarity of the sold items per day. Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the sum of item_cnt_day per day.\nresult = adfuller(y)\nprint(\"Daily Basis:\")\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n    \n# p-value is smaller than 0.05 so we can reject the Null Hypothesis, \n# the time series is stationary and has no time dependent structure","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The p-value is smaller than 0.05 so we can reject the Null Hypothesis, the time series is stationary and has no time dependent structure."},{"metadata":{},"cell_type":"markdown","source":"#### 3.3. Forecast Time Series with ARIMA"},{"metadata":{},"cell_type":"markdown","source":"Analysing the parameters (p=season, d=trend, q=noise) for the seasonal ARIMA (Autoregressive Integrated Moving Average) to recieve the best AIC (Akaike’s Information Criterion).\nAIC estimates the quality of a model, relative to each of other models. The lower AIC score is, the better the model is. Therefore, a model with lowest AIC - in comparison to others, is chosen.\n\nSince we saw in the analysis before that there is a seasonality over the year, we will use the model SARIMAX, this model allows us to set a seasonality of 12 months.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"p = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(y,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n            results = mod.fit()\n            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.4. Fitting the SARIMAX model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The best AIC is:\n# ARIMA(1, 1, 1)x(1, 1, 0, 12)12 - AIC:115.62002802642752\n\nmod = sm.tsa.statespace.SARIMAX(y,\n                                order=(1, 1, 1),\n                                seasonal_order=(1, 1, 0, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\nresults = mod.fit()\nprint(results.summary().tables[1])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"results.plot_diagnostics(figsize=(16, 8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The diagnostic plots gave us the suggests that the model residuals are near normally distributed."},{"metadata":{},"cell_type":"markdown","source":"#### 3.5. Validating forecasts"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = results.get_prediction(start=pd.to_datetime('2015-01-01'), dynamic=False)\npred_ci = pred.conf_int()\nax = y['2013':].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_xlabel('date')\nax.set_ylabel('item_cnt_day')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_forecasted = pred.predicted_mean\ny_truth = y['2015-01-01':]\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse), 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.6. Visualization of the forecast"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_uc = results.get_forecast(steps=100)\npred_ci = pred_uc.conf_int()\nax = y.plot(label='observed', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('date')\nax.set_ylabel('item_cnt_day')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_uc = results.get_forecast(steps=3)\npred_ci = pred_uc.conf_int()\nax = y.plot(label='observed', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('date')\nax.set_ylabel('item_cnt_day')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_uc = results.get_forecast(steps=24)\npred_ci = pred_uc.conf_int()\nax = y.plot(label='observed', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Date',fontsize=18, weight = \"bold\")\nax.set_ylabel('Amount of sold items',fontsize=18, weight = \"semibold\")\nplt.title(\"Forecast sold products next 2 years\",y= 1.1, fontsize=18, weight = \"semibold\" )\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Focusing on certain shops"},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_lst = list(sales_train.shop_id.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"online_shop = sales_train[(sales_train[\"shop_id\"] == 12)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"offline_shop = sales_train[(sales_train[\"shop_id\"] != 12)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.1. Offline Shops: sold products per day"},{"metadata":{"trusted":true},"cell_type":"code","source":"offline_sales = offline_shop.groupby(['date',\"shop_id\"])['item_cnt_day'].sum().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"offline_sales = offline_sales.set_index('date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"offline_sales.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot time series for each offline shop\nfor i in shops_lst:\n    sales_shop = offline_sales[(offline_sales[\"shop_id\"] == i)]\n    y = sales_shop[\"item_cnt_day\"].resample(\"MS\").mean()\n    \n    y.plot()\n   \n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2. Online Shop: sold products per day"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_per_online_shop = online_shop.groupby(['date',\"shop_id\"])['item_cnt_day'].sum().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_per_online_shop = sales_per_online_shop.set_index('date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"o = sales_per_online_shop[\"item_cnt_day\"].resample(\"MS\").mean()\n#X = sales_per_online_shop.index\n\n#plt.plot(X, coefficients[0]*X +residuals, color=\"red\")\naxo = o.plot()\n\nplt.title(\"Online Sales\", y= 1.1, fontsize=18, weight = \"semibold\")\nplt.xlabel(\"Date\", fontsize=14, weight = \"semibold\")\nplt.ylabel(\"# sold products\", fontsize=14, weight = \"semibold\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefficients, residuals, _, _, _ = np.polyfit(range(len(o.index)),o,1,full=True)\nmse = residuals[0]/(len(o.index))\nnrmse = np.sqrt(mse)/(o.max() - o.min())\nprint('Slope ' + str(coefficients[0]))\nprint('NRMSE: ' + str(nrmse))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(o[33]-o[0])/o[0]*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 4.2.1 Analysing historical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"rcParams['figure.figsize'] = 18, 8\ndecomposition = sm.tsa.seasonal_decompose(o, freq=12, model='additive')\nfig = decomposition.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 4.2.2. Test Stationarity with Augmented Dicky Fuller Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the sum of item_cnt_day per day.\nresult = adfuller(o)\nprint(\"Daily Basis:\")\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n    \n# p-value is smaller than 0.05 so we can reject the Null Hypothesis, \n# the time series is stationary and has no time dependent structure","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 4.2.3. Forecast Time Series with ARIMA"},{"metadata":{"trusted":true},"cell_type":"code","source":"p = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 4.2.4. Fitting the SARIMAX model"},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(o,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n            results = mod.fit()\n            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The best AIC is:\n# ARIMA(1, 1, 0)x(1, 1, 0, 12)12 - AIC:87.79639573691884\n\nmod = sm.tsa.statespace.SARIMAX(o,\n                                order=(1, 1, 0),\n                                seasonal_order=(1, 1, 0, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\nresults = mod.fit()\nprint(results.summary().tables[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 4.2.5. Validating forecasts"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = results.get_prediction(start=pd.to_datetime('2015-01-01'), dynamic=False)\npred_ci = pred.conf_int()\nax = o['2013':].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_xlabel('date')\nax.set_ylabel('item_cnt_day')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"o_forecasted = pred.predicted_mean\no_truth = o['2015-01-01':]\nmse = ((o_forecasted - o_truth) ** 2).mean()\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse), 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 4.2.6. Visualization of the forecast"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_uc = results.get_forecast(steps=100)\npred_ci = pred_uc.conf_int()\nax = o.plot(label='observed', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('date')\nax.set_ylabel('item_cnt_day')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_uc = results.get_forecast(steps=15)\npred_ci = pred_uc.conf_int()\nax = o.plot(label='observed', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('date')\nax.set_ylabel('item_cnt_day')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.3. Extra analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"last_offline_sales = sales_train[(sales_train.date_block_num == 33) & (sales_train.shop_id != 12)]\nw = last_offline_sales.item_cnt_day.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_online_sales = sales_train[(sales_train.date_block_num == 33) & (sales_train.shop_id == 12)]\nz = last_online_sales.item_cnt_day.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['Offline', \"online\"]\nsizes = [(w/(w+z)),(z/(w+z))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explode = (0, 0.1)\nplt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',  startangle=60)\nplt.axis('equal', fontsize=14, weight = \"semibold\")\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.6"}},"nbformat":4,"nbformat_minor":1}