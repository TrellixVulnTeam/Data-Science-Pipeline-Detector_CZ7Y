{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plots\nimport sklearn as sk # models\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom string import punctuation # punctuation array\nfrom xgboost import XGBRegressor # XGBoost Regression\nfrom xgboost import plot_importance\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n%matplotlib inline \nimport gc, warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\ndef print_files():\n    import os\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\ndef downcast_dtypes(df, inplace=False):\n    '''\n    input  df: object\n    output df: object\n    \n    reject size of col type\n    '''\n    if not inplace:\n        data = df.copy()\n    else:\n        data = df\n    float_cols = [c for c in data if data[c].dtype in [\"float32\", \"float64\"]]\n    int_cols = [c for c in data if data[c].dtype in [\"int64\", \"int32\"]]\n    data[float_cols] = data[float_cols].astype(np.float16)\n    data[int_cols] = data[int_cols].astype(np.int16)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print_files()\nitems = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\nsales_train = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\ntest = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv', index_col=\"ID\")\nsample_submission = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')\nitem_categories = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sales_train processing\n<hr>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduse data size\ndowncast_dtypes(sales_train, inplace=True)\nprint(sales_train.info())\n# No missing values in sales_train\nprint(\"Num of missing values in sales_train: %d\" %sales_train.isnull().sum().sum())\n# 6 duplicated rows in sales_train\nprint(\"Num of duplicated rows in sales_train: %d\" %sales_train.duplicated().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Sales_train outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(25,5))\n(ax1,ax2) = fig.subplots(1,2)\nax1.boxplot(x=sales_train.item_price, vert=False)\nax1.set_xlabel(\"item_price\")\nax2.boxplot(x=sales_train.item_cnt_day, vert=False)\nax1.set_xlabel(\"item_cnt_day\")\nprint(\"item_price outliers item_id\", *sales_train[sales_train.item_price>45000].index.values)\nprint(\"item_cnt_day outliers item_id\", *sales_train[sales_train.item_cnt_day>999].index.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are items with large prices and sales.   \nI've researched it in details and decided to remove items with price > 100000 and sales > 1001  \nWe have item_name = 'Доставка (EMS)' - delivery in every case.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Price less then zero index\", *sales_train[sales_train.item_price<0].index.values)\nmed = sales_train[(sales_train.shop_id==32)&(sales_train.item_id==11365)\\\n            &(sales_train.item_price>0)&(sales_train.date_block_num==4)].median()\nsales_train.iloc[484683] = med","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = sales_train[sales_train.item_price<100000][sales_train.item_cnt_day<1001]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Shops processing\n<hr>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Сarefully examining `shops`, you can see that there are duplicates  \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(shops.iloc[np.r_[10,11,23,24,39,40,0,57,1,58]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The structure of the `shops`  \nShop City | Shop type | Shop name\n\nHere we clean duplicates and extract feature `shop_type` , `shop_city`, `shop_name_c`  \nEncode all data with LabelEncoder()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Map duplicated shop_id through a dictionary\nd = {0:57, 1:58, 10:11, 23:24, 39:40}\nshops[\"shop_id\"] = shops[\"shop_id\"].apply(lambda x: d[x] if x in d.keys() else x)\nsales_train[\"shop_id\"] = sales_train[\"shop_id\"].apply(lambda x: d[x] if x in d.keys() else x)\ntest[\"shop_id\"] = test[\"shop_id\"].apply(lambda x: d[x] if x in d.keys() else x)\n# Remove all punctuation in shop_name\nshops['shop_name_c'] = shops['shop_name'].apply(lambda string: \"\".join([pt for pt in string if pt not in punctuation])) \nshops['shop_name_c'] = shops['shop_name_c'].str.lower()\n\n#Exctract new features\n\n#shop_type\nshops['shop_type'] = shops['shop_name_c'].apply(lambda x: 'мтрц' if 'мтрц' in x else 'трц' if 'трц' in x else 'трк' if 'трк' in x else 'тц' if 'тц' in x else 'тк' if 'тк' in x else 'тц')\n\n#shop_city\nshops['shop_city'] = shops['shop_name_c'].str.partition(' ')[0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#OneHotEncoder for shop_type\nOHE = OneHotEncoder(handle_unknown='ignore', dtype=np.int8)\nOHE.fit(np.array(shops['shop_type'].unique()).reshape(len(shops['shop_type'].unique()), 1))\nOneHot_transform = OHE.transform(np.array(shops['shop_type']).reshape(-1, 1))\n#prepare to megre\nOneHot_transform = pd.DataFrame(data=OneHot_transform.toarray(), columns=[\"lab_mtrc\",\"lab_tk\",\"lab_trk\",\"lab_trc\",\"lab_tc\"])\n\nshops_merged = pd.merge(shops, OneHot_transform, how='left',left_on=\"shop_id\", right_on=OneHot_transform.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LabelEncoding shop_city\nshops_merged['shop_city_code'] = LabelEncoder().fit_transform(shops['shop_city'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_prepared = shops_merged.drop([\"shop_name\",\"shop_name_c\", \"shop_type\", \"shop_city\"], axis=1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Item_categories processing\n<hr>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#(84, 2)\nitem_categories.shape\n#No duplicates\nitem_categories[\"item_category_name\"].duplicated().sum()\nitem_categories.rename(columns={'item_category_id': 'category_item_id'}, inplace=True)\nitems.rename(columns={'item_category_id': 'category_item_id'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_rows = item_categories.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean type and encode category freature\nitem_categories[\"category\"] = item_categories[\"item_category_name\"]\\\n                            .str.split(\"-\")\\\n                            .map(lambda x: x[0].strip())\nitem_categories[\"category_code\"] = LabelEncoder().fit_transform(item_categories['category'])\n#Extract subcategory\nitem_categories[\"subcat\"] = item_categories[\"item_category_name\"]\\\n                            .str.split(\"-\")\\\n                            .map(lambda x: x[1].strip() if len(x) > 1 else 'no_SUB') # 0 -> 1\nitem_categories[\"subcat_code\"] = LabelEncoder().fit_transform(item_categories['subcat'])\nitem_categories_prepared = item_categories.drop([\"item_category_name\",\"category\", \"subcat\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Items processing\n<hr>\nWe encode `additional` feature in brackets () - (UNV).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"itemsa = items.copy()\nimport re\nitemsa.item_name = itemsa.item_name.map(lambda x: re.search(r\"(\\(.*?\\))|(\\[(.*?)\\])\", x))\nitemsa.item_name = itemsa.item_name.map(lambda x: (x.group() if type(x)!=type(None) else \"no_feature\"))\nitemsa.item_name = itemsa.item_name.map(lambda x: x.strip(\"().[]+\")) \nitems[\"item_name_add_code\"] = LabelEncoder().fit_transform(itemsa.item_name)\nitems_prepared = items.drop([\"item_name\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Joining to train\n<hr>\nSo, what do we have on this moment?\nWe've prepared data set with standart features. Now we going to concatinate all data in one set  \n  \nBut first look at the test set. It has shape (214200,1). There is product of some shops and some items within 34 month.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test.item_id.nunique() * test.shop_id.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Test set has 363 unique `item_id`, they were met in train set newer. We can calculate monthly sales for train set and extend it with *zero* sales for each unique pair. Following this idea the train set will be similar to test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import product\nproduct_train = []\ncolms = ['date_block_num', 'shop_id', 'item_id']\nfor m in range(34):\n    sales = sales_train[sales_train.date_block_num==m]\n    product_train.append(np.array(list(product([m], sales.shop_id.unique(), sales.item_id.unique()))))\nproduct_train = pd.DataFrame(np.vstack(product_train), columns=colms)\nproduct_train.sort_values(colms, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now need to aggregate train set by shop/item pairs to calculate target sum, then clip(0,20) target value. This way train target will be similar to the test predictions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"group = sales_train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nproduct_train = pd.merge(product_train, group, on=colms, how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_train.item_cnt_month = product_train.item_cnt_month.fillna(0).clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Append test set as 34th month","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"date_block_num\"] = 34","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"product_train = pd.concat([product_train, test], ignore_index=True, sort=False, keys=colms)\nproduct_train.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Join other data to set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"product_train = pd.merge(product_train, shop_prepared, on=['shop_id'], how='left')\nproduct_train = pd.merge(product_train, items_prepared, on=['item_id'], how='left')\nproduct_train = pd.merge(product_train, item_categories_prepared, on=['category_item_id'], how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"New duplicated rows appeared, drop them!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"product_train = product_train.drop_duplicates().reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LAG FEATURES  \n### item_cnt_month lag","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature(df, lags, col):\n    '''\n    General function for compute lags.\n    input: df: pd.DataFrame, lags: list of lags, col: name of lagging column(string)\n    output: df with LAGged feature(s)\n    '''\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data downcast\nfor x in ['date_block_num','shop_id', 'lab_mtrc', 'lab_tk', 'lab_trk', 'lab_trc', 'lab_tc', 'shop_city_code', 'category_item_id', 'category_code', 'subcat_code']:\n    product_train[x] = product_train[x].astype(np.int8)\n    \nproduct_train['item_name_add_code'] = product_train['item_name_add_code'].astype(np.int16)\nproduct_train['item_cnt_month'] = product_train['item_cnt_month'].astype(np.float16)\nproduct_train['item_id'] = product_train['item_id'].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_train = lag_feature(product_train, [1,2,3,7,12], 'item_cnt_month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Mean encoded features\n\nCalculate mean encoded features and find most interesting lags with acf and pacf plots","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"product_train.columns.to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize = (16,6), dpi = 80)\nproduct_train.item_cnt_month = product_train.item_cnt_month.astype(np.int16)\ncols = ['subcat_code']\nplot_acf(product_train.groupby(['date_block_num', *cols]).agg({'item_cnt_month': ['mean']}), ax = ax1, lags = 13)\nplot_pacf(product_train.groupby(['date_block_num', *cols]).agg({'item_cnt_month': ['mean']}), ax = ax2, lags = 13);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_encode(df, grop_b:list, lags:list):\n    grouped = df.groupby(['date_block_num', *grop_b]).agg({'item_cnt_month': ['mean']})\n    col_name = [str_part.split('_')[0] for str_part in grop_b]\n    col_name = str('date_'+'_'.join(col_name)+'_avg_item_cnt')\n    grouped.columns = [col_name]\n    grouped.reset_index(inplace=True)\n\n    df = pd.merge(df, grouped, on=['date_block_num',*grop_b], how='left')\n    df[col_name] = df[col_name].astype(np.float16)\n    df = lag_feature(df, lags, col_name)\n    df.drop([col_name], axis=1, inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nproduct_train = mean_encode(product_train, ['item_id'], [1,2,3])\nproduct_train = mean_encode(product_train, ['shop_id','category_item_id'], [1,6])\nproduct_train = mean_encode(product_train, ['shop_id','shop_city_code'], [1,6])\nproduct_train = mean_encode(product_train, ['shop_city_code'], [1])\nproduct_train = mean_encode(product_train, ['item_id', 'category_item_id'], [1])\nproduct_train = mean_encode(product_train, ['category_item_id'], [1,2,3])\nproduct_train = mean_encode(product_train, ['subcat_code'], [1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Trend item price features\n> I was looking for the closest not nullable price, to compare it with the avg price. If an item costs less than in the past - its is a positive thend, otherwise - negative. The more difference between closest price and avg price - the more trend value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"group = sales_train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nproduct_train = pd.merge(product_train, group, on=['item_id'], how='left')\nproduct_train['item_avg_item_price'] = product_train['item_avg_item_price'].astype(np.float16)\n\ngroup = sales_train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nproduct_train = pd.merge(product_train, group, on=['date_block_num','item_id'], how='left')\nproduct_train['date_item_avg_item_price'] = product_train['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nproduct_train = lag_feature(product_train, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    product_train['delta_price_lag_'+str(i)] = \\\n        (product_train['date_item_avg_item_price_lag_'+str(i)] - product_train['item_avg_item_price']) / product_train['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nproduct_train['delta_price_lag'] = product_train.apply(select_trend, axis=1)\nproduct_train['delta_price_lag'] = product_train['delta_price_lag'].astype(np.float16)\nproduct_train['delta_price_lag'].fillna(0, inplace=True)\n\n# https://stackoverflow.com/questions/31828240/first-non-null-value-per-row-from-a-list-of-pandas-columns/31828559\n# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n# Invalid dtype for backfill_2d [float16]\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nproduct_train.drop(fetures_to_drop, axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train['revenue'] = sales_train['item_price'] *  sales_train['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Month revenue trend in shop\ngroup = sales_train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nproduct_train = pd.merge(product_train, group, on=['date_block_num','shop_id'], how='left')\nproduct_train['date_shop_revenue'] = product_train['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nproduct_train = pd.merge(product_train, group, on=['shop_id'], how='left')\nproduct_train['shop_avg_revenue'] = product_train['shop_avg_revenue'].astype(np.float32)\n\nproduct_train['delta_revenue'] = (product_train['date_shop_revenue'] - product_train['shop_avg_revenue']) / product_train['shop_avg_revenue']\nproduct_train['delta_revenue'] = product_train['delta_revenue'].astype(np.float16)\n\nproduct_train = lag_feature(product_train, [1], 'delta_revenue')\n\nproduct_train.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Months since the last sale for each shop/item pair and for item only.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncache = {}\nproduct_train['item_shop_last_sale'] = -1\nproduct_train['item_shop_last_sale'] = product_train['item_shop_last_sale'].astype(np.int8)\nfor idx, row in product_train.iterrows():    \n    key = str(row.item_id)+' '+str(row.shop_id)\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        product_train.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n        cache[key] = row.date_block_num       ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Months since the first sale for each shop/item pair and for item only.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"product_train['item_shop_first_sale'] = product_train['date_block_num'] - product_train.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nproduct_train['item_first_sale'] = product_train['date_block_num'] - product_train.groupby('item_id')['date_block_num'].transform('min')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Date features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ADD month\nproduct_train['month'] = product_train['date_block_num'] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n# ADD day in month respectively\nproduct_train['days'] = product_train['month'].map(days).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fill na in lag and del first 12 month\nIn order to haven't data leakage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nproduct_train = product_train[product_train.date_block_num > 11]\nproduct_train = fill_na(product_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset prepared","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"product_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nproduct_train.to_pickle('data.pkl')\ndel cache\ndel OneHot_transform\ndel shops_merged\ndel shop_prepared\ndel item_categories_prepared\ndel group\ndel items_prepared\n# del test\ndel items\ndel shops\ndel item_categories\ndel sales_train\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_pickle('/kaggle/input/datapkl/data.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"dataset = dataset[['date_block_num',\n       'shop_id',\n       'item_id',\n       'item_cnt_month',\n#        'lab_mtrc',\n#        'lab_tk',\n#        'lab_trk',\n#        'lab_trc',\n#        'lab_tc',\n       'shop_city_code',\n       'category_item_id',\n       'item_name_add_code',\n       'category_code',\n       'subcat_code',\n       'item_cnt_month_lag_1',\n       'item_cnt_month_lag_2',\n       'item_cnt_month_lag_3',\n#        'item_cnt_month_lag_7',\n#        'item_cnt_month_lag_12',\n       'date_item_avg_item_cnt_lag_1',\n       'date_item_avg_item_cnt_lag_2',\n       'date_item_avg_item_cnt_lag_3',\n       'date_shop_category_avg_item_cnt_lag_1',\n#        'date_shop_category_avg_item_cnt_lag_6',\n       'date_shop_shop_avg_item_cnt_lag_1',\n#        'date_shop_shop_avg_item_cnt_lag_6',\n       'date_shop_avg_item_cnt_lag_1',\n       'date_item_category_avg_item_cnt_lag_1',\n       'date_category_avg_item_cnt_lag_1',\n       'date_category_avg_item_cnt_lag_2',\n       'date_category_avg_item_cnt_lag_3',\n       'date_subcat_avg_item_cnt_lag_1',\n       'delta_price_lag',\n       'delta_revenue_lag_1',\n       'item_shop_last_sale',\n       'item_shop_first_sale',\n       'item_first_sale',\n       'month',\n       'days'\n        ]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train 13-33   \nValidation 34  \nTest 35  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = dataset[dataset.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = dataset[dataset.date_block_num < 33]['item_cnt_month']\nX_valid = dataset[dataset.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = dataset[dataset.date_block_num == 33]['item_cnt_month']\nX_test = dataset[dataset.date_block_num == 34].drop(['item_cnt_month'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del dataset\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xbg_model = XGBRegressor(\n    tree_method = 'gpu_hist',\n    max_depth=10,\n    n_estimators=800,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=29)\n\nxbg_model.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\ncatboost_model = CatBoostRegressor(\n    iterations=1000,\n    max_ctr_complexity=8,\n    random_seed=29,\n    od_type='Iter',\n    od_wait=25,\n    verbose=50,\n    task_type='GPU',\n    loss_function='RMSE',\n    learning_rate = 0.3\n)\ncatboost_model.fit(\n    X_train, Y_train,\n    eval_set=(X_valid, Y_valid)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##XGBoost\n# from sklearn.model_selection import GridSearchCV\n# xgb = XGBRegressor(\n#     tree_method = 'gpu_hist'\n#     )\n# params = {'max_depth':[4,8,10],\n#     'n_estimators':[800,1000],\n#     'min_child_weight':[300], \n#     'colsample_bytree':[0.7,0.8], \n#     'subsample':[0.8], \n#     'eta':[0.1,0.2,0.3],    \n#     'seed':[27]}\n# model = GridSearchCV(xgb, param_grid=params, n_jobs=1)\n\n\n# model.fit(\n#     X_train, \n#     Y_train,  \n#     eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n#     verbose=10, \n#     eval_metric='rmse',\n#     early_stopping_rounds = 5)\n\n##CatBoost\n\n# from catboost import CatBoostRegressor\n# catboost_model = CatBoostRegressor()\n# params = {\n#     'iterations':[1000,800],\n#     'max_ctr_complexity':[8,5,6],\n#     'random_seed':[29],\n#     'od_type':['Iter'],\n#     'od_wait':[25,15,20],\n#     'verbose':[50],\n#     'task_type':['GPU'],\n#     'loss_function':['RMSE'],\n# }\n# catboost_model.fit(\n#     X_train, Y_train,\n#     eval_set=(X_valid, Y_valid)\n# )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_xgb = xbg_model.predict(X_valid).clip(0, 20)\nY_pred_cat = catboost_model.predict(X_valid).clip(0, 20)\n\nY_test_xgb = xbg_model.predict(X_test).clip(0, 20)\nY_test_cat = catboost_model.predict(X_test).clip(0, 20)\n\nX_lev2_train = pd.DataFrame(np.array([Y_pred_xgb,Y_pred_cat]).T, columns=['XBG','CAT'])\nX_lev2_test = pd.DataFrame(np.array([Y_test_xgb,Y_test_cat]).T, columns=['XBG','CAT'])\n\nmodel_level2 =  LinearRegression()\nmodel_level2.fit(X_lev2_train, Y_valid)\nY_answer = model_level2.predict(X_lev2_test).clip(0, 20)\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_answer\n})\nsubmission.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}