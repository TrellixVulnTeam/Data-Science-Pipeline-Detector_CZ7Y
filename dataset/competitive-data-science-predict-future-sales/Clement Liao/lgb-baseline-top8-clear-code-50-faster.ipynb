{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np  # linear algebra\nimport pandas as pd  #\nfrom datetime import datetime\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV,Ridge,Lasso,ElasticNet\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor,RandomForestClassifier\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.svm import SVR, LinearSVC\nfrom sklearn.pipeline import make_pipeline,Pipeline\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nimport os\nimport re\nimport shap\nimport pandas_profiling as ppf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport re\nfrom collections import Counter\nfrom operator import itemgetter\nimport time\nfrom itertools import product\nimport datetime as dt\nimport calendar\nimport gc\n\nRANDOM_SEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/competitive-data-science-predict-future-sales'\nitems = pd.read_csv(PATH + '/items.csv')\nshops = pd.read_csv(PATH + '/shops.csv')\ncats = pd.read_csv(PATH + '/item_categories.csv')\ntrain = pd.read_csv(PATH + '/sales_train.csv')\n# set index to ID to avoid droping it later\ntest  = pd.read_csv(PATH + '/test.csv').set_index('ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.date = pd.to_datetime(train.date.index)\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def summary_stats_table(data):\n    '''\n    a functino to summerize all types of data\n    '''\n    # count of nulls\n    missing_counts = pd.DataFrame(data.isnull().sum())\n    missing_counts.columns = ['count_null']\n\n    # numeric column stats\n    num_stats = data.select_dtypes(include=['int64','float64']).describe().loc[['count','min','max']].transpose()\n    num_stats['dtype'] = data.select_dtypes(include=['int64','float64']).dtypes.tolist()\n\n    # non-numeric value stats\n    non_num_stats = data.select_dtypes(exclude=['int64','float64']).describe().transpose()\n    non_num_stats['dtype'] = data.select_dtypes(exclude=['int64','float64']).dtypes.tolist()\n    non_num_stats = non_num_stats.rename(columns={\"first\": \"min\", \"last\": \"max\"})\n\n    # merge all \n    stats_merge = pd.concat([num_stats, non_num_stats], axis=0, join='outer', ignore_index=False, keys=None,\n              levels=None, names=None, verify_integrity=False, copy=True).fillna(\"\").sort_values('dtype')\n\n    column_order = ['dtype', 'count', 'count_null','unique','min','max','top','freq']\n    summary_stats = pd.merge(stats_merge, missing_counts, left_index=True, right_index=True)[column_order]\n    return(summary_stats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check data"},{"metadata":{},"cell_type":"markdown","source":"## File\n* sales_train.csv - the training set. Daily historical data from January 2013 to October 2015.\n* test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.\n* sample_submission.csv - a sample submission file in the correct format.\n* items.csv - supplemental information about the items/products.\n* item_categories.csv  - supplemental information about the items categories.\n* shops.csv- supplemental information about the shops.\n\n## Columns\n\n* ID - an Id that represents a (Shop, Item) tuple within the test set\n* shop_id - unique identifier of a shop\n* item_id - unique identifier of a product\n* item_category_id - unique identifier of item category\n* item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n* item_price - current price of an item\n* date - date in format dd/mm/yyyy\n* date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n* item_name - name of item\n* shop_name - name of shop\n* item_category_name - name of item category"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.head())\nprint(test.head())\n\n# print(items.head())\n# print(shops.head())\n# print(cats.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_stats_table(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data clearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Drop item_price and item_cnt_day too high\ntrain = train[train.item_price<100000] # drop 1\ntrain = train[train.item_cnt_day<1000] # drop 2\n\n## Drop negative price \ntrain = train[train.item_price > 0].reset_index(drop=True) # drop 1\n\n## The item has been returned \n# train.loc[train.item_cnt_day < -2, 'item_cnt_day'] = -3\ntrain.loc[train.item_cnt_day < 0, 'item_cnt_day'] = 0\n\n## Fuse some shop in train and test set\n# Якутск Орджоникидзе, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# Якутск ТЦ \"Центральный\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# Жуковский ул. Чкалова 39м²\ntrain.loc[train.shop_id == 11, 'shop_id'] = 10\ntest.loc[test.shop_id == 11, 'shop_id'] = 10\n# РостовНаДону ТРК \"Мегацентр Горизонт\" Островной\ntrain.loc[train.shop_id == 40, 'shop_id'] = 39\ntest.loc[test.shop_id == 40, 'shop_id'] = 39\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Categorize shops info\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Define shop city and category\nshops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops['category'] = shops['shop_name'].str.split(' ').map(lambda x:x[1]).astype(str)\n\nshops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\n\n## Shop category\ncategory = ['ТЦ', 'ТРК', 'ТРЦ', 'ТК']\nshops.category = shops.category.apply(lambda x: x if (x in category) else 'etc')\nshops.groupby(['category']).sum()\n\n## Shop city\nshops['shop_city'] = shops.city\nshops['shop_category'] = shops.category\n\nshops['shop_city'] = LabelEncoder().fit_transform(shops['shop_city'])\nshops['shop_category'] = LabelEncoder().fit_transform(shops['shop_category'])\n\nshops = shops[['shop_id','shop_city', 'shop_category']]\nshops.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# cats info"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fuse some item_category_name\ncats['type_code'] = cats.item_category_name.apply(lambda x: x.split(' ')[0]).astype(str)\ncats.loc[(cats.type_code == 'Игровые') | (cats.type_code == 'Аксессуары'), 'type_code'] = 'Игры'\ncats.loc[cats.type_code == 'PC', 'type_code'] = 'Музыка'\n\n## Labelencoder item_category_name (main category) to type_code\ncategory = ['Игры', 'Карты', 'Кино', 'Книги','Музыка', 'Подарки', 'Программы', 'Служебные', 'Чистые']\ncats['type_code'] = cats.type_code.apply(lambda x: x if (x in category) else 'etc')\ncats['type_code'] = LabelEncoder().fit_transform(cats['type_code'])\n\n## Labelencoder item_category_name (sub category) to subtype_code\ncats['split'] = cats.item_category_name.apply(lambda x: x.split('-'))\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\n\ncats = cats[['item_category_id','type_code', 'subtype_code']]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# items info"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Extract info in [] and ()\nitems['name_1'], items['name_2'] = items['item_name'].str.split('[', 1).str\nitems['name_1'], items['name_3'] = items['item_name'].str.split('(', 1).str\n\n## Replace symbol by ',])'... by ' ' \nitems['name_2'] = items['name_2'].str.replace('[^A-Za-z0-9А-Яа-я]+', ' ').str.lower()\nitems['name_3'] = items['name_3'].str.replace('[^A-Za-z0-9А-Яа-я]+', ' ').str.lower()\nitems = items.fillna('0')\n\n## Test\nresult_1 = Counter(' '.join(items['name_2'].values.tolist()).split(' ')).items()\nresult_1 = sorted(result_1, key=itemgetter(1))\nresult_1 = pd.DataFrame(result_1, columns=['feature', 'count'])\nresult_1 = result_1[(result_1['feature'].str.len() > 1) & (result_1['count'] > 200)]\n\nresult_2 = Counter(' '.join(items['name_3'].values.tolist()).split(\" \")).items()\nresult_2 = sorted(result_2, key=itemgetter(1))\nresult_2 = pd.DataFrame(result_2, columns=['feature', 'count'])\nresult_2 = result_2[(result_2['feature'].str.len() > 1) & (result_2['count'] > 200)]\n\nresult = pd.concat([result_1, result_2])\nresult = result.drop_duplicates(subset=['feature']).reset_index(drop=True)\n\nprint('Most common aditional features:', result)\n\n\n## Fuse some types\nitems['type'] = items.name_2.apply(lambda x: x[0:8] if x.split(' ')[0] == 'xbox' else x.split(' ')[0])\nitems.loc[(items.type == 'x360') | (items.type == 'xbox360'), 'type'] = 'xbox 360'\nitems.loc[items.type == '', 'type'] = 'mac'\nitems.type = items.type.apply(lambda x: x.replace(' ',''))\nitems.loc[(items.type == 'pc') | (items.type == 'pс') | (items.type == 'рс'), 'type'] = 'pc'\nitems.loc[(items.type == 'рs3'), 'type'] = 'ps3'\n\n## Find low frequence item type\n# group_count = items[['item_id', 'type']].groupby('type').count()\n# drop_list = group_count.loc[group_count.item_id < 10].index\ngroup_sum = items.groupby('type').sum()\ndrop_list = group_sum.loc[group_sum.item_category_id < 200].index\n\nprint('drop list:', drop_list)\n\n## Replece low frequence item type by etc in name_2\nitems.name_2 = items.type.apply(lambda x: 'etc' if x in drop_list else x)\nitems = items.drop(['type'], axis=1)\nprint(items.groupby('name_2').count()[['item_id']])\n\n## Labelencoder name_2, name_3\nitems['name_2'] = LabelEncoder().fit_transform(items['name_2']).astype(np.int8)\nitems['name_3'] = LabelEncoder().fit_transform(items['name_3']).astype(np.int16)\nitems.drop(['item_name', 'name_1'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Month sales treatment\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Prepare matrix for (shop_id，item_id) couple\nts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\nprint('Use time:', time.time() - ts)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## revenue from one item on one day's sale\ntrain['revenue'] = train['item_price'] *  train['item_cnt_day']\n\n## Fill known month sale data in the matrix (all (shop_id，item_id) couple)\nts = time.time()\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\n\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)#.astype(np.float16))\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))\nprint('Use time:', time.time() - ts)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test set preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add test 34 month to matrix\ntest['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)\n\nts = time.time()\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month\nprint('Use time:', time.time() - ts)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Shops/Items/Cats features"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Attach shop, item, item_category information to the matrix\n\nts = time.time()\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['shop_city'] = matrix['shop_city'].astype(np.int8)\nmatrix['shop_category'] = matrix['shop_category'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\n\nprint('Use time:', time.time() - ts)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Traget lags"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'-lag'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lag 1,2,3 month\nts = time.time()\nmatrix = lag_feature(matrix, [1,2,3], 'item_cnt_month')\nprint('Use time:', time.time() - ts)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean encoded features"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"## Simplyfy feauture name dictionaries:\ndict_simple = {'date_block_num': 'date', 'item_id': 'item', 'shop_id': 'shop', \n               'item_category_id': 'itemcate', 'item_price':'price', \n               \n               'item_cnt_month': 'cnt', }\n\n## Functions for groupby and aggregate\ndef sum_names(name_list):\n    names = ''\n    for x in name_list:\n        names += x+'+'\n    return names\n    \ndef group_agg(matrix, groupby_feats, transform_feat, aggtype='mean'):\n    group = matrix.groupby(groupby_feats).agg({transform_feat: [aggtype]})\n    groupby_feats_simple = [dict_simple[x] if x in dict_simple.keys() else x\n                            for x in groupby_feats]\n    transform_feat_simple = dict_simple[transform_feat] \\\n                            if transform_feat in dict_simple.keys() else transform_feat\n    group_name = f'{sum_names(groupby_feats_simple)[:-1]}-{aggtype.upper()}-{transform_feat_simple}'\n    group.columns = [ group_name ]\n    group.reset_index(inplace=True)\n    return group, group_name\n    \ndef add_groupmean_lag(matrix, groupby_feats, transform_feat, lags):\n    group, group_name = group_agg(matrix, groupby_feats, transform_feat)\n    \n    matrix = pd.merge(matrix, group, on=groupby_feats, how='left')\n    matrix[group_name] = matrix[group_name].astype(np.float16)\n    if lags != []:\n        matrix = lag_feature(matrix, lags, group_name)\n        matrix.drop([group_name], axis=1, inplace=True)\n    return matrix\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\n## Compute mean item_cnt_month (cnt) \ntransform_feat = 'item_cnt_month'\n\n\ngroupby_feats = ['date_block_num']\nlags = [1]\nmatrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n\ngroupby_feats = ['date_block_num', 'item_id']\nlags = [1,2,3]\nmatrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n\ngroupby_feats = ['date_block_num', 'shop_id']\nlags = [1,2,3]\nmatrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n\ngroupby_feats = ['date_block_num', 'item_category_id']\nlags = [1]\nmatrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n\ngroupby_feats = ['date_block_num', 'shop_id', 'item_category_id']\nlags = [1]\nmatrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n\ngroupby_feats = ['date_block_num', 'shop_id', 'item_id']\nlags = [1]\nmatrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n\ngroupby_feats = ['date_block_num', 'shop_id', 'subtype_code']\nlags = [1]\nmatrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n\ngroupby_feats = ['date_block_num', 'shop_city']\nlags = [1]\nmatrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n\ngroupby_feats = ['date_block_num', 'item_id', 'shop_city']\nlags = [1]\nmatrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n\nprint('Use time:', time.time() - ts)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trend Features\n## Price trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\nfetures_to_drop = []\n\n## caculate mean price for every item\ntransform_feat = 'item_price'\ngroupby_feats = ['item_id']\ngroup, mean_price_col = group_agg(train, groupby_feats, \n                                  transform_feat, aggtype='mean')\nmatrix = pd.merge(matrix, group, on=groupby_feats, how='left')\nmatrix[mean_price_col] = matrix[mean_price_col].astype(np.float16)\n\n\n## caculate mean date (monthly) price for every item\ntransform_feat = 'item_price'\ngroupby_feats = ['date_block_num','item_id']\ngroup, mean_monthlyprice_col = group_agg(train, groupby_feats, \n                                         transform_feat, aggtype='mean')\nmatrix = pd.merge(matrix, group, on=groupby_feats, how='left')\nmatrix[mean_monthlyprice_col] = matrix[mean_monthlyprice_col].astype(np.float16)\n\n\n## create time lags for date date (monthly) price\nlags = [1,2,3]\nmatrix = lag_feature(matrix, lags, mean_monthlyprice_col)\n\n## delta between date (monthly) mean and mean price for every lag\nfor i in lags:\n    matrix['delta_price-lag'+str(i)] = \\\n    (matrix[f'{mean_monthlyprice_col}-lag'+str(i)] - matrix[mean_price_col])\\\n    / matrix[mean_price_col]\n\n\n############### Too slow ###############\n# def select_trend(row):\n#     for i in lags:\n#         if row['delta_price-lag'+str(i)]:\n#             return row['delta_price-lag'+str(i)]\n#     return 0\n# matrix['delta_price-lag'] = matrix.apply(select_trend, axis=1)\n\n############### 10 times faster ###############\nmatrix['delta_price-lag']=0\nbool_loc = np.ones(len(matrix))==1\nfor i in lags:   \n    matrix.loc[bool_loc, 'delta_price-lag'] = matrix.loc[bool_loc,'delta_price-lag'+str(i)]\n    bool_loc &= matrix['delta_price-lag'+str(i)]==0\nmatrix['delta_price-lag'] = matrix['delta_price-lag'].astype(np.float16)\nmatrix['delta_price-lag'].fillna(0, inplace=True)\n\n## Only keep 'delta_price_lag' feature\nfetures_to_drop.append(mean_price_col)\nfetures_to_drop.append(mean_monthlyprice_col)\nfor i in lags:\n    fetures_to_drop += [f'{mean_monthlyprice_col}-lag'+str(i)]\n    fetures_to_drop += ['delta_price-lag'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\nprint('Use time:', time.time() - ts)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Revenue trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\n## Monthly shop revenue sum\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\n\n## Shop mean revenue from Monthly shop revenue sum\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\n## delta between date (monthly) revenue and mean revenue\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\n## revenue lags\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\n## Only keep lag 'delta_revenue' feature\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\n\nprint('Use time:', time.time() - ts)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Date info"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Date information\ntotal_block_num = 35\ndate_block_num = np.arange(total_block_num)\ndate_block = [pd.Timestamp(2013, 1, 1)+pd.DateOffset(months=x) for x in date_block_num]\n\n\ndf_date = pd.DataFrame(date_block_num, columns=['date_block_num'])\ndf_date['date_block'] = date_block\ndf_date['year'] = df_date['date_block'].dt.year\ndf_date['month'] = df_date['date_block'].dt.month\n\nfor i in range(len(df_date)):\n    day_to_count = 0\n    calendar_matrix = calendar.monthcalendar(df_date['year'].iloc[i],df_date['month'].iloc[i])\n    for j in range(7): # 7 days a week\n        num_days = sum(1 for x in calendar_matrix if x[j] != 0)\n        df_date.loc[i, f'week{j}'] = num_days\ndf_date = df_date[['date_block_num', 'year','month','week0','week1',\n                   'week2','week3','week4','week5','week6']]  \ndf_date['days'] = df_date[['week0','week1','week2','week3','week4','week5','week6']].sum(axis=1)\ndf_date['year'] = df_date['year']-2012\ndf_date = df_date.astype(np.int8)\n\nmatrix = pd.merge(matrix, df_date, on=['date_block_num'], how='left')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## The first month when one item is on sale\nmatrix['item_shop_first_sale'] = \\\nmatrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n\nmatrix['item_first_sale'] = \\\nmatrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Drop 3 first date (because of lag)\nmatrix = matrix[matrix.date_block_num > 3]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############### ???????????? ###############\n\ndef fill_na(df):\n    for col in df.columns:\n        if ('-lag' in col) & (df[col].isnull().any()):\n            print(col)\n            if ('cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nimport gc\n\ndel group\ndel items\ndel shops\ndel cats\ndel train\n# leave test for submission\ngc.collect();\n\nmatrix.to_pickle('../working/data.pkl')\n\ndel matrix\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_pickle('../working/data.pkl')\n# data = matrix\n# del matrix\n\ntest  = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv').set_index('ID')\n\nprint(len(data.columns))\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data = data[['date_block_num', 'shop_id', 'item_id', 'item_cnt_month', 'shop_city',\n#        'shop_category', 'item_category_id', 'name_2', 'name_3', 'type_code',\n#        'subtype_code', 'item_cnt_month-lag1', 'item_cnt_month-lag2',\n#        'item_cnt_month-lag3', 'date-MEAN-cnt-lag1',\n#        'date+item-MEAN-cnt-lag1', 'date+item-MEAN-cnt-lag2',\n#        'date+item-MEAN-cnt-lag3', 'date+shop-MEAN-cnt-lag1',\n#        'date+shop-MEAN-cnt-lag2', 'date+shop-MEAN-cnt-lag3',\n#        'date+itemcate-MEAN-cnt-lag1', 'date+shop+itemcate-MEAN-cnt-lag1',\n#        'date+shop+item-MEAN-cnt-lag1',\n#        'date+shop+subtype_code-MEAN-cnt-lag1',\n#        'date+shop_city-MEAN-cnt-lag1', 'date+item+shop_city-MEAN-cnt-lag1',\n#        'delta_price-lag',\n#        'delta_revenue-lag1', 'month', 'days', 'item_shop_first_sale',\n#        'item_first_sale']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\n\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\ndel data\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train, Y_train)\nlgb_eval = lgb.Dataset(X_valid, Y_valid, reference=lgb_train)\n\ndel X_train\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n# params = {'num_leaves': 512, 'max_depth': 25, 'max_bin': 128, 'n_estimators': 3000, \n#           'bagging_freq': 7, 'bagging_fraction': 0.6, \n#           'feature_fraction': 0.2, 'min_data_in_leaf': 78, \n#           'learning_rate': 0.1, 'num_threads': 6, \n#           'min_sum_hessian_in_leaf': 6,\n\n#           'random_state' : RANDOM_SEED,\n#           'verbosity' : 1,\n#           'bagging_seed' : RANDOM_SEED,\n#           'boost_from_average' : 'true',\n#           'boost' : 'gbdt',\n#           'metric' : 'rmse',\n# }\n\nparams = {'num_leaves': 2000, 'max_depth': 19, 'max_bin': 107, 'n_estimators': 3747,\n          'bagging_freq': 1, 'bagging_fraction': 0.7135681370918421, \n          'feature_fraction': 0.49446461478601994, 'min_data_in_leaf': 88, \n          'learning_rate': 0.015980721586917768, 'num_threads': 3, \n          'min_sum_hessian_in_leaf': 6,\n         \n          'random_state' : RANDOM_SEED,\n          'verbosity' : 1,\n          'bagging_seed' : RANDOM_SEED,\n          'boost_from_average' : 'true',\n          'boost' : 'gbdt',\n          'metric' : 'rmse',}\n\nmodel = lgb.train(params,\n                lgb_train,\n                num_boost_round=20,\n                valid_sets=[lgb_train,lgb_eval],\n                early_stopping_rounds=20,\n                verbose_eval=1,\n                )\ny_pred = model.predict(X_valid)\nrmsle(Y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('lgb_submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('lgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('lgb_test.pickle', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from lightgbm import plot_importance\n# from xgboost import plot_importance\nfrom lightgbm import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10,14))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importance = model.feature_importance()\ndf_importance = pd.DataFrame(feat_importance, columns=['importance'], index=X_test.columns)\ndf_importance = df_importance.sort_values(by='importance', ascending=False)\ndf_importance.index\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python (dash)","language":"python","name":"dash"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}