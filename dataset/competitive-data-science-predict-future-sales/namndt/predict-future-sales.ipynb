{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Read data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nsales_train = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\nitems = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\nitem_categories = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\ntest = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Using Oct, 2015 as importance feature to predict Nov, 2015 sold.<br>\ndate_block_num of Oct,2015 is 33."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train[sales_train['date_block_num'] == 33].sort_values(by='item_cnt_day', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Something wrong with first row(2909818). Let check it"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train[sales_train['item_id'] == 11373].sort_values(['item_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train['item_price'][2909818] = np.nan\nsales_train['item_cnt_day'][2909818] = np.nan\n\nsales_train['item_price'][2909818] = sales_train[\n    (sales_train['shop_id'] ==12) & \n    (sales_train['item_id'] == 11373) &\n    (sales_train['date_block_num'] == 33)]['item_price'].median()\n\nsales_train['item_cnt_day'][2909818] = round(sales_train[(sales_train['shop_id'] ==12) &\n                                                         (sales_train['item_id'] == 11373) &\n                                                         (sales_train['date_block_num'] == 33)]['item_cnt_day'].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Almost item_id 11373 sold by shop_id 12. I will check that shop ~~"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train[sales_train['shop_id'] == 12].sort_values(by='item_price', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train[sales_train['item_id'] == 6066].sort_values(['item_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train[sales_train['item_id'] == 11365].sort_values(by='item_price', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train['item_price'][885138] = np.nan\nsales_train['item_price'][885138] = sales_train[(sales_train['item_id'] == 11365) &\n                                              (sales_train['shop_id'] ==12) &\n                                              (sales_train['date_block_num'] == 8)]['item_price'].median()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\n\nflierprops = dict(marker='o', markerfacecolor='purple', markersize=6,\n                  linestyle='none', markeredgecolor='black')\nsns.boxplot(x=sales_train.item_cnt_day, flierprops=flierprops)\n\nplt.figure(figsize=(10,4))\nplt.xlim(sales_train.item_price.min(), sales_train.item_price.max()*1.1)\n\nsns.boxplot(x=sales_train.item_price, flierprops=flierprops)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = sales_train[(sales_train.item_price < 300000 )& (sales_train.item_cnt_day < 1000)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = sales_train[sales_train.item_price > 0].reset_index(drop = True)\nsales_train.loc[sales_train.item_cnt_day < 1, \"item_cnt_day\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.loc[sales_train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n\nsales_train.loc[sales_train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n\nsales_train.loc[sales_train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Droping all the shop are in sales_train but are not in test"},{"metadata":{"trusted":true},"cell_type":"code","source":"in_train = np.sort(sales_train['shop_id'].unique())\nin_test = np.sort(test['shop_id'].unique())\n\nprint('list shop_id are in train set but are not in test set')\nfor i in in_train:\n    if i not in in_test:\n        print(i, end=', ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Before:', sales_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = sales_train.merge(test[['shop_id']].drop_duplicates(), how='inner')\n\nsales_train['date'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('After:', sales_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aggregate data"},{"metadata":{},"cell_type":"markdown","source":"For every month we create a grid from all shops/items combinations from that month"},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import product\n\ngrid = []\nfor block_num in sales_train['date_block_num'].unique():\n    cur_shops = sales_train[sales_train['date_block_num'] == block_num]['shop_id'].unique()\n    cur_items = sales_train[sales_train['date_block_num'] == block_num]['item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\nidx_columns = ['shop_id', 'item_id', 'date_block_num']\ngrid = pd.DataFrame(np.vstack(grid), columns = idx_columns, dtype = np.int32)\ngrid.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"in Evalution :\"*Submissions are evaluated by root mean squared error (RMSE). True target values are clipped into [0,20] range.\"* So, clip\\[0,20\\] in train and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train['item_cnt_day'] = sales_train['item_cnt_day'].clip(0,20)\n\ngb_cnt = sales_train.groupby(idx_columns)['item_cnt_day'].agg(['sum']).reset_index().rename(columns = {'sum': 'item_cnt_month'})\ngb_cnt['item_cnt_month'] = gb_cnt['item_cnt_month'].clip(0,20).astype(np.int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"join aggregated data to the grid"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(grid, gb_cnt, how='left', on=idx_columns).fillna(0)\ntrain['item_cnt_month'] = train['item_cnt_month'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cur_dtypes = [i for i in train if train[i].dtype in ['int32', 'int64']]\ntrain[cur_dtypes] = train[cur_dtypes].astype(np.int16)\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sort train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sort_values(['date_block_num','shop_id','item_id'], inplace=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sales_train['item_cnt_day'].sum())\nprint(train['item_cnt_month'].sum())\nprint(gb_cnt['item_cnt_month'].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(items[['item_id', 'item_category_id']], on = ['item_id'], how = 'left')\ntest = test.merge(items[['item_id', 'item_category_id']], on = ['item_id'], how = 'left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encode item_category_name"},{"metadata":{},"cell_type":"markdown","source":"I using item_categories-translated by [**deargle**](https://www.kaggle.com/c/competitive-data-science-predict-future-sales/discussion/54949)"},{"metadata":{"trusted":true},"cell_type":"code","source":"categ_translated = pd.read_csv('../input/predict-future-sales-english/item_categories-translated.csv')\n\nitem_categories = item_categories.merge(categ_translated[['item_category_name_translated', 'item_category_id']], on=['item_category_id'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, name in enumerate(item_categories.item_category_name_translated):\n    print(idx, name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_categ = list(item_categories.item_category_name_translated)\n\nfor i in range(1, 8):\n    list_categ[i] = 'Accessories'\nfor i in range(10, 18):\n    list_categ[i] = 'Game Consoles'\nfor i in range(18, 25):\n    list_categ[i] = 'Games'\nlist_categ[25] = 'Accessories for games'\nfor i in range(26, 28):\n    list_categ[i] = 'Phone Games'\nfor i in range(28, 32):\n    list_categ[i] = 'PC Games'\nfor i in range(32, 37):\n    list_categ[i] = 'Cards'\nfor i in range(37, 42):\n    list_categ[i] = 'Cinema'\nfor i in range(42, 55):\n    list_categ[i] = 'Books'\nfor i in range(55, 61):\n    list_categ[i] = 'Music'\nfor i in range(61, 73):\n    list_categ[i] = 'Gifts'\nfor i in range(73, 79):\n    list_categ[i] = 'Programs'\nfor i in range(79, 81):\n    list_categ[i] = 'Office'\nfor i in range(81, 83):\n    list_categ[i] = 'Clean'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nlb_encode = preprocessing.LabelEncoder()\nitem_categories['item_category_id_fixed'] = lb_encode.fit_transform(list_categ)\n\ntrain = train.merge(item_categories[['item_category_id_fixed', 'item_category_id']], on = ['item_category_id'], how = 'left')\n\ntest = test.merge(item_categories[['item_category_id_fixed', 'item_category_id']], on = ['item_category_id'], how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add item/shop pair mean-encodings"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.model_selection import KFold\n\nglobal_mean =  train['item_cnt_month'].mean()\ny_train = train['item_cnt_month'].values\n\nmean_encoded_col = ['shop_id', 'item_id', 'item_category_id', 'item_category_id_fixed']\n\nfor col in tqdm(mean_encoded_col):\n    col_train = train[[col] + ['item_cnt_month']]\n    corr_coefs = pd.DataFrame(columns = ['Cor'])\n    # Mean encoding KFold scheme\n    kf = KFold(n_splits = 5, shuffle = False, random_state = 0)\n    \n    col_train[col + '_cnt_month_mean_KFold'] = global_mean\n    for idx_train, idx_val in kf.split(col_train):\n        X_train, X_val = col_train.iloc[idx_train], col_train.iloc[idx_val]\n        means = X_val[col].map(X_train.groupby(by=col)['item_cnt_month'].mean())\n        X_val[col + '_cnt_month_mean_KFold'] = means\n        col_train.iloc[idx_val] = X_val\n        print(X_val.head(5))\n    col_train.fillna(global_mean, inplace=True)\n    corr_coefs.loc[col + '_cnt_month_mean_KFold'] = np.corrcoef(y_train, col_train[col + '_cnt_month_mean_KFold'])[0][1]\n    \n    # Mean encodings - Leave-one-out scheme\n    item_id_target_sum = col_train.groupby(col)['item_cnt_month'].sum()\n    item_id_target_count = col_train.groupby(col)['item_cnt_month'].count()\n    col_train[col + '_cnt_month_sum'] = col_train[col].map(item_id_target_sum)\n    col_train[col + '_cnt_month_count'] = col_train[col].map(item_id_target_count)\n    col_train[col + '_target_mean_LOO'] = (col_train[col + '_cnt_month_sum'] - col_train['item_cnt_month']) / (col_train[col + '_cnt_month_count'] - 1)\n    col_train.fillna(global_mean, inplace = True)\n    corr_coefs.loc[col + '_target_mean_LOO'] = np.corrcoef(y_train, col_train[col + '_target_mean_LOO'])[0][1]\n    \n    # Mean encodings - Smoothing\n    item_id_target_mean = col_train.groupby(col)['item_cnt_month'].mean()\n    item_id_target_count = col_train.groupby(col)['item_cnt_month'].count()\n    col_train[col + '_cnt_month_mean'] = col_train[col].map(item_id_target_mean)\n    col_train[col + '_cnt_month_count'] = col_train[col].map(item_id_target_count)\n    alpha = 100\n    col_train[col + '_cnt_month_mean_Smooth'] = (col_train[col + '_cnt_month_mean'] *  col_train[col + '_cnt_month_count'] + global_mean * alpha) / (alpha + col_train[col + '_cnt_month_count'])\n    col_train[col + '_cnt_month_mean_Smooth'].fillna(global_mean, inplace=True)\n    corr_coefs.loc[col + '_cnt_month_mean_Smooth'] = np.corrcoef(y_train, col_train[col + '_cnt_month_mean_Smooth'])[0][1]\n    \n    # 3.1.4 Mean encodings - Expanding mean scheme\n    cumsum = col_train.groupby(col)['item_cnt_month'].cumsum() - col_train['item_cnt_month']\n    sumcnt = col_train.groupby(col).cumcount()\n    col_train[col + '_cnt_month_mean_Expanding'] = cumsum / sumcnt\n    col_train[col + '_cnt_month_mean_Expanding'].fillna(global_mean, inplace=True)\n    corr_coefs.loc[col + '_cnt_month_mean_Expanding'] = np.corrcoef(y_train, col_train[col + '_cnt_month_mean_Expanding'])[0][1]\n    train = pd.concat([train, col_train[corr_coefs['Cor'].idxmax()]], axis = 1)\n    print(corr_coefs.sort_values('Cor'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nov, 2015\ntest['date_block_num'] = 34\n\nall_data = pd.concat([train, test], axis = 0)\nall_data = all_data.drop(columns = ['ID'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"float_col = [f for f in all_data if all_data[f].dtype == 'float64']\nint_col = [i for i in all_data if all_data[i].dtype == 'int64']\n\nall_data[float_col] = all_data[float_col].astype(np.float32)\nall_data[int_col] = all_data[int_col].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index_cols = ['shop_id', 'item_id', 'item_category_id', 'item_category_id_fixed', 'date_block_num']\ncols_to_rename = list(all_data.columns.difference(index_cols))\n\nshift_range = [1, 2, 3, 4, 12]\nfor month_shift in tqdm(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n    \n    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\ndel train_shift","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12]\n\nlag_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]]\n\nfloat_col = [f for f in all_data if all_data[f].dtype == 'float64']\nint_col = [i for i in all_data if all_data[i].dtype in ['int64', 'int32']]\n\nall_data[float_col] = all_data[float_col].astype(np.float32)\nall_data[int_col] = all_data[int_col].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating date feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"dates_train = sales_train[['date', 'date_block_num']].drop_duplicates()\ndates_test = dates_train[dates_train['date_block_num'] == 34-12]\ndates_test['date_block_num'] = 34\ndates_test['date'] = dates_test['date'] + pd.DateOffset(years=1)\ndates_all = pd.concat([dates_train, dates_test])\n\n\ndates_all['dow'] = dates_all['date'].dt.dayofweek\ndates_all['year'] = dates_all['date'].dt.year\ndates_all['month'] = dates_all['date'].dt.month\n\ndates_all = pd.get_dummies(dates_all, columns=['dow'])\n\ndow_col = ['dow_' + str(x) for x in range(7)]\n\ndate_features = dates_all.groupby(['year', 'month', 'date_block_num'])[dow_col].agg('sum').reset_index()\ndate_features['days_of_month'] = date_features[dow_col].sum(axis=1)\ndate_features['year'] = date_features['year'] - 2013\ndate_features = date_features[['month', 'year', 'days_of_month', 'date_block_num']]\n\nall_data = all_data.merge(date_features, on = 'date_block_num', how = 'left')\n\ndate_columns = date_features.columns.difference(set(index_cols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scale feature columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ntrain = all_data[all_data['date_block_num']!= all_data['date_block_num'].max()]\ntest = all_data[all_data['date_block_num']== all_data['date_block_num'].max()]\nsc = StandardScaler()\n\nto_drop_cols = ['date_block_num']\nfeature_columns = list(set(lag_cols + index_cols + list(date_columns)).difference(to_drop_cols))\n\ntrain[feature_columns] = sc.fit_transform(train[feature_columns])\n\ntest[feature_columns] = sc.transform(test[feature_columns])\n\nall_data = pd.concat([train, test], axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"float_col = [f for f in all_data if all_data[f].dtype == 'float64']\nint_col = [i for i in all_data if all_data[i].dtype in ['int64', 'int32']]\n\nall_data[float_col] = all_data[float_col].astype(np.float32)\nall_data[int_col] = all_data[int_col].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"dates = all_data['date_block_num']\nprint('Test `date_block_num` is %d' % dates.max())\nprint(len(feature_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[all_data[\"date_block_num\"]==34].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = all_data[all_data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = all_data[all_data.date_block_num < 33]['item_cnt_month']\nX_valid = all_data[all_data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = all_data[all_data.date_block_num == 33]['item_cnt_month']\nX_test = all_data[all_data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train = Y_train.clip(0, 20)\nY_valid = Y_valid.clip(0, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom matplotlib.pylab import rcParams\n\nmodel = XGBRegressor(max_depth=10, n_estimators=1000, min_child_weight=0.5, colsample_bytree=0.8, subsample=0.8, eta=0.1, seed=42)\n\nmodel.fit(X_train, Y_train, eval_metric=\"rmse\", eval_set=[(X_train, Y_train), (X_valid, Y_valid)], verbose=True, early_stopping_rounds = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_1 = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({'ID': test_1.index, 'item_cnt_month': Y_test})\nsubmission.to_csv('xgb_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10,14))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nscoringMethod = 'r2'\nnum_first_level_models = 3\n\n# Train meta-features M = 15 (12 + 15 = 27)\nmonths_to_generate_meta_features = range(27,last_block +1)\nmask = dates.isin(months_to_generate_meta_features)\n\ntarget = 'item_cnt_month'\n\ny_all_level2 = all_data[target][mask].values\n\nX_all_level2 = np.zeros([y_all_level2.shape[0], num_first_level_models])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, SGDRegressor\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\nslice_start = 0\nTarget = 'item_cnt_month'\n\nfor cur_block_num in tqdm(months_to_generate_meta_features):\n    print('-' * 50)\n    print('Start training for month%d'% cur_block_num)\n\n    cur_X_train = all_data.loc[dates <  cur_block_num][feature_columns]\n    cur_X_test =  all_data.loc[dates == cur_block_num][feature_columns]\n\n    cur_y_train = all_data.loc[dates <  cur_block_num, Target].values\n    cur_y_test =  all_data.loc[dates == cur_block_num, Target].values\n\n    # Create Numpy arrays of train, test and target dataframes to feed into models\n    train_x = cur_X_train.values\n    train_y = cur_y_train.ravel()\n    test_x = cur_X_test.values\n    test_y = cur_y_test.ravel()\n\n    preds = []\n\n    sgdr= SGDRegressor(penalty = 'l2', random_state = 0 )\n    lgb_params = {'feature_fraction': 0.75, 'metric': 'rmse', 'nthread':1, 'min_data_in_leaf': 2**7,\n                  'bagging_fraction': 0.75, 'learning_rate': 0.03, 'objective': 'mse', 'bagging_seed': 2**7,\n                  'num_leaves': 2**7, 'bagging_freq':1, 'verbose':0 }\n\n    estimators = [sgdr]\n    \n    for estimator in estimators:\n        print('Training Model %d: %s'%(len(preds), estimator.__class__.__name__))\n        estimator.fit(train_x, train_y)\n        pred_test = estimator.predict(test_x)\n        preds.append(pred_test)\n\n        # pred_train = estimator.predict(train_x)\n        # print('Train RMSE for %s is %f' % (estimator.__class__.__name__, sqrt(mean_squared_error(cur_y_train, pred_train))))\n        print('Test RMSE for %s is %f' % (estimator.__class__.__name__, sqrt(mean_squared_error(cur_y_test, pred_test))))\n\n    print('Training Model %d: %s'%(len(preds), 'lightgbm'))\n    estimator = lgb.train(lgb_params, lgb.Dataset(train_x, label=train_y), 300)\n    pred_test = estimator.predict(test_x)\n    preds.append(pred_test)\n    \n    # pred_train = estimator.predict(train_x)\n    # print('Train RMSE for %s is %f' % ('lightgbm', sqrt(mean_squared_error(cur_y_train, pred_train))))\n    print('Test RMSE for %s is %f' % ('lightgbm', sqrt(mean_squared_error(cur_y_test, pred_test))))\n\n    print('Training Model %d: %s'%(len(preds), 'keras'))\n\n    def baseline_model():\n        model = Sequential()\n        model.add(Dense(20, input_dim=train_x.shape[1], kernel_initializer='uniform', activation='softplus'))\n        model.add(Dense(1, kernel_initializer='uniform', activation = 'relu'))\n        \n        # Compile model\n        model.compile(loss='mse', optimizer='Nadam', metrics=['mse'])\n        # model.compile(loss='mean_squared_error', optimizer='adam')\n        return model\n\n    estimator = KerasRegressor(build_fn=baseline_model, verbose=1, epochs=5, batch_size = 55000)\n    estimator.fit(train_x, train_y)\n    pred_test = estimator.predict(test_x)\n    preds.append(pred_test)\n\n\n    slice_end = slice_start + cur_X_test.shape[0]\n\n    X_all_level2[ slice_start : slice_end , :] = np.c_[preds].transpose()\n\n    slice_start = slice_end","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_nrow = len(preds[0])\nX_train_level2 = X_all_level2[ : -test_nrow, :]\nX_test_level2 = X_all_level2[ -test_nrow: , :]\ny_train_level2 = y_all_level2[ : -test_nrow]\ny_test_level2 = y_all_level2[ -test_nrow : ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, SGDRegressor\n\npred_list = {}\n\nlr = LinearRegression()\nlr.fit(X_train_level2, y_train_level2)\n\ntest_preds_lr_stacking = lr.predict(X_test_level2)\ntrain_preds_lr_stacking = lr.predict(X_train_level2)\n\nprint('Train R-squared for %s is %f' %('train_preds_lr_stacking', sqrt(mean_squared_error(y_train_level2, train_preds_lr_stacking))))\n\npred_list['test_preds_lr_stacking'] = test_preds_lr_stacking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgdr= SGDRegressor(penalty = 'l2', random_state = 0 )\n\nsgdr.fit(X_train_level2, y_train_level2)\n\ntest_preds_sgdr_stacking = sgdr.predict(X_test_level2)\ntrain_preds_sgdr_stacking = sgdr.predict(X_train_level2)\n\nprint('Train R-squared for %s is %f' %('train_preds_lr_stacking', sqrt(mean_squared_error(y_train_level2, train_preds_sgdr_stacking))))\n\npred_list['test_preds_sgdr_stacking'] = test_preds_sgdr_stacking","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Submision"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = sample_submission\nsubmission_path = './'\n\nfor pred_ver in ['lr_stacking', 'sgdr_stacking']:\n    print(pred_list['test_preds_' + pred_ver].clip(0,20).mean())\n    \n    submission['item_cnt_month'] = pred_list['test_preds_' + pred_ver].clip(0,20)\n    \n    submission[['ID', 'item_cnt_month']].to_csv('{0}/{1}.csv'.format(submission_path, pred_ver), index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}