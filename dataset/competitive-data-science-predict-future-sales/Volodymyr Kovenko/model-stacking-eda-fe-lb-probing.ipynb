{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello everyone! The notebook is related to Future sales prediction task. It consists of EDA, feature engineering, leaderboard probing and finally model training. I tried to grasp all the concepts learned in the course (https://www.coursera.org/learn/competitive-data-science) and fullfill them here. Please notice that some of concepts were borrowed from other competitors and from forum, you will find the links to them by following the notebook.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Part 1 : EDA","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"! ls ../input/competitive-data-science-predict-future-sales/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all, we will read and explore data. We will start with item_categories dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nitem_categories.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories['item_category_id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should also check for some additional features at item_category_name field.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories['item_category_name'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's obvious that from the related field we can already make some new.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we can move on to items.csv.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.item_id.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.item_category_id.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how much items related to each category we have.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,14))\nitems.groupby('item_category_id')['item_id'].size().plot.barh(rot=0)\nplt.title('Number of items related to different categories')\nplt.xlabel('Categories')\nplt.ylabel('Number of items');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the chart above that's obvious that there are some categories that are the most popular.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's understand what is the name of the categories which conist of maximum and minumum number of items.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items.groupby('item_category_id')['item_id'].size().mean(), items.groupby('item_category_id')['item_id'].size().max(),items.groupby('item_category_id')['item_id'].size().min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories[item_categories['item_category_id'].isin(items.groupby('item_category_id')['item_id'].size().nlargest(5).index)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's pretty obvious that lots of items are related to movies stuff.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories[item_categories['item_category_id']\\\n                .isin((items.groupby('item_category_id')['item_id'].size()[items.groupby('item_category_id')['item_id'].size()==1])\\\n                      .index)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that there are few categories that have only one related item. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It's useful to examine if we have any category that doesn't have any item or if we have any item that belongs to more than one category.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(items.groupby('item_category_id')['item_id'].size()==0).astype(int).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that all the items belong to at least one category.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(items.groupby('item_id')['item_category_id'].size()>=2).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And obviously we have only one item per category. Now let's merge two dataframes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items_categories_merged = items.merge(item_categories,left_on='item_category_id',right_on='item_category_id',how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_categories_merged.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ncounter = Counter([i for i in np.hstack(items_categories_merged['item_name'].str.split(' ').values) if i])\nsorted(counter.items(),key=lambda x: x[1])[::-1][:30]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that there are some words that tend to appear much frequently than others. Maybe we should make a feature based on it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(items_categories_merged), len(items)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the length of data before and after merge is the same, it seems that we haven't missed any values. Now we will go on with data containing shops.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(shops)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that from the shop_name we can already retrieve two new features : name of the city of the shop and type of the shop.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.shop_name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's move to train_sales data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check for NaN's.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales.isnull().sum(axis=1).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obviously there are no NaN's or they are imputed by other values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that our item_price field contains -1 value as minimum and 3e05 as maximum. This values could be NaN's or outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check if our dataset is shuffled.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"to_plot = train_sales['item_cnt_day'].rolling(5).sum()\nplt.figure(figsize=(14,8))\nplt.plot(range(len(to_plot.index)),to_plot.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From plot above that's obvious that data is shuffled.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's make two new features for simplicity, mainly year, month and day.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales['day'] = train_sales['date'].apply(lambda x: x.split('.')[0])\ntrain_sales['month'] = train_sales['date'].apply(lambda x: x.split('.')[1])\ntrain_sales['year'] = train_sales['date'].apply(lambda x: x.split('.')[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's important to now in which date range we are working, so let's understand what is the minimum and maximum dates.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales['date'] = pd.to_datetime(train_sales['date'],format='%d.%m.%Y')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales['date'].min(),train_sales['date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My hypotesis is that the data is grouped by date_block_num, shop_id and item_id as the date is a bit unordered.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's make some analysis of timesires data. First of all we will plot sum of item_cnt_day grouped by different date related columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1,figsize=(16,20))\nfig.tight_layout(pad=3.0)\n\nto_plot = train_sales.groupby('date',as_index=False)['item_cnt_day'].sum().reset_index()\nz = np.polyfit(y=to_plot['item_cnt_day'],x=to_plot['index'], deg=1)\np = np.poly1d(z)\nax1.plot(to_plot['date'],to_plot['item_cnt_day'],'-')\nax1.plot(to_plot['date'],p(to_plot['index'].values),'--r')\nax1.legend(['Sum of sold items','Trend line'])\nax1.title.set_text('Sum of sold items by date')\n\nto_plot = train_sales.groupby('day')['item_cnt_day'].sum()\nax2.plot(to_plot.values,'-o')\nax2.title.set_text('Sum of sold items by day')\nax2.set_xticks(range(len(to_plot)))\n\nto_plot = train_sales.groupby('month')['item_cnt_day'].sum()\nax3.plot(to_plot.values,'-o')\nax3.title.set_text('Sum of sold items by month')\nax3.set_xticks(range(len(to_plot)))\n\nto_plot = train_sales.groupby('year',as_index=False)['item_cnt_day'].sum()\nax4.plot(to_plot['item_cnt_day'].values,'-o')\nax4.title.set_text('Sum of sold items by year')\nax4.set_xticks(range(len(to_plot)))\nax4.set_xticklabels(list(to_plot['year'].values));\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's obvious that with time number of item sold is decreasing, let's now plot the situation for the price.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1,figsize=(16,20))\nfig.tight_layout(pad=3.0)\n\nto_plot = train_sales.groupby('date',as_index=False)['item_price'].mean().reset_index()\nz = np.polyfit(y=to_plot['item_price'],x=to_plot['index'], deg=1)\np = np.poly1d(z)\nax1.plot(to_plot['date'],to_plot['item_price'],'-')\nax1.plot(to_plot['date'],p(to_plot['index'].values),'--r')\nax1.legend(['Mean price of items','Trend line'])\nax1.title.set_text('Mean price of items by date')\n\nto_plot = train_sales.groupby('day')['item_price'].mean()\nax2.plot(to_plot.values,'-o')\nax2.title.set_text('Mean price of items by day')\nax2.set_xticks(range(len(to_plot)))\n\nto_plot = train_sales.groupby('month')['item_price'].mean()\nax3.plot(to_plot.values,'-o')\nax3.title.set_text('Mean price of items by month')\nax3.set_xticks(range(len(to_plot)))\n\nto_plot = train_sales.groupby('year',as_index=False)['item_price'].mean()\nax4.plot(to_plot['item_price'].values,'-o')\nax4.title.set_text('Mean price of items by year')\nax4.set_xticks(range(len(to_plot)))\nax4.set_xticklabels(list(to_plot['year'].values));\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that as mean price goes higher, the sum of sold items goes lower meaning that there is a dependency between price and item_cnt. We should probably use this as feature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's plot the averaged, maximum and minumum revenue per month during all the time. But first of all let's see how monthes are distributed across the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_monthes = dict(train_sales['month'].value_counts())\nmonthes, frequencies = zip(*sorted(dict_monthes.items(),key=lambda x: int(x[0][1]) if x[0][0]=='0' else int(x[0])))\nplt.figure(figsize=(15,12))\nplt.bar(range(len(monthes)),frequencies)\nplt.title('Distribution of monthes in dataset')\nplt.xticks(range(len(monthes)),monthes);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see monthes occurancies are in general evenly distributed across dataset, only January (01) appears more freuqently than others.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales['revenue'] = train_sales['item_price']*train_sales['item_cnt_day']\nplt.figure(figsize=(14,8))\ntrain_sales.groupby('month')['revenue'].mean().plot.bar(rot=0)\nplt.title('Averaged revenue per month')\nplt.xlabel('Monthes')\nplt.ylabel('Revenue');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's obvious that the averaged revenue is bigger during the 12th month, as the dates related to it are close to New Year holiday. Let's now visualize min and max values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\ntrain_sales.groupby('month')['revenue'].max().plot.bar(rot=0)\nplt.title('Maximum revenue per month')\nplt.xlabel('Monthes')\nplt.ylabel('Revenue');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hm, we have a maximum value for 11th month, that's interesting, it could be because of \"Black Friday\". As we in revenue column we can have negative values (if the goods are returned) we will visualize only the data rows that have values > 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\ntrain_sales[train_sales['revenue']>0].groupby('month')['revenue'].min().plot.bar()\nplt.title('Minimum revenue per month')\nplt.xlabel('Monthes')\nplt.ylabel('Revenue');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\ntrain_sales.groupby('date_block_num')['revenue'].mean().plot.bar(rot=0)\nplt.title('Averaged revenue per month (count)')\nplt.xlabel('Relative number of monthes')\nplt.ylabel('Revenue');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also want to make the same for day column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\ntrain_sales.groupby('day')['revenue'].mean().plot.bar(rot=0)\nplt.title('Averaged revenue per day')\nplt.xlabel('Days')\nplt.ylabel('Revenue');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_revenue_day_month = train_sales.groupby(['month','day'])['revenue'].mean()\n\nmean_revenue_day_month[mean_revenue_day_month.isin(mean_revenue_day_month.nlargest(5))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we search for date 2013/11/29, we will find out that it was the date of black friday in russia, thus that's adequate to have the maximum revenue on this day.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's visualize the revenue for each year and for each week day and make some additional charts.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales['year'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\ntrain_sales.groupby('year')['revenue'].mean().plot.bar(rot=0)\nplt.title('Averaged revenue per year')\nplt.xlabel('Year')\nplt.ylabel('Revenue');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales['dayname'] = train_sales['date'].dt.day_name()\ntrain_sales.groupby('dayname')['revenue'].mean().plot.bar(rot=90)\nplt.title('Averaged revenue per week day')\nplt.xlabel('Week day')\nplt.ylabel('Revenue');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales['dayname'].value_counts().plot.bar()\nplt.title('Distribution of week days in dataframe');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also visualize distribution of averaged item_price.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\nplt.title('Distribution of mean item_price')\nmean_price = train_sales.groupby(['shop_id','item_id','date_block_num'])['item_price'].mean().values\nplt.hist(mean_price,bins=30)\nplt.xlabel('Values')\nplt.ylabel('Frequency');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\nplt.title('Distribution of mean item_price on log scale')\nplt.hist(np.log1p(train_sales.groupby(['shop_id','item_id','date_block_num'])['item_price'].mean().values),bins=30)\nplt.xlabel('Values')\nplt.ylabel('Frequency');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that item_prices are normally distributed with some outliers, thus we will need to clip them or to use log scale.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(train_sales['month'],train_sales['item_price']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales[train_sales['item_price']==train_sales['item_price'].max()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales[train_sales['item_price']==train_sales['item_price'].min()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's derive some deeper insights from our data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_per_item = (train_sales.groupby('item_id')['shop_id'].nunique()>=2).astype(int).sum()\nprint('There are {0} items that relate to more than one shop'.format(shops_per_item))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's obviously that we can have multiple shops that sell one item. Let's now understand the difference between training and testing datasets. We already now that data in testing dataset is montly aggregated.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(train_sales['item_id'].value_counts()==1).astype(int).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For sure there are some items that are out of date. Let's compare our dataframe with test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sales = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\ntest_sales.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sales['shop_id'].value_counts().unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sales['item_id'].value_counts().unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's strange that we have the same amount of events related to each shop and each item. By multiplying those values together we will get the exact number of rows as in dataset, and that's very strange, mainly it seems that the test set is just a catalog of items for which we need to predict prices. The other thing is that if we look on shop_id and item_id columns we will notice that the data is ordered a bit. Ordered by the shop_id and item_id columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diff_test_items = set(train_sales.item_id.unique()).difference(test_sales.item_id.unique())\nprint('Number of items that are in train set, but are not in test one : {0}'.format(len(diff_test_items))) \ndiff_train_items = set(test_sales.item_id.unique()).difference(train_sales.item_id.unique())\nprint('Number of items that are in test set, but are not in train one : {0}'.format(len(diff_train_items))) \ndiff_test_shops = set(train_sales.shop_id.unique()).difference(test_sales.shop_id.unique())\nprint('Number of shops that are in train set, but are not in test one : {0}'.format(len(diff_test_shops))) \ndiff_train_shops = set(test_sales.shop_id.unique()).difference(train_sales.shop_id.unique())\nprint('Number of shops that are in test set, but are not in train one : {0}'.format(len(diff_train_shops))) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that there are items which weren't used in the test set at all! And the same for the train one. We can also see that there some shops which are not included in test set. To deal with items we will than make the empty dataframe which will have all the possible products of item_id,shop_id and date_block_num and merge it with our ones. For now let's work with item_cnt_day column and make some usefull plots.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,12))\ndict_returned = dict(train_sales[train_sales['item_cnt_day']<0].month.value_counts())\ndict_returned = dict(sorted(dict_returned.items(), key=lambda x: int(x[0][1]) if x[0][0]=='0' else int(x[0])))\nplt.bar(range(len(dict_returned.values())),dict_returned.values())\nplt.xticks(range(len(dict_returned.values())),dict_returned.keys())\nplt.title('Number of times the goods were returned during different monthes')\nplt.xlabel('Monthes')\nplt.ylabel('Cases of returning the goods');\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,12))\ndict_returned = dict(train_sales[train_sales['item_cnt_day']<0].date_block_num.value_counts())\ndict_returned = dict(sorted(dict_returned.items(), key=lambda x: int(x[0])))\nplt.bar(range(len(dict_returned.values())),dict_returned.values())\nplt.xticks(range(len(dict_returned.values())),dict_returned.keys())\nplt.title('Number of times the goods were returned during different date_block_num')\nplt.xlabel('date_block_num')\nplt.ylabel('Cases of returning the goods');\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plots above we can say that people tend to return items right after the New Year. Maybe it's because their presents weren't so good. Basically, our hypotezis can be wrong, as events related to January seems to appear oftener than others, but I don't think that this is the case, as events related to December (12) appears oftener than the ones related to February (02), but still more items are returned during February. Now let's see what items are returned most often.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(train_sales[train_sales['item_cnt_day']<0]['item_id'].value_counts()).nlargest(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = train_sales[train_sales['item_cnt_day']<0]['item_id'].value_counts()\nidx = list(sales[sales>=10].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_categories_merged[items_categories_merged['item_id'].isin(idx)].item_category_name.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that most of all - games are returned. Basically the above analysis didn't help us to derive new features but we got the point that we should probably make features based on categories and types of items. So far we got the following things:\n\n1. We can make additional features from item_categories df such as type of category. \n1. We can make additional features from shop_name field in shops df such as shop_name and shop_type.\n1. We can make additional features based on item name, maybe using tfidf or count vectorizer.\n1. We should probably concat our data with all the other shops, date_block_nums and item_ids, if there is a missing one it means that it just wasn't sold. Also it's benefitial to make our data of the same format as test one.\n1. There is a dependency between the revenue and month number, thus people tend to by more products during monthes that have holidays, thus we can add a new feature indicating if month has a holiday plus number of month.\n1. There is a dependency between price and number of sold items, we can make some time series features based on it, but we should also remember to deal with outliers in item price. Also there is a way to construct new features via mean encoding (as we have lot's of categorical features).\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Part 2: Leader board probing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')\nsubmission[\"item_cnt_month\"] = 1\nsubmission.to_csv('lb_probing1.csv',index=False)\nsubmission[\"item_cnt_month\"] = 0\nsubmission.to_csv('lb_probing2.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now submit two predictions and calculate the mean of the leader board target. We then can use it to make our score better and to align cross_validation set with test one. We will use the following calculations (full conversation about LB probing is accessible by the following [link](https://www.kaggle.com/c/competitive-data-science-predict-future-sales/discussion/79142)).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"$MSE0 = \\frac{ \\sum{i=1}^{N} (yi - 0)^2 }{N} = \\frac{ \\sum{i=1}^{N} y_i^2 }{N}$\n\n$MSE1 = \\frac{ \\sum{i=1}^{N} (yi - 1)^2 }{N} = \\frac{ \\sum{i=1}^{N} (yi^2 - 2yi + 1) }{N} = \\frac{ \\sum{i=1}^{N} yi^2 - 2 \\sum{i=1}^{N} yi + N }{N} = MSE0 - \\frac{2}{N} \\sum{i=1}^{N} y_i + 1$\n\n$\\frac{\\sum{i=1}^{N} yi}{N} = \\frac{MSE1 - MSE0 - 1}{-2}$","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After the submission we see that the score for MSE0 is $1.25011^2$ and for MSE1 is $1.41241^2$. Let's now calculate the target mean of public leaderboard.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_mean = (1.41241**2-1.25011**2-1)/-2\nprint('Mean of target values in public leaderboard is : {0}'.format(y_hat_mean))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 3: Feature engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## (1,3). Additional features based on items_categories_merged df.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items_categories_merged.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the code below we will make a new feature name type_of_category, make some item_name cleaning (exlucde mess) and construct tfidf features based on it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def exclude_preprositions(x):\n    x = x.split(' ')\n    x = ' '.join(i for i in x if not i in prepositions_to_exclude).strip()\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nitems_categories_merged['type_of_category']=items_categories_merged['item_category_name'].apply(lambda x: x.split(' ')[0].strip())\ndict_types = dict(items_categories_merged['type_of_category'].value_counts())\ncat, _ = zip(*sorted(dict_types.items(),key=lambda x: x[1])[::-1][:5])\nprint('Most frequent types of categories : {0}'.format(cat))\nnum_features = 10\nsymbols_to_exclude = ['[',']','!','.',',','*','(',')','\"',':']\nprepositions_to_exclude = ['в','на','у','the','a','an','of','для']\nfor symbol in symbols_to_exclude:\n    items_categories_merged['item_name'] = items_categories_merged['item_name'].str.replace(symbol,'')\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.lower()\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.replace('-',' ')\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.replace('/',' ')\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.strip()\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].apply(exclude_preprositions)\nvectorizer = TfidfVectorizer(max_features=num_features)\nres = vectorizer.fit_transform(items_categories_merged['item_name'])\nprint('Top {0} features of tfidf : {1}'.format(num_features,vectorizer.get_feature_names()))\ncount_vect_df = pd.DataFrame(res.todense(), columns=vectorizer.get_feature_names())\nitems_categories_merged = pd.concat([items_categories_merged,count_vect_df],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_categories_merged['type_of_category'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_categories_merged.drop(columns=['item_name','item_category_name'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ndel vectorizer, res, count_vect_df\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Additional features based on shop_name.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef create_city_name(x):\n    for i in not_city:\n        if i in x:\n            return 'unk_city'\n    return x.split(' ')[0].strip()\ndef create_shop_type(x):\n    to_return = 'unk_type'\n    for i in type_of_shops:\n        regex = re.compile(i)\n        if re.search(regex,x):\n                to_return = i \n    return to_return\nnot_city = ['Выездная Торговля','Интернет-магазин','Цифровой склад 1С-Онлайн']\ntype_of_shops = ['ТРЦ', 'ТЦ','ТРК','ТК','МТРЦ']+not_city\nshops['city_name'] = shops['shop_name'].apply(create_city_name)\nshops['shop_type'] = shops['shop_name'].apply(create_shop_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.drop(columns='shop_name',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Aggregating data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We need our training data to be very similar to the test one. In the test data there are many items that were not sold, as we need to predict number of sales for a catalog. To achieve the similarity of train and test data we will basically, create a product data frame which consists of each pair of shop and item for a unique month, by this we will achieve the same target distribution as in test set (the idea is retrieved from this [notebook](https://www.kaggle.com/dlarionov/feature-engineering-xgboost)).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = train_sales.groupby(['date_block_num','shop_id','item_id'])['item_cnt_day'].sum().mean()\nprint('Mean of target value in train data : {0}'.format(mean))\nif np.abs(mean-y_hat_mean)<0.2:\n    print('The mean of train and test targets is aligned!')\nelse:\n    print('The mean of train and test targets is not aligned!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import product\nmatrix = []\ncols = ['shop_id','item_id','date_block_num']\nfor i in range(34):\n    sales = train_sales[train_sales.date_block_num==i]\n    matrix.append(np.array(list(product(sales.shop_id.unique(), sales.item_id.unique(),[i])), dtype='int16'))\n\nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fn(x):\n    return list(x)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales = train_sales.groupby(['shop_id','item_id','date_block_num'],as_index=False).agg({'item_cnt_day': np.sum,'item_price' : np.mean,\n                                               'month' : fn})\ntrain_sales = matrix.merge(train_sales,on=['shop_id','item_id','date_block_num'],how='left')\ntrain_sales['item_cnt_month'] = train_sales['item_cnt_day'].fillna(0).clip(0,20)\ntrain_sales.drop(columns='item_cnt_day',inplace=True)\nprint('Mean of target value in train_sales column : {0}'.format(train_sales['item_cnt_month'].mean()))\nif np.abs(train_sales['item_cnt_month'].mean()-y_hat_mean)<2:\n    print('The mean of train and test targets is aligned!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also concat everything with test data in order to use lagged features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sales['date_block_num'] = 34\ntest_sales.drop(columns='ID',inplace=True)\ndata = pd.concat([train_sales,test_sales],ignore_index=True, sort=False, keys=['shop_id','item_id','date_block_num'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_mapping = data[['month','date_block_num']].dropna().drop_duplicates().sort_values(by=['date_block_num'])\\\n.set_index('date_block_num').to_dict()['month']\nmonth_mapping.update({34:'11'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.sort_values(by=['date_block_num','shop_id','item_id'])\ndata['item_price'] = data['item_price'].fillna(0)\ndata['month'] = data['date_block_num'].map(month_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also clean unuseful data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_sales, train_sales, matrix\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## 5. Additional features based on monthes","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will add a feature based on monthes, that indicates if the month contains holiday or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"holiday_monthes = ['01','02','03','05','06','11']\ndata['is_holiday']=data['month'].apply(lambda x: 1 if x in holiday_monthes else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Features based on time-series, merging everything togather","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before all of this, we should delete outliers in item_price column. For this step we will use a technique called winsorization.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['item_price'] = data['item_price'].fillna(0)\nlower, upper = np.percentile(data['item_price'].values,[1,99])\ndata['item_price'] = data['item_price'].clip(lower,upper)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(np.log1p(data['item_price'].values));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['revenue'] = data['item_price']*data['item_cnt_month']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that we basically dealed with outliers using winsorization.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature(df, lags, col):\n    tmp = df[['shop_id','item_id','date_block_num',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['shop_id','item_id','date_block_num',col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def aggregated_previous(data,column,target_col,lags=[1],type_='mean'):\n    for i in lags:\n        tmp_data = data.copy()\n        tmp_data.loc[:,'date_block_num'] +=1\n        if isinstance(column,list):\n            to_group = ['date_block_num']+column\n            name = '_'.join(i for i in column)\n        else:\n            to_group = ['date_block_num']+[column]\n            name = column\n        tmp_data = tmp_data.groupby(to_group).agg({target_col:type_})\n        tmp_data.rename(columns={target_col:target_col+'_previous_{0}_by_'.format(type_)+name+'_lag_'+str(i)},inplace=True)\n        data = data.merge(tmp_data,how='left',right_index=True,left_on=to_group)\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add lag features based on price and item_cnt_month (it can take some time).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = lag_feature(data, [1,3,6,12], 'item_cnt_month')\ndata = lag_feature(data, [1,3,6,12], 'item_price')\ndata = aggregated_previous(data,'shop_id','item_cnt_month',[1,3])\ndata = aggregated_previous(data,'item_id','item_cnt_month',[1,3])\ndata = aggregated_previous(data,'shop_id','revenue',[1])\ndata = aggregated_previous(data,'item_id','revenue',[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns='revenue',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now merge our data with other dataframes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.merge(shops,on='shop_id',how='left')\ndata = data.merge(items_categories_merged,on='item_id',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data = aggregated_previous(data,['shop_id','item_category_id'],'item_cnt_month',[1])\n# data = aggregated_previous(data,['shop_id','item_category_id'],'item_cnt_month',[1],'sum')\ndata.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del items, shops, items_categories_merged\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns='item_price',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 4: Feature processing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For now we gonna only use the xgboost as our main model (that was also an idea to use lstm or to make ensemble, but that ideas will be exploited with time), thus we only need to factorize our categorical columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"to_encode = ['month','city_name','shop_type','type_of_category']\nnunique_cat = {}\nfor i in to_encode:\n    data[i] = data[i].factorize()[0]\n    nunique_cat.update({i:data[i].nunique()})\nnunique_cat.update({'shop_id':data['shop_id'].nunique()})\nnunique_cat.update({'item_id':data['item_id'].nunique()})\nnunique_cat.update({'item_category_id':data['item_category_id'].nunique()})\nprint('Factorized all the columns!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 5: Machine learning part","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Casting data to related dtypes and basic preparation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will firstly reduce the memory usage by casting columns to appropriate dtypes and split the data by month.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cast_categorical(data):\n    data['is_holiday'] = data['is_holiday'].astype('uint8')\n    data['shop_id'] = data['shop_id'].astype('uint8')\n    data['month'] = data['month'].astype('uint8')\n    data['shop_type'] = data['shop_type'].astype('uint8')\n    data['city_name'] = data['city_name'].astype('uint8')\n    data['item_category_id'] = data['item_category_id'].astype('uint8')\n    data['date_block_num'] = data['date_block_num'].astype('uint8')\n    data['item_id'] = data['item_id'].astype('uint16')\n    data['type_of_category'] = data['type_of_category'].astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cast_numerical(data):\n    for i in data.columns:\n        if 'float' in str(data[i].dtype):\n            data[i] = data[i].astype('float16')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cast_categorical(data)\ncast_numerical(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.isfinite(data).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = data[data.date_block_num<34],data[data.date_block_num==34]\ndel data\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"partA = train[train.date_block_num<32]\npartB = train[train.date_block_num == 32]\npartC = train[train.date_block_num == 33]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"part_A_x = partA.drop(columns=['item_cnt_month','date_block_num'])\npart_A_y = partA['item_cnt_month']\npart_B_x = partB.drop(columns=['item_cnt_month','date_block_num'])\npart_B_y = partB['item_cnt_month']\npart_C_x = partC.drop(columns=['item_cnt_month','date_block_num'])\npart_C_y = partC['item_cnt_month']\ntest = test.drop(columns=['item_cnt_month','date_block_num'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train, partA,partB, partC\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# First level models : LGBM, NN, Lasso, Ridge.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## LGBM training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"to_rename = {'версия':'version','регион':'region','русская':'rus','цифровая':'numeric','фигурка':'figure',\n            'фирм':'firm','коллекция':'collection'}\npart_A_x.rename(columns=to_rename,inplace=True)\npart_B_x.rename(columns=to_rename,inplace=True)\npart_C_x.rename(columns=to_rename,inplace=True)\ntest.rename(columns=to_rename,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_set = [(part_A_x,part_A_y),(part_B_x,part_B_y),(part_C_x,part_C_y)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom lightgbm import plot_importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model = lgb.LGBMRegressor(feature_fraction= 0.75,\n               metric = 'rmse',\n               max_depth = 8, \n               min_data_in_leaf = 2**7, \n               bagging_fraction = 0.75, \n               learning_rate = 0.03, \n               objective = 'mse', \n               bagging_seed = 2**7, \n               num_leaves = 100,\n               bagging_freq =1,\n               verbose = 1,\n            random_state=5,\n                             n_estimators=300)\nlgb_model.fit(part_A_x,part_A_y,eval_metric=\"rmse\", \n    eval_set=eval_set, \n    verbose=True, \n    early_stopping_rounds = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_importance(lgb_model,ax=plt.subplots(1,1,figsize=(15,12))[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_B = lgb_model.predict(part_B_x)\nlgb_C = lgb_model.predict(part_C_x)\nlgb_test = lgb_model.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model._Booster.__del__()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NN training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Function make_arch - makes neural network architecture. In order to work correctly with categorical columns - embedding layer is used. Also a spatial dropout along with dropout is used to reduce overfitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_arch(numerical_cols,categorical_cols):\n    tf.keras.backend.clear_session()\n    tf.random.set_seed(5)\n    inputs = []\n    embeddings = []\n    for cat_col in categorical_cols:\n        if not cat_col=='is_holiday':\n            no_of_unique_cat = nunique_cat[cat_col]\n            embedding_size = int(min(np.ceil((no_of_unique_cat)/2), 50))\n            input = tf.keras.layers.Input(shape = (1,),name='input_for_{0}'.format(cat_col))\n            embs = tf.keras.layers.Embedding(no_of_unique_cat+1, embedding_size, name = 'embeddings_for_{0}'.format(cat_col))(input)\n            drop = tf.keras.layers.SpatialDropout1D(0.4)(embs)\n            reshape = tf.keras.layers.Reshape(target_shape = (embedding_size,),name='reshape_for_{0}'.format(cat_col))(drop)\n            embeddings.append(reshape)\n            inputs.append(input)\n        else:\n            input = tf.keras.layers.Input(shape = (1,),name='input_for_{0}'.format(cat_col))\n            embs = tf.keras.layers.Dense(4,activation='relu')(input)\n            embeddings.append(reshape)\n            inputs.append(input)\n    numeric_input = tf.keras.layers.Input(shape=(len(numerical_cols),), name='input_for_numerical')\n    numeric_embs = tf.keras.layers.Dense(32)(numeric_input)\n    leaky_relu = tf.keras.layers.LeakyReLU(0.1)(numeric_embs)\n    drop_concat = tf.keras.layers.Dropout(0.2)(leaky_relu)\n    inputs.append(numeric_input)\n    embeddings.append(drop_concat)\n    concat = tf.keras.layers.Concatenate()(embeddings)\n    concat_dense = tf.keras.layers.Dense(8)(concat)\n    leaky_relu2 = tf.keras.layers.LeakyReLU(0.1)(concat_dense)\n    last_dense = tf.keras.layers.Dense(1,activation='relu')(leaky_relu2)\n    model = tf.keras.Model(outputs=last_dense,inputs=inputs)\n    return model\n\ndef root_mean_squared_error(y_true, y_pred):\n        return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true))) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns are divided into numerical and categorical. Numerical columns are scaled.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\ncategorical_cols = [i for i in part_A_x.columns if 'int' in str(part_A_x[i].dtype)]\nnumerical_cols = [i for i in part_A_x.columns if 'float' in str(part_A_x[i].dtype)]\n\ncategorical_input_A = [part_A_x[i].values for i in categorical_cols]\nscaler = StandardScaler()\nnumerical_input_A = scaler.fit_transform(part_A_x[numerical_cols].astype('float32'))\ncategorical_input_A.append(numerical_input_A)\n\ncategorical_input_B = [part_B_x[i].values for i in categorical_cols]\nnumerical_input_B = scaler.transform(part_B_x[numerical_cols].astype('float32'))\ncategorical_input_B.append(numerical_input_B)\n\ncategorical_input_C = [part_C_x[i].values for i in categorical_cols]\nnumerical_input_C = scaler.transform(part_C_x[numerical_cols].astype('float32'))\ncategorical_input_C.append(numerical_input_C)\n\ncategorical_input_test = [test[i].values for i in categorical_cols]\nnumerical_input_test = scaler.transform(test[numerical_cols].astype('float32'))\ncategorical_input_test.append(numerical_input_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = make_arch(numerical_cols,categorical_cols)\n\nmodel.compile(loss=root_mean_squared_error,optimizer=tf.keras.optimizers.SGD(momentum=0.1,lr=0.009))\n\nhistory = model.fit(x=categorical_input_A,y=part_A_y.values,validation_data = [categorical_input_B,part_B_y.values],\n     batch_size=512,epochs=4,callbacks=[tf.keras.callbacks.EarlyStopping(patience=1)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now plot results of the network.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, val_loss = history.history['loss'],history.history['val_loss']\nplt.figure(figsize=(13,8))\nplt.title('NN training loss versus validation')\nplt.plot(range(len(loss)),loss,'b')\nplt.plot(range(len(val_loss)),val_loss,'r')\nplt.xticks(range(len(val_loss)));\nplt.yticks(np.arange(min(val_loss),max(loss),0.01));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(categorical_input_C,part_C_y.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating nn predictions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_B = model.predict(categorical_input_B)\nnn_C = model.predict(categorical_input_C)\nnn_test = model.predict(categorical_input_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lasso Regression training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.metrics import r2_score, mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso(random_state=SEED,alpha=0.04)\nlasso.fit(numerical_input_A,part_A_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_B = r2_score(y_true=part_B_y,y_pred=lasso.predict(numerical_input_B))\nmse_B = np.sqrt(mean_squared_error(y_true=part_B_y,y_pred=lasso.predict(numerical_input_B)))\nprint('RMSE on B part: {0}'.format(mse_B))\nprint('r2_score on B part: {0}'.format(r2_B))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_C = r2_score(y_true=part_C_y,y_pred=lasso.predict(numerical_input_C))\nmse_C = np.sqrt(mean_squared_error(y_true=part_C_y,y_pred=lasso.predict(numerical_input_C)))\nprint('RMSE on C part: {0}'.format(mse_C))\nprint('r2_score on C part: {0}'.format(r2_C))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_B = lasso.predict(numerical_input_B)\nlasso_C = lasso.predict(numerical_input_C)\nlasso_test = lasso.predict(numerical_input_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ridge regression training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge(random_state=SEED,alpha=0.04)\nridge.fit(numerical_input_A,part_A_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_B = r2_score(y_true=part_B_y,y_pred=ridge.predict(numerical_input_B))\nmse_B = np.sqrt(mean_squared_error(y_true=part_B_y,y_pred=ridge.predict(numerical_input_B)))\nprint('RMSE on B part: {0}'.format(mse_B))\nprint('r2_score on B part: {0}'.format(r2_B))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_C = r2_score(y_true=part_C_y,y_pred=ridge.predict(numerical_input_C))\nmse_C = np.sqrt(mean_squared_error(y_true=part_C_y,y_pred=ridge.predict(numerical_input_C)))\nprint('RMSE on C part: {0}'.format(mse_C))\nprint('r2_score on C part: {0}'.format(r2_C))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_B = ridge.predict(numerical_input_B)\nridge_C = ridge.predict(numerical_input_C)\nridge_test = ridge.predict(numerical_input_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2nd lvl model training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We now will gather all the predictions and train second lvl model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"part_B_2 = pd.DataFrame(index=range(len(nn_B)))\npart_B_2['lasso'] = lasso_B\npart_B_2['ridge'] = ridge_B\npart_B_2['lgb'] = lgb_B\npart_B_2['nn'] = nn_B\ncols = part_B_2.columns\nfor i in cols:\n    for j in cols:\n        if i!=j:\n            part_B_2['{0}_{1}_distance'.format(i,j)] = part_B_2[i]-part_B_2[j]\npart_B_2['target'] = part_B_y.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.heatmap(part_B_2.corr(), \n        xticklabels=part_B_2.corr().columns,\n        yticklabels=part_B_2.corr().columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"part_C_2 = pd.DataFrame(index=range(len(nn_C)))\npart_C_2['lasso'] = lasso_C\npart_C_2['ridge'] = ridge_C\npart_C_2['lgb'] = lgb_C\npart_C_2['nn'] = nn_C\ncols = part_C_2.columns\nfor i in cols:\n    for j in cols:\n        if i!=j:\n            part_C_2['{0}_{1}_distance'.format(i,j)] = part_C_2[i]-part_C_2[j]\npart_C_2['target'] = part_C_y.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.heatmap(part_C_2.corr(), \n        xticklabels=part_C_2.corr().columns,\n        yticklabels=part_C_2.corr().columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_2 = pd.DataFrame(index=range(len(nn_test)))\ntest_2['lasso'] =lasso_test\ntest_2['ridge'] =ridge_test\ntest_2['lgb'] = lgb_test\ntest_2['nn'] = nn_test\ncols = test_2.columns\nfor i in cols:\n    for j in cols:\n        if i!=j:\n            test_2['{0}_{1}_distance'.format(i,j)] = test_2[i]-test_2[j]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_2.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = part_B_2.columns.tolist()\ntarget = features.pop(features.index('target'))\nX_B , Y_B = part_B_2[features], part_B_2[target]\nX_C , Y_C = part_C_2[features], part_C_2[target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = SGDRegressor(alpha=0.001,random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_B,Y_B)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_B = np.sqrt(r2_score(y_true=Y_B,y_pred=lr.predict(X_B)))\nmse_B = np.sqrt(mean_squared_error(y_true=Y_B,y_pred=lr.predict(X_B)))\nprint('RMSE on B part: {0}'.format(mse_B))\nprint('r2_score on B part: {0}'.format(r2_B))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_C = np.sqrt(r2_score(y_true=Y_C,y_pred=lr.predict(X_C)))\nmse_C = np.sqrt(mean_squared_error(y_true=Y_C,y_pred=lr.predict(X_C)))\nprint('RMSE on C part: {0}'.format(mse_C))\nprint('r2_score on C part: {0}'.format(r2_C))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_preds = np.clip(lr.predict(test_2),0,20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\nto_merge = test\nY_test = final_preds\nto_merge['item_cnt_month'] = Y_test\nsub_to_merge = to_merge[['shop_id','item_id','item_cnt_month']].copy()\nsubmission = submission.merge(sub_to_merge,how='left',on=['shop_id','item_id'])\nsubmission = submission[['ID','item_cnt_month']]\nsubmission.to_csv('submission_stacking.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}