{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom itertools import product\nimport keras\nfrom keras import Model, Sequential,regularizers\nfrom keras.layers import Dense, Flatten, Embedding, LeakyReLU, Input, Dropout\nfrom keras.metrics import RootMeanSquaredError\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom keras.models import load_model\nfrom keras import backend as K\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder, MinMaxScaler\nfrom IPython.display import clear_output\n\ndef downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float` type to `float16`\n                `int`   type to `int16`\n    '''    \n    # Select columns to downcast\n    float_cols = [c for c in df if (df[c].dtype == \"float64\") | (df[c].dtype == \"float32\")]\n    int_cols =   [c for c in df if (df[c].dtype == \"int64\") | (df[c].dtype == \"int32\")]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float16)\n    df[int_cols]   = df[int_cols].astype(np.int16)\n    return df\n\n# Function by Vadim Sokolov\n# https://www.kaggle.com/vadimsokolov/sales-predictions-final-project-in-course-hse/data#Prepare-and-feature-engineering-data\ndef lag_feature(data, lags, column):\n    temp = data[['date_block_num', 'shop_id', 'item_id', column]]\n    for lag in lags:\n        shifted = temp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', column + '_lag_' + str(lag)]\n        shifted['date_block_num'] += lag\n        data = pd.merge(data, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n        data[column+'_lag_'+str(lag)] = data[column+'_lag_'+str(lag)].astype('float32')\n    return data\n\n#Split a list into parts of roughly equal length\ndef split(a, n):\n    k, m = divmod(len(a), n)\n    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n\ndef assemble_df_for_nn(df_for_nn):\n    #Normalize numerical features\n    scaler = StandardScaler()\n    #scaler = MinMaxScaler(feature_range=(-10,10))\n    cols_to_scale = df_for_nn.select_dtypes(include=[np.number]).columns.to_list()\n    not_to_scale = ['item_cnt_month','shop_id','item_id', 'date_block_num','item_category_id']\n    cols_to_scale = [c for c in cols_to_scale if c not in not_to_scale]\n    split_col_list = list(split(cols_to_scale, 25))\n    # Scale columns in batches to avoid out-of-memory exception\n    for cols in cols_to_scale:\n        df_for_nn[cols] = scaler.fit_transform(df_for_nn[cols].values.reshape(-1, 1))\n\n    print('downcasting dtypes:')\n    df_for_nn = downcast_dtypes(df_for_nn)\n    print('Separate prediction frame:')\n    df_for_prediction = df_for_nn.loc[df_for_nn.date_block_num==34].reset_index().drop('index',axis=1)\n    df_for_prediction['ID'] = test['ID']\n    return df_for_nn, df_for_prediction\n\ndef assemble_df_for_nn_tanh(df_for_nn):\n    #Normalize numerical features\n    cols_to_scale = df_for_nn.select_dtypes(include=[np.number]).columns.to_list()\n    not_to_scale = ['item_cnt_month','item_cnt_month_lag_1','item_cnt_month_lag_2','item_cnt_month_lag_3',\n                    'shop_id','item_id', 'date_block_num','item_category_id',\n                    'item_new_in_store','item_new_in_store_lag_1','item_new_in_store_lag_2','item_new_in_store_lag_3'\n                    'opening_month','opening_month_lag_1','opening_month_lag_2','opening_month_lag_3']\n    cols_to_scale = [c for c in cols_to_scale if c not in not_to_scale]\n    print('Scaling columns')\n    for column_name in cols_to_scale:\n        # using tanh estimators for scaling\n        column = df_for_nn[column_name].astype(float)\n        std = column.std()\n        mean = column.mean()\n        tanh_estimators = 0.5* ( np.tanh( 0.01*((column-mean)/(std+0.0001)) ) + 1)\n        df_for_nn[column_name] = tanh_estimators\n\n    #print('downcasting dtypes')\n    #df_for_nn = downcast_dtypes(df_for_nn)\n    print('Separate prediction frame')\n    df_for_prediction = df_for_nn.loc[df_for_nn.date_block_num==34].reset_index().drop('index',axis=1)\n    df_for_prediction['ID'] = test['ID']\n    return df_for_nn, df_for_prediction\n\ndef assemble_df_tanh_standard(df_for_nn):\n    #Normalize numerical features\n    scaler = StandardScaler()\n    cols_to_scale = df_for_nn.select_dtypes(include=[np.number]).columns.to_list()\n    not_to_scale = ['item_cnt_month','item_cnt_month_lag_1','item_cnt_month_lag_2','item_cnt_month_lag_3',\n                    'shop_id','item_id', 'date_block_num','item_category_id',\n                    'item_new_in_store','item_new_in_store_lag_1','item_new_in_store_lag_2','item_new_in_store_lag_3'\n                    'opening_month','opening_month_lag_1','opening_month_lag_2','opening_month_lag_3']\n    cols_to_scale = [c for c in cols_to_scale if c not in not_to_scale]\n    print('Scaling columns')\n    for column_name in cols_to_scale:\n        # using tanh estimators for scaling\n        column = df_for_nn[column_name].astype(float)\n        std = column.std()\n        mean = column.mean()\n        tanh_estimators = 0.5* ( np.tanh( 0.01*((column-mean)/(std+0.0001)) ) + 1)\n        #df_for_nn[column_name] = tanh_estimators\n        df_for_nn[column_name] = scaler.fit_transform(tanh_estimators.values.reshape(-1, 1))\n\n    print('downcasting dtypes')\n    df_for_nn = downcast_dtypes(df_for_nn)\n    print('Separate prediction frame')\n    df_for_prediction = df_for_nn.loc[df_for_nn.date_block_num==34].reset_index().drop('index',axis=1)\n    df_for_prediction['ID'] = test['ID']\n    return df_for_nn, df_for_prediction\n\ndef root_mean_squared_error(y_true, y_pred):\n    from keras import backend as K\n    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\ndata_path = '/kaggle/input/competitive-data-science-predict-future-sales'\npath = '/kaggle/input/combined-version-eda-fe-model-training'\nitems = pd.read_csv('%s/items.csv' % data_path)\nitem_categories  = pd.read_csv('%s/item_categories.csv' % data_path)\nshops = pd.read_csv('%s/shops.csv' % data_path)\ntransactions  = pd.read_csv('%s/sales_train.csv' % data_path)\ntest  = pd.read_csv('%s/test.csv' % data_path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-01T22:25:07.598695Z","iopub.execute_input":"2021-09-01T22:25:07.599104Z","iopub.status.idle":"2021-09-01T22:25:15.308637Z","shell.execute_reply.started":"2021-09-01T22:25:07.599013Z","shell.execute_reply":"2021-09-01T22:25:15.307687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"### Plotting data as graphs","metadata":{}},{"cell_type":"code","source":"# Monthly sales of shops over time\nfig2 = plt.figure()\nax2 = fig2.add_subplot(111)\n\nmonthly_shop_sales = transactions.groupby(['date_block_num','shop_id']).sum()\nfor _,df in monthly_shop_sales.reset_index().groupby('shop_id'):\n    ax2.plot(df['date_block_num'], df['item_cnt_day'])","metadata":{"execution":{"iopub.status.busy":"2021-09-01T19:08:50.901159Z","iopub.execute_input":"2021-09-01T19:08:50.901575Z","iopub.status.idle":"2021-09-01T19:08:51.502765Z","shell.execute_reply.started":"2021-09-01T19:08:50.901529Z","shell.execute_reply":"2021-09-01T19:08:51.501751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Item price over time\nfig3 = plt.figure()\nax3 = fig3.add_subplot(111)\ntransactions_with_items = pd.merge(transactions,items, on='item_id')\nmean_price_of_itemcat_over_time = transactions_with_items.groupby(['date_block_num','item_category_id'])['item_price'].mean()\nfor _,df in mean_price_of_itemcat_over_time.reset_index().groupby('item_category_id'):\n    ax3.plot(df['date_block_num'], df['item_price'].clip(0,30000))","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:28.525168Z","iopub.execute_input":"2021-09-01T18:12:28.525625Z","iopub.status.idle":"2021-09-01T18:12:29.801801Z","shell.execute_reply.started":"2021-09-01T18:12:28.525579Z","shell.execute_reply":"2021-09-01T18:12:29.800852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_price_of_itemcat_over_time.reset_index().groupby(['item_category_id'])['item_price'].std().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:29.803868Z","iopub.execute_input":"2021-09-01T18:12:29.80418Z","iopub.status.idle":"2021-09-01T18:12:29.823869Z","shell.execute_reply.started":"2021-09-01T18:12:29.804149Z","shell.execute_reply":"2021-09-01T18:12:29.82277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_price_of_itemcat16_over_time = transactions_with_items.loc[transactions_with_items.item_category_id == 16].groupby(['date_block_num','item_category_id'])['item_price'].mean()\nfig3 = plt.figure()\nax3 = fig3.add_subplot(111)\nfor _,df in mean_price_of_itemcat16_over_time.reset_index().groupby('item_category_id'):\n    ax3.plot(df['date_block_num'], df['item_price'].clip(0,30000))","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:29.827614Z","iopub.execute_input":"2021-09-01T18:12:29.827939Z","iopub.status.idle":"2021-09-01T18:12:30.183592Z","shell.execute_reply.started":"2021-09-01T18:12:29.827907Z","shell.execute_reply":"2021-09-01T18:12:30.181675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Take-aways\n- Significant sales boost in december (as expected)\n- Shops and items are not continuous over all 33 months (gaps in graphs)\n- Item cat 16 either got mislabelled prices in the first two months, or the item price changed drastically (e.g. by adding some very expensive items to that category)","metadata":{}},{"cell_type":"code","source":"del transactions_with_items","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:30.190789Z","iopub.execute_input":"2021-09-01T18:12:30.191233Z","iopub.status.idle":"2021-09-01T18:12:30.211014Z","shell.execute_reply.started":"2021-09-01T18:12:30.191186Z","shell.execute_reply":"2021-09-01T18:12:30.209819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Histograms of various columns","metadata":{}},{"cell_type":"code","source":"# All\ntransactions.hist()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:30.212746Z","iopub.execute_input":"2021-09-01T18:12:30.213398Z","iopub.status.idle":"2021-09-01T18:12:31.229738Z","shell.execute_reply.started":"2021-09-01T18:12:30.213352Z","shell.execute_reply":"2021-09-01T18:12:31.228552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sales\ntransactions.item_cnt_day.hist(bins=20)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:31.231519Z","iopub.execute_input":"2021-09-01T18:12:31.231939Z","iopub.status.idle":"2021-09-01T18:12:31.498165Z","shell.execute_reply.started":"2021-09-01T18:12:31.231896Z","shell.execute_reply":"2021-09-01T18:12:31.496921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transactions.item_cnt_day.clip(0,10).hist(bins=10)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:31.499949Z","iopub.execute_input":"2021-09-01T18:12:31.500404Z","iopub.status.idle":"2021-09-01T18:12:31.775963Z","shell.execute_reply.started":"2021-09-01T18:12:31.500353Z","shell.execute_reply":"2021-09-01T18:12:31.775028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Price\ntransactions.item_price.hist(bins=30)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:31.777463Z","iopub.execute_input":"2021-09-01T18:12:31.777866Z","iopub.status.idle":"2021-09-01T18:12:32.085983Z","shell.execute_reply.started":"2021-09-01T18:12:31.777823Z","shell.execute_reply":"2021-09-01T18:12:32.085073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transactions.item_price.clip(0,7000).hist(bins=30)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:32.087377Z","iopub.execute_input":"2021-09-01T18:12:32.087834Z","iopub.status.idle":"2021-09-01T18:12:32.527332Z","shell.execute_reply.started":"2021-09-01T18:12:32.08779Z","shell.execute_reply":"2021-09-01T18:12:32.526327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total sales of shops per month\nmonthly_shop_sales.item_cnt_day.hist(bins = 60)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:32.528903Z","iopub.execute_input":"2021-09-01T18:12:32.529513Z","iopub.status.idle":"2021-09-01T18:12:32.948799Z","shell.execute_reply.started":"2021-09-01T18:12:32.529471Z","shell.execute_reply":"2021-09-01T18:12:32.947883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total sales of shops\ntransactions.groupby(['shop_id']).sum().item_cnt_day.hist(bins = 60)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:32.950245Z","iopub.execute_input":"2021-09-01T18:12:32.950664Z","iopub.status.idle":"2021-09-01T18:12:33.356197Z","shell.execute_reply.started":"2021-09-01T18:12:32.950619Z","shell.execute_reply":"2021-09-01T18:12:33.355094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Take-Away\n- Item_cnt_month and item_price has some extreme outliers that need to be clipped","metadata":{}},{"cell_type":"code","source":"# Note: apparently this one item with very high sales is a shopping bag\n# see: https://www.google.com/search?q=%D0%A4%D0%B8%D1%80%D0%BC%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9+%D0%BF%D0%B0%D0%BA%D0%B5%D1%82+%D0%BC%D0%B0%D0%B9%D0%BA%D0%B0+1%D0%A1+%D0%98%D0%BD%D1%82%D0%B5%D1%80%D0%B5%D1%81+%D0%B1%D0%B5%D0%BB%D1%8B%D0%B9&client=firefox-b-d&source=lnms&tbm=isch&sa=X&ved=2ahUKEwitvdeVhd7yAhV5RvEDHaDSAqUQ_AUoAXoECAEQAw&biw=1920&bih=927\nprint(transactions.loc[transactions['item_id']==20949].item_cnt_day.sum())\nprint(items.loc[items['item_id']==20949].item_name)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:33.358424Z","iopub.execute_input":"2021-09-01T18:12:33.358836Z","iopub.status.idle":"2021-09-01T18:12:33.378843Z","shell.execute_reply.started":"2021-09-01T18:12:33.358795Z","shell.execute_reply":"2021-09-01T18:12:33.377736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Quick look at the test set ","metadata":{}},{"cell_type":"code","source":"print(transactions.item_id.value_counts().size)\nprint(transactions.shop_id.value_counts().size)\nprint('====================================')\nprint(test.item_id.nunique())\nprint(test.shop_id.nunique())\ntest.groupby(['shop_id','item_id']).nunique().max()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:33.380529Z","iopub.execute_input":"2021-09-01T18:12:33.380926Z","iopub.status.idle":"2021-09-01T18:12:33.539111Z","shell.execute_reply.started":"2021-09-01T18:12:33.380886Z","shell.execute_reply":"2021-09-01T18:12:33.538329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Take-Aways\n- Not all 60 shops and all 21807 items are present in the test set\n- Every combination of these 5100 items at these 42 shops will be asked for in the test set\n- Not all shops will sell every item -> high percentage of zeros (that are not present in the training data)","metadata":{}},{"cell_type":"code","source":"transactions.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:35.166261Z","iopub.execute_input":"2021-09-01T18:12:35.166689Z","iopub.status.idle":"2021-09-01T18:12:35.184717Z","shell.execute_reply.started":"2021-09-01T18:12:35.166654Z","shell.execute_reply":"2021-09-01T18:12:35.183394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing of items and shops as suggested by Gordon Henderson","metadata":{}},{"cell_type":"code","source":"# Code written by Gordon Henderson\n# https://www.kaggle.com/gordotron85/future-sales-xgboost-top-3#Cleaning-Shop-Data\n# Якутск Орджоникидзе, 56\ntransactions.loc[transactions.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# Якутск ТЦ \"Центральный\"\ntransactions.loc[transactions.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# Жуковский ул. Чкалова 39м²\ntransactions.loc[transactions.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11\n\nshops.loc[ shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"',\"shop_name\" ] = 'СергиевПосад ТЦ \"7Я\"'\nshops[\"city\"] = shops.shop_name.str.split(\" \").map( lambda x: x[0] )\nshops[\"category\"] = shops.shop_name.str.split(\" \").map( lambda x: x[1] )\nshops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n\ncategory = []\nfor cat in shops.category.unique():\n    if len(shops[shops.category == cat]) >= 5:\n        category.append(cat)\nshops.category = shops.category.apply( lambda x: x if (x in category) else \"other\" )\n\nshops[\"shop_category\"] = LabelEncoder().fit_transform( shops.category )\nshops[\"shop_city\"] = LabelEncoder().fit_transform( shops.city )\nshops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]\n\n#Cleaning item category data\nitem_categories[\"type_code\"] = item_categories.item_category_name.apply( lambda x: x.split(\" \")[0] ).astype(str)\nitem_categories.loc[ (item_categories.type_code == \"Игровые\")| (item_categories.type_code == \"Аксессуары\"), \"category\" ] = \"Игры\"\n\ncategory = []\nfor cat in item_categories.type_code.unique():\n    if len(item_categories[item_categories.type_code == cat]) >= 5: \n        category.append( cat )\nitem_categories.type_code = item_categories.type_code.apply(lambda x: x if (x in category) else \"etc\")\n\nitem_categories.type_code = LabelEncoder().fit_transform(item_categories.type_code)\nitem_categories[\"split\"] = item_categories.item_category_name.apply(lambda x: x.split(\"-\"))\nitem_categories[\"subtype\"] = item_categories.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitem_categories[\"subtype_code\"] = LabelEncoder().fit_transform( item_categories[\"subtype\"] )\nitem_categories = item_categories[[\"item_category_id\", \"subtype_code\", \"type_code\"]]\n\n# Cleaning item data\nimport re\ndef name_correction(x):\n    x = x.lower() # all letters lower case\n    x = x.partition('[')[0] # partition by square brackets\n    x = x.partition('(')[0] # partition by curly brackets\n    x = re.sub('[^A-Za-z0-9А-Яа-я]+', ' ', x) # remove special characters\n    x = x.replace('  ', ' ') # replace double spaces with single spaces\n    x = x.strip() # remove leading and trailing white space\n    return x\n\n# split item names by first bracket\nitems[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\nitems[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n\n# replace special characters and turn to lower case\nitems[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\nitems[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\n\n# fill nulls with '0'\nitems = items.fillna('0')\n\nitems[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n\n# return all characters except the last if name 2 is not \"0\" - the closing bracket\nitems.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")\n\n# Clean item type\nitems[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\nitems.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nitems.loc[ items.type == \"\", \"type\"] = \"mac\"\nitems.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\nitems.loc[ (items.type == 'pc' )| (items.type == 'pс') | (items.type == \"pc\"), \"type\" ] = \"pc\"\nitems.loc[ items.type == 'рs3' , \"type\"] = \"ps3\"\n\ngroup_sum = items.groupby([\"type\"]).agg({\"item_id\": \"count\"})\ngroup_sum = group_sum.reset_index()\ndrop_cols = []\nfor cat in group_sum.type.unique():\n    if group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0] <40:\n        drop_cols.append(cat)\nitems.name2 = items.name2.apply( lambda x: \"other\" if (x in drop_cols) else x )\nitems = items.drop([\"type\"], axis = 1)\n\nitems.name2 = LabelEncoder().fit_transform(items.name2)\nitems.name3 = LabelEncoder().fit_transform(items.name3)\n\nitems.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)\n\nitems = items.merge(item_categories,on='item_category_id')\n\nitems.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T21:39:17.934598Z","iopub.execute_input":"2021-09-01T21:39:17.93492Z","iopub.status.idle":"2021-09-01T21:39:18.662514Z","shell.execute_reply.started":"2021-09-01T21:39:17.934888Z","shell.execute_reply":"2021-09-01T21:39:18.661299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature extraction, encoding of categorical features, preprocessing and scaling","metadata":{}},{"cell_type":"code","source":"load_data_frame = True\nif not load_data_frame:\n    # include month 34 so it will have the aggregated statistics such as lag features\n    test_subset = test[['shop_id','item_id']]\n    test_subset['date_block_num']=34\n    transactions = transactions.append(test_subset,ignore_index=True)\n    del test_subset\n\n    # aggregate the monthly transaction statistic for each item in each shop\n    monthly_trans = transactions.groupby(['date_block_num','shop_id','item_id'])[[\"item_cnt_day\"]].sum()\n    monthly_trans = monthly_trans.reset_index()\n\n    # since it's summed for the entire month, item_cnt_day is now actually item_cnt_month\n    monthly_trans.rename(columns={'item_cnt_day':'item_cnt_month'}, inplace=True)\n\n    print(monthly_trans.shape)\n\n    # Fill missing item/shop transactions with item_cnt_month = 0\n    # Code adapted from user'c14103'\n    # https://www.kaggle.com/c/competitive-data-science-predict-future-sales/discussion/57123\n    # Instead of looking through ALL possible shop/item pairs I include only the ones present in the test set. \n    # This makes the model more vulnerable to changes in the test set and worse at generalizing, but this is necessary due to very limited RAM\n    extend_frame_with_0_sales = True\n    if extend_frame_with_0_sales:\n        grid = []    \n        for month in monthly_trans['date_block_num'].drop_duplicates():\n            shop = test['shop_id'].drop_duplicates()   \n            item = test['item_id'].drop_duplicates()\n            grid.append( np.asarray(   list( product( *[shop,item,[month]] ) )    )  )     \n        cols = ['shop_id','item_id','date_block_num']   \n        grid = pd.DataFrame(np.vstack(grid), columns = cols, dtype=np.int32)    \n        monthly_trans = pd.merge(grid,monthly_trans, on = cols, how = 'left').fillna(0)\n        del grid\n        del item\n        del shop\n\n    print('filled missing:')\n    print(monthly_trans.shape)\n\n    #include mean item price for a given shop/item pair per month\n    monthly_trans = monthly_trans.merge(transactions.groupby(['date_block_num','shop_id','item_id'])[[\"item_price\"]].mean().reset_index(),how='left',on=['date_block_num','shop_id','item_id']).fillna(0)\n    print('item_price:')\n    print(monthly_trans.shape)\n    print(monthly_trans.item_price.max())\n\n\n    # clip extreme outliers in the prices\n    q_hi  = monthly_trans['item_price'].quantile(0.95)\n    monthly_trans['item_price'] = monthly_trans['item_price'].clip(0, q_hi)\n\n    # clip item_sales close to the range used in the competition\n    monthly_trans['item_cnt_month'] = monthly_trans['item_cnt_month'].clip(0, 20)\n\n    # Extend the unknown item prices for month 0 with the first known price\n    first_price = transactions.sort_values('date_block_num', ascending=True).drop_duplicates(['shop_id','item_id'])[['shop_id','item_id','item_price','date_block_num']].fillna(0)\n    first_price['date_block_num']=0\n    unknown_prices_month_0 = monthly_trans.loc[((monthly_trans.date_block_num==0))][['shop_id','item_id','item_price']]\n    unknown_prices_month_0 = unknown_prices_month_0.merge(first_price,on=['shop_id','item_id'],how='outer',suffixes=['_DROP',''])\n    monthly_trans = monthly_trans.merge(unknown_prices_month_0[['shop_id','item_id','item_price','date_block_num']],on=['shop_id','item_id','date_block_num'],suffixes=['_DROP',''],how='left') \n    monthly_trans['item_price']=monthly_trans['item_price'].fillna(monthly_trans['item_price_DROP'])\n    monthly_trans = monthly_trans.drop(['item_price_DROP'],axis=1)\n\n    # Extend the item prices for months 0-34 with the last known item price at that shop\n    for i in monthly_trans.date_block_num.unique():\n        last_known_price = monthly_trans.loc[((monthly_trans.date_block_num<=(i))&(monthly_trans.item_price!=0))].sort_values('date_block_num', ascending=False).drop_duplicates(['shop_id','item_id'])[['shop_id','item_id','item_price','date_block_num']]\n        unknown_prices = monthly_trans.loc[((monthly_trans.date_block_num==(i))&(monthly_trans.item_price==0))][['shop_id','item_id','item_price']]\n        unknown_prices = unknown_prices.merge(last_known_price,on=['shop_id','item_id'],how='outer',suffixes=['_DROP',''])\n        unknown_prices = unknown_prices.drop('item_price_DROP',axis=1)\n        unknown_prices['date_block_num'] = (i)\n        monthly_trans = monthly_trans.merge(unknown_prices[['shop_id','item_id','item_price','date_block_num']],on=['shop_id','item_id','date_block_num'],suffixes=['_DROP',''],how='left')\n        monthly_trans['item_price']=monthly_trans['item_price'].fillna(monthly_trans['item_price_DROP'])\n        monthly_trans = monthly_trans.drop(['item_price_DROP'],axis=1)\n    del unknown_prices\n    del last_known_price\n    del unknown_prices_month_0\n\n    # calculate monthly revenue for item at shop, then keep only the lag values (revenue will be wrong for month 34)\n    monthly_trans[\"revenue\"] = monthly_trans[\"item_cnt_month\"] * monthly_trans[\"item_price\"]\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'revenue').fillna(0)\n    monthly_trans = monthly_trans.drop('revenue',axis=1)\n\n    print('extended_price:')\n    print(monthly_trans.shape)\n\n    # include more data about the items \n    monthly_trans = pd.merge(monthly_trans,items, on='item_id')\n    # include additional shop info such as city or \n    monthly_trans = monthly_trans.merge(shops,on='shop_id')\n\n    print('item data')\n    print(monthly_trans.shape)\n\n    # rolling average of item sales at shop    \n    item_shop_group = monthly_trans.groupby(['shop_id','item_id'])\n    rolling = item_shop_group[['date_block_num','item_cnt_month']].rolling(2, min_periods=1,on='date_block_num').mean()\n    rolling['rolling_average_2'] = rolling['item_cnt_month'].shift(1,fill_value=0)\n    monthly_trans = monthly_trans.merge(rolling.drop('item_cnt_month',axis=1),on=['shop_id','item_id','date_block_num'])\n\n    item_shop_group = monthly_trans.groupby(['shop_id','item_id'])\n    rolling = item_shop_group[['date_block_num','item_cnt_month']].rolling(3, min_periods=1,on='date_block_num').mean()\n    rolling['rolling_average_3'] = rolling['item_cnt_month'].shift(1,fill_value=0)\n    monthly_trans = monthly_trans.merge(rolling.drop('item_cnt_month',axis=1),on=['shop_id','item_id','date_block_num'])\n\n    item_shop_group = monthly_trans.groupby(['shop_id','item_id'])\n    rolling = item_shop_group[['date_block_num','item_cnt_month']].rolling(4, min_periods=1,on='date_block_num').mean()\n    rolling['rolling_average_4'] = rolling['item_cnt_month'].shift(1,fill_value=0)\n    monthly_trans = monthly_trans.merge(rolling.drop('item_cnt_month',axis=1),on=['shop_id','item_id','date_block_num'])\n\n    del rolling\n\n    print('rolling_average:')\n    print(monthly_trans.shape)\n\n\n    # Month 0-11\n    monthly_trans['month_id']=monthly_trans['date_block_num']%12\n    # Year 0-2\n    monthly_trans['year_id']=monthly_trans['date_block_num'].floordiv(12)\n    # How many days in that month\n    days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n    monthly_trans[\"days\"] = monthly_trans[\"month_id\"].map(days).astype(np.int8)\n\n    # Compare the monthly sales total (over all shops and items) to the yearly average\n    encodings_year = (monthly_trans.groupby('year_id')['item_cnt_month'].sum()/12).rename('yearly_total_sales_all_shops_all_items')\n    encodings = monthly_trans.groupby(['month_id','year_id'])['item_cnt_month'].sum().rename('monthly_total_sales_all_shops_all_items').replace(0, np.nan)\n\n    # Make a prediction of the total sales for November and December 2014 based on previous years trends\n    oct_to_nov_scale_year_0 = encodings.loc[10][0]/encodings.loc[9][0]\n    oct_to_nov_scale_year_1 = encodings.loc[10][1]/encodings.loc[9][1]\n    nov_year_2_estimate = encodings.loc[9][2]* 0.5*(oct_to_nov_scale_year_0+oct_to_nov_scale_year_1)\n    encodings.loc[10][2] = nov_year_2_estimate\n    dec_year_2_estimate = encodings.loc[10][2]* 0.5*(encodings.loc[11][0]/encodings.loc[10][0]+encodings.loc[11][1]/encodings.loc[10][1])\n\n    # Update the total sales for year 2 so that it includes the estimated values of November and December\n    encodings_year = (monthly_trans.groupby('year_id')['item_cnt_month'].sum()/12).rename('yearly_total_sales_all_shops_all_items')\n    encodings_year[2]+=((nov_year_2_estimate+dec_year_2_estimate)/12)\n\n    monthly_trans = monthly_trans.merge(encodings_year,on='year_id')\n    monthly_trans = monthly_trans.merge(encodings,on=['month_id','year_id']).sort_values(['date_block_num','shop_id','item_id'])\n\n    # By how much do sales of a given month differ compared to the yearly average\n    monthly_trans['dif_of_month_to_yearly_average'] = monthly_trans['monthly_total_sales_all_shops_all_items']-monthly_trans['yearly_total_sales_all_shops_all_items']\n    monthly_trans = monthly_trans.drop(['monthly_total_sales_all_shops_all_items','yearly_total_sales_all_shops_all_items'],axis=1)\n\n    del encodings_year\n    del encodings\n\n    print('Monthly difference to yearly average:')\n    print(monthly_trans.shape)\n\n    # If a shop sells items for the first time it probably just opened that month (this will be wrong for month 0, so I really should drop the data from month 0)\n    encodings = monthly_trans.loc[monthly_trans.item_cnt_month>0].groupby('shop_id')['date_block_num'].min()\n    monthly_trans['opening_month'] =  monthly_trans['shop_id'].map(encodings).fillna(0)\n    monthly_trans['opening_month'] = (monthly_trans['opening_month'] == monthly_trans['date_block_num']).astype(np.int8)\n\n    # If a shop sells an item for the first time it is assumed that the item is new in this store\n    encodings = monthly_trans.loc[monthly_trans.item_cnt_month>0].groupby(['shop_id','item_id'])['date_block_num'].min().rename('item_new_in_store')\n    monthly_trans =  monthly_trans.merge(encodings.reset_index(),on=['shop_id','item_id'],how='left').fillna(0)\n    monthly_trans['item_new_in_store'] = (monthly_trans['item_new_in_store'] == monthly_trans['date_block_num']).astype(np.int8)\n\n    # calculates the monthly mean sales for each item. \n    encodings = monthly_trans.groupby(['item_id','date_block_num'])['item_cnt_month'].mean()\n    encodings = encodings.reset_index()\n    encodings['monthly_mean_sales_of_item'] = encodings['item_cnt_month']\n    encodings.drop('item_cnt_month',axis=1, inplace=True)\n    monthly_trans = pd.merge(monthly_trans,encodings, on=['date_block_num','item_id'])\n\n    # calculates the monthly mean sales for each shop\n    encodings = monthly_trans.groupby(['shop_id','date_block_num'])['item_cnt_month'].mean()\n    encodings = encodings.reset_index()\n    encodings['monthly_mean_sales_at_shop'] = encodings['item_cnt_month']\n    encodings.drop('item_cnt_month',axis=1, inplace=True)\n    monthly_trans = pd.merge(monthly_trans,encodings, on=['date_block_num','shop_id'])\n\n    # calculates the monthly total sales for each item. \n    encodings = monthly_trans.groupby(['item_id','date_block_num'])['item_cnt_month'].sum()\n    encodings = encodings.reset_index()\n    encodings['monthly_total_sales_of_item'] = encodings['item_cnt_month']\n    encodings.drop('item_cnt_month',axis=1, inplace=True)\n    monthly_trans = pd.merge(monthly_trans,encodings, on=['date_block_num','item_id'])\n\n    # calculates the monthly total sales for each shop\n    encodings = monthly_trans.groupby(['shop_id','date_block_num'])['item_cnt_month'].sum()\n    encodings = encodings.reset_index()\n    encodings['monthly_total_sales_at_shop'] = encodings['item_cnt_month']\n    encodings.drop('item_cnt_month',axis=1, inplace=True)\n    monthly_trans = pd.merge(monthly_trans,encodings, on=['date_block_num','shop_id'])\n\n    del encodings\n\n\n    #Create lag features different sales attributes\n    #monthly_trans = lag_feature(monthly_trans, [1,2,3], 'opening_month').fillna(0)\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'item_new_in_store').fillna(0)\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'item_cnt_month').fillna(0)\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'item_price').fillna(0)\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'monthly_mean_sales_of_item').fillna(0)\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'monthly_mean_sales_at_shop').fillna(0)\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'monthly_total_sales_of_item').fillna(0)\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'monthly_total_sales_at_shop').fillna(0)\n\n    print('lag feature:')\n    print(monthly_trans.shape)\n\n    # shop/item sales trend\n    monthly_trans['sale_trend_1_month'] = monthly_trans['item_cnt_month_lag_1']-monthly_trans['item_cnt_month_lag_2']\n    monthly_trans['sale_trend_2_month'] = monthly_trans['item_cnt_month_lag_1']-monthly_trans['item_cnt_month_lag_3']\n\n    # shop price trend\n    monthly_trans['price_trend_1_month'] = monthly_trans['item_price']-monthly_trans['item_price_lag_1']\n\n    # Item sales trend\n    monthly_trans['item_trend_1_month'] = monthly_trans['monthly_total_sales_of_item_lag_1']-monthly_trans['monthly_total_sales_of_item_lag_2']\n    monthly_trans['item_trend_2_month'] = monthly_trans['monthly_total_sales_of_item_lag_1']-monthly_trans['monthly_total_sales_of_item_lag_3']\n\n    # Shop sales trend\n    monthly_trans['shop_trend_1_month'] = monthly_trans['monthly_total_sales_at_shop_lag_1']-monthly_trans['monthly_total_sales_at_shop_lag_2']\n    monthly_trans['shop_trend_2_month'] = monthly_trans['monthly_total_sales_at_shop_lag_1']-monthly_trans['monthly_total_sales_at_shop_lag_3']\n\n\n    # Lag trends\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'sale_trend_1_month').fillna(0)\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'sale_trend_2_month').fillna(0)\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'price_trend_1_month').fillna(0)\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'item_trend_1_month').fillna(0)\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'item_trend_2_month').fillna(0)\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'shop_trend_1_month').fillna(0)\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'shop_trend_2_month').fillna(0)\n\n    # Columns for current month have to be dropped, since all these values are unknown for month 34 (except for item_cnt_month, this is the y needed for training/evaluation)\n    # Lag features are not as important now since we have trends instead (but keep most item_cnt_month_lag features, since this is what we are trying to predict)\n    # As long as we are not out of memory I'll keep most of the lag features for now\n    cols_to_drop = [\n        #'item_price_lag_1','item_price_lag_2','item_price_lag_3'\n        'monthly_mean_sales_of_item','monthly_mean_sales_at_shop','monthly_total_sales_of_item','monthly_total_sales_at_shop',\n        'monthly_total_sales_of_item_lag_1','monthly_total_sales_of_item_lag_2','monthly_total_sales_of_item_lag_3',\n        'monthly_total_sales_at_shop_lag_1','monthly_total_sales_at_shop_lag_2','monthly_total_sales_at_shop_lag_3'\n    ]\n    monthly_trans = monthly_trans.drop(cols_to_drop,axis=1)\n\n    del transactions\n    del shops\n    del items\n\n    # Use frequency encoding for the item category id\n    monthly_trans['freq_encoded_item_id'] = monthly_trans.item_id.map(monthly_trans.groupby('item_id').size()/len(monthly_trans))\n    monthly_trans['freq_encoded_item_category_id'] = monthly_trans.item_category_id.map(monthly_trans.groupby('item_category_id').size()/len(monthly_trans))\n    monthly_trans['freq_encoded_shop_id'] = monthly_trans.shop_id.map(monthly_trans.groupby('shop_id').size()/len(monthly_trans))\n\n    print('frequency encodings done')\n\n    # Use mean encoding for several categories\n    mean_encodings = monthly_trans.groupby(['item_id'])['item_cnt_month'].mean().to_dict()\n    monthly_trans['mean_encoded_item_id'] =  monthly_trans['item_id'].map(mean_encodings)\n\n    mean_encodings = monthly_trans.groupby(['shop_id'])['item_cnt_month'].mean().to_dict()\n    monthly_trans['mean_encoded_shop_id'] =  monthly_trans['shop_id'].map(mean_encodings)\n\n    mean_encodings = monthly_trans.groupby(['date_block_num'])['item_cnt_month'].mean()\n    monthly_trans['mean_encoded_prev_month'] =  monthly_trans['date_block_num'].map(mean_encodings.shift().fillna(0).to_dict())\n\n    mean_encodings = monthly_trans.groupby(['item_category_id'])['item_cnt_month'].mean().to_dict()\n    monthly_trans['mean_encoded_item_category_id'] =  monthly_trans['item_category_id'].map(mean_encodings)\n\n    # Lag ONLY mean_encoded_prev_month here, that's the only one that depends on date_block_month\n    monthly_trans = lag_feature(monthly_trans, [1,2,3], 'mean_encoded_prev_month').fillna(0)\n\n    #Potentially irrelevant features\n    #######################################################################################\n    mean_encodings = monthly_trans.groupby(['name2'])['item_cnt_month'].mean().to_dict()\n    monthly_trans['mean_encoded_name2'] =  monthly_trans['name2'].map(mean_encodings)\n    #\n    mean_encodings = monthly_trans.groupby(['name3'])['item_cnt_month'].mean().to_dict()\n    monthly_trans['mean_encoded_name3'] =  monthly_trans['name3'].map(mean_encodings)\n    #\n    mean_encodings = monthly_trans.groupby(['type_code'])['item_cnt_month'].mean().to_dict()\n    monthly_trans['mean_encoded_type_code'] =  monthly_trans['type_code'].map(mean_encodings)\n    #\n    mean_encodings = monthly_trans.groupby(['subtype_code'])['item_cnt_month'].mean().to_dict()\n    monthly_trans['mean_encoded_subtype_code'] =  monthly_trans['subtype_code'].map(mean_encodings)\n    #\n    mean_encodings = monthly_trans.groupby(['shop_city'])['item_cnt_month'].mean().to_dict()\n    monthly_trans['mean_encoded_shop_city'] =  monthly_trans['shop_city'].map(mean_encodings)\n    #\n    mean_encodings = monthly_trans.groupby(['shop_category'])['item_cnt_month'].mean().to_dict()\n    monthly_trans['mean_encoded_shop_category'] =  monthly_trans['shop_category'].map(mean_encodings)\n    #\n    mean_encodings = monthly_trans.groupby(['item_category_id'])['item_cnt_month'].mean().to_dict()\n    monthly_trans['mean_encoded_item_category_id'] =  monthly_trans['item_category_id'].map(mean_encodings)\n    #######################################################################################\n    del mean_encodings\n\n    print('mean encodings done')\n\n    #Encodings for item_at_shop\n    #############################\n    # Important: These seem to have a negative impact on the validation. These values, especially mean and var, are very important features for the training prediction,\n    # but the regressor focusses way too much on them and doesn't learn more general features.\n    # Removing them improves the prediction result\n    #############################\n    #mean_encodings = monthly_trans.groupby(['item_id','shop_id'])['item_cnt_month'].mean().rename('mean_encoded_item_at_shop',inplace=True)\n    #monthly_trans =  monthly_trans.merge(mean_encodings,on=['item_id','shop_id'])\n    #mean_encodings = monthly_trans.groupby(['item_id','shop_id'])['item_cnt_month'].median().rename('median_encoded_item_at_shop',inplace=True)\n    #monthly_trans =  monthly_trans.merge(mean_encodings,on=['item_id','shop_id'])\n    #mean_encodings = monthly_trans.groupby(['item_id','shop_id'])['item_cnt_month'].var().rename('var_encoded_item_at_shop',inplace=True)\n    #monthly_trans =  monthly_trans.merge(mean_encodings,on=['item_id','shop_id'])\n    #mean_encodings = monthly_trans.groupby(['item_id','shop_id'])['item_cnt_month'].std().rename('std_encoded_item_at_shop',inplace=True)\n    #monthly_trans =  monthly_trans.merge(mean_encodings,on=['item_id','shop_id'])\n\n    # Don't lag these features, they are not month dependent\n    \n    #del mean_encodings\n    #print('encodings for item_at_shop done')\n\n    #Variance encoding for the same categories\n    variance_encoded_item_id = monthly_trans.groupby(['item_id'])['item_cnt_month'].var().to_dict()\n    monthly_trans['variance_encoded_item_id'] =  monthly_trans['item_id'].map(variance_encoded_item_id).fillna(0)\n    del variance_encoded_item_id\n\n    variance_encoded_shop_id = monthly_trans.groupby(['shop_id'])['item_cnt_month'].var().to_dict()\n    monthly_trans['variance_encoded_shop_id'] =  monthly_trans['shop_id'].map(variance_encoded_shop_id).fillna(0)\n    del variance_encoded_shop_id\n\n    var_encoded_city_cat = monthly_trans.groupby(['shop_city'])['item_cnt_month'].var().to_dict()\n    monthly_trans['variance_encoded_city'] =  monthly_trans['shop_city'].map(var_encoded_city_cat)\n    del var_encoded_city_cat\n\n    var_encoded_shopcat_cat = monthly_trans.groupby(['shop_category'])['item_cnt_month'].var().to_dict()\n    monthly_trans['variance_encoded_shop_category'] =  monthly_trans['shop_category'].map(var_encoded_shopcat_cat)\n    del var_encoded_shopcat_cat\n\n    #Drop the original (not encoded) categorical features\n    monthly_trans = monthly_trans.drop([\"shop_category\", \"shop_city\",\"item_category_id\", \"subtype_code\", \"type_code\",'name2','name3'],axis=1)\n\n    print('variance encodings done')\n    \n    # Remove the first three months from the dataframe (since lag features and trends will be wrong for them)\n    monthly_trans = monthly_trans.loc[monthly_trans.date_block_num>2]\n\n\n    #df_for_nn, df_for_predictions = assemble_df_for_nn_tanh(monthly_trans)\n    #df_for_nn, df_for_predictions = assemble_df_for_nn(monthly_trans)\n    df_for_nn, df_for_predictions = assemble_df_tanh_standard(monthly_trans)\n    df_for_nn.to_pickle('df_for_nn.pkl')\n    df_for_predictions.to_pickle('df_for_predictions.pkl')\n\n    print(monthly_trans.shape)\n    monthly_trans.head()\nelse:\n    df_for_nn = pd.read_pickle(path + \"/df_for_nn.pkl\")\n    df_for_predictions = pd.read_pickle(path + \"/df_for_predictions.pkl\")\n    #df_for_nn = pd.read_pickle(\"df_for_nn.pkl\")\n    #df_for_predictions = pd.read_pickle(\"df_for_predictions.pkl\")\n\n    #df_for_nn = df_for_nn.drop(['item_id','mean_encoded_item_id','variance_encoded_item_id'],axis=1)\n    #df_for_predictions = df_for_predictions.drop(['mean_encoded_item_id','variance_encoded_item_id'],axis=1)\n    print(list(set(df_for_predictions.columns)-set(df_for_nn.columns)))","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:00:52.069434Z","iopub.execute_input":"2021-09-01T22:00:52.069756Z","iopub.status.idle":"2021-09-01T22:00:59.975995Z","shell.execute_reply.started":"2021-09-01T22:00:52.069726Z","shell.execute_reply":"2021-09-01T22:00:59.974256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sanity checks - no nans, no +- inf\n#[monthly_trans[column].isna().max() for column in monthly_trans.columns]\n#[monthly_trans[column].max() for column in monthly_trans.select_dtypes(include=np.number).columns]\n#[monthly_trans[column].min() for column in monthly_trans.select_dtypes(include=np.number).columns]","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:56.056419Z","iopub.execute_input":"2021-09-01T18:12:56.056809Z","iopub.status.idle":"2021-09-01T18:12:56.063472Z","shell.execute_reply.started":"2021-09-01T18:12:56.056769Z","shell.execute_reply":"2021-09-01T18:12:56.062595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_for_nn.head().T","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:12:57.009511Z","iopub.execute_input":"2021-09-01T18:12:57.009903Z","iopub.status.idle":"2021-09-01T18:12:57.032528Z","shell.execute_reply.started":"2021-09-01T18:12:57.009872Z","shell.execute_reply":"2021-09-01T18:12:57.031003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural network approach","metadata":{}},{"cell_type":"code","source":"load_model_bool = False\nif not load_model_bool:\n    # Setting up a train-test split according to date_block_num (train on 0-32, validate on 33, test on 34)\n    df_for_nn_train = df_for_nn.loc[(df_for_nn['date_block_num']<33)]\n    df_for_nn_test = df_for_nn.loc[df_for_nn['date_block_num']==33]\n\n    df_for_nn_train_x = df_for_nn_train.drop('item_cnt_month', axis=1)\n    df_for_nn_train_y = df_for_nn_train['item_cnt_month']\n    df_for_nn_test_x = df_for_nn_test.drop('item_cnt_month', axis=1)\n    df_for_nn_test_y = df_for_nn_test['item_cnt_month']\n\n    del df_for_nn_train\n    del df_for_nn_test\n\n    train_x = df_for_nn_train_x.values\n    train_y = df_for_nn_train_y.values\n    test_x = df_for_nn_test_x.values\n    test_y = df_for_nn_test_y.values\n\n    # Include callbacks for model checkpoints and early stopping\n    best_model_name = 'best_model_foldno_' + str(0)+'.h5'\n    es = EarlyStopping(monitor='val_root_mean_squared_error', mode='min', patience=100,min_delta=0.001, verbose=1)\n    mc = ModelCheckpoint(best_model_name, monitor='val_root_mean_squared_error', mode='min', save_best_only=True, verbose=1)\n    cb_list = [es, mc]\n\n    # Create a multilayer perceptron neural network\n    model = Sequential()\n    model.add(Input(shape=train_x.shape[1]))\n\n    model.add(Dense(140, activation=None,\n                    #kernel_regularizer=regularizers.l1_l2(l1=0.000001, l2=0.000001),\n                    #bias_regularizer=regularizers.l2(0.000001),\n                    #activity_regularizer=regularizers.l2(0.000001)\n                   ))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dropout(0.1))\n\n    model.add(Dense(70, activation=None,\n                    #kernel_regularizer=regularizers.l1_l2(l1=0.000001, l2=0.000001),\n                    #bias_regularizer=regularizers.l2(0.000001),\n                    #activity_regularizer=regularizers.l2(0.000001)\n                   ))\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dropout(0.1))\n\n    model.add(Dense(1, activation=None))\n    model.compile(loss=root_mean_squared_error, optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])\n\n    # Train the model with high batchsize and high learning rate\n    print('Starting training')\n    history = model.fit(x=train_x,y=train_y,batch_size=32000,validation_data=(test_x,test_y), epochs=2000, callbacks=cb_list, verbose = 0)\n    score=model.evaluate(test_x,test_y)\n    model.save('nn_model')\n\n    # summarize history for root_mean_squared_error\n    plt.close()\n    plt.plot(history.history['root_mean_squared_error'][3:])\n    plt.plot(history.history['val_root_mean_squared_error'][3:])\n    plt.title('model accuracy')\n    plt.ylabel('root_mean_squared_error')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'][3:])\n    plt.plot(history.history['val_loss'][3:])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n\n    print('_____________________________________________')","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:01:21.634338Z","iopub.execute_input":"2021-09-01T22:01:21.634714Z","iopub.status.idle":"2021-09-01T22:16:41.457532Z","shell.execute_reply.started":"2021-09-01T22:01:21.634681Z","shell.execute_reply":"2021-09-01T22:16:41.456444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Fine plots:","metadata":{}},{"cell_type":"code","source":"if not load_model_bool:\n    plt.close()\n    plt.plot(history.history['root_mean_squared_error'][13:])\n    plt.plot(history.history['val_root_mean_squared_error'][13:])\n    plt.title('model RMSE')\n    plt.ylabel('root_mean_squared_error')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'][128:])\n    plt.plot(history.history['val_loss'][128:])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:58:26.237411Z","iopub.execute_input":"2021-09-01T17:58:26.238042Z","iopub.status.idle":"2021-09-01T17:58:26.56435Z","shell.execute_reply.started":"2021-09-01T17:58:26.237997Z","shell.execute_reply":"2021-09-01T17:58:26.563284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine training with different batch size\nUsually not necessary, but occasionally it brings a significant improvement","metadata":{}},{"cell_type":"code","source":"continue_training = False\nif continue_training:\n    # Loading the model overrides the optimizer status\n    #model = load_model('best_model_foldno_0.h5')\n    #model = load_model('nn_model')\n    from keras import backend as K\n    K.set_value(model.optimizer.learning_rate, 0.001)\n\n    es = EarlyStopping(monitor='val_root_mean_squared_error', mode='min', verbose=1, patience=50,min_delta=0.001)\n    mc = ModelCheckpoint('fine_'+best_model_name, monitor='val_root_mean_squared_error', mode='min', save_best_only=True, verbose=1)\n    cb_list = [es, mc]\n\n    history = model.fit(x=train_x,y=train_y,batch_size=1000,validation_data=(test_x,test_y), epochs=2000, callbacks=cb_list, verbose = 0)\n    score=model.evaluate(test_x,test_y)\n\n    # summarize history for accuracy\n    plt.close()\n    plt.plot(history.history['root_mean_squared_error'])\n    plt.plot(history.history['val_root_mean_squared_error'])\n    plt.title('model accuracy')\n    plt.ylabel('root_mean_squared_error')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n\n    print('_____________________________________________')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make predictions using the model with the best evaluation score","metadata":{}},{"cell_type":"code","source":"if load_model_bool:\n    best_model = load_model(path+'best_model_foldno_0.h5', custom_objects={'root_mean_squared_error': root_mean_squared_error})\nelse:\n    best_model = load_model('best_model_foldno_0.h5', custom_objects={'root_mean_squared_error': root_mean_squared_error})\npredictions = best_model.predict(df_for_predictions.drop(['ID','item_cnt_month'],axis=1).values)\ndf_for_predictions['item_cnt_month'] = predictions","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:16:41.459312Z","iopub.execute_input":"2021-09-01T22:16:41.459674Z","iopub.status.idle":"2021-09-01T22:16:46.333164Z","shell.execute_reply.started":"2021-09-01T22:16:41.459635Z","shell.execute_reply":"2021-09-01T22:16:46.332259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare and save the dataframe for submission\ndf_for_submission = test.copy()\ndf_for_submission = df_for_submission.merge(df_for_predictions, how='left', suffixes=('','_'),on=['item_id','shop_id']).fillna(0)[['ID','item_cnt_month']]\n\nprint('Mean: ',df_for_submission['item_cnt_month'].mean(),'|| Min: ',df_for_submission['item_cnt_month'].min(),'|| Max: ', df_for_submission['item_cnt_month'].max(),'|| Sum: ', df_for_submission['item_cnt_month'].sum())\nprint('Percentage of predicted values: ', predictions.size/df_for_submission['item_cnt_month'].size) \n\ndf_for_submission['item_cnt_month'] = df_for_submission['item_cnt_month'].clip(0,20).round(3)\n\ndf_for_submission.to_csv('df_for_submission.csv', index=False)\n\n# Rounding is usually detrimental for the result, I recommend using the unrounded results\ndf_for_submission_rounded = df_for_submission.copy()\ndf_for_submission_rounded['item_cnt_month'] = df_for_submission_rounded['item_cnt_month'].round()\n\ndf_for_submission_rounded.to_csv('df_for_submission_rounded.csv', index=False)\n\nprint('===========================================================================')\nprint('Mean: ',df_for_submission['item_cnt_month'].mean(),'|| Min: ',df_for_submission['item_cnt_month'].min(),'|| Max: ', df_for_submission['item_cnt_month'].max(),'|| Sum: ', df_for_submission['item_cnt_month'].sum())\nprint('Percentage of zeros: ',df_for_submission['item_cnt_month'].round().value_counts()[0]/df_for_submission['item_cnt_month'].size) \nprint('Percentage of predicted values: ', predictions.size/df_for_submission['item_cnt_month'].size) ","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:16:46.334767Z","iopub.execute_input":"2021-09-01T22:16:46.335085Z","iopub.status.idle":"2021-09-01T22:16:47.70655Z","shell.execute_reply.started":"2021-09-01T22:16:46.335048Z","shell.execute_reply":"2021-09-01T22:16:47.705493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check for high memory consumption and potentially delete some for the next step\nAlthough it might be better to run the xgboost version only if you didn't run the neural network version. Due to memory constraints I can only run either the feature extraction, or the NN, or the xgboost model.\nThis was no problem when these were 3 different notebooks, but I tried to combine those into one for the Coursera submission and now I can not save the dataframes, the NN and the xgboost model all as output.","metadata":{}},{"cell_type":"code","source":"import sys\n\n# These are the usual ipython objects\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:20:14.635003Z","iopub.execute_input":"2021-09-01T22:20:14.63542Z","iopub.status.idle":"2021-09-01T22:20:15.148746Z","shell.execute_reply.started":"2021-09-01T22:20:14.635382Z","shell.execute_reply":"2021-09-01T22:20:15.147892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_for_nn_train_x\ndel df_for_nn_train_y\ndel transactions\ndel df_for_nn_test_x\ndel df_for_nn_test_y\ndel df_for_submission\ndel df_for_submission_rounded\ndel item_categories\ndel shops","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:22:38.586536Z","iopub.execute_input":"2021-09-01T22:22:38.586876Z","iopub.status.idle":"2021-09-01T22:22:38.603313Z","shell.execute_reply.started":"2021-09-01T22:22:38.586845Z","shell.execute_reply":"2021-09-01T22:22:38.602454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGB based approach - very fast with good (but not best) results, has feature importance plots","metadata":{}},{"cell_type":"code","source":"use_pca = False\nif use_pca:\n    from sklearn.decomposition import PCA\n    n_components = 10\n    df = df_for_nn.drop(['item_cnt_month','date_block_num','shop_id','item_id','item_category_id'],axis=1)\n\n    pca = PCA(n_components)\n    pca.fit(df)\n\n    plt.plot(pca.explained_variance_ratio_)\n    plt.ylabel('Explained Variance')\n    plt.xlabel('Components')\n    plt.show()\n\n    df_pca = pd.DataFrame(pca.transform(df), columns=['PCA%i' % i for i in range(n_components)], index=df.index)\n    df_pca[['item_cnt_month','date_block_num','shop_id','item_id','item_category_id']] = df_for_nn[['item_cnt_month','date_block_num','shop_id','item_id','item_category_id']]\n    df_pca.to_pickle('df_for_pca.pkl')\n\n    df = df_for_predictions.drop(['ID','item_cnt_month','date_block_num','shop_id','item_id','item_category_id'], axis=1)\n    df_pca_test = pd.DataFrame(pca.transform(df), columns=['PCA%i' % i for i in range(n_components)], index=df.index)\n    df_pca_test[['item_cnt_month','date_block_num','shop_id','item_id','item_category_id']] = df_for_predictions[['item_cnt_month','date_block_num','shop_id','item_id','item_category_id']]\n    df_pca_test.to_pickle('df_pca_predictions.pkl')\n    df_for_predictions = df_pca_test\n    \n    data_pred_dmatrix = xgb.DMatrix(data=df_pca_test.drop(['item_cnt_month'], axis=1),label=None)\n    data_train_dmatrix = xgb.DMatrix(data=df_pca.loc[df_pca['date_block_num']<33].drop('item_cnt_month', axis=1),label=df_pca.loc[df_pca['date_block_num']<33]['item_cnt_month'])\n    data_test_dmatrix = xgb.DMatrix(data=df_pca.loc[df_pca['date_block_num']==33].drop('item_cnt_month', axis=1),label=df_pca.loc[df_pca['date_block_num']==33]['item_cnt_month'])\n    \n    del df_pca_test\n    del df\n\nelse:\n    data_pred_dmatrix = xgb.DMatrix(data=df_for_predictions.drop(['item_cnt_month','ID'], axis=1),label=None)\n    data_train_dmatrix = xgb.DMatrix(data=df_for_nn.loc[df_for_nn['date_block_num']<33].drop('item_cnt_month', axis=1),label=df_for_nn.loc[df_for_nn['date_block_num']<33]['item_cnt_month'])\n    data_test_dmatrix = xgb.DMatrix(data=df_for_nn.loc[df_for_nn['date_block_num']==33].drop('item_cnt_month', axis=1),label=df_for_nn.loc[df_for_nn['date_block_num']==33]['item_cnt_month'])\n\n\nparams = {\"objective\":\"reg:squarederror\",'colsample_bytree': 1,'learning_rate': 0.1,\n                'max_depth': 11, 'alpha': 2,'lambda': 1,'subsample': 1.0, 'gamma': 1,'tree_method':'gpu_hist'}\n\nxg_reg = xgb.train(params=params, dtrain=data_train_dmatrix, evals=[(data_train_dmatrix, 'train'), (data_test_dmatrix, 'valid')], verbose_eval = 20, num_boost_round=2000, early_stopping_rounds=50)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:26:34.089005Z","iopub.execute_input":"2021-09-01T22:26:34.089367Z","iopub.status.idle":"2021-09-01T22:27:40.240085Z","shell.execute_reply.started":"2021-09-01T22:26:34.089325Z","shell.execute_reply":"2021-09-01T22:27:40.239108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = xg_reg.predict(data_pred_dmatrix)\n\ndf_for_submission = test.copy()\n\ndf_for_predictions['item_cnt_month'] = predictions\nprint(df_for_predictions['item_cnt_month'].mean())\n\ndf_for_submission = df_for_submission.merge(df_for_predictions, how='left', suffixes=('','_'),on=['item_id','shop_id']).fillna(0)[['ID','item_cnt_month']]\n\nprint('Mean: ',df_for_submission['item_cnt_month'].mean(),'|| Min: ',df_for_submission['item_cnt_month'].min(),'|| Max: ', df_for_submission['item_cnt_month'].max(),'|| Sum: ', df_for_submission['item_cnt_month'].sum())\n#print('Percentage of zeros: ',df_for_submission['item_cnt_month'].value_counts()[0]/df_for_submission['item_cnt_month'].size) \nprint('Percentage of predicted values: ', predictions.size/df_for_submission['item_cnt_month'].size) \ndf_for_submission['item_cnt_month'] = df_for_submission['item_cnt_month'].clip(0, 20).round(3)\n\ndf_for_submission.to_csv('xgb_df_for_submission.csv', index=False)\n\ndf_for_submission_rounded = df_for_submission.copy()\ndf_for_submission_rounded['item_cnt_month'] = df_for_submission_rounded['item_cnt_month'].round()\n\ndf_for_submission_rounded.to_csv('xgb_df_for_submission_rounded.csv', index=False)\n\nprint('Mean: ',df_for_submission['item_cnt_month'].mean(),'|| Min: ',df_for_submission['item_cnt_month'].min(),'|| Max: ', df_for_submission['item_cnt_month'].max(),'|| Sum: ', df_for_submission['item_cnt_month'].sum())\nprint('Percentage of zeros: ',df_for_submission['item_cnt_month'].round().value_counts()[0]/df_for_submission['item_cnt_month'].size) \nprint('Percentage of predicted values: ', predictions.size/df_for_submission['item_cnt_month'].size) ","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:28:14.386574Z","iopub.execute_input":"2021-09-01T22:28:14.386923Z","iopub.status.idle":"2021-09-01T22:28:18.257243Z","shell.execute_reply.started":"2021-09-01T22:28:14.386892Z","shell.execute_reply":"2021-09-01T22:28:18.256309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot decision tree and feature importance\n... and save them as a PDF because they are not readable inside the notebook","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = [200, 100]\nxgb.plot_tree(xg_reg)\nplt.savefig(\"temp.pdf\")","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:28:23.850512Z","iopub.execute_input":"2021-09-01T22:28:23.850875Z","iopub.status.idle":"2021-09-01T22:28:36.099194Z","shell.execute_reply.started":"2021-09-01T22:28:23.850845Z","shell.execute_reply":"2021-09-01T22:28:36.098139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.plot_importance(xg_reg)\nplt.savefig(\"temp2.pdf\")","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:28:36.100669Z","iopub.execute_input":"2021-09-01T22:28:36.100983Z","iopub.status.idle":"2021-09-01T22:28:42.822066Z","shell.execute_reply.started":"2021-09-01T22:28:36.100951Z","shell.execute_reply":"2021-09-01T22:28:42.821256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}