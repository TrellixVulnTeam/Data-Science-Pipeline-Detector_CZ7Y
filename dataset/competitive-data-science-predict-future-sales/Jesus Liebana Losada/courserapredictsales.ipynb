{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom pandas import read_csv\nimport seaborn as sns\nimport time\n\nfrom collections import Counter\nfrom scipy import stats\nimport random\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring and Predicting Sales\n\n## Descrition of this competition:\nThis challenge serves as final project for the \"How to win a data science competition\" Coursera course.\n\nIn this competition you will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company. \n\nWe are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.\n\n<b>Data fields</b><br>\n<b>ID</b> - an Id that represents a (Shop, Item) tuple within the test set<br>\n<b>shop_id</b> - unique identifier of a shop<br>\n<b>item_id</b> - unique identifier of a product<br>\n<b>item_category_id</b> - unique identifier of item category<br>\n<b>item_cnt_day</b> - number of products sold. You are predicting a monthly amount of this measure<br>\n<b>item_price</b> - current price of an item<br>\n<b>date</b> - date in format dd/mm/yyyy<br>\n<b>date_block_num</b> - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33<br>\n<b>item_name</b> - name of item<br>\n<b>shop_name</b> - name of shop<br>\n<b>item_category_name</b> - name of item category","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:#tqdm(df.columns):\n        col_type = df[col].dtypes\n\n        if col_type=='object':\n            df[col] = df[col].astype('category')\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nrootfolder = '/kaggle/input/competitive-data-science-predict-future-sales'\nitems_df = pd.read_csv(f'{rootfolder}/items.csv')\nshops_df = pd.read_csv(f'{rootfolder}/shops.csv')\nicats_df = pd.read_csv(f'{rootfolder}/item_categories.csv')\nsales_train = pd.read_csv(f'{rootfolder}/sales_train.csv')\nsmpsb_df = pd.read_csv(f'{rootfolder}/sample_submission.csv')\ntest  = pd.read_csv(f'{rootfolder}/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"icats_df.info()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"New feature the total price\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Explore target feature\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nplt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=sales_train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(sales_train.item_price.min(), sales_train.item_price.max()*1.1)\nsns.boxplot(x=sales_train.item_price)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=sales_train.date,y=sales_train.date_block_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsales_train = sales_train[sales_train.item_price<100000]\nsales_train = sales_train[sales_train.item_cnt_day<1001]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nThere is one item with price below zero. Fill it with median.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"median = sales_train[(sales_train.shop_id==32)&(sales_train.item_id==2973)&(sales_train.date_block_num==4)&(sales_train.item_price>0)].item_price.median()\nsales_train.loc[sales_train.item_price<0, 'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Month","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(35,10))\nsns.countplot(x='date_block_num', data=sales_train);\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shops","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(35,10))\nsns.countplot(x='shop_id', data=sales_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extract feature based on Categories\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Function utils","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items_categories_merged = pd.merge(icats_df,items_df,on='item_category_id',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def exclude_preprositions(x):\n    x = x.split(' ')\n    x = ' '.join(i for i in x if not i in prepositions_to_exclude).strip()\n    return x\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nitems_categories_merged['type_of_category']=items_categories_merged['item_category_name'].apply(lambda x: x.split(' ')[0].strip())\ndict_types = dict(items_categories_merged['type_of_category'].value_counts())\ncat, _ = zip(*sorted(dict_types.items(),key=lambda x: x[1])[::-1][:5])\nprint('Most frequent types of categories : {0}'.format(cat))\nnum_features = 10\nsymbols_to_exclude = ['[',']','!','.',',','*','(',')','\"',':']\nprepositions_to_exclude = ['в','на','у','the','a','an','of','для']\nfor symbol in symbols_to_exclude:\n    items_categories_merged['item_name'] = items_categories_merged['item_name'].str.replace(symbol,'')\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.lower()\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.replace('-',' ')\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.replace('/',' ')\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.strip()\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].apply(exclude_preprositions)\nvectorizer = TfidfVectorizer(max_features=num_features)\nres = vectorizer.fit_transform(items_categories_merged['item_name'])\nprint('Top {0} features of tfidf : {1}'.format(num_features,vectorizer.get_feature_names()))\ncount_vect_df = pd.DataFrame(res.todense(), columns=vectorizer.get_feature_names())\nitems_categories_merged = pd.concat([items_categories_merged,count_vect_df],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_categories_merged.drop(columns=['item_name','item_category_name'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features based on name:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nimport re\ndef create_city_name(x):\n    for i in not_city:\n        if i in x:\n            return 'unk_city'\n    return x.split(' ')[0].strip()\ndef create_shop_type(x):\n    to_return = 'unk_type'\n    for i in type_of_shops:\n        regex = re.compile(i)\n        if re.search(regex,x):\n                to_return = i \n    return to_return\n\nnot_city = ['Выездная Торговля','Интернет-магазин','Цифровой склад 1С-Онлайн']\ntype_of_shops = ['ТРЦ', 'ТЦ','ТРК','ТК','МТРЦ']+not_city\nshops_df['city_name'] = shops_df['shop_name'].apply(create_city_name)\nshops_df['shop_type'] = shops_df['shop_name'].apply(create_shop_type)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nshops_df.drop(columns='shop_name',inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_df.goup()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## Aggregate test train data\n\nWe going to aggregate the data.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales = sales_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    train_sales[\"date\"] = pd.to_datetime(train_sales[\"date\"], format=\"%d.%m.%Y\") # seting the column as pandas datetime\n    train_sales[\"month\"] = train_sales['date'].dt.day # extracting month\n    train_sales.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ny_hat_mean = (1.41241**2-1.25011**2-1)/-2\nprint('Mean of target values in public leaderboard is : {0}'.format(y_hat_mean))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nlen(list(set(test.item_id) - set(test.item_id).intersection(set(test.item_id)))), len(list(set(test.item_id))), len(test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmean = sales_train.groupby(['date_block_num','shop_id','item_id'])['item_cnt_day'].sum().mean()\nprint('Mean of target value in train data : {0}'.format(mean))\nif np.abs(mean-y_hat_mean)<0.2:\n    print('The mean of train and test targets is aligned!')\nelse:\n    print('The mean of train and test targets is not aligned!')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import product\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train_sales[train_sales.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales['revenue'] = train_sales['item_price'] *  train_sales['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_sales.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales['shop_id'] = train_sales['shop_id'].astype(np.int8)\ntrain_sales['item_id'] = train_sales['item_id'].astype(np.int16)\ntrain_sales['date_block_num'] = train_sales['date_block_num'].astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Meged test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aggregate features \nWe going aggregate the features of category month and day","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = pd.merge(matrix, shops_df, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items_categories_merged, on=['item_id'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoded features using mean","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix['month'] = matrix['date_block_num'] % 12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature preproccessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"to_encode = ['city_name','shop_type','type_of_category']\nnunique_cat = {}\nfor i in to_encode:\n    matrix[i] = matrix[i].factorize()[0]\n    nunique_cat.update({i:matrix[i].nunique()})\nnunique_cat.update({'shop_id':matrix['shop_id'].nunique()})\nnunique_cat.update({'item_id':matrix['item_id'].nunique()})\nnunique_cat.update({'item_category_id':matrix['item_category_id'].nunique()})\nprint('Factorized all the columns!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\ndataset = downcast_dtypes(matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.to_pickle('final_dataset.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine learning part","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport lightgbm as lgb\nfrom lightgbm import plot_importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_pickle('final_dataset.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columnsNames = ['date_block_num',                  \n'shop_id',                          \n'item_id',                          \n'item_cnt_month',                                          \n'item_category_id',                 \n'type_of_category',   \n'city_name',               \n'bd',                               \n'cd',                               \n'dvd',                              \n'jewel',                            \n'mp3',                              \n'pc',                               \n'версия',                           \n'регион',                           \n'русская',                          \n'цифровая',  \n'item_cnt_month_lag_1',           \n'item_cnt_month_lag_2',        \n'item_cnt_month_lag_3',         \n'item_cnt_month_lag_6',         \n'item_cnt_month_lag_12',          \n'date_avg_item_cnt_lag_1',                  \n'date_item_avg_item_cnt_lag_1',     \n'date_item_avg_item_cnt_lag_2',     \n'date_item_avg_item_cnt_lag_3',     \n'date_item_avg_item_cnt_lag_6',     \n'date_item_avg_item_cnt_lag_12',    \n'date_shop_avg_item_cnt_lag_1',     \n'date_shop_avg_item_cnt_lag_2',     \n'date_shop_avg_item_cnt_lag_3',     \n'date_shop_avg_item_cnt_lag_6',     \n'date_shop_avg_item_cnt_lag_12',    \n'date_cat_avg_item_cnt_lag_1',      \n'date_shop_cat_avg_item_cnt_lag_1', \n'month']         \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = dataset[columnsNames]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom xgboost import plot_importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": X_test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}