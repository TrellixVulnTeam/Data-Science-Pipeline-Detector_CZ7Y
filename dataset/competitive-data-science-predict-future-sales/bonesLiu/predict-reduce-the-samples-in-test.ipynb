{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nimport datetime\nimport warnings\nimport os\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/competitive-data-science-predict-future-sales/'\n\nos.listdir(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.read_csv(path + 'items.csv')\nitem_categories = pd.read_csv(path + 'item_categories.csv')\ntrain = pd.read_csv(path + 'sales_train.csv')\nshop = pd.read_csv(path + 'shops.csv')\ntest = pd.read_csv(path + 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The dataset of items"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The information of the dataset'.center(50, '-'))\nprint('The shape of the dataset is {}'.format(items.shape))\nprint('The number of the goods {}'.format(items.item_id.nunique()))\nprint('The category of the goods {}'.format(items.item_category_id.nunique()))\nitems.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The missing value of the dataset\ntotal = items.isnull().sum()\npercentage = total / items.shape[0]\ntypes = items.dtypes\npd.concat([total, percentage, types], axis = 1, keys = ['Total', 'Percentage', 'Types'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The dataset of train"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The information of the dataset'.center(50, '-'))\nprint('The shape of the dataset is {}'.format(train.shape))\nprint('The number of the goods {}'.format(train.item_id.nunique()))\ntrain.sample(4)\n# the numner of the goods 21807 mean that some goods hasn't sale at all.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['date'] = train['date'].apply(lambda x: datetime.datetime.strptime(x, '%d.%m.%Y'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The missing value of the dataset\ntotal = train.isnull().sum()\npercentage = total / train.shape[0]\ntypes = train.dtypes\npd.concat([total, percentage, types], axis = 1, keys = ['Total', 'Percentage', 'Types'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - The dataset of shop"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The information of the shop dataset'.center(50, '-'))\nprint('The shape of the shop dataset {}'.format(shop.shape))\nprint('The shop information {} and the colums {}'.format(shop.nunique(), shop.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The dataset of item_categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The information of the item_categories dataset'.center(80, '-'))\nprint('The shape of the shop dataset {}'.format(item_categories.shape))\nprint('The shop information {} and the colums {}'.format(item_categories.nunique(), item_categories.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - The dataset after merged"},{"metadata":{"trusted":true},"cell_type":"code","source":"salesData = pd.merge(train, items, how = 'inner', on = 'item_id')\nsalesData = pd.merge(salesData, shop, how = 'inner', on = 'shop_id')\nsalesData = pd.merge(salesData, item_categories, how = 'inner', on = 'item_category_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_shop_name = salesData['shop_name']\nsalesData.drop('shop_name', axis = 1, inplace = True)\nsalesData.insert(3, 'shop_name', temp_shop_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Handled the train data and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It mean that the test data samples less than train data\nprint(train['shop_id'].nunique() is test['shop_id'].nunique())\ntemp_shop_id = test['shop_id'].unique()\ntemp_item_id = test['item_id'].unique()\ntrain_lk = salesData[salesData['shop_id'].isin(temp_shop_id)]\ntrain_lk = train_lk[train_lk['item_id'].isin(temp_item_id)]\nprint('Before'.center(50, '-'))\nprint('The shape of the train {}'.format(train.shape))\nprint('After'.center(50, '-'))\nprint('The shape ot the train_lk {}, and the leakage samples are {}'.format(train_lk.shape, train.shape[0] - train_lk.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"salesData.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This means that some items have never been sold in three years, and the predicted value of this sample in test can be set to 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(items['item_id'].nunique() - train['item_id'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_item_id = train['item_id'].unique()\n# ~ mean that turn the false to true\nitems_lk = items[~items['item_id'].isin(train_item_id)]\ntemp_items_lk = items_lk['item_id']\ntest_lk = test[test['item_id'].isin(temp_items_lk)]\nprint('The shape of the test_lk is {}, and the predicted value of this sample can be set to 0'.format(test_lk.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"salesData.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly = train_lk[['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'item_cnt_day']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly = train_monthly.loc[train_monthly['item_price'] > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly = train_monthly.sort_values(by = 'date').groupby(['date_block_num', 'shop_id', 'item_category_id', 'item_id']).agg({'item_price': ['sum', 'mean'], 'item_cnt_day': ['sum', 'mean', 'count']}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly.columns = ['date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'mean_item_price', 'item_cnt', 'mean_item_cnt', 'transactions']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def handleMonth(date):\n    if date < 12:\n        month = date + 1\n    elif date % 12 == 0:\n        month = 12\n    else:\n        month = (date % 12) + 1\n    return month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly['year'] = train_monthly['date_block_num'].apply(lambda x: ((x//12) + 2013))\ntrain_monthly['month'] = train_monthly['date_block_num'].apply(handleMonth)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_mean = train_monthly.groupby(['month'])['item_cnt'].mean().reset_index()\nmonth_sum = train_monthly.groupby(['month'])['item_cnt'].sum().reset_index()\n\nmonth_data = pd.merge(month_mean, month_sum, how = 'inner', on = 'month')\n\ncategory_mean = train_monthly.groupby(['item_category_id'])['item_cnt'].mean().reset_index()\ncategory_sum = train_monthly.groupby(['item_category_id'])['item_cnt'].sum().reset_index()\n\ncategory_data = pd.merge(category_mean, category_sum, how = 'inner', on = 'item_category_id')\n\nshop_mean = train_monthly.groupby(['shop_id'])['item_cnt'].mean().reset_index()\nshop_sum = train_monthly.groupby(['shop_id'])['item_cnt'].sum().reset_index()\n\nshop_data = pd.merge(shop_mean, shop_sum, how = 'inner', on = 'shop_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_data.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize = (22, 4))\nsns.set_style('whitegrid')\nax1 = sns.pointplot(x = 'month', y = 'item_cnt_x', data = month_data, linestyles = '-', ax = axes[0])\nax2 = sns.pointplot(x = 'month', y = 'item_cnt_y', data = month_data, linestyles = '-', ax = axes[1])\nax1.set(title = 'mothly mean', ylabel = 'item_cnt')\nax2.set(title = 'mothly sum', ylabel = 'item_cnt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_data.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize = (18, 8))\nsns.set_style('whitegrid')\nax1 = sns.barplot(x = 'item_category_id', y = 'item_cnt_x', data = category_data, ax = axes[0])\nax2 = sns.barplot(x = 'item_category_id', y = 'item_cnt_y', data = category_data, ax = axes[1])\nax1.set(title = 'category mean', ylabel = 'item_cnt')\nax2.set(title = 'category sum', ylabel = 'item_cnt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_data.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize = (18, 8), sharex = True)\nsns.set_style('whitegrid')\nax1 = sns.barplot(x = 'shop_id', y = 'item_cnt_x', data = shop_data, ax = axes[0], palette = 'mako')\nax2 = sns.barplot(x = 'shop_id', y = 'item_cnt_y', data = shop_data, ax = axes[1], palette = 'mako')\nax1.set(title = 'shop mean', ylabel = 'item_cnt')\nax2.set(title = 'shop sum', ylabel = 'item_cnt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly = train_monthly.loc[train_monthly['item_cnt'] >= 0].loc[train_monthly['item_cnt'] <= 20].loc[train_monthly['item_price'] < 400000]\n# tht following code much better than the above\n# train_monthly = train_monthly.query('item_cnt >= 0 and item_cnt <= 20 and item_price < 400000')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly['item_price_unit'] = train_monthly['item_price'] // train_monthly['item_cnt']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gp_item_price = train_monthly.sort_values('date_block_num').groupby(['item_id']).agg({'item_price':[np.min, np.max]}).reset_index()\ngp_item_price.columns = ['item_id', 'hist_min_item_price', 'hist_max_item_price']\n\ntrain_monthly = pd.merge(train_monthly, gp_item_price, on='item_id', how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_monthly['price_increase'] = train_monthly['item_price'] - train_monthly['hist_min_item_price']\ntrain_monthly['price_decrease'] = train_monthly['hist_max_item_price'] - train_monthly['item_price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Min value\nf_min = lambda x: x.rolling(window=3, min_periods=1).min()\n# Max value\nf_max = lambda x: x.rolling(window=3, min_periods=1).max()\n# Mean value\nf_mean = lambda x: x.rolling(window=3, min_periods=1).mean()\n# Standard deviation\nf_std = lambda x: x.rolling(window=3, min_periods=1).std()\n\nfunction_list = [f_min, f_max, f_mean, f_std]\nfunction_name = ['min', 'max', 'mean', 'std']\n\nfor i in range(len(function_list)):\n    train_monthly[('item_cnt_%s' % function_name[i])] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].apply(function_list[i])\n\n# Fill the empty std features with 0\ntrain_monthly['item_cnt_std'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The shape of the dataset {}'.format(train_monthly.shape))\ntrain_monthly.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# maybe some added some codes later","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - to be continue"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}