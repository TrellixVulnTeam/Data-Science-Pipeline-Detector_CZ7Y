{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# System pacakges\nimport os\nimport sys\nimport re\nimport gc\nimport time\nimport datetime\nimport warnings\n\n# Data pacakges\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Modelling pacakges\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\n# Pacakge settings\n%matplotlib inline\nsns.set(style=\"darkgrid\")\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.float_format', lambda x: '%.2f' % x)\nsys.version_info","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DATA_FOLDER = '../input/competitive-data-science-predict-future-sales/'\n\ntrain = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv'))\nitems = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\n# Drop name to save space\nitems = items.drop('item_name', axis=1)\n# Set index to ID to avoid droping it later\ntest  = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv')).set_index('ID')\n# Add date_block_num = 34\ntest['date_block_num'] = 34\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the ranges of each feature to select the most appropriate data size\nprint ('-------------------------------------------------------')\nprint ('train:')\nfor f in train.columns.values:\n    print ('%s: %s ~ %s' %(f, train[f].min(), train[f].max()))\nprint ('-------------------------------------------------------')\nprint ('items:')\nfor f in items.columns.values:\n    print ('%s: %s ~ %s' %(f, items[f].min(), items[f].max()))\nprint ('-------------------------------------------------------')\nprint ('test:')\nfor f in test.columns.values:\n    print ('%s: %s ~ %s' %(f, test[f].min(), test[f].max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compress_columns(df,columns,keyword,search_type,datatype):\n    if search_type=='in':\n        valid_features = [x for x in columns if keyword in x]\n    elif search_type=='start':\n        valid_features = [x for x in columns if x.startswith(keyword)]\n    if len(valid_features):\n        for f in valid_features:\n            df[f] = df[f].round().astype(datatype)\n    return df\n\ndef data_compression(df):\n    features = df.columns.values\n    # Original features\n    if 'date_block_num' in features:\n        df['date_block_num'] = df['date_block_num'].astype(np.int8)\n    if 'shop_id' in features:\n        df['shop_id'] = df['shop_id'].astype(np.int8)\n    if 'item_category_id' in features:\n        df['item_category_id'] = df['item_category_id'].astype(np.int8)\n    if 'item_id' in features:\n        df['item_id'] = df['item_id'].astype(np.int16)\n    if 'item_price' in features:\n        df['item_price'] = df['item_price'].astype(np.float32)\n    if 'item_id_avg_item_price' in features:\n        df['item_id_avg_item_price'] = df['item_id_avg_item_price'].astype(np.float32)\n        \n    # Mean encoded features & lag features\n    df = compress_columns(df,features,'item_id_sum_item_cnt_day','in',np.int16)\n    df = compress_columns(df,features,'item_id_avg_item_cnt_day','in',np.float16)\n    \n    df = compress_columns(df,features,'shop_id_avg_item_price','in',np.float16)\n    df = compress_columns(df,features,'shop_id_sum_item_cnt_day','in',np.int16)\n    df = compress_columns(df,features,'shop_id_avg_item_cnt_day','in',np.float16)\n    \n    df = compress_columns(df,features,'item_category_id_avg_item_price','in',np.float16)\n    df = compress_columns(df,features,'item_category_id_sum_item_cnt_day','in',np.int32)\n    df = compress_columns(df,features,'item_category_id_avg_item_cnt_day','in',np.float16)\n    \n    df = compress_columns(df,features,'item_cnt_day','start',np.int16)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compress features according to range\ntrain = data_compression(train)\nitems = data_compression(items)\ntest = data_compression(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Include Category id\ntrain = pd.merge(train,items,on='item_id',how='left')\ntest = pd.merge(test,items, on='item_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def box_plot(df,f):\n    plt.figure(figsize=(10,4))\n    plt.title(f+' distribution')\n    x_min = int(df[f].min() - (abs(df[f].min())*0.1))\n    x_max = int(df[f].max() + (abs(df[f].max())*0.1))\n    if x_min==0:\n        x_min = -1\n    if x_max==0:\n        x_max = 1\n    plt.xlim(x_min,x_max)\n    sns.boxplot(x=df[f])\n\nplot_features = [x for x in train.columns.values if train[x].dtype != 'object']\nfor f in plot_features:\n    box_plot(train,f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting rid of the outliers & negative values\ntrain = train[(train['item_price']<100000) & (train['item_price']>=0)]\ntrain = train[(train['item_cnt_day']<1000) & (train['item_cnt_day']>=0)]\n\n# distribution after outliers removal\nplot_features = ['item_price','item_cnt_day']\nfor f in plot_features:\n    box_plot(train,f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a grid with columns\nindex_cols = ['shop_id','item_id','date_block_num']\n\n# For every month we create a grid for all shops/items pair\ngrid = []\nfor block_num in train['date_block_num'].unique():\n    cur_shops = train.loc[train['date_block_num']==block_num,'shop_id'].unique()\n    cur_items = train.loc[train['date_block_num']==block_num,'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops,cur_items,[block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid),columns=index_cols,dtype=np.int32)\ngrid = data_compression(grid)\ngrid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group items per month, per shop, per item, sum the sales of the item, mean the price\n# There is a big difference between np.mean and pandas mean\ntrain_m = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day':'sum','item_price':np.mean}).reset_index()\ntrain_m = pd.merge(grid,train_m,on=['date_block_num','shop_id','item_id'],how='left').fillna(0)\ntrain_m = pd.merge(train_m,items,on='item_id',how='left')\ntrain_m = data_compression(train_m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the mean encoded features\nfor type_id in ['item_id', 'shop_id', 'item_category_id']:\n    for column_id, aggregator, aggtype in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:\n        mean_df = train.groupby([type_id,'date_block_num']).aggregate(aggregator).reset_index()[[column_id,type_id,'date_block_num']]\n        mean_df.columns = [type_id+'_'+aggtype+'_'+column_id,type_id,'date_block_num']\n        train_m = pd.merge(train_m, mean_df, on=['date_block_num',type_id], how='left')\n        del mean_df\n        gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in train_m.columns:\n    if 'item_cnt' in f:\n        train_m[f] = train_m[f].fillna(0)\n    elif 'item_price' in f:\n        train_m[f] = train_m[f].fillna(train_m[f].median())\n\n# Compress data\ntrain_m = data_compression(train_m)\ntrain_m.info(verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the positions of the base lag features\ntrain_m.columns.values[6:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all the monthly features, which means the Mean Encoded fatures are all monthly based\nlag_features = list(train_m.columns[6:])+['item_cnt_day']\n# The selected months from current month\nlags = [1,2,3,6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for lag in lags:\n    train_new_df = train_m.copy()\n    # Get the current month\n    train_new_df['date_block_num'] += lag\n    train_new_df = train_new_df[['date_block_num','shop_id','item_id']+lag_features]\n    # Name the columns as lag features of the month\n    train_new_df.columns = ['date_block_num','shop_id','item_id'] + [x+'_lag_'+str(lag) for x in lag_features]\n    train_m = pd.merge(train_m,train_new_df,on=['date_block_num','shop_id','item_id'],how='left')\n    del train_new_df\n    gc.collect()\n    print ('lag %s processed' %lag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill NaNs\nfor f in train_m.columns:\n    if 'item_cnt' in f:\n        train_m[f] = train_m[f].fillna(0)\n    elif 'item_price' in f:\n        train_m[f] = train_m[f].fillna(train_m[f].median())\n\ntrain_m = data_compression(train_m)\ntrain_m.info(verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the maximum clip value\nmax_clip = 30\ntrain_m['item_cnt_day'] = train_m['item_cnt_day'].clip(0,max_clip).astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add lag variables\nfor lag in lags:\n    train_new_df = train_m.copy()\n    # Get the current month\n    train_new_df['date_block_num'] += lag\n    train_new_df = train_new_df[['date_block_num','shop_id','item_id']+lag_features]\n    # Name the columns as lag features of the month\n    train_new_df.columns = ['date_block_num','shop_id','item_id'] + [x+'_lag_'+str(lag) for x in lag_features]\n    test = pd.merge(test,train_new_df,on=['date_block_num','shop_id','item_id'],how='left')\n    del train_new_df\n    gc.collect()\n    print ('lag %s processed' %lag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill NaNs\nfor f in test.columns:\n    if 'item_cnt' in f:\n        test[f] = test[f].fillna(0)\n    elif 'item_price' in f:\n        test[f] = test[f].fillna(test[f].median())\n\ntest = data_compression(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = lag_features[:-1] + ['item_price']\nprint ('Columns to drop')\nprint (cols_to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cols = train_m.columns.values\ntest_cols = test.columns.values\nfor c in cols_to_drop:\n    if c in train_cols:\n        train_m = train_m.drop(c,axis=1)\n    if c in test_cols:\n        test = test.drop(c,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Month number\ntrain_m['month'] = train_m['date_block_num']%12\ntrain_m['month'] = train_m['month'].astype(np.int8)\n# Number of days in a month, no leap years here\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\ntrain_m['days'] = train_m['month'].map(days).astype(np.int8)\n\ntest['month'] = 11\ntest['month'] = test['month'].astype(np.int8)\ntest['days'] = 30\ntest['days'] = test['days'].astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assert all the columns are the same except target column\nset(train_m.columns.values) ^ set(test.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_m.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['shop_id','item_id']+['item_cnt_day_lag_'+str(x) for x in [1,2,3]]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_m[train_m['shop_id'] == 5][train_m['item_id'] == 5037][train_m['date_block_num'] == 33]['item_cnt_day'])\nprint(train_m[train_m['shop_id'] == 5][train_m['item_id'] == 5037][train_m['date_block_num'] == 32]['item_cnt_day'])\nprint(train_m[train_m['shop_id'] == 5][train_m['item_id'] == 5037][train_m['date_block_num'] == 31]['item_cnt_day'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_m = train_m[train_m['date_block_num']>12]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = train_m[train_m['date_block_num']<33]\nval_set = train_m[train_m['date_block_num']==33]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save data\ntrain_set.to_pickle('train.pkl')\nval_set.to_pickle('val.pkl')\ntest.to_pickle('test.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_m\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# divide data into x & y\ntrain_x = train_set.drop(['item_cnt_day'],axis=1)\ntrain_y = train_set['item_cnt_day']\nval_x = val_set.drop(['item_cnt_day'],axis=1)\nval_y = val_set['item_cnt_day']\n\nfeatures = list(train_x.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For saving data & output results / models\ndef post_processing(model,model_name,train_x,val_x,test_x,train_y,val_y,test):\n    # Here we once again clip the output to 0~20\n    train_pred = model.predict(train_x).clip(0, 20)\n    val_pred = model.predict(val_x).clip(0, 20)\n    test_pred = model.predict(test_x).clip(0, 20)\n\n    # Get rmse scores\n    train_rmse = np.sqrt(mean_squared_error(train_y, train_pred))\n    print(\"Train RMSE: %f\" % (train_rmse))\n    val_rmse = np.sqrt(mean_squared_error(val_y, val_pred))\n    print(\"Val RMSE: %f\" % (val_rmse))\n    \n    # Export submission\n    submission = pd.DataFrame({'ID':test.index,'item_cnt_month': test_pred})\n    submission.to_csv('%s_submission.csv'%model_name,index=False)\n\n    # save model to file\n    pickle.dump(lm, open(\"%s_model.pickle\" %model_name, \"wb\"))\n    return train_pred,val_pred,test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For plotting feature importance\ndef plot_feature_importances(importances,indices,features,title,dimensions):\n    plt.figure(figsize=dimensions)\n    plt.title(title)\n    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n    plt.yticks(range(len(indices)), [features[i] for i in indices])\n    plt.xlabel('Relative Importance')\n    plt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalise data\nscaler = preprocessing.StandardScaler()\nscaler.fit(train_x.values)\ntrain_x_norm = scaler.transform(train_x.values)\nval_x_norm = scaler.transform(val_x.values)\ntest_norm = scaler.transform(test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training\ngc.collect()\nts = time.time()\n# Training\nlm = linear_model.Ridge()\nlm.fit(train_x_norm,train_y)\nprint ('Training time: %s' %(time.time() - ts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performance and test predictions\ntrain_pred1,val_pred1,test_pred1 = post_processing(lm,'ridge',train_x_norm,val_x_norm,test_norm,train_y,val_y,test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Importance\nimportances = abs(lm.coef_)\nindices = np.argsort(importances)\ntitle = 'Linear Regression Feature Importances'\nplot_feature_importances(importances,indices,features,title,(8,16))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_x_norm\ndel val_x_norm\ndel test_norm\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training\ngc.collect()\nts = time.time()\nxgbtrain = xgb.DMatrix(train_x.values, train_y.values)\n\nparam = {'max_depth':8, \n         'subsample':1,\n         'min_child_weight':0.5,\n         'eta':0.3, \n         'num_round':800, \n         'seed':1,\n         'verbosity':2,\n         'eval_metric':'rmse'} # random parameters\n\nbst = xgb.train(param, xgbtrain)\nprint ('Training time: %s' %(time.time() - ts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performance and test predictions\ntrain_pred2,val_pred2,test_pred2 = post_processing(bst,'xgboost',xgb.DMatrix(train_x.values),xgb.DMatrix(val_x.values),xgb.DMatrix(test.values),train_y,val_y,test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}