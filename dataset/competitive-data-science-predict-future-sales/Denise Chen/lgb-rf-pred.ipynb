{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-16T18:24:36.154885Z","iopub.execute_input":"2021-08-16T18:24:36.155253Z","iopub.status.idle":"2021-08-16T18:24:36.187029Z","shell.execute_reply.started":"2021-08-16T18:24:36.155197Z","shell.execute_reply":"2021-08-16T18:24:36.185788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn\nfrom sklearn.model_selection import GridSearchCV, PredefinedSplit\nimport lightgbm as lgb\nimport scipy.sparse \nimport lightgbm as lgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle, gc\nfrom tqdm import tqdm_notebook\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nimport warnings\nfrom itertools import product\nwarnings.filterwarnings('ignore')\n%matplotlib inline \n\npd.set_option('display.max_rows', 600)\npd.set_option('display.max_columns', 50)\nsns.set(rc={'figure.figsize':(20, 10)})","metadata":{"execution":{"iopub.status.busy":"2021-08-16T18:24:39.951063Z","iopub.execute_input":"2021-08-16T18:24:39.951453Z","iopub.status.idle":"2021-08-16T18:24:42.608779Z","shell.execute_reply.started":"2021-08-16T18:24:39.951422Z","shell.execute_reply":"2021-08-16T18:24:42.607604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\nitems = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nitem_cats = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\ntest = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T18:24:45.52335Z","iopub.execute_input":"2021-08-16T18:24:45.523838Z","iopub.status.idle":"2021-08-16T18:24:48.49162Z","shell.execute_reply.started":"2021-08-16T18:24:45.523793Z","shell.execute_reply":"2021-08-16T18:24:48.490357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA\n- the total sales by each month\n    - the top sales is reached in month 11 and 23.\n- shop and items combinations per month\n    - there are more shops selling items in month 0, 2, 11 and 23.\n- total sales from each shop\n    - There are over 300,000 sales from shop 31. And, the shop 25 has over 200,000 sales.\n- total amount sold to customers per item\n    - item id 20949 has the highest sales.\n- total items sold in each category\n    - 40 and 30 category items have the most sales.\n    \n    ","metadata":{}},{"cell_type":"code","source":"sns.set_context(\"talk\", font_scale=1.4)\nsales_month = pd.DataFrame(sales.groupby(['date_block_num']).sum().item_cnt_day).reset_index()\nsales_month.columns = ['date_block_num', 'sum_items_sold']\nsns.barplot(x ='date_block_num', y='sum_items_sold', \n            data=sales_month.reset_index());\nplt.plot(sales_month.sum_items_sold)\nplt.title('Distribution of the sum of sales per month')\ndel sales_month","metadata":{"execution":{"iopub.status.busy":"2021-08-16T15:59:34.905446Z","iopub.execute_input":"2021-08-16T15:59:34.905848Z","iopub.status.idle":"2021-08-16T15:59:35.762913Z","shell.execute_reply.started":"2021-08-16T15:59:34.905811Z","shell.execute_reply":"2021-08-16T15:59:35.761477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comb_shop_item = pd.DataFrame(sales[['date_block_num', 'shop_id', \n                                     'item_id']].drop_duplicates().groupby('date_block_num').size()).reset_index()\ncomb_shop_item.columns = ['date_block_num', 'item-shop_comb']\nsns.barplot(x ='date_block_num', y='item-shop_comb', data=comb_shop_item);\nplt.plot(comb_shop_item['item-shop_comb']);\nplt.title('Number of combinations shop-it with sales per month')\ndel comb_shop_item","metadata":{"execution":{"iopub.status.busy":"2021-08-16T15:59:45.846258Z","iopub.execute_input":"2021-08-16T15:59:45.846667Z","iopub.status.idle":"2021-08-16T15:59:46.848365Z","shell.execute_reply.started":"2021-08-16T15:59:45.846628Z","shell.execute_reply":"2021-08-16T15:59:46.846954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_context(\"talk\", font_scale=1)\nsales_month_shop_id = pd.DataFrame(sales.groupby(['shop_id']).sum().item_cnt_day).reset_index()\nsales_month_shop_id.columns = ['shop_id', 'sum_sales']\nsns.barplot(x ='shop_id', y='sum_sales', data=sales_month_shop_id, palette='Paired')\nplt.title('Distribution of sales per shop');\ndel sales_month_shop_id","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:00:03.456134Z","iopub.execute_input":"2021-08-16T16:00:03.456535Z","iopub.status.idle":"2021-08-16T16:00:05.448638Z","shell.execute_reply.started":"2021-08-16T16:00:03.456499Z","shell.execute_reply":"2021-08-16T16:00:05.447302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_context(\"talk\", font_scale=1.4)\nsales_item_id = pd.DataFrame(sales.groupby(['item_id']).sum().item_cnt_day)\nplt.xlabel('item id')\nplt.ylabel('sales')\nplt.plot(sales_item_id);","metadata":{"execution":{"iopub.status.busy":"2021-08-16T18:25:01.865109Z","iopub.execute_input":"2021-08-16T18:25:01.865491Z","iopub.status.idle":"2021-08-16T18:25:02.388306Z","shell.execute_reply.started":"2021-08-16T18:25:01.865458Z","shell.execute_reply":"2021-08-16T18:25:02.387314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fig, ax = plt.subplots()\nsns.set_context(\"talk\", font_scale=0.8)\nsales_item_cat = sales.merge(items, how='left', on='item_id').groupby('item_category_id').item_cnt_day.sum()\nsns.barplot(x ='item_category_id', y='item_cnt_day',\n            data=sales_item_cat.reset_index(), \n            palette='Paired'\n           );\ndel sales_item_cat","metadata":{"execution":{"iopub.status.busy":"2021-08-16T18:33:50.547684Z","iopub.execute_input":"2021-08-16T18:33:50.548348Z","iopub.status.idle":"2021-08-16T18:33:53.9545Z","shell.execute_reply.started":"2021-08-16T18:33:50.548294Z","shell.execute_reply":"2021-08-16T18:33:53.953412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data LeakageÂ¶\n\nOnly the 41% of the all combinations item-shop that we have data appears in the test set.\n\nWe model in order to take advantage of this data leakage as much as possible.","metadata":{}},{"cell_type":"code","source":"tuples_df = pd.Series(list(sales[['item_id', 'shop_id']].itertuples(index=False, name=None)))\ntuples_test = pd.Series(list(test[['item_id', 'shop_id']].itertuples(index=False, name=None)))\nprint(str(round(tuples_df.isin(tuples_test).sum()/len(tuples_df),2)*100)+'%')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:01:33.560047Z","iopub.execute_input":"2021-08-16T16:01:33.560442Z","iopub.status.idle":"2021-08-16T16:01:37.794723Z","shell.execute_reply.started":"2021-08-16T16:01:33.560409Z","shell.execute_reply":"2021-08-16T16:01:37.793358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get a feature matrix\n- geneate dataframe from all shops/items combinations each month\n- features: \n    - target_shop, target_item, target (total sales per month)\n    - mean encoding features: item_target_enc\n    - lagged features of 1, 2, 3 months: target, target_shop, target_item\n","metadata":{}},{"cell_type":"code","source":"sales.groupby(['shop_id', 'item_id', 'date_block_num'])['item_cnt_day'].sum().reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:02:16.180479Z","iopub.execute_input":"2021-08-16T16:02:16.181041Z","iopub.status.idle":"2021-08-16T16:02:17.019499Z","shell.execute_reply.started":"2021-08-16T16:02:16.181005Z","shell.execute_reply":"2021-08-16T16:02:17.01836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_feature_matrix(sales, test, items, list_lags, date_block_threshold):\n    \n    \"\"\" This function create the model tablon\"\"\"\n  \n    # Create \"grid\" with columns\n    index_cols = ['shop_id', 'item_id', 'date_block_num']\n\n    # For every month we create a grid from all shops/items combinations from that month\n    grid = [] \n    new_items = pd.DataFrame()\n    cur_items_aux=np.array([])\n    for block_num in sales['date_block_num'].unique():\n        cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n        cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].append(pd.Series(cur_items_aux)).unique()\n        cur_items_aux = cur_items[pd.Series(cur_items).isin(test.item_id)]\n        grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n    # Turn the grid into a dataframe\n    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n    # Add submission shop_id-item_id in order to test predictions\n    test['date_block_num'] = 34\n    grid = grid.append(test[['shop_id', 'item_id', 'date_block_num']])\n\n    # Groupby data to get shop-item-month aggregates\n#     gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n    gb = sales.groupby(['shop_id', 'item_id', 'date_block_num'])['item_cnt_day'].sum().reset_index()\n    gb.rename(columns={\"item_cnt_day\": \"target\"}, inplace=True)\n    # Fix column names\n#     gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n    # Join it to the grid\n    all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n    # Same as above but with shop-month aggregates\n#     gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n    gb = sales.groupby(['shop_id', 'date_block_num'])['item_cnt_day'].sum().reset_index()\n    gb.rename(columns={\"item_cnt_day\": \"target_shop\"}, inplace=True)\n#     gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n    all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n    # Same as above but with item-month aggregates\n#     gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n    gb = sales.groupby(['item_id', 'date_block_num'])['item_cnt_day'].sum().reset_index()\n    gb.rename(columns={\"item_cnt_day\": \"target_item\"}, inplace=True)\n#     gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n    all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n    \n    # mean encoding\n#     shop_id_target_mean = all_data.groupby('shop_id').target.mean()\n#     all_data['shop_target_enc'] = all_data['shop_id'].map(shop_id_target_mean)\n#     all_data['shop_target_enc'].fillna(0.3343, inplace=True) \n    item_id_target_mean = all_data.groupby('item_id').target.mean()\n    all_data['item_target_enc'] = all_data['item_id'].map(item_id_target_mean)\n    all_data['item_target_enc'].fillna(0.3343, inplace=True) \n#     gb = sales[['item_target_enc', 'item_id', 'date_block_num']]\n#     all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num'])\n    print(all_data.head())\n\n    # Downcast dtypes from 64 to 32 bit to save memory\n    all_data = downcast_dtypes(all_data)\n    del grid, gb \n    gc.collect()\n    # List of columns that we will use to create lags\n    cols_to_rename = list(all_data.columns.difference(index_cols)) \n    cols_to_rename.remove('item_target_enc')\n    print(cols_to_rename)\n    shift_range = list_lags\n\n    for month_shift in tqdm_notebook(shift_range):\n        train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n        train_shift = train_shift.rename(columns=foo)\n\n        all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\n    del train_shift\n\n    # Don't use old data from year 2013\n    all_data = all_data[all_data['date_block_num'] >= date_block_threshold] \n    # List of all lagged features\n    fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n    # We will drop these at fitting stage\n    to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n    # Category for each item\n    item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n\n    all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n    all_data = downcast_dtypes(all_data)\n    gc.collect();\n    \n    return [all_data, to_drop_cols]\n\ndef downcast_dtypes(df):\n    \n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df\n\ndef clip20(x):\n    return np.clip(x, 0, 20)\n\ndef clip40(x):\n    return np.clip(x, 0, 20)\n\ndef rmse(*args):\n    \n    \"\"\" Funcion that calculates the root mean squared error\"\"\"\n    return np.sqrt(mean_squared_error(*args))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:02:21.25774Z","iopub.execute_input":"2021-08-16T16:02:21.258168Z","iopub.status.idle":"2021-08-16T16:02:21.286488Z","shell.execute_reply.started":"2021-08-16T16:02:21.258133Z","shell.execute_reply":"2021-08-16T16:02:21.28486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_lags = [1, 2, 3]\ndate_block_threshold = 12\nsales_for_modelling = sales[sales.item_id.isin(test.item_id)]\n[all_data, to_drop_cols]  = get_feature_matrix(sales_for_modelling, test, items, list_lags, date_block_threshold)\nall_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:02:25.165632Z","iopub.execute_input":"2021-08-16T16:02:25.166333Z","iopub.status.idle":"2021-08-16T16:02:45.400953Z","shell.execute_reply.started":"2021-08-16T16:02:25.16629Z","shell.execute_reply":"2021-08-16T16:02:45.399808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_context(\"talk\", font_scale=1.4)\nplt.title('Number of different shop-item combinations in data per month')\nall_data.groupby('date_block_num').size().plot();","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:03:02.488955Z","iopub.execute_input":"2021-08-16T16:03:02.48953Z","iopub.status.idle":"2021-08-16T16:03:02.874479Z","shell.execute_reply.started":"2021-08-16T16:03:02.489474Z","shell.execute_reply":"2021-08-16T16:03:02.87331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Advanced Feature Engineering\nmean / variance encoding for category-id last month","metadata":{}},{"cell_type":"code","source":"mean_enc_item_cat = pd.DataFrame(all_data.groupby(['shop_id', \n                                                    'item_category_id']).target.agg(['mean', 'var']).reset_index())\nmean_enc_item_cat.columns = ['shop_id', 'item_category_id', 'mean_enc_cat_id', 'var_enc_cat_id']\nall_data = pd.merge(all_data, mean_enc_item_cat, how='left', on=['shop_id', 'item_category_id'])\ndel mean_enc_item_cat\nall_data = downcast_dtypes(all_data)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:03:52.545816Z","iopub.execute_input":"2021-08-16T16:03:52.546256Z","iopub.status.idle":"2021-08-16T16:03:53.501635Z","shell.execute_reply.started":"2021-08-16T16:03:52.546213Z","shell.execute_reply":"2021-08-16T16:03:53.500576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_data = all_data[all_data.date_block_num==34].fillna(0)\nall_data = all_data[all_data.date_block_num<34].fillna(0)\nsub_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:04:03.009447Z","iopub.execute_input":"2021-08-16T16:04:03.009879Z","iopub.status.idle":"2021-08-16T16:04:03.35525Z","shell.execute_reply.started":"2021-08-16T16:04:03.009832Z","shell.execute_reply":"2021-08-16T16:04:03.354214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dates = all_data['date_block_num']\nboolean_test = (dates.isin([22,31,32,33])) # & (boolean)\nboolean_train = ~boolean_test\ndates_train = dates[boolean_train]\ndates_val  = dates[boolean_test]\n\nX_train = all_data.loc[boolean_train].drop(to_drop_cols, axis=1)\nX_val =  all_data.loc[boolean_test].drop(to_drop_cols, axis=1)\n\ny_train = all_data.loc[boolean_train, 'target'].values\ny_val =  all_data.loc[boolean_test, 'target'].values","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:04:06.665394Z","iopub.execute_input":"2021-08-16T16:04:06.665749Z","iopub.status.idle":"2021-08-16T16:04:06.959944Z","shell.execute_reply.started":"2021-08-16T16:04:06.665719Z","shell.execute_reply":"2021-08-16T16:04:06.95879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuples_validation_submission = pd.Series(list(all_data.loc[all_data['date_block_num']==33, ['item_id', 'shop_id']].itertuples(index=False, name=None)))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:04:20.14674Z","iopub.execute_input":"2021-08-16T16:04:20.147153Z","iopub.status.idle":"2021-08-16T16:04:20.376797Z","shell.execute_reply.started":"2021-08-16T16:04:20.147095Z","shell.execute_reply":"2021-08-16T16:04:20.37549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('X_train shape is ' + str(X_train.shape))\nprint('X_val shape is ' + str(X_val.shape))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:04:25.085533Z","iopub.execute_input":"2021-08-16T16:04:25.085913Z","iopub.status.idle":"2021-08-16T16:04:25.092931Z","shell.execute_reply.started":"2021-08-16T16:04:25.085882Z","shell.execute_reply":"2021-08-16T16:04:25.091297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuples_validation_submission = pd.Series(list(test[['item_id', 'shop_id']][dates_val==33].itertuples(index=False, name=None)))\ntuples_validation_submission = pd.Series(list(all_data.loc[all_data['date_block_num']==33, ['item_id', 'shop_id']].itertuples(index=False, name=None)))\nprint(f'The {round(tuples_test.isin(tuples_validation_submission).sum()/len(tuples_test),2)*100} % of the item_id-shop_id are in the cv set ')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:04:31.672104Z","iopub.execute_input":"2021-08-16T16:04:31.672516Z","iopub.status.idle":"2021-08-16T16:04:32.145497Z","shell.execute_reply.started":"2021-08-16T16:04:31.67248Z","shell.execute_reply":"2021-08-16T16:04:32.144408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling & Hyperparameter tuning\n- Light Gradient Boosting\n- RandomForestRegressor","metadata":{}},{"cell_type":"code","source":"learning_rates = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\nbest_rmse = 9999999999999\nfor lr in learning_rates:\n    lgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': lr, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }\n\n    lgb_model = lgb.train(lgb_params, lgb.Dataset(X_train, label=clip40(y_train)), int(100 * (lr / 0.03)))\n    pred_lgb_val = lgb_model.predict(X_val)\n    score = rmse(clip20(y_val), clip20(pred_lgb_val))\n\n    if score < best_rmse:\n        best_rmse = score\n        best_lr = lr\n        best_lgb = lgb_model","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:04:56.326818Z","iopub.execute_input":"2021-08-16T16:04:56.327226Z","iopub.status.idle":"2021-08-16T16:14:03.203533Z","shell.execute_reply.started":"2021-08-16T16:04:56.32719Z","shell.execute_reply":"2021-08-16T16:14:03.201695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_lr\n# 0.04","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:08:48.01537Z","iopub.execute_input":"2021-08-08T17:08:48.015819Z","iopub.status.idle":"2021-08-08T17:08:48.023336Z","shell.execute_reply.started":"2021-08-08T17:08:48.015782Z","shell.execute_reply":"2021-08-08T17:08:48.021992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X_train.append(X_val)\ny = np.append(y_train, y_val)\n\nbest_lgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.04, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }\nbest_lgb = lgb.train(lgb_params, lgb.Dataset(X, label=clip40(y)), int(100 * (lr / 0.03)))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:14:38.916939Z","iopub.execute_input":"2021-08-16T16:14:38.917328Z","iopub.status.idle":"2021-08-16T16:16:39.470874Z","shell.execute_reply.started":"2021-08-16T16:14:38.917295Z","shell.execute_reply":"2021-08-16T16:16:39.469694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'best_lgb_2.sav'\npickle.dump(best_lgb, open(filename, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:18:16.003277Z","iopub.execute_input":"2021-08-16T16:18:16.003811Z","iopub.status.idle":"2021-08-16T16:18:16.299017Z","shell.execute_reply.started":"2021-08-16T16:18:16.003774Z","shell.execute_reply":"2021-08-16T16:18:16.298237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest\nX = X_train.append(X_val)\nY = np.concatenate([y_train, y_val])\n\nregr = RandomForestRegressor(bootstrap=0.7, criterion='mse', max_depth=12,\n           max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           n_estimators=300, n_jobs=4,\n           verbose=0, warm_start=False)\nregr.fit(X, Y)\n\n# train_ind=np.zeros(X.shape[0])\n# for i in range(0, len(X_train)):\n#     train_ind[i] = -1\n# ps = PredefinedSplit(test_fold=(train_ind))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:20:24.177566Z","iopub.execute_input":"2021-08-16T16:20:24.178018Z","iopub.status.idle":"2021-08-16T16:38:52.932337Z","shell.execute_reply.started":"2021-08-16T16:20:24.177973Z","shell.execute_reply":"2021-08-16T16:38:52.931414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'best_rf_2.sav'\npickle.dump(regr, open(filename, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:42:14.086614Z","iopub.execute_input":"2021-08-16T16:42:14.087016Z","iopub.status.idle":"2021-08-16T16:42:14.164398Z","shell.execute_reply.started":"2021-08-16T16:42:14.086981Z","shell.execute_reply":"2021-08-16T16:42:14.163266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# param_grid={'bootstrap':[0.7, 0.8], 'max_features':[4, 6, 8], \n#             'max_depth' : [None, 4, 6, 8, 10, 12]}\n# gs = GridSearchCV(cv = None, \n#                   estimator = RandomForestRegressor(n_estimators=300, n_jobs=4), \n#                   param_grid=param_grid, scoring='neg_mean_squared_error')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gs.fit(X, clip40(Y))\n# best_rf = gs.best_estimator_\n# best_rf = gs.best_estimator_\n# best_rf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Prediction","metadata":{}},{"cell_type":"code","source":"rf = pickle.load(open('/kaggle/input/model2/best_rf_2.sav', 'rb'))\nlgb = pickle.load(open('/kaggle/input/model2/best_lgb_2.sav', 'rb'))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:43:32.657794Z","iopub.execute_input":"2021-08-16T16:43:32.658219Z","iopub.status.idle":"2021-08-16T16:43:33.467256Z","shell.execute_reply.started":"2021-08-16T16:43:32.658181Z","shell.execute_reply":"2021-08-16T16:43:33.466088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_lgb_val = lgb.predict(X_val)\nprint('Train RMSE for lgb is  %f' % rmse(clip20(y_train), clip20(lgb.predict(X_train))))\nprint('Val RMSE for lgb is %f' % rmse(clip20(y_val), clip20(pred_lgb_val)))\n# Train RMSE for lgb is  0.836673\n# Val RMSE for lgb is 0.803742","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.027241Z","iopub.execute_input":"2021-08-16T16:44:10.027669Z","iopub.status.idle":"2021-08-16T16:45:06.99098Z","shell.execute_reply.started":"2021-08-16T16:44:10.027628Z","shell.execute_reply":"2021-08-16T16:45:06.989719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importances = pd.Series(lgb.feature_importance(), index=X_val.columns)\nfeat_importances = feat_importances.nlargest(20)\nfeat_importances.plot(kind='barh')\nplt.title('Feature importance LGB')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:45:17.822988Z","iopub.execute_input":"2021-08-16T16:45:17.823718Z","iopub.status.idle":"2021-08-16T16:45:18.162868Z","shell.execute_reply.started":"2021-08-16T16:45:17.823671Z","shell.execute_reply":"2021-08-16T16:45:18.161627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_rf_val = clip20(rf.predict(X_val.fillna(0)))\nprint('Train RMSE for rf is %f' % rmse(clip20(y_train), clip20(rf.predict(X_train))))\nprint('Val RMSE for rf is %f' % rmse(clip20(y_val), pred_rf_val))\n\n# Train RMSE for rf is 0.990761\n# Val RMSE for rf is 0.936114","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:45:32.171337Z","iopub.execute_input":"2021-08-16T16:45:32.172063Z","iopub.status.idle":"2021-08-16T16:45:55.683339Z","shell.execute_reply.started":"2021-08-16T16:45:32.172012Z","shell.execute_reply":"2021-08-16T16:45:55.682064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importances = pd.Series(rf.feature_importances_, index=X_val.columns)\nfeat_importances = feat_importances.nlargest(20)\nfeat_importances.plot(kind='barh')\nplt.title('Feature importance RF')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:46:10.139317Z","iopub.execute_input":"2021-08-16T16:46:10.139694Z","iopub.status.idle":"2021-08-16T16:46:10.593361Z","shell.execute_reply.started":"2021-08-16T16:46:10.139661Z","shell.execute_reply":"2021-08-16T16:46:10.591973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(clip20(pred_rf_val), clip20(pred_lgb_val))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:46:38.42623Z","iopub.execute_input":"2021-08-16T16:46:38.426673Z","iopub.status.idle":"2021-08-16T16:46:40.921453Z","shell.execute_reply.started":"2021-08-16T16:46:38.426638Z","shell.execute_reply":"2021-08-16T16:46:40.920216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val_level2 = np.c_[pred_rf_val, pred_lgb_val]","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:46:49.524937Z","iopub.execute_input":"2021-08-16T16:46:49.525516Z","iopub.status.idle":"2021-08-16T16:46:49.533507Z","shell.execute_reply.started":"2021-08-16T16:46:49.525457Z","shell.execute_reply":"2021-08-16T16:46:49.532414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensembles\n### Stacking with linear regression\n- combine the validation prediction from random forest regressor and light gradient boost","metadata":{}},{"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_val_level2, clip20(y_val))\npred_lr_val =  clip20(lr.predict(X_val_level2))\nprint('Test rmse for stacking variables is %f' % rmse(clip20(y_val), pred_lr_val))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:54.930652Z","iopub.execute_input":"2021-08-16T16:49:54.931036Z","iopub.status.idle":"2021-08-16T16:49:55.017605Z","shell.execute_reply.started":"2021-08-16T16:49:54.931004Z","shell.execute_reply":"2021-08-16T16:49:55.016406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:57.939252Z","iopub.execute_input":"2021-08-16T16:49:57.939625Z","iopub.status.idle":"2021-08-16T16:49:58.015779Z","shell.execute_reply.started":"2021-08-16T16:49:57.939584Z","shell.execute_reply":"2021-08-16T16:49:58.014548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sam_sub = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')\nsam_sub.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:50:03.283199Z","iopub.execute_input":"2021-08-16T16:50:03.283617Z","iopub.status.idle":"2021-08-16T16:50:03.389349Z","shell.execute_reply.started":"2021-08-16T16:50:03.283578Z","shell.execute_reply":"2021-08-16T16:50:03.38816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_data.drop(to_drop_cols, axis=1).fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:50:06.068034Z","iopub.execute_input":"2021-08-16T16:50:06.068528Z","iopub.status.idle":"2021-08-16T16:50:06.115625Z","shell.execute_reply.started":"2021-08-16T16:50:06.068492Z","shell.execute_reply":"2021-08-16T16:50:06.114543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test_rf = rf.predict(sub_data.drop(to_drop_cols, axis=1).fillna(0))\npred_test_lgb = lgb.predict(sub_data.drop(to_drop_cols, axis=1).fillna(0))\nX_test_level2 = np.c_[clip20(pred_test_rf), clip20(pred_test_lgb)]\ntest_pred = clip20(lr.predict(X_test_level2))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:50:12.376788Z","iopub.execute_input":"2021-08-16T16:50:12.377192Z","iopub.status.idle":"2021-08-16T16:50:17.774943Z","shell.execute_reply.started":"2021-08-16T16:50:12.377156Z","shell.execute_reply":"2021-08-16T16:50:17.773755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions['shop_id'] = test.shop_id\npredictions['item_id'] = test.item_id\npredictions['item_cnt_month'] = test_pred\nsubmision = test[['ID', 'shop_id', 'item_id']].merge(predictions, on=['shop_id', 'item_id'], how='left').fillna(0)\nsubmision[['ID', 'item_cnt_month']].to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:50:31.897694Z","iopub.execute_input":"2021-08-16T16:50:31.898062Z","iopub.status.idle":"2021-08-16T16:50:32.758625Z","shell.execute_reply.started":"2021-08-16T16:50:31.89803Z","shell.execute_reply":"2021-08-16T16:50:32.757574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}