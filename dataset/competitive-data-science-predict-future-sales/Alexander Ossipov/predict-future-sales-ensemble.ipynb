{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predict Future Sales: feature engineering and ensemble approach"},{"metadata":{},"cell_type":"markdown","source":"**The goal of this notebook was to apply concepts and methods learned in the Coursera course \"How to Win a Data Science Competition: Learn from Top Kagglers**\". \n\nThe first part includes EDA and feature engineering similar to what you find in many other notebooks here. \n\nIn the second part I tried different ensemble approaches including simple convex mix and stacking applied for four different models: **CatBoost, Light GBM, Neural Network and XGBoost**. I tried also to slecet the best features. You will find the public scores that I received for most of my attempts in the notebook. I hope this can help you and save your time in your experiments. \n\nSurprisingly, the best score that I got was for the Light GBM alone, although some scores from the ensemble approaches were very close to it.  My conclusion is that apparently  Light GBM was fine tuned in the best way and one needs to improve the hyperparameter turning for other models in order to get an advantage from the ensemble approach.\n\n\n**Please upvote, if you find this notebook useful!**\n**Any feedback is very welcome!**\n"},{"metadata":{},"cell_type":"markdown","source":"## Instructions:\n\n* Please note that it may take a lot of time to run the whole notebook.\n\n* If you run it for the first time, you need first to generate the data for models. Run the notebook up to [this point](#stop_point) first.\n\n* Next you train specific models that you want. XGBoost is the most time consuming.\n\n* In order to get the best submission, you need to run only Light GBM, which is quite fast.\n\n* If you want to get a submission file for a specific model, you need to uncomment the corresponding cell.\n\n* If you want to run it many times then you, then you can save your time by using saved data and models. You might need to correct paths to saved files in your enviroment. "},{"metadata":{"_uuid":"55f0ed2ac468f0eb59c24726ca0c9bdc8884c345"},"cell_type":"markdown","source":"\n\n#### Exploratary Data Analysis\n* load data\n* trend of sales\n* distribution of target\n\n#### Data Cleaning & Feature Engineering\n* heal data and remove outliers\n* work with shops/items/cats objects and features\n* expand training set to include all item-shop pairs\n* clip item_cnt_month by (0,20)\n* append test to the matrix, fill 34 month nans with zeros\n* merge shops/items/cats dataframe to training set.\n* add group sale stats in recent months\n* add lag features\n* add trend features\n* add month and year\n* add months since last sale/months since first sale features\n* cut first year and drop columns which can not be calculated for the test set"},{"metadata":{"_uuid":"f03379ee467570732ebb2b3d20062fea0584d57d"},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0a1c729d4fb3d6609f9dfb163ebe92fa9dc654c","trusted":true},"cell_type":"code","source":"# load data\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\ncats = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\ntrain = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\n# set index to ID to avoid droping it later\ntest  = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv').set_index('ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4441f3aa0223a0c82dc0857668f2b66b51796617"},"cell_type":"markdown","source":"Test set is a product of some shops and some items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new compared to the train. "},{"metadata":{"_uuid":"bffdf025a7d56c073baae2be28dc020ea970c705","trusted":true},"cell_type":"code","source":"print('train size, item in train, shop in train', train.shape[0], train.item_id.nunique(), train.shop_id.nunique())\nprint('train size, item in train, shop in train', test.shape[0], test.item_id.nunique(),test.shop_id.nunique())\nprint('new items:', len(list(set(test.item_id) - set(test.item_id).intersection(set(train.item_id)))), len(list(set(test.item_id))), len(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check for missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56159823a46bb6dad909a3e1e86c1b7eafd3c54e"},"cell_type":"markdown","source":"There is no missing value in the training set.\n\nPlot the total sale of each month, we see a clear trend and seasonality. The overall sale is decreasing with time, and there are peaks in November."},{"metadata":{"_uuid":"8da35b420ce7058eeb91d44378317d233759436a","trusted":true},"cell_type":"code","source":"sale_by_month = train.groupby('date_block_num')['item_cnt_day'].sum()\nsale_by_month.plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eafc3711b504497b24f60e1fc1038b961e996ad4"},"cell_type":"markdown","source":"The distribution of sale grouped by month, item and shop, we see most item-shop pairs have small monthly sale."},{"metadata":{"_uuid":"a8bc02e1ca222b8ce670cf53e3697bc7cb3d9566","trusted":true},"cell_type":"code","source":"block_item_shop_sale = train.groupby(['date_block_num','item_id','shop_id'])['item_cnt_day'].sum()\nblock_item_shop_sale.clip(0,20).plot.hist(bins=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed7a190645750a818e29a6291ba2553a91764c7c"},"cell_type":"markdown","source":"\n# Feature engineering and data cleaning"},{"metadata":{"_uuid":"425d8f2dc08378977b393bf80c5fdcf0fba2c992"},"cell_type":"markdown","source":"#### remove outliers\nRemove outliers with very large item_cnt_day and item_price."},{"metadata":{"_uuid":"5a864412fafc3129a3e9bd5bb1f18a7cf0c62935","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)\n\ntrain = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1001]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2f99368478e3063b1c379537944e954d7186928"},"cell_type":"markdown","source":"There is one item with price below zero. Fill it with median."},{"metadata":{"_uuid":"0fc6b90b22fe232f4240ac8f965cc52b3db5526a","trusted":true},"cell_type":"code","source":"median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0, 'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7da194c285d696b5c6978148bf0143b9b2a7b0c5"},"cell_type":"markdown","source":"Several shops are duplicates of each other (according to its name). Fix train and test set."},{"metadata":{"_uuid":"00fe91e9c482ea413abd774ff903fe3d152785dd","trusted":true},"cell_type":"code","source":"# Якутск Орджоникидзе, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# Якутск ТЦ \"Центральный\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# Жуковский ул. Чкалова 39м²\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a30f0521464e1fa20444e66d24bbdcb76b93f6de"},"cell_type":"markdown","source":"#### Shops/Cats/Items preprocessing\nObservations:\n* Each shop_name starts with the city name.\n* Each category contains type and subtype in its name."},{"metadata":{"_uuid":"12fae4c8d0c8f3e817307d1e0ffc6831e9a8d696","trusted":true},"cell_type":"code","source":"shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\ncats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n# if subtype is nan then type\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]\n\nitems.drop(['item_name'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62c5f83fa222595da99294f465ab28e80ce415e9"},"cell_type":"markdown","source":"## Monthly sales\nMost of the items in the test set target value should be zero, while train set contains only pairs which were sold or returned in the past. So we expand the train set to include those item-shop pairs with zero monthly sales. This way train data will be similar to test data."},{"metadata":{"_uuid":"7626c7455ea71b65894c6c866519df15080fa2ac","trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"867e91a7570dd78b4834f4f1a166e58f80b63f93"},"cell_type":"markdown","source":"Aggregate train set by shop/item pairs to calculate target aggreagates, then <b>clip(0,20)</b> target value. This way train target will be similar to the test predictions.\n\nDowncast item_cnt_month to float32 -- float16 was too small to perform sum operation."},{"metadata":{"_uuid":"9fef5477060be7d2e6c85dcb79d8e18e6253f7dd","trusted":true},"cell_type":"code","source":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dd27181918fc7df89676e24d72130d183929d2d","trusted":true},"cell_type":"code","source":"\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float32))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"315bc6107a93f3926a64fd09ea9244e9281ee41f"},"cell_type":"markdown","source":"## Test set\nTo use time tricks append test pairs to the matrix."},{"metadata":{"_uuid":"29d02bdb4fa768577607bf735b918ca81da85d41","trusted":true},"cell_type":"code","source":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"177fbbab94c8057d67d61357d29581248468a74d","trusted":true},"cell_type":"code","source":"\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"233e394a6cebf36ef002dc76fef8d430026a52b3"},"cell_type":"markdown","source":"## Shops/Items/Cats features"},{"metadata":{"_uuid":"7dfd5df3e2bcaee4c312f3979736f52c40f2560f","trusted":true},"cell_type":"code","source":"\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8358b291fdc8e0e7d1b5700974803b3f104715f7"},"cell_type":"markdown","source":"## Traget lags"},{"metadata":{"_uuid":"9cd7bcc7643ce4545475e8e6f80d09a979aac42d","trusted":true},"cell_type":"code","source":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78bf7ece93ebc4629ad0e48cd6a9927788d8706d","trusted":true},"cell_type":"code","source":"\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9623850fd14c1a67cb8d68f536b32d974729032b"},"cell_type":"markdown","source":"## Group sale stats in recent\ncreate stats (mean/var) of sales of certain groups during the past 12 months"},{"metadata":{"_uuid":"46eb3502a9c5cc2dbd179613f09cc587d17128f5","trusted":true},"cell_type":"code","source":"def add_group_stats(matrix_, groupby_feats, target, enc_feat, last_periods):\n    if not 'date_block_num' in groupby_feats:\n        print ('date_block_num must in groupby_feats')\n        return matrix_\n    \n    group = matrix_.groupby(groupby_feats)[target].sum().reset_index()\n    max_lags = np.max(last_periods)\n    for i in range(1,max_lags+1):\n        shifted = group[groupby_feats+[target]].copy(deep=True)\n        shifted['date_block_num'] += i\n        shifted.rename({target:target+'_lag_'+str(i)},axis=1,inplace=True)\n        group = group.merge(shifted, on=groupby_feats, how='left')\n    group.fillna(0,inplace=True)\n    for period in last_periods:\n        lag_feats = [target+'_lag_'+str(lag) for lag in np.arange(1,period+1)]\n        # we do not use mean and svd directly because we want to include months with sales = 0\n        mean = group[lag_feats].sum(axis=1)/float(period)\n        mean2 = (group[lag_feats]**2).sum(axis=1)/float(period)\n        group[enc_feat+'_avg_sale_last_'+str(period)] = mean\n        group[enc_feat+'_std_sale_last_'+str(period)] = (mean2 - mean**2).apply(np.sqrt)\n        group[enc_feat+'_std_sale_last_'+str(period)].replace(np.inf,0,inplace=True)\n        # divide by mean, this scales the features for NN\n        group[enc_feat+'_avg_sale_last_'+str(period)] /= group[enc_feat+'_avg_sale_last_'+str(period)].mean()\n        group[enc_feat+'_std_sale_last_'+str(period)] /= group[enc_feat+'_std_sale_last_'+str(period)].mean()\n    cols = groupby_feats + [f_ for f_ in group.columns.values if f_.find('_sale_last_')>=0]\n    matrix = matrix_.merge(group[cols], on=groupby_feats, how='left')\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"041621b2e2e5d28f91bfc6ef58dea9b0a65a08d9","trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = add_group_stats(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'item', [6,12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'shop', [6,12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'category', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'city', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'type_code'], 'item_cnt_month', 'type', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'subtype_code'], 'item_cnt_month', 'subtype', [12])\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c67bf4dbcef884ffe9d19c65d37bc4de1f287ef6"},"cell_type":"markdown","source":"## Lag features"},{"metadata":{"_uuid":"79c8e250e42eaabddc4bc6c6569e70e1a23e2cfe","trusted":true},"cell_type":"code","source":"#first use target encoding each group, then shift month to creat lag features\ndef target_encoding(matrix_, groupby_feats, target, enc_feat, lags):\n    print ('target encoding for',groupby_feats)\n    group = matrix_.groupby(groupby_feats).agg({target:'mean'})\n    group.columns = [enc_feat]\n    group.reset_index(inplace=True)\n    matrix = matrix_.merge(group, on=groupby_feats, how='left')\n    matrix[enc_feat] = matrix[enc_feat].astype(np.float16)\n    matrix = lag_feature(matrix, lags, enc_feat)\n    matrix.drop(enc_feat, axis=1, inplace=True)\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56d7d2b826231476ce450e90a9c8d9e68ed471e5","trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = target_encoding(matrix, ['date_block_num'], 'item_cnt_month', 'date_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'date_item_avg_item_cnt', [1,2,3,6,12])\nmatrix = target_encoding(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'date_shop_avg_item_cnt', [1,2,3,6,12])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'date_cat_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'shop_id', 'item_category_id'], 'item_cnt_month', 'date_shop_cat_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'date_city_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_id', 'city_code'], 'item_cnt_month', 'date_item_city_avg_item_cnt', [1])\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bcea31d93ab035ca3fa1ed7c0afddbf602c414a"},"cell_type":"markdown","source":"## Trend features"},{"metadata":{"_uuid":"0504e9613087237c255914d9ebd165fac4e88cd0"},"cell_type":"markdown","source":"Price trend for the last six months."},{"metadata":{"_uuid":"0da2ded8502e273137991fd2bebbadaf19c19622","trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\n# https://stackoverflow.com/questions/31828240/first-non-null-value-per-row-from-a-list-of-pandas-columns/31828559\n# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n# Invalid dtype for backfill_2d [float16]\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17765ddb48f52abd88847a42c0a3ffe974e5b121"},"cell_type":"markdown","source":"Last month shop revenue trend"},{"metadata":{"_uuid":"e633be47f1a22b41487866ce67fb874bd296339e","trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47e06af411b7d26cd93dad3d6735e48e5fbdee50"},"cell_type":"markdown","source":"## Add month and year"},{"metadata":{"_uuid":"bb521e1f33d4124a3b90b47447bdb29150770b6e","trusted":true},"cell_type":"code","source":"matrix['month'] = matrix['date_block_num'] % 12\nmatrix['year'] = (matrix['date_block_num'] / 12).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add month since the last and first sale\nThe code has been simplified to reduce run time, though still may not be optimal -- ideally we don't need to compute max for each month."},{"metadata":{"_uuid":"3458a7056c963167760921417d1f863f074f2b39","trusted":true},"cell_type":"code","source":"#Month since last sale for each shop/item pair.\nts = time.time()\nlast_sale = pd.DataFrame()\nfor month in range(1,35):    \n    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby(['item_id','shop_id'])['date_block_num'].max()\n    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n                       'item_id': last_month.index.get_level_values(0).values,\n                       'shop_id': last_month.index.get_level_values(1).values,\n                       'item_shop_last_sale': last_month.values})\n    last_sale = last_sale.append(df)\nlast_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n\nmatrix = matrix.merge(last_sale, on=['date_block_num','item_id','shop_id'], how='left')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Month since last sale for each item.\nts = time.time()\nlast_sale = pd.DataFrame()\nfor month in range(1,35):    \n    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby('item_id')['date_block_num'].max()\n    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n                       'item_id': last_month.index.values,\n                       'item_last_sale': last_month.values})\n    last_sale = last_sale.append(df)\nlast_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n\nmatrix = matrix.merge(last_sale, on=['date_block_num','item_id'], how='left')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad0869709bbada35726d5ca41dd913d817249f8e","trusted":true},"cell_type":"code","source":"# Months since the first sale for each shop/item pair and for item only.\nts = time.time()\nmatrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final preparations\nBecause of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).\nLightgbm and XGBboost can deal with missing values, so we will leave the NaNs as it is. Later for neural network, we will fill na with 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = matrix[matrix.date_block_num > 11]\nmatrix.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.to_pickle('data.pkl')\ndel matrix\ndel group\ndel items\ndel shops\ndel cats\ndel train\n# leave test for submission\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read data from file"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_pickle('../input/predict-future-sales-ensemble/data.pkl')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[[\n    'date_block_num',\n    'shop_id',\n    #'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code','subtype_code',\n    'item_cnt_month_lag_1','item_cnt_month_lag_2','item_cnt_month_lag_3','item_cnt_month_lag_6','item_cnt_month_lag_12',\n    'item_avg_sale_last_6', 'item_std_sale_last_6',\n    'item_avg_sale_last_12', 'item_std_sale_last_12',\n    'shop_avg_sale_last_6', 'shop_std_sale_last_6',\n    'shop_avg_sale_last_12', 'shop_std_sale_last_12',\n    'category_avg_sale_last_12', 'category_std_sale_last_12',\n    'city_avg_sale_last_12', 'city_std_sale_last_12',\n    'type_avg_sale_last_12', 'type_std_sale_last_12',\n    'subtype_avg_sale_last_12', 'subtype_std_sale_last_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1','date_item_avg_item_cnt_lag_2','date_item_avg_item_cnt_lag_3','date_item_avg_item_cnt_lag_6','date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1','date_shop_avg_item_cnt_lag_2','date_shop_avg_item_cnt_lag_3','date_shop_avg_item_cnt_lag_6','date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month','year',\n    'item_shop_last_sale','item_last_sale',\n    'item_shop_first_sale','item_first_sale',\n]]\n\ncat_feats = ['shop_id','city_code','item_category_id','type_code','subtype_code']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Set up validation strategy**\n\nValidation strategy is 34 month for the test set, 33 month for the validation set and 13-32 months for the train."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\ndel data\ngc.collect();\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"stop_point\"></a> Run the notebook up to this point first.\n## Then chose the model(s) you want to train."},{"metadata":{},"cell_type":"markdown","source":"# Models"},{"metadata":{},"cell_type":"markdown","source":"## CatBoost"},{"metadata":{},"cell_type":"markdown","source":"### Public score: 0.94059"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\nmodel_catboost = CatBoostRegressor(verbose=False, random_seed=566)\n\n\nmodel_catboost.fit(X_train,y_train);\nprediction_catboost = model_catboost.predict(X_valid)\nprint(mean_squared_error(y_valid, prediction_catboost, squared=False))\n\nnp.save('prediction_catboost.npy', prediction_catboost)\npickle.dump(model_catboost, open('model_catboost.pkl', 'wb'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction = model_catboost.predict(X_test)\n# prediction_catboost = prediction\n\n# submission = pd.DataFrame({\n#     \"ID\": np.arange(prediction.shape[0]), \n#     \"item_cnt_month\": prediction.clip(0, 20)\n# })\n# submission.to_csv('submission_catboost.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Identifying the most important features"},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = pd.DataFrame({'feature':X_train.columns, 'importance': model_catboost.feature_importances_})\nimportance.sort_values('importance', ascending=False).set_index('feature').plot(kind='barh', figsize=(10,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance.sort_values('importance', ascending=False).feature.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train using the most important features"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\nmain_features = ['item_cnt_month_lag_1', 'item_first_sale',\n       'date_item_avg_item_cnt_lag_1', 'item_category_id']\n\n\n\nmodel_catboost = CatBoostRegressor(verbose=False, random_seed=566)\n\n\nmodel_catboost.fit(X_train[main_features],y_train);\nprediction_catboost = model_catboost.predict(X_valid[main_features])\nprint(mean_squared_error(y_valid, prediction_catboost, squared=False))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion: feature selection doesn't improve the error"},{"metadata":{},"cell_type":"markdown","source":"## LightGBM"},{"metadata":{},"cell_type":"markdown","source":"### Public score: 0.90429"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\n\n\n\nmodel_lgbm = LGBMRegressor(\n    max_depth = 8,\n    n_estimators = 500,\n    colsample_bytree=0.7,\n    min_child_weight = 300,\n    reg_alpha = 0.1,\n    reg_lambda = 1,\n    random_state = 42,\n)\n\nmodel_lgbm.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_valid, y_valid)], \n    verbose=10, \n    early_stopping_rounds = 40,\n    categorical_feature = cat_feats) \n\n\nprediction_lgbm = model_lgbm.predict(X_valid)\nprint(mean_squared_error(y_valid, prediction_lgbm, squared=False))\n\nnp.save('prediction_lgbm.npy', prediction_lgbm)\npickle.dump(model_lgbm, open('model_lgbm.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_lgbm = model_lgbm.predict(X_valid)\nprint(mean_squared_error(y_valid, prediction_lgbm, squared=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model_lgbm.predict(X_test)\nprediction_lgbm = prediction\n\nsubmission = pd.DataFrame({\n    \"ID\": np.arange(prediction.shape[0]), \n    \"item_cnt_month\": prediction.clip(0, 20)\n})\nsubmission.to_csv('submission_lgbm.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neural Network"},{"metadata":{},"cell_type":"markdown","source":"### Public score: 0.97956"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.regularizers import l2, l1\nfrom keras.optimizers import RMSprop, Adam\n#from tensorflow import set_random_seed\nimport tensorflow as tf    \n\nnp.random.seed(566)\n\n# define model\ndef Sales_prediction_model(input_shape):\n    in_layer = Input(input_shape)\n    x = Dense(16,kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(in_layer)\n    x = Dense(8, kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(x)\n    x = Dense(1, kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(x)\n    \n    model = Model(inputs = in_layer, outputs = x, name='Sales_prediction_model')\n    return model\n\n# NN cannot take missing values, fill NaN with 0.\nX_train.fillna(0,inplace=True)\nX_valid.fillna(0,inplace=True)\nX_test.fillna(0,inplace=True)\n\n# We do no feature scaling here. \n# Some features like 'item_avg_sale_last_6' are already scaled in feature engineering part.\n\ninput_shape = [X_train.shape[1]]\nmodel_nn = Sales_prediction_model(input_shape)\nmodel_nn.compile(optimizer = Adam(lr=0.0005) , loss = [\"mse\"], metrics=['mse'])\nmodel_nn.fit(X_train, y_train, validation_data = (X_valid, y_valid), batch_size = 10000, epochs=5)\n\n\nprediction_nn = model_nn.predict(X_valid)\nprint(mean_squared_error(y_valid, prediction_nn, squared=False))\n\nnp.save('prediction_nn.npy', prediction_nn)\n# pickle.dump(model_nn, open('model_nn.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction.clip(0, 20).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction = model_nn.predict(X_test)\n# prediction_nn = prediction\n\n# submission = pd.DataFrame({\n#     \"ID\": np.arange(prediction.shape[0]), \n#     \"item_cnt_month\": prediction.clip(0, 20).flatten()\n# })\n# submission.to_csv('submission_nn.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost"},{"metadata":{},"cell_type":"markdown","source":"## Public score: 0.91555"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom xgboost import XGBRegressor\n\nmodel_xgb = XGBRegressor(\n    max_depth=7,\n    n_estimators=1000,\n    min_child_weight=300,   \n    colsample_bytree=0.8, \n    subsample=0.8, \n    gamma = 0.005,\n    eta=0.1,    \n    seed=42)\n\nmodel_xgb.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_valid, y_valid)], \n    verbose=10, \n    early_stopping_rounds = 40,\n    )\n\n\nprediction_xgb = model_xgb.predict(X_valid)\nprint(mean_squared_error(y_valid, prediction_xgb, squared=False))\n\nnp.save('prediction_xgb.npy', prediction_xgb)\npickle.dump(model_xgb, open('model_xgb.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction = model_xgb.predict(X_test)\n# prediction_xgb = prediction\n\n# submission = pd.DataFrame({\n#     \"ID\": np.arange(prediction.shape[0]), \n#     \"item_cnt_month\": prediction.clip(0, 20)\n# })\n# submission.to_csv('submission_xgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble based on different models"},{"metadata":{},"cell_type":"markdown","source":"## Meta features"},{"metadata":{},"cell_type":"markdown","source":"#### Load models, if they were saved before"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Skip this part if the models haven't been saved before or correct paths in your enviroment\nmodel_catboost = pickle.load(open('../input/predict-future-sales-models/model_catboost.pkl','rb'))\nmodel_lgbm = pickle.load(open('../input/predict-future-sales-models/model_lgbm.pkl','rb'))\nmodel_xgb = pickle.load(open('../input/model-xgb-new/model_xgb.pkl','rb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodels = [model_lgbm, model_catboost, model_nn, model_xgb] # public score 0.90556\n\n# models = [model_lgbm, model_catboost] # public score 0.90728\n\nmeta_X_train = [model.predict(X_valid) for model in models]\n\nmeta_X_train[2] = meta_X_train[2].flatten()\n\nmeta_X_train = np.array(meta_X_train).T\n\nmeta_X_test = [model.predict(X_test) for model in models]\n\nmeta_X_test[2] = meta_X_test[2].flatten()\n\nmeta_X_test = np.array(meta_X_test).T\n\nmeta_y_train = y_valid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simple convex mix"},{"metadata":{},"cell_type":"markdown","source":"## Two models"},{"metadata":{},"cell_type":"markdown","source":"## Public score: 0.90462"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n# choose models \n# model_index = [0,1]  # lgbm and catboost, public score 0.90617\n# model_index = [0,2]  # lgbm and nn, public score \nmodel_index = [0,-1]  # lgbm and xgb, public score 0.90462\n\nalphas_to_try = np.linspace(0, 1, 1001)\nrmse_scores = np.empty_like(alphas_to_try)\n\nfor ind, alpha in enumerate(alphas_to_try):\n    simple_mix_pred = alpha * meta_X_train[:,model_index[0]] + (1 - alpha) * meta_X_train[:,model_index[1]]\n    rmse_scores[ind] = mean_squared_error(meta_y_train, simple_mix_pred, squared=False)\n\n\nbest_alpha = alphas_to_try[np.argmin(rmse_scores)]\nrmse_train_simple_mix = np.min(rmse_scores) \n\nprint('Best alpha: %f; Corresponding rmse score on train: %f' % (best_alpha, rmse_train_simple_mix))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Use the best $\\alpha$ to make predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# # remember that meta_X_test is a prediction for a model for X_test\n# meta_prediction = best_alpha * meta_X_test[:,model_index[0]] + (1 - best_alpha) * meta_X_test[:,model_index[1]]\n# submission = pd.DataFrame({\n#     \"ID\": np.arange(meta_prediction.shape[0]), \n#     \"item_cnt_month\": meta_prediction.clip(0, 20).flatten()\n# })\n# submission.to_csv('submission_meta_simple_mix_lgbm_xgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Three models"},{"metadata":{},"cell_type":"markdown","source":"## Public score: 0.90682"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.metrics import mean_squared_error\n\n\nalphas_to_try = np.linspace(0, 1, 51)\nbetas_to_try = np.linspace(0, 1, 51)\ngammas_to_try = np.linspace(0, 1, 51)\nrmse_scores = np.zeros((len(alphas_to_try), len(betas_to_try),len(gammas_to_try)))\n\nfor ind1, alpha in enumerate(alphas_to_try):\n    for ind2, beta in enumerate(betas_to_try):  \n        for ind3, gamma in enumerate(gammas_to_try):  \n            simple_mix_pred = alpha * meta_X_train[:,0] + beta * meta_X_train[:,1] +  gamma * meta_X_train[:,3]\n            rmse_scores[ind1, ind2, ind3] = mean_squared_error(meta_y_train, simple_mix_pred, squared=False)\n\n\nind1, ind2, ind3 = np.unravel_index(rmse_scores.argmin(), rmse_scores.shape)\nbest_alpha, best_beta, best_gamma = alphas_to_try[ind1], betas_to_try[ind2], gammas_to_try[ind3]\nrmse_train_simple_mix = np.min(rmse_scores) \n\nprint('Best alpha: %f; best beta: %f; best gamma: %f; Corresponding rmse score on train: %f' % (best_alpha, best_beta, best_gamma, rmse_train_simple_mix))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Use the best $\\alpha$, $\\beta$ and $\\gamma$ to make predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # remember that meta_X_test is a prediction for a model for X_test\n# meta_prediction = best_alpha * meta_X_test[:,0] + best_beta * meta_X_test[:,1] + best_gamma * meta_X_test[:,3]\n# submission = pd.DataFrame({\n#     \"ID\": np.arange(meta_prediction.shape[0]), \n#     \"item_cnt_month\": meta_prediction.clip(0, 20).flatten()\n# })\n# submission.to_csv('submission_meta_simple_mix_3_models.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking"},{"metadata":{},"cell_type":"markdown","source":"## Public score: meta_model LR 0.90443"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMRegressor\n\nmeta_model = LinearRegression()\n\n# meta_model =LGBMRegressor(\n#     max_depth = 2,\n#     n_estimators = 500,\n#     colsample_bytree=0.7,\n#     min_child_weight = 300,\n#     reg_alpha = 0.1,\n#     reg_lambda = 1,\n#     random_state = 42,\n# )\n\n\nmeta_model.fit(meta_X_train, meta_y_train)\n\nmeta_prediction = meta_model.predict(meta_X_test)\nprint(mean_squared_error(meta_y_train, meta_model.predict(meta_X_train), squared=False))\n\n# np.save('meta_prediction_lr_2.npy', meta_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission = pd.DataFrame({\n#     \"ID\": np.arange(meta_prediction.shape[0]), \n#     \"item_cnt_month\": meta_prediction.clip(0, 20).flatten()\n# })\n# submission.to_csv('submission_meta_lr_4_models.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}