{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\n%matplotlib inline\nimport itertools\nfrom time import sleep\n\nimport gc\nfrom pathlib2 import Path\nfrom tqdm import tqdm_notebook\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\nitem_categories = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\ntrain = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ntest  = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\nsample_submission=pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')\n\ngroupby_cols = ['date_block_num', 'shop_id', 'item_id']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[(train.shop_id == 32) & (train.item_id == 2973) & (train.date_block_num == 4) & (\n            train.item_price > 0)].item_price.median()\ntrain = train[train.item_price < 100000]\ntrain = train[train.item_cnt_day < 1001]\n\nmedian = train[(train.shop_id == 32) & (train.item_id == 2973) & (train.date_block_num == 4) & (\n            train.item_price > 0)].item_price.median()\ntrain.loc[train.item_price < 0, 'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['date_block_num'] = 34","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Adding new features**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"category = items[['item_id', 'item_category_id']].drop_duplicates()\ncategory.set_index(['item_id'], inplace=True)\ncategory = category.item_category_id\ntrain['category'] = train.item_id.map(category)\ncategory\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories['meta_category'] = item_categories.item_category_name.apply(lambda x: x.split(' ')[0])\nitem_categories['meta_category'] = pd.Categorical(item_categories.meta_category).codes\nitem_categories.set_index(['item_category_id'], inplace=True)\nmeta_category = item_categories.meta_category\ntrain['meta_category'] = train.category.map(meta_category)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops['city'] = shops.shop_name.apply(lambda x: str.replace(x, '!', '')).apply(lambda x: x.split(' ')[0])\nshops['city'] = pd.Categorical(shops['city']).codes\ncity = shops.city\ntrain['city'] = train.shop_id.map(city)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"year = pd.concat([train.date_block_num, train.date.apply(lambda x: int(x.split('.')[2]))], axis=1).drop_duplicates()\nyear.set_index(['date_block_num'], inplace=True)\nyear = year.date.append(pd.Series([2015], index=[34]))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month = pd.concat([train.date_block_num, train.date.apply(lambda x: int(x.split('.')[1]))], axis=1).drop_duplicates()\nmonth.set_index(['date_block_num'], inplace=True)\nmonth = month.date.append(pd.Series([11], index=[34]))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_shops_items = []\n\nfor block_num in train['date_block_num'].unique():\n    unique_shops = train[train['date_block_num'] == block_num]['shop_id'].unique()\n    unique_items = train[train['date_block_num'] == block_num]['item_id'].unique()\n    all_shops_items.append(np.array(list(itertools.product([block_num], unique_shops, unique_items)), dtype='int32'))\n\ndf = pd.DataFrame(np.vstack(all_shops_items), columns=groupby_cols, dtype='int32')\ndf = df.append(test, sort=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ID'] = df.ID.fillna(-1).astype('int32')\ndf['year'] = df.date_block_num.map(year)\ndf['month'] = df.date_block_num.map(month)\ndf['category'] = df.item_id.map(category)\ndf['meta_category'] = df.category.map(meta_category)\ndf['city'] = df.shop_id.map(city)\ntrain['category'] = train.item_id.map(category)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ngb = train.groupby(by=groupby_cols, as_index=False).agg({'item_cnt_day': ['sum']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_cnt_day_sum': 'target'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=groupby_cols)\n\ngb = train.groupby(by=['date_block_num', 'item_id'], as_index=False).agg({'item_cnt_day': ['sum']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_cnt_day_sum': 'target_item'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=['date_block_num', 'item_id'])\n\ngb = train.groupby(by=['date_block_num', 'shop_id'], as_index=False).agg({'item_cnt_day': ['sum']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_cnt_day_sum': 'target_shop'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=['date_block_num', 'shop_id'])\n\ngb = train.groupby(by=['date_block_num', 'category'], as_index=False).agg({'item_cnt_day': ['sum']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_cnt_day_sum': 'target_category'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=['date_block_num', 'category'])\n\ngb = train.groupby(by=['date_block_num', 'item_id'], as_index=False).agg({'item_price': ['mean', 'max']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_price_mean': 'target_price_mean', 'item_price_max': 'target_price_max'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=['date_block_num', 'item_id'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['target_price_mean'] = np.minimum(df['target_price_mean'], df['target_price_mean'].quantile(0.99))\ndf['target_price_max'] = np.minimum(df['target_price_max'], df['target_price_max'].quantile(0.99))\n\ndf.fillna(0, inplace=True)\ndf['target'] = df['target'].clip(0, 20)\ndf['target_zero'] = (df['target'] > 0).astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfor enc_cols in [['shop_id', 'category'], ['shop_id', 'item_id'], ['shop_id'], ['item_id']]:\n\n    col = '_'.join(['enc', *enc_cols])\n    col2 = '_'.join(['enc_max', *enc_cols])\n    df[col] = np.nan\n    df[col2] = np.nan\n\n    for d in tqdm_notebook(df.date_block_num.unique()):\n        f1 = df.date_block_num < d\n        f2 = df.date_block_num == d\n\n        gb = df.loc[f1].groupby(enc_cols)[['target']].mean().reset_index()\n        enc = df.loc[f2][enc_cols].merge(gb, on=enc_cols, how='left')[['target']].copy()\n        enc.set_index(df.loc[f2].index, inplace=True)\n        df.loc[f2, col] = enc['target']\n\n        gb = df.loc[f1].groupby(enc_cols)[['target']].max().reset_index()\n        enc = df.loc[f2][enc_cols].merge(gb, on=enc_cols, how='left')[['target']].copy()\n        enc.set_index(df.loc[f2].index, inplace=True)\n        df.loc[f2, col2] = enc['target']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef downcast_dtypes(df):\n    float32_cols = [c for c in df if df[c].dtype == 'float64']\n    int32_cols = [c for c in df if df[c].dtype in ['int64', 'int16', 'int8']]\n\n    df[float32_cols] = df[float32_cols].astype(np.float32)\n    df[int32_cols] = df[int32_cols].astype(np.int32)\n\n    return df\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.fillna(0, inplace=True)\ndf = downcast_dtypes(df)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n%%time\n\nshift_range = [1, 2, 3, 4, 5, 12]\n\nshifted_columns = [c for c in df if 'target' in c]\n\nfor shift in tqdm_notebook(shift_range):\n    shifted_data = df[groupby_cols + shifted_columns].copy()\n    shifted_data['date_block_num'] = shifted_data['date_block_num'] + shift\n\n    foo = lambda x: '{}_lag_{}'.format(x, shift) if x in shifted_columns else x\n    shifted_data = shifted_data.rename(columns=foo)\n\n    df = pd.merge(df, shifted_data, how='left', on=groupby_cols).fillna(0)\n    df = downcast_dtypes(df)\n\n    del shifted_data\n    gc.collect()\n    sleep(1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['target_trend_1_2'] = df['target_lag_1'] - df['target_lag_2']\ndf['target_predict_1_2'] = df['target_lag_1'] * 2 - df['target_lag_2']\n\ndf['target_trend_3_4'] = df['target_lag_1'] + df['target_lag_2'] - df['target_lag_3'] - df['target_lag_4']\ndf['target_predict_3_4'] = (df['target_lag_1'] + df['target_lag_2']) * 2 - df['target_lag_3'] - df['target_lag_4']\n\ndf['target_item_trend_1_2'] = df['target_item_lag_1'] - df['target_item_lag_2']\ndf['target_item_trend_3_4'] = df['target_item_lag_1'] + df['target_item_lag_2'] - df['target_item_lag_3'] - df['target_item_lag_4']\ndf['target_shop_trend_1_2'] = df['target_shop_lag_1'] - df['target_shop_lag_2']\ndf['target_shop_trend_3_4'] = df['target_shop_lag_1'] + df['target_shop_lag_2'] - df['target_shop_lag_3'] - df['target_shop_lag_4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = downcast_dtypes(df)\ndf.to_pickle('df.pkl')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}