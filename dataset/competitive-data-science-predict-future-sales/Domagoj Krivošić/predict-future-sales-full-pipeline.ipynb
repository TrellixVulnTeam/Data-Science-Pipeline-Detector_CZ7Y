{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":false},"cell_type":"code","source":"import os\nimport math\n\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"data_dir = '/kaggle/input'\nos.listdir(data_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\nitems = pd.read_csv(os.path.join(data_dir, 'items.csv'))\nshops = pd.read_csv(os.path.join(data_dir,'shops.csv'))\nsales = pd.read_csv(os.path.join(data_dir, 'sales_train.csv'))\ntest_data = pd.read_csv(os.path.join(data_dir, 'test.csv'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis"},{"metadata":{"trusted":false},"cell_type":"code","source":"items.columns, shops.columns, sales.columns, test_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales.shape, test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x = sales.groupby('date_block_num').agg({'item_cnt_day': 'sum'})\nplt.title('Total sales by date_block_num')\nplt.plot(x.index, x['item_cnt_day'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.title('Total sales by month')\n\nx_1 = x[x.index < 12]\nplt.plot(x_1.index + 1, x_1['item_cnt_day'], color='red', label='2013')\n\nx_2 = x[x.index >= 12]\nx_2 = x_2[x_2.index < 24]\nplt.plot(range(1, len(x_2.index) + 1), x_2['item_cnt_day'], color='green', label='2014')\n\nx_3 = x[x.index >= 24]\nplt.plot(range(1, len(x_3.index) + 1), x_3['item_cnt_day'], color='blue', label='2015')\n\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# There is only one item more expensive than 100000 (outlier).\nsales[sales['item_price'] > 100000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x_price = sales[sales['item_price'] < 40000] # drop outliers\nx_price = x_price.groupby('item_price').agg({'item_cnt_day': 'sum'})\nx_price = x_price[x_price['item_cnt_day'].values < 20000] # drop outliers\nplt.title('Items sold by price')\nplt.scatter(x_price.index, x_price['item_cnt_day'])\nplt.xlabel('Price')\nplt.ylabel('item_cnt_day')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"Remove first year of sales data"},{"metadata":{"trusted":false},"cell_type":"code","source":"sales = sales[sales['date_block_num'] > 11]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aggregate and sort the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales[sales['date_block_num']==block_num]['shop_id'].unique()\n    cur_items = sales[sales['date_block_num']==block_num]['item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n#turn the grid into pandas dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n#get aggregated values for (shop_id, item_id, month)\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n\n#fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n# #join aggregated data to the grid\nall_data = pd.merge(grid,gb,how='left',on=index_cols).fillna(0)\n\n#sort the data\nall_data.sort_values(['date_block_num','shop_id','item_id'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fix the duplicated shop id"},{"metadata":{"trusted":false},"cell_type":"code","source":"all_data.loc[all_data['shop_id'] == 0, 'shop_id'] = 57\nall_data.loc[all_data['shop_id'] == 1, 'shop_id'] = 58\nall_data.loc[all_data['shop_id'] == 11, 'shop_id'] = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate lag features"},{"metadata":{"trusted":false},"cell_type":"code","source":"lags = [1, 2, 3, 6, 12]\n\nfor lag in lags:\n    lag_col_name = 'target_lag_' + str(lag)\n    shifted = all_data[index_cols + ['target']].copy()\n    shifted.columns = index_cols + [lag_col_name]\n    shifted['date_block_num'] += lag\n    all_data = pd.merge(all_data, shifted, on=index_cols, how='left')\n    all_data[lag_col_name].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add item category id"},{"metadata":{"trusted":false},"cell_type":"code","source":"item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Expanding mean encoding sales by item id"},{"metadata":{"trusted":false},"cell_type":"code","source":"cumsum = all_data.groupby('item_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_id')['target'].cumcount()\nall_data['item_target_enc_exp'] = cumsum / cumcnt\n\ntarget_mean = all_data['target'].mean()\nall_data['item_target_enc_exp'].fillna(target_mean, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Expanding mean encoding sales by shop id"},{"metadata":{"trusted":false},"cell_type":"code","source":"cumsum = all_data.groupby('shop_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('shop_id')['target'].cumcount()\nall_data['shop_target_enc_exp'] = cumsum / cumcnt\n\ntarget_mean = all_data['target'].mean()\nall_data['shop_target_enc_exp'].fillna(target_mean, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Expanding mean encoding sales by item category id"},{"metadata":{"trusted":false},"cell_type":"code","source":"cumsum = all_data.groupby('item_category_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_category_id')['target'].cumcount()\nall_data['item_category_target_enc_exp'] = cumsum / cumcnt\n\ntarget_mean = all_data['target'].mean()\nall_data['item_category_target_enc_exp'].fillna(target_mean, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add last sale for shop_id, item_id pairs"},{"metadata":{"trusted":true},"cell_type":"code","source":"last_sale_df = []\nfor d in range(1, 35):\n    df = sales[sales.date_block_num < d].groupby(['shop_id', 'item_id'], as_index=False)['date_block_num'].max()\n    df['last_sale_ago'] = d - df.date_block_num\n    df.date_block_num = d\n    last_sale_df.append(df)\nlast_sale_df = pd.concat(last_sale_df)\n\nall_data = pd.merge(all_data, last_sale_df, on=['shop_id', 'item_id', 'date_block_num'], how='left')\nall_data['last_sale_ago'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add last shop sale"},{"metadata":{"trusted":true},"cell_type":"code","source":"last_shop_sale_df = []\nfor d in range(1, 35):\n    df = sales[sales.date_block_num < d].groupby('shop_id', as_index=False)['date_block_num'].max()\n    df['last_shop_sale_ago'] = d - df.date_block_num\n    df.date_block_num = d\n    last_shop_sale_df.append(df)\nlast_shop_sale_df = pd.concat(last_shop_sale_df)\n\nall_data = pd.merge(all_data, last_shop_sale_df, on=['shop_id', 'date_block_num'], how='left')\nall_data['last_shop_sale_ago'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add last item sale"},{"metadata":{"trusted":true},"cell_type":"code","source":"last_item_sale_df = []\nfor d in range(1, 35):\n    df = sales[sales.date_block_num < d].groupby('item_id', as_index=False)['date_block_num'].max()\n    df['last_item_sale_ago'] = d - df.date_block_num\n    df.date_block_num = d\n    last_item_sale_df.append(df)\nlast_item_sale_df = pd.concat(last_item_sale_df)\n\nall_data = pd.merge(all_data, last_item_sale_df, on=['item_id', 'date_block_num'], how='left')\nall_data['last_item_sale_ago'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encode the time as year and month"},{"metadata":{"trusted":false},"cell_type":"code","source":"all_data['year_index'] = all_data['date_block_num'] // 12\nall_data['month'] = all_data['date_block_num'] % 12 + 1\nall_data = all_data.drop(columns='date_block_num')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add number of days in a month"},{"metadata":{"trusted":false},"cell_type":"code","source":"days = pd.Series([0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]) # There is no 0 month\nall_data['days_in_month'] = all_data['month'].map(days)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One-hot encode year and month for linear models"},{"metadata":{"trusted":false},"cell_type":"code","source":"year_oh = pd.get_dummies(all_data['year_index'], prefix='year')\nmonth_oh = pd.get_dummies(all_data['month'], prefix='month')\nall_data_oh = all_data.drop(columns=['shop_id', 'item_id', 'item_category_id', 'year_index', 'month', 'target', 'last_sale_ago', 'last_shop_sale_ago', 'last_item_sale_ago'])\nall_data_oh = pd.concat([all_data_oh, year_oh, month_oh], axis=1)\nall_data_oh.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train/validation split by time"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_b_index = (all_data['year_index'] == 2) & (all_data['month'] == 9)\ntrain_c_index = (all_data['year_index'] == 2) & (all_data['month'] == 10)\ntrain_a_index = ~train_b_index & ~train_c_index\n\nX_train_a = all_data[train_a_index]\ny_train_a = X_train_a['target'].clip(0, 20)\nX_train_a = X_train_a.drop(columns='target')\n\nX_train_b = all_data[train_b_index]\ny_train_b = X_train_b['target'].clip(0, 20)\nX_train_b = X_train_b.drop(columns='target')\n\nX_train_c = all_data[train_c_index]\ny_train_c = X_train_c['target'].clip(0, 20)\nX_train_c = X_train_c.drop(columns='target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do the split for linear models dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_a_oh = all_data_oh[train_a_index]\nX_train_b_oh = all_data_oh[train_b_index]\nX_train_c_oh = all_data_oh[train_c_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the models on train_a"},{"metadata":{},"cell_type":"markdown","source":"Train XGBoost"},{"metadata":{"trusted":false},"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(max_depth=4, learning_rate=0.5, n_jobs=-1)\nmodel_xgb.fit(X_train_a, y_train_a)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train k-NN"},{"metadata":{"trusted":false},"cell_type":"code","source":"# model_knn = KNeighborsRegressor(n_neighbors=3, n_jobs=-1, leaf_size=500)\n# model_knn.fit(X_train_a_oh.values, y_train_a)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train MLP"},{"metadata":{"trusted":false},"cell_type":"code","source":"model_mlp = MLPRegressor(hidden_layer_sizes=(100, 100), activation='relu', learning_rate_init=0.01, max_iter=10, shuffle=False, verbose=True)\nmodel_mlp.fit(X_train_a_oh.values, y_train_a)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train random forest regressor"},{"metadata":{"trusted":false},"cell_type":"code","source":"model_rf = RandomForestRegressor(n_estimators=10, criterion='mse', max_depth=None, n_jobs=-1, verbose=1)\nmodel_rf.fit(X_train_a.values, y_train_a)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_1 = model_xgb.predict(X_train_b)\nrmse = math.sqrt(mean_squared_error(y_train_b, y_pred_1))\nR_score = r2_score(y_train_b, y_pred_1)\nprint('XGBoost rmse: ' + str(rmse) + ', R2: ' + str(R_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_2 = model_mlp.predict(X_train_b_oh)\nrmse = math.sqrt(mean_squared_error(y_train_b, y_pred_2))\nR_score = r2_score(y_train_b, y_pred_2)\nprint('MLP rmse: ' + str(rmse) + ', R2: ' + str(R_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_3 = model_rf.predict(X_train_b)\nrmse = math.sqrt(mean_squared_error(y_train_b, y_pred_3))\nR_score = r2_score(y_train_b, y_pred_3)\nprint('Random forest rmse: ' + str(rmse) + ', R2: ' + str(R_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# y_pred_4 = model_knn.predict(X_train_b_oh)\n# rmse = math.sqrt(mean_squared_error(y_train_b, y_pred_4))\n# R_score = r2_score(y_train_b, y_pred_4)\n# print('k-NN rmse: ' + str(rmse) + ', R2: ' + str(R_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Level 2 model"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_b_2 = np.stack([y_pred_1, y_pred_2, y_pred_3], axis=-1)\nmodel = LinearRegression(n_jobs=-1)\nmodel.fit(X_train_b_2, y_train_b)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate model on train_c"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_1 = model_xgb.predict(X_train_c)\ny_pred_2 = model_mlp.predict(X_train_c_oh)\ny_pred_3 = model_rf.predict(X_train_c)\n# y_pred_4 = model_knn.predict(X_train_c_oh)\nX_train_c_2 = np.stack([y_pred_1, y_pred_2, y_pred_3], axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stack_pred = model.predict(X_train_c_2)\nrmse = math.sqrt(mean_squared_error(y_train_c, stack_pred))\nR_score = r2_score(y_train_c, y_pred_1)\nprint('Ensemble rmse: ' + str(rmse) + ', R2: ' + str(R_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train level 2 model on train_b + train_c"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_bc_2 = np.concatenate([X_train_b_2, X_train_c_2], axis=0)\ny_train_bc = np.concatenate([y_train_b, y_train_c], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.fit(X_train_bc_2, y_train_bc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make predictions"},{"metadata":{},"cell_type":"markdown","source":"Add additional features to the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature extraction: fix the duplicated shop id\ntest_data.loc[test_data['shop_id'] == 0, 'shop_id'] = 57\ntest_data.loc[test_data['shop_id'] == 1, 'shop_id'] = 58\ntest_data.loc[test_data['shop_id'] == 11, 'shop_id'] = 10\n\n# Generate lag features\ntest_data['date_block_num'] = 34\nlags = [1, 2, 3, 6, 12]\nall_data['date_block_num'] = all_data['year_index'] * 12 + all_data['month'] - 1\n\nfor lag in lags:\n    lag_col_name = 'target_lag_' + str(lag)\n    shifted = all_data[index_cols + ['target']].copy()\n    shifted.columns = index_cols + [lag_col_name]\n    shifted['date_block_num'] += lag\n    test_data = pd.merge(test_data, shifted, on=index_cols, how='left')\n    test_data[lag_col_name].fillna(0, inplace=True)\n\n# Add item category\ntest_data = pd.merge(test_data, item_category_mapping, how='left', on='item_id')\n\n# Add expanding mean encoding for item_id\nitem_id_mean = all_data.groupby('item_id')['target'].mean()\ntest_data['item_target_enc_exp'] = test_data['item_id'].map(item_id_mean)\ntest_data['item_target_enc_exp'].fillna(target_mean, inplace=True)\n\n# Add expanding mean encoding for shop_id\nshop_id_mean = all_data.groupby('shop_id')['target'].mean()\ntest_data['shop_target_enc_exp'] = test_data['shop_id'].map(shop_id_mean)\ntest_data['shop_target_enc_exp'].fillna(target_mean, inplace=True)\n\n# Add expanding mean encoding for item_id\nitem_id_mean = all_data.groupby('item_category_id')['target'].mean()\ntest_data['item_category_target_enc_exp'] = test_data['item_category_id'].map(item_id_mean)\ntest_data['item_category_target_enc_exp'].fillna(target_mean, inplace=True)\n\n# Add last sale ago for shop_id, item_id pairs\ntest_data = pd.merge(test_data, last_sale_df, on=['shop_id', 'item_id', 'date_block_num'], how='left')\ntest_data['last_sale_ago'].fillna(0, inplace=True)\n\n# Add last shop sale\ntest_data = pd.merge(test_data, last_shop_sale_df, on=['shop_id', 'date_block_num'], how='left')\ntest_data['last_shop_sale_ago'].fillna(0, inplace=True)\n\n# Add last item sale\ntest_data = pd.merge(test_data, last_item_sale_df, on=['item_id', 'date_block_num'], how='left')\ntest_data['last_item_sale_ago'].fillna(0, inplace=True)\n\n# Add year index and month\ntest_data['year_index'] = 2\ntest_data['month'] = 11\n\n# Add days in month\ntest_data['days_in_month'] = 30\n\n# Drop id column\ntest_data.drop(columns='ID', inplace=True)\n\n# Drop date_block_num\ntest_data.drop(columns='date_block_num', inplace=True)\n\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create test dataset for linear models"},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data_oh = test_data.drop(columns=['shop_id', 'item_id', 'item_category_id', 'year_index', 'month', 'last_sale_ago', 'last_shop_sale_ago', 'last_item_sale_ago'])\nfor year in range(1,3):\n    test_data_oh['year_' + str(year)] = int(year == 2)\nfor month in range(1, 13):\n    test_data_oh['month_' + str(month)] = int(month == 11)\ntest_data_oh.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test_2 = np.stack([model_xgb.predict(test_data), model_mlp.predict(test_data_oh), model_rf.predict(test_data)], axis=-1)\npredictions = model.predict(X_test_2)\n\ndf_pred = pd.DataFrame({'item_cnt_month': predictions})\ndf_pred.to_csv('submission.csv', index_label='ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('submission.csv')\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}