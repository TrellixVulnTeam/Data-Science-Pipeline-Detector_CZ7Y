{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Predict Future Sales**\n\nFinal project for \"How to win a data science competition\" Coursera course\nThis challenge serves as final project for the \"How to win a data science competition\" Coursera course.\n\nIn this competition you will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.\n\nWe are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.\n\nWe have the following 6 data sets for this project\n\n* test\n* train\n* shops\n* items[](http://)\n* item categorty\n* submission"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import libraries \nimport pandas as pd\nimport numpy as np\nimport warnings \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nwarnings.filterwarnings(\"ignore\")\nfrom string import ascii_letters\nimport plotly.express as px\nimport sys\nimport gc\nimport pickle\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_train = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ndf_test = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\ndf_submission = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')\ndf_shop = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\ndf_item_cat = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\ndf_item = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# View head of df_train data set\nprint(df_train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View the shape and size of the train data set\nprint(\"Shape of the data Set is :  \",df_train.shape)\nprint(\"Size of the data Set is  :  \",df_train.size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verify the result, you can see the Dtype of 'date' is changed to datetime64\nprint(df_train.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['date'] = pd.to_datetime(df_train['date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verify the result, you can see the Dtype of 'date' is changed to datetime64\nprint(df_train.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is a Graph to show the Item ID sold by month \nplt.figure(figsize = (18,10))\nplt.title('Items Sold Per Month', fontsize=22)\nax = sns.barplot(x='date_block_num', y=\"item_id\", data=df_train, estimator=np.sum)\nax = ax.set(xlabel='Date Blocks', ylabel='Item ID')\nplt.xticks(rotation=45)\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is a Graph to show the Item ID sold by Shop Id \nplt.figure(figsize = (18,10))\nplt.title('Items Sold Shop ID')\nax = sns.barplot(x='shop_id', y=\"item_id\", data=df_train, estimator=np.mean)\nax = ax.set(xlabel='Shop Id', ylabel='Item ID')\nplt.xticks(rotation=45)\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#View the headers of the data set \ndf_shop.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_shop['shop_name']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find out the unique count of shop id \ndf_shop['shop_id'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I have learnt this from one of the notebooks there are dupicates and this needs correction \ndf_shop.loc[df_shop['shop_id']==0, 'shop_id'] = 57\ndf_shop.loc[df_shop['shop_id']==1, 'shop_id'] = 58\ndf_shop.loc[df_shop['shop_id']==10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate_shop_names = {\n    df_shop.loc[df_shop['shop_id']==57, 'shop_name'].values[0]:df_shop.loc[df_shop['shop_id']==57, 'shop_name'].values[1],\n    df_shop.loc[df_shop['shop_id']==58, 'shop_name'].values[0]:df_shop.loc[df_shop['shop_id']==58, 'shop_name'].values[1],\n    df_shop.loc[df_shop['shop_id']==11, 'shop_name'].values[0]:df_shop.loc[df_shop['shop_id']==11, 'shop_name'].values[1] \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_shop.head(100)\ndf_shop = df_shop.drop_duplicates(subset='shop_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Count of unique shop id after data cleansing \ndf_shop['shop_id'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_shop['city'] = df_shop['shop_name'].str.split(' ').apply(lambda x: x[0])\n# We will concat one of rows which has some white spaces names in shop name \ndf_shop.loc[df_shop.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n# We will remove special characters in the city \ndf_shop.loc[df_shop['city'] == '!Якутск', 'city'] = 'Якутск'\n# View first 2 rows of the data set\ndf_shop.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is a Graph to show the Item ID sold by Shop Id \nplt.figure(figsize = (18,10))\nplt.title('# of stores in the city', fontsize=24)\nax = sns.barplot(x='city', y=\"shop_id\", data=df_shop, estimator=np.sum)\nax = ax.set(xlabel='Stores', ylabel='City')\nplt.xticks(rotation=45)\n\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View # of unique shops in the data set\nprint(\"Number of unique shops: {}\".format(len(df_shop['shop_id'].unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the unique shop ID in train data set\ndf_train['shop_id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shop id have to be corrected in train data set as well. \n\n## Shopnames for 0 and 57 are the same so changing shop_id 0 to 57\ndf_train.loc[df_train['shop_id']==0, 'shop_id'] = 57\n\n## Shopnames for 1 and 58 are the same so changing shop_id 1 to 58\ndf_train.loc[df_train['shop_id']==1, 'shop_id'] = 58\n\n## Shopnames for 10 and 11 are the same so changing shop_id 00 to 11\ndf_train.loc[df_train['shop_id']==10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verify the unique shop ID in train data set\ndf_train['shop_id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view first 2 rows of the dataset\ndf_item.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the unique number of item names in the dataset\ndf_item['item_name'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View the first 2 records of item category data set \ndf_item_cat.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new column by name category and split the same by, instead of -\ndf_item_cat['category']=df_item_cat['item_category_name'].str.split('-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new column by name type and split the first part of the category\ndf_item_cat['type']=df_item_cat['category'].apply(lambda x: x[0].strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new column by name sub type and split the second part of the category \ndf_item_cat['sub_type'] = df_item_cat['category'].apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now our data set looks good and we will remove 2 columns\ndf_item_cat.drop(['item_category_name','category'], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the first 4 records of the data set\ndf_item_cat.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets find out now the count of unique type and sub type \nprint(\"    Unique Types: \",df_item_cat['type'].nunique())\nprint(\"Unique Sub Types: \",df_item_cat['sub_type'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is a Graph to show the Item Category ID and the item type name \n\nplt.figure(figsize = (18,10))\nplt.title('# of types of items category wise', fontsize=24)\nax = sns.barplot(x='item_category_id', y='type', data=df_item_cat, estimator=np.sum)\nax = ax.set(xlabel='Category ID', ylabel='Type')\nplt.xticks(rotation=45)\n\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new data frame that will merge data from all the above data sets\n# we will call this df as ,master data frame \n# we will merge Train and item on item item_id\nmaster_data = pd.merge(df_train, df_item, how='left',on='item_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will merge the newly create master data with item category on item category id \nmaster_data = pd.merge(master_data, df_item_cat, how='left', on='item_category_id')\n\n# We will further merge the data with Shop data set on shop id \nmaster_data = pd.merge(master_data, df_shop, how='left',on='shop_id')\n\n# lets look at the size and shape of newly created data frame master data\nprint(\" Shape: \", master_data.shape)\nprint(\"  Size: \", master_data.size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping data by month, shop id and Item ID\nmonth = master_data.groupby(['date_block_num','shop_id','item_id']).agg({'item_price':'mean','item_cnt_day':'sum'}).reset_index()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining the data frames\n\n# Merge data with item \nmonth = pd.merge(month,df_item,how='left',on='item_id')\n\n# Merge data with item categories \nmonth = pd.merge(month, df_item_cat,how='left',on='item_category_id')\n\n# merge data with shops\n\nmonth = pd.merge(month, df_shop, how='left',on='shop_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets look at the size and shape of month data \nprint(\" Shape: \", month.shape)\nprint(\"  Size: \", month.size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new colums called total sales \nmonth['tota_sales'] = month['item_price']*month['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot a line graph to show the monthly sales \nplt.figure(figsize = (18,10))\nplt.title('# Monthly sales', fontsize=24)\nax = sns.lineplot(x='date_block_num', y='tota_sales', data=month, estimator=np.sum)\nax = ax.set(xlabel='Date Block', ylabel='Total Sales')\nplt.xticks(rotation=45)\n\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a column called month\nmonth['month']= month['date_block_num'].apply(lambda month: (month+1)%12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dummifying categorical columns \n\n# First let us work on shop \nmonth = pd.concat([month, pd.get_dummies(month['shop_id'],drop_first=True, prefix='shop_')], axis=1)\n\n# Let us work on type \n\nmonth = pd.concat([month, pd.get_dummies(month['type'],drop_first=True, prefix='type')], axis=1)\n\n# Finally work on sub type \n\nmonth = pd.concat([month, pd.get_dummies(month['sub_type'],drop_first=True, prefix='sub_type')], axis=1)\n\n# lets look at the size and shape of month data \nprint(\" Shape: \", month.shape)\nprint(\"  Size: \", month.size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the names of feature columns for shop, type and sub_type \n\n# Defining shop column \n\nshop_col = [col for col in month.columns if 'shop__' in col]\n\n# Defining type column \n\ntype_col = [col for col in month.columns if 'type_' in col]\n\n# Defining sub_type column \n\nsub_type_col = [ col for col in month.columns if 'sub_type_' in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining features and target \nfeatures = ['month', 'shop_id','item_id', 'item_price'] + type_col + sub_type_col\ntarget = ['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import for splitting data \nfrom sklearn.model_selection import train_test_split\n\n# Setup for feature and target variable\nX_feature = month[features].fillna(value=0)\nY_target = month[target].fillna(value=0)\n\n# Split and train and test \nX_train, X_test, Y_train, Y_test = train_test_split(X_feature, Y_target, test_size=0.3, random_state=0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Linear Regression Model \n\n# Get lr function by importing from sklearn \nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\n# Fit the lr model on the training data \n\nlr.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the score on the data set\nlr.score(X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add the date_block_num and month column \ndf_test['date_block_num'] = 34\ndf_test['month'] = 11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_price = month[['item_id', 'item_price']].groupby('item_id')['item_price'].mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the test data frame \n\n# Merge with item price \ndf_test = pd.merge(df_test, item_price,how='left',on='item_id')\n\n# Merge with items \ndf_test = pd.merge(df_test, df_item ,how='left',on='item_id')\n\n# Merge with item categories  \ndf_test = pd.merge(df_test, df_item_cat ,how='left',on='item_category_id')\n\n# Merge with shop \ndf_test = pd.merge(df_test, df_shop ,how='left',on='shop_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dummifying the categorical columns to get the test on shop, type and sub_type \n\n# Creating dummies for shop \ndf_test = pd.concat([df_test, pd.get_dummies(df_test['shop_id'], drop_first=True, prefix='shop_')], axis=1)\n\n## Creating dummies for type\ndf_test = pd.concat([df_test, pd.get_dummies(df_test['type'], drop_first=True, prefix='type')], axis=1)\n\n## Creating dummies for sub_type\ndf_test = pd.concat([df_test, pd.get_dummies(df_test['sub_type'], drop_first=True, prefix='sub_type')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the names of the feature columns\n\n# Collecting shop feature names\ndf_test_shop_col = [col for col in df_test.columns if 'shop__' in col]\n\n# Collecting type feature names\ndf_test_type_col = [col for col in df_test.columns if 'type_' in col]\n\n# Collecting sub_type feature names\ndf_test_sub_type_col = [col for col in df_test.columns if 'sub_type_' in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating features for test data \ndf_test_features = ['month', 'item_id' + 'shop_id', 'item_price'] + df_test_type_col + df_test_sub_type_col\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting common features \ncommon_features = list(set(features) & set(df_test_features)) \nprint(f\"Number of common features: {len(common_features)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# verify if the item_price is available in common features \n'item_price' in common_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing data for modeling\n\n# Setting feature and target variables\nX = month[common_features].fillna(value=0)\ny = month[target].fillna(value=0)\n\n## Splitting train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Getting LR function\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\n## Fitting on training data\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# \ndf_test = df_test.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict\nlr.predict(df_test[common_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['preds'] = lr.predict(df_test[common_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a file for submission \npreds = df_test[['ID', 'preds']]\npreds.columns = ['ID', 'item_cnt_month']\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving the submission dataframe\npreds.to_csv('my_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Coming soon will work on the Second model using XG Boost\n\n#from xgboost import XGBRegressor\n#xg = XGBRegressor()\n\n## Fitting on training data\n#xg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.utils import resample\nfrom sklearn.pipeline import Pipeline\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Flatten, Dropout\nfrom tensorflow.keras.layers import LeakyReLU, PReLU, ELU\nfrom keras.utils import np_utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree.predict(df_test[common_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['predsdt'] = decision_tree.predict(df_test[common_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predsdt = df_test[['ID', 'preds']]\npredsdt.columns = ['ID', 'item_cnt_month']\npredsdt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predsdt.to_csv('my_submission1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFR = RandomForestRegressor(n_estimators = 100)\nRFR.fit(X_train,Y_train)\n\nprint('Train set mse:', mean_squared_error(Y_train, RFR.predict(X_train)))\nprint('Test set mse:', mean_squared_error(Y_test, RFR.predict(X_test)))\nprint('Test set score:', RFR.score(X_train,Y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = RFR.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = list(map(round, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')\nprint(df_submission.shape)\ndf_submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}