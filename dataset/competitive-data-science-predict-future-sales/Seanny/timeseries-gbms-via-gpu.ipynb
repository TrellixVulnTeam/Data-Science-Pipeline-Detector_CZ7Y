{"cells":[{"metadata":{"_uuid":"b69932efb440af8f6435f3cd802fbcd15682af71","collapsed":true},"cell_type":"markdown","source":"# timeseries ml via gpu\n\ngpu guides:   \nhttps://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89004  \nhttps://www.kaggle.com/raimonds1993/gbms-cpu-vs-gpu-400-features-augmentation/"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg' \nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nimport matplotlib.pyplot as plt\n\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import plot_importance\n\nimport time, sys, gc, pickle\n\n#from itertools import product\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a54364495b1818e9f069efa0c53500bf9e21d5f9","trusted":false,"collapsed":true},"cell_type":"code","source":"start = time.time()\ndata = pd.read_pickle('../input/feature-engineering-xgboost/data.pkl')\nprint('data input costs {:.2f} seconds'.format(time.time()-start))\nprint('data has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\ntest  = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv').set_index('ID')\nprint('test has {} rows and {} columns'.format(test.shape[0], test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfc928a916bb8b285b2fe90fb1a311cf2fbbf2e3","trusted":false,"collapsed":true},"cell_type":"code","source":"data = data[[\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code',\n    'subtype_code',\n    'item_cnt_month_lag_1',\n    'item_cnt_month_lag_2',\n    'item_cnt_month_lag_3',\n    'item_cnt_month_lag_6',\n    'item_cnt_month_lag_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_2',\n    'date_item_avg_item_cnt_lag_3',\n    'date_item_avg_item_cnt_lag_6',\n    'date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1',\n    'date_shop_avg_item_cnt_lag_2',\n    'date_shop_avg_item_cnt_lag_3',\n    'date_shop_avg_item_cnt_lag_6',\n    'date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    #'date_shop_type_avg_item_cnt_lag_1',\n    #'date_shop_subtype_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    #'date_type_avg_item_cnt_lag_1',\n    #'date_subtype_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month',\n    'days',\n    'item_shop_last_sale',\n    'item_last_sale',\n    'item_shop_first_sale',\n    'item_first_sale',\n]]\n\n#data = data.sort_values('date_block_num')\nx_train = data[data.date_block_num <= 33].drop(['item_cnt_month'], axis=1)\ny_train = data[data.date_block_num <= 33]['item_cnt_month']\n#X_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\n#Y_valid = data[data.date_block_num == 33]['item_cnt_month']\nx_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\ndel data\ngc.collect();\n\ndef run_cross_validate(x_train, y_train, test, N_FOLDS=5):\n    model = XGBRegressor(\n        objective='reg:squarederror',\n        learning_rate=0.05, \n        max_depth=8,\n        n_estimators=1000,\n        min_child_weight=200, \n        colsample_bytree=0.8, \n        subsample=0.8, \n        seed=42,\n        tree_method='gpu_hist' # turn GPU on!!!\n    )\n    tspl = TimeSeriesSplit(n_splits=N_FOLDS)\n    oof_preds = np.zeros(len(x_train))                      # 交叉验证预测结果\n    test_preds = np.zeros(len(test))                        # 测试集预测结果\n    oof_losses = []\n    print('gpu training begins ...')\n    for fold, (trn_idx, val_idx) in enumerate(tspl.split(x_train, y_train)):\n        print('Starting fold: ', fold + 1)\n        x_trn, x_val = x_train.iloc[trn_idx], x_train.iloc[val_idx]\n        y_trn, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n\n        model.fit(x_trn, y_trn, \n                    eval_metric='rmse', \n                    eval_set=[(x_trn, y_trn),(x_val, y_val)], \n                    verbose=10, \n                    early_stopping_rounds = 10)             # set early stopping\n        val_preds = model.predict(x_val) \n        oof_preds[val_idx] = val_preds\n        loss = np.sqrt(mean_squared_error(y_val, val_preds))\n        print('fold {} RMSE is {:.5f}'.format(fold + 1, loss))\n        oof_losses.append(loss)\n        preds = model.predict(test) \n        test_preds += preds / N_FOLDS\n        print('-' * 50)\n        print('\\n')\n    print('Mean OOF RMSE across folds is {:.5f}'.format(np.mean(oof_losses))) # 每一折的验证分数平均\n    print('GPU Xgb costs {:.2f} seconds'.format(time.time()-start))\n    return test_preds, oof_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds, oof_preds = run_cross_validate(x_train, y_train, x_test, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test = test_preds.clip(0, 20)\nsubmission = pd.DataFrame({ 'ID': test.index, 'item_cnt_month': Y_test})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_cross_validate(x_train, y_train, test, N_FOLDS=5):\n    model = CatBoostRegressor(eval_metric='RMSE',\n                          iterations=1000,\n                          max_ctr_complexity=4,\n                          random_seed=42,\n                          od_type='Iter',\n                          od_wait=100,\n                          verbose=50,\n                          depth=8,\n                          metric_period = 50,\n                          task_type='GPU'\n                            )\n    tspl = TimeSeriesSplit(n_splits=N_FOLDS)\n    oof_preds = np.zeros(len(x_train))                      # 交叉验证预测结果\n    test_preds = np.zeros(len(test))                        # 测试集预测结果\n    oof_losses = []\n    print('gpu training begins ...')\n    for fold, (trn_idx, val_idx) in enumerate(tspl.split(x_train, y_train)):\n        print('Starting fold: ', fold + 1)\n        x_trn, x_val = x_train.iloc[trn_idx], x_train.iloc[val_idx]\n        y_trn, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n        y_trn = np.array(y_trn).astype('float32')\n        y_val = np.array(y_val).astype('float32')\n        model.fit(x_trn, y_trn,\n                    eval_set=[(x_val, y_val)],  # Multiple eval sets are not supported on GPU\n                    verbose=True,\n                    use_best_model=True)            \n        val_preds = model.predict(x_val) \n        oof_preds[val_idx] = val_preds\n        loss = np.sqrt(mean_squared_error(y_val, val_preds))\n        print('fold {} RMSE is {:.5f}'.format(fold + 1, loss))\n        oof_losses.append(loss)\n        preds = model.predict(test) \n        test_preds += preds / N_FOLDS\n        print('-' * 50)\n        print('\\n')\n    print('Mean OOF RMSE across folds is {:.5f}'.format(np.mean(oof_losses))) # 每一折的验证分数平均\n    print('GPU Xgb costs {:.2f} seconds'.format(time.time()-start))\n    return test_preds, oof_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds, oof_preds = run_cross_validate(x_train, y_train, x_test, 3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acef75c36501f808d45f81fc69f9708fc3283bc3","trusted":false,"collapsed":true},"cell_type":"code","source":"'''\nstart = time.time()\nprint('cpu training begins ...')\nmodel = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42\n)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric='rmse', \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\nprint('CPU Xgb costs {:.2f} seconds'.format(time.time()-start))\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to use GPU version of Xgboost, remember to turn the 'Always use latest environment' option on!"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nstart = time.time()\nprint('gpu training begins ...')\nmodel = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42,\n    tree_method='gpu_hist' # turn GPU on!!!\n)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric='rmse', \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=10, \n    early_stopping_rounds = 10)\n\nprint('GPU Xgb costs {:.2f} seconds'.format(time.time()-start))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplt.rcParams['figure.facecolor'] = 'white'\nplot_features(model, (7.5,10))\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[TypeError: No matching signature found](https://github.com/catboost/catboost/issues/1233)"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nprint('learning rate:',model.learning_rate_)\nY_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({ 'ID': test.index, 'item_cnt_month': Y_test})\nsubmission.to_csv('submission.csv', index=False)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nstart = time.time()\nprint('gpu training begins ...')\nmodel = CatBoostRegressor(eval_metric='RMSE',\n                          iterations=1000,\n                          max_ctr_complexity=4,\n                          random_seed=42,\n                          od_type='Iter',\n                          od_wait=100,\n                          verbose=50,\n                          depth=8,\n                          metric_period = 50,\n                          task_type='GPU'\n)\nY_train = np.array(Y_train).astype('float32')\nY_valid = np.array(Y_valid).astype('float32')\nmodel.fit(X_train, \n          Y_train,\n          eval_set=[(X_valid, Y_valid)], \n          verbose=True, \n          use_best_model=True)\nprint('GPU Cat costs {:.2f} seconds'.format(time.time()-start))    \n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ff5a80a22d046c5ca1cb27e938c757b607551d2","trusted":false,"collapsed":true},"cell_type":"code","source":"# save predictions for an ensemble\n# pickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\n# pickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8adc7c93323eb77baeceb2e8db17390b5c4deb3","trusted":false,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}