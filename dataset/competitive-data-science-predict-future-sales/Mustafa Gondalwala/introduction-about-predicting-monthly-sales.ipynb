{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Introduction**\n\nOur mission in this project is to find a mapping between **a(t)** and **a(t - k)**, where ***a*** is the amount of selling each item in each shop and ***t*** is time (months). In other words we want to find a series A where n = A(n-1). \n\n**Table of contents**\n- Loading and downcasting data\n- Exploratory data analysis\n    * Pair plot\n    * Outliers removal\n    * Duplicated sales removal\n    * Sales history\n    * Test set distribution\n- Cleaning data\n    * Shops names preprocessing\n    * Duplicated shops removal\n    * Items names preprocessing\n- Feature extraction\n    * [Lag features](https://machinelearningmastery.com/basic-feature-engineering-time-series-data-python/)\n    * Date features\n    * Feature interactions\n- Modeling\n    * X_train, X_val splitting\n    * Training LightGBM model\n    * Evaluate model"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport datetime\nimport gc\nfrom itertools import product\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nfrom statsmodels.tsa.stattools import acf\ndata_path = '/kaggle/input/competitive-data-science-predict-future-sales/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    start_size = df.memory_usage(deep = True).sum() / 1024**2\n    print('Memory usage: {:.2f} MB'.format(start_size))\n\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int32)\n    end_size = df.memory_usage(deep = True).sum() / 1024**2\n    print('New Memory usage: {:.2f} MB'.format(end_size))\n    return df\n\ndef create_record_for_features(df, attrs, target, time_col, aggfunc = np.sum, fill = 0):\n    target_for_attrs = df.pivot_table(index = attrs,\n                                   values = target, \n                                   columns = time_col, \n                                   aggfunc = aggfunc, \n                                   fill_value = fill,\n                                  ).reset_index()\n    target_for_attrs.columns = target_for_attrs.columns.map(str)\n    target_for_attrs = target_for_attrs.reset_index(drop = True).rename_axis(None, axis = 1)\n    return target_for_attrs\n\ndef display_df_info(df, name):\n    print('-----------Shape of '+ name + '-------------')\n    print(df.shape)\n    print('-----------Missing values---------')\n    print(df.isnull().sum())\n    print('-----------Null values------------')\n    print(df.isna().sum())\n    print('-----------Data types-------------')\n    print(df.dtypes)\n    print('-----------Memory usage (MB)------')\n    print(np.round(df.memory_usage(deep = True).sum() / 1024**2, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sales = pd.read_csv(data_path + 'sales_train.csv')\nsales = downcast_dtypes(sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.read_csv(data_path + 'items.csv')\nitems = downcast_dtypes(items)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories = pd.read_csv(data_path + 'item_categories.csv')\nitem_categories = downcast_dtypes(item_categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops = pd.read_csv(data_path + 'shops.csv')\nshops = downcast_dtypes(shops)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(data_path + 'test.csv')\ntest = downcast_dtypes(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"display_df_info(sales, 'Sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_df_info(items, 'items')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_df_info(item_categories, 'item Categories')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_df_info(shops, 'shops')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_df_info(test, 'Test set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pair plot for sales\nThis is an interesting plot because:\n- it shows the histogram (the diagonal) for each columns\n- it shows if there is outliers in each columns\n- it draws each columns with respect to other columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_sampled = sales.sample(n = 10000)\nsns.pairplot(sales_sampled[['date_block_num', 'shop_id', 'item_id', 'item_price', 'item_cnt_day']], diag_kind = 'kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sales_sampled\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## item_price outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = sales['item_price'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.loc[:, 'item_price'] = sales.loc[:, 'item_price'].clip(-1, 10**5)\nsale_with_negative_price = sales[sales['item_price'] < 0]\nsale_with_negative_price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sale = sales[(sales.shop_id == 32) & (sales.item_id == 2973) & (sales.date_block_num == 4) & (sales.item_price > 0)]\nmedian = sale.item_price.median()\nsales.loc[sales.item_price < 0, 'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sale \ndel median\ndel sale_with_negative_price\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## item_cnt_day outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(sales['item_cnt_day'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_temp = sales[sales['item_cnt_day'] > 500]\nprint('Sold item outliers')\nitems[items['item_id'].isin(sales_temp['item_id'].values)].merge(sales_temp[['item_id', 'item_cnt_day', 'date_block_num']], on = 'item_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will clip all sales amount for shops/items to [0, 20] when we constract the train set.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"del sales_temp\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Delete duplicated records"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of duplicates:', len(sales[sales.duplicated()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales.drop_duplicates(keep = 'first')\nprint('Number of duplicates:', len(sales[sales.duplicated()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert sales.date form string to datetime"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nsales.date = sales.date.apply(lambda x: datetime.datetime.strptime(x, '%d.%m.%Y'))\nprint('First sale took place: ', sales.date.min())\nprint('Last sale took place: ', sales.date.max())\nprint('It tooks: ', round(time.time() - start), 'seconds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sales history"},{"metadata":{},"cell_type":"markdown","source":"### shop/item pair sales history"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\npairs_trans = create_record_for_features(sales, ['shop_id', 'item_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.count_nonzero, fill = 0)\nprint('It tooks: ', round(time.time() - start), 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for month in range(1, 12):\n    pairs_temp = pairs_trans[['shop_id', 'item_id']][pairs_trans.loc[:,'0': str(month)].sum(axis = 1) == 0]\n    print('From month: 0 until ', month,', ', np.round(100 * len(pairs_temp) / len(pairs_trans), 2), '% of the item/shop pairs have made no sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for month in range(21, 33):\n    pairs_temp = pairs_trans[['shop_id', 'item_id']][pairs_trans.loc[:,str(month): '33'].sum(axis = 1) == 0]\n    print('From month: ', month, ' until month: 33, ', np.round(100 * len(pairs_temp) / len(pairs_trans), 2), '% of the item/shop pairs have made no sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del pairs_trans\ndel pairs_temp\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### item sales history"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nitems_trans = create_record_for_features(sales, ['item_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.count_nonzero, fill = 0)\nprint('It tooks: ', round(time.time() - start), 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for month in range(1, 12):\n    items_temp = items_trans['item_id'][items_trans.loc[:,'0': str(month)].sum(axis = 1) == 0]\n    print('From month: 0 until ', month,', ', np.round(100 * len(items_temp) / len(items_trans), 2), '% of the items have made no sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for month in range(21, 33):\n    items_temp = items_trans['item_id'][items_trans.loc[:,str(month): '33'].sum(axis = 1) == 0]\n    print('From month: ', month, ' until month: 33, ', np.round(100 * len(items_temp) / len(items_trans), 2), '% of the items have made no sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del items_trans\ndel items_temp\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### shop sales history"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nshops_trans = create_record_for_features(sales, ['shop_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.count_nonzero, fill = 0)\nprint('It tooks: ', round(time.time() - start), 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for month in range(1, 12):\n    shops_temp = shops_trans['shop_id'][shops_trans.loc[:,'0': str(month)].sum(axis = 1) == 0]\n    print('From month: 0 until ', month,', ', np.round(100 * len(shops_temp) / len(shops_trans), 2), '% of the shops have made no sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for month in range(21, 33):\n    shops_temp = shops_trans['shop_id'][shops_trans.loc[:, str(month): '33'].sum(axis = 1) == 0]\n    print('From month: ', month, ' until month: 33, ', np.round(100 * len(shops_temp) / len(shops_trans), 2), '% of the shops have made no sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del shops_trans\ndel shops_temp\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary\nAs we see, We don't have full sale history for items, shops and item/shop pairs during 2 years and 10 months. For example\n\n- for the first year 38.54 % of the item/shop pairs have made no sales\n- for the last year 57.91 % of the item/shop pairs have made no sales\n\nThe absence of sales historical data will make the prediction is a challenging task.\n\nWe have 214200 pairs of shop/item that we want to predict their future sales for the month 34. The problem is modeled as:\n\ntarget_pair1(t = 0), target_pair1(t = 1), target_pair1(t = 2), ..., target_pair1(t = k) >>>*predict*>>> target_pair1(t = 34).\n\ntarget_pair2(t = 0), target_pair2(t = 1), target_pair2(t = 2), ..., target_pair2(t = k) >>>*predict*>>> target_pair2(t = 34)\n\ntarget_pair_m(t = 0), target_pair_m(t = 1), target_pair_m(t = 2), ..., target_pair_m(t = k) >>>*predict*>>> target_pair_m(t = 34)\n\n**Note**\n\nThe amount of selling items is dependant on the time and the shop as well, see the following plot:"},{"metadata":{"trusted":true},"cell_type":"code","source":"item_id = 20949\nshop_ids = [25, 24]\nitem_sales = sales[(sales['item_id'] == item_id) & (sales['shop_id'].isin(shop_ids))]\nfig, axs = plt.subplots(figsize = (10, 6),  constrained_layout=True)\nsns.pointplot(x = 'date_block_num', y = 'item_cnt_day', hue = 'shop_id', data = item_sales)\naxs.set_title('Sales for item: ' + str(item_id))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test set distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('test shape: ', test.shape)\nprint('number of items in test set: ', test['item_id'].nunique())\nprint('number of shops in test set: ', test['shop_id'].nunique())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['item_id'].nunique() * test['shop_id'].nunique() == len(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_test = set(test.item_id)\nitems_sales = set(sales.item_id)\nitem_in_test_and_sales = items_test.intersection(items_sales)\nitem_in_test_not_sales = set(test.item_id) - items_sales.intersection(items_test)\nprint('There is sales history for:', np.round(100 * len(item_in_test_and_sales) / len(items_test), 2) , '% items in test set')\nprint('There is No sales history for:', np.round(100 * len(item_in_test_not_sales) / len(items_test), 2), '% items in test set')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_test = set(test.shop_id)\nshops_sales = set(sales.shop_id)\nshops_in_test_and_sales = shops_test.intersection(shops_sales)\nshops_in_test_not_sales = set(test.shop_id) - shops_test.intersection(shops_sales)\nprint('There is sales history for:', np.round(100 * len(shops_in_test_and_sales) / len(shops_test), 2), '% shops in test set')\nprint('There is No sales history for:', np.round(100 * len(shops_in_test_not_sales) / len(shops_test), 2), '% shops in test set')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_shop_test = set(test.item_id.astype(str) + '_' + test.shop_id.astype(str))\nitem_shop_sales = set(sales.item_id.astype(str) + '_' + sales.shop_id.astype(str))\npairs_with_history = len(item_shop_test.intersection(item_shop_sales) )\npairs_with_no_history = len(shops_in_test_and_sales) * len(item_in_test_not_sales)\njust_item_with_history = test.shape[0] - (pairs_with_history + pairs_with_no_history)\nprint('There is sales history for:', np.round(100 * pairs_with_history / len(test), 2) , '% items in the same shops')\nprint('There is No sales history for:',  np.round(100 * pairs_with_no_history / len(test), 2), '% shop/item pairs')\nprint('There is sales history for:',  np.round(100 * just_item_with_history / len(test), 2), '% items but in different shops')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del items_test\ndel items_sales\ndel item_in_test_and_sales\ndel item_in_test_not_sales\ndel shops_test\ndel shops_sales\ndel shops_in_test_and_sales\ndel shops_in_test_not_sales\ndel item_shop_test\ndel item_shop_sales\ndel pairs_with_history\ndel pairs_with_no_history\ndel just_item_with_history\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning"},{"metadata":{},"cell_type":"markdown","source":"## Shop name preprocessing\n\nShop name contains the following info(https://www.kaggle.com/kyakovlev):\n\ncity | shop_type | shop_name"},{"metadata":{"trusted":true},"cell_type":"code","source":"shops['shop_name'] = shops['shop_name'].apply(lambda x: x.lower()).str.replace('[^\\w\\s]', '').str.replace('\\d+','').str.strip()\nshops['shop_city'], shops['shop_name'] = shops['shop_name'].str.split(' ', 1).str\nshops['shop_type'] = shops['shop_name'].apply(lambda x: 'мтрц' if 'мтрц' in x else 'трц' if 'трц' in x else 'трк' if 'трк' in x else 'тц' if 'тц' in x else 'тк' if 'тк' in x else 'unkown')\nshops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shops number:', shops['shop_id'].nunique())\nprint('Shop names number:', shops['shop_name'].nunique())\nprint('Shop cities number:', shops['shop_city'].nunique())\nprint('Shop types number:', shops['shop_type'].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove duplicated shops\n\nThe shops below have similar names"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.loc[sales['shop_id'] == 11, 'shop_id'] = 10\nshops.loc[shops['shop_id'] == 11, 'shop_id'] = 10\n\nsales.loc[sales['shop_id'] == 23, 'shop_id'] = 24\nshops.loc[shops['shop_id'] == 23, 'shop_id'] = 24\n\nsales.loc[sales['shop_id'] == 0, 'shop_id'] = 57\nshops.loc[shops['shop_id'] == 0, 'shop_id'] = 57\n\nsales.loc[sales['shop_id'] == 1, 'shop_id'] = 58\nshops.loc[shops['shop_id'] == 1, 'shop_id'] = 58\n\nsales.loc[sales['shop_id'] == 40, 'shop_id'] = 39\nshops.loc[shops['shop_id'] == 40, 'shop_id'] = 39\n\nshops = shops.drop_duplicates(subset = 'shop_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shops number:', shops['shop_id'].nunique())\nprint('Shop names number:', shops['shop_name'].nunique())\nprint('Shop cities number:', shops['shop_city'].nunique())\nprint('Shop types number:', shops['shop_type'].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Item_name preprocessing\nitem_name contains the name, type, and subtype in following format (https://www.kaggle.com/kyakovlev):\n    \n- item_name [item_type] (item_subtype)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_names(df, cols):\n    for col in cols:\n        df[col] = df[col].str.replace('[^A-Za-z0-9А-Яа-я]+', ' ').str.lower()\n        df[col] = df[col].str.strip()\n        df.loc[df[col] == '', col] = 'unknown'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items['item_name'], items['item_type'] = items['item_name'].str.split('[', 1).str\nitems['item_name'], items['item_subtype'] = items['item_name'].str.split('(', 1).str\nclean_names(items, ['item_name', 'item_type', 'item_subtype'])\nitems = items.fillna('unkown')\nitems.head()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of items:', items['item_id'].nunique())\nprint('Number of item_name:', items['item_name'].nunique())\nprint('Number of item_type:', items['item_type'].nunique())\nprint('Number of item_subtype:', items['item_subtype'].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales.merge(items[['item_id', 'item_category_id']], on = 'item_id', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lag features"},{"metadata":{},"cell_type":"markdown","source":"In order to create lag features, we need to determine how much the previous values of the features affect the prediction of the current one.\n\nI will use Autocorrelation function **(acf)** to perfom this analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"time_shift = 14","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How much to go back in time for pairs(shop/item)\nI will choose 1000 pairs and compute how much the previous values of the series (lags) may be helpful in predicting the current value."},{"metadata":{"trusted":true},"cell_type":"code","source":"pairs_sales = create_record_for_features(sales, ['shop_id', 'item_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_items_sales_temp = pairs_sales.sample(1000)#.nlargest(10**4, columns = [str(itr) for itr in range(27, 33)])\npairs_acf = np.zeros((shops_items_sales_temp.shape[0], time_shift + 1))\nfor i, (ind, shop_item_sales) in enumerate(shops_items_sales_temp.iterrows()):\n    pair = shop_item_sales.loc['0': ]\n    acf_12 = acf(pair, nlags = time_shift, fft = True)\n    pairs_acf[i, :] = acf_12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avgs = np.mean(pairs_acf, axis = 0)\nplt.bar(x = np.arange(time_shift + 1), height = avgs)\nplt.title('lag importance of shop/item pairs')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pair_lags = [1, 2, 3, 4, 8, 10, 11, 12]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del shops_items_sales_temp\ndel pairs_sales\ndel shop_item_sales\ndel pair\ndel acf_12\ndel pairs_acf\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How much to go back in time for items"},{"metadata":{"trusted":true},"cell_type":"code","source":"items_sales = create_record_for_features(sales, ['item_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_acf = np.zeros((items_sales.shape[0], time_shift + 1))\nfor i, item_sales in items_sales.iterrows():\n    item_temp = item_sales.loc['0': ]\n    if np.sum(item_temp) != 0:\n        acf_12 = acf(item_temp, nlags = time_shift, fft = True)\n        items_acf[i, :] = acf_12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avgs = np.mean(items_acf, axis = 0)\nplt.bar(x = np.arange(time_shift + 1), height = avgs)\nplt.title('lag importance of items')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_lags = [1, 2, 3, 4, 5, 10, 11, 12]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del items_sales\ndel item_sales\ndel items_acf\ndel item_temp\ndel acf_12\ndel avgs\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How much to go back in time for shops"},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_sales = create_record_for_features(sales, ['shop_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)\nshops_acf = np.zeros((shops_sales.shape[0], time_shift + 1))\nfor i, shop_sales in shops_sales.iterrows():\n    shop_temp = shop_sales.loc['0': ]\n    acf_12 = acf(shop_temp, nlags = time_shift, fft = True)\n    shops_acf[i, :] = acf_12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avgs = np.mean(shops_acf, axis = 0)\nplt.bar(x = np.arange(time_shift + 1), height = avgs)\nplt.title('lag importance of shops')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_lags = [1, 2, 3, 4, 7, 8, 10, 12]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del shops_sales\ndel shop_sales\ndel shops_acf\ndel shop_temp\ndel acf_12\ndel avgs\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How much to go back in time for categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"categories_sales = create_record_for_features(sales, ['item_category_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)\ncategories_acf = np.zeros((categories_sales.shape[0], time_shift + 1))\nfor i, category_sales in categories_sales.iterrows():\n    category_temp = category_sales.loc['0': ]\n    acf_12 = acf(category_temp, nlags = time_shift, fft = True)\n    categories_acf[i, :] = acf_12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avgs = np.mean(categories_acf, axis = 0)\nplt.bar(x = np.arange(time_shift + 1), height = avgs)\nplt.title('lag importance of categories')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_lags = [1, 2, 3, 4, 5, 6, 12]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del categories_sales\ndel category_sales\ndel categories_acf\ndel category_temp\ndel acf_12\ndel avgs\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create training grid"},{"metadata":{"trusted":true},"cell_type":"code","source":"index_cols = ['shop_id', 'item_id', 'date_block_num'] \n\ndef create_train_set(df, index_cols = index_cols):\n    grid = []\n    for month in df['date_block_num'].unique():\n        curr_shops = df[df['date_block_num'] == month]['shop_id'].unique()\n        curr_items = df[df['date_block_num'] == month]['item_id'].unique()\n        grid.append(np.array(list(product(*[curr_shops, curr_items, [month]])), dtype = 'int32'))\n\n    grid = pd.DataFrame(np.vstack(grid), columns = index_cols, dtype = np.int32)\n\n    gb = df.groupby(index_cols, as_index = False).agg({'item_cnt_day':'sum'})\n    gb.columns = index_cols + ['target']\n    all_data = pd.merge(grid, gb, how = 'left', on = index_cols).fillna(0)\n    all_data.sort_values(['date_block_num','shop_id','item_id'], inplace = True)\n    all_data.loc[:, 'target'] = all_data['target'].clip(0, 20).astype(np.float32)\n    print('Sales data shape:', df.shape)\n    print('Generated Train data shape:', all_data.shape)\n    del curr_shops\n    del curr_items\n    del gb\n    del grid\n    gc.collect()\n    return all_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Concatenate test set to the grid"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\ngrid = create_train_set(sales, index_cols)\ntest['date_block_num'] = 34\ngrid = pd.concat([grid, test[['item_id', 'shop_id', 'date_block_num']]], ignore_index = True, sort = False, keys = index_cols)\ngrid = grid.merge(items[['item_id', 'item_category_id']], on = 'item_id', how = 'left')\nprint(round(time.time() - start), 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_features(df, features, go_back_in_time):\n    for month_shift in go_back_in_time:\n        df_shift = df[index_cols + features].copy()\n        df_shift['date_block_num'] = df_shift['date_block_num'] + month_shift\n        lag_cols = lambda x: '{}_lag_{}'.format(x, month_shift) if x in features else x\n        df_shift = df_shift.rename(columns = lag_cols)\n        df = pd.merge(df, df_shift, on = index_cols, how='left')\n    return df\n\ndef fast_lag_features(df, features, go_back_in_time): \n    features_sales = create_record_for_features(sales, features, 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)\n    for month in go_back_in_time:\n        max_month = df.date_block_num.max()\n        cols = [str(itr) for itr in np.arange(0, max_month)]\n        gb = features_sales.melt( id_vars = features, \n                                 var_name = 'date_block_num' , \n                                 value_vars= cols, \n                                 value_name = 'target_' + '_'.join(features) + '_lag_' + str(month)\n                                )\n        gb.date_block_num = gb.date_block_num.astype(np.int16)\n        gb.date_block_num = gb.date_block_num + month\n        df = pd.merge(df, gb, on = features + ['date_block_num'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add lag feature to grid"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\ngrid = lag_features(grid, ['target'], pair_lags)\nprint(round(time.time() - start), 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\ngrid = fast_lag_features(grid, ['item_id'], item_lags)\nprint(round(time.time() - start), 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\ngrid = fast_lag_features(grid, ['shop_id'], shop_lags)\nprint(round(time.time() - start), 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\ngrid = fast_lag_features(grid, ['item_category_id'], category_lags)\nprint(round(time.time() - start), 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = grid[grid['date_block_num'] > 11]\ngrid = grid.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = downcast_dtypes(grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Date Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"dates_train = sales.loc[:, ['date', 'date_block_num']].drop_duplicates()\ndates_train = dates_train.reset_index(drop = True)\ndates_test = dates_train.loc[dates_train.loc[:, 'date_block_num'] == 34 - 12]\ndates_test = dates_test.reset_index(drop=True)\ndates_test.loc[:,'date_block_num'] = 34\ndates_test.loc[:, 'date'] = dates_test.loc[:, 'date'] + pd.DateOffset(years = 1)\ndates_all = pd.concat([dates_train, dates_test])\ndates_all.loc[:, 'dow'] = dates_all.loc[:, 'date'].dt.dayofweek\ndates_all.loc[:, 'year'] = dates_all.loc[:, 'date'].dt.year\ndates_all.loc[:, 'month'] = dates_all.loc[:, 'date'].dt.month\n\ndates_all = pd.get_dummies(dates_all, columns = ['dow'])\ndow_col = ['dow_' + str(x) for x in range(7)]\ndate_features = dates_all.groupby(['year', 'month', 'date_block_num'])[dow_col].agg('sum').reset_index()\ndate_features.loc[:, 'days_of_month'] = date_features.loc[:, dow_col].sum(axis=1)\ndate_features.loc[:, 'year'] = date_features.loc[:, 'year'] - 2013\ndate_features = date_features.loc[:, ['month', 'date_block_num']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = grid.merge(date_features, on = 'date_block_num', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del dates_train\ndel dates_test\ndel dates_all\ndel dow_col\ndel date_features\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Interaction"},{"metadata":{},"cell_type":"markdown","source":"### Category/Shop"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid['category_shop_inter'] = grid['item_category_id'].astype(str) + '_' + grid['shop_id'].astype(str)\ngrid.loc[:, 'category_shop_inter'] = LabelEncoder().fit_transform(grid.loc[:, 'category_shop_inter'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = downcast_dtypes(grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['target', 'date_block_num']\ndef train_val_test_split(df):\n    dates = df['date_block_num']\n    last_block = dates.max()\n    print('Test `date_block_num` is %d' % last_block)\n    print('Validation `date_block_num` is %d' % (last_block - 1))\n    print('Train `date_block_num` is < %d' % (last_block - 1))\n    print('------------------------------')\n\n    X_train = df.loc[dates < last_block - 1].drop(cols_to_drop, axis = 1)\n    X_val = df.loc[dates == last_block - 1].drop(cols_to_drop, axis = 1)\n    X_test =  df.loc[dates == last_block].drop(cols_to_drop, axis = 1)\n\n    y_train = df.loc[dates < last_block - 1, 'target'].values\n    y_val =  df.loc[dates == last_block - 1, 'target'].values\n    \n    print('X_train shape: ', X_train.shape)\n    print('y_train shape: ', y_train.shape)\n    print('------------------------------')\n    print('X_val shape: ', X_val.shape)\n    print('y_val shape: ', y_val.shape)\n    print('------------------------------')\n    print('X_test shape: ', X_test.shape)\n    print('------------------------------')\n    return (X_train, y_train, X_val, y_val, X_test)\n\ndef rmse(y, y_hat):\n    return np.sqrt(mean_squared_error(y, y_hat))\n\ndef create_lgbm_model(X_train, y_train, X_val, y_val, params, cat_feats):\n    n_estimators = 8000\n    d_train = lgb.Dataset(X_train, y_train)\n    d_valid = lgb.Dataset(X_val, y_val)\n    watchlist = [d_train, d_valid]\n    evals_result = {}\n    model = lgb.train(params, \n                      d_train, \n                      n_estimators,\n                      valid_sets = watchlist, \n                      evals_result = evals_result, \n                      early_stopping_rounds = 50,\n                      verbose_eval = 0,\n                      categorical_feature = cat_feats,\n                    )\n    lgb.plot_metric(evals_result)\n    return model\n\ndef evaluate_model(model, X_train, y_train, X_val, y_val): \n    y_hat = model.predict(X_train)\n    print('Training error;', rmse(y_train, y_hat))\n    y_val_hat = model.predict(X_val)\n    print('Validation error:', rmse(y_val, y_val_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['shop_id', 'item_category_id', 'month', 'category_shop_inter']\nfor col in categorical_features:\n    grid.loc[:, col] = grid[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train/val split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train, X_val, y_val, X_test = train_val_test_split(grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nparams = {\n  'metric': 'rmse',\n  'objective': 'mse',\n  'verbose': 0, \n  'learning_rate': 0.1,\n  'num_leaves': 31,\n  'min_data_in_leaf': 20 ,\n  'max_depth': -1,\n  'save_binary': True,\n  'bagging_fraction': 0.8,\n  'bagging_freq': 1,\n  'bagging_seed': 2**7, \n  'feature_fraction': 0.8,\n}\nlgbm_model = create_lgbm_model(X_train, y_train, X_val, y_val, params, categorical_features)\nprint('it tooks: ', round(time.time() - start), 'seconds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate model"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nevaluate_model(lgbm_model, X_train, y_train, X_val, y_val)\nprint('it tooks: ', round(time.time() - start), 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = lgb.plot_importance(lgbm_model, max_num_features = 40, figsize = (8, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = lgbm_model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.ID, \n    \"item_cnt_month\": y_test_pred\n})\nsubmission.to_csv('lgbm_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.item_cnt_month.hist()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}