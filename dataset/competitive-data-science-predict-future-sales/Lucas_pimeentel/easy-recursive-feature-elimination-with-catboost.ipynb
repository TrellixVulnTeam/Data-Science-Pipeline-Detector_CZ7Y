{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, I am going to teach you how to easily do Recursive Feature Elimination (or RFE, for short) using Catboost.\n\nThe beauty of RFE is that it is one of the most intuitive and straightforward feature selection methods.\n\nAnd if you are someone like me who likes to create dozens and dozens of features and only later see which ones are useful, this feature selection technique will be like a godsend to you.\n\nIn short, RFE gradually removes the worst-performing features of your dataset, leaving you only with the ones that helps the performance of your model... (If you are interested in diving deeper into this topic, [here's a great article about it](https://machinelearningmastery.com/rfe-feature-selection-in-python/))\n\nThe only problem is that I haven't seen many notebooks teaching how to easily implement this technique, and I have had to do my fair share of testing and experimenting before I found this easy approach to it (only a handful lines of code will be needed).\n\nNow, the dataset I am going to use in this notebook is the one I am using for the [Predict Future Sales Competition](https://www.kaggle.com/c/competitive-data-science-predict-future-sales). For the sake of simplicity, I will only concern myself with the RFE technique. So I won't talk about data-preprocessing, feature engineering and etc.\n(My final model is still a work in progress and I plan to release a public notebook when I finish it, so stay tuned)\n\n\nAnd without further a do, let's get right dive deep into the RFE.","metadata":{"_uuid":"58138a2e-026c-4a01-9e72-a54d723591c0","_cell_guid":"330b354f-a4a0-4fb7-9683-584c9b1954ff","trusted":true}},{"cell_type":"markdown","source":"<h2> Importing Libraries & Datasets </h2>","metadata":{"_uuid":"7e5c5a89-fac5-4faf-9708-45d49e0c48fa","_cell_guid":"02e7d252-76aa-484d-8baf-a1b8bf111240","trusted":true}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# \npd.set_option('display.max_columns',None)\n\n#\ndf = pd.read_pickle('../input/predict-future-sales-df/predict_future_sales_df.pkl')\ndf.head(5)","metadata":{"_uuid":"23d34279-b619-459a-983e-596c1b91fd4a","_cell_guid":"e58d89f7-dd04-4bbb-8a7b-22ee3ce03e19","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-09T18:33:07.947465Z","iopub.execute_input":"2022-06-09T18:33:07.948516Z","iopub.status.idle":"2022-06-09T18:33:08.44358Z","shell.execute_reply.started":"2022-06-09T18:33:07.94844Z","shell.execute_reply":"2022-06-09T18:33:08.442553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now before we continue, here's a quick breakdown of the features of this dataset. **(Feel free to jump this section if you want to)**\n\n**Shop_id**: Refers to the unique shop ID of a given store\n\n**Item_id**: Refers to the unique item ID of a given product.\n\n**mo**: Refers to the unique time index of the events.\n\n**mo revenue**: (scaled) values of the monthly revenue of a given shop/item combination.\n\n**Features ending in _mean**: Respective mean encoded feature.\n\n**Features ending in _cluster:** Respective cluster category of a given feature.\n\n**City**: City where a given shop is located.\n\n**Shop_category_name**: The name of the category to which a given item belongs.\n\n**Item_name_lenght**: Some discussion threads in this competition concluded that the length of the item name string contains useful information for our model. Is this true? We will soon find out.\n\n**Features starting with rolling_**: Moving average of a particular feature.\n\n**Features ending with lag**: Lagged features in relation to the current time index.\n\n**Features ending with age**: Some shops and items have different launch dates. So I created this feature to find out if this information is indeed important for our model.","metadata":{"_uuid":"81f8494b-8775-44dc-aecf-8476c79cc068","_cell_guid":"caee27b0-679a-48f6-93b5-d27f356e785e","trusted":true}},{"cell_type":"markdown","source":"<h2> Separating Train Set, Validation Set, and Test Set </h2>","metadata":{}},{"cell_type":"markdown","source":"The first step for using RFE is to create the train set, validation set, and test set.\n\nThis is a very important step, but most people completely overlook it. However, if you commit this mistake, the quality of your final model will suffer. So please take the time to study the problem at hand and find out what should be your validation strategy.\n\n\nSince we are dealing time series problem in this particular competition, the validation strategy we will use is walk-forward validation. That is, we will use the current month as the validation set, all months preceding it as the train set, and the next month (the month we are trying to forecast) as the prediction set.\n","metadata":{}},{"cell_type":"code","source":"# dropping some features to avoid data leakage/overfitting of this particilar dataset\ncolumns_to_drop = ['mo_sales','item_category_id','item_price','mo_revenue','rev_times_sales','item_id','shop_id']\n\n# Creating the train set (all months with the index values smaller thant the index of the current month)\nx_train = df[df['mo'] < 33].drop(columns_to_drop,axis=1)\ny_train = df[df['mo'] < 33]['mo_sales']\n\n# Creating the valid set (month time index == 33)\nx_valid = df[df['mo'] == 33].drop(columns_to_drop,axis=1)\ny_valid = df[df['mo'] == 33]['mo_sales']\n\n# Creating the test set (the month we are trying to predict)\ntest = df[df['mo'] == 34].drop(columns_to_drop,axis=1)","metadata":{"_uuid":"32245de3-0ac9-4837-9746-9ff12c9dc8de","_cell_guid":"883aeb23-42df-4a0e-953b-1761282d8738","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-09T18:33:08.445088Z","iopub.execute_input":"2022-06-09T18:33:08.445449Z","iopub.status.idle":"2022-06-09T18:33:08.858021Z","shell.execute_reply.started":"2022-06-09T18:33:08.445415Z","shell.execute_reply":"2022-06-09T18:33:08.856661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Creating CatBoost's Model </h2>","metadata":{"execution":{"iopub.status.busy":"2022-06-07T18:47:37.119289Z","iopub.execute_input":"2022-06-07T18:47:37.119744Z","iopub.status.idle":"2022-06-07T18:47:37.126907Z","shell.execute_reply.started":"2022-06-07T18:47:37.119701Z","shell.execute_reply":"2022-06-07T18:47:37.125325Z"}}},{"cell_type":"markdown","source":"And now its finally time for us to crete our model and start selecting features.\n\nThe first step is to import and call Catboost Regressor.","metadata":{}},{"cell_type":"code","source":"categorical_features = ['City','category_name','Shop Type','Category Cluster','Shop Cluster','City Cluster','Shop Type Cluster','Cat_name Cluster']\n\nfrom catboost import CatBoostRegressor\n\nregressor = CatBoostRegressor(eval_metric='RMSE', # Evalution metric of this competition\n                              iterations = 100, # Number of Gradient boost itertions we will use in this model\n                              cat_features = categorical_features, # Categorical features of the df\n                              random_state = 123\n                              )","metadata":{"_uuid":"fd62ff73-5045-436a-856c-8e0547534608","_cell_guid":"d4652067-c192-4007-8ed6-68e20de6ee34","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-09T18:33:08.859433Z","iopub.execute_input":"2022-06-09T18:33:08.859879Z","iopub.status.idle":"2022-06-09T18:33:08.869165Z","shell.execute_reply.started":"2022-06-09T18:33:08.859839Z","shell.execute_reply":"2022-06-09T18:33:08.867682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, unlike most other models, Catboost regressor allow you to preprocess the categorical features of the dataset when creating the regressor.\n\nOf course, you can preprocess them on your own, but I find it unnecessary since Catboost will do it for you.\n\nWith that out of the way, lets move on.\n\n---\n\n<h2> Selecting Features </h2>\n\nNow, the next step on the RFE pipeline is to call the **select_features** method on the Catboost regressor we have just created.\n\nIn a nutshell, this method will remove the least useful features of the dataset and inform us of how the model performs along the way. As simple as that.\n\nHere are the most important parameters of this method.\n\n**X** = The x_train set you have created in the previous step.\n\n**y** = The y_train set you have created in the previous step.\n\n**eval_set** = Short for evaluation set. Please specify the validation sets you will use to score the performance of your model. This parameter takes a tuple with (x_valid,y_valid) values as an argument.\n\n**features_for_select**: Specify the features you want to be evaluated in the RFE. Since I want to perform it on the entire dataset, I will select features 0 to 36. But if you want to split-test only a few features and leave the rest of the dataset untouched, you can specify them here.\n\n**num_features_to_select**: Specify how many features above you want to keep.\n\n**steps**: Specify how many rounds of the model you to be performed in total. \n\n**Verbose**: Specifiy after how many iterations of the model you want the current model's score to be printed. You can also turn it off by setting the parameter to False\n\n**train_final_model**: Set it to True if you want catboost to automatically train your model with the selected features after the RFE is finished.\n\n**plot**: Set it to true if you want catboost to plot the model's score graph right after finishing the RFE.\n\nNow, after completing the RFE, the select_features method will return a dictionary with the following values: selected_features; selected_features_names; eliminated_features; eliminated_features_names.\n\nFeel free to save it in a dictionary if you think this information will be useful for later use.\n\n\nFor start, let's only keep 10 features out of the 37 and see how the model performs. After that, we will be able to optimize the model accordingly.","metadata":{}},{"cell_type":"code","source":"rfe_dict = regressor.select_features(X = x_train, \n                                     y = y_train, \n                                     eval_set = (x_valid,y_valid), # Walkforward validation set we have created earlier\n                                     features_for_select = '0-36', # Features that will be selected on the RFE\n                                     num_features_to_select = 10, # Number of features to keep from the selected\n                                     steps = 5, # Number of model iterations performed in the RFE\n                                     verbose = 50, #\n                                     train_final_model = False, # Train final model after RFE is finished\n                                     plot = True # plot the ??? after the RFE is finished\n                                     )","metadata":{"_uuid":"fd62ff73-5045-436a-856c-8e0547534608","_cell_guid":"d4652067-c192-4007-8ed6-68e20de6ee34","execution":{"iopub.status.busy":"2022-06-09T18:33:08.871812Z","iopub.execute_input":"2022-06-09T18:33:08.872532Z","iopub.status.idle":"2022-06-09T18:34:57.744846Z","shell.execute_reply.started":"2022-06-09T18:33:08.872471Z","shell.execute_reply":"2022-06-09T18:34:57.743854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\nIn my opinion, what makes Catboost's RFE so great is that you can visualize the performance of the validation score as features get eliminated and use it to gauge how many features (and which ones of them specifically) you should ideally get rid of.\n\nAs we can see from the interactive graph above, the ideal number of features to remove seems to be 9. After that, it seems like we reach a point of diminishing returns and our score gets worse as we remove more features.\n\n\nNow, the heavy lifting is already done, and the next logical step is to retrain the model. For that, you can simply re-do the RFE (but now changing the number of features to be selected), or you can find out (by hovering the mouse on the interactive graph) what features were removed, drop them from your dataset, and retrain the model.\n\n\nFor the sake of simplicity, I will simply re-do the RFE.","metadata":{}},{"cell_type":"code","source":"regressor.select_features(x_train,y_train,eval_set=(x_valid,y_valid),\n                                     features_for_select='0-36',\n                                     num_features_to_select = 28,\n                                     steps = 5,\n                                     verbose = False,\n                                     train_final_model = True,\n                                     plot = False)\ny_pred = regressor.predict(test)\n\n### Transforming predictions into a csv file following the rule of this particular competition ###\ntest['item_cnt_month'] = y_pred\ntest['item_cnt_month'].clip(0,20,inplace=True)\nsub = test['item_cnt_month'].reset_index(drop=True)\nsubmission = pd.DataFrame({\"ID\": sub.index,\"item_cnt_month\": sub.values})\nsubmission.to_csv('sub.csv', index=False)\nprint('Done!')","metadata":{"_uuid":"243a484a-5116-4762-9398-3d9a973efb22","_cell_guid":"20777b2e-0720-492b-bac9-f987524de1f8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-09T18:34:57.746256Z","iopub.execute_input":"2022-06-09T18:34:57.746812Z","iopub.status.idle":"2022-06-09T18:37:50.461303Z","shell.execute_reply.started":"2022-06-09T18:34:57.746771Z","shell.execute_reply":"2022-06-09T18:37:50.4592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now you have an easy-to-implement framework for filtering useful features from the dozens of dozens you have created in a particular dataset.\n\nAs you could see, doing RFE with CatBoost is extremely easy and we only had to call two methods.\n\nFeel free to copy and paste all of the codes in this notebook if you find them useful.\n\nHope you have learned a thing or two that you can use to improve your models.\n\nThanks for your time and attention!","metadata":{}}]}