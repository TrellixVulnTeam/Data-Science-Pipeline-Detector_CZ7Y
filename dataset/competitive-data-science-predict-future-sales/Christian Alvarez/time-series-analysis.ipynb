{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='background:lightblue; border:0; color:black'><center>Time series analysis for predict daily sales</center></h1>"},{"metadata":{},"cell_type":"markdown","source":"Along this notebook we will focus in analyse the total number of sold items of a company. Instead of predict the number of a particular product in a store sold over a month, we will try to predict the total number of sold in over all the stores.\n\nIn the first part we will make a classical approach, using ARIMA models. In the second part we will use a slightly different methodology, using Recurrent Neural Networks (RNN), widely used in the COVID-19 analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom dateutil.parser import parse\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.stattools import adfuller\nfrom tqdm import tqdm_notebook\nfrom itertools import product\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nimport math\nfrom sklearn.metrics import mean_squared_error as mse","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import all of them \nsales=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\n\n# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nitem_cat=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nitem=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nsub=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sample_submission.csv\")\nshops=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\ntest=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data we use for this analysis is the one propose in the kaggle competition Predict Future Sales."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sales['date'] = pd.to_datetime(sales['date'],format = '%d.%m.%Y')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we mentioned in the introduction, we only focus in the total number of sold item, so that the next is to sum over all the products grouping by days."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts=sales.groupby([\"date\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nts=ts.to_frame()\nts.reset_index(inplace=True)\nts.date = pd.to_datetime(ts.date)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our time series has the form"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(12,8), dpi= 80)\nplt.plot( 'item_cnt_day', data=ts)\n\n# Decoration\nplt.ylim(50, 15000)\nplt.xticks( rotation=0, horizontalalignment='center', alpha=.7)\nplt.yticks(alpha=.7) \nplt.title(\"Nº o sold items in a day from 2013 - 2016\")\nplt.grid(axis='both', alpha=.3)\n\n# Remove borders\nplt.gca().spines[\"top\"].set_alpha(0.0)    \nplt.gca().spines[\"bottom\"].set_alpha(0.3)\nplt.gca().spines[\"right\"].set_alpha(0.0)    \nplt.gca().spines[\"left\"].set_alpha(0.3)   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see our time series has a very noisy form. To reduce all that noise we will make a Box-Cox transform."},{"metadata":{},"cell_type":"markdown","source":"<h2 style='background:lightblue; border:0; color:black'><center>Box-Cox transform</center></h2>\n"},{"metadata":{},"cell_type":"markdown","source":"The Box-Cox transformation is defined as"},{"metadata":{},"cell_type":"markdown","source":"$$ y_{i}^{(\\lambda)}= \\left\\{\\begin{matrix}\n\\frac{y_{i}^{\\lambda}-1}{\\lambda} \\quad   \\lambda\\neq 0\\\\\nln(y_{i}) \\quad  \\lambda = 0\\end{matrix}\\right. $$"},{"metadata":{},"cell_type":"markdown","source":"We use the function boxcox of the packet stats, to calculate the transform and the $\\lambda$ parameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.boxcox(ts['item_cnt_day'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts['item_box']=stats.boxcox(ts['item_cnt_day'])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(12,8), dpi= 80)\nplt.plot( 'item_box', data=ts)\n\n# Decoration\nplt.ylim(2.525, 2.7)\nplt.xticks( rotation=0, horizontalalignment='center', alpha=.7)\nplt.yticks(alpha=.7) \nplt.title(\"Nº o sold items in a day from 2013 - 2016\")\nplt.grid(axis='both', alpha=.3)\n\n# Remove borders\nplt.gca().spines[\"top\"].set_alpha(0.0)    \nplt.gca().spines[\"bottom\"].set_alpha(0.3)\nplt.gca().spines[\"right\"].set_alpha(0.0)    \nplt.gca().spines[\"left\"].set_alpha(0.3)   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our series has some noise but is slightly better than the previous case. \nThe next step is study the autocorrelation and the partial autocorrelation function."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12,4), dpi= 80)\nplot_acf(ts.item_box.tolist(), ax=ax1, lags=100)\nplot_pacf(ts.item_box.tolist(), ax=ax2, lags=80)\n\n# Decorate\n# lighten the borders\nax1.spines[\"top\"].set_alpha(.3); ax2.spines[\"top\"].set_alpha(.3)\nax1.spines[\"bottom\"].set_alpha(.3); ax2.spines[\"bottom\"].set_alpha(.3)\nax1.spines[\"right\"].set_alpha(.3); ax2.spines[\"right\"].set_alpha(.3)\nax1.spines[\"left\"].set_alpha(.3); ax2.spines[\"left\"].set_alpha(.3)\n\n# font size of tick labels\nax1.tick_params(axis='both', labelsize=12)\nax2.tick_params(axis='both', labelsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The autocorrelation and the partial autocorrelation function suggest that the time series has a seasonal and a trend component. In the next plot can be watching better."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Import Data\nts.set_index(ts['date'], inplace=True)\n\n# Decompose\nresult = seasonal_decompose(ts['item_box'], model='multiplicative')\n\n# Plot\nplt.rcParams.update({'figure.figsize': (10,10)})\nresult.plot().suptitle('Time Series Decomposition of sold items')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style='background:lightblue; border:0; color:black'><center>The ARIMA model</center></h2>\n"},{"metadata":{},"cell_type":"markdown","source":"We know that the time series has a trend component. The first thing we have to do is eliminate that trend. To do this we differentiate with lag 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"diff = ts.diff()\ndiff.drop(diff.index[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"result = seasonal_decompose(diff['item_box'], model='additive')\n# Plot\nplt.rcParams.update({'figure.figsize': (10,10)})\nresult.plot().suptitle('Time Series Decomposition of sold items')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The trend has gone. We will see what happens now with the autocorrelation and the partial autocorrelation function."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12,4), dpi= 80)\nplot_acf(diff.item_box.tolist(), ax=ax1, lags=100)\nplot_pacf(diff.item_box.tolist(), ax=ax2, lags=80)\n\n# Decorate\n# lighten the borders\nax1.spines[\"top\"].set_alpha(.3); ax2.spines[\"top\"].set_alpha(.3)\nax1.spines[\"bottom\"].set_alpha(.3); ax2.spines[\"bottom\"].set_alpha(.3)\nax1.spines[\"right\"].set_alpha(.3); ax2.spines[\"right\"].set_alpha(.3)\nax1.spines[\"left\"].set_alpha(.3); ax2.spines[\"left\"].set_alpha(.3)\n\n# font size of tick labels\nax1.tick_params(axis='both', labelsize=12)\nax2.tick_params(axis='both', labelsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is obvious that our time series has a seasonality with lag 7, and it something that makes sense if one thinks about that this lag has the same length as the number of the days of the week.To solve this problem we will make a differentiation with lag 7."},{"metadata":{"trusted":true},"cell_type":"code","source":"diff_s = diff.diff(7)\ndiff_s.drop(diff_s.index[0:7], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12,4), dpi= 80)\nplot_acf(diff_s.item_box.tolist(), ax=ax1, lags=100)\nplot_pacf(diff_s.item_box.tolist(), ax=ax2, lags=80)\n\n# Decorate\n# lighten the borders\nax1.spines[\"top\"].set_alpha(.3); ax2.spines[\"top\"].set_alpha(.3)\nax1.spines[\"bottom\"].set_alpha(.3); ax2.spines[\"bottom\"].set_alpha(.3)\nax1.spines[\"right\"].set_alpha(.3); ax2.spines[\"right\"].set_alpha(.3)\nax1.spines[\"left\"].set_alpha(.3); ax2.spines[\"left\"].set_alpha(.3)\n\n# font size of tick labels\nax1.tick_params(axis='both', labelsize=12)\nax2.tick_params(axis='both', labelsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have a series that don't have the trend and the seasonality. We will try now to find the right parameters of the ARIMA model. To do this we make a simple function to find this parameters based on the aic score."},{"metadata":{"trusted":true},"cell_type":"code","source":"def SARIMA_PARAMETERS(parameters,time_serie):\n    \"\"\"\n    parameters-> list of SARIMA parameters (p,d,q,P,D,Q,s)\n    \n    \"\"\"\n    list_param=[]\n    \n    for i in tqdm_notebook(parameters):\n        try:\n            model=SARIMAX(time_serie, order=(i[0], i[1], i[2]), seasonal_order=(i[3], i[4], i[5], i[6])).fit(disp=-1)\n        except:\n            continue\n            \n        aic = model.aic\n        list_param.append([i, aic])   \n        \n    list_param_df = pd.DataFrame(list_param)\n    list_param_df.columns = ['(p,d,q)x(P,D,Q)s', 'AIC']\n    list_param_df = list_param_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n    \n    return list_param_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = range(0, 3, 1)\nd = range(1,2)\nq = range(0, 3, 1)\nP = range(0, 3, 1)\nD = range(1, 2, 1)\nQ = range(0, 3, 1)\ns = range(7,8)\nparameters = product(p,d, q, P,D, Q,s)\nparameters_list = list(parameters)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see our function has 7 parameters. The p,q correspond with the order of the ARMA model associated, the P and Q are the order of the seasonal component. The d is the number of differentiation in the model, and D and s are the number of differentiations and the lag of the seasonal component. The reader could think that why we take care about the number of differentiations and the lag of the seasonal component in the previus section, well the answer is simple, if we have the number of differenciations we have less parameters to determinate. Right now we have a total of 81 posible combinations, but the number increase incredible fast if we have to estime three more parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df=SARIMA_PARAMETERS(parameters_list,ts['item_box'][0:1004])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use all the time serie except of the last 30 days, which we will use for test our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best model is the $(1, 1, 1)( 1, 1, 1)_{7}$ with an aic socre of -6257"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model = SARIMAX(ts['item_box'][0:1004], order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))\nres=model.fit(dis=-1)\nfcast = res.get_forecast(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"a=res.plot_diagnostics(figsize=(15,12))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in the plot, our residuals don't fall a normal distribution, but at least they are uncorrelated."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ts['arima_model'] = res.fittedvalues\nforecast = res.predict(start=ts['item_box'][:1004].shape[0], end=ts['item_box'].shape[0]-1)\nforecast = ts['arima_model'][:1004].append(forecast)\nplt.figure(figsize=(18, 7.5))\nplt.plot(forecast[10:1004], color='r', label='model')\nplt.plot(ts['item_box'][10:1004], label='actual')\n\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In red we have the model and the real data is in blue. As we can see the fit is quite good in all the serie. The next part is to compare the test data with the prediction."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"x=list(range(1,31))\ny_error =[abs(fcast.conf_int(alpha=0.01)['lower item_box'].values-forecast[1004:].values), fcast.conf_int(alpha=0.01)['upper item_box'].values-forecast[1004:].values] \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(15, 7.5))\n\nplt.errorbar(x,forecast[1004:].values,yerr=y_error,fmt='o')\nplt.errorbar(x,ts['item_box'][1004:].values, label='actual', fmt='o',color='r')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In blue we have the prediction with a confidence band of 95% and in red is the real data."},{"metadata":{},"cell_type":"markdown","source":"<h2 style='background:lightblue; border:0; color:black'><center>Advanced methods</center></h2>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nscaler.fit(ts.take([ 2], axis=1))\nscaled_train_data = scaler.transform(ts.take([2], axis=1)[0:1004])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_input = 12\nn_features= 1\ngenerator = TimeseriesGenerator(scaled_train_data, scaled_train_data, length=n_input, batch_size=1)\n\n\nlstm_model = Sequential()\nlstm_model.add(LSTM(200, activation='relu', input_shape=(n_input, n_features)))\nlstm_model.add(Dense(1))\nlstm_model.compile(optimizer='adam', loss='mse')\n\nlstm_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_model.fit_generator(generator,epochs=20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 7.5))\nlosses_lstm = lstm_model.history.history['loss']\nplt.figure(figsize=(12,4))\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.xticks(np.arange(0,21,1))\nplt.plot(range(len(losses_lstm)),losses_lstm);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_predictions_scaled = list()\n\nbatch = scaled_train_data[-n_input:]\ncurrent_batch = batch.reshape((1, n_input, n_features))\n\nfor i in range(30):   \n    lstm_pred = lstm_model.predict(current_batch)[0]\n    lstm_predictions_scaled.append(lstm_pred) \n    current_batch = np.append(current_batch[:,1:,:],[[lstm_pred]],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_predictions = scaler.inverse_transform(lstm_predictions_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data=pd.DataFrame()\ntest_data['LSTM_Predictions'] = lstm_predictions.reshape(30)\ntest_data.index=ts.index[1004:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 7.5))\nplt.errorbar(x,test_data['LSTM_Predictions'],fmt='o')\nplt.errorbar(x,ts['item_box'][1004:], label='actual', fmt='o', color='r')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style='background:lightblue; border:0; color:black'><center>Comparison</center></h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return math.sqrt(mse(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse(ts['item_box'][1004:],test_data['LSTM_Predictions'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse(ts['item_box'][1004:],forecast[1004:])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}