{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-24T13:34:04.845041Z","iopub.execute_input":"2022-01-24T13:34:04.845841Z","iopub.status.idle":"2022-01-24T13:34:04.8767Z","shell.execute_reply.started":"2022-01-24T13:34:04.845743Z","shell.execute_reply":"2022-01-24T13:34:04.876081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PREDICT FUTURE SALES\n## INDEX\n1. Introduction\n2. Import Libraries\n3. Data Overwiew\n4. Exploratory Data Analysis\n* Basic Analysis\n* Analysis On Date\n* Analysis on Number of Units Sold(item_cnt_day)\n* Analysis On Item Price\n* Q. How does sales over each day looks like?\n* Q. Do price of an item changer over time?\n* Analysis On Shop_id, item_id and item_category_id\n* Q. Do all item ids and shop ids present in train is present in test?\n* Q. Which shop id, item id and item category id have maximum sales?\n* Q. Are there any items which are more than in one category.ie Do an item belongs to single category or not?\n* Q. What about Monthly Sales?\n* Q. Do all the shops sold items on all months from 2013 Jan to 2015 Oct?\n5. Modeling\n\n* Train validation Split\n* Checking Coorelation Among Train Features\n* Baseline models\n\n* Random Forest Model\n* Xgboost Model\n* Submission","metadata":{}},{"cell_type":"markdown","source":"# 1. Introduction\n## Understanding Business Objective\nHere we will work with a challenging time-series dataset consisting of daily sales data, provided by one of the largest Russian software firms - 1C Company. We are given sales for 34 months from 2013-Jan to 2015 October.We need to predict no of each of the item that will gets sold in the month of november 2015 for given shop.ie, We will have a shop_id(unique identifier of a shop) and an item_id(unique identifier of an item) and we have to predict the number of units that item will gets sold in the month of november.\n\n\nNote: Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n\n\n# 2. Importing Libraries\n","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport matplotlib.gridspec as gridspec\nfrom termcolor import colored\nimport gc\n\nplt.style.use('seaborn-whitegrid')\nimport warnings\nwarnings.simplefilter(\"ignore\")\npd.set_option('display.float_format', lambda x: '%.2f' % x)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:38:04.165662Z","iopub.execute_input":"2022-01-24T13:38:04.166451Z","iopub.status.idle":"2022-01-24T13:38:05.164756Z","shell.execute_reply.started":"2022-01-24T13:38:04.166398Z","shell.execute_reply":"2022-01-24T13:38:05.163892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Overview\nWe have mainly 5 files:","metadata":{}},{"cell_type":"markdown","source":"1. train.csv  -> showing item price and number of items sold on each date. \n2. shop.csv -> Details of shops corresponding to shop id in train.csv.\n3. item.csv -> Details of items corresponding to item id in train.csv\n4. item_categories.csv -> Details of item category corresponding to category id in item.csv\n5. test.csv -> test data for prediction\n\nLet us give an glimpse on each of the files\n\ntrain.csv\n--------\n\n- date: date in format dd/mm/yy.\n- date_block_num: a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1 and so on.\n- shop_id: unique identifier of a shop.\n- item_id: unique identifier of a product.\n- item_price: current price of an item.\n- item_cnt_day: number of products sold. We are predicting a monthly amount of this measure.\n\n\nshop.csv\n--------\n- shop_name: shop name corresponding to shop id in train.csv\n- shop_id\n\nitem.csv\n--------\n- item_name: item name corresponding to item id in train.csv\n- item_id\n- item_category_id: category id of item\n\n\nitem_category.csv\n-----------------\n- item_category_name: category name of item cooresponding to item_category_id in item.csv \n- item_category_id\n\ntest.csv\n----------\n- ID - an Id that represents a (Shop, Item) tuple within the test set\n- shop_id\n- item_id","metadata":{}},{"cell_type":"markdown","source":"# 4. Exploratory Data Analysis\nBasic Analysis","metadata":{}},{"cell_type":"code","source":"# file paths\nDIR_PATH = '/kaggle/input/competitive-data-science-predict-future-sales'\nTRAIN_SALES_CSV = '/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv'\nSHOPS_CSV = '/kaggle/input/competitive-data-science-predict-future-sales/shops.csv'\nITEMS_CSV= '/kaggle/input/competitive-data-science-predict-future-sales/items.csv'\nITEM_CATEGORY_CSV = '/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv'\nTEST_CSV = '/kaggle/input/competitive-data-science-predict-future-sales/test.csv'","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:40:00.355174Z","iopub.execute_input":"2022-01-24T13:40:00.355478Z","iopub.status.idle":"2022-01-24T13:40:00.360491Z","shell.execute_reply.started":"2022-01-24T13:40:00.355447Z","shell.execute_reply":"2022-01-24T13:40:00.359716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_data(df_train,heading='TRAIN DATA'):\n    \n    \"\"\"\n    function which gives basic data information\n    Args:\n        df_train: pandas dataframe\n        heading: deading to display\n    Returns:\n        None\n    \"\"\"\n\n    print(colored(heading,'red'))\n    print('')\n    print('Date shape')\n    print(f'shape:{df_train.shape}')\n    print('')\n    print('--'*50)\n    print('')\n    print('Sample:')\n    print(df_train.head(3).to_markdown())\n    print('')\n    print('--'*50)\n    print('')\n    print('Columns and data types:')\n    print('')\n    print(df_train.info())","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:40:15.596021Z","iopub.execute_input":"2022-01-24T13:40:15.596565Z","iopub.status.idle":"2022-01-24T13:40:15.604959Z","shell.execute_reply.started":"2022-01-24T13:40:15.596514Z","shell.execute_reply":"2022-01-24T13:40:15.603818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(TRAIN_SALES_CSV)\ndf_shop = pd.read_csv(SHOPS_CSV)\ndf_item = pd.read_csv(ITEMS_CSV)\ndf_itemcat = pd.read_csv(ITEM_CATEGORY_CSV)\n\nshow_data(df_train,heading='TRAIN DATA')\nprint('')\nprint('__'*40)\nprint('')\nshow_data(df_shop,heading='SHOP DATA')\nprint('')\nprint('__'*40)\nprint('')\nshow_data(df_item,heading='ITEM DETAILS DATA')\nprint('__'*40)\nprint('')\nshow_data(df_itemcat,heading='ITEM CATEGORY DATA')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:40:23.976929Z","iopub.execute_input":"2022-01-24T13:40:23.977485Z","iopub.status.idle":"2022-01-24T13:40:26.790439Z","shell.execute_reply.started":"2022-01-24T13:40:23.97743Z","shell.execute_reply":"2022-01-24T13:40:26.788694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(TEST_CSV)\nshow_data(df_test,heading='TEST DATA')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:40:38.764639Z","iopub.execute_input":"2022-01-24T13:40:38.764895Z","iopub.status.idle":"2022-01-24T13:40:38.910541Z","shell.execute_reply.started":"2022-01-24T13:40:38.764867Z","shell.execute_reply":"2022-01-24T13:40:38.909707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Observation:\nAs discussed earlier for test data we just have shop id and item id. Train data is spread over 4 files. From above samples we know that there are common ids on multiple files.It will be great to move forward once we merge those train files based on common id.","metadata":{}},{"cell_type":"code","source":"### Merging all dataframes together\ndff = df_train.merge(df_item,on=\"item_id\")\ndff = dff.merge(df_itemcat,on=\"item_category_id\")\ndff = dff.merge(df_shop,on=\"shop_id\")\ndff = dff.drop(columns=[\"item_name\"])\ndff.to_csv('merged_original.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:41:02.449731Z","iopub.execute_input":"2022-01-24T13:41:02.450046Z","iopub.status.idle":"2022-01-24T13:41:26.142992Z","shell.execute_reply.started":"2022-01-24T13:41:02.450011Z","shell.execute_reply":"2022-01-24T13:41:26.141199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#masking a copy\ndf = dff.copy()\n# df.to_csv('merged_original.csv',index=False)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:41:26.147825Z","iopub.execute_input":"2022-01-24T13:41:26.150178Z","iopub.status.idle":"2022-01-24T13:41:26.727297Z","shell.execute_reply.started":"2022-01-24T13:41:26.150117Z","shell.execute_reply":"2022-01-24T13:41:26.726319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:41:26.728512Z","iopub.execute_input":"2022-01-24T13:41:26.728806Z","iopub.status.idle":"2022-01-24T13:41:27.241586Z","shell.execute_reply.started":"2022-01-24T13:41:26.728765Z","shell.execute_reply":"2022-01-24T13:41:27.240732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok. Now we have our merged data. Let us go through each of the features\n\n\n## Analysis On Date","metadata":{}},{"cell_type":"code","source":"df[\"date\"]=  pd.to_datetime(df[\"date\"], format='%d.%m.%Y')\ndf.sort_values(by=\"date\", ascending=True, inplace=True)\nprint(f'Minimum data present: {df[\"date\"].min()}')\nprint(f'Maximum date present: {df[\"date\"].max()}')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:43:13.685956Z","iopub.execute_input":"2022-01-24T13:43:13.686966Z","iopub.status.idle":"2022-01-24T13:43:15.302994Z","shell.execute_reply.started":"2022-01-24T13:43:13.686904Z","shell.execute_reply":"2022-01-24T13:43:15.302087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sorting dataframe based on date\ndf = df.sort_values(by='date').reset_index(drop=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:43:19.967215Z","iopub.execute_input":"2022-01-24T13:43:19.967782Z","iopub.status.idle":"2022-01-24T13:43:20.503583Z","shell.execute_reply.started":"2022-01-24T13:43:19.967739Z","shell.execute_reply":"2022-01-24T13:43:20.502764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation:\n\n- Our 'date' was not in date time format.We have converted it to datetime and sorted our dataframe based on that.","metadata":{}},{"cell_type":"markdown","source":"## Analysis on number of units sold (item_cnt_day)\nitem_cnt_day is the number of units of that item sold in that shop on a particular day.","metadata":{}},{"cell_type":"code","source":"fig,axes = plt.subplots(1,2,figsize=(15,5))\nsns.boxplot(df['item_cnt_day'],ax=axes[0])\naxes[0].set_title('Boxplot')\nsns.distplot(df['item_cnt_day'],ax=axes[1])\naxes[1].set_title('Distribution')\nplt.suptitle('No of units sold(Item Cnt day)',fontsize=\"20\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:43:53.824831Z","iopub.execute_input":"2022-01-24T13:43:53.825158Z","iopub.status.idle":"2022-01-24T13:44:04.383285Z","shell.execute_reply.started":"2022-01-24T13:43:53.825124Z","shell.execute_reply":"2022-01-24T13:44:04.38235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['item_cnt_day'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:44:04.384744Z","iopub.execute_input":"2022-01-24T13:44:04.38501Z","iopub.status.idle":"2022-01-24T13:44:04.46167Z","shell.execute_reply.started":"2022-01-24T13:44:04.384982Z","shell.execute_reply":"2022-01-24T13:44:04.460761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Minimum value {df[\"item_cnt_day\"].min()}')\nprint(f'Maximum value {df[\"item_cnt_day\"].max()}')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:44:31.331537Z","iopub.execute_input":"2022-01-24T13:44:31.331808Z","iopub.status.idle":"2022-01-24T13:44:31.346635Z","shell.execute_reply.started":"2022-01-24T13:44:31.331779Z","shell.execute_reply":"2022-01-24T13:44:31.345759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us print percentile values\nfor i in range(0,101,10):\n    print(f'{i}th percentile value for item_cnt_day is {np.percentile(df[\"item_cnt_day\"],i)}')\n    \nprint('--'*50)\n\nfor i in range(90,100):\n    print(f'{i}th percentile value for item_cnt_day is {np.percentile(df[\"item_cnt_day\"],i)}')\n    \nprint('--'*50)\n\nfor i in range(1,10):\n    k = 99 + i/10 \n    print(f'{k}th percentile value for item_cnt_day is {np.percentile(df[\"item_cnt_day\"],k)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:44:41.464465Z","iopub.execute_input":"2022-01-24T13:44:41.464985Z","iopub.status.idle":"2022-01-24T13:44:42.098917Z","shell.execute_reply.started":"2022-01-24T13:44:41.464919Z","shell.execute_reply":"2022-01-24T13:44:42.097724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will remove some extreme out layers\ndf[df['item_cnt_day'] > df['item_cnt_day'].quantile(0.95)]","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:44:48.018071Z","iopub.execute_input":"2022-01-24T13:44:48.018384Z","iopub.status.idle":"2022-01-24T13:44:48.101047Z","shell.execute_reply.started":"2022-01-24T13:44:48.018353Z","shell.execute_reply":"2022-01-24T13:44:48.10013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['item_cnt_day'] < 0]","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:44:55.983813Z","iopub.execute_input":"2022-01-24T13:44:55.984665Z","iopub.status.idle":"2022-01-24T13:44:56.011747Z","shell.execute_reply.started":"2022-01-24T13:44:55.98462Z","shell.execute_reply":"2022-01-24T13:44:56.010986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'shape of data after before outliers: {df.shape}')\ndf = df[df['item_cnt_day'] >= 0]\nupper_quantile = df['item_cnt_day'].quantile(0.95)\nprint(f'Removing values greater that upper_quantile {upper_quantile} and less than 0')\ndf['item_cnt_day'] = np.where(df['item_cnt_day'] > upper_quantile, upper_quantile, df['item_cnt_day'])\n\nprint(f'shape of data after removing outliers: {df.shape}')\nprint(f'Minimum units of product sold a time {df[\"item_cnt_day\"].min()}')\nprint(f'Maximum units of product sold a time {df[\"item_cnt_day\"].max()}')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:45:03.036375Z","iopub.execute_input":"2022-01-24T13:45:03.036656Z","iopub.status.idle":"2022-01-24T13:45:03.250359Z","shell.execute_reply.started":"2022-01-24T13:45:03.036627Z","shell.execute_reply":"2022-01-24T13:45:03.249326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation:\n- Most of the items are only sold 1 in quantity.Even 75th percentile value is 1.\n- We have some values less that zeros which ideally should not present. ie, no of units sold cannot be less than or equal to zero.Also we have some extreme outliers. After some analysis, we decide to remove all those values > 0.95 percentile.","metadata":{}},{"cell_type":"markdown","source":"## Analysis On Item Price","metadata":{}},{"cell_type":"code","source":"fig,axes = plt.subplots(1,2,figsize=(15,5))\nsns.boxplot(df['item_price'],ax=axes[0])\naxes[0].set_title('Boxplot')\nsns.distplot(df['item_price'],ax=axes[1])\naxes[1].set_title('Distribution')\nplt.suptitle('Item Price per unit',fontsize=\"20\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:45:35.000729Z","iopub.execute_input":"2022-01-24T13:45:35.001063Z","iopub.status.idle":"2022-01-24T13:45:45.426682Z","shell.execute_reply.started":"2022-01-24T13:45:35.001027Z","shell.execute_reply":"2022-01-24T13:45:45.425709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['item_price'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:45:50.903303Z","iopub.execute_input":"2022-01-24T13:45:50.903821Z","iopub.status.idle":"2022-01-24T13:45:51.0163Z","shell.execute_reply.started":"2022-01-24T13:45:50.903789Z","shell.execute_reply":"2022-01-24T13:45:51.015415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us print percentile values\nfor i in range(0,101,10):\n    print(f'{i}th percentile value for item_price is {np.percentile(df[\"item_price\"],i)}')\n    \nprint('--'*50)\n\nfor i in range(90,100):\n    print(f'{i}th percentile value for item_price is {np.percentile(df[\"item_price\"],i)}')\n    \nprint('--'*50)\n\nfor i in range(1,10):\n    k = 99 + i/10 \n    print(f'{k}th percentile value for item_price is {np.percentile(df[\"item_price\"],k)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:45:58.528019Z","iopub.execute_input":"2022-01-24T13:45:58.528733Z","iopub.status.idle":"2022-01-24T13:45:59.596811Z","shell.execute_reply.started":"2022-01-24T13:45:58.528696Z","shell.execute_reply":"2022-01-24T13:45:59.595746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we have an extreme outlier value in item price. Let us remove it\n\nprint(f'shape of data before removing outliers: {df.shape}')\ndf = df[df['item_price'] >= 0]\nupper_quantile = df['item_price'].quantile(0.95)\ndf['item_price'] = np.where(df['item_price'] > upper_quantile, upper_quantile, df['item_price'])\nprint(f'shape of data after removing outliers: {df.shape}')\n\nprint(f'Minimum price of a single item {df[\"item_price\"].min()}')\nprint(f'Maximum price ofa single item {df[\"item_price\"].max()}')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:46:06.564705Z","iopub.execute_input":"2022-01-24T13:46:06.566205Z","iopub.status.idle":"2022-01-24T13:46:06.864441Z","shell.execute_reply.started":"2022-01-24T13:46:06.566115Z","shell.execute_reply":"2022-01-24T13:46:06.863221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation:\n- We have some item price less that zeros which ideally should not present.\n- Also we have some extreme outliers. After some analysis, we decide to remove all those values > 0.95 percentile similar to item_cnt_day.\n- After removing outliers, We have some products with price low as 0.07(Might be some chocolate). Also we have some item which costs about 2683(Might be a chair)","metadata":{}},{"cell_type":"markdown","source":"Question:\nDo price of an item develop over time?","metadata":{}},{"cell_type":"markdown","source":"Before getting in to let us take a sample of items which are getting sold on each day. Also it is important to note that there might be price change for the same item in different shops.So let us consider price development of sample of items in the same shop.","metadata":{}},{"cell_type":"code","source":"df_tmp = df[df['shop_id'] == 31][['date','item_id','item_price']].reset_index(drop=True)\nitems = df_tmp['item_id'].unique()[0:8]\n\nfig,axes = plt.subplots(1,1,figsize=(25,8))\ncolors = ['red','orange','blue','green','yellow','purple','cyan','brown']\nfor i,item in enumerate(items):\n    dprice = df_tmp[df_tmp['item_id'] == item][['item_price','date']]\n    \n    sns.lineplot(x=dprice['date'],y=dprice['item_price'],ax=axes,color=colors[i],label=item)\n    \naxes.set_title('Price development of items - shop_id 31',fontsize=\"28\")\naxes.legend()\nplt.show()\n\n\ndf_tmp = df[df['shop_id'] == 28][['date','item_id','item_price']].reset_index(drop=True)\nitems = df_tmp['item_id'].unique()[0:8]\n\nfig,axes = plt.subplots(1,1,figsize=(25,8))\ncolors = ['red','orange','blue','green','yellow','purple','cyan','brown']\nfor i,item in enumerate(items):\n    dprice = df_tmp[df_tmp['item_id'] == item][['item_price','date']]\n    \n    sns.lineplot(x=dprice['date'],y=dprice['item_price'],ax=axes,color=colors[i],label=item)\n    \naxes.set_title('Price development of items - shop_id 28',fontsize=\"28\")\naxes.legend()\nplt.show()\n\n\n\ndf_tmp = df[df['shop_id'] == 21][['date','item_id','item_price']].reset_index(drop=True)\nitems = df_tmp['item_id'].unique()[0:8]\n\nfig,axes = plt.subplots(1,1,figsize=(25,8))\ncolors = ['red','orange','blue','green','yellow','purple','cyan','brown']\nfor i,item in enumerate(items):\n    dprice = df_tmp[df_tmp['item_id'] == item][['item_price','date']]\n    \n    sns.lineplot(x=dprice['date'],y=dprice['item_price'],ax=axes,color=colors[i],label=item)\n    \naxes.set_title('Price development of items - shop_id 21',fontsize=\"28\")\naxes.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:46:51.901769Z","iopub.execute_input":"2022-01-24T13:46:51.902688Z","iopub.status.idle":"2022-01-24T13:46:53.976914Z","shell.execute_reply.started":"2022-01-24T13:46:51.902639Z","shell.execute_reply":"2022-01-24T13:46:53.976092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation:\n- We can see that price of the same item is not same over the time(for some items).\n- Some items are also not selling on shops after some time.(might have stopped selling those product)\n- It increases and decreases over time.\n- During feature engineering it may be useful, if we can flag some items whose price remain constant over time\n- Here we can see that item price is dependent on time. This is the beauty of time series.","metadata":{}},{"cell_type":"markdown","source":"Question:\nHow does sales over each day looks like?\n\nLet us see how sales distribution looks for a single day. Before that let us create a new feature which is the turn over for an item.We can consider it as total sales for an item from a particular shop on a particular day.","metadata":{}},{"cell_type":"code","source":"# creating a new feature\ndf['Sales_per_item'] = df['item_cnt_day'] * df['item_price']","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:59:44.56272Z","iopub.execute_input":"2022-01-24T13:59:44.563168Z","iopub.status.idle":"2022-01-24T13:59:44.579676Z","shell.execute_reply.started":"2022-01-24T13:59:44.563099Z","shell.execute_reply":"2022-01-24T13:59:44.578616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(25,7))\ngs = fig.add_gridspec(1, 3)\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax02 = fig.add_subplot(gs[0,2])\n# setting size of xlabel and ylabel\nax00.tick_params(axis='both', labelsize=15)\nax01.tick_params(axis='both', labelsize=15)\nax02.tick_params(axis='both', labelsize=15)\nax00.set_title('Sales per item', fontsize=20)\nax01.set_title('Item price distribution', fontsize=20)\nax02.set_title('Item count distribution', fontsize=20)\nsns.histplot(data = df ,x=\"Sales_per_item\", kde=True, bins=50,ax=ax00, color=\"violet\")\nsns.histplot(data = df ,x=\"item_price\", kde=True, ax=ax01, bins=50, color=\"tomato\")\nsns.histplot(data = df ,x=\"item_cnt_day\", kde=False, ax=ax02, bins=20, color=\"cornflowerblue\")\n\nfig.subplots_adjust(top=0.8)\nfig.suptitle('Sales Feature Distributions per Day', fontsize=\"28\");","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:59:52.996691Z","iopub.execute_input":"2022-01-24T13:59:52.997503Z","iopub.status.idle":"2022-01-24T14:00:17.044022Z","shell.execute_reply.started":"2022-01-24T13:59:52.997446Z","shell.execute_reply":"2022-01-24T14:00:17.042904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation:\n- We are looking at sales figure on daily basis. It is different from our target distribution\n- In most of the days only 1 item is sold.\n- We are also getting a clear idea on price range(after removing top 5% items.)","metadata":{}},{"cell_type":"markdown","source":"Analysis On Shop_id, item_id and item_category_id\n\nQuestion:\n* Do all shop id present in test data present in train data and viceversa?\n* Do all item id present in test data present in train data and viceversa?\n* Do all shop id - item id pairs present in test data present in train data and viceversa?","metadata":{}},{"cell_type":"code","source":"train_shop_ids = set(df['shop_id'].unique())\ntest_shop_ids = set(df_test['shop_id'].unique())\n\ntrain_item_ids = set(df['item_id'].unique())\ntest_item_ids = set(df_test['item_id'].unique())\n\nprint(f'There are about {len(train_shop_ids)} unique shop ids in train data and {len(test_shop_ids)} shop ids in test data')\nprint(f'There are about {len(train_item_ids)} unique item ids in train data and {len(test_item_ids)} item ids in test data')\nprint('--'*50)\n\ndf['pair'] = df[['shop_id','item_id']].apply(lambda x: str(x['shop_id'])+'_'+str(x['item_id']),axis=1)\ndf_test['pair'] = df_test[['shop_id','item_id']].apply(lambda x: str(x['shop_id'])+'_'+str(x['item_id']),axis=1)\ntrain_pair_ids = set(df['pair'].unique())\ntest_pair_ids = set(df_test['pair'].unique())\n\nprint(f'There are {len(train_shop_ids - test_shop_ids)} shop ids present in train data which are not in test data')\nprint(f'There are {len(train_item_ids - test_item_ids)} item ids present in train data which are not in test data')\nprint(f'There are {len(train_pair_ids - test_pair_ids)} shop id item id pairs present in train data which are not in test data')\n\nprint('--'*50)\n\nprint(f'There are {len(test_item_ids - train_item_ids)} item ids present in test data which are not in train data')\nprint(f'There are {len(test_shop_ids - train_shop_ids)} shop ids present in test data which are not in train data')\nprint(f'There are {len(test_pair_ids - train_pair_ids)} shop id item id pairs present in test data which are not in train data')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:00:50.320895Z","iopub.execute_input":"2022-01-24T14:00:50.321242Z","iopub.status.idle":"2022-01-24T14:02:04.240814Z","shell.execute_reply.started":"2022-01-24T14:00:50.321207Z","shell.execute_reply":"2022-01-24T14:02:04.239707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation:\n- We have about 60 unique shop ids 21807 unique item ids present in train data\n- It is important to note that about 363 item ids present in test data is not present in train data. So we have approximatly 363*60(no of unique shop ids)=102796 pairs which are note present in train data. So we can say that our model needs to be robust to capture this unseen patterns.\n\nQuestion:\nWhich shop id, item id and item category id have maximum sales?","metadata":{}},{"cell_type":"code","source":"print(f'Total number of unique shop ids: {df[\"shop_id\"].nunique()}')\ndf_tmp = df[[\"shop_id\",\"Sales_per_item\",\"item_cnt_day\"]]\ndf_tmp= pd.pivot_table(data=df_tmp,index=[\"shop_id\"],aggfunc={\"item_cnt_day\":np.sum,\"Sales_per_item\":np.sum}).reset_index()\n\n\nfig, axes = plt.subplots(2,1,figsize=(20,10))\nsns.barplot(x=df_tmp[\"shop_id\"],y=df_tmp[\"item_cnt_day\"],ax=axes[0])\naxes[0].set_title(\"Total number of units sold among various shops\")\nsns.barplot(x=df_tmp[\"shop_id\"],y=df_tmp[\"Sales_per_item\"],ax=axes[1])\naxes[1].set_title('Total turn over in various shops')\nplt.suptitle('Shop id', fontsize=\"28\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:03:19.284578Z","iopub.execute_input":"2022-01-24T14:03:19.284867Z","iopub.status.idle":"2022-01-24T14:03:21.674711Z","shell.execute_reply.started":"2022-01-24T14:03:19.284837Z","shell.execute_reply":"2022-01-24T14:03:21.673666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Total number of unique item ids: {df[\"item_id\"].nunique()}')\n\ndf_tmp = df[[\"item_id\",\"Sales_per_item\",\"item_cnt_day\"]]\ndf_tmp= pd.pivot_table(data=df_tmp,index=[\"item_id\"],aggfunc={\"item_cnt_day\":np.sum,\"Sales_per_item\":np.sum}).reset_index()\ndf_tmp_sales = df_tmp.sort_values(by=['Sales_per_item'],ascending=False).head(50).reset_index(drop=True)\ndf_tmp_count = df_tmp.sort_values(by=['item_cnt_day'],ascending=False).head(50).reset_index(drop=True)\n\nfig, axes = plt.subplots(2,1,figsize=(20,15))\nsns.barplot(x=df_tmp_count[\"item_id\"],y=df_tmp_count[\"item_cnt_day\"],ax=axes[0])\naxes[0].set_title(\"Top selling items of no of units sold\")\naxes[0].set_xticklabels(axes[0].get_xticklabels(),rotation=45)\nsns.barplot(x=df_tmp_sales[\"item_id\"],y=df_tmp_sales[\"Sales_per_item\"],ax=axes[1])\naxes[1].set_title('Top selling items in terms of Turn over')\naxes[1].set_xticklabels(axes[1].get_xticklabels(),rotation=45)\nplt.suptitle('Item id', fontsize=\"28\")\nplt.show()\n# del df_tmp,df_tmp_count,df_tmp_sales","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:03:32.171649Z","iopub.execute_input":"2022-01-24T14:03:32.171972Z","iopub.status.idle":"2022-01-24T14:03:34.178478Z","shell.execute_reply.started":"2022-01-24T14:03:32.171925Z","shell.execute_reply":"2022-01-24T14:03:34.177484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Total number of unique item categorical ids: {df[\"item_category_id\"].nunique()}')\n\ndf_tmp = df[[\"item_category_id\",\"Sales_per_item\",\"item_cnt_day\"]]\ndf_tmp= pd.pivot_table(data=df_tmp,index=[\"item_category_id\"],aggfunc={\"item_cnt_day\":np.sum,\"Sales_per_item\":np.sum}).reset_index()\ndf_tmp_sales = df_tmp.sort_values(by=['Sales_per_item'],ascending=False).head(50).reset_index(drop=True)\ndf_tmp_count = df_tmp.sort_values(by=['item_cnt_day'],ascending=False).head(50).reset_index(drop=True)\n\nfig, axes = plt.subplots(2,1,figsize=(20,15))\nsns.barplot(x=df_tmp_count[\"item_category_id\"],y=df_tmp_count[\"item_cnt_day\"],ax=axes[0])\naxes[0].set_title(\"Top selling items of no of units sold\")\naxes[0].set_xticklabels(axes[0].get_xticklabels(),rotation=45)\nsns.barplot(x=df_tmp_sales[\"item_category_id\"],y=df_tmp_sales[\"Sales_per_item\"],ax=axes[1])\naxes[1].set_title('Top selling items in terms of Turn over')\naxes[1].set_xticklabels(axes[1].get_xticklabels(),rotation=45)\nplt.suptitle('Item Categorical id', fontsize=\"28\")\nplt.show()\ndel df_tmp,df_tmp_count,df_tmp_sales","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:03:43.531319Z","iopub.execute_input":"2022-01-24T14:03:43.531586Z","iopub.status.idle":"2022-01-24T14:03:45.110827Z","shell.execute_reply.started":"2022-01-24T14:03:43.531557Z","shell.execute_reply":"2022-01-24T14:03:45.109957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation:\n- We have maxium units sold as well as maxiumum sales in shop id 31.\n- We have maximum units sold for item 20949. But maximum sales in terms of Turn over is for item number 3732. Its more over like selling of chocolate and television.\n- Similarly for item categories, we have maximum number of unit sold for item_category_id 40 while maximum sales in terms of turn over occured to category id 19.\n\nQuestion:\nAre there any items which are more than in one category.ie Do an item belongs to single category or not?","metadata":{}},{"cell_type":"code","source":"item_categories = df['item_category_id'].unique()\ntmp = df[['item_id','item_category_id']].groupby(by=\"item_id\").nunique().reset_index()\ntmp.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:04:12.968649Z","iopub.execute_input":"2022-01-24T14:04:12.968925Z","iopub.status.idle":"2022-01-24T14:04:13.895272Z","shell.execute_reply.started":"2022-01-24T14:04:12.968894Z","shell.execute_reply":"2022-01-24T14:04:13.894338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp['item_category_id'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:04:19.845067Z","iopub.execute_input":"2022-01-24T14:04:19.845433Z","iopub.status.idle":"2022-01-24T14:04:19.853573Z","shell.execute_reply.started":"2022-01-24T14:04:19.845393Z","shell.execute_reply":"2022-01-24T14:04:19.852743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation:\n- We can see that there is only 1 category corresponding to a item_id\n\nQuestion:\nWhat about Monthly Sales?","metadata":{}},{"cell_type":"code","source":"#creating some new features\ndf[\"Year\"] = df[\"date\"].dt.year\ndf[\"Month\"] = df[\"date\"].dt.month\ndf[\"day_of_month\"] = df[\"date\"].dt.day\ndf[\"day_of_week\"] = df[\"date\"].dt.day_of_week","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:04:58.506639Z","iopub.execute_input":"2022-01-24T14:04:58.507063Z","iopub.status.idle":"2022-01-24T14:04:59.619245Z","shell.execute_reply.started":"2022-01-24T14:04:58.506982Z","shell.execute_reply":"2022-01-24T14:04:59.618239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes = plt.subplots(2,1,figsize=(25,12))\ndf_tmp = df[['date_block_num','Month','Sales_per_item']].groupby(by=['date_block_num']).aggregate(\"sum\").reset_index()\nsns.lineplot(x=df_tmp['date_block_num'],y=df_tmp['Sales_per_item'],ax=axes[0])\naxes[0].set_title('Total turn over (Total Sales)',fontsize=\"25\")\naxes[0].set_xlabel('Date',fontsize=\"20\")\naxes[0].set_ylabel('Turn over per month',fontsize=\"20\")\n\n\ndf_tmp = df[['date_block_num','Month','item_cnt_day']].groupby(by=['date_block_num']).aggregate(\"sum\").reset_index()\nsns.lineplot(x=df_tmp['date_block_num'],y=df_tmp['item_cnt_day'],ax=axes[1])\naxes[1].set_title('Total units sold',fontsize=\"25\")\naxes[1].set_xlabel('Date',fontsize=\"20\")\naxes[1].set_ylabel('Turn over per month',fontsize=\"20\")\n\nplt.tight_layout()\ndel df_tmp\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:05:05.939021Z","iopub.execute_input":"2022-01-24T14:05:05.939355Z","iopub.status.idle":"2022-01-24T14:05:06.937336Z","shell.execute_reply.started":"2022-01-24T14:05:05.93932Z","shell.execute_reply":"2022-01-24T14:05:06.93597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes = plt.subplots(1,2,figsize=(25,8))\ndf_tmp = df[['Year','Month','Sales_per_item']].pivot_table(index=['Month'],columns=['Year'],aggfunc={\"Sales_per_item\":np.sum})\naxes[0].plot(df_tmp)\naxes[0].set_title('Total turn over (Total Sales)')\naxes[0].legend(labels=[i[1] for i in df_tmp.columns])\n\ndf_tmp = df[['Year','Month','item_cnt_day']].pivot_table(index=['Month'],columns=['Year'],aggfunc={\"item_cnt_day\":np.sum})\naxes[1].plot(df_tmp)\naxes[1].set_title('Total no of units sold')\naxes[1].legend(labels=[i[1] for i in df_tmp.columns])\nplt.suptitle('Monthly Sales - Yearly',fontsize=\"28\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:05:14.679858Z","iopub.execute_input":"2022-01-24T14:05:14.680155Z","iopub.status.idle":"2022-01-24T14:05:15.465397Z","shell.execute_reply.started":"2022-01-24T14:05:14.680124Z","shell.execute_reply":"2022-01-24T14:05:15.464211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation:\n* We can see that number of products sold as well as total turnover over the months follow almost same pattern.\n* We have maximum sales in the month of november and december.\n* Since we are predicting sales for november 2015 , we expect a raise in no of units sold\n\nQuestion:\n* Do all the shops sold items on all months from 2013 Jan to 2015 Oct?\n* Do some shops is deactive on mean time and become active again?","metadata":{}},{"cell_type":"code","source":"df_tmp = df[['date_block_num','shop_id','item_cnt_day']]\ndf_tmp.groupby(by='date_block_num').aggregate({'shop_id':'nunique'}).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:06:22.255406Z","iopub.execute_input":"2022-01-24T14:06:22.255713Z","iopub.status.idle":"2022-01-24T14:06:22.853272Z","shell.execute_reply.started":"2022-01-24T14:06:22.255682Z","shell.execute_reply":"2022-01-24T14:06:22.852036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us take a sample of shops and plot the number of total items sold on each month. This gives an idea about the status of the shop in that month","metadata":{}},{"cell_type":"code","source":"df_tmp = df[['date_block_num','shop_id','item_cnt_day']]\ndt = pd.pivot_table(index='date_block_num',data=df_tmp,columns='shop_id',aggfunc=\"sum\").reset_index(drop=True)\ndt = dt.item_cnt_day\ndt.columns.name = 'Month'\ndt","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:06:42.469735Z","iopub.execute_input":"2022-01-24T14:06:42.470109Z","iopub.status.idle":"2022-01-24T14:06:42.702071Z","shell.execute_reply.started":"2022-01-24T14:06:42.470069Z","shell.execute_reply":"2022-01-24T14:06:42.700764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation:\n- We have a total of 60 shops.But all of them are note active during all the months\n- For example in case of shop id 9 , it is active only in 9th 21th and 33th month. Shop id 0 and shop id 1 is only active in first two months. Shop id 52 is active on all the months.(From above pivot table) - We can see that some of the shops are not active in certain months and again they become active - On feature engineering we can create lag average of sales on shops as a feature.","metadata":{}},{"cell_type":"markdown","source":"# 5. Modeling","metadata":{}},{"cell_type":"code","source":"def remove_outliers(df):\n    #remove outliers from item_cnt_day\n    df = df[df['item_cnt_day'] >= 0]\n    upper_quantile = df['item_cnt_day'].quantile(0.95)\n    df['item_cnt_day'] = np.where(df['item_cnt_day'] > upper_quantile, upper_quantile, df['item_cnt_day'])\n    \n    df = df[df['item_price'] >= 0]\n    upper_quantile = df['item_price'].quantile(0.95)\n    df['item_price'] = np.where(df['item_price'] > upper_quantile, upper_quantile, df['item_price'])\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:07:36.040862Z","iopub.execute_input":"2022-01-24T14:07:36.041329Z","iopub.status.idle":"2022-01-24T14:07:36.048954Z","shell.execute_reply.started":"2022-01-24T14:07:36.041269Z","shell.execute_reply":"2022-01-24T14:07:36.047915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load the merged data\ndf_train = pd.read_csv('merged_original.csv')\ndf_test = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\nprint(df_train.shape,df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:07:42.860755Z","iopub.execute_input":"2022-01-24T14:07:42.861238Z","iopub.status.idle":"2022-01-24T14:07:48.136242Z","shell.execute_reply.started":"2022-01-24T14:07:42.861195Z","shell.execute_reply":"2022-01-24T14:07:48.135522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing outliers as per our analysis\ndf_train = remove_outliers(df_train)\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:07:51.57806Z","iopub.execute_input":"2022-01-24T14:07:51.580777Z","iopub.status.idle":"2022-01-24T14:07:52.618241Z","shell.execute_reply.started":"2022-01-24T14:07:51.580725Z","shell.execute_reply":"2022-01-24T14:07:52.613873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clippping values as per kaggle discussion\ndf_train['item_cnt_day'] = df_train['item_cnt_day'].clip(0,20)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:07:57.333678Z","iopub.execute_input":"2022-01-24T14:07:57.334684Z","iopub.status.idle":"2022-01-24T14:07:57.408745Z","shell.execute_reply.started":"2022-01-24T14:07:57.334455Z","shell.execute_reply":"2022-01-24T14:07:57.407675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating train data\ndf = df_train[['date_block_num','shop_id','item_id','item_cnt_day']]\ndata = pd.pivot_table(data=df,index=['shop_id','item_id'],columns=['date_block_num'],fill_value=0,values='item_cnt_day',aggfunc=\"sum\")\ndata = data.reset_index()\ndata.columns.name = None\nprint(df.shape)\nprint(data.shape)\ndata.head(4)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:08:04.265096Z","iopub.execute_input":"2022-01-24T14:08:04.26583Z","iopub.status.idle":"2022-01-24T14:08:07.77437Z","shell.execute_reply.started":"2022-01-24T14:08:04.265777Z","shell.execute_reply":"2022-01-24T14:08:07.772796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generating test data\n#merge\ntest_data = df_test.drop(columns=['ID'])\ntest_data = test_data.merge(data,on=['shop_id','item_id'],how=\"left\")\ntest_data.fillna(0,inplace=True)\ntest_data = test_data.drop(columns=['shop_id','item_id'])\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:08:13.058884Z","iopub.execute_input":"2022-01-24T14:08:13.059429Z","iopub.status.idle":"2022-01-24T14:08:13.494108Z","shell.execute_reply.started":"2022-01-24T14:08:13.059388Z","shell.execute_reply":"2022-01-24T14:08:13.492968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train validation Split\nWe must be cautious while doing a train validation split. We have data from 2013 Jan to 2015 October and we need to predict Sales of month November.ie we have data of 34 months and we need to predict sales of 35th month.\n\n- Here we will take data from 1st month to 32nd month as train data (and we will predict 33nd month) \n- We will take data from 2nd month to 33nd month as validation data (and we will predict 34nd month) \n- Finally we will have data from 3rd to 34th month as test data","metadata":{}},{"cell_type":"code","source":"# train_data -> columns 0 to 32\nX_train = data.drop(columns=['shop_id','item_id',33,32],axis=1)\ny_train = data[32]\n\n# val_data -> columns 1 to 32\nX_val = data.drop(columns=['shop_id','item_id',0,33],axis=1)\ny_val = data[33]\n\n#test data-> columns 2 to 33\nX_test = test_data.drop(columns=[0,1])\n\nprint(X_train.shape,X_val.shape,X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:08:49.985069Z","iopub.execute_input":"2022-01-24T14:08:49.985439Z","iopub.status.idle":"2022-01-24T14:08:50.192023Z","shell.execute_reply.started":"2022-01-24T14:08:49.985405Z","shell.execute_reply":"2022-01-24T14:08:50.190792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking Coorelation Among Train Features","metadata":{}},{"cell_type":"code","source":"df_corr = X_train.corr()\nplt.figure(figsize=(20,20))\n\nsns.heatmap(df_corr,annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:09:08.270627Z","iopub.execute_input":"2022-01-24T14:09:08.27092Z","iopub.status.idle":"2022-01-24T14:09:14.429908Z","shell.execute_reply.started":"2022-01-24T14:09:08.27089Z","shell.execute_reply":"2022-01-24T14:09:14.428947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation:\n- We donot found any strong coorelated features\n\n### Baseline models\n### Random Forest\nAs of now we have enough information for going towards modeling. Let us create a baseline model to work with.We will go with a Random Forest model.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:13:01.443636Z","iopub.execute_input":"2022-01-24T14:13:01.444484Z","iopub.status.idle":"2022-01-24T14:13:01.866141Z","shell.execute_reply.started":"2022-01-24T14:13:01.444415Z","shell.execute_reply":"2022-01-24T14:13:01.864655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hyper parameter tuning and modeling","metadata":{}},{"cell_type":"code","source":"# from itertools import product\n# # params\n# max_depth = [2,5,10]\n# n_estimators = [50,100,150,300]\n# min_samples_split = [2,4]\n\n# params =  [max_depth,n_estimators,min_samples_split]\n# parameters = list(product(*params))\n# min_rmse = float('inf')\n# best_params = parameters[0]\n# for p in parameters:\n#     depth = p[0]\n#     estimators = p[1]\n#     min_sample_split = p[2]\n    \n#     print(f\"Fitting params -> max_depth: {depth},n_estimators: {estimators} , min_samples_split:{min_sample_split}\")\n#     model = RandomForestRegressor(random_state=42,max_depth=depth,n_estimators=estimators,min_samples_split=min_sample_split)\n#     model.fit(X_train,y_train)\n#     y_train_pred = model.predict(X_train)\n#     y_val_pred = model.predict(X_val)\n    \n#     train_rmse = mean_squared_error(y_train,y_train_pred,squared=False)\n#     val_rmse = mean_squared_error(y_val,y_val_pred,squared=False)\n    \n#     print(f'Train rmse: {train_rmse}')\n#     print(f'Val rmse: {val_rmse}')\n    \n#     if val_rmse < min_rmse:\n#         min_rmse = val_rmse\n#         best_params = p\n        \n#     print('--'*50)\n\n    \n# print(f'Found following best parameters: max_depth: {best_params[0]},\\\n# n_estimators: {best_params[1]} , min_samples_split:{best_params[2]} with validation loss {min_rmse}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model\n# parameters found by grid search\n\n\nmodel = RandomForestRegressor(random_state=42,max_depth=5,n_estimators=50,min_samples_split=4)\nmodel.fit(X_train,y_train)\ny_train_pred = model.predict(X_train)\ny_val_pred = model.predict(X_val)\ny_test_pred = model.predict(X_test)\n\ntrain_rmse = mean_squared_error(y_train,y_train_pred,squared=False)\nval_rmse = mean_squared_error(y_val,y_val_pred,squared=False)\n\nprint(f'Train rmse: {train_rmse}')\nprint(f'Val rmse: {val_rmse}')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:13:47.361173Z","iopub.execute_input":"2022-01-24T14:13:47.361519Z","iopub.status.idle":"2022-01-24T14:14:23.807998Z","shell.execute_reply.started":"2022-01-24T14:13:47.361481Z","shell.execute_reply":"2022-01-24T14:14:23.806701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Xgboost Model","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:14:23.810392Z","iopub.execute_input":"2022-01-24T14:14:23.810742Z","iopub.status.idle":"2022-01-24T14:14:23.912625Z","shell.execute_reply.started":"2022-01-24T14:14:23.810697Z","shell.execute_reply":"2022-01-24T14:14:23.911905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hyperparameter tuning (Random Search)","metadata":{}},{"cell_type":"code","source":"# import xgboost as xgb\n# from itertools import product\n# import random\n\n# #params\n# booster = 'gbtree'\n# silent = 0\n\n# eta = [0.01,0.1,0.2,0.3]  #learning rate in gbm\n# min_child_weight = [1,2,0.5]  #minimum sum of weights of all observations required in a child\n# max_depth = [3,6,8]  #maximum depth of the tree\n# gamma = [0,1]  #a split will occur only when reduction in loss function > gamma \n# subsample = [0.5,0.7,1] #fraction of rows to be sampled to make a decision tree\n# colsample_bytree = [0.5,0.7,1] #fraction of columns to be sampled to make a decision tree\n# # lambda_ = [0.4,0.8,1] #L2 regularization weights\n# alpha = [0,1]  #L1 reg weights\n\n\n\n# params =  [eta,min_child_weight,max_depth,gamma,subsample,colsample_bytree,alpha]\n# parameters = list(product(*params))\n# parameters = random.sample(parameters,100)\n# len(parameters)\n\n\n# min_rmse = float('inf')\n# best_params = parameters[0]\n# for p in parameters:\n#     eta = p[0]\n#     min_child_weight = p[1]\n#     max_depth = p[2]\n#     gamma = p[3]\n#     subsample = p[4]\n#     colsample_bytree = p[5]\n#     alpha = p[6]\n\n#     print('Random Search On Hyperparamters Xgboost')\n#     print(f\"Fitting params -> eta: {eta},\\\n# min_child_weight: {min_child_weight} , max_depth:{max_depth}, gamma: {gamma}, subsample : {subsample}, \\\n# col_sample_bytree: {colsample_bytree:},alpha: {alpha}\")\n#     model = xgb.XGBRegressor(random_state=42,booster = 'gbtree',verbosity=0,\n#                              eta=eta,min_child_weight=min_child_weight,max_depth=max_depth,gamma=gamma,\n#                              subsample=subsample,colsample_bytree=colsample_bytree,alpha=alpha)\n    \n#     model.fit(X_train,y_train)\n#     y_train_pred = model.predict(X_train)\n#     y_val_pred = model.predict(X_val)\n    \n#     train_rmse = mean_squared_error(y_train,y_train_pred,squared=False)\n#     val_rmse = mean_squared_error(y_val,y_val_pred,squared=False)\n    \n#     print(f'Train rmse: {train_rmse}')\n#     print(f'Val rmse: {val_rmse}')\n    \n#     if val_rmse < min_rmse:\n#         min_rmse = val_rmse\n#         best_params = p\n        \n#     print('--'*50)\n\n    \n# print(f'Found following best parameters: eta: {best_params[0]},\\\n# min_child_weight: {best_params[1]} , max_depth:{best_params[2]}, gamma: {best_params[3]}, subsample : {best_params[4]}, \\\n# col_sample_bytree: {best_params[5]},alpha: {best_params[6]} with validation loss {min_rmse}')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:14:39.172719Z","iopub.execute_input":"2022-01-24T14:14:39.173654Z","iopub.status.idle":"2022-01-24T14:14:39.180741Z","shell.execute_reply.started":"2022-01-24T14:14:39.173582Z","shell.execute_reply":"2022-01-24T14:14:39.18004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eta = 0.01\nmin_child_weight = 2\nmax_depth = 8\ngamma = 0\nsubsample  = 1\ncolsample_bytree = 0.5\nalpha = 1\n\nmodel = xgb.XGBRegressor(random_state=42,booster = 'gbtree',verbosity=0,\n                             eta=eta,min_child_weight=min_child_weight,max_depth=max_depth,gamma=gamma,\n                             subsample=subsample,colsample_bytree=colsample_bytree,alpha=alpha)\nmodel.fit(X_train,y_train)\ny_train_pred = model.predict(X_train)\ny_val_pred = model.predict(X_val)\ny_test_pred = model.predict(X_test)\n    \ntrain_rmse = mean_squared_error(y_train,y_train_pred,squared=False)\nval_rmse = mean_squared_error(y_val,y_val_pred,squared=False)\n    \nprint(f'Train rmse: {train_rmse}')\nprint(f'Val rmse: {val_rmse}')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:14:47.300421Z","iopub.execute_input":"2022-01-24T14:14:47.301491Z","iopub.status.idle":"2022-01-24T14:15:18.326002Z","shell.execute_reply.started":"2022-01-24T14:14:47.301432Z","shell.execute_reply":"2022-01-24T14:15:18.325186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({'ID':df_test['ID'],'item_cnt_month':y_test_pred})\nsubmission.to_csv('submission.csv',index=False)\nsubmission.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T14:15:26.430485Z","iopub.execute_input":"2022-01-24T14:15:26.430763Z","iopub.status.idle":"2022-01-24T14:15:27.102608Z","shell.execute_reply.started":"2022-01-24T14:15:26.430734Z","shell.execute_reply":"2022-01-24T14:15:27.101151Z"},"trusted":true},"execution_count":null,"outputs":[]}]}