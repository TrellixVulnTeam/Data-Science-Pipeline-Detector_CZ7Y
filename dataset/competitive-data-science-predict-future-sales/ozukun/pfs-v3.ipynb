{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom itertools import product\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\npd.options.display.float_format = '{:.2f}'.format","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"items = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\ncats = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\n\ntrain = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ntest  = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1001]\ntrain[\"item_cnt_day\"]=train[\"item_cnt_day\"].fillna(0)\n\ntrain = train[train.item_cnt_day>0]\nmedian = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0, 'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_nulls(df_train):\n    # check distinct values for the columns that contains null value\n    # remove nan values that is total count less than 5 percent\n    for d in df_train.columns:\n        if  df_train[d].isnull().values.any():\n            print(\"column \"+d)\n            if df_train[d].dtype.kind in 'bifc':\n                df_train[d].fillna(0,inplace = True)\n            else:\n                print(\"column \"+d)\n                df_train[d].fillna(\"NULL_VALUE\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\ncats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n# if subtype is nan then type\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# check shop_id pk or not for shops\ncheck_count = shops.groupby([\"shop_id\"])['shop_id'].size()\nprint(check_count[check_count > 1])\n# check item_category_id pk or not  for cats\ncheck_count = cats.groupby([\"item_category_id\"])['item_category_id'].size()\nprint(check_count[check_count > 1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_count = train.groupby([\"date\",\"date_block_num\",\"shop_id\",\"item_id\"])['shop_id'].size()\nprint(check_count[check_count > 1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.query(\"date_block_num==16 and shop_id==50 and item_id==3423\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"date_block_num\"]=34\ntest = pd.merge(test, shops, on=['shop_id'], how='left',suffixes=('', '_x'))\ntest = pd.merge(test, items, on=['item_id'], how='left',suffixes=('', '_y'))\ntest = pd.merge(test, cats, on=['item_category_id'], how='left')\n\ntest.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_count = test.groupby([\"shop_id\",\"item_id\"])['shop_id'].size()\nprint(check_count[check_count > 1])\n\n# test df PK: shop_id,item_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a dataframe for per month  cartesian of shop_id*item_id\nimport itertools\ntrain_2 = []\nfor i in range(34):\n    train_list_1 = train[train.date_block_num==i].date_block_num.unique().tolist()\n    train_list_2 = train[train.date_block_num==i].shop_id.unique().tolist()\n    train_list_3 = train[train.date_block_num==i].item_id.unique().tolist()\n\n    train_list_f1=list(itertools.product(train_list_1,train_list_2,train_list_3))\n    train_2.append(train_list_f1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# row to columns\ntrain_2 = pd.DataFrame(np.vstack(train_2) )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_2.rename(columns={0: \"date_block_num\", 1: \"shop_id\", 2: \"item_id\"} , inplace = True)\ntrain_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=['date_block_num','shop_id','item_id']\n\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\ntrain_2 = pd.merge(train_2, group, on=cols, how='left')\ntrain_2['item_cnt_month'] = (train_2['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_2 = pd.merge(train_2, shops, on=['shop_id'], how='left',suffixes=('', '_x'))\ntrain_2 = pd.merge(train_2, items, on=['item_id'], how='left',suffixes=('', '_y'))\ntrain_2 = pd.merge(train_2, cats, on=['item_category_id'], how='left')\n\n#train_2[\"revenue_d\"]=train_2[\"item_price\"]*train_2[\"item_cnt_day\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_count = train_2.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])['shop_id'].size()\nprint(check_count[check_count > 1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_2.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['shop_id','item_id']\ntrain_2 = pd.concat([train_2, test], ignore_index=True, sort=False, keys=cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_2.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sum_df(df,columns,target):\n    x= [] \n    name=target+\"_sum\"\n    data = columns.split(\",\")\n    for col in data:\n        x.append(col)\n    print(x)\n    return df.groupby(x) [target].sum().to_frame(name).reset_index()\n\ndef mean_df(df,columns,target):\n    x= [] \n    name=target+\"_mean\"\n    data = columns.split(\",\")\n    for col in data:\n        x.append(col)\n    print(x)\n    return df.groupby(x) [target].mean().to_frame(name).reset_index()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_2.describe()\n#train_2.rename(columns = {'date_block_num_x':'date_block_num'}, inplace = True) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_2 = lag_feature(train_2, [1,2,3,6,12], 'item_cnt_month')\ntrain_2.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_2.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\ntrain_2 = pd.merge(train_2, group, on=['date_block_num'], how='left')\ntrain_2['date_avg_item_cnt'] = train_2['date_avg_item_cnt'].astype(np.float16)\ntrain_2 = lag_feature(train_2, [1], 'date_avg_item_cnt')\ntrain_2.drop(['date_avg_item_cnt'], axis=1, inplace=True)\n\n\ngroup = train_2.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\ntrain_2 = pd.merge(train_2, group, on=['date_block_num','item_id'], how='left')\ntrain_2['date_item_avg_item_cnt'] = train_2['date_item_avg_item_cnt'].astype(np.float16)\ntrain_2 = lag_feature(train_2, [1,2,3,6,12], 'date_item_avg_item_cnt')\ntrain_2.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\n\n\ngroup = train_2.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\ntrain_2 = pd.merge(train_2, group, on=['date_block_num','shop_id'], how='left')\ntrain_2['date_shop_avg_item_cnt'] = train_2['date_shop_avg_item_cnt'].astype(np.float16)\ntrain_2 = lag_feature(train_2, [1,2,3,6,12], 'date_shop_avg_item_cnt')\ntrain_2.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\n\n\ngroup = train_2.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\ntrain_2 = pd.merge(train_2, group, on=['date_block_num','item_category_id'], how='left')\ntrain_2['date_cat_avg_item_cnt'] = train_2['date_cat_avg_item_cnt'].astype(np.float16)\ntrain_2 = lag_feature(train_2, [1], 'date_cat_avg_item_cnt')\ntrain_2.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ngroup = train_2.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\ntrain_2 = pd.merge(train_2, group, on=['date_block_num', 'city_code'], how='left')\ntrain_2['date_city_avg_item_cnt'] = train_2['date_city_avg_item_cnt'].astype(np.float16)\ntrain_2 = lag_feature(train_2, [1], 'date_city_avg_item_cnt')\ntrain_2.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\n\n\n\ngroup = train_2.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\ntrain_2 = pd.merge(train_2, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\ntrain_2['date_item_city_avg_item_cnt'] = train_2['date_item_city_avg_item_cnt'].astype(np.float16)\ntrain_2 = lag_feature(train_2, [1], 'date_item_city_avg_item_cnt')\ntrain_2.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\n\n\n\ngroup = train_2.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\ntrain_2 = pd.merge(train_2, group, on=['date_block_num', 'type_code'], how='left')\ntrain_2['date_type_avg_item_cnt'] = train_2['date_type_avg_item_cnt'].astype(np.float16)\ntrain_2 = lag_feature(train_2, [1], 'date_type_avg_item_cnt')\ntrain_2.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\n\n\n\n\ngroup = train_2.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\ntrain_2 = pd.merge(train_2, group, on=['date_block_num', 'subtype_code'], how='left')\ntrain_2['date_subtype_avg_item_cnt'] = train_2['date_subtype_avg_item_cnt'].astype(np.float16)\ntrain_2 = lag_feature(train_2, [1], 'date_subtype_avg_item_cnt')\ntrain_2.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_2.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\ntrain_2 = pd.merge(train_2, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\ntrain_2['date_shop_cat_avg_item_cnt'] = train_2['date_shop_cat_avg_item_cnt'].astype(np.float16)\ntrain_2 = lag_feature(train_2, [1], 'date_shop_cat_avg_item_cnt')\ntrain_2.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_2.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_month': ['median']})\ngroup.columns = ['date_shop_item_med_item_cnt']\ngroup.reset_index(inplace=True)\n\ntrain_2 = pd.merge(train_2, group, on=['date_block_num', 'shop_id', 'item_id'], how='left')\ntrain_2['date_shop_item_med_item_cnt'] = train_2['date_shop_item_med_item_cnt'].astype(np.float16)\ntrain_2 = lag_feature(train_2, [1,2,3,6,12], 'date_shop_item_med_item_cnt')\ntrain_2.drop(['date_shop_item_med_item_cnt'], axis=1, inplace=True)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_2.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['median']})\ngroup.columns = ['date_shop_med_item_cnt']\ngroup.reset_index(inplace=True)\n\ntrain_2 = pd.merge(train_2, group, on=['date_block_num', 'shop_id'], how='left')\ntrain_2['date_shop_med_item_cnt'] = train_2['date_shop_med_item_cnt'].astype(np.float16)\ntrain_2 = lag_feature(train_2, [1], 'date_shop_med_item_cnt')\ntrain_2.drop(['date_shop_med_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_2.groupby(['date_block_num', 'shop_id','item_category_id']).agg({'item_cnt_month': ['median']})\ngroup.columns = ['date_shop_cat_med_item_cnt']\ngroup.reset_index(inplace=True)\n\ntrain_2 = pd.merge(train_2, group, on=['date_block_num', 'shop_id','item_category_id'], how='left')\ntrain_2['date_shop_cat_med_item_cnt'] = train_2['date_shop_cat_med_item_cnt'].astype(np.float16)\ntrain_2 = lag_feature(train_2, [1], 'date_shop_cat_med_item_cnt')\ntrain_2.drop(['date_shop_cat_med_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_2.columns\n#train_2.drop(['item_cnt_month_y'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_2.groupby(['date_block_num', 'shop_id','item_id']) ['item_cnt_month'].quantile(.25)\ngroup.columns = ['date_shop_item_q25_item_cnt']\ndfx=pd.DataFrame()\ndfx[\"date_shop_item_q25_item_cnt\"]=group\n\nprint(train_2.columns)\ntrain_2 = pd.merge(train_2, dfx, on=['date_block_num', 'shop_id','item_id'], how='left')\ntrain_2['date_shop_item_q25_item_cnt'] = train_2['date_shop_item_q25_item_cnt'].astype(np.float16)\ntrain_2 = lag_feature(train_2, [1], 'date_shop_item_q25_item_cnt')\ntrain_2.drop(['date_shop_item_q25_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train_2.groupby(['date_block_num', 'shop_id','item_id']) ['item_cnt_month'].quantile(.75)\ngroup.columns = ['date_shop_item_q75_item_cnt']\ndfx=pd.DataFrame()\ndfx[\"date_shop_item_q75_item_cnt\"]=group\n\nprint(train_2.columns)\ntrain_2 = pd.merge(train_2, dfx, on=['date_block_num', 'shop_id','item_id'], how='left')\ntrain_2['date_shop_item_q75_item_cnt'] = train_2['date_shop_item_q75_item_cnt'].astype(np.float16)\ntrain_2 = lag_feature(train_2, [1], 'date_shop_item_q75_item_cnt')\ntrain_2.drop(['date_shop_item_q75_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_2['month'] = train_2['date_block_num'] % 12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\ntrain_2 = fill_na(train_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_2.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final=train_2[[\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code',\n    'subtype_code',\n    \n     'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3', 'item_cnt_month_lag_6', 'date_item_avg_item_cnt_lag_1', \n    'date_item_avg_item_cnt_lag_2', 'date_item_avg_item_cnt_lag_3', 'date_item_avg_item_cnt_lag_6', 'date_cat_avg_item_cnt_lag_1', \n    'date_item_city_avg_item_cnt_lag_1', 'date_subtype_avg_item_cnt_lag_1', \n    'date_shop_cat_avg_item_cnt_lag_1', 'date_shop_item_med_item_cnt_lag_1', \n    'date_shop_item_med_item_cnt_lag_2', 'date_shop_item_med_item_cnt_lag_3', \n    'date_shop_item_med_item_cnt_lag_6', 'date_shop_cat_med_item_cnt_lag_1',\n    'date_shop_item_q25_item_cnt_lag_1', 'date_shop_item_q75_item_cnt_lag_1'\n            ]]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def numeric_corr(df_train):\n    \n    # get the list of numeric columns  that has corr more than 0.3 or less than -0.3\n    f= df_train.corrwith(df_train.item_cnt_month, axis = 0)\n    lister12 = []\n    for i,r in f.items():\n        if r>0.2 or r<-0.2:\n            lister12.append(i)\n    return lister12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( numeric_corr(train_2) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check relation between predicted value with all categorical variables\ndef One_way_ANOVA(df):\n    lister12 = []\n    for col in df.columns:\n        if df[col].dtype.kind not in 'bifc':\n            import statsmodels.api as sm\n            from statsmodels.formula.api import ols\n            model = ols('item_cnt_month ~'+col,data=df).fit()\n            table = sm.stats.anova_lm(model, typ=2)\n            #print(col)\n            #print(table[\"PR(>F)\"][0])\n            if table[\"PR(>F)\"][0] < 0.05:\n                lister12.append(col)\n    return lister12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_check=train_2[(train_2.date_block_num<=5)]\ntrain_check['item_category_id_char']=train_check['item_category_id'].apply(str)\ntrain_check['type_code_char']=train_check['type_code'].apply(str)\ntrain_check['subtype_code_char']=train_check['subtype_code'].apply(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_check['subtype_code_char']\n#train_check[\"item_category_id\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"One_way_ANOVA(train_check[[\"city\",\"shop_name\",\"item_category_id_char\",\"subtype_code_char\",\"type_code_char\",\"item_cnt_month\"]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.rename(columns = {'item_cnt_month':'item_monthly_sum_x'}, inplace = True) \n\nfinal[\"item_monthly_sum_x\"] = final[\"item_monthly_sum_x\"]\n\n\nX_train = final[final.date_block_num < 33].drop(['item_monthly_sum_x'], axis=1)\nY_train = final[final.date_block_num < 33]['item_monthly_sum_x']\nX_valid = final[final.date_block_num == 33].drop(['item_monthly_sum_x'], axis=1)\nY_valid = final[final.date_block_num == 33]['item_monthly_sum_x']\n\n\n\nX_test = final[final.date_block_num == 34].drop(['item_monthly_sum_x'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.query('date_block_num == 34')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.metrics import r2_score\n\n\n\n'''\nmodel = xgb.XGBRegressor(    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n'''\n\nmodel = xgb.XGBRegressor()\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\n\n\"\"\" \nmodel.fit(X_train, Y_train)\nxgb_pred = (model.predict(X_valid).clip(0, 20))\n\nrmse = sqrt(mean_squared_error(Y_valid.clip(0, 20),  xgb_pred) )\nprint(\"RMSE RESULT \",rmse)\nprint(\"*******\")\n\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1.00553","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test = model.predict(X_test)\n#print(X_test)\n\n\nsubmission = pd.DataFrame({\n    \"ID\": test.ID, \n    \"item_cnt_month\": Y_test.clip(0, 20)\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}