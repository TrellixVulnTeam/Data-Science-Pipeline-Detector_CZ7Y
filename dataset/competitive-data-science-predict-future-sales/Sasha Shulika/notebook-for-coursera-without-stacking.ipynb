{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Modules import, reading data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom itertools import product\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom multiprocessing import Pool\n\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import style\nstyle.use('seaborn')\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv').set_index('ID')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\ncategories = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data investigation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"At first, let's drop duplicates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop_duplicates(inplace=True, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot features 'item_cnt_day' and 'item_price' to see their distributions and find outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train['item_cnt_day'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train['item_price'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's inspect rows with item_price 300,000 and -1 and rows with item_cnt_day 1000 and 2000+","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['item_cnt_day'].argmax()]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"items[items['item_id'] == 11373]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['item_id']==11373]['item_cnt_day'].values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's seems like an outlier. Item name is smth like \"Delivery to post office\" and we can see that item_cnt_day - 2169 is really too big value for it. So we will delete it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['item_cnt_day'] == 1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items[items['item_id'] == 20949]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['item_id']==20949]['item_cnt_day'].values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The name product is a package with some print. Item_cnt_day 1000 seems too big even for it, but I think it really possible to sell 1000 per day, so I'll keep it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.iloc[train['item_price'].argmax()]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"items[items['item_id'] == 6066]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test['item_id'] == 6066]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So this item is really costs so much, because it's some kind of corporative software. But we don't have this item in test set, so we can delete it form train set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.iloc[train['item_price'].argmin()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fill -1 price with mean item price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['item_price'].argmin(), 'item_price'] = train[train['item_id'] == 2973].item_price.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deleting outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train['item_cnt_day'] <= 1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train['item_price'] < 300000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['date_block_num', 'shop_id', 'item_id']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating training seet","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"But at firts take a look at shops info","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shops","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we generate train set, we can see that some shop names seems very similar. So let's fix shop_id values it  train and test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['shop_id'] == 0, 'shop_id'] = 57\ntest.loc[test['shop_id'] == 0, 'shop_id'] = 57\ntrain.loc[train['shop_id'] == 1, 'shop_id'] = 58\ntest.loc[test['shop_id'] == 1, 'shop_id'] = 58\ntrain.loc[train['shop_id'] == 10, 'shop_id'] = 11\ntest.loc[test['shop_id'] == 10, 'shop_id'] = 11\ntrain.loc[train['shop_id'] == 40, 'shop_id'] = 39\ntest.loc[test['shop_id'] == 40, 'shop_id'] = 39","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate training dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndata = []\nfor block in range(34):\n    tmp = train[train['date_block_num'] == block]\n    data.append(np.array(list(product([block], tmp['shop_id'].unique(), tmp['item_id'].unique())), dtype='int16'))\n\ndel tmp\n\ndata = pd.DataFrame(data=np.vstack(data), columns=cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Always downcast datatypes if possible","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['date_block_num'] = data['date_block_num'].astype('int8')\ndata['shop_id'] = data['shop_id'].astype('int8')\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sort_values(cols, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train.groupby(cols).agg({'item_cnt_day': 'sum'})\ngroup.columns = ['target']\ngroup.reset_index(inplace=True)\n\ndata = data.merge(group, how='left', on=cols)\n\ndata['target'] = data['target'].fillna(0).clip(0, 20).astype('float16')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TO HDF FILE\nVery basic version of dataset is ready. Saving it to hdf file for fast backup.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.to_hdf('data.hdf5', 'df')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_hdf('data.hdf5', 'df')\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some explorations of other files","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shops","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's extract city names from shop names","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shops['city_name'] = shops['shop_name'].apply(lambda x: x.split(' ')[0])\nshops.replace('!Якутск', 'Якутск', inplace=True)\nshops['city_name'], _ = pd.factorize(shops['city_name'])\nshops['city_name'] = shops['city_name'].astype('int8')\n\nshops.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also let's extract the first words from categories names","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categories['category_general_name'] = categories['item_category_name'].apply(lambda x: x.split(' ')[0])\ncategories['category_general_name'], _ = pd.factorize(categories['category_general_name'])\ncategories['category_general_name'] = categories['category_general_name'].astype('int8')\n\ncategories.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding test data to train data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test['date_block_num'] = 34\ntest['shop_id'] = test['shop_id'].astype('int8')\ntest['date_block_num'] = test['date_block_num'].astype('int8')\ntest['item_id'] = test['item_id'].astype('int16')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([data, test], ignore_index=True).fillna(-1)\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MERGE SHOPS AND CATEGORIES WITH TRAIN DF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.merge(shops[['shop_id', 'city_name']], how='left', on='shop_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = pd.merge(items[['item_id', 'item_category_id']], categories[['item_category_id','category_general_name']], how='left', on='item_category_id')\ndata = data.merge(a, how='left', on='item_id')\ndel a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['item_category_id'] = data['item_category_id'].astype('int8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Month feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['month'] = data['date_block_num'] % 12 + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generating mean encoded features and adding lag","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_generator(df, col, lags):\n    tmp = df[['date_block_num', 'shop_id', 'item_id', col]]\n    for lag in lags:\n        a = tmp.copy()\n        a['date_block_num'] += lag\n        a.columns = ['date_block_num', 'shop_id', 'item_id', f'{col}_lag_{lag}']\n        df = df.merge(a, how='left', on=['date_block_num', 'shop_id', 'item_id'])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlags = [1, 2, 3]\ndata = lag_generator(data, 'target', lags)\n\ndata.fillna(-1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = data.groupby(['date_block_num', 'shop_id']).agg({'target': 'mean'})\ngroup.columns = ['target_mean_date_shop']\ngroup.reset_index(inplace=True)\n\ndata = data.merge(group, how='left', on=['date_block_num', 'shop_id']).fillna(-1)\n\ndata = lag_generator(data, 'target_mean_date_shop', [1])\ndata.drop(columns='target_mean_date_shop', inplace=True)\n\ndata.fillna(-1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = data.groupby(['date_block_num', 'item_id']).agg({'target': 'mean'})\ngroup.columns = ['target_mean_date_item']\ngroup.reset_index(inplace=True)\n\ndata = data.merge(group, how='left', on=['date_block_num', 'item_id']).fillna(-1)\n\ndata = lag_generator(data, 'target_mean_date_item', [1, 2, 3])\ndata.drop(columns='target_mean_date_item', inplace=True)\n\ndata.fillna(-1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = data.groupby(['date_block_num', 'item_category_id']).agg({'target': 'mean'})\ngroup.columns = ['target_mean_date_category']\ngroup.reset_index(inplace=True)\n\ndata = data.merge(group, how='left', on=['date_block_num', 'item_category_id']).fillna(-1)\n\ndata = lag_generator(data, 'target_mean_date_category', [1])\ndata.drop(columns='target_mean_date_category', inplace=True)\n\ndata.fillna(-1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = data.groupby(['date_block_num', 'city_name']).agg({'target': 'mean'})\ngroup.columns = ['target_mean_date_city']\ngroup.reset_index(inplace=True)\n\ndata = data.merge(group, how='left', on=['date_block_num', 'city_name']).fillna(-1)\n\ndata = lag_generator(data, 'target_mean_date_city', [1])\ndata.drop(columns='target_mean_date_city', inplace=True)\n\ndata.fillna(-1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = data.groupby(['date_block_num', 'category_general_name']).agg({'target': 'mean'})\ngroup.columns = ['target_mean_date_gencategory']\ngroup.reset_index(inplace=True)\n\ndata = data.merge(group, how='left', on=['date_block_num', 'category_general_name']).fillna(-1)\n\ndata = lag_generator(data, 'target_mean_date_gencategory', [1])\ndata.drop(columns='target_mean_date_gencategory', inplace=True)\n\ndata.fillna(-1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checkpoint","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.to_hdf('data1.hdf5', 'df')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_hdf('data1.hdf5', 'df')\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train.groupby('item_id').agg({'item_price': 'mean'})\ngroup['item_price'] = group['item_price'].astype('float32')\ngroup.columns = ['item_mean_price']\ngroup.reset_index(inplace=True)\n\ndata = data.merge(group, how='left', on='item_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate time from last sale","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = train.groupby(['shop_id', 'item_id'], sort=False)['date_block_num'].unique()\n\ngroup.name = 'last_sales'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.merge(group.reset_index(), how='left', on=['shop_id', 'item_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_prev_sel(arr):\n    try:\n        date_block = arr[0]\n        last_sale = arr[1]\n        return last_sale[last_sale < date_block].max()\n    except:\n        return np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npool = Pool(2)\n\ndata['last_sale'] = pool.map(find_prev_sel, data[['date_block_num', 'last_sales']].values)\n\npool.close()\npool.join()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train.groupby(['item_id', 'date_block_num'], as_index=False, sort=False).agg({'item_price': 'mean'})\ngroup.columns = ['item_id', 'last_sale', 'item_date_mean_price_prev_sale']\n\ndata = data.merge(group, how='left', on=['item_id', 'last_sale'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['delta_item_prev_price'] = data['item_mean_price'] - data['item_date_mean_price_prev_sale']\ndata['prev_sold_delta'] = data['date_block_num'] - data['last_sale']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=['last_sale', 'item_mean_price', 'item_date_mean_price_prev_sale', 'last_sales'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.fillna(-1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['prev_sold_delta'] = data['prev_sold_delta'].astype('int8')\ndata['delta_item_prev_price'] = data['delta_item_prev_price'].astype('float32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SHOP REVENUE LAG","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['revenue'] = train['item_price'] * train['item_cnt_day']\ngroup = train.groupby(['date_block_num', 'shop_id']).agg({'revenue': 'sum'})\ngroup.columns = ['revenue_lag_1']\ngroup.reset_index(inplace=True)\ngroup['date_block_num'] += 1\n\ndata = data.merge(group, how='left', on=['date_block_num', 'shop_id'])\n\ndata.fillna(-1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.to_hdf('data2.hdf5', 'df')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_hdf('data2.hdf5', 'df')\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Deleting garbage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del train\ndel items\ndel test\ndel shops\ndel categories\ndel group\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data['date_block_num'] >= 3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train/val","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = lgb.Dataset(data[data['date_block_num'] < 33].drop(columns=['date_block_num', 'target']), label=data[data['date_block_num'] < 33].target.values, categorical_feature=['shop_id', 'item_id', 'city_name', 'item_category_id', 'category_general_name', 'month', 'prev_sold_delta'])\nval_data = lgb.Dataset(data[data['date_block_num'] == 33].drop(columns=['date_block_num', 'target']), label=data[data['date_block_num'] == 33].target.values, categorical_feature=['shop_id', 'item_id', 'city_name', 'item_category_id', 'category_general_name', 'month', 'prev_sold_delta'], reference=train_data)\ntest_data = data[data['date_block_num'] == 34].drop(columns=['date_block_num', 'target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams = {'metric': 'rmse',\n          'learning_rate': 0.01,\n          'max_depth': 13,\n          'num_leaves': 1673,\n          'random_state': 42,\n          'num_iterations': 500,\n          'early_stopping_round': 12,\n          'num_threads': 2\n         }\n\nmodel = lgb.train(params, train_data, valid_sets=[val_data, train_data])\n\n#model = lgb.train(params, train_data, valid_sets=train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.plot_importance(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['item_cnt_month'] = preds\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('best_lgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The next step is downloading data2.hdf file and training other models on it","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}