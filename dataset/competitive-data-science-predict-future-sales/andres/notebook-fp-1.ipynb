{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predict futures sales"},{"metadata":{},"cell_type":"markdown","source":"The Future Sales competition is the final assesment in the 'How to win a Data Science' course in the Advanced Machine Learning specialisation from HSE University, Moscow. The aim is to predict the monthly sales of items in specific shops, given historical data. The sale counts are clipped between 0 and 20."},{"metadata":{},"cell_type":"markdown","source":"## Import libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import *\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nimport time\nimport pickle\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ncats = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\nsample_submission = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sales table view\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Revision of missing values\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe sales dataset has no missing value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sales graph per month\nsale_by_month = train.groupby('date_block_num')['item_cnt_day'].sum()\nsale_by_month.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the graph of total sales for each month, we see that sales are decreasing over time and seasonally we see peaks for November."},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering and data cleaning"},{"metadata":{},"cell_type":"markdown","source":"### Remove outliers and duplicated data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Chart of quantity of items sold\nplt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nflierprops = dict(marker='o', markerfacecolor='red', markersize=6,\n                  linestyle='none', markeredgecolor='black')\nsns.boxplot(x=train.item_cnt_day, flierprops=flierprops)\n\n#Chart of item prices\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price, flierprops=flierprops)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most relevant outliers are eliminated. In this case the articles that were sold more than 1001 in one day and the articles with prices over 100.000."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removal of prices over 100.000\ntrain = train[train.item_price<100000]\n#Removal of items over 1.001\ntrain = train[train.item_cnt_day<1001]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The item with a price lower than zero will be replaced by a value calculated by the median."},{"metadata":{"trusted":true},"cell_type":"code","source":"median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0, 'item_price'] = median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training and test data are adjusted. for duplicate stores."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Якутск Орджоникидзе, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# Якутск ТЦ \"Центральный\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# Жуковский ул. Чкалова 39м²\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## pre-processing in the shops, cats and item data set\n\n* The data sets of the warehouses are reconstructed. Separating the city from the store name, standardizing and coding the records.\n* The same procedure is performed with the category data set. Separating the type and subtype from the category name."},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\ncats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n\n# if subtype is nan then type\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]\n\nitems.drop(['item_name'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Monthly sales**\n\nThe train data set is extended with the pairs of items and stores with zero monthly sales. To make it more similar to the test data set\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It adds to the data set train the pairs shops/item and then shortens them in a range of (0 to 20). So that the train target be similar to the test prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float32))\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) \ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Shops/Items/Cats features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Traget lags**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Group sale stats in recent**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_group_stats(matrix_, groupby_feats, target, enc_feat, last_periods):\n    if not 'date_block_num' in groupby_feats:\n        print ('date_block_num must in groupby_feats')\n        return matrix_\n    \n    group = matrix_.groupby(groupby_feats)[target].sum().reset_index()\n    max_lags = np.max(last_periods)\n    for i in range(1,max_lags+1):\n        shifted = group[groupby_feats+[target]].copy(deep=True)\n        shifted['date_block_num'] += i\n        shifted.rename({target:target+'_lag_'+str(i)},axis=1,inplace=True)\n        group = group.merge(shifted, on=groupby_feats, how='left')\n    group.fillna(0,inplace=True)\n    for period in last_periods:\n        lag_feats = [target+'_lag_'+str(lag) for lag in np.arange(1,period+1)]\n        # we do not use mean and svd directly because we want to include months with sales = 0\n        mean = group[lag_feats].sum(axis=1)/float(period)\n        mean2 = (group[lag_feats]**2).sum(axis=1)/float(period)\n        group[enc_feat+'_avg_sale_last_'+str(period)] = mean\n        group[enc_feat+'_std_sale_last_'+str(period)] = (mean2 - mean**2).apply(np.sqrt)\n        group[enc_feat+'_std_sale_last_'+str(period)].replace(np.inf,0,inplace=True)\n        # divide by mean, this scales the features for NN\n        group[enc_feat+'_avg_sale_last_'+str(period)] /= group[enc_feat+'_avg_sale_last_'+str(period)].mean()\n        group[enc_feat+'_std_sale_last_'+str(period)] /= group[enc_feat+'_std_sale_last_'+str(period)].mean()\n    cols = groupby_feats + [f_ for f_ in group.columns.values if f_.find('_sale_last_')>=0]\n    matrix = matrix_.merge(group[cols], on=groupby_feats, how='left')\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = add_group_stats(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'item', [6,12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'shop', [6,12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'category', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'city', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'type_code'], 'item_cnt_month', 'type', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'subtype_code'], 'item_cnt_month', 'subtype', [12])\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lag features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First use the target coding of each group, then change the month to create lags functions\ndef target_encoding(matrix_, groupby_feats, target, enc_feat, lags):\n    print ('target encoding for',groupby_feats)\n    group = matrix_.groupby(groupby_feats).agg({target:'mean'})\n    group.columns = [enc_feat]\n    group.reset_index(inplace=True)\n    matrix = matrix_.merge(group, on=groupby_feats, how='left')\n    matrix[enc_feat] = matrix[enc_feat].astype(np.float16)\n    matrix = lag_feature(matrix, lags, enc_feat)\n    matrix.drop(enc_feat, axis=1, inplace=True)\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = target_encoding(matrix, ['date_block_num'], 'item_cnt_month', 'date_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'date_item_avg_item_cnt', [1,2,3,6,12])\nmatrix = target_encoding(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'date_shop_avg_item_cnt', [1,2,3,6,12])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'date_cat_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'shop_id', 'item_category_id'], 'item_cnt_month', 'date_shop_cat_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'date_city_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_id', 'city_code'], 'item_cnt_month', 'date_item_city_avg_item_cnt', [1])\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Trend features**\n\nPrice trend for the last six months."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Last month shop revenue trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Add month and year**"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix['month'] = matrix['date_block_num'] % 12\nmatrix['year'] = (matrix['date_block_num'] / 12).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Add month since the last and first sale**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Month since the last sale of each pair of shops/article.\nts = time.time()\nlast_sale = pd.DataFrame()\nfor month in range(1,35):    \n    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby(['item_id','shop_id'])['date_block_num'].max()\n    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n                       'item_id': last_month.index.get_level_values(0).values,\n                       'shop_id': last_month.index.get_level_values(1).values,\n                       'item_shop_last_sale': last_month.values})\n    last_sale = last_sale.append(df)\nlast_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n\nmatrix = matrix.merge(last_sale, on=['date_block_num','item_id','shop_id'], how='left')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Month since the last sale of each item.\nts = time.time()\nlast_sale = pd.DataFrame()\nfor month in range(1,35):    \n    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby('item_id')['date_block_num'].max()\n    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n                       'item_id': last_month.index.values,\n                       'item_last_sale': last_month.values})\n    last_sale = last_sale.append(df)\nlast_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n\nmatrix = matrix.merge(last_sale, on=['date_block_num','item_id'], how='left')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Months from the first sale for each pair of shops/item and for item only.\nts = time.time()\nmatrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nFinally, the columns with calculated values that cannot be calculated for the test set are eliminated and the first 12 months are also eliminated, since they were used as lags values"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = matrix[matrix.date_block_num > 11]\nmatrix.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = matrix[[\n    'date_block_num',\n    'shop_id',\n    #'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code','subtype_code',\n    'item_cnt_month_lag_1','item_cnt_month_lag_2','item_cnt_month_lag_3','item_cnt_month_lag_6','item_cnt_month_lag_12',\n    'item_avg_sale_last_6', 'item_std_sale_last_6',\n    'item_avg_sale_last_12', 'item_std_sale_last_12',\n    'shop_avg_sale_last_6', 'shop_std_sale_last_6',\n    'shop_avg_sale_last_12', 'shop_std_sale_last_12',\n    'category_avg_sale_last_12', 'category_std_sale_last_12',\n    'city_avg_sale_last_12', 'city_std_sale_last_12',\n    'type_avg_sale_last_12', 'type_std_sale_last_12',\n    'subtype_avg_sale_last_12', 'subtype_std_sale_last_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1','date_item_avg_item_cnt_lag_2','date_item_avg_item_cnt_lag_3','date_item_avg_item_cnt_lag_6','date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1','date_shop_avg_item_cnt_lag_2','date_shop_avg_item_cnt_lag_3','date_shop_avg_item_cnt_lag_6','date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month','year',\n    'item_shop_last_sale','item_last_sale',\n    'item_shop_first_sale','item_first_sale',\n]]\n\ncat_feats = ['shop_id','city_code','item_category_id','type_code','subtype_code']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Partitioning the data set for training and testing.\n\nThe partition is:\n* 34 months for the test set.\n* 33 months for the validation set.\n* and from 13 to 32 months for the training team."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models\n\nIn this section we proceed to create, train and predict the models."},{"metadata":{},"cell_type":"markdown","source":"* ### LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\nmodel_lgbm = LGBMRegressor(\n    max_depth = 8,\n    n_estimators = 500,\n    colsample_bytree=0.7,\n    min_child_weight = 300,\n    reg_alpha = 0.1,\n    reg_lambda = 1,\n    random_state = 42,\n)\n\nmodel_lgbm.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=10, \n    early_stopping_rounds = 40,\n    categorical_feature = cat_feats) # use LGBM's build-in categroical features.\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_lgbm = model_lgbm.predict(X_valid).clip(0, 20)\nY_test_lgbm = model_lgbm.predict(X_test).clip(0, 20)\n\nX_train_lgbm = pd.DataFrame({\n    \"ID\": np.arange(Y_pred_lgbm.shape[0]), \n    \"item_cnt_month\": Y_pred_lgbm\n})\nX_train_lgbm.to_csv('lgb_valid.csv', index=False)\n\nsubmission_lgbm = pd.DataFrame({\n    \"ID\": np.arange(Y_test_lgbm.shape[0]), \n    \"item_cnt_month\": Y_test_lgbm\n})\nsubmission_lgbm.to_csv('submission_lgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\nmodel_xgb = XGBRegressor(\n    max_depth=7,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    gamma = 0.005,\n    eta=0.1,    \n    seed=42)\n\nmodel_xgb.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=10, \n    early_stopping_rounds = 40,\n    )\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_xgb = model_xgb.predict(X_valid).clip(0, 20)\nY_test_xgb = model_xgb.predict(X_test).clip(0, 20)\n\nX_train_xgb = pd.DataFrame({\n    \"ID\": np.arange(Y_pred_xgb.shape[0]), \n    \"item_cnt_month\": Y_pred_xgb\n})\nX_train_xgb.to_csv('xgb_valid.csv', index=False)\n\nsubmission_xgb = pd.DataFrame({\n    \"ID\": np.arange(Y_test_xgb.shape[0]), \n    \"item_cnt_month\": Y_test_xgb\n})\nsubmission_xgb.to_csv('submission_xgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Serialization of the models and data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pkl_filename = \"pickle_model_LGBM.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(model_lgbm, file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pkl_filename = \"pickle_model_XGB.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(model_xgb, file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pkl_filename = \"data.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(data, file)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **References:**\n\n* https://www.kaggle.com/dlarionov/feature-engineering-xgboost\n* https://www.kaggle.com/uladzimirkapeika/feature-engineering-lightgbm-top-1\n* https://www.kaggle.com/gordotron85/future-sales-xgboost-top-3"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}