{"cells":[{"metadata":{},"cell_type":"markdown","source":"Welcome to Predict Future Sales Challenge. In this kernel, I will focus on doing some illustrative data visualizations and then use XGBoost to predict November 2015's sale","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport os\nimport random\nrandom.seed(1234)\nimport gc\nimport os\nimport re\nfrom itertools import product\nimport pickle\nimport pandas as pd\nimport numpy as np\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nimport matplotlib.pylab as plt\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"items = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\ncats = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\ntrain = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find out missing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are items with strange prices and sales. After detailed exploration I decided to remove items with price > 100000 and sales > 1001 (1000 is ok).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,4))\nplt.xlim(-100, 3000)\nsns.boxplot( x= train.item_cnt_day )\nplt.figure( figsize = (10,4) )\nplt.xlim(train.item_price.min(), train.item_price.max())\nsns.boxplot( x = train.item_price )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape[0])\nprint(len( train[train.item_cnt_day >999 ] ))\nprint( len(train[ train.item_cnt_day > 500  ]) )\nprint(len(train[train.item_price >100000 ]))\nprint(train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[(train.item_price < 100000 )& (train.item_cnt_day < 1000)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,4))\nplt.xlim( -100, 1000 )\nsns.boxplot( x= train.item_cnt_day )\nplt.figure( figsize = (10,3) )\nplt.xlim( train.item_price.min(), train.item_price.max()*1.1 )\nsns.boxplot( x = train.item_price )\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len( train[train.item_price < 0 ] ))\n# delete that petty 1 row\n\nprint(len( train[train.item_cnt_day < 1 ] ))\n# restore value with 0 instead of deleting","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train.item_price > 0].reset_index(drop = True)\ntrain.loc[train.item_cnt_day < 1, \"item_cnt_day\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts=train.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(16,8))\nplt.title('Total Sales of the company')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.plot(ts.rolling(window=12,center=False).mean(),label='Rolling Mean');\nplt.plot(ts.rolling(window=12,center=False).std(),label='Rolling sd');\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quick observations: There is an obvious \"seasonality\" (Eg: peak sales around a time of year) and a decreasing \"Trend\".","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# SHOPS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shops","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.shop_id == 0, \"shop_id\"] = 57\ntest.loc[test.shop_id == 0 , \"shop_id\"] = 57\ntrain.loc[train.shop_id == 1, \"shop_id\"] = 58\ntest.loc[test.shop_id == 1 , \"shop_id\"] = 58\ntrain.loc[train.shop_id == 11, \"shop_id\"] = 10\ntest.loc[test.shop_id == 11, \"shop_id\"] = 10\ntrain.loc[train.shop_id == 40, \"shop_id\"] = 39\ntest.loc[test.shop_id == 40, \"shop_id\"] = 39","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# renaming\nshops.loc[ shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"',\"shop_name\" ] = 'СергиевПосад ТЦ \"7Я\"'\n# split the column into 2 subcategories\nshops[\"city\"] = shops.shop_name.str.split(\" \").map( lambda x: x[0] )\nshops[\"category\"] = shops.shop_name.str.split(\" \").map( lambda x: x[1] )\nshops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category = []\nfor cat in shops.category.unique():\n    print(cat, len(shops[shops.category == cat]) )\n    if len(shops[shops.category == cat]) > 4:\n        category.append(cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keep the name if the unique value is greater than 4, otherwise rename the rest of them as \"etc\"\nshops.category = shops.category.apply( lambda x: x if (x in category) else \"etc\" )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now see the unique value\nfor cat in shops.category.unique():\n    print(cat, len(shops[shops.category == cat]) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Labelling the unique names\n\nshops[\"shop_category\"] = LabelEncoder().fit_transform( shops.category )\nshops[\"shop_city\"] = LabelEncoder().fit_transform( shops.city )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CATEGORIES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rename the names which seems familiar\ncats[\"type_code\"] = cats.item_category_name.apply( lambda x: x.split(\" \")[0] ).astype(str)\ncats.loc[ (cats.type_code == \"Игровые\")| (cats.type_code == \"Аксессуары\"), \"category\" ] = \"Игры\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cats.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category = []\nfor cat in cats.type_code.unique():\n    print(cat, len(cats[cats.type_code == cat]))\n    if len(cats[cats.type_code == cat]) > 4: \n        category.append( cat )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cats.type_code = cats.type_code.apply(lambda x: x if (x in category) else \"etc\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for cat in cats.type_code.unique():\n    print(cat, len(cats[cats.type_code == cat]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cats.type_code = LabelEncoder().fit_transform(cats.type_code)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"why","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncats[\"split\"] = cats.item_category_name.apply(lambda x: x.split(\"-\"))\n# if there is something after '-' strip it, otherwise strip the remaining first chunk of words\ncats[\"subtype\"] = cats.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats.subtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cats[\"subtype_code\"] = LabelEncoder().fit_transform( cats[\"subtype\"] )\ncats.subtype_code\ncats = cats[[\"item_category_id\", \"type_code\", \"subtype_code\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* type_code is the unique chronology of names\n* subtype_code is the ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cats.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **ITEMS**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef name_correction(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9А-Яа-я]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.item_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# include item_name in items dataframe\nitems[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\nitems[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n\nitems[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\nitems[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\nitems = items.fillna('0')\n\nitems[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n# exclude the last letter if it's not zero to remove square bracket.\nitems.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.name2.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'm limiting the range of xbox into 8 letters.\nitems[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | \n          (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nitems.loc[ items.type == \"\", \"type\"] = \"mac\"\nitems.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\nitems.loc[ (items.type == 'pc' )| (items.type == 'pс') | (items.type == \"pc\"), \"type\" ] = \"pc\"\nitems.loc[ items.type == 'рs3' , \"type\"] = \"ps3\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group_sum = items.groupby([\"type\"]).agg({\"item_id\": \"count\"})\ngroup_sum = group_sum.reset_index()\n\nprint(group_sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = []\nfor cat in group_sum.type.unique():\n#     print(group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0])\n    if group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0] <40:\n        drop_cols.append(cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.name1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is too many unique value in number1(201611 out of 22170), so I will drop it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items.name1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.name2.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.name3.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I can count on them(name2 and name3).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items.name2 = items.name2.apply( lambda x: \"etc\" if (x in drop_cols) else x )\nitems = items.drop([\"type\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nitems.name2 = LabelEncoder().fit_transform(items.name2)\nitems.name3 = LabelEncoder().fit_transform(items.name3)\n\nitems.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SALES TRAIN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['date_block_num'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to deal with monthly item count, and the total month is 34.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# product acts like nested for loop.\nmatrix = []\n\n# id doesn't mean anything, so I'm gonna put the unique shop and product of a particular date like 2.1.2013\ncols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\ncols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\nfor i in range(34):\n    sales = train[train.date_block_num == i]\n    matrix.append( np.array(list( product( [i], sales.shop_id.unique(), sales.item_id.unique() ) ), dtype = np.int16) )\n\nmatrix = pd.DataFrame( np.vstack(matrix), columns = cols )\nmatrix[\"date_block_num\"] = matrix[\"date_block_num\"].astype(np.int8)\nmatrix[\"shop_id\"] = matrix[\"shop_id\"].astype(np.int8)\nmatrix[\"item_id\"] = matrix[\"item_id\"].astype(np.int16)\nmatrix.sort_values( cols, inplace = True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"revenue\"] = train[\"item_cnt_day\"] * train[\"item_price\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aggregation usage in dataframe and series :\n* https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\n* https://towardsdatascience.com/why-and-how-to-use-merge-with-pandas-in-python-548600f7e738","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train.groupby( [\"date_block_num\", \"shop_id\", \"item_id\"] ).agg( {\"item_cnt_day\": [\"sum\"]} )\ngroup.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group.columns = [\"item_cnt_month\"]\ngroup.reset_index( inplace = True)\ngroup.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\n\n# here \"cols\" reflects the three column stated before and those columns should be on the left side.\nmatrix = pd.merge( matrix, group, on = cols, how = \"left\" )\n# as mention in, the range of the output should be in between 0 and 20\nmatrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].fillna(0).clip(0,20).astype(np.float16)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To use time tricks append test pairs to the matrix. The plan is to get the result for the 34th month in case of test dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"date_block_num\"] = 34\ntest[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\ntest[\"shop_id\"] = test.shop_id.astype(np.int8)\ntest[\"item_id\"] = test.item_id.astype(np.int16)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"add test dataframe with matrix(train dataframe)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmatrix = pd.concat([matrix, test.drop([\"ID\"],axis = 1)], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna( 0, inplace = True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check out to know more about \"how\" and \"on\" here : https://jakevdp.github.io/PythonDataScienceHandbook/03.07-merge-and-join.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmatrix = pd.merge( matrix, shops, on = [\"shop_id\"], how = \"left\" )\nmatrix = pd.merge(matrix, items, on = [\"item_id\"], how = \"left\")\nmatrix = pd.merge( matrix, cats, on = [\"item_category_id\"], how = \"left\" )\nmatrix[\"shop_city\"] = matrix[\"shop_city\"].astype(np.int8)\nmatrix[\"shop_category\"] = matrix[\"shop_category\"].astype(np.int8)\nmatrix[\"item_category_id\"] = matrix[\"item_category_id\"].astype(np.int8)\nmatrix[\"subtype_code\"] = matrix[\"subtype_code\"].astype(np.int8)\nmatrix[\"name2\"] = matrix[\"name2\"].astype(np.int8)\nmatrix[\"name3\"] = matrix[\"name3\"].astype(np.int16)\nmatrix[\"type_code\"] = matrix[\"type_code\"].astype(np.int8)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del items\ndel cats\ndel shops\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TIME SERIES LAG FEATURE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature( df,lags, cols ):\n    for col in cols:\n        print(col)\n        tmp = df[[\"date_block_num\", \"shop_id\",\"item_id\",col ]]\n        for i in lags:\n            shifted = tmp.copy()\n            shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\"+str(i)]\n            shifted.date_block_num = shifted.date_block_num + i\n            df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmatrix = lag_feature( matrix, [1,2,3], [\"item_cnt_month\"] )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MEAN ENCODED FEATURES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = matrix.groupby( [\"date_block_num\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1], [\"date_avg_item_cnt\"] )\nmatrix.drop( [\"date_avg_item_cnt\"], axis = 1, inplace = True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix.date_item_avg_item_cnt = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3], ['date_item_avg_item_cnt'])\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = matrix.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_shop_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1,2,3], [\"date_shop_avg_item_cnt\"] )\nmatrix.drop( [\"date_shop_avg_item_cnt\"], axis = 1, inplace = True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = matrix.groupby( [\"date_block_num\",\"shop_id\",\"item_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_item_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\",\"item_id\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_shop_item_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1,2,3], [\"date_shop_item_avg_item_cnt\"] )\nmatrix.drop( [\"date_shop_item_avg_item_cnt\"], axis = 1, inplace = True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix.date_shop_subtype_avg_item_cnt = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_shop_subtype_avg_item_cnt'])\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = matrix.groupby(['date_block_num', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_city_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', \"shop_city\"], how='left')\nmatrix.date_city_avg_item_cnt = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_city_avg_item_cnt'])\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = matrix.groupby(['date_block_num', 'item_id', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'shop_city'], how='left')\nmatrix.date_item_city_avg_item_cnt = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_item_city_avg_item_cnt'])\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TREND FEATURES","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Price trend for the last 3 months.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = train.groupby( [\"item_id\"] ).agg({\"item_price\": [\"mean\"]})\ngroup.columns = [\"item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge( group, on = [\"item_id\"], how = \"left\" )\nmatrix[\"item_avg_item_price\"] = matrix.item_avg_item_price.astype(np.float16)\n\n\ngroup = train.groupby( [\"date_block_num\",\"item_id\"] ).agg( {\"item_price\": [\"mean\"]} )\ngroup.columns = [\"date_item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge(group, on = [\"date_block_num\",\"item_id\"], how = \"left\")\nmatrix[\"date_item_avg_item_price\"] = matrix.date_item_avg_item_price.astype(np.float16)\nlags = [1, 2, 3]\nmatrix = lag_feature( matrix, lags, [\"date_item_avg_item_price\"] )\nfor i in lags:\n    matrix[\"delta_price_lag_\" + str(i) ] = (matrix[\"date_item_avg_item_price_lag_\" + str(i)]- matrix[\"item_avg_item_price\"] )/ matrix[\"item_avg_item_price\"]\n\ndef select_trends(row) :\n    for i in lags:\n        if row[\"delta_price_lag_\" + str(i)]:\n            return row[\"delta_price_lag_\" + str(i)]\n    return 0\n\nmatrix[\"delta_price_lag\"] = matrix.apply(select_trends, axis = 1)\nmatrix[\"delta_price_lag\"] = matrix.delta_price_lag.astype( np.float16 )\nmatrix[\"delta_price_lag\"].fillna( 0 ,inplace = True)\n\nfeatures_to_drop = [\"item_avg_item_price\", \"date_item_avg_item_price\"]\nfor i in lags:\n    features_to_drop.append(\"date_item_avg_item_price_lag_\" + str(i) )\n    features_to_drop.append(\"delta_price_lag_\" + str(i) )\nmatrix.drop(features_to_drop, axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Last month shop revenue trend.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngroup = train.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"revenue\": [\"sum\"] })\ngroup.columns = [\"date_shop_revenue\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge( group , on = [\"date_block_num\", \"shop_id\"], how = \"left\" )\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby([\"shop_id\"]).agg({ \"date_block_num\":[\"mean\"] })\ngroup.columns = [\"shop_avg_revenue\"]\ngroup.reset_index(inplace = True )\n\nmatrix = matrix.merge( group, on = [\"shop_id\"], how = \"left\" )\nmatrix[\"shop_avg_revenue\"] = matrix.shop_avg_revenue.astype(np.float32)\nmatrix[\"delta_revenue\"] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\nmatrix[\"delta_revenue\"] = matrix[\"delta_revenue\"]. astype(np.float32)\n\nmatrix = lag_feature(matrix, [1], [\"delta_revenue\"])\nmatrix[\"delta_revenue_lag_1\"] = matrix[\"delta_revenue_lag_1\"].astype(np.float32)\nmatrix.drop( [\"date_shop_revenue\", \"shop_avg_revenue\", \"delta_revenue\"] ,axis = 1, inplace = True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of days in a month. There are no leap years.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix[\"month\"] = matrix[\"date_block_num\"] % 12","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Assign the number of days according to the month","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix[\"days\"] = matrix[\"month\"].map(days).astype(np.int8)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmatrix[\"item_shop_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\",\"shop_id\"])[\"date_block_num\"].transform('min')\nmatrix[\"item_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\"])[\"date_block_num\"].transform('min')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because of the using 3(highest) as lag value, I have to drop first 3 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmatrix = matrix[matrix[\"date_block_num\"] > 3]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = matrix.copy()\ndel matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data[\"date_block_num\"]==34].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Validation strategy is 34th month for the test set, 33th month for the validation set and (4-33)th months for the train.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our memory is almost full, let's do some garbage collecting stuffs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del data\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Extreme Gradient Boost (XGBoost) is a powerful optimized decision tree-based grident boost algorithm, and it is possible to accelerate by GPU. Grident boost uses large numbers of relative small and weak decision trees to model the residues for each iteration. In each iteration, it gives a 'direction', then update the predictions and residues until stop criteria are reached.\n\n* Load Dataset and Setting\n* Model Hyperparameter Tuning and Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmodel = XGBRegressor(\n    max_depth=10,\n    n_estimators=1000,\n    min_child_weight=0.5, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    tree_method='gpu_hist', gpu_id=0,\n    seed=42)\n\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose= True, \n    early_stopping_rounds = 5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10,14))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}