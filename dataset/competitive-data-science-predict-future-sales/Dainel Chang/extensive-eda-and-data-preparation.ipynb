{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom IPython.display import display, HTML\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport matplotlib.dates as md\nfrom multiprocessing import  Pool\nfrom datetime import datetime\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import GridSearchCV, train_test_split, RepeatedKFold, KFold, cross_val_score, ShuffleSplit\nimport xgboost as xgb\nimport tensorflow as tf\nfrom sklearn.metrics import mean_squared_error\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-07T10:21:34.562115Z","iopub.execute_input":"2021-11-07T10:21:34.56261Z","iopub.status.idle":"2021-11-07T10:21:34.567776Z","shell.execute_reply.started":"2021-11-07T10:21:34.56258Z","shell.execute_reply":"2021-11-07T10:21:34.567068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load data:\n#items_df = pd.read_csv(\"items.csv\")\n#item_categories_df = pd.read_csv(\"item_categories.csv\")\n#shops_df = pd.read_csv(\"shops.csv\")\n#train_df = pd.read_csv(\"sales_train.csv\")\n#test_df = pd.read_csv(\"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:34.569024Z","iopub.execute_input":"2021-11-07T10:21:34.569538Z","iopub.status.idle":"2021-11-07T10:21:34.584739Z","shell.execute_reply.started":"2021-11-07T10:21:34.569498Z","shell.execute_reply":"2021-11-07T10:21:34.584097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading data:\nPATH = \"/kaggle/input/competitive-data-science-predict-future-sales/\"\nitems_df = pd.read_csv(PATH + \"items.csv\")\nitem_categories_df = pd.read_csv(PATH + \"item_categories.csv\")\nshops_df = pd.read_csv(PATH + \"shops.csv\")\ntrain_df = pd.read_csv(PATH + \"sales_train.csv\")\ntest_df = pd.read_csv(PATH + \"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:34.586243Z","iopub.execute_input":"2021-11-07T10:21:34.586631Z","iopub.status.idle":"2021-11-07T10:21:35.966477Z","shell.execute_reply.started":"2021-11-07T10:21:34.586603Z","shell.execute_reply":"2021-11-07T10:21:35.965454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:35.967919Z","iopub.execute_input":"2021-11-07T10:21:35.96822Z","iopub.status.idle":"2021-11-07T10:21:35.98621Z","shell.execute_reply.started":"2021-11-07T10:21:35.96819Z","shell.execute_reply":"2021-11-07T10:21:35.985293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#change the date format and sort ascendingly\neda_df = train_df.copy()\neda_df[\"date\"]=  pd.to_datetime(eda_df[\"date\"], format='%d.%m.%Y')\neda_df.sort_values(by=\"date\", ascending=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:35.987564Z","iopub.execute_input":"2021-11-07T10:21:35.987934Z","iopub.status.idle":"2021-11-07T10:21:36.844254Z","shell.execute_reply.started":"2021-11-07T10:21:35.987895Z","shell.execute_reply":"2021-11-07T10:21:36.843087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#obtain the top 5% for item_price & item_cnt_day and sales_per_item by item_price times item_cnt_day\nfor col in [\"item_price\", \"item_cnt_day\"]:\n    upper_quantile = eda_df[col].quantile(0.95)\n    eda_df[col] = np.where(eda_df[col]>upper_quantile, upper_quantile,eda_df[col])\n\neda_df[\"sales_per_item\"] = eda_df[\"item_price\"] * eda_df[\"item_cnt_day\"]","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:36.845833Z","iopub.execute_input":"2021-11-07T10:21:36.846267Z","iopub.status.idle":"2021-11-07T10:21:36.951624Z","shell.execute_reply.started":"2021-11-07T10:21:36.846226Z","shell.execute_reply":"2021-11-07T10:21:36.950608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda_df","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:36.953603Z","iopub.execute_input":"2021-11-07T10:21:36.953907Z","iopub.status.idle":"2021-11-07T10:21:36.972627Z","shell.execute_reply.started":"2021-11-07T10:21:36.953852Z","shell.execute_reply":"2021-11-07T10:21:36.971685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#merge data with other dataframe (items_df, item_categories_df)\neda_df = pd.merge(eda_df, items_df, on='item_id', how='inner')\neda_df = pd.merge(eda_df, item_categories_df, on='item_category_id', how='inner')\neda_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:36.975286Z","iopub.execute_input":"2021-11-07T10:21:36.975558Z","iopub.status.idle":"2021-11-07T10:21:37.994563Z","shell.execute_reply.started":"2021-11-07T10:21:36.97553Z","shell.execute_reply":"2021-11-07T10:21:37.99362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create item_category_info_df which is item category information\nitem_category_info_df = pd.DataFrame(\n    columns=[\"name\", \"num_products\", \"first_sold\", \"last_sold\", \"min_price\", \"max_price\", \"mean_price\", \"median_price\",\n              \"mean_item_cnt_month\", \"median_item_cnt_month\", \"mean_cnt_jan\", \"mean_cnt_feb\", \"mean_cnt_mar\", \"mean_cnt_apr\",\n              \"mean_cnt_may\", \"mean_cnt_jun\", \"mean_cnt_jul\", \"mean_cnt_aug\", \"mean_cnt_sep\", \"mean_cnt_oct\", \"mean_cnt_nov\", \"mean_cnt_dez\"],\n    index = item_categories_df[\"item_category_id\"].unique())\n\nfor cid in item_category_info_df.index:\n    item_category_info_df.at[cid, \"name\"] = item_categories_df[item_categories_df[\"item_category_id\"]==cid][\"item_category_name\"].values\n    item_category_info_df.at[cid, \"num_products\"] = items_df[items_df[\"item_category_id\"]==cid][\"item_id\"].nunique()\n    cdf= eda_df[eda_df[\"item_category_id\"]==cid].copy()\n    item_category_info_df.at[cid, \"first_sold\"] = cdf[\"date\"].min()\n    item_category_info_df.at[cid, \"last_sold\"] = cdf[\"date\"].max()\n    item_category_info_df.at[cid, \"min_price\"] = cdf[\"item_price\"].min()\n    item_category_info_df.at[cid, \"max_price\"] = cdf[\"item_price\"].max()\n    item_category_info_df.at[cid, \"mean_price\"] = cdf[\"item_price\"].mean()\n    item_category_info_df.at[cid, \"median_price\"] = cdf[\"item_price\"].median()\n    \n    \n    \"\"\"\n    change the date feature to the first of each month so that each date in a month is mapped to the first day of the month.\n    Then we group by this new date. We then get a dataframe with the values sum_sales and sum_item_cnt for each month in each year.\n    As a final step, we group by month to get the mean value for each month.\n    \"\"\"\n    \n    cdf[\"month\"] = cdf[\"date\"].dt.month\n    cdf[\"year\"] = cdf[\"date\"].dt.year\n    cdf[\"date\"] = pd.to_datetime(cdf[[\"year\", \"month\"]].assign(DAY=1))\n    cdf = cdf[[\"date\", \"item_cnt_day\", \"sales_per_item\"]].groupby(\"date\").sum().reset_index()\n    cdf = cdf[[\"date\", \"item_cnt_day\", \"sales_per_item\"]].groupby(cdf[\"date\"].dt.month).mean().reset_index()\n    cdf.rename(columns ={\"item_cnt_day\": \"item_cnt_month\", \"sales_per_item\": \"sales_per_month\"}, inplace=True)\n   \n    item_category_info_df.at[cid, \"mean_item_cnt_month\"] = cdf[\"item_cnt_month\"].mean()\n    item_category_info_df.at[cid, \"median_item_cnt_month\"] = cdf[\"item_cnt_month\"].median()\n    \n    #average number of items sold per month\n    month_mapping = {1: \"mean_cnt_jan\", 2: \"mean_cnt_feb\", 3: \"mean_cnt_mar\", 4: \"mean_cnt_apr\", 5: \"mean_cnt_may\", 6: \"mean_cnt_jun\",\n              7: \"mean_cnt_jul\", 8: \"mean_cnt_aug\", 9: \"mean_cnt_sep\", 10: \"mean_cnt_oct\", 11: \"mean_cnt_nov\", 12: \"mean_cnt_dez\"}\n    for m in cdf[\"date\"].unique():\n        item_category_info_df.at[cid, month_mapping[m]] = cdf[cdf[\"date\"]==m][\"item_cnt_month\"].values[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:37.998241Z","iopub.execute_input":"2021-11-07T10:21:37.998518Z","iopub.status.idle":"2021-11-07T10:21:41.265554Z","shell.execute_reply.started":"2021-11-07T10:21:37.998492Z","shell.execute_reply":"2021-11-07T10:21:41.264495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_category_info_df","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:41.267115Z","iopub.execute_input":"2021-11-07T10:21:41.267499Z","iopub.status.idle":"2021-11-07T10:21:41.299121Z","shell.execute_reply.started":"2021-11-07T10:21:41.267459Z","shell.execute_reply":"2021-11-07T10:21:41.297752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_cat_sales_dev_df = pd.DataFrame(columns=[\"item_cat_id\",\"month\", \"mean_cnt\"])\ni = 0\nfor index, row in item_category_info_df.iterrows():\n    for m in month_mapping.keys():\n        item_cat_sales_dev_df.at[i, \"item_cat_id\"] = index\n        item_cat_sales_dev_df.at[i, \"month\"] = m\n        item_cat_sales_dev_df.at[i, \"mean_cnt\"] = row[month_mapping[m]]\n        i += 1;\nitem_cat_sales_dev_df = item_cat_sales_dev_df.astype({\"item_cat_id\": \"int8\", \"month\": \"int32\", \"mean_cnt\": \"float32\"});","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:41.300698Z","iopub.execute_input":"2021-11-07T10:21:41.301116Z","iopub.status.idle":"2021-11-07T10:21:42.078273Z","shell.execute_reply.started":"2021-11-07T10:21:41.301072Z","shell.execute_reply":"2021-11-07T10:21:42.077437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cast date to datetime\nm_df = eda_df.copy()\n\nm_df['date']= pd.to_datetime(m_df['date'])\nm_df[\"month\"] = m_df[\"date\"].dt.month\nm_df[\"year\"] = m_df[\"date\"].dt.year\nm_df['date'] = pd.to_datetime(m_df[['year', 'month']].assign(DAY=28))\nm_df = m_df[[\"date\", \"item_cnt_day\", \"sales_per_item\"]].groupby(\"date\").sum().reset_index()\nm_df.rename(columns ={\"item_cnt_day\": \"item_cnt_month\", \"sales_per_item\": \"sales_per_month\"}, inplace=True)\nm_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:42.079405Z","iopub.execute_input":"2021-11-07T10:21:42.079633Z","iopub.status.idle":"2021-11-07T10:21:43.153457Z","shell.execute_reply.started":"2021-11-07T10:21:42.07961Z","shell.execute_reply":"2021-11-07T10:21:43.152434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#add the shop_id \nm_shop_df = eda_df.copy()\n\nm_shop_df['date']= pd.to_datetime(m_shop_df['date'])\nm_shop_df[\"month\"] = m_shop_df[\"date\"].dt.month\nm_shop_df[\"year\"] = m_shop_df[\"date\"].dt.year\nm_shop_df['date'] = pd.to_datetime(m_shop_df[['year', 'month']].assign(DAY=28))\nm_shop_df = m_shop_df[[\"date\", \"item_cnt_day\", \"sales_per_item\", \"shop_id\"]].groupby([\"date\", \"shop_id\"]).sum().reset_index()\nm_shop_df.rename(columns ={\"item_cnt_day\": \"item_cnt_month\", \"sales_per_item\": \"sales_per_month\"}, inplace=True)\nm_shop_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:43.154826Z","iopub.execute_input":"2021-11-07T10:21:43.155145Z","iopub.status.idle":"2021-11-07T10:21:44.296777Z","shell.execute_reply.started":"2021-11-07T10:21:43.155117Z","shell.execute_reply":"2021-11-07T10:21:44.295804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create shop_info_df which is shop informationn\nshop_info_df = pd.DataFrame(columns=[\"shop_name\", \"num_products\", \"fist_business_m\", \"last_business_m\",\n                                     \"min_price\", \"max_price\", \"mean_price\", \"median_price\", \"mean_sales_pm\", \"median_sales_pm\"],\n                            index = m_shop_df[\"shop_id\"].unique())\n\nfor sid in m_shop_df[\"shop_id\"].unique():\n    shop_info_df.at[sid, \"shop_name\"] = shops_df[shops_df[\"shop_id\"]==sid][\"shop_name\"].values[0]\n    shop_info_df.at[sid, \"num_products\"] = train_df[train_df[\"shop_id\"]==sid][\"item_id\"].nunique()\n    sdf= m_shop_df[m_shop_df[\"shop_id\"]==sid]\n    shop_info_df.at[sid, \"fist_business_m\"] = sdf[\"date\"].min()\n    shop_info_df.at[sid, \"last_business_m\"] = sdf[\"date\"].max()\n    shop_info_df.at[sid, \"min_price\"] = train_df[train_df[\"shop_id\"]==sid][\"item_price\"].min()\n    shop_info_df.at[sid, \"max_price\"] = train_df[train_df[\"shop_id\"]==sid][\"item_price\"].max()\n    shop_info_df.at[sid, \"mean_price\"] = train_df[train_df[\"shop_id\"]==sid][\"item_price\"].mean()\n    shop_info_df.at[sid, \"median_price\"] = train_df[train_df[\"shop_id\"]==sid][\"item_price\"].median()\n    shop_info_df.at[sid, \"mean_sales_pm\"] = sdf[\"sales_per_month\"].mean()\n    shop_info_df.at[sid, \"median_sales_pm\"] = sdf[\"sales_per_month\"].median()\n    \"\"\"\n    We will do it a little bit different for this shop dataframe. Here, the average turnover per month would\n    not be so interesting.Instead, we can use our m_shop_df for the sales figures per month.\n    \"\"\"\n# change datatypes (datetime float and int)\nshop_info_df['fist_business_m']= pd.to_datetime(shop_info_df['fist_business_m'])\nshop_info_df['last_business_m']= pd.to_datetime(shop_info_df['last_business_m'])\nshop_info_df[\"fist_business_m\"] = shop_info_df[\"fist_business_m\"].dt.strftime(\"%Y-%m\")\nshop_info_df[\"last_business_m\"] = shop_info_df[\"last_business_m\"].dt.strftime(\"%Y-%m\")\nshop_info_df = shop_info_df.astype({'num_products': 'int32',\n                                    \"min_price\": 'float32',\n                                    \"max_price\": 'float32',\n                                    \"mean_price\": 'float32',\n                                    \"median_price\": 'float32',\n                                    \"mean_sales_pm\": 'float32',\n                                    \"median_sales_pm\": 'float32'})","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:44.298279Z","iopub.execute_input":"2021-11-07T10:21:44.298668Z","iopub.status.idle":"2021-11-07T10:21:46.06831Z","shell.execute_reply.started":"2021-11-07T10:21:44.298628Z","shell.execute_reply":"2021-11-07T10:21:46.067243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data cleaning & Grouping by month**","metadata":{}},{"cell_type":"code","source":"def preprocessing(data, item_data=items_df, shop_data=shops_df, category_data=item_categories_df):\n    \"\"\"\n    Some basic stuff.\n    \"\"\" \n    print(50*'-')\n    print(\"preprocessing...\")\n    # 1). Create a copy of the Dataframe.\n    df = data.copy()\n    # 2). Remove all values with item_cnt_day < 1.\n    df = df[df[\"item_cnt_day\"]>0]\n    #3). Add Month feature\n    df[\"date\"]= pd.to_datetime(df[\"date\"], format='%d.%m.%Y')\n    df[\"month\"] = df[\"date\"].dt.month\n    #4). Group by date_block_nu$m.\n    df = df[[\"month\", \"date_block_num\", \"shop_id\", \"item_id\", \"item_price\", \"item_cnt_day\"]].groupby(\n        [\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n        {\"item_price\": \"mean\",\"item_cnt_day\": \"sum\", \"month\": \"min\"}).reset_index()\n    df.rename(columns={\"item_cnt_day\": \"item_cnt_month\"}, inplace=True)\n    #5). Add category_id and item_name.\n    df = pd.merge(df, item_data, on=\"item_id\", how=\"inner\")\n    #6). Add shop_name \n    df = pd.merge(df, shop_data, on=\"shop_id\", how=\"inner\")\n    #7). Add category_name\n    df = pd.merge(df, category_data, on=\"item_category_id\", how=\"inner\")\n    print(\"done.\")\n    print(50*'-')\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:46.069893Z","iopub.execute_input":"2021-11-07T10:21:46.070292Z","iopub.status.idle":"2021-11-07T10:21:46.084397Z","shell.execute_reply.started":"2021-11-07T10:21:46.070253Z","shell.execute_reply":"2021-11-07T10:21:46.080845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"piped_df = preprocessing(data=train_df)\npiped_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:46.085877Z","iopub.execute_input":"2021-11-07T10:21:46.0863Z","iopub.status.idle":"2021-11-07T10:21:48.767755Z","shell.execute_reply.started":"2021-11-07T10:21:46.086257Z","shell.execute_reply":"2021-11-07T10:21:48.766883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_city_feature(data):\n    data.loc[data[\"shop_name\"] == 'Сергиев Посад ТЦ \"7Я\"', \"shop_name\"] = 'СергиевПосад ТЦ \"7Я\"'\n    data[\"city\"] = data[\"shop_name\"].str.split(\" \").map(lambda x: x[0])\n    data.loc[data[\"city\"] == \"!Якутск\", \"city\"] = \"Якутск\"\n    data[\"city_code\"] = data[\"city\"].factorize()[0]\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:48.768978Z","iopub.execute_input":"2021-11-07T10:21:48.769253Z","iopub.status.idle":"2021-11-07T10:21:48.774737Z","shell.execute_reply.started":"2021-11-07T10:21:48.769226Z","shell.execute_reply":"2021-11-07T10:21:48.773693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"piped_df=add_city_feature(piped_df)\npiped_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:48.776164Z","iopub.execute_input":"2021-11-07T10:21:48.776436Z","iopub.status.idle":"2021-11-07T10:21:53.657845Z","shell.execute_reply.started":"2021-11-07T10:21:48.776408Z","shell.execute_reply":"2021-11-07T10:21:53.656949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shop_info_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:53.658962Z","iopub.execute_input":"2021-11-07T10:21:53.659215Z","iopub.status.idle":"2021-11-07T10:21:53.675557Z","shell.execute_reply.started":"2021-11-07T10:21:53.659191Z","shell.execute_reply":"2021-11-07T10:21:53.674502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add city\nshop_info_df=add_city_feature(shop_info_df)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:53.677025Z","iopub.execute_input":"2021-11-07T10:21:53.677273Z","iopub.status.idle":"2021-11-07T10:21:53.690056Z","shell.execute_reply.started":"2021-11-07T10:21:53.677249Z","shell.execute_reply":"2021-11-07T10:21:53.689092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shop_info_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:53.691424Z","iopub.execute_input":"2021-11-07T10:21:53.691786Z","iopub.status.idle":"2021-11-07T10:21:53.714636Z","shell.execute_reply.started":"2021-11-07T10:21:53.69174Z","shell.execute_reply":"2021-11-07T10:21:53.713583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert data to int\nfist_date = datetime.strptime(\"2013-01-01\", \"%Y-%m-%d\").date()\n\nshop_info_df['fist_business_m']= pd.to_datetime(shop_info_df['fist_business_m'])\nshop_info_df['last_business_m']= pd.to_datetime(shop_info_df['last_business_m'])\n\nshop_info_df[\"fist_business_m\"] = shop_info_df[\"fist_business_m\"].map(lambda x: (x.date() - fist_date).days)\nshop_info_df[\"last_business_m\"] = shop_info_df[\"last_business_m\"].map(lambda x: (x.date() - fist_date).days)\nshop_info_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:53.717847Z","iopub.execute_input":"2021-11-07T10:21:53.718133Z","iopub.status.idle":"2021-11-07T10:21:53.74208Z","shell.execute_reply.started":"2021-11-07T10:21:53.718103Z","shell.execute_reply":"2021-11-07T10:21:53.740913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# do pca and clustering\ndef get_cluster_feature(data, columns, index_name, cluster_name, num_cluster=5, linkage=\"average\", n_pca_components=3, svd_solver=\"auto\", visualize=True):\n    df = data[columns].copy()\n    \n    # PCA\n    pca = PCA(n_components=n_pca_components)\n    components = pca.fit_transform(df)\n    components = pd.DataFrame(components)\n    \n    # Clustering\n    clusterer = AgglomerativeClustering(n_clusters=num_cluster, linkage=linkage)\n    labels = clusterer.fit_predict(components)\n    x = components[0]\n    y = components[1]\n    \n    # Evaluation:\n    scorelist = []\n    nrange = range(2, 6)\n    for n in nrange:\n        clusterer = AgglomerativeClustering(n_clusters=n)\n        l = clusterer.fit_predict(components)\n        silscore = silhouette_score(df, l)\n        scorelist.append(silscore)\n        \n    for i in df.index:\n        df.at[i, cluster_name] = labels[i]\n        df = df[cluster_name].reset_index()\n        df.rename(columns={\"index\": index_name}, inplace = True)\n    \n    # plotting:    \n    if visualize:\n        fig = plt.figure(figsize=(25,15))\n        gs = fig.add_gridspec(2, 2)\n        ax00 = fig.add_subplot(gs[0,0])\n        ax01 = fig.add_subplot(gs[0,1])\n        ax02 = fig.add_subplot(gs[1,:])\n        ax00.tick_params(axis='both', labelsize=15)\n        ax01.tick_params(axis='both', labelsize=15)\n        ax02.tick_params(axis='both', labelsize=15)\n        ax00.set_title('PCA Components', fontsize=20)\n        ax01.set_title('Cluster Score by Number of Clusters', fontsize=20)\n        ax02.set_title('Clustering', fontsize=20)\n\n        ax00.set(xlabel='component number', ylabel='covered variance')\n        ax01.set(xlabel='number of clusters', ylabel='silhouette score')\n        ax02.set(xlabel='component 1 score', ylabel='component 2 score')\n\n        sns.barplot(x=list(range(pca.n_components_)), y=pca.explained_variance_ratio_, ax=ax00, palette=\"Set2\")\n        sns.lineplot(x=nrange, y=scorelist, ax=ax01, color=\"darkblue\")\n        sns.scatterplot(x=x, y=y, hue=labels, ax=ax02, palette=\"dark\")\n\n        fig.subplots_adjust(top=0.9)\n        fig.suptitle(f\"PCA and Clustering Output (num_cluster = {num_cluster})\", fontsize=\"28\")\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:53.743837Z","iopub.execute_input":"2021-11-07T10:21:53.744517Z","iopub.status.idle":"2021-11-07T10:21:53.757434Z","shell.execute_reply.started":"2021-11-07T10:21:53.744467Z","shell.execute_reply":"2021-11-07T10:21:53.75679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shop_info_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:53.758797Z","iopub.execute_input":"2021-11-07T10:21:53.759479Z","iopub.status.idle":"2021-11-07T10:21:53.781023Z","shell.execute_reply.started":"2021-11-07T10:21:53.759401Z","shell.execute_reply":"2021-11-07T10:21:53.780176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#find numeric columns\nnumeric_shop_columns = shop_info_df.select_dtypes(include=[\"int32\", \"int64\", \"float32\"]).columns\nshop_cluster_df = get_cluster_feature(data=shop_info_df, columns=numeric_shop_columns, n_pca_components=2, index_name=\"shop_id\", cluster_name=\"shop_cluster\");","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:53.782396Z","iopub.execute_input":"2021-11-07T10:21:53.782837Z","iopub.status.idle":"2021-11-07T10:21:54.636225Z","shell.execute_reply.started":"2021-11-07T10:21:53.782807Z","shell.execute_reply":"2021-11-07T10:21:54.635147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#add shop_cluster_df\npiped_df = pd.merge(piped_df, shop_cluster_df, on=\"shop_id\", how=\"inner\")\npiped_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:54.63775Z","iopub.execute_input":"2021-11-07T10:21:54.638126Z","iopub.status.idle":"2021-11-07T10:21:55.024883Z","shell.execute_reply.started":"2021-11-07T10:21:54.638085Z","shell.execute_reply":"2021-11-07T10:21:55.024076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_category_info_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:55.026151Z","iopub.execute_input":"2021-11-07T10:21:55.026506Z","iopub.status.idle":"2021-11-07T10:21:55.052126Z","shell.execute_reply.started":"2021-11-07T10:21:55.026468Z","shell.execute_reply":"2021-11-07T10:21:55.050848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert data columns to int\nitem_category_info_df['first_sold']= pd.to_datetime(item_category_info_df['first_sold'])\nitem_category_info_df['last_sold']= pd.to_datetime(item_category_info_df['last_sold'])\n\nitem_category_info_df[\"first_sold\"] = item_category_info_df[\"first_sold\"].map(lambda x: (x.date() - fist_date).days)\nitem_category_info_df[\"last_sold\"] = item_category_info_df[\"last_sold\"].map(lambda x: (x.date() - fist_date).days)\nitem_category_info_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:55.053469Z","iopub.execute_input":"2021-11-07T10:21:55.053856Z","iopub.status.idle":"2021-11-07T10:21:55.090688Z","shell.execute_reply.started":"2021-11-07T10:21:55.053816Z","shell.execute_reply":"2021-11-07T10:21:55.089627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#obtain numeric columns\n#fill na\n\nitem_category_info_df = item_category_info_df.astype({\"name\": \"string\"})\nnumeric_item_columns = item_category_info_df.select_dtypes(include=[\"float64\", \"int64\", \"object\"]).columns\nitem_category_info_df.fillna(0, inplace=True);","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:55.092178Z","iopub.execute_input":"2021-11-07T10:21:55.092456Z","iopub.status.idle":"2021-11-07T10:21:55.116709Z","shell.execute_reply.started":"2021-11-07T10:21:55.09243Z","shell.execute_reply":"2021-11-07T10:21:55.11571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_cat_claster = get_cluster_feature(data=item_category_info_df, columns=numeric_item_columns, index_name=\"item_category_id\", cluster_name=\"item_cat_cluster\")","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:55.117648Z","iopub.execute_input":"2021-11-07T10:21:55.11792Z","iopub.status.idle":"2021-11-07T10:21:55.960994Z","shell.execute_reply.started":"2021-11-07T10:21:55.117893Z","shell.execute_reply":"2021-11-07T10:21:55.960095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#add item categorie cluster\npiped_df = pd.merge(piped_df, item_cat_claster, on=\"item_category_id\", how=\"inner\")","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:55.962234Z","iopub.execute_input":"2021-11-07T10:21:55.962504Z","iopub.status.idle":"2021-11-07T10:21:56.418036Z","shell.execute_reply.started":"2021-11-07T10:21:55.962477Z","shell.execute_reply":"2021-11-07T10:21:56.417085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"piped_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:56.419129Z","iopub.execute_input":"2021-11-07T10:21:56.419383Z","iopub.status.idle":"2021-11-07T10:21:56.437491Z","shell.execute_reply.started":"2021-11-07T10:21:56.419358Z","shell.execute_reply":"2021-11-07T10:21:56.436571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#adding lag feature and label\ndef add_lag_feature_and_label(args):\n    \n    def add_lags(df, date_block):\n        for lag in range(num_lags):\n            if (date_block-lag-1) in item_df[\"date_block_num\"].values:\n                lag_value = item_df[item_df[\"date_block_num\"]==date_block-lag-1][\"item_cnt_month\"].values[0]\n                df.at[index, f\"lag_{lag+1}\"] = lag_value\n        return df \n    df = args[0].copy()\n    num_lags = args[1]\n    target_date_block = args[2]\n    for lag in range(num_lags):\n        df[f\"lag_{lag+1}\"] = 0\n    for shop in df[\"shop_id\"].unique():\n        shop_df = df[df[\"shop_id\"]==shop].copy()\n        for item in shop_df[\"item_id\"].unique():\n            item_df = shop_df[shop_df[\"item_id\"]==item].copy()\n            last_index = 0\n            for index, row in item_df.iterrows():\n                date_block = row[\"date_block_num\"]\n                if target_date_block and date_block == target_date_block:\n                    df = add_lags(df, date_block)\n                if target_date_block is None:\n                    df = add_lags(df, date_block)\n    if target_date_block:\n        df = df[df[\"date_block_num\"]==target_date_block].copy()\n    df.rename(columns={\"item_cnt_month\":\"label\"}, inplace=True)\n    return df\n\ndef parallelize_lag_and_target_processing(df, num_lags, target_date_block_num=None, func=add_lag_feature_and_label, n_cores=4, shops=None, items=None):\n    if target_date_block_num:\n        # get list of valid date_block_num values:\n        valid_date_blocks = range(target_date_block_num - num_lags, target_date_block_num + 1)\n        df = df[df[\"date_block_num\"].isin(valid_date_blocks)].copy()\n    if shops:\n        df = df[df[\"shop_id\"].isin(shops)].copy()\n    if items:\n        df = df[df[\"item_id\"].isin(items).copy()]\n    df.sort_values(by=\"shop_id\", inplace=True)\n    df_split = np.array_split(df, n_cores)\n    param_list = [[df_, num_lags, target_date_block_num] for df_ in df_split]\n    pool = Pool(n_cores)\n    df = pd.concat(pool.map(func, param_list))\n    pool.close()\n    pool.join()\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:56.439013Z","iopub.execute_input":"2021-11-07T10:21:56.439316Z","iopub.status.idle":"2021-11-07T10:21:56.452523Z","shell.execute_reply.started":"2021-11-07T10:21:56.439286Z","shell.execute_reply":"2021-11-07T10:21:56.451449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model**","metadata":{}},{"cell_type":"code","source":"\n#evaluate model\ndef evaluate_xgboost(model):\n        results = model.evals_result()\n        fig = plt.figure(figsize=(25,10))\n        gs = fig.add_gridspec(1, 2)\n        ax00 = fig.add_subplot(gs[0,0])\n        ax01 = fig.add_subplot(gs[0,1])\n        ax00.tick_params(axis='both', labelsize=15)\n        ax01.tick_params(axis='both', labelsize=15)\n        ax00.set_title('Feature Importance', fontsize=20)\n        ax01.set_title('loss vs validation loss', fontsize=20)\n        ax00.set(xlabel='Importance', ylabel='Feature')\n        ax01.set(xlabel='n_estimators', ylabel='rmse')\n        sns.barplot(y=model.get_booster().feature_names, x=model.feature_importances_, ax=ax00, palette=\"Set2\", orient=\"h\")\n        sns.lineplot(x=range(model.n_estimators), y=results[\"validation_0\"][\"rmse\"], ax=ax01, color=\"darkblue\", label=\"loss\")\n        sns.lineplot(x=range(model.n_estimators), y=results[\"validation_1\"][\"rmse\"], ax=ax01, color=\"orange\", label=\"validation loss\")\n        fig.subplots_adjust(top=0.9)\n        fig.suptitle(\"Model Evaluation\", fontsize=\"28\")","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:56.454157Z","iopub.execute_input":"2021-11-07T10:21:56.454796Z","iopub.status.idle":"2021-11-07T10:21:56.463948Z","shell.execute_reply.started":"2021-11-07T10:21:56.454755Z","shell.execute_reply":"2021-11-07T10:21:56.463328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#adding lag feature and label\n#avg_lags per item feature\ndef add_avg_lag_feature(args):\n    \n    def add_avg_lags(df, date_block):\n        for lag in range(num_lags):\n            if (date_block-lag-1) in item_df[\"date_block_num\"].values:\n                lag_df = item_df[item_df[\"date_block_num\"]==date_block-lag-1]\n                lag_value = lag_df[\"item_cnt_month\"].mean()\n                df.at[index, f\"avg_lag_{lag+1}\"] = lag_value\n        return df \n    \n    df = args[0].copy()\n    num_lags = args[1]\n    target_date_block = args[2]\n    for lag in range(num_lags):\n        df[f\"avg_lag_{lag+1}\"] = 0\n\n    for item in df[\"item_id\"].unique():\n        item_df = df[df[\"item_id\"]==item].copy()\n        last_index = 0\n        for index, row in item_df.iterrows():\n            date_block = row[\"date_block_num\"]\n            if target_date_block and date_block == target_date_block:\n                df = add_avg_lags(df, date_block)\n            if target_date_block is None:\n                df = add_avg_lags(df, date_block)\n    return df\n\ndef parallelize_avg_lag_processing(df, num_lags, target_date_block_num=None, func=add_avg_lag_feature, n_cores=4, shops=None, items=None):\n    if target_date_block_num:\n        # get list of valid date_block_num values:\n        valid_date_blocks = range(target_date_block_num - num_lags, target_date_block_num + 1)\n        df = df[df[\"date_block_num\"].isin(valid_date_blocks)].copy()\n    if shops:\n        df = df[df[\"shop_id\"].isin(shops)].copy()\n    if items:\n        df = df[df[\"item_id\"].isin(items).copy()]\n    df.sort_values(by=\"item_id\", inplace=True)\n    df_split = np.array_split(df, n_cores)\n    param_list = [[df_, num_lags, target_date_block_num] for df_ in df_split]\n    pool = Pool(n_cores)\n    df = pd.concat(pool.map(func, param_list))\n    pool.close()\n    pool.join()\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:56.465349Z","iopub.execute_input":"2021-11-07T10:21:56.465931Z","iopub.status.idle":"2021-11-07T10:21:56.479522Z","shell.execute_reply.started":"2021-11-07T10:21:56.465889Z","shell.execute_reply":"2021-11-07T10:21:56.478801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#combines previous steps without the preprocessing function\ndef pipeline(data, num_lags, num_avg_lags, target_date_block_num=None, first_data_str=\"2013-01-01\",\n             shop_info_data=shop_info_df, categorie_info_data=item_category_info_df,\n             shops=None, items=None):\n    \n    print(100*\"#\")\n    print(f\"running pipeline for target_date_block_num {target_date_block_num}  with {num_lags} lags...\")\n    fist_date = datetime.strptime(first_data_str, \"%Y-%m-%d\").date()\n    print(\"adding city feature...\")\n    df = add_city_feature(data)\n    print(\"done.\")\n    print(\"adding shop_cluster feature...\")\n    # preprocessing shop_info_df\n    shop_info_data['fist_business_m']= pd.to_datetime(shop_info_data['fist_business_m'])\n    shop_info_data['last_business_m']= pd.to_datetime(shop_info_data['last_business_m'])\n    shop_info_data[\"fist_business_m\"] = shop_info_data[\"fist_business_m\"].map(lambda x: (x.date() - fist_date).days)\n    shop_info_data[\"last_business_m\"] = shop_info_data[\"last_business_m\"].map(lambda x: (x.date() - fist_date).days)\n    numeric_shop_columns = shop_info_data.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\"]).columns\n    shop_cluster_df = get_cluster_feature(data=shop_info_data, columns=numeric_shop_columns, n_pca_components=2,\n                                          index_name=\"shop_id\", cluster_name=\"shop_cluster\", visualize=False)\n    # add shop_cluster\n    df = pd.merge(df, shop_cluster_df, on=\"shop_id\", how=\"inner\")\n    print(\"done.\")\n    print(\"adding item_cat_cluster feature...\")\n    # preprocessing categorie_info_data\n    categorie_info_data['first_sold']= pd.to_datetime(categorie_info_data['first_sold'])\n    categorie_info_data['last_sold']= pd.to_datetime(categorie_info_data['last_sold'])\n    categorie_info_data[\"first_sold\"] = categorie_info_data[\"first_sold\"].map(lambda x: (x.date() - fist_date).days)\n    categorie_info_data[\"last_sold\"] = categorie_info_data[\"last_sold\"].map(lambda x: (x.date() - fist_date).days)\n    categorie_info_data = categorie_info_data.astype({\"name\": \"string\"})\n    numeric_item_columns = categorie_info_data.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\", \"object\"]).columns\n    categorie_info_data.fillna(0, inplace=True)\n    item_cluster_df = get_cluster_feature(data=categorie_info_data, columns=numeric_item_columns, n_pca_components=3,\n                                          index_name=\"item_category_id\", cluster_name=\"item_cat_cluster\", visualize=False)\n    # add item_cat_cluster\n    df = pd.merge(df, item_cluster_df, on=\"item_category_id\", how=\"inner\")\n    print(\"done.\")\n    print(\"adding avg lag features...\")\n    df = parallelize_avg_lag_processing(df=df, shops=shops, items=items, num_lags=num_lags, target_date_block_num=target_date_block_num)\n    #df = add_avg_lag_feature(df, num_avg_lags, target_date_block_num)\n    print(\"done.\")\n    print(\"adding lag features...\")\n    df = parallelize_lag_and_target_processing(df, shops=shops, items=items, num_lags=num_lags, target_date_block_num=target_date_block_num)\n    print(\"done.\")\n    \n    print(\"dropping item_name, shop_name, item_category_name and city...\")\n    df.drop([\"item_name\", \"shop_name\", \"item_category_name\", \"city\"], axis=1, inplace=True)\n    print(\"done.\")\n    print(100*\"#\")\n    return df\n\ndef get_training_data(num_lags, num_avg_lags,  target_data_block_number=None, train_data=train_df):\n    df = pipeline(data=preprocessing(data=train_data), num_lags=num_lags, num_avg_lags=num_avg_lags,\n                  target_date_block_num=target_data_block_number)\n    #clap the label and all lag features:\n    columns = [col for col in df.columns if col[:3]==\"lag\"]\n    columns = [\"label\"] + columns\n    for col in columns:\n        df[col] = np.where(df[col]>20, 20,df[col])\n    #delete the the fürst \"num_lags\" date_block_nums\n    valid_data_block_num = range(num_lags,34)\n    df = df[df[\"date_block_num\"].isin(valid_data_block_num)]\n\n    y = df[\"label\"]\n    X = df.copy()\n    X.drop([\"label\"], axis=1, inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:56.481101Z","iopub.execute_input":"2021-11-07T10:21:56.481501Z","iopub.status.idle":"2021-11-07T10:21:56.499338Z","shell.execute_reply.started":"2021-11-07T10:21:56.481459Z","shell.execute_reply":"2021-11-07T10:21:56.498294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#obtain training data\n#split into test set which is the final month and training set which is first 33 months\nDF = get_training_data(num_lags=3, num_avg_lags=3)\n\ntrain = DF[DF.date_block_num != 33]\ntest = DF[DF.date_block_num == 33]\nX_train = train.copy()\nX_train.drop([\"label\"], axis=1, inplace=True)\nX_test = test.copy()\nX_test.drop([\"label\"], axis=1, inplace=True)\n\ny_train = train[\"label\"]\ny_test = test[\"label\"]","metadata":{"execution":{"iopub.status.busy":"2021-11-07T10:21:56.500835Z","iopub.execute_input":"2021-11-07T10:21:56.5015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train model\nevalset = [(X_train, y_train), (X_test,y_test)]\nxgb_model2 = xgb.XGBRegressor(max_depth=5, n_estimators=100, subsample=0.6, eval_metric='rmse', learning_rate=0.1)\nxgb_model2.fit(X_train, y_train, eval_set=evalset)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict and calculat the rmse for test & train data\ntest_prediction = xgb_model2.predict(X_test)\ntrain_prediction = xgb_model2.predict(X_train)\nrmse_test = mean_squared_error(y_true = y_test, y_pred = test_prediction)**(0.5)\nrmse_train = mean_squared_error(y_true = y_train, y_pred = train_prediction)**(0.5)\nprint(50*\"*\")\nprint(f\"RMSE test: {rmse_test}\")\nprint(f\"RMSE train: {rmse_train}\")\nprint(50*\"*\")\n#evaluate model\nevaluate_xgboost(xgb_model2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}