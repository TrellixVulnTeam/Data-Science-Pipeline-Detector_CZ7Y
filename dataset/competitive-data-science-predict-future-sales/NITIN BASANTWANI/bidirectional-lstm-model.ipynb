{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras \nfrom keras.utils import np_utils \nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"root = '../input/competitive-data-science-predict-future-sales/'\nt_root = '../input/translated-data/'\nd1 = pd.read_csv(t_root + 'categories.csv')\nd2 = pd.read_csv(t_root + 'items.csv')\nd3 = pd.read_csv(root + 'sales_train.csv')\nd4 = pd.read_csv(t_root + 'shops.csv')\ntest = pd.read_csv(root + 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=list()\nd.append(d1),d.append(d2),d.append(d3),d.append(d4),\nfor i in range(4):\n    print(d[i].head())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d3.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the sales_train data i.e d3 has both shop_id & item_id and have no missing values it can be used as an primary Dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.merge(d3,d2, on='item_id',how='left')\nsales = pd.merge(sales,d1, on='category_id',how='left')\nsales = pd.merge(sales,d4, on='shop_id',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking for missing Values "},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Since the Count of all the columns we are equal, we can conclude that we got no missing values."},{"metadata":{},"cell_type":"markdown","source":"### **checking for outliers**\n###### *the columns that we gotta be extra careful are item_cnt_day,item_price*"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(sales['item_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(sales['item_cnt_day'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(sales['item_cnt_day'],sales['item_price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So both the columns have a few extreme values, since there are only a few of them we can simply ignore them or just impute them "},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['item_price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['item_price'].max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's Try to impute the values of item_price using the simple quantile method of dealing with outliers,\nIt assumes that our data is normally distributed "},{"metadata":{"trusted":true},"cell_type":"code","source":"temp1 = sales.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q1 = temp1['item_price'].quantile(0.25)\nq3 = temp1['item_price'].quantile(0.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iqr = q3 - q1\niqr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"upper_lm = q3 + 1.5*iqr\nlower_lm = 0\nupper_lm,lower_lm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imputer(value):\n    if value > upper_lm:\n        return upper_lm\n    if value < lower_lm:\n        return lower_lm\n    else:\n        return value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp1['item_price'] = temp1['item_price'].apply(imputer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp1['item_price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(temp1['item_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(temp1['item_price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's do a quick sanity check and see if we use this method for treating outliers how well it's gonna perform on dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp1['item_price'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= sales[sales['item_price'] > 2124]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see this imputation by quantile method though makes the data normally distributed but makes us lose a lot of vital information and therefore can lead to a false predictions "},{"metadata":{},"cell_type":"markdown","source":"so let's see if we can ignore the outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(sales['item_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## since the data got only one outliers and rest other data is in less 70000\nsales.item_price.max()\nx = sales[sales['item_price'] > 50000]\nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### since the data got only three such values which are greater than 50000 let's see if we can ignore them\n#### for this we can check out the test data and see if the test data have the following rows or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a= test[test['item_id']==6066]\nb = test[test['item_id']==13199]\nc = test[test['item_id']==11365]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the item_id with 11365 is present in the test data we can't ignore that value and hence the upper limit of data should be set to 60000 while the lower limit should be set to 0 since we can't have a -ve value as price. lol that would imply cashier paying you for buying the product."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales[sales['item_price'] < 60000]\nsales['item_price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now to check the items with price below 0 "},{"metadata":{"trusted":true},"cell_type":"code","source":"x = sales[sales['item_price'] < 0]\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = test[test['item_id'] == 2973]\ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since this item_id isn't present in the test dataset it's better to ignore this isntead of imputing it using 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales[sales['item_price'] > 0]\nsales.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Treating the outliers in the column item_cnt_day"},{"metadata":{},"cell_type":"markdown","source":"Fortunately sometimes ignoring abnormally high value in a single columnn results in treating outliers or high values in other columns since the whole row gets deleted. But it's always better to crosscheck the same. \n#Safe_practices"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['item_cnt_day'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that we have a minimumm value of -22 which might be the number of that certain item re\nreturend back to the shop, this isn't a outlier but since we only want to predict the number of item being sold it's better to remove this info as it might be redundant and can cause problem at the time of final prediction. "},{"metadata":{"trusted":true},"cell_type":"code","source":"returned_items = sales[sales['item_cnt_day'] < 0]\nreturned_items['item_cnt_day'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"returned_items","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But since we there are around more than 7000 rows that have -ve values which implies the items that were returned back to the store on different dates. Just ignoring all of them would mean loosing a lot of imforamtion so it's better to make a copy of data before proceeding further.\n\nWe can generate new feature using this new data frame, like the frequency of a particular item being returned in a month so better train our model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_copy = sales.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales[sales['item_cnt_day'] > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['item_cnt_day'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(sales['item_cnt_day'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since most of the data has the values below approx 1100, let's see what item is the outlier and if it's present in the test data as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = sales[sales['item_cnt_day'] >= 1000]\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = test[test['item_id']==11373 ]\ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the item is available in the data it won't be a good choice to simply remove this,\nsince a very high value can sometimes alter the whole population mean and just because we have the evaluation metircs as RMSE which takes the mean squared error so keeping this value as it is might affect the whole model's accuracy and loss.\nlet's first let's make a copy of the sales data and in the copied data, \nwe'll make two models 1st with the original data with the count of the item unchnaged & The 2nd model with the item count changed to 1000 since that is the upper limit.\nfuthermore the product with item_id 20949, Corporate package T-shirt 1C Interest white is also a single item but a pretty high value, it might affect the mean too, so in the copied data we can clip it's value to 700 as all the other values are below 669."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_copy = sales.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_copy1 = sales_copy[sales_copy['item_cnt_day'] <= 1000]\nsales_copy1['item_cnt_day'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_copy2 = sales_copy.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_copy2['item_cnt_day'] = np.where(sales_copy2['item_cnt_day'] >=1000 , int(670),sales_copy2['item_cnt_day'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_copy2['item_cnt_day'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Basic LSTM model."},{"metadata":{},"cell_type":"markdown","source":"for LSTM model we'll need a sequence of timestamps to be fed to LSTM, since we already have date_block_num which hows the month, we can take up a sum of item_cnt."},{"metadata":{},"cell_type":"markdown","source":"For this model i'm gonna use the Sales_copy dataset which have the values 2169 included."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_copy['item_id'].min()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So a LSTM model works with Time stamps or Sequences and since we already have date_num_blocks as the months number in a increasing order, we can use that as the timeseq varaible."},{"metadata":{},"cell_type":"markdown","source":"so the approach is goning to be pretty basic i'm gonna use the information provided in the train data about the item_id, shop_id merge it with the test data, since we need the final prediction on test data, i'm gonna use the first 33 months as input and try to get the prediction on of 34th month i.e october 2015, and compare our predictions with the values of 34th month that we already have. So we are going to use the 34th months values as label to our data.\n\n</br> After this I'm gonna feed the value of months from 2-34 column since my LSTM would expect a imput of 33 values and try to predict the values for 35th month i.e november."},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_dat = pd.pivot_table(sales_copy,index=['shop_id','item_id'],columns=['date_block_num'],values=['item_cnt_day'],fill_value=0,aggfunc=sum)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I had to pivot the table on both the shop_id & item_id cause every shop have same ids for every item and taking a sum of them without segregating them would result in the items_count getting summed over all the shops or shops getting summed by different item_id and since in the test dataset the items are segregated shop_wise and then item_wise."},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_dat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_dat.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_dat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merging the data would result in the item_cnt getting mapped to the respective item_ids according to months and the redundant rows getting dropped since it'll be a left join."},{"metadata":{"trusted":true},"cell_type":"code","source":"time_seq_dat1 = pd.merge(test,lstm_dat,on=['item_id','shop_id'],how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_seq_dat1.fillna(0,inplace=True)\ntime_seq_dat1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the working of a LSTM is pretty simple we give a sequence of the numbers in a increasing order of timestamp and we predict the next the value in the series. Since we won't need ID, Item_id, Shop_id \nwe'll drop them"},{"metadata":{"trusted":true},"cell_type":"code","source":"time_seq_dat = time_seq_dat1.drop(columns=['ID','item_id','shop_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_seq_dat.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have a sequence of the item_cnt_days in a increasing order of time."},{"metadata":{"trusted":true},"cell_type":"code","source":"time_seq_dat.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we gotta make inputs and output variable, so for input we'll use the first 33 months, and use the lables of 34th month i.e october 2015 as output to be the labels or output variable.\n\n--> for X_test we'll use the columns or timestamp 2-33 and try to predict the output for 35th i.e November 2015  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"time_seq_dat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = np.reshape(time_seq_dat.values[:,:-1],newshape=(-1,33,1))\ny_train = np.reshape(time_seq_dat.values[:,-1:],newshape=(-1,1,1))\nx_train.shape,y_train.shape ##sanity check ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = np.reshape(time_seq_dat.values[:,1:],newshape=(-1,33,1))\nx_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now to make the final model\nA bidirectional LSTM model we learn the input in both the directions forward and backwards. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras \nfrom keras.models import Sequential\nfrom keras import layers as l \n\nmodel1 = Sequential()\nmodel1.add(l.Bidirectional(l.LSTM(100,activation = 'relu',input_shape = (33,1))))\nmodel1.add(l.Dense(1))\n\nmodel1.compile(optimizer='adam', loss='mse',metrics = ['MSE'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape[0]/2048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model1.fit(x_train,y_train,batch_size = 2048 ,epochs = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1 = model1.predict(x_test,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_copy['item_cnt_day'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'ID':test['ID'],'item_cnt_month':sub1.ravel()})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('sub14.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}