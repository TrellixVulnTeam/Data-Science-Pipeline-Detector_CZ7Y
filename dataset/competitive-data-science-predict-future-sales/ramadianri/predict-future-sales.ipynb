{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel is going to solve [Predict Future Sales Competition](https://www.kaggle.com/c/competitive-data-science-predict-future-sales) on Kaggle.\n\n**Competition Description:**\n\n\nThis challenge serves as final project for the [\"How to win a data science competition\"](https://www.coursera.org/learn/competitive-data-science/home/welcome) Coursera course.\n\nIn this competition you will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - [1C Company](http://1c.ru/eng/title.htm). \n\nWe are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries\nFirst, we import necessary libraries, such as:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import The Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"items = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\nitem_categories = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\n\ntrain = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\ntest = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\n\nsample_submission = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read The Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('item_categories')\ndisplay(item_categories.head())\n\nprint('items')\ndisplay(items.head())\n\nprint('shops')\ndisplay(shops.head())\n\nprint('train')\ndisplay(train.head())\n\nprint('test')\ndisplay(test.head())\n\nprint('sample_submission')\ndisplay(sample_submission.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Check train info","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Check for missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train')\ndisplay(train.isnull().sum())\n\nprint('test')\ndisplay(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Quick look using ```describe()``` function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train')\ndisplay(train.describe(include='all'))\n\nprint('test')\ndisplay(test.describe(include='all'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Quick observations:**\n- There are no missing values.\n- The train and test datasets did not match in term of features.\n- There is minus value(s) in item_price.\n- There is minus value(s) in item_cnt_day.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Removing Duplicates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop duplicates\nsubset = ['date','date_block_num','shop_id','item_id','item_cnt_day']\nprint(train.duplicated(subset=subset).value_counts())\ntrain.drop_duplicates(subset=subset, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check negative values in item_price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['item_price'] < 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there is only 1 negative value in item_price, we can just drop that because it won't affect the prediction too much.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop negative value in item_price\ntrain = train[train['item_price'] > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train['item_cnt_day'] > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning item_price and item_cnt_day\n- Check min and max","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(train['item_price']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(train['item_cnt_day']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Drop outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a drop outliers function\ndef drop_outliers(df, feature, percentile_high = .99):\n    '''df (dataframe)           : dataset\n       feature (string)         : column\n       percentile_high (float)  : upper limit\n       .........................................................\n    '''\n    #train size before dropping values\n    shape_init = df.shape[0]\n    \n    #get percentile value\n    max_value = df[feature].quantile(percentile_high)\n    \n    #drop outliers\n    print('dropping outliers...')\n    df = df[df[feature] < max_value]\n    \n    print(str(shape_init - df.shape[0]) + ' ' + feature + ' values over ' + str(max_value) + ' have been removed' )\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop outliers in item_price feature\ntrain = drop_outliers(train, 'item_price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop outliers in item_cnt_day\ntrain = drop_outliers(train, 'item_cnt_day')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Price\nMake a dataframe with item_price feature group by shop_id and item_id to get price for each item per shop. We can use this dataframe to create item_price feature for test dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prices_shop_df = train[['shop_id','item_id','item_price']]\nprices_shop_df = prices_shop_df.groupby(['shop_id','item_id']).apply(lambda df: df['item_price'][-2:].mean())\nprices_shop_df = prices_shop_df.to_frame(name = 'item_price')\n\nprices_shop_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can merge this dataframe with test dataset to create item_price feature in test dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.merge(test, prices_shop_df, how='left', left_on=['shop_id','item_id'], right_on=['shop_id','item_id'])\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for missing values\ntest['item_price'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are still missing values in test's item_price. We will fill this later by creating more features from item_categories.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Transform Data in Train Dataset As Monthly","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#split content in date into month and year\ntrain['month'] = [date.split('.')[1] for date in train['date']]\ntrain['year'] = [date.split('.')[2] for date in train['date']]\n\n#drop date and date_block_num features\ntrain.drop(['date','date_block_num'], axis=1, inplace=True)\n\n#create month and year features fot test dataset\ntest['month'] = '11'\ntest['year'] = '2015'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#change item_cnt_day into item_cnt_month\ntrain_monthly = train.groupby(['year','month','shop_id','item_id'], as_index=False)[['item_cnt_day']].sum()\ntrain_monthly.rename(columns={'item_cnt_day': 'item_cnt_month'}, inplace=True)\n\ntrain_monthly = pd.merge(train_monthly, prices_shop_df, how='left', left_on=['shop_id','item_id'], right_on=['shop_id','item_id'])\n\ntrain_monthly.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_monthly","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reindex test dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.reindex(columns=['ID','year','month','shop_id','item_id','item_price'])\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring other datasets\n- Exploring Item Categories dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract main categories\nitem_categories['main_category'] = [x.split(' - ')[0] for x in item_categories['item_category_name']]\n\n#some items don't have sub-categories. For those, we will use None as a sub-category (consider the main category as a sub)\nsub_categories = []\nfor i in range(len(item_categories)):\n    try:\n        sub_categories.append(item_categories['item_category_name'][i].split(' - ')[1])\n        \n    except IndexError as e:\n        sub_categories.append('None')\n        #sub_categories.append(item_categories['main_category'][i])\n\nitem_categories['sub_category'] = sub_categories\n\n#drop item_category_name\nitem_categories.drop(['item_category_name'], axis=1, inplace=True)\n\nitem_categories.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Exploring Items Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge with item_categories\nitems = pd.merge(items, item_categories, how='left')\n\n#drop item_name and item_category_id\nitems.drop(['item_name','item_category_id'], axis=1, inplace=True)\n\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge to train and test datasets\ntrain = pd.merge(train, items, how='left')\ntest = pd.merge(test, items, how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Exploring Shops Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from string import punctuation\n\n# replace all the punctuation in the shop_name columns\nshops[\"shop_name_cleaned\"] = shops[\"shop_name\"].apply(lambda s: \"\".join([x for x in s if x not in punctuation]))\n\n# extract the city name\nshops[\"shop_city\"] = shops[\"shop_name_cleaned\"].apply(lambda s: s.split()[0])\n\n#extract the type\nshops[\"shop_type\"] = shops[\"shop_name_cleaned\"].apply(lambda s: s.split()[1])\n\n#extract shop's name\nshops[\"shop_name\"] = shops[\"shop_name_cleaned\"].apply(lambda s: \" \".join(s.split()[2:]))\n\nshops.drop(['shop_name_cleaned'], axis=1, inplace=True)\n\nshops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge to train and test datasets\ntrain = pd.merge(train, shops, how='left')\ntest = pd.merge(test, shops, how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Display current train and test datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train')\ndisplay(train.head())\n\nprint('test')\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fill missing values in item_price (by item categories)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill missing values with median of each main_category and sub_category\ntest['item_price'] = test.groupby(['main_category','sub_category'])['item_price'].apply(lambda df: df.fillna(df.median()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['item_price'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill missing values with median of each sub_category\ntest['item_price'] = test.groupby(['sub_category'])['item_price'].apply(lambda df: df.fillna(df.median()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['item_price'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Show remaining missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test['item_price'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All remaining item_price's missing values have same main_category and sub_category. This main and sub categories are not in the test dataset, but in train dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill missing values with median of main_category and sub_category from train dataset\nfiller = train[(train['main_category'] == 'PC') & (train['sub_category'] == 'Гарнитуры/Наушники')]['item_price'].median()\n\ntest['item_price'].fillna(filler, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['item_price'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis: Epilogue\n- From competition's evaluation note, target values are clipped into [0,20] range.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['item_cnt_month'] = train['item_cnt_month'].clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Define target_array","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target_array = train['item_cnt_month']\ntrain.drop(['item_cnt_month'], axis=1, inplace=True)\n\ntest_id = test['ID']\ntest.drop(['ID'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Drop shop_id & item_id","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['shop_id','item_id'], axis=1, inplace=True)\ntest.drop(['shop_id','item_id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Reduce memory usage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    '''df (dataframe)  : data\n       Changes column types in the dataframe\n           `float64` type to `float32`\n           `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reduce memory\ndowncast_dtypes(train)\ndowncast_dtypes(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Check for missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for any missing data\nprint('missing data in the train dataset : ', train.isnull().any().sum())\nprint('missing data in the test dataset : ', test.isnull().any().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Normality test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a normality test function\ndef normalityTest(data, alpha=0.05):\n    \"\"\"data (array)   : The array containing the sample to be tested.\n\t   alpha (float)  : Significance level.\n\t   return True if data is normal distributed\"\"\"\n    \n    from scipy import stats\n    \n    statistic, p_value = stats.normaltest(data)\n    \n    #null hypothesis: array comes from a normal distribution\n    if p_value < alpha:  \n        #The null hypothesis can be rejected\n        is_normal_dist = False\n    else:\n        #The null hypothesis cannot be rejected\n        is_normal_dist = True\n    \n    return is_normal_dist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check normality of all numericaal features and transform it if not normal distributed\nfor feature in train.columns:\n    if (train[feature].dtype != 'object'):\n        if normalityTest(train[feature]) == False:\n            train[feature] = np.log1p(train[feature])\n            test[feature] = np.log1p(test[feature])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use numpy.log1p in order to target_array follows a normal distribution\ntarget_array = np.log1p(target_array)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\nenc = OrdinalEncoder()\n\nX = enc.fit_transform(train)\ny = target_array\n\nX_predict = enc.fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating a model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We begin by splitting data into two subsets: for training data and for testing data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .1, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use XGBRegressor model to predict total sales for every product and store in the next month.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n\n#create a model\nmodel = XGBRegressor()\n\n#fitting\nmodel.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_test, y_test)], \n    verbose=True, \n    early_stopping_rounds = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate Mean Squared Error\nfrom sklearn.metrics import mean_squared_error\n\nprint('MSE : ', mean_squared_error(y_test, model.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make a prediction\ny_predict = model.predict(X_predict)\n\n#transform the values back\ny_predict = np.expm1(y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sava results to a file\nresults = pd.DataFrame({'ID': test_id, 'item_cnt_month': y_predict})\nresults.to_csv('my_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}