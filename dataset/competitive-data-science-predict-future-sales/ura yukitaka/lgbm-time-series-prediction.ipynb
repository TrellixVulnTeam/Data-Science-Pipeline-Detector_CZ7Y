{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Ensemble  time series prediction"},{"metadata":{},"cell_type":"markdown","source":"This time, using Predict Future Sales as the subject, extraction of features by time series analysis and creation of time series feature quantities were performed to make future predictions. <br>\nFor feature quantity prediction, nonlinear regression LGBM and linear regression Ridge, Lasso, and ElasticNet were combined, and prediction was performed as ensemble learning."},{"metadata":{},"cell_type":"markdown","source":"### Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Libraries\nimport datetime\n\n# Visualization\nfrom matplotlib import pyplot as plt\nplt.style.use('fivethirtyeight')\nimport seaborn as sns\n\nimport statsmodels.api as sm\n\n# Statistics library\nfrom scipy.stats import norm\nfrom scipy import stats\nimport scipy\n\n# Data preprocessing\nfrom sklearn.model_selection import train_test_split\n\n# Machine learning\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\n\n# Validataion\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data loading and checking"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_items = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\", header=0)\ndf_shops = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\", header=0)\ndf_sales_train = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv\", header=0)\ndf_test = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/test.csv\", header=0)\ndf_category = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv\", header=0)\nsample = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_items.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_items.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_shops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_shops.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sales_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sales_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_category.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_category.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Null data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Null data df_sales_train:{}\".format(df_sales_train.isnull().sum().sum()))\nprint(\"Null data df_items:{}\".format(df_items.isnull().sum().sum()))\nprint(\"Null data df_shops:{}\".format(df_shops.isnull().sum().sum()))\nprint(\"Null data df_category:{}\".format(df_category.isnull().sum().sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Null value does not exist in the provided data."},{"metadata":{},"cell_type":"markdown","source":"### Data preprocessing\nFirst, data processing was performed for time series analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Data preprocessing, df_train, datetime\ndf_sales_train[\"date_dt\"] = pd.to_datetime(df_sales_train[\"date\"], format='%d.%m.%Y')\n\ndf_sales_train[\"year\"] = df_sales_train[\"date_dt\"].dt.year\ndf_sales_train[\"month\"] = df_sales_train[\"date_dt\"].dt.month\ndf_sales_train[\"day\"] = df_sales_train[\"date_dt\"].dt.day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sales_train[\"item_sales\"] = df_sales_train[\"item_price\"]*df_sales_train[\"item_cnt_day\"]\ndf_sales_train = pd.merge(df_sales_train, df_items[[\"item_id\", \"item_category_id\"]], left_on=\"item_id\", right_on=\"item_id\", how=\"left\")\ntrain_df = df_sales_train.drop(\"date\", axis=1).sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA\n## Time series of daily sales"},{"metadata":{},"cell_type":"markdown","source":"### Total daily sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"tot_daily_sales = train_df.groupby(\"date_dt\").sum()[\"item_sales\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Time series\nfig, ax1 = plt.subplots(figsize=(20,6))\nax1.plot(tot_daily_sales.index, tot_daily_sales/1000, linewidth=1)\nax1.set_ylabel(\"Sales(k)\")\nax2 = ax1.twinx()\nax2.plot(tot_daily_sales.index, tot_daily_sales.cumsum()/1000000, linewidth=1, color=\"red\")\nax2.grid()\nax2.set_ylabel(\"Total Sales(M)\")\nplt.xlabel(\"time\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at daily sales in chronological order, you can see strong sales growth in January, June and December. In addition, from December to January, sales are not on a daily basis, but the lower limit is also rising, indicating that there are periods when purchasing is strong on a monthly basis. On the other hand, the lower limit has not risen at the beginning of June, and it is speculated that it may be a short-term campaign-like event.<br>\n\nThe forecasted month is the position where the sales start to rise, and it can be seen that it is more important to capture the seasonality than the context."},{"metadata":{"trusted":true},"cell_type":"code","source":"# item sales distribution\nplt.figure(figsize=(10,6))\nsns.distplot(tot_daily_sales, kde=False, bins=50)\nplt.ylabel(\"Frequency\")\nplt.yscale(\"log\")\nplt.xlabel(\"Total daily sales\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the distribution of daily sales, there are many with less sales. Occasionally, there are days when you'll record big sales like spikes, but that's noise in the forecast. Since this is likely to have an adverse effect on future learning models, we decided to exclude it as an abnormal value from training data."},{"metadata":{},"cell_type":"markdown","source":"Therefore, an abnormal value is detected for the price and the number of sales that have an influence on sales, and the distributions that exclude the abnormal value are compared."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,2,figsize=(20,12))\nplt.subplots_adjust(hspace=0.5)\nsns.boxplot(train_df[\"item_cnt_day\"], ax=ax[0,0])\nax[0,0].set_title(\"item_cnt_day\")\n\nsns.boxplot(train_df[train_df[\"item_cnt_day\"]<800][\"item_cnt_day\"], ax=ax[0,1])\nax[0,1].set_title(\"item_cnt_day Remove outlier\")\n\nsns.boxplot(train_df[\"item_price\"], ax=ax[1,0])\nax[1,0].set_title(\"item_price\")\n\nsns.boxplot(train_df[train_df[\"item_price\"]<70000][\"item_price\"], ax=ax[1,1])\nax[1,1].set_title(\"item_price Remove outlier\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update\ntrain_df = train_df[train_df[\"item_cnt_day\"]<800]\ntrain_df = train_df[train_df[\"item_price\"]<70000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re aggrecate tot_daily_sales\ntot_daily_sales = train_df.groupby(\"date_dt\").sum()[\"item_sales\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decomposition of time series components\nSales were conducted on the total sales, the objects classified by shop, the objects classified by category, and a sample of some items. In order to see the seasonality, we decomposed it by month and year."},{"metadata":{},"cell_type":"markdown","source":"## Total daily salse"},{"metadata":{},"cell_type":"markdown","source":"### Annual trend, Total daily sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"# freq = 365 day\nres = sm.tsa.seasonal_decompose(tot_daily_sales, freq=365)\n\n# Decomposition\ntrend = res.trend\nseaso = res.seasonal\nresid = res.resid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization\nfig, ax = plt.subplots(4,1, figsize=(15,15))\nplt.subplots_adjust(hspace=0.5)\n\nax[0].plot(tot_daily_sales.index, tot_daily_sales, color=\"black\")\nax[0].set_title(\"Time series\")\nax[0].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\")\nax[0].set_xlabel(\"Time\")\n\nax[1].plot(trend.index, trend, color=\"red\")\nax[1].set_title(\"Trend\")\nax[1].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\")\nax[1].set_xlabel(\"Time\")\n\nax[2].plot(seaso.index, seaso, color=\"blue\")\nax[2].set_title(\"Seasonal\")\nax[2].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\")\nax[2].set_xlabel(\"Time\")\n\nax[3].plot(resid.index, resid, color=\"green\")\nax[3].set_title(\"Resid\")\nax[3].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\")\nax[3].set_xlabel(\"Time\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Decomposition was performed to confirm the annual periodicity. Looking at the results, the sales growth from December to January that I saw earlier became clearer. In addition, trends that were not noticed as a whole became clear, and it was found that sales are on a slightly downward trend compared to last year."},{"metadata":{"trusted":true},"cell_type":"code","source":"del trend, seaso, resid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Monthly trend, Total daily sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"# freq = 30 day\nres = sm.tsa.seasonal_decompose(tot_daily_sales, freq=30)\n\n# Decomposition\ntrend = res.trend\nseaso = res.seasonal\nresid = res.resid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization\nfig, ax = plt.subplots(4,1, figsize=(15,15))\nplt.subplots_adjust(hspace=0.5)\n\nax[0].plot(tot_daily_sales.index[-365:], tot_daily_sales[-365:], color=\"black\")\nax[0].set_title(\"Time series\")\nax[0].set_ylabel(\"Daily_sales\\n(Frequeycy:30day)\")\nax[0].set_xlabel(\"Time\")\n\nax[1].plot(trend.index[-365:], trend[-365:], color=\"red\")\nax[1].set_title(\"Trend\")\nax[1].set_ylabel(\"Daily_sales\\n(Frequeycy:30day)\")\nax[1].set_xlabel(\"Time\")\n\nax[2].plot(seaso.index[-365:], seaso[-365:], color=\"blue\")\nax[2].set_title(\"Seasonal\")\nax[2].set_ylabel(\"Daily_sales\\n(Frequeycy:30day)\")\nax[2].set_xlabel(\"Time\")\n\nax[3].plot(resid.index[-365:], resid[-365:], color=\"green\")\nax[3].set_title(\"Resid\")\nax[3].set_ylabel(\"Daily_sales\\n(Frequeycy:30day)\")\nax[3].set_xlabel(\"Time\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, when the periodic component was extracted and confirmed on a monthly basis, the growth on a specific day can also be confirmed. Also, looking at the trend without periodicity, we can see that the forecast month will increase sales by about 30% compared to the previous month."},{"metadata":{"trusted":true},"cell_type":"code","source":"del trend, seaso, resid, tot_daily_sales","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Shop daily sales"},{"metadata":{},"cell_type":"markdown","source":"For the sales analysis, the dataset also provides information on shops and categories. Probably, the sales trend will change from shop to shop, and it can be inferred that it also differs from category to category. Therefore, time series analysis was performed separately for each. The trend and periodic components were decomposed as before so that the features could be confirmed."},{"metadata":{},"cell_type":"markdown","source":"### Annnual trend, shop daily sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pivot by shops\nshops_pivot = pd.pivot_table(train_df, index=\"date_dt\", columns=\"shop_id\", values=\"item_sales\", aggfunc=\"sum\", fill_value=0)\n\n# Shops sample, id=0 & 2 & 3\nsample_0 = shops_pivot[0]\nsample_1 = shops_pivot[2]\nsample_2 = shops_pivot[3]\n\n# freq = 365 day\nres_0 = sm.tsa.seasonal_decompose(sample_0, freq=365)\nres_1 = sm.tsa.seasonal_decompose(sample_1, freq=365)\nres_2 = sm.tsa.seasonal_decompose(sample_2, freq=365)\n\n# Decomposition\ntrend_0 = res_0.trend\nseaso_0 = res_0.seasonal\nresid_0 = res_0.resid\n\ntrend_1 = res_1.trend\nseaso_1 = res_1.seasonal\nresid_1 = res_1.resid\n\ntrend_2 = res_2.trend\nseaso_2 = res_2.seasonal\nresid_2 = res_2.resid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization\nfig, ax = plt.subplots(4,3, figsize=(25,15))\nplt.subplots_adjust(hspace=0.5,)\n\nax[0,0].plot(sample_0.index, sample_0, color=\"black\")\nax[0,0].set_title(\"Shop0_Time series\")\nax[0,0].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[0,0].tick_params(axis='x', labelsize=10)\n\nax[1,0].plot(trend_0.index, trend_0, color=\"red\")\nax[1,0].set_title(\"Shop0_Trend\")\nax[1,0].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[1,0].set_xlabel(\"Time\")\nax[1,0].tick_params(axis='x', labelsize=10)\n\nax[2,0].plot(seaso_0.index, seaso_0, color=\"blue\")\nax[2,0].set_title(\"Shop0_Seasonal\")\nax[2,0].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[2,0].set_xlabel(\"Time\")\nax[2,0].tick_params(axis='x', labelsize=10)\n\nax[3,0].plot(resid_0.index, resid_0, color=\"green\")\nax[3,0].set_title(\"Shop0_Resid\")\nax[3,0].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[3,0].set_xlabel(\"Time\")\nax[3,0].tick_params(axis='x', labelsize=10)\n\nax[0,1].plot(sample_1.index, sample_1, color=\"black\")\nax[0,1].set_title(\"Shop2_Time series\")\nax[0,1].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[0,1].set_xlabel(\"Time\")\nax[0,1].tick_params(axis='x', labelsize=10)\n\nax[1,1].plot(trend_1.index, trend_1, color=\"red\")\nax[1,1].set_title(\"Shop2_Trend\")\nax[1,1].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[1,1].set_xlabel(\"Time\")\nax[1,1].tick_params(axis='x', labelsize=10)\n\nax[2,1].plot(seaso_1.index, seaso_1, color=\"blue\")\nax[2,1].set_title(\"Shop2_Seasonal\")\nax[2,1].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[2,1].set_xlabel(\"Time\")\nax[2,1].tick_params(axis='x', labelsize=10)\n\nax[3,1].plot(resid_1.index, resid_1, color=\"green\")\nax[3,1].set_title(\"Shop2_Resid\")\nax[3,1].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[3,1].set_xlabel(\"Time\")\nax[3,1].tick_params(axis='x', labelsize=10)\n\nax[0,2].plot(sample_2.index, sample_2, color=\"black\")\nax[0,2].set_title(\"Shop3_Time series\")\nax[0,2].set_ylabel(\"Daily_sales(Frequeycy:365day)\", fontsize=15)\nax[0,2].set_xlabel(\"Time\")\nax[0,2].tick_params(axis='x', labelsize=10)\n\nax[1,2].plot(trend_2.index, trend_2, color=\"red\")\nax[1,2].set_title(\"Shop3_Trend\")\nax[1,2].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[1,2].set_xlabel(\"Time\")\nax[1,2].tick_params(axis='x', labelsize=10)\n\nax[2,2].plot(seaso_2.index, seaso_2, color=\"blue\")\nax[2,2].set_title(\"Shop3_Seasonal\")\nax[2,2].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[2,2].set_xlabel(\"Time\")\nax[2,2].tick_params(axis='x', labelsize=10)\n\nax[3,2].plot(resid_2.index, resid_2, color=\"green\")\nax[3,2].set_title(\"Shop3_Resid\")\nax[3,2].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[3,2].set_xlabel(\"Time\")\nax[3,2].tick_params(axis='x', labelsize=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I extracted three shops, 0, 2, and 3. Looking at the results, it can be seen that the tendency is particularly different for No. 0. Initially there was sales, but since then it has disappeared. In the background, it can be guessed that the shop is gone, but the important thing is that each is different. We thought that it was necessary to have information for each shop, create separate feature quantities, and add them to the prediction model."},{"metadata":{"trusted":true},"cell_type":"code","source":"del res_0, res_1, res_2, trend_0, seaso_0, resid_0, trend_1, seaso_1, resid_1, trend_2, seaso_2, resid_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Category daily sales"},{"metadata":{},"cell_type":"markdown","source":"### Annnual trend, Category sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pivot by category\ncategory_pivot = pd.pivot_table(train_df, index=\"date_dt\", columns=\"item_category_id\", values=\"item_sales\", aggfunc=\"sum\", fill_value=0)\n\n# Shops sample, id=0 & 2 & 3\nsample_0 = category_pivot[0]\nsample_1 = category_pivot[2]\nsample_2 = category_pivot[3]\n\n# freq = 365 day\nres_0 = sm.tsa.seasonal_decompose(sample_0, freq=365)\nres_1 = sm.tsa.seasonal_decompose(sample_1, freq=365)\nres_2 = sm.tsa.seasonal_decompose(sample_2, freq=365)\n\n# Decomposition\ntrend_0 = res_0.trend\nseaso_0 = res_0.seasonal\nresid_0 = res_0.resid\n\ntrend_1 = res_1.trend\nseaso_1 = res_1.seasonal\nresid_1 = res_1.resid\n\ntrend_2 = res_2.trend\nseaso_2 = res_2.seasonal\nresid_2 = res_2.resid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization\nfig, ax = plt.subplots(4,3, figsize=(25,15))\nplt.subplots_adjust(hspace=0.5,)\n\nax[0,0].plot(sample_0.index, sample_0, color=\"black\")\nax[0,0].set_title(\"Category0_Time series\")\nax[0,0].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[0,0].set_xlabel(\"Time\")\nax[0,0].tick_params(axis='x', labelsize=10)\n\nax[1,0].plot(trend_0.index, trend_0, color=\"red\")\nax[1,0].set_title(\"Category0_Trend\")\nax[1,0].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[1,0].set_xlabel(\"Time\")\nax[1,0].tick_params(axis='x', labelsize=10)\n\nax[2,0].plot(seaso_0.index, seaso_0, color=\"blue\")\nax[2,0].set_title(\"Category0_Seasonal\")\nax[2,0].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[2,0].set_xlabel(\"Time\")\nax[2,0].tick_params(axis='x', labelsize=10)\n\nax[3,0].plot(resid_0.index, resid_0, color=\"green\")\nax[3,0].set_title(\"Category0_Resid\")\nax[3,0].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[3,0].set_xlabel(\"Time\")\nax[3,0].tick_params(axis='x', labelsize=10)\n\nax[0,1].plot(sample_1.index, sample_1, color=\"black\")\nax[0,1].set_title(\"Category2_Time series\")\nax[0,1].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[0,1].set_xlabel(\"Time\")\nax[0,1].tick_params(axis='x', labelsize=10)\n\nax[1,1].plot(trend_1.index, trend_1, color=\"red\")\nax[1,1].set_title(\"Category2_Trend\")\nax[1,1].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[1,1].set_xlabel(\"Time\")\nax[1,1].tick_params(axis='x', labelsize=10)\n\nax[2,1].plot(seaso_1.index, seaso_1, color=\"blue\")\nax[2,1].set_title(\"Category2_Seasonal\")\nax[2,1].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[2,1].set_xlabel(\"Time\")\nax[2,1].tick_params(axis='x', labelsize=10)\n\nax[3,1].plot(resid_1.index, resid_1, color=\"green\")\nax[3,1].set_title(\"Category2_Resid\")\nax[3,1].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[3,1].set_xlabel(\"Time\")\nax[3,1].tick_params(axis='x', labelsize=10)\n\nax[0,2].plot(sample_2.index, sample_2, color=\"black\")\nax[0,2].set_title(\"Category3_Time series\")\nax[0,2].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[0,2].set_xlabel(\"Time\")\nax[0,2].tick_params(axis='x', labelsize=10)\n\nax[1,2].plot(trend_2.index, trend_2, color=\"red\")\nax[1,2].set_title(\"Category3_Trend\")\nax[1,2].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[1,2].set_xlabel(\"Time\")\nax[1,2].tick_params(axis='x', labelsize=10)\n\nax[2,2].plot(seaso_2.index, seaso_2, color=\"blue\")\nax[2,2].set_title(\"Category3_Seasonal\")\nax[2,2].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[2,2].set_xlabel(\"Time\")\nax[2,2].tick_params(axis='x', labelsize=10)\n\nax[3,2].plot(resid_2.index, resid_2, color=\"green\")\nax[3,2].set_title(\"Category3_Resid\")\nax[3,2].set_ylabel(\"Daily_sales\\n(Frequeycy:365day)\", fontsize=15)\nax[3,2].set_xlabel(\"Time\")\nax[3,2].tick_params(axis='x', labelsize=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that the categories also have different tendencies, similar to shop. As before, some have sales only for the first time and then become 0, while others, such as Category 2 and Category 3, have sales up to the latest, but some are on a downward trend, while others are flat. Therefore, each category also has information and needs to be added to the feature amount."},{"metadata":{"trusted":true},"cell_type":"code","source":"del res_0, res_1, res_2, trend_0, seaso_0, resid_0, trend_1, seaso_1, resid_1, trend_2, seaso_2, resid_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Item time series"},{"metadata":{},"cell_type":"markdown","source":"### Time series analysis, Item sales & Item count & Item price"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pivot by category\nitem_pivot_sales = pd.pivot_table(train_df, index=\"date_dt\", columns=\"item_id\", values=\"item_sales\", aggfunc=\"mean\", fill_value=0)\nitem_pivot_count = pd.pivot_table(train_df, index=\"date_dt\", columns=\"item_id\", values=\"item_price\", aggfunc=\"count\", fill_value=0)\nitem_pivot_price = pd.pivot_table(train_df, index=\"date_dt\", columns=\"item_id\", values=\"item_price\", aggfunc=\"mean\", fill_value=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sample 1000, 2000, 10000\nfig, ax = plt.subplots(3,3, figsize=(25,20))\n\nitem_list = [1000,2000,10000]\npivot_list = [item_pivot_sales, item_pivot_count, item_pivot_price]\n\nfor i in range(len(item_list)):\n    for k in range(len(pivot_list)):\n        ax[i,k].plot(pivot_list[k][item_list[i]].index, pivot_list[k][item_list[i]])\n        ax[i,k].set_xlabel(\"Time\")\n        ax[i,k].tick_params(axis='x', labelsize=10)\n        ax[i,0].set_ylabel(\"Sales\")\n        ax[i,1].set_ylabel(\"Count\")\n        ax[i,2].set_ylabel(\"Price\")\n        ax[i,k].set_title(\"item_id:{}\".format(item_list[i]))\n\ndel item_pivot_sales, item_pivot_count, item_pivot_price","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At the end of the time series analysis, some items were extracted and plotted for each day's sales, number and price. Looking at the results, it can be seen that not only time-series sales fall, but there are times when they rise, which is not due to an increase in the number of sales but to an increase in prices. In order to predict sales, it may be important to track not only the number sold but the price. The number of sales may be reflected in the price."},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"Based on the results of the time series analysis up to this point, we have decided the following policy regarding the information for establishing the price forecast model."},{"metadata":{},"cell_type":"markdown","source":"### Direction of features engineering\n- Important information<br>\nTrend : Information on the downward trend of the current year is required<br>\nSeasonal : reflect the trend information of the previous year<br>\nAlso, reflect time series information for each of shop, category, item, need both sales and price information.<br>\n\nTrain the model with a dataset and features that can reflect this information.\n\n### Features\nEach shops and categorys and items, I made the following features.<br>\n Rag features : 1, 2, 3, 6 lag<br>\n Trend features : Use 3 points from one year ago, periods of 1~3~6 month.<br>\n Seasonal features : Use 4 points from one year ago, periods of 1~2~3~6 month.  "},{"metadata":{},"cell_type":"markdown","source":"## How to create a dataset\n### Test data set\nTime series until last month : predict next month.\n\n### Training data set\nUse the information up to a year ago to make sure that the forecast months are the same.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepairing dataset\nmaster = train_df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define class<br>\nCalculate monthly lag:1,2,3,6 and trend lag:1,3,6 and seasonal lag:1,2,3,6. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define class\nclass feature_eng:\n    def __init__(self, data_ser, data_df, seasonal_len, name):\n        self.date = data_ser\n        self.data = data_df\n        self.seas = seasonal_len\n        self.name = name\n        self.col = data_df.columns\n        \n    def sep_lag_trend_seaso_train(self):\n        max_list = []\n        mean_list = []\n        lag1_list = []\n        lag2_list = []\n        lag3_list = []\n        lag4_6_list = []\n        seas1_list = []\n        seas2_list = []\n        seas3_list = []\n        seas4_6_list = []        \n        \n        for i in self.col:\n            # Calculate trend and seasonal facter\n            res = sm.tsa.seasonal_decompose(self.data[i], freq=self.seas)\n            last = self.data[i].values[-2]\n            max_ = self.data[i].values[-2].max()\n            mean = self.data[i].values[-8:-2].mean()\n            # Append to list\n            max_list.append(self.data[i].values[:-2].max())\n            mean_list.append(self.data[i].values[:-2].mean())\n            lag1_list.append((last - self.data[i].values[-3])*1)\n            lag2_list.append((last - self.data[i].values[-4])*2)\n            lag3_list.append((last - self.data[i].values[-5])*3)\n            lag4_6_list.append((last - (self.data[i].values[-6]+self.data[i].values[-7]+self.data[i].values[-8]))*15)\n            seas1_list.append((res.seasonal.values[-self.seas-1])*1)\n            seas2_list.append((res.seasonal.values[-self.seas-2])*2)\n            seas3_list.append((res.seasonal.values[-self.seas-3])*3)\n            seas4_6_list.append((res.seasonal.values[-self.seas-4]+res.seasonal.values[-self.seas-5]+res.seasonal.values[-self.seas-6])*15)\n        # Output data frame\n        out = pd.DataFrame({\"id\":self.col,\n                            \"{}_max\".format(self.name):max_list,\n                            \"{}_mean\".format(self.name):mean_list,\n                            \"{}_lag1\".format(self.name):lag1_list,\n                            \"{}_lag2\".format(self.name):lag2_list,\n                            \"{}_lag3\".format(self.name):lag3_list,\n                            \"{}_lag4_6\".format(self.name):lag4_6_list,\n                            \"{}_seas1\".format(self.name):seas1_list,\n                            \"{}_seas2\".format(self.name):seas2_list,\n                            \"{}_seas3\".format(self.name):seas3_list,\n                            \"{}_seas4_6\".format(self.name):seas4_6_list\n                           })\n        return out\n    \n    def sep_lag_trend_seaso_test(self):\n        max_list = []\n        mean_list = []\n        lag1_list = []\n        lag2_list = []\n        lag3_list = []\n        lag4_6_list = []\n        seas1_list = []\n        seas2_list = []\n        seas3_list = []\n        seas4_6_list = []   \n        \n        for i in self.col:\n            # Calculate trend and seasonal facter\n            res = sm.tsa.seasonal_decompose(self.data[i], freq=self.seas)\n            last = self.data[i].values[-1]\n            max_ = self.data[i][:-1].values.max()\n            mean = self.data[i].values[-19:-1].mean()\n            # Append to list\n            max_list.append(self.data[i].values[:-1].max())\n            mean_list.append(self.data[i].values[:-1].mean())\n            lag1_list.append((last - self.data[i].values[-2])*1)\n            lag2_list.append((last - self.data[i].values[-3])*2)\n            lag3_list.append((last - self.data[i].values[-4])*3)\n            lag4_6_list.append((last - (self.data[i].values[-5]+self.data[i].values[-6]+self.data[i].values[-7]))*15)\n            seas1_list.append((res.seasonal.values[-self.seas-1])*1)\n            seas2_list.append((res.seasonal.values[-self.seas-2])*2)\n            seas3_list.append((res.seasonal.values[-self.seas-3])*3)\n            seas4_6_list.append((res.seasonal.values[-self.seas-4]+res.seasonal.values[-self.seas-5]+res.seasonal.values[-self.seas-6])*15)\n        # Output data frame\n        out = pd.DataFrame({\"id\":self.col,\n                            \"{}_max\".format(self.name):max_list,\n                            \"{}_mean\".format(self.name):mean_list,\n                            \"{}_lag1\".format(self.name):lag1_list,\n                            \"{}_lag2\".format(self.name):lag2_list,\n                            \"{}_lag3\".format(self.name):lag3_list,\n                            \"{}_lag4_6\".format(self.name):lag4_6_list,\n                            \"{}_seas1\".format(self.name):seas1_list,\n                            \"{}_seas2\".format(self.name):seas2_list,\n                            \"{}_seas3\".format(self.name):seas3_list,\n                            \"{}_seas4_6\".format(self.name):seas4_6_list\n                           })\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define class\nclass feature_eng:\n    def __init__(self, data_ser, data_df, seasonal_len, name):\n        self.date = data_ser\n        self.data = data_df\n        self.seas = seasonal_len\n        self.name = name\n        self.col = data_df.columns\n        \n    def sep_lag_trend_seaso_train(self):\n        max_list = []\n        mean_list = []\n        lag1_list = []\n        lag2_list = []\n        lag3_list = []\n        lag4_6_list = []\n        tre1_list = []\n        tre3_list = []\n        tre4_6_list = []\n        seas1_list = []\n        seas2_list = []\n        seas3_list = []\n        seas4_6_list = []        \n        \n        for i in self.col:\n            # Calculate trend and seasonal facter\n            res = sm.tsa.seasonal_decompose(self.data[i], freq=self.seas)\n            last = self.data[i].values[-2]\n            max_ = self.data[i].values[-2].max()\n            mean = self.data[i].values[-8:-2].mean()\n            # Append to list\n            max_list.append(self.data[i].values[:-2].max())\n            mean_list.append(self.data[i].values[:-2].mean())\n            lag1_list.append((last - self.data[i].values[-3])*1)\n            lag2_list.append((last - self.data[i].values[-4])*2)\n            lag3_list.append((last - self.data[i].values[-5])*3)\n            lag4_6_list.append((last - (self.data[i].values[-6]+self.data[i].values[-7]+self.data[i].values[-8]))*15)\n            tre1_list.append((last - res.trend.values[-int(self.seas*0.5)-1])*1)\n            tre3_list.append((last - res.trend.values[-int(self.seas*0.5)-3])*3)\n            tre4_6_list.append((last - (res.trend.values[-int(self.seas*0.5)-4]+res.trend.values[-int(self.seas*0.5)-5]+res.trend.values[-int(self.seas*0.5)-6]))*15)\n            seas1_list.append((res.seasonal.values[-self.seas-1])*1)\n            seas2_list.append((res.seasonal.values[-self.seas-2])*2)\n            seas3_list.append((res.seasonal.values[-self.seas-3])*3)\n            seas4_6_list.append((res.seasonal.values[-self.seas-4]+res.seasonal.values[-self.seas-5]+res.seasonal.values[-self.seas-6])*15)\n        # Output data frame\n        out = pd.DataFrame({\"id\":self.col,\n                            \"{}_max\".format(self.name):max_list,\n                            \"{}_mean\".format(self.name):mean_list,\n                            \"{}_lag1\".format(self.name):lag1_list,\n                            \"{}_lag2\".format(self.name):lag2_list,\n                            \"{}_lag3\".format(self.name):lag3_list,\n                            \"{}_lag4_6\".format(self.name):lag4_6_list,\n                            \"{}_tre1\".format(self.name):tre1_list,\n                            \"{}_tre3\".format(self.name):tre3_list,\n                            \"{}_tre4_6\".format(self.name):tre4_6_list,\n                            \"{}_seas1\".format(self.name):seas1_list,\n                            \"{}_seas2\".format(self.name):seas2_list,\n                            \"{}_seas3\".format(self.name):seas3_list,\n                            \"{}_seas4_6\".format(self.name):seas4_6_list\n                           })\n        return out\n    \n    def sep_lag_trend_seaso_test(self):\n        max_list = []\n        mean_list = []\n        lag1_list = []\n        lag2_list = []\n        lag3_list = []\n        lag4_6_list = []\n        tre1_list = []\n        tre3_list = []\n        tre4_6_list = []\n        seas1_list = []\n        seas2_list = []\n        seas3_list = []\n        seas4_6_list = []   \n        \n        for i in self.col:\n            # Calculate trend and seasonal facter\n            res = sm.tsa.seasonal_decompose(self.data[i], freq=self.seas)\n            last = self.data[i].values[-1]\n            max_ = self.data[i][:-1].values.max()\n            mean = self.data[i].values[-19:-1].mean()\n            # Append to list\n            max_list.append(self.data[i].values[:-1].max())\n            mean_list.append(self.data[i].values[:-1].mean())\n            lag1_list.append((last - self.data[i].values[-2])*1)\n            lag2_list.append((last - self.data[i].values[-3])*2)\n            lag3_list.append((last - self.data[i].values[-4])*3)\n            lag4_6_list.append((last - (self.data[i].values[-5]+self.data[i].values[-6]+self.data[i].values[-7]))*15)\n            tre1_list.append((last - res.trend.values[-int(self.seas*0.5)-1])*1)\n            tre3_list.append((last - res.trend.values[-int(self.seas*0.5)-3])*3)\n            tre4_6_list.append((last - (res.trend.values[-int(self.seas*0.5)-4]+res.trend.values[-int(self.seas*0.5)-5]+res.trend.values[-int(self.seas*0.5)-6]))*15)\n            seas1_list.append((res.seasonal.values[-self.seas-1])*1)\n            seas2_list.append((res.seasonal.values[-self.seas-2])*2)\n            seas3_list.append((res.seasonal.values[-self.seas-3])*3)\n            seas4_6_list.append((res.seasonal.values[-self.seas-4]+res.seasonal.values[-self.seas-5]+res.seasonal.values[-self.seas-6])*15)\n        # Output data frame\n        out = pd.DataFrame({\"id\":self.col,\n                            \"{}_max\".format(self.name):max_list,\n                            \"{}_mean\".format(self.name):mean_list,\n                            \"{}_lag1\".format(self.name):lag1_list,\n                            \"{}_lag2\".format(self.name):lag2_list,\n                            \"{}_lag3\".format(self.name):lag3_list,\n                            \"{}_lag4_6\".format(self.name):lag4_6_list,\n                            \"{}_tre1\".format(self.name):tre1_list,\n                            \"{}_tre3\".format(self.name):tre3_list,\n                            \"{}_tre4_6\".format(self.name):tre4_6_list,\n                            \"{}_seas1\".format(self.name):seas1_list,\n                            \"{}_seas2\".format(self.name):seas2_list,\n                            \"{}_seas3\".format(self.name):seas3_list,\n                            \"{}_seas4_6\".format(self.name):seas4_6_list\n                           })\n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### shop lag and trend and seasonal"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Each shops count feature\nshop_ts = pd.pivot_table(data=master, index=[\"year\",\"month\"], columns=\"shop_id\", values=\"item_cnt_day\", aggfunc=\"sum\", fill_value=0)\n\ndate_ser = shop_ts.reset_index().drop([\"year\", \"month\"], axis=1).index\ndata_df = shop_ts.reset_index().drop([\"year\", \"month\"], axis=1)\nseasonal_len = 12\nname = \"shop_count\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply class for train data\nshop_time_count = feature_eng(date_ser, data_df, seasonal_len, name)\nshop_time_count_train = shop_time_count.sep_lag_trend_seaso_train()\n# Apply class for test data\nshop_time_count_test = shop_time_count.sep_lag_trend_seaso_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_time_count_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Each shops price feature\nshop_ts = pd.pivot_table(data=master, index=[\"year\",\"month\"], columns=\"shop_id\", values=\"item_price\", aggfunc=\"mean\").fillna(method=\"ffill\").fillna(0)\n\ndate_ser = shop_ts.reset_index().drop([\"year\", \"month\"], axis=1).index\ndata_df = shop_ts.reset_index().drop([\"year\", \"month\"], axis=1)\nseasonal_len = 12\nname = \"shop_price\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply class for train data\nshop_time_price = feature_eng(date_ser, data_df, seasonal_len, name)\nshop_time_price_train = shop_time_price.sep_lag_trend_seaso_train()\n# Apply class for test data\nshop_time_price_test = shop_time_price.sep_lag_trend_seaso_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_time_price_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### category lag and trend and seasonal"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Each Category count feature\ncate_ts = pd.pivot_table(data=master, index=[\"year\",\"month\"], columns=\"item_category_id\", values=\"item_cnt_day\", aggfunc=\"sum\", fill_value=0)\n\ndate_ser = cate_ts.reset_index().drop([\"year\", \"month\"], axis=1).index\ndata_df = cate_ts.reset_index().drop([\"year\", \"month\"], axis=1)\nseasonal_len = 12\nname = \"category_count\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply class for train data\ncate_time_count = feature_eng(date_ser, data_df, seasonal_len, name)\ncate_time_count_train = cate_time_count.sep_lag_trend_seaso_train()\n# Apply class for test data\ncate_time_count_test = cate_time_count.sep_lag_trend_seaso_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cate_time_count_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Each Category price feature\ncate_ts = pd.pivot_table(data=master, index=[\"year\",\"month\"], columns=\"item_category_id\", values=\"item_price\", aggfunc=\"mean\").fillna(method=\"ffill\").fillna(0)\n\ndate_ser = cate_ts.reset_index().drop([\"year\", \"month\"], axis=1).index\ndata_df = cate_ts.reset_index().drop([\"year\", \"month\"], axis=1)\nseasonal_len = 12\nname = \"category_count\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Apply class for train data\ncate_time_price = feature_eng(date_ser, data_df, seasonal_len, name)\ncate_time_price_train = cate_time_price.sep_lag_trend_seaso_train()\n# # Apply class for test data\ncate_time_price_test = cate_time_price.sep_lag_trend_seaso_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cate_time_price_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### item lag and trend and seasonal"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Each Item count feature\nitem_ts = pd.pivot_table(data=master, index=[\"year\",\"month\"], columns=\"item_category_id\", values=\"item_cnt_day\", aggfunc=\"sum\", fill_value=0)\n\ndate_ser = item_ts.reset_index().drop([\"year\", \"month\"], axis=1).index\ndata_df = item_ts.reset_index().drop([\"year\", \"month\"], axis=1)\nseasonal_len = 12\nname = \"item_count\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply class for train data\nitem_time_count = feature_eng(date_ser, data_df, seasonal_len, name)\nitem_time_count_train = item_time_count.sep_lag_trend_seaso_train()\n# Apply class for test data\nitem_time_count_test = item_time_count.sep_lag_trend_seaso_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_time_count_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Each Item Price feature\nitem_ts = pd.pivot_table(data=master, index=[\"year\",\"month\"], columns=\"item_id\", values=\"item_price\", aggfunc=\"mean\").fillna(method=\"ffill\").fillna(0)\n\ndate_ser = item_ts.reset_index().drop([\"year\", \"month\"], axis=1).index\ndata_df = item_ts.reset_index().drop([\"year\", \"month\"], axis=1)\nseasonal_len = 12\nname = \"category_count\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply class for train data\nitem_time_price = feature_eng(date_ser, data_df, seasonal_len, name)\nitem_time_price_train = item_time_price.sep_lag_trend_seaso_train()\n# Apply class for test data\nitem_time_price_test = item_time_price.sep_lag_trend_seaso_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_time_price_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del shop_time_count, shop_time_price, cate_time_count, cate_time_price, item_time_count, item_time_price","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparing for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Type conversion to preserve memory\ndef dtype_change(df):\n    columns = df.dtypes.index\n    dtype = df.dtypes\n    dtype = [str(d) for d in dtype]\n    for i in range(len(columns)):\n        if dtype[i] == 'int64':\n            df[columns[i]] = df[columns[i]].astype(\"int32\")\n        elif dtype[i] == 'float64':\n            df[columns[i]] = df[columns[i]].astype(\"float32\")\n        else:\n            pass\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training data\nshop_time_count_train = dtype_change(shop_time_count_train)\nshop_time_price_train = dtype_change(shop_time_price_train)\ncate_time_count_train = dtype_change(cate_time_count_train)\ncate_time_price_train = dtype_change(cate_time_price_train)\nitem_time_count_train = dtype_change(item_time_count_train)\nitem_time_price_train = dtype_change(item_time_price_train)\n# Test data\nshop_time_count_test = dtype_change(shop_time_count_test)\nshop_time_price_test = dtype_change(shop_time_price_test)\ncate_time_count_test = dtype_change(cate_time_count_test)\ncate_time_price_test = dtype_change(cate_time_price_test)\nitem_time_count_test = dtype_change(item_time_count_test)\nitem_time_price_test = dtype_change(item_time_price_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data merging"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign test data ID to training data\nmaster = pd.merge(master, df_test, left_on=[\"shop_id\", \"item_id\"], right_on=[\"shop_id\", \"item_id\"], how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by shop_id and item_id, and group them in data blocks in the column direction.\npivot = pd.pivot_table(data=master, index=[\"shop_id\", \"item_id\"], columns=\"date_block_num\", values=\"item_cnt_day\", aggfunc=\"sum\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The last value (base point of test data), the last value of last year (target value of training) and the previous one (base point of training data) are extracted.\nlast_test_block = pivot.iloc[:,-1].reset_index()\nlast_train_block = pivot.iloc[:,-2].reset_index()\nlast_train_2ndblock = pivot.iloc[:,-14].reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine test data frame\nBase = pd.merge(df_test, last_train_2ndblock, left_on=[\"shop_id\", \"item_id\"], right_on=[\"shop_id\", \"item_id\"], how=\"left\")\nBase = pd.merge(Base, last_train_block, left_on=[\"shop_id\", \"item_id\"], right_on=[\"shop_id\", \"item_id\"], how=\"left\")\nBase = pd.merge(Base, last_test_block, left_on=[\"shop_id\", \"item_id\"], right_on=[\"shop_id\", \"item_id\"], how=\"left\")\n\nBase = dtype_change(Base)\ndel last_test_block, last_train_block, last_train_2ndblock","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create data with a corresponding relationship between item_id and category_id\ncategory = train_df[[\"item_id\", \"item_category_id\"]].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge Base and category\nBase = pd.merge(Base, category, left_on=\"item_id\", right_on=\"item_id\", how=\"left\")\n\ndel category","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data check\n# Null data\nBase.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data shape\nBase.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the aggregated data, it can be seen that most of the total data is Null data. The ratio is 90% or more.\nHere, it is necessary to deal with this Null value. For variables, you can enter information according to shop_id, item, and category. On the other hand, although it is the target value and the value that is the base point, it is information that is not in the sales history and it is assumed that it is a combination that does not have sales in the first place, so we decided to fill it with 0 this time."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trainin data\n# shop data\nTrain = pd.merge(Base, shop_time_count_train, left_on=\"shop_id\", right_on=\"id\", how=\"left\")\nTrain = pd.merge(Train, shop_time_price_train, left_on=\"shop_id\", right_on=\"id\", how=\"left\")\n\n# category data\nTrain = pd.merge(Train, cate_time_count_train, left_on=\"item_category_id\", right_on=\"id\", how=\"left\")\nTrain = pd.merge(Train, cate_time_price_train, left_on=\"item_category_id\", right_on=\"id\", how=\"left\")\n\n# item data\nTrain = pd.merge(Train, item_time_count_train, left_on=\"item_id\", right_on=\"id\", how=\"left\")\nTrain = pd.merge(Train, item_time_price_train, left_on=\"item_id\", right_on=\"id\", how=\"left\")\n\nTrain.fillna(0, inplace=True)\n\nTrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide learning data into explanatory variables and target values\n# Train data\nX_Train = Train.drop([\"ID\", \"item_id\", 33, 20, \"id_x\", \"id_y\", \"shop_id\", \"item_category_id\"], axis=1)\n\ny_Train = Train[33].clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Test data\n# shop data\nTest = pd.merge(Base, shop_time_count_test, left_on=\"shop_id\", right_on=\"id\", how=\"left\")\nTest = pd.merge(Test, shop_time_price_test, left_on=\"shop_id\", right_on=\"id\", how=\"left\")\n# category data\nTest = pd.merge(Test, cate_time_count_test, left_on=\"item_category_id\", right_on=\"id\", how=\"left\")\nTest = pd.merge(Test, cate_time_price_test, left_on=\"item_category_id\", right_on=\"id\", how=\"left\")\n# item data\nTest = pd.merge(Test, item_time_count_test, left_on=\"item_id\", right_on=\"id\", how=\"left\")\nTest = pd.merge(Test, item_time_price_test, left_on=\"item_id\", right_on=\"id\", how=\"left\")\n\nTest.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide learning data into explanatory variables\n# Test data\nX_Test = Test.drop([\"ID\", \"item_id\", 32, 20, \"id_x\", \"id_y\", \"shop_id\", \"item_category_id\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X_Train shape:{}\".format(X_Train.shape))\nprint(\"y_Train shape:{}\".format(y_Train.shape))\nprint(\"X_Test shape:{}\".format(X_Test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Trainin and validation"},{"metadata":{},"cell_type":"markdown","source":"The training data is divided into model training and evaluation data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train test data split\nX_train, X_val, y_train, y_val = train_test_split(X_Train, y_Train, test_size=0.2, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The models used for machine learning this time are LGBM for nonlinear prediction, and Ridge for linear prediction.<br>\nEach result was confirmed by residuals and plots, and the results that came out were ensembled to be the final predicted values."},{"metadata":{},"cell_type":"markdown","source":"### Light GBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create instance\nlgbm = lgb.LGBMRegressor()\n\nparams = {'learning_rate': [0.14, 0.18, 0.20], 'max_depth': [8, 10, 12]}\n\n# Fitting\ncv_lg = GridSearchCV(lgbm, params, cv = 10, n_jobs =1)\ncv_lg.fit(X_train, y_train)\n\nprint(\"Best params:{}\".format(cv_lg.best_params_))\n\nbest_lg = cv_lg.best_estimator_\n\n# prediction\ny_train_pred_lg = best_lg.predict(X_train)\ny_val_pred_lg = best_lg.predict(X_val)\n\n# prediction\ny_train_pred_lg = cv_lg.predict(X_train)\ny_val_pred_lg = cv_lg.predict(X_val)\n\nprint(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_lg)))\nprint(\"MSE val;{}\".format(mean_squared_error(y_val, y_val_pred_lg)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_lg)))\nprint(\"R2 score val:{}\".format(r2_score(y_val, y_val_pred_lg)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training and score\nridge = Ridge()\nparams = {'alpha': [10000, 3000, 2000, 1000, 100, 10, 1]}\n\n# Fitting\ncv_r = GridSearchCV(ridge, params, cv = 10, n_jobs =1)\ncv_r.fit(X_train, y_train)\n\nprint(\"Best params:{}\".format(cv_r.best_params_))\n\nbest_r = cv_r.best_estimator_\n\n# prediction\ny_train_pred_r = best_r.predict(X_train)\ny_val_pred_r = best_r.predict(X_val)\n\nprint(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_r)))\nprint(\"MSE val;{}\".format(mean_squared_error(y_val, y_val_pred_r)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_r)))\nprint(\"R2 score val:{}\".format(r2_score(y_val, y_val_pred_r)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Val data check"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nplt.scatter(y_val_pred_lg, y_val_pred_lg - y_val, c=\"red\", marker='o', alpha=0.5, label=\"LGBM\")\nplt.scatter(y_val_pred_r, y_val_pred_r - y_val, c=\"green\", marker='o', alpha=0.5, label=\"Rigde\")\nplt.xlabel('Predicted values')\nplt.ylabel('Residuals')\nplt.legend(loc = 'upper left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the results, you can see that it is quite difficult to predict. The residuals are not uniform and are biased towards the positive side."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nplt.scatter(y_val.clip(0,20), y_val_pred_lg.clip(0,20), c=\"red\", marker='o', alpha=0.5, label=\"LGBM\")\nplt.scatter(y_val.clip(0,20), y_val_pred_r.clip(0,20), c=\"green\", marker='o', alpha=0.2, label=\"Rigde\")\nplt.xlabel('y_val data')\nplt.ylabel('y_predcition')\nplt.xlim([-2,22])\nplt.ylim([-2,22])\nplt.legend(loc = 'upper left')\n\nprint(\"MSE val LGBM:{}\".format(mean_squared_error(y_val.clip(0,20), y_val_pred_lg.clip(0,20))))\nprint(\"MSE val Ridge:{}\".format(mean_squared_error(y_val.clip(0,20), y_val_pred_r.clip(0,20))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the clipped results, we can see that LGBM has lower prediction MSE. In the result of not clipping, it was the opposite, but it was found that LGBM is the best in terms of submission rules."},{"metadata":{},"cell_type":"markdown","source":"The final predicted results were "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Test prediction\ny_test_pred = best_lg.predict(X_Test).clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictin visualization\nplt.figure(figsize=(10,6))\nsns.distplot(y_test_pred, kde=False, bins=20)\nplt.xlabel(\"prediction\")\nplt.xlim([-0.5,20.5])\nplt.xticks(range(21))\nplt.ylabel(\"Frequency\")\nplt.yscale(\"log\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit dataframe\nsubmit = sample.copy()\nsubmit[\"item_cnt_month\"] = y_test_pred\n\nsubmit.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}