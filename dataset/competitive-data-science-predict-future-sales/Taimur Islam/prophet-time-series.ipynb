{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Basic packages\nimport numpy as np \nimport pandas as pd\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 20)\nimport pandas_profiling as pp\nfrom pandas import Series as Series\nimport datatable as dt\nimport random as rd \nimport datetime\nimport gc\nimport os\nfrom tqdm import tqdm\nimport time\nimport sys\n\n\n# Visualization\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport plotly as py\nimport plotly.express as px\nimport plotly.graph_objects as go\npy.offline.init_notebook_mode(connected = True)\n\n# For time series analysis\nfrom datetime import datetime\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic, pacf\n! pip install pmdarima\nfrom pmdarima import auto_arima\nfrom pandas.plotting import autocorrelation_plot\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\nfrom fbprophet import Prophet\n\n# Model Development Related\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error, median_absolute_error, mean_squared_error, mean_squared_log_error\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nimport pickle\n\n# Ignore warning \nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Overview About Competition :**\n1. This time-series dataset consisting of daily sales data (largest Russian software firms - 1C Company)\n2. We have to predict total sales for every product and store in the next month"},{"metadata":{},"cell_type":"markdown","source":"#### Import Python Packages\n"},{"metadata":{},"cell_type":"markdown","source":"#### Import All Files"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:28:47.457725Z","start_time":"2021-02-10T07:28:37.167Z"},"code_folding":[],"trusted":true},"cell_type":"code","source":"# List Of Files\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2021-02-10T07:56:55.327522Z","end_time":"2021-02-10T07:56:59.038444Z"},"code_folding":[],"trusted":true},"cell_type":"code","source":"# Main Data File Import\ntrain_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\ntest_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\nitems_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nitems_cat_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\nshops_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\n# Convert to date time data type\ntrain_df['date'] = pd.to_datetime(train_df['date'], dayfirst = True)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:20:08.506433Z","start_time":"2021-02-10T07:20:08.495428Z"},"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:01.029419Z","start_time":"2021-02-10T07:00:00.702856Z"},"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Basic Exploratory Data Analysis (EDA)"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:01.434027Z","start_time":"2021-02-10T07:00:01.031415Z"},"trusted":true},"cell_type":"code","source":"print(\"Total Number Of row : \", train_df.shape[0])\nprint(\"Total Unique Days : \", train_df['date'].nunique())\nprint(\"Starting Days : \", train_df['date'].min())\nprint(\"Ending Days : \", train_df['date'].max())\nprint('---------------------------')\nmissing_values_count = train_df.isnull().sum()[train_df.isna().sum() > 0].sort_values(ascending=False)\ntotal_cells = np.product(train_df.shape)\ntotal_missing = missing_values_count.sum()\nprint('NAN Valued Columns: %d' % train_df.isna().any().sum())\nprint (\"Missing data = \",str(round((total_missing/total_cells) * 100, 2))+'%')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-27T11:40:21.021098Z","start_time":"2021-01-27T11:40:21.015113Z"}},"cell_type":"markdown","source":"*Quick Observations:*\n> 1. Total 1034 unique days means almost 3 (2.8) years data is given\n> 2. Data entry starts from January(2013) to October(2015)"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-01T06:21:46.310019Z","start_time":"2021-02-01T06:21:46.296274Z"},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:03.012428Z","start_time":"2021-02-10T07:00:01.436768Z"},"trusted":true},"cell_type":"code","source":"print(\"Total Unique Date Block : \", train_df['date_block_num'].nunique())\nprint(\"Total Unique Shop : \", shops_df['shop_id'].nunique())\nprint(\"Total Unique Item Category : \", items_df['item_category_id'].nunique())\nprint(\"Total Unique Item : \", train_df['item_id'].nunique())\nprint('Total Number of duplicates:', len(train_df[train_df.duplicated()]))\n# Only shops that exist in test set.\nprint('Total Number of Common Unique Shops in Training&Test Set:',train_df[train_df['shop_id'].isin(test_df['shop_id'])]['shop_id'].nunique())\n# Only items that exist in test set.\nprint('Total Number of Common Unique Items in Training&Test Set:',train_df[train_df['item_id'].isin(test_df['item_id'])]['item_id'].nunique())\nprint(\"Number Of Items with Price 0 : \", train_df[train_df['item_price']<=0]['item_id'].count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Quick Observations:*\n> 1. Total 1034 daye is also divided into 34 date blocks (2 year 10 month = 34 month )\n2. Only 6 duplicates value : <br>\na. We can remove it <br>\nb. But I think only 6 duplicate values will not make any difference"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:12.901007Z","start_time":"2021-02-10T07:00:03.015421Z"},"trusted":true},"cell_type":"code","source":"monthly_sales = train_df.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\"date\",\"item_price\",\"item_cnt_day\"]\\\n                   .agg({\"date\":[\"min\",'max'], \"item_price\":\"mean\", \"item_cnt_day\":\"sum\"})","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:13.261568Z","start_time":"2021-02-10T07:00:12.90293Z"},"trusted":true},"cell_type":"code","source":"monthly_sales.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:13.886317Z","start_time":"2021-02-10T07:00:13.263562Z"},"trusted":true},"cell_type":"code","source":"# Understand item category and items per category\nitems_per_category = items_df.groupby(['item_category_id']).count()\nitems_per_category = items_per_category.sort_values(by='item_id', ascending=False)\nprint(\"Understand distribution Of product in each category :\\n\", items_per_category['item_id'].describe())\nitems_per_category = items_per_category.iloc[0:15].reset_index()\n\n# Visualize top item category(Category with grater number of items)\nplt.figure(figsize=(14,6))\nax= sns.barplot(items_per_category.item_category_id, items_per_category.item_id, alpha=0.8)\nplt.title(\"Items per Category\")\nplt.ylabel('Number of items', fontsize=10)\nplt.xlabel('Item Category', fontsize=10)\nplt.show()\nprint(\"Category with more than 800 items : \", items_per_category[items_per_category['item_id']>800]['item_id'].count())","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-27T17:26:01.577753Z","start_time":"2021-01-27T17:26:01.568777Z"}},"cell_type":"markdown","source":"*Quick Observations:*\n> 1. Some Category has very large number of items (max - 5k+)\n2. 50% category has less than 50 items \n3. 75% category has less than 300 items\n4. On average most of the category has less than 263 items\n5. Only 4 item category has very large number of items (more than 800)"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:13.91628Z","start_time":"2021-02-10T07:00:13.888316Z"},"trusted":true},"cell_type":"code","source":"(train_df[train_df['item_cnt_day']<0]['item_cnt_day'].count()/train_df.shape[0])*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Quick Observations:*\n> 1. item_cnt_day : number of products sold. You are predicting a monthly amount of this measure\n2. We can see that there are almost .25% negative value. Is is possible !!\n3. Maybe by mistake it happend, so convert those neg values to positive "},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:14.561181Z","start_time":"2021-02-10T07:00:13.918232Z"},"trusted":true},"cell_type":"code","source":"# Convert all neg value of \"item_cnt_day\" to positive\ntrain_df['item_cnt_day'] = train_df['item_cnt_day'].apply(abs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Time Series Analysis"},{"metadata":{},"cell_type":"markdown","source":"*Quick Observations:*\n> First of all, we have to understand the time series data <br>\n> Then we will apply the following models:\n1. AR (Autoregressive Model)\n2. MA (Moving Average Model)\n3. ARIMA (Autoregressive Integrated Moving Average)\n4. SARIMA (Seasonal Autoregressive Integrated Moving Average)"},{"metadata":{},"cell_type":"markdown","source":"##### Item Count and Price "},{"metadata":{"ExecuteTime":{"start_time":"2021-02-10T07:57:07.912294Z","end_time":"2021-02-10T07:57:08.701781Z"},"trusted":true},"cell_type":"code","source":"# Item Sales Per Month Calculation\nitem_sales_per_month = train_df.groupby(['date_block_num'])['item_cnt_day'].sum()\nplt.figure(figsize=(16, 4))\nplt.title('Total item sales of the company')\nplt.xlabel('Month Number')\nplt.ylabel('Item Sales')\nplt.plot(item_sales_per_month, color = 'blue', linewidth = 2, markersize = 12);\nplt.axvspan(10,12,linestyle=':',linewidth=2,label='First Year Peak',color='darkorange',alpha=.2)\nplt.axvspan(22,24,linestyle=':',linewidth=2,label='Second Year Peak',color='green',alpha=.2)\nplt.legend(fontsize=12, ncol=1, loc='upper right');","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:15.113902Z","start_time":"2021-02-10T07:00:14.800573Z"},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nplt.plot(item_sales_per_month.rolling(window=15,center=False).mean(),label='Rolling: Mean Item Sales');\nplt.plot(item_sales_per_month.rolling(window=15,center=False).std(),label='Rolling: Standard Deviation');\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:16.62911Z","start_time":"2021-02-10T07:00:15.115862Z"},"trusted":true},"cell_type":"code","source":"# Year & Month wise Item Count\ntrain_df['year'] = train_df['date'].dt.year\ntrain_df['month'] = train_df['date'].dt.month\n\nplt.figure(figsize=(16,10))\nax1 = plt.subplot(211)\nax2 = plt.subplot(212)\n\ngrouped_item_count = pd.DataFrame(train_df.groupby(['year','month'])['item_cnt_day'].sum().reset_index())\nsns.pointplot(x='month', y='item_cnt_day', hue='year', data=grouped_item_count, ax = ax1)\n\n#Price\ngrouped_item_price = pd.DataFrame(train_df.groupby(['year','month'])['item_price'].mean().reset_index())\nsns.pointplot(x='month', y='item_price', hue='year', data=grouped_item_price, ax = ax2)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:17.356121Z","start_time":"2021-02-10T07:00:16.631105Z"},"trusted":true},"cell_type":"code","source":"res = sm.tsa.seasonal_decompose(item_sales_per_month.values,freq=12,model=\"multiplicative\")\nres.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:17.832843Z","start_time":"2021-02-10T07:00:17.358085Z"},"trusted":true},"cell_type":"code","source":"res = sm.tsa.seasonal_decompose(item_sales_per_month.values,freq=12,model=\"additive\")\nres.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-29T18:12:29.112178Z","start_time":"2021-01-29T18:12:29.103201Z"}},"cell_type":"markdown","source":"*Quick Observations:*\n> 1. Overall selling items following decreasing treand (after 5 months).\n> 2. Clearly showing \"seasonality\" (peak sales around a time of year, mostly in Q4 of a year)\n> 3. Last two months of the year having more sales.\n> 4. 2015, expecting more sales."},{"metadata":{},"cell_type":"markdown","source":"##### Outlayers"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:18.13023Z","start_time":"2021-02-10T07:00:17.834818Z"},"trusted":true},"cell_type":"code","source":"# There are some outdated items : didn't sell those items in last 6 months \nout_dated_items = items_df[items_df['item_id'].isin(train_df[train_df['date_block_num'] > 27]['item_id'])== False]['item_id']\nprint(\"Outdated items in last 6 months (training set): \", out_dated_items.nunique())\nprint(\"Outdated items in last 6 months % (training set): \", (out_dated_items.nunique()/train_df['item_id'].nunique())*100)\n\nprint(\"Outdated items in last 6 months (test set): \", test_df[test_df['item_id'].isin(out_dated_items)==True]['item_id'].nunique())\nprint(\"Outdated items in last 6 months %(test set): \", (test_df[test_df['item_id'].isin(out_dated_items)==True]['item_id'].nunique()/test_df['item_id'].nunique())*100)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-29T20:01:27.093945Z","start_time":"2021-01-29T20:01:27.084004Z"}},"cell_type":"markdown","source":"*Quick Observations:*\n> 1. Almost 60% items didn't sell in last 6 months\n> 2. But in test set it's only 10%"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:59.836722Z","start_time":"2021-02-10T07:00:18.132024Z"},"trusted":true},"cell_type":"code","source":"# Understand item sales count outlayers\nplt.figure(figsize = (16,4))\nplt.xlim(-100, 3000)\nsns.violinplot(x = train_df.item_cnt_day)\n# Understand item sales price outlayers\nplt.figure(figsize = (16,4))\nplt.xlim(train_df.item_price.min(), train_df.item_price.max()*1.1)\nsns.violinplot(x=train_df.item_price)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:00:59.867639Z","start_time":"2021-02-10T07:00:59.838725Z"},"trusted":true},"cell_type":"code","source":"print('Sale item outliers:',train_df['item_id'][train_df['item_cnt_day']>500].unique())\nprint('Item price outliers:',train_df['item_id'][train_df['item_price']>50000].unique())","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:01:06.255058Z","start_time":"2021-02-10T07:00:59.869634Z"},"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,4))\nsns.jointplot(x=\"item_cnt_day\", y=\"item_price\", data=train_df, height=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Quick Observations:*\n> 1. Removing outlayers may increase model performance.\n> 2. Items sales and prices have some outlayers.\n> 3.  remove items with price > 100000 and item sales > 1000"},{"metadata":{},"cell_type":"markdown","source":"##### Stationarity Checking"},{"metadata":{},"cell_type":"markdown","source":"> Time series is stationary:- easier to model. <br>\n*Stationarity Checking Methods:*\n\n>1. ADF( Augmented Dicky Fuller Test)\n>2. KPSS\n>3. PP (Phillips-Perron test)\n"},{"metadata":{},"cell_type":"markdown","source":"###### ADF( Augmented Dicky Fuller Test)"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:01:06.680353Z","start_time":"2021-02-10T07:01:06.257028Z"},"trusted":true},"cell_type":"code","source":"# Stationarity tests\ndef test_stationarity(timeseries):\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    print('-'*50)\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n\ntest_stationarity(item_sales_per_month)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Quick Observations:*\n> 1. test-statistics : More  negative means more likely to be stationary\n> 2. p-value (small): reject null-hypothesis, reject non-stationary\n> 3. Here p vaule is high (not bellow 5% )\n> 4. Non-Stationary Series : not suitable for time series model \n> 5. Convert Non Stationary : Stationary\n> 6. To make it Stationary : remove trends and seasonality (p value bellow 5%)"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:01:06.69636Z","start_time":"2021-02-10T07:01:06.685376Z"},"trusted":true},"cell_type":"code","source":"# Create a differenced series\n# To make No-Stationary series to Stationary series we have to calculate difference\n# difference = Y(t)-Y(t-1)\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return Series(diff)\n\n# invert differenced forecast\ndef inverse_difference(last_ob, value):\n    return value + last_ob","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:01:07.452414Z","start_time":"2021-02-10T07:01:06.699319Z"},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,16))\nplt.subplot(311)\nplt.title('Original (p-vale : '+str(round(adfuller(item_sales_per_month, autolag='AIC')[1], 4))+')')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(item_sales_per_month)\n\n# Seasonality is 1 months interval\nnew_ts=difference(item_sales_per_month)\nplt.subplot(312)\nplt.title('After De-trend (p-vale : '+str(round(adfuller(new_ts, autolag='AIC')[1], 4))+')')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(new_ts)\n\n# Assuming the seasonality is 12 months long\nnew_ts=difference(item_sales_per_month,12) \nplt.subplot(313)\nplt.title('After De-seasonalization (p-vale : '+str(round(adfuller(new_ts, autolag='AIC')[1], 4))+')')\nplt.xlabel('Time')\nplt.ylabel('Sales')    \nplt.plot(new_ts)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:01:07.48229Z","start_time":"2021-02-10T07:01:07.454365Z"},"trusted":true},"cell_type":"code","source":"# now testing the stationarity again after de-seasonality\ntest_stationarity(new_ts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * Data Series bocome stationary "},{"metadata":{},"cell_type":"markdown","source":"##### Moving Average Model(MA) For Smoothing"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:01:07.639464Z","start_time":"2021-02-10T07:01:07.48628Z"},"trusted":true},"cell_type":"code","source":"def mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:01:08.940945Z","start_time":"2021-02-10T07:01:07.641433Z"},"trusted":true},"cell_type":"code","source":"def plot_moving_average(series, window, plot_intervals=False, scale=1.96):\n\n    rolling_mean = series.rolling(window=window).mean()\n    \n    plt.figure(figsize=(16,4))\n    plt.title('Moving average\\n window size = {}'.format(window))\n    plt.plot(rolling_mean, 'g', label='Rolling mean trend')\n    \n    #Plot confidence intervals for smoothed values\n    if plot_intervals:\n        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n        deviation = np.std(series[window:] - rolling_mean[window:])\n        lower_bound = rolling_mean - (mae + scale * deviation)\n        upper_bound = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bound, 'r--', label='Upper bound / Lower bound')\n        plt.plot(lower_bound, 'r--')\n            \n    plt.plot(series[window:], label='Actual values')\n    plt.legend(loc='best')\n    plt.grid(True)\n    \n#Smooth by the previous 1 month \nplot_moving_average(item_sales_per_month, 1)\n\n#Smooth by the previous 1 quarter\nplot_moving_average(item_sales_per_month, 3)\n\n#Smooth by previous 6 month\nplot_moving_average(item_sales_per_month, 6, plot_intervals=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Exponential Smoothing"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:01:09.176003Z","start_time":"2021-02-10T07:01:08.942939Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"def exponential_smoothing(series, alpha):\n\n    result = [series[0]] # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result\n  \ndef plot_exponential_smoothing(series, alphas):\n \n    plt.figure(figsize=(17, 8))\n    for alpha in alphas:\n        plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n    plt.plot(series.values, \"c\", label = \"Actual\")\n    plt.legend(loc=\"best\")\n    plt.axis('tight')\n    plt.title(\"Exponential Smoothing\")\n    plt.grid(True);\n\nplot_exponential_smoothing(item_sales_per_month, [0.05, 0.3])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Double exponential smoothing"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:01:09.444821Z","start_time":"2021-02-10T07:01:09.177965Z"},"trusted":true},"cell_type":"code","source":"def double_exponential_smoothing(series, alpha, beta):\n\n    result = [series[0]]\n    for n in range(1, len(series)+1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series): # forecasting\n            value = result[-1]\n        else:\n            value = series[n]\n        last_level, level = level, alpha * value + (1 - alpha) * (level + trend)\n        trend = beta * (level - last_level) + (1 - beta) * trend\n        result.append(level + trend)\n    return result\n\ndef plot_double_exponential_smoothing(series, alphas, betas):\n     \n    plt.figure(figsize=(17, 8))\n    for alpha in alphas:\n        for beta in betas:\n            plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n    plt.plot(series.values, label = \"Actual\")\n    plt.legend(loc=\"best\")\n    plt.axis('tight')\n    plt.title(\"Double Exponential Smoothing\")\n    plt.grid(True)\n    \nplot_double_exponential_smoothing(item_sales_per_month, alphas=[0.9, 0.02], betas=[0.9, 0.02])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Dickey-Fuller Test : Stationarity "},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:01:09.46074Z","start_time":"2021-02-10T07:01:09.446777Z"},"trusted":true},"cell_type":"code","source":"def tsplot(y, lags=None, figsize=(10, 8), style='bmh',title=''):\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (3, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        qq_ax = plt.subplot2grid(layout, (2, 0))\n        pp_ax = plt.subplot2grid(layout, (2, 1))\n        \n        y.plot(ax=ts_ax)\n        ts_ax.set_title(title)\n        smt.graphics.plot_acf (y, lags=lags, ax=acf_ax, alpha=0.5)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n        sm.qqplot(y, line='s', ax=qq_ax)\n        qq_ax.set_title('QQ Plot')        \n        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)\n\n        plt.tight_layout()\n     ","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:01:11.274599Z","start_time":"2021-02-10T07:01:09.462735Z"},"trusted":true},"cell_type":"code","source":"tsplot(item_sales_per_month, lags=3)\n\n# Take the first difference to remove to make the process stationary\ndata_diff = item_sales_per_month - item_sales_per_month.shift(1)\n\ntsplot(data_diff[1:], lags=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Data Cleaning"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:01:11.290513Z","start_time":"2021-02-10T07:01:11.27655Z"},"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### ARIMA Model"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-10T07:01:12.799485Z","start_time":"2021-02-10T07:01:11.292508Z"},"trusted":true},"cell_type":"code","source":"# Finding best ARIMA parameter p,d,q\nstepwise_fit = auto_arima(item_sales_per_month, trace=True, suppress_warnings=True)\nstepwise_fit.summary()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2021-02-10T07:57:19.998715Z","end_time":"2021-02-10T07:57:20.04459Z"},"trusted":true},"cell_type":"code","source":"# Add Month index \nitem_sales_per_month_with_date = item_sales_per_month.copy()\nitem_sales_per_month_with_date.index = pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nitem_sales_per_month_with_date = item_sales_per_month_with_date.reset_index()\nitem_sales_per_month_with_date.columns=['ds','y']\nitem_sales_per_month_with_date.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2021-02-10T07:20:16.652Z"},"trusted":true},"cell_type":"code","source":"# Prophet Model Development\nmodel = Prophet(yearly_seasonality=True) \n#fit the model with dataframe\nmodel.fit(item_sales_per_month_with_date, algorithm='Newton') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}