{"cells":[{"metadata":{"_uuid":"d3caf5c4222eeb3c4c5398885edfa27eccf6f39c"},"cell_type":"markdown","source":"(Work in progress...)\n\nThe first part of this notebook includes exploratory analysis.\n\nThe second part will feature future prediction."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # graphs\nimport plotly.offline \nimport plotly.graph_objs as go\nplotly.offline.init_notebook_mode(connected=True)\niplot = plotly.offline.iplot\n\nfrom IPython.display import display","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"129bcea2135167a14feb7055345e2f78d3db33f2"},"cell_type":"markdown","source":"## Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"cats = pd.read_csv('../input/item_categories.csv')\nitems = pd.read_csv('../input/items.csv')\nshops = pd.read_csv('../input/shops.csv')\ntrain = pd.read_csv('../input/sales_train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nprint(f'''Shapes:\nItem categories: {cats.shape}\nItems: {items.shape}\nShops: {shops.shape}\nTrain set: {train.shape}\nTest set: {test.shape}''')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4eb0d2d274fd8ae0b4bb0135c713a5ca32093db2"},"cell_type":"markdown","source":"### Item categories\n\nThere are 84 categories. They are all in Russian!"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c5a7189165932c66c6cad327fb76c8464d2b51f0"},"cell_type":"code","source":"print(cats.info())\ncats.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2109399c0266be8ea762e24315ddb5f13cf03d79"},"cell_type":"markdown","source":"### Items\n\nThere are 22170 items"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"48975be44aa39ef5e0478fa986b26dfec62c3b4c"},"cell_type":"code","source":"print(items.info())\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d7c1c71c92b4d2095490c351e658c56af10f999"},"cell_type":"markdown","source":"### Shops\n\nThere are 60 shops"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"30d2f380c633aff9f4b874d55668fb82778fa310"},"cell_type":"code","source":"print(shops.info())\nshops.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e8733854a684cf9846b583695d64b200f8e6a84"},"cell_type":"markdown","source":"### Train set\n6 features: date, date_block_num, shop_id, item_id, item_price, item_cnt_day\n\n2935849 records"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d9da7b446923d6bd622ce58cfcd1dd7e51b7a929"},"cell_type":"code","source":"print(train.info())\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a536ae34c830b8ae79ceb3e9b10d64390bc6eb2"},"cell_type":"markdown","source":"### Test set\n214200 entries"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"3dfd32e21389a87ccb03087a8295d0166123a3f0"},"cell_type":"code","source":"print(test.info())\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49024af65204131ee8f877422de5e77ed4b516bf"},"cell_type":"markdown","source":"## Target variable: item_cnt_day\n\nNumber of products sold. We are predicting a monthly amount of this measure.\n\nThe majority of them are just 1 item.\nSome of them are negative (returned items).\nSome of them are over 20 items.\n\nThe maximum is 2169 items, on 28/10/2105, shop_id 12, item_id 11373 (Boxberry)\n"},{"metadata":{"trusted":true,"_uuid":"0064e888cc339eb2709e254343f70ffa64a61952"},"cell_type":"code","source":"# train['item_cnt_day'].value_counts().sort_index()\npd.cut(train['item_cnt_day'], [-np.inf] + list(range(0, 21)) + [np.inf]).value_counts()\n# items[items.item_id == train.sort_values('item_cnt_day', ascending=False).head()['item_id'].iloc[0]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d8f7b7c5ce58ec8a542c4faad1fce7adbe117a5"},"cell_type":"markdown","source":"## Predictor variables\n\nWe are creating a new feature which is item_price × item_cnt_day, and call it item_sale\n\nWe will also merge the item_caregory_id from items into the train and test set"},{"metadata":{"trusted":true,"_uuid":"88e2397cdae7b9f908ad75e36f77f068f7503f8a"},"cell_type":"code","source":"train['item_sale'] = train['item_price'] * train['item_cnt_day'] # a new feature to calculate total sale for the item\ntrain = pd.merge(train, items[['item_id', 'item_category_id']], how='left', on='item_id') # merge train & items to get item_category_id\ntest = pd.merge(test, items[['item_id', 'item_category_id']], how='left', on='item_id')\ndisplay(train.head(), test.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9133df7ba47da6d797a5b33997d404f20ad7536d"},"cell_type":"markdown","source":"We check if each item_id should ONLY belong to 1 item_category_id"},{"metadata":{"trusted":true,"_uuid":"75a0fd987fe9715abbc9fe0cec9ad29055c8f7ba"},"cell_type":"code","source":"train.groupby('item_id')['item_category_id'].agg(lambda x: x.nunique()).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48c03985e50322e6a92967aaf3fa7e1228c5e231"},"cell_type":"markdown","source":"### date\nConvert from string to datetime type and add year and month column"},{"metadata":{"trusted":true,"_uuid":"ae8c529a67adb4c3992d5fd1ab510abeac8578f8"},"cell_type":"code","source":"train['date'] = pd.to_datetime(train['date'], format='%d.%m.%Y')\n\n# add year and month as they can be useful for future prediction\ntrain['year'] = train['date'].dt.year\ntrain['month'] = train['date'].dt.month\n\n# add date_block_num, year, month for test too\ntest['date_block_num'] = 34 # continue from 33 from train\ntest['year'] = 2015\ntest['month'] = 11","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c578ea8ff9c1e1fe30c430138a97d2f0250dc60b"},"cell_type":"markdown","source":"Now we have date (day, month, year, week, etc) as well as item_id, item_category_id, shop_id. We want to group them and look at the item_cnt_day and item_sale\n\n\nThere is a clear weekly cycle, with more sales on Thu, Fri and Sat\n\nIf we group them by month, we can clearly see that:\n- Dec and Jan had the highest item_cnt_day.\n- Dec also had the highest item_sale (likely Xmas)\n- Nov had the lowest item_cnt_day but Jul had the lowes item_sale\n\nAlso, there are a few peaks over the years, they are:\n- End of Nov 2013, lots of revenues\n- End of Dec 2013, lots of item_cnt_day, but relatively lower item_sale compared to Nov 2013\n- End of Dec 2014, lots of item_cnt_day\n- There are also peaks around the end of May 2014 and 2015\n\nWe also see declines in item_cnt_day and item_sale from 2013 to 2015"},{"metadata":{"trusted":true,"_uuid":"75daee0ad8e98446ac75b5c2da3c8df50cdc1c35"},"cell_type":"code","source":"# this code block below plots interactive graphs\ndef groupby(thing,  label = ''):\n    # this function is used by the updatemenus, buttons dict further down in line 26\n    tmp = train.groupby(thing)[['item_cnt_day', 'item_sale']].sum()\n    return dict(\n        args=[{\n            'x': [tmp.index, tmp.index],\n            'y': [tmp['item_cnt_day'], tmp['item_sale']],\n        }],\n        method='update', label=label\n    )\n\ntmp = groupby('date')\ntrace1 = go.Scatter(x=tmp['args'][0]['x'][0], y=tmp['args'][0]['y'][0], opacity=0.75, name='item_cnt_day')\ntrace2 = go.Scatter(x=tmp['args'][0]['x'][1], y=tmp['args'][0]['y'][1], opacity=0.75, name='item_sale', yaxis='y2')\ndata = [trace1, trace2]\nlayout = go.Layout(\n    yaxis=dict(title='Item counts'),\n    yaxis2=dict(title='Item sale', overlaying='y', side='right')\n)\n\n\nfig = go.Figure(data=data, layout=layout)\nfig.layout.updatemenus = list([\n    dict(\n        buttons=[groupby(i, l) for i, l in [\n            (train.date.dt.date, 'date'),\n            (train.date.dt.dayofyear, 'day of year'),\n            (train.date.dt.day, 'day of month'),\n            (train.date.dt.dayofweek, 'day of week'),\n            (train.date.dt.month, 'month'),\n            (train.date.dt.week, 'week'),\n            (train.date.dt.weekofyear, 'week of year'),\n            (train.date.dt.year, 'year')\n        ]] ,\n        direction = 'down',\n        showactive = True,\n        x = 0, xanchor = 'left',\n        y = 1.25, yanchor = 'top' \n    ),\n])\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0b3f7086ccb4e8c1eb82c3929fd865e834f6cc4"},"cell_type":"markdown","source":"In total, there were 3.6M items sold, with a combined revenue of \\$3.39 billions.\n\nBy item, the most popular were:\n- By item_cnt_day was: item_id 20949, with 187642 units sold, generated $929K\n- By item_sale was: item_id 6675, worth \\$219M in revenue, with 10289 units sold\n\nThe worst are: item 1590, 11871, 18062, 13474, 13477. Shops lost money on them\n\nBy shop:\n- shop_id 31 had the highest item_cnt_day (310777 items sold), and also the higest item_sale (\\$235M)\n- shop_id 36 had the lowest item_cnt_day (330), with a revenue of \\$377K\n\nItem_category. The most popular:\n- By item_cnt_day is item_category_id 40, with 634171 units sold, generating \\$170M\n- By item_sale is item_category_id 19, with 254887 units sold (\\$412M)\n\nThe worst performers are: item_category_id 51, with only 1 unit sold (\\$129), item_category_id 50, with 3 units sold (\\$24)"},{"metadata":{"trusted":true,"_uuid":"ba2ae13ed6039a1f1962d97e1f12bf144581df32"},"cell_type":"code","source":"def groupby(thing, sort_values = 'item_cnt_day'):\n    tmp = train.groupby(thing)[['item_cnt_day', 'item_sale']].sum().sort_values(sort_values).reset_index()[:-100:-1]\n    return dict(\n        args=[{\n            'x': [tmp[thing], tmp[thing]],\n            'y': [tmp['item_cnt_day'], tmp['item_sale']],\n        }, {\n            'xaxis': dict(type='category')\n        }],\n        method='update', label=thing + ' sort by ' + sort_values\n    )\n\ntmp = groupby('item_id')\ntrace1 = go.Bar(x=tmp['args'][0]['x'][0], y=tmp['args'][0]['y'][0], opacity=0.5, name='item_cnt_day')\ntrace2 = go.Bar(x=tmp['args'][0]['x'][1], y=tmp['args'][0]['y'][1], opacity=0.5, name='item_sale', yaxis='y2')\ndata = [trace1, trace2]\nlayout = go.Layout(\n    xaxis=dict(type='category'),\n    yaxis=dict(title='Item counts'),\n    yaxis2=dict(title='Item sale', overlaying='y', side='right')\n)\nfig = go.Figure(data=data, layout=layout)\nfig.layout.updatemenus = list([\n    dict(\n        buttons=[groupby(i, s) for i, s in [\n            ('item_id', 'item_cnt_day'),\n            ('item_id', 'item_sale'),\n            ('item_category_id', 'item_cnt_day'),\n            ('item_category_id', 'item_sale'),\n            ('shop_id', 'item_cnt_day'),\n            ('shop_id', 'item_sale'),\n        ]] ,\n        direction = 'down',\n        showactive = True,\n        x = 0, xanchor = 'left',\n        y = 1.25, yanchor = 'top' \n    ),\n])\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c8285d5c00c9fc94858983078cfff5e27845452"},"cell_type":"markdown","source":"We can examine the activities of individual items over time. We can see that some products were popular at the beginning then vanished off. Some products were more popular near the end of the time scale. Some products have peaks at vairous time points."},{"metadata":{"trusted":true,"_uuid":"4e0f1ed6e7e0eb1911d2987fe605fb2be9fa5558"},"cell_type":"code","source":"# this code block takes a long time to run, so only selec the top 25 items for displaying\ntraces = []\nfor i in train.groupby('item_id')['item_cnt_day'].sum().sort_values(ascending=False).index[:25]:\n    tmp = train[train['item_id'] == i].groupby('date_block_num')['item_cnt_day'].sum()\n    traces.append(go.Scatter(x = tmp.index, y = tmp, opacity=0.5, name=str(i), visible='legendonly'))\n\niplot({\n    'data': traces,\n    'layout': {\n        'xaxis': { 'title': 'date_block_num' },\n        'yaxis': { 'title': 'item_cnt_day' },\n        'title': 'Number of item_cnt_day per item_id',\n    },\n})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a5b2fe29fce59bccf0814b5b2a11e1fff6b7b6f"},"cell_type":"markdown","source":"We can also examine the activites of item category."},{"metadata":{"trusted":true,"_uuid":"9be99e1842de236262f0509d8e6abd8f7c1b9f9a"},"cell_type":"code","source":"traces = []\nfor i in train.groupby('item_category_id')['item_cnt_day'].sum().sort_values(ascending=False).index:\n    tmp = train[train['item_category_id'] == i].groupby('date_block_num')['item_cnt_day'].sum()\n    traces.append(go.Scatter(x = tmp.index, y = tmp, opacity=0.5, name=str(i), visible='legendonly'))\n\niplot({\n    'data': traces,\n    'layout': {\n        'xaxis': { 'title': 'date_block_num' },\n        'yaxis': { 'title': 'item_cnt_day' },\n        'title': 'Number of item_cnt_day per item_category_id',\n    },\n})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec1b84b216b90ea6cd6adec42c8030b30da1963a"},"cell_type":"markdown","source":"Similarly, We can examine the activities of individual shops over time."},{"metadata":{"trusted":true,"_uuid":"5ecf7ccb1ad7f16206092c1252db92c78d6242e6"},"cell_type":"code","source":"traces = []\nfor i in train.groupby('shop_id')['item_cnt_day'].sum().sort_values(ascending=False).index:\n    tmp = train[train['shop_id'] == i].groupby('date_block_num')['item_cnt_day'].sum()\n    traces.append(go.Scatter(x = tmp.index, y = tmp, opacity=0.5, name=str(i), visible='legendonly'))\n\niplot({\n    'data': traces,\n    'layout': {\n        'xaxis': { 'title': 'date_block_num' },\n        'yaxis': { 'title': 'item_cnt_day' },\n        'title': 'Number of item_cnt_day per shop',\n    },\n})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6d0e14b27149c1de9a86512774b54fbd3d1d2fa"},"cell_type":"markdown","source":"Now we want to explore the correlations between item_cnt_day and item_price. We see that most items do not seem to have a strong correlation of item_cnt_day and item_price"},{"metadata":{"trusted":true,"_uuid":"4bbe0384035ccb8dc58f3cadc1b31f36b6d3e4f4"},"cell_type":"code","source":"def count_and_corr(x):\n    count = len(x)\n    corr = np.corrcoef(x['item_cnt_day'], x['item_price'])[0, 1] if count > 1 else np.nan\n    return pd.Series([count, corr], index=['count', 'corr'])\n\ntrain.groupby('item_id').apply(count_and_corr).sort_values('corr', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e577b3af431e86d462b17c75fb305ae3c0553df0"},"cell_type":"markdown","source":"We now want to see the distributions of items and shops in the test set\n- For shop_id, all shop_ids present in the test set are also present in the train set\n- All item_category_id are present in test set and train set.\n- For item_id, there are 363 items present only in the test set but are not in the train set."},{"metadata":{"trusted":true,"_uuid":"d92e185c2322af26b55a6960884fe3904918b36e"},"cell_type":"code","source":"len(set(test['shop_id']).difference(set(train['shop_id']))) # all shop_id in the test set are present in the train set\nlen(set(test['item_category_id']).difference(set(train['item_category_id']))) # all categories are present in both\nlen(set(test['item_id']).difference(set(train['item_id']))) # 363 item_id in the test set are NOT present in the train set","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c770807bb8894008210218bc6eb4ed0d68d7b2e7"},"cell_type":"markdown","source":"## Basic prediction\n\nThe most basic prediction would be to use the result from the previous month. In this case, data from the train set where date_block_num == 33\n\nThis will give a public score of 8.53027, which is really bad.\n\nBut if we clip the item_cnt to 0 ~ 20, the public score will be 1.16777\n\nClipping to -10 ~ 30 gives a public score of 1.23867\n\nNow our goal is to beat the 1.16777"},{"metadata":{"trusted":true,"_uuid":"81f1c0ff3f4f59dd71ca07bf866419ab3747da09"},"cell_type":"code","source":"# first we define some helper functions: scoring and saving to csv\n\nfrom sklearn.metrics import mean_squared_error\n\ndef score(*y): return mean_squared_error(*y) ** 0.5\n\n# save to csv for submission\ndef to_csv(predicted_values, filename):\n    pd.DataFrame({\n        'ID': test['ID'],\n        'item_cnt_month': predicted_values.clip(0, 20)\n    }).to_csv(filename, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b9aa42f23c2405342d2f8444447aa4bf2f6b3c7"},"cell_type":"code","source":"tmp = train[train['date_block_num'] == 33].groupby(['shop_id', 'item_id'])['item_cnt_day'].sum().reset_index().rename(columns={ 'item_cnt_day': 'item_cnt_month' }) # agg item_cnt_day by shop_id and item_id\ntmp = test.merge(tmp, how='left', on=['shop_id', 'item_id']) # merge test with tmp using shop_id and item_id\n\n# use as is\nto_csv(tmp['item_cnt_month'].fillna(0), 'basic.csv') # there are lots of NaN in item_cnt LB 8.53027\n\n# clip from 0 to 20\nto_csv(tmp['item_cnt_month'].fillna(0).clip(0, 20), 'basic_clipped_0_20.csv') # LB 1.16777\n\n# fill na with median groupby shop\nto_csv(tmp['item_cnt_month'].fillna(tmp.groupby('shop_id')['item_cnt_month'].transform('median')).clip(0, 20), 'basic_fillna_median.csv') # LB 1.41848\n\n# jsut having a look\ntmp.item_cnt_month.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e5abf2d1aa820f378b41191e2e2ea8b82ca3131"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b6bedf3b706d890f63bf8dda8b59644844e6072"},"cell_type":"markdown","source":"## Prediction with machine learning\n\nWe are going to try with: RandomForest, lightGBM, XGBoost\n\nFirst, we need to restructure the data and add more features\n\n### Trial 1, add item_cnt from previous month"},{"metadata":{"trusted":true,"_uuid":"37ee02fb537b88ce40f32b46d846f6e1c09ba615"},"cell_type":"code","source":"# we re format the train to aggregate item_cnt_day to item_cnt_month\ntmp = train.groupby(['date_block_num', 'year', 'month', 'shop_id', 'item_id', 'item_category_id']).agg({\n    'item_price': 'median',\n    'item_cnt_day': 'sum',\n}).reset_index().rename(columns={ 'item_cnt_day': 'item_cnt_month' })\n\n# then add the previous item_cnt_month\ntmp = tmp.merge(\n    tmp.assign(date_block_num=tmp['date_block_num'] + 1)[['date_block_num', 'shop_id', 'item_id', 'item_cnt_month']].rename(columns={ 'item_cnt_month': 'item_cnt_month_pre' }),\n    how='left',\n    on=['date_block_num', 'shop_id', 'item_id'],\n)\n\n# remove the date_block_num 0\ntmp = tmp[tmp['date_block_num'] != 0]\n\n# fillna with 0\ntmp['item_cnt_month_pre'].fillna(0, inplace=True)\n\n# train.groupby(['date_block_num', 'year', 'month']).size() # this is to check date_block_num vs year / month all agree\n\n# these are our predictor features\ncols = ['date_block_num', 'year', 'month', 'shop_id', 'item_id', 'item_category_id', 'item_price', 'item_cnt_month_pre']\n\n# then create train and val set\nX_train = tmp[tmp['date_block_num'] < 33][cols]\ny_train = tmp[tmp['date_block_num'] < 33]['item_cnt_month']\n\nX_val = tmp[tmp['date_block_num'] == 33][cols]\ny_val = tmp[tmp['date_block_num'] == 33]['item_cnt_month']\n\n# show the data\ndisplay(tmp.head())\n\n# also add extra features to test\ntmp = pd.merge(\n    test,\n    tmp[tmp['date_block_num'] == 33][['shop_id', 'item_id', 'item_price', 'item_cnt_month']].rename(columns={ 'item_cnt_month': 'item_cnt_month_pre' }),\n    how='left',\n    on=['shop_id', 'item_id'],\n    suffixes=['', '_y']\n).fillna(0)[cols]\n\n# then show the data\ndisplay(tmp.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f98f9d8c3a91ff80dcaf2b76bb1e0f15f18f75a"},"cell_type":"markdown","source":"### RandomForest\n\nIt takes a long time to run and the results from the default parameters are not great. "},{"metadata":{"trusted":true,"_uuid":"da87a8f56c6df9ce7ef3b2bb046ab2c3fab3bc76","scrolled":true},"cell_type":"code","source":"# %%time\n# from sklearn.ensemble import RandomForestRegressor\n\n# model = RandomForestRegressor(n_estimators=100, random_state=0, n_jobs=-1)\n# model.fit(X_train, y_train)\n\n# print(f'''RMSE\n# train: {score(model.predict(X_train), y_train):.4f}\n# val: {score(model.predict(X_val), y_val):.4f}''')\n\n# # save to csv\n# to_csv(model.predict(test[cols]), 'submission_rf.csv')\n\n# # repeat with clipping y\n# model = RandomForestRegressor(n_estimators=100, random_state=0, n_jobs=-1)\n# model.fit(X_train, y_train.clip(0, 20))\n\n# print(f'''RMSE with clipping in y\n# train: {score(model.predict(X_train), y_train.clip(0, 20)):.4f}\n# val: {score(model.predict(X_val), y_val.clip(0, 20)):.4f}''')\n\n# to_csv(model.predict(test[cols]), 'submission_rf2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd1037f3ead2e9c34e236ffd903f7ca346191e30"},"cell_type":"code","source":"# run out of memory when n_estimators>=200, so not run this\n# %%time\n# hist = { 'train': [], 'val': [], 'i': [], }\n# for i in [10, 100, 200, 300, 400, 500]:\n#     print(i)\n#     model = RandomForestRegressor(n_estimators=i, max_depth=10, random_state=0, n_jobs=-1)\n#     model.fit(X_train, y_train)\n#     hist['i'].append(i)\n#     hist['train'].append(score(model.predict(X_train), y_train))\n#     hist['val'].append(score(model.predict(X_val), y_val))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df9bc3e78fb4f3fafcb950bd78bccd4c6c6d561b"},"cell_type":"markdown","source":"### lightGBM\n\nUsing the default parameters"},{"metadata":{"trusted":true,"_uuid":"ff11a517306a190aab320e8e3a111faa7d363ce5"},"cell_type":"code","source":"%%time\n\nimport lightgbm as lgb\n\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'seed': 0,\n    'nthread': 4,\n}\n\nmodel = lgb.train(\n    params,\n    lgb.Dataset(X_train, y_train),\n    5000,\n    valid_sets=lgb.Dataset(X_val, y_val),\n    early_stopping_rounds=100,\n    verbose_eval=100\n)\n\nto_csv(model.predict(tmp), 'lgb1.csv') # this give LB 5.52450","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06c32ef0f8c99340cd93b90171f452beaf4b72e2"},"cell_type":"markdown","source":"LightGBM, clip the y_train during training."},{"metadata":{"trusted":true,"_uuid":"86f0ca93075c064f3b71e312c039e64fd409653d"},"cell_type":"code","source":"%%time\n\nmodel = lgb.train(\n    params,\n    lgb.Dataset(X_train, y_train.clip(0, 20)), # clip 0 ~ 20 here\n    5000,\n    valid_sets=lgb.Dataset(X_val, y_val.clip(0, 20)),\n    early_stopping_rounds=100,\n    verbose_eval=100\n)\n\nto_csv(model.predict(tmp), 'lgb1b.csv') # this give LB 2.78524","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5ccbfa7dc9b49988e9e74b9dada6b7d1f2018e1"},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true,"_uuid":"f3e72f2415d994afa0810e6cd4ddca53adc57dea"},"cell_type":"code","source":"%%time\n\nimport xgboost as xgb\n\nmodel = xgb.train(\n    params={ 'eta': 0.15, 'silent': 1,  },\n    dtrain=xgb.DMatrix(X_train, label=y_train, silent=True),\n    num_boost_round=100,\n    evals=[(xgb.DMatrix(X_val, label=y_val, silent=True), 'test')],\n    early_stopping_rounds=10\n)\n\nto_csv(model.predict(xgb.DMatrix(tmp)), 'xgb1.csv') # LB 18.51368","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72f5ae83631502a6d5a72b8cb127911230b68dd2"},"cell_type":"code","source":"%%time\n# clipping y to 0, 20 during training\n\nimport xgboost as xgb\n\nmodel = xgb.train(\n    params={ 'eta': 0.15, 'silent': 1,  },\n    dtrain=xgb.DMatrix(X_train, label=y_train.clip(0, 20), silent=True),\n    num_boost_round=100,\n    evals=[(xgb.DMatrix(X_val, label=y_val.clip(0, 20), silent=True), 'test')],\n    early_stopping_rounds=10\n)\n\n\nto_csv(model.predict(xgb.DMatrix(tmp)), 'xgb1b.csv') # LB is 3.32154","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2c9b633f430272cb8ea4faf8c825d3cbacc2412"},"cell_type":"markdown","source":"### Trial 2, use item_cnt up to 6 previous months\n\nInstead of using only previous month item_cnt, we can use more.\n\nBelow, we create a new dataframe, train3 to use item_cnt from previous 6 months"},{"metadata":{"trusted":true,"_uuid":"eacc9dfc28567d31316749592ce9052d806284df","scrolled":true},"cell_type":"code","source":"# as before, agg item_cnt_month\ntmp = train.groupby(['date_block_num', 'year', 'month', 'shop_id', 'item_id', 'item_category_id']).agg({\n    'item_price': 'median',\n    'item_cnt_day': 'sum',\n}).reset_index().rename(columns={ 'item_cnt_day': 'item_cnt_month' })\n\n# then use item_cnt_month from the previous 6 months\nfor i in range(1, 7):\n    tmpi = tmp[['date_block_num', 'shop_id', 'item_id', 'item_cnt_month']].copy()\n    tmpi['date_block_num'] += i\n    tmpi.rename(columns={ 'item_cnt_month': 'item_cnt_month_pre_' + str(i) }, inplace=True)\n    tmp = tmp.merge(tmpi, how='left', on=['date_block_num', 'shop_id', 'item_id']).fillna(0)\n\n    \n# these are our predictor features\ncols = ['date_block_num', 'year', 'month', 'shop_id', 'item_id', 'item_category_id', 'item_price',\n        'item_cnt_month_pre_1', 'item_cnt_month_pre_2', 'item_cnt_month_pre_3', 'item_cnt_month_pre_4', 'item_cnt_month_pre_5', 'item_cnt_month_pre_6']\n\n# clip the item_cnt_month to 0 to 20\nfor i in ['item_cnt_month', 'item_cnt_month_pre_1', 'item_cnt_month_pre_2', 'item_cnt_month_pre_3', 'item_cnt_month_pre_4', 'item_cnt_month_pre_5', 'item_cnt_month_pre_6']:\n    tmp[i] = tmp[i].clip(0, 20)\n\ndisplay(tmp.head())\n\n# create train and val set\nX_train = tmp[(tmp['date_block_num'] < 33) & (tmp['date_block_num'] > 5)][cols] # remove the first 6 months in the dataset where date_block_num is in 0 to 5\ny_train = tmp[(tmp['date_block_num'] < 33) & (tmp['date_block_num'] > 5)]['item_cnt_month']\n\nX_val = tmp[tmp['date_block_num'] == 33][cols]\ny_val = tmp[tmp['date_block_num'] == 33]['item_cnt_month']\n\n# test set features need to be updated too\ntmp = pd.merge(\n    test[['ID', 'date_block_num', 'year', 'month', 'shop_id', 'item_id', 'item_category_id']],\n    tmp.assign(date_block_num = tmp.date_block_num + 1),\n    how='left',\n    on=['date_block_num', 'shop_id', 'item_id'],\n    suffixes=['', '_y']\n).fillna(0)[cols]\n\nprint(tmp.shape)\ndisplay(tmp.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ade07cc489f60246d119dff271d98385d5353a3"},"cell_type":"markdown","source":"Now try to run lightGBM again with default parameters"},{"metadata":{"trusted":true,"_uuid":"cbc7968cc590db2bfbf8590d703c702c4577d8e4"},"cell_type":"code","source":"%%time\nimport lightgbm as lgb\n\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'seed': 0,\n    'nthread': 4,\n}\n\nmodel = lgb.train(\n    params,\n    lgb.Dataset(X_train, y_train),\n    5000,\n    valid_sets=lgb.Dataset(X_val, y_val),\n    early_stopping_rounds=100,\n    verbose_eval=100\n)\n\nto_csv(model.predict(tmp), 'lgb2.csv') # this gives LB 2.89164 sadly","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb93115324ef693702fbfcb6c95adc40ee7b9064"},"cell_type":"markdown","source":"Since the defaul paramaters do not give good results, now time for hyperparameter tuning ☕🍵\n\nWe are going to use hyperopt, a package for automated hyperparameter tuning using Bayesian Optimization (more [details](https://github.com/WillKoehrsen/hyperparameter-optimization))."},{"metadata":{"trusted":true,"_uuid":"20e75b2f3b7867ed00f87c02defbef5e13185dde"},"cell_type":"code","source":"from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\nfrom time import time\nimport lightgbm as lgb\n\n# iterations\ni = 0\n\n# define spaces to search\nspace = {\n    'num_leaves': hp.quniform('num_leaves', 25, 75, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.05)),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.75, 1.0),\n}\n\n# objective function: to minimise the cost\ndef objective(params):\n    t = time() # keep track of duration for each iteration\n    global i\n    i += 1 # increase iteration count\n    for j in ['num_leaves', 'subsample_for_bin', 'min_child_samples']: params[j] = int(params[j]) # need to explicitly turn them into int\n\n    model = lgb.train(\n        { **params, 'objective': 'regression', 'metric': 'rmse', 'nthread': 4, 'bagging_freq': 5, 'seed': 0 }, # merge params together\n        lgb.Dataset(X_train, y_train),\n        5000,\n        valid_sets=lgb.Dataset(X_val, y_val),\n        early_stopping_rounds=100,\n        verbose_eval=0\n    )\n    loss = model.best_score['valid_0']['rmse']\n    \n    t = time() - t\n    print(f'{i}) {t:.1f}s, {loss:.4f}, {params}')\n    return loss\n\n# our trial history\ntrials = Trials()\n\n# run the baysian optimisation for params tuning\n# best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=1000, trials=trials, rstate=np.random.RandomState(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b76cb6068c686158312e0fec4d93227ae3f208c"},"cell_type":"code","source":"# because it takes so long for tuning, and the kernel can only run for 6 hours, so every now and then I save the params to a pickle file, then upload this file to else where\n# later on, I can download the pickle file again and continue the training where it left.\n# I use https://file.io for simple file storage.\n\n# import pickle\n# with open('trials.pickle', 'wb') as handle: pickle.dump(trials, handle, protocol=pickle.HIGHEST_PROTOCOL)\n# with open('trials.pickle', 'rb') as handle: trials = pickle.load(handle)\n# !curl -F \"file=@trials.pickle\" https://file.io\n\n# print(len(trials.trials))\n# sorted(trials, key=lambda k: k['result']['loss'] if 'loss' in k['result'] else np.inf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"235bb23d619fa175eb709454c0251425199c1eab"},"cell_type":"code","source":"%%time\n# we have found the best params\n\nparams = {\n    'objective': 'regression', 'metric': 'rmse', 'nthread': 4, 'bagging_freq': 5, 'seed': 0,\n    'bagging_fraction': 0.9706689662242649,\n    'colsample_by_tree': 0.7315494493281669,\n    'learning_rate': 0.016357695097584456,\n    'min_child_samples': 150,\n    'num_leaves': 65,\n    'reg_alpha': 0.053334028461403574,\n    'reg_lambda': 0.111413264877147,\n    'subsample_for_bin': 240000,\n}\n\nmodel = lgb.train(\n    params,\n    lgb.Dataset(X_train, y_train),\n    5000,\n    valid_sets=lgb.Dataset(X_val, y_val),\n    early_stopping_rounds=100,\n    verbose_eval=100\n)\n\nto_csv(model.predict(tmp), 'lgb2b.csv') # this gives LB 3.00277, :-(","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5fd2b31acc680e514fb9e19e98460e6238d49a1"},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true,"_uuid":"0b6207faa9b03d5c5d148d26bbb2fd09f2d24155"},"cell_type":"code","source":"%%time\n\nimport xgboost as xgb\n\nmodel = xgb.train(\n    params={ 'eta': 0.15, 'silent': 1,  },\n    dtrain=xgb.DMatrix(X_train, label=y_train, silent=True),\n    num_boost_round=100,\n    evals=[(xgb.DMatrix(X_val, label=y_val, silent=True), 'test')],\n    early_stopping_rounds=10\n)\n\nto_csv(model.predict(xgb.DMatrix(tmp)), 'xgb2.csv') # LB 2.35866","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e03df115745fd510b46f923754886f841de8d2b1"},"cell_type":"markdown","source":"### Trial 3\n\nWe pivot the data, so that columns are date_block_num, row values are item_cnt"},{"metadata":{"trusted":true,"_uuid":"43fd210c34f3caca30f59c52b0b14a0a47c71e93"},"cell_type":"code","source":"tmp = train.pivot_table(index=['shop_id', 'item_id'], columns=['date_block_num'], values='item_cnt_day', aggfunc=np.sum).fillna(0)\n\ntmp.head()\n\nX_train = tmp.loc[:, :32].values\ny_train = tmp[32].values\n\nX_val = tmp.loc[:, 1:33].values\ny_val = tmp[33].values\n\ntmp = test[['shop_id', 'item_id']].merge(tmp, how='left', on=['shop_id', 'item_id']).fillna(0).loc[:, range(2:34)].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e6fae7566276eab93dcfea2e27539a4a3ba2205"},"cell_type":"code","source":"%%time\n\nimport lightgbm as lgb\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'seed': 0,\n    'nthread': -1,\n}\n\nmodel = lgb.train(\n    params,\n    lgb.Dataset(X_train, y_train),\n    5000,\n    valid_sets=lgb.Dataset(X_val, y_val),\n    early_stopping_rounds=100,\n    verbose_eval=100\n)\n\nto_csv(model.predict(tmp), 'lgb3.csv') # LB 1.08520","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7651d51ba3419c49efeb73f014ec351170688ad"},"cell_type":"code","source":"%%time\n\nimport xgboost as xgb\n\nmodel = xgb.train(\n    params={ 'eta': 0.15, 'silent': 1,  },\n    dtrain=xgb.DMatrix(X_train, label=y_train, silent=True),\n    num_boost_round=100,\n    evals=[(xgb.DMatrix(X_val, label=y_val, silent=True), 'test')],\n    early_stopping_rounds=10\n)\n\nto_csv(model.predict(xgb.DMatrix(tmp)), 'xgb3.csv') # LB 1.05809","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"851f70b621700cc5aa70de5567736e00ee51fb6c"},"cell_type":"markdown","source":"We can clip the values to 0, 20 during training"},{"metadata":{"trusted":true,"_uuid":"64060e11723cb43aa32b6e6d1bc32e1686ac210b"},"cell_type":"code","source":"%%time\n\nimport lightgbm as lgb\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'seed': 0,\n    'nthread': -1,\n}\n\nmodel = lgb.train(\n    params,\n    lgb.Dataset(X_train, y_train.clip(0, 20)),\n    5000,\n    valid_sets=lgb.Dataset(X_val, y_val.clip(0, 20)),\n    early_stopping_rounds=100,\n    verbose_eval=100\n)\n\nto_csv(model.predict(tmp), 'lgb3b.csv') # LB 1.03805","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"074dda8ba0742f174babd02c311adf74e2a460e0"},"cell_type":"code","source":"%%time\n\nimport xgboost as xgb\n\nmodel = xgb.train(\n    params={ 'eta': 0.15, 'silent': 1,  },\n    dtrain=xgb.DMatrix(X_train, label=y_train, silent=True),\n    num_boost_round=100,\n    evals=[(xgb.DMatrix(X_val, label=y_val, silent=True), 'test')],\n    early_stopping_rounds=10\n)\n\nto_csv(model.predict(xgb.DMatrix(tmp)), 'xgb3b.csv') # LB 1.03369","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b96b50da5db0ed1d51df5ce2a60f5ef3aab3d88"},"cell_type":"markdown","source":"### Trial 4\nWe clip all the values to 0, 20"},{"metadata":{"trusted":true,"_uuid":"f0322c45c7951f9692cc846fbceea9c4ea5256ca"},"cell_type":"code","source":"tmp = train.pivot_table(index=['shop_id', 'item_id'], columns=['date_block_num'], values='item_cnt_day', aggfunc=np.sum).fillna(0).clip(0, 20)\n\ntmp.head()\n\nX_train = tmp.loc[:, :32].values\ny_train = tmp[32].values\n\nX_val = tmp.loc[:, 1:33].values\ny_val = tmp[33].values\n\ntmp = test[['shop_id', 'item_id']].merge(tmp, how='left', on=['shop_id', 'item_id']).fillna(0).loc[:, range(2:34)].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fde1922005b4cb507db997860a529134f2f40918"},"cell_type":"code","source":"%%time\n\nimport lightgbm as lgb\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'seed': 0,\n    'nthread': -1,\n}\n\nmodel = lgb.train(\n    params,\n    lgb.Dataset(X_train, y_train),\n    5000,\n    valid_sets=lgb.Dataset(X_val, y_val),\n    early_stopping_rounds=100,\n    verbose_eval=100\n)\n\nto_csv(model.predict(tmp), 'lgb4.csv') # LB 1.03592\n# to_csv(model.predict(tmp).round(), 'lgb4b.csv') # LB 1.05436","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f632f5e0f84ba28c6a4508965229bc0526b7a945"},"cell_type":"markdown","source":"We try swapping X_train and X_val during training"},{"metadata":{"trusted":true,"_uuid":"4950efbb1b6b01389869e9b205380c7591e977cc"},"cell_type":"code","source":"%%time\n\nimport lightgbm as lgb\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'seed': 0,\n    'nthread': -1,\n}\n\nmodel = lgb.train(\n    params,\n    lgb.Dataset(X_val, y_val), # swap the X_train and X_val here\n    5000,\n    valid_sets=lgb.Dataset(X_train, y_train),\n    early_stopping_rounds=100,\n    verbose_eval=100\n)\n\nto_csv(model.predict(tmp), 'lgb4c.csv') # LB 1.02844","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"723c7b2ba3af8af3139843931494b7b6b144b552"},"cell_type":"markdown","source":"Hyperparams tuning for lightbgm models"},{"metadata":{"trusted":true,"_uuid":"d0f927c1d851be94e060d110664dc346335a037f"},"cell_type":"code","source":"from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\nfrom time import time\nimport lightgbm as lgb\n\n# iterations\ni = 0\n\n# define spaces to search\nspace = {\n    'num_leaves': hp.quniform('num_leaves', 25, 75, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.05)),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.75, 1.0),\n}\n\n# objective function: to minimise the cost\ndef objective(params):\n    t = time() # keep track of duration for each iteration\n    global i\n    i += 1 # increase iteration count\n    for j in ['num_leaves', 'subsample_for_bin', 'min_child_samples']: params[j] = int(params[j]) # need to explicitly turn them into int\n\n    model = lgb.train(\n        { **params, 'objective': 'regression', 'metric': 'rmse', 'nthread': 4, 'bagging_freq': 5, 'seed': 0 }, # merge params together\n        lgb.Dataset(X_val, y_val),\n        5000,\n        valid_sets=lgb.Dataset(X_train, y_train),\n        early_stopping_rounds=100,\n        verbose_eval=0\n    )\n    loss = model.best_score['valid_0']['rmse']\n    \n    t = time() - t\n    print(f'{i}) {t:.1f}s, {loss:.4f}, {params}')\n    return loss\n\n# our trial history\ntrials = Trials()\n\n# run the baysian optimisation for params tuning\n# best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=1000, trials=trials, rstate=np.random.RandomState(0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d23c35d10ff998d7b0efbd3eefc0d85a5cc88a7"},"cell_type":"markdown","source":"Having run the hyperparams optimisation above, we otain the best params and run them as below:"},{"metadata":{"trusted":true,"_uuid":"07c1292f5ae0dffd23866334285e7e0af6be8f9a"},"cell_type":"code","source":"%%time\n\nimport lightgbm as lgb\n\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'seed': 0,\n    'nthread': -1,\n    'bagging_fraction': 0.8525450579226199,\n    'colsample_by_tree': 0.9705350658278141,\n    'learning_rate': 0.04395833528778046,\n    'min_child_samples': 460,\n    'num_leaves': 60,\n    'reg_alpha': 0.18424221349470526,\n    'reg_lambda': 0.7506282086709485,\n    'subsample_for_bin': 140000,\n}\n\nmodel = lgb.train(\n    params,\n    lgb.Dataset(X_train, y_train),\n    5000,\n    valid_sets=lgb.Dataset(X_val, y_val),\n    early_stopping_rounds=100,\n    verbose_eval=100\n)\n\nto_csv(model.predict(tmp), 'lgb4d.csv') # LB\n\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'seed': 0,\n    'nthread': -1,\n    'bagging_fraction': 0.7730681635329509,\n    'colsample_bytree': 0.9440491158187898,\n    'learning_rate': 0.04806809101732328,\n    'min_child_samples': 400,\n    'num_leaves': 69,\n    'reg_alpha': 0.5385150471866715,\n    'reg_lambda': 0.8089922399251372,\n    'subsample_for_bin': 60000\n}\n\nmodel = lgb.train(\n    params,\n    lgb.Dataset(X_val, y_val),\n    5000,\n    valid_sets=lgb.Dataset(X_train, y_train),\n    early_stopping_rounds=100,\n    verbose_eval=100\n)\n\nto_csv(model.predict(tmp), 'lgb4e.csv') # LB 1.08228, bad","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a57baf63ee6ae85ee28c0fe0ab9f46a5b0347215"},"cell_type":"markdown","source":"#### Xgb"},{"metadata":{"trusted":true,"_uuid":"0fd2cbaa7cb2236a3117e4ec44fe6b57df6a6694"},"cell_type":"code","source":"%%time\n\nimport xgboost as xgb\n\nmodel = xgb.train(\n    params={ 'eta': 0.15, 'silent': 1,  },\n    dtrain=xgb.DMatrix(X_train, label=y_train, silent=True),\n    num_boost_round=100,\n    evals=[(xgb.DMatrix(X_val, label=y_val, silent=True), 'test')],\n    early_stopping_rounds=10\n)\n\nto_csv(model.predict(xgb.DMatrix(tmp)), 'xgb4.csv') # LB 1.03369","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d11b33ff934f8f85d8415c414cfc2efc95ac302f"},"cell_type":"markdown","source":"Again, we swap X_train and X_val during training"},{"metadata":{"trusted":true,"_uuid":"76591e4d242bc26396e7376028f5e27a2e9f16c7"},"cell_type":"code","source":"%%time\n\nimport xgboost as xgb\n\nmodel = xgb.train(\n    params={ 'eta': 0.15, 'silent': 1,  },\n    dtrain=xgb.DMatrix(X_val, label=y_val, silent=True), # swap X_train & X_val\n    num_boost_round=100,\n    evals=[(xgb.DMatrix(X_train, label=y_train, silent=True), 'test')],\n    early_stopping_rounds=10\n)\n\nto_csv(model.predict(xgb.DMatrix(tmp)), 'xgb4b.csv') # LB 1.02826","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8f52e986d8361964d06d73d32308f719bb2ccd7"},"cell_type":"markdown","source":"While we are here, we may as well try simpler models such as: linear, Ridge, Bayesian Ridge, and KNN."},{"metadata":{"trusted":true,"_uuid":"c05e0848be84c0866bc7b09e9c2b0c9914a06afe"},"cell_type":"code","source":"# simple linear model\nfrom sklearn import linear_model\n\nmodel = linear_model.LinearRegression()\nmodel.fit(X_train, y_train)\nprint(score(model.predict(X_val), y_val))\nto_csv(model.predict(tmp), 'linear_regression4.csv') # LB 1.03363\n\n# repeat again but just swapping the train and val\nmodel = linear_model.LinearRegression()\nmodel.fit(X_val, y_val)\nprint(score(model.predict(X_train), y_train))\nto_csv(model.predict(tmp), 'linear_regression4b.csv') # 1.03111","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e897b4ea1552ebca2e5471d13750ff5d44b749b"},"cell_type":"code","source":"# Ridge\n\nmodel = linear_model.RidgeCV(alphas=np.logspace(-1, 0, 10), cv=5)\nmodel.fit(X_train, y_train)\nprint(score(model.predict(X_val), y_val))\nto_csv(model.predict(tmp), 'lasso4.csv') # LB 1.03361\n\nmodel = linear_model.RidgeCV(alphas=np.logspace(-1, 0, 10), cv=5)\nmodel.fit(X_val, y_val)\nprint(score(model.predict(X_train), y_train))\nto_csv(model.predict(tmp), 'lasso4b.csv') # LB 1.03111","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d65ea7aea1423cfa22c46b4e0f77053f2bb24fa"},"cell_type":"code","source":"# Baysian Ridge\n\nmodel = linear_model.BayesianRidge()\nmodel.fit(X_train, y_train)\nprint(score(model.predict(X_val), y_val))\nto_csv(model.predict(tmp), 'BayesianRidge4.csv') # LB 1.03363\n\nmodel = linear_model.BayesianRidge()\nmodel.fit(X_val, y_val)\nprint(score(model.predict(X_train), y_train))\nto_csv(model.predict(tmp), 'BayesianRidge4b.csv') # LB 1.03112","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7424f689c6a06c533af3af93ac264fb4f31a5f7"},"cell_type":"code","source":"# KNN, but this take too long to run, and the LB is bad \n# from sklearn.neighbors import KNeighborsRegressor\n# model = KNeighborsRegressor(n_neighbors=2)\n# model.fit(X_train, y_train)\n# print(score(model.predict(X_val), y_val))\n# to_csv(model.predict(tmp), 'knn4.csv') # LB 1.12914","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58aa596f70eb5b3e6e879c982897fbfedb60f027"},"cell_type":"markdown","source":"### Trial 5\nSimilar to trial 4, but the X_train and X_val will be split randomly by row instead of by date_block_num"},{"metadata":{"trusted":true,"_uuid":"37efd0426777cc6b0b90712d05bf4285add5b375"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold, cross_val_score\n\n# cv = KFold(n_splits=5, shuffle=True, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d514f388ff515da08bd077b89e1c9af2fb14b610"},"cell_type":"code","source":"tmp = train.pivot_table(index=['shop_id', 'item_id'], columns=['date_block_num'], values='item_cnt_day', aggfunc=np.sum).fillna(0).clip(0, 20)\n\nX_train, X_val, y_train, y_val = train_test_split(tmp[list(range(33))], tmp[33], random_state=0)\n\ntmp = test[['shop_id', 'item_id']].merge(tmp, how='left', on=['shop_id', 'item_id']).fillna(0).loc[:, range(1, 34)].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3c5af7736cdf1e7064cd02257f2a0a44146ca4c"},"cell_type":"code","source":"%%time\n\nimport lightgbm as lgb\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'seed': 0,\n    'nthread': -1,\n}\n\nmodel = lgb.train(\n    params,\n    lgb.Dataset(X_train, y_train),\n    5000,\n    valid_sets=lgb.Dataset(X_val, y_val),\n    early_stopping_rounds=100,\n    verbose_eval=100\n)\n\nto_csv(model.predict(tmp), 'lgb5.csv') # LB 1.05852, no improvement","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cd760cd185a1417d972b5b3a2cf964d77a1ab22"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d300e58d267a9ae808468b2bd64784565ee93e6a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3aee1c690d3c7a8ef750a88c347cd1b9e7d3108"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11157d15f2c487a8fd2ed8a043c3f6bc7ea23102"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}