{"cells":[{"metadata":{},"cell_type":"markdown","source":"# I. Input Data\nThe first thing that we need to do is inputing the data. In order to do that, we need to import the necessary libraries first. These necessary libraries are *numpy* and *pandas*. After the data has been input, we can see to the data to see how we can analyze this, what information that we need the most etc.  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nitem_categories = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\ntrain = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train dataset is the most important dataset here. This dataset is also the baseline for this project. There are a lot of items in this dataset, we can see it from the *item_id*. But, we need to know what items are these, so we can combine them with *item_category_id* from **item_categories** dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.join(items.drop(['item_name'], axis=1).set_index('item_id'), on='item_id')\ntrain['date'] = pd.to_datetime(train.date)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some *item_cnt_day* value that has value less than 0. This is obviously an anomaly so we have to fix this. In order to do that, we will change every values that are less than 0 to be 1. "},{"metadata":{"trusted":true},"cell_type":"code","source":"anomaly_counts = list(pd.DataFrame(train.item_cnt_day[train.item_cnt_day < 0].value_counts()).index)\ntrain['item_cnt_day'] = train['item_cnt_day'].replace(anomaly_counts, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# II. Exploratory Data Analysis (EDA)\nNext step, we want to see how each features affect each others. A little knowledge about this can help us develop a better model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def cnt_sum(item_cat):\n    cnt_ave = []\n    for i in [2013, 2014, 2015]:\n        for j in list(range(1, 13)):\n            x = new_train.item_cnt_day[(new_train.year == i) & (new_train.month == j) &\n                                       (new_train.item_category_id == item_cat)].sum()\n            cnt_ave.append(x)\n    return cnt_ave","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train = train\nnew_train['year'] = [x.year for x in train.date]\nnew_train['month'] = [x.month for x in train.date]\n\nyears = []\nmonths = []\n\nfor i in [2013, 2014, 2015]:\n    for j in list(range(1, 13)):\n        years.append(i)\n        months.append(j)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Item Categories Trend in 2013-2015\nLet's see how some categories trend. I picked 5 categories randomly and see how the trend is from the beginning of 2013 to the end of 2015.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_exm = pd.DataFrame()\ndf_exm['year'] = years\ndf_exm['month'] = months\n\nfor i in [12, 8, 36, 50, 63]:\n    df_exm[str(i)] = cnt_sum(i)\n\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=df_exm[['12', '8', '36', '50', '63']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then I tried the same thing with another 5 random categories. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_exm = pd.DataFrame()\ndf_exm['year'] = years\ndf_exm['month'] = months\n\nfor i in [34, 17, 68, 47, 79]:\n    df_exm[str(i)] = cnt_sum(i)\n    \nplt.figure(figsize=(8, 6))\nsns.lineplot(data=df_exm[['34', '17', '68', '47', '79']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from two graphs above, there is no similar patterns between categories. Each category have their own trends. Other than that, there are certain categories that has far less demands than others. The categories like this are even harder how the trend goes. \n\nThere are categories that have peak demand each year and there are other categories that only peak once in 3 years. But, from all these categories that are shown in these graphs, there is one similarity. That is the declining sells at the end of the graph, which means there is a declining sells at December 2015. That can happen because of the fact that that's what really happened, or maybe that can also happened because of the incomplete data.  "},{"metadata":{},"cell_type":"markdown","source":"## Most Expensive Item Categories\nWe also need to know the price distribution and how price contribute to the sells. "},{"metadata":{"trusted":true},"cell_type":"code","source":"most_exp = pd.DataFrame()\nmost_exp['item_category'] = item_categories.item_category_id\n\nexp = []\nfor i in most_exp['item_category']:\n    x = train.item_price[train.item_category_id == i].mean()\n    exp.append(x)\nmost_exp['mean_price'] = exp\n\nmost_exp = most_exp.sort_values('mean_price', ascending=False)\n\nplt.figure(figsize=(10, 7))\nsns.barplot(data=most_exp.iloc[:10], x='item_category', y='mean_price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Most Sells Item Categories\nThis is the point of this project. We need to know what item categories that has most sells. "},{"metadata":{"trusted":true},"cell_type":"code","source":"most_bought = pd.DataFrame()\nmost_bought['item_category'] = item_categories.item_category_id\n\nbought = []\nfor i in most_bought['item_category']:\n    x = train.item_cnt_day[train.item_category_id == i].sum()\n    bought.append(x)\nmost_bought['cnt_total'] = bought\n\nmost_bought = most_bought.sort_values('cnt_total', ascending=False)\n\nplt.figure(figsize=(10, 7))\nsns.barplot(data=most_bought.iloc[:10], x='item_category', y='cnt_total')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Which Year Has Most Sells?"},{"metadata":{"trusted":true},"cell_type":"code","source":"sells_year = pd.DataFrame()\nsells_year['year'] = [2013, 2014, 2015]\nsells_year['sells'] = [train.item_cnt_day[train.year == x].sum() for x in sells_year['year']]\n\nplt.figure(figsize=(8, 6))\nsns.barplot(data=sells_year, x='year', y='sells')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the plot above, the sells is declining each year, making 2013 as a year with most sells. "},{"metadata":{},"cell_type":"markdown","source":"## Months With Most Sells\nAfter seeing how many items were sold each year, maybe we should also know how many items were sold in each month. We will make a graph that shows how many sells each month in respect to each years. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sells_month = pd.DataFrame()\n\ns_year = []\nfor i in [2013, 2014, 2015]: \n    for j in range(12): s_year.append(i)\n\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'June', 'July', 'Aug',\n          'Sep', 'Oct', 'Nov', 'Dec']\ns_months = []\nfor i in range(3): \n    for j in range(12): s_months.append(months[j]) \n        \nsells_month['year'] = s_year\nsells_month['month'] = s_months\n      \nthe_sells = []    \nfor y in [2013, 2014, 2015]:\n    x = [train.item_cnt_day[(train.month == x) & (train.year == y)].sum() for x in range(1, 13)]\n    for j in x: the_sells.append(j)\n\nsells_month['sells'] = the_sells\n\nplt.figure(figsize=(9, 6))\nsns.lineplot(data=sells_month, x='month', y='sells', hue='year')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the graph, 2013 is the year with most sells followed by 2014 and 2015 respectively. The year 2013 and 2014 almost have the same trend. \n\nWe also see that they all have similarties in October. The sells in November is drastically different from October, either declining like what happened in 2015 or inclining like what happened in 2013 and 2014.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"the_categories = list(most_bought.head(9).item_category)\nthe_months = months\nthe_years = [2013, 2014, 2015]\n\ncounts_input = []\nyears_input = []\nmonths_input = []\ncategory_input = []\n\nfor y in the_years:\n    for mm, m in zip(range(1, 13), the_months):\n        for c in the_categories:\n            the_input = train.item_cnt_day[(train.year == y) &\n                                           (train.month == mm) &\n                                           (train.item_category_id == c)].sum()\n            counts_input.append(the_input)\n            years_input.append(y)\n            months_input.append(m)\n            category_input.append(c)\n\ndf_exm_categories = pd.DataFrame()\ndf_exm_categories['year'] = years_input\ndf_exm_categories['month'] = months_input\ndf_exm_categories['category_id'] = category_input\ndf_exm_categories['counts_month'] = counts_input\n\nplt.figure(figsize=(12, 8))\nsns.relplot(data=df_exm_categories, x='month', y='counts_month', hue='year', \n             col='category_id', col_wrap=3, kind='line')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All 9 graphs above showed the sells trend for 9 item categories with most sells. We can see that 8 item categories followed the same trend as all-sells-monthly trend (showed by previous graph). Only *category_id* 20 that has different trend.  "},{"metadata":{},"cell_type":"markdown","source":"## Shops Monthly Sells in 2013-2015\nIn this project we will make predictions of how many sells does a shop make for a certain item in the future. Which means *shop_id* is a very important features. It is a necessary to see how a shop related to the sells.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_counts = pd.DataFrame()\nshops_counts['shop_id'] = shops.shop_id\nshops_counts['counts'] = [train.item_cnt_day[train.shop_id == x].sum() for x in shops.shop_id]\nbest_shops = list(shops_counts.sort_values('counts', ascending=False).head(9).shop_id)\n\ncounts_input = []\nyears_input = []\nmonths_input = []\nshops_input = []\n\nfor y in the_years:\n    for mm, m in zip(range(1, 13), the_months):\n        for s in best_shops:\n            the_input = train.item_cnt_day[(train.year == y) &\n                                           (train.month == mm) &\n                                           (train.shop_id == s)].sum()\n            counts_input.append(the_input)\n            years_input.append(y)\n            months_input.append(m)\n            shops_input.append(s)\n\ndf_exm_shops = pd.DataFrame()\ndf_exm_shops['year'] = years_input\ndf_exm_shops['month'] = months_input\ndf_exm_shops['shop_id'] = shops_input\ndf_exm_shops['counts'] = counts_input\n\nplt.figure(figsize=(12, 8))\nsns.relplot(data=df_exm_shops, x='month', y='counts', hue='year',\n            col='shop_id', col_wrap=3, kind='line')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see with what happened in item categories, the sells have a trend. That also happened to the shops. There is a trend applied to each shops. We can see form 9 most popular shops above, almost all of it have a same common trend, which also applied to total-sells trend.  "},{"metadata":{},"cell_type":"markdown","source":"# II. Building the Model\nAfter exploring the data using our visualizations, the next step is building a model that will be able to predict how the market work in the future. In order to build such models, we need to prepare data for training and validation. So, we need to split the train data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train[['item_id', 'shop_id', 'year', 'month', 'item_category_id']]\ny = train['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To ease the memory up during training, the data that we already used and won't use later need to be deleted.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"del items\ndel item_categories\ndel shops\ndel anomaly_counts\ndel new_train\ndel df_exm\ndel most_exp\ndel most_bought\ndel sells_year\ndel sells_month\ndel the_sells\ndel the_categories\ndel the_months\ndel the_years\ndel counts_input\ndel years_input\ndel months_input\ndel category_input\ndel df_exm_categories\ndel shops_counts\ndel best_shops\ndel df_exm_shops","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What Models?\nIn this project we decided to compare two set of models that use two different methods. Actually, both of these methods have similarity, which is they are both **ensemble** method. Ensemble method is a machine learning model that uses the results of many models prediction to generate one prediction. This method is developed from **decision tree** which means ensemble method uses many decision tree to generate predictions.\n\nThere are two types of ensemble methods:\n1. Bagging (Bootstrap Aggregating) which is an ensemble method that uses many decision trees that are trained at the same time. They only trained with different set of training data. the example of this method is **random forest**.\n2. Boosting which is an ensemble method that uses decision trees that work gradually. There is only one decision tree that is trained one at a time. The next decision tree that will be trained will use the result of the previous decision tree result in the training dataset. The example of boosting models are AdaBoost, gradient boost and extreme gradient boost.  "},{"metadata":{},"cell_type":"markdown","source":"## Bagging - Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\nestimator_rf = RandomForestClassifier(random_state=42, max_depth=10, n_estimators=400,\n                                      max_samples=0.15, max_leaf_nodes=5)\nestimator_rf.fit(x_train, y_train)\nrf_prediction = estimator_rf.predict(x_test)\nrf_pred_score = accuracy_score(y_test, rf_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Boosting - AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator_ab = AdaBoostClassifier(n_estimators=100, learning_rate=0.05)\nestimator_ab.fit(x_train, y_train)\nab_prediction = estimator_ab.predict(x_test)\nab_pred_score = accuracy_score(y_test, ab_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validation Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Random Forest Prediction:', rf_pred_score)\nprint('Ada Boost Prediction:', ab_pred_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the result, they both have the same results. The similarity is very precise. If we have to make a guess, this was caused by the exact same results that these two models made while predicting the validation data.\n\nThis means we can choose any model that we want to use disregarding the accuracy (because both of these two models have the same accuracy)."},{"metadata":{},"cell_type":"markdown","source":"# III. Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\nitems = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"the_id = test.ID","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.join(items.drop(['item_name'], axis=1).set_index('item_id'), on='item_id')\ntest['year'] = np.full(len(test), 2016)\ntest['month'] = np.full(len(test), 1)\n\nold_test = test\ntest = test[['item_id', 'shop_id', 'year', 'month', 'item_category_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = estimator_ab.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['ID'] = the_id  \nsubmission['item_cnt_month'] = prediction \n\nsubmission.to_csv('future_sales_pred.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}