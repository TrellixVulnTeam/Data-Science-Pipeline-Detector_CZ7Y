{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Future Sales Predictions\nlets first import necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport pickle\nimport re\nimport gc\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.preprocessing as prep \nimport sklearn.ensemble as ens\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import GridSearchCV\nfrom catboost import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we import the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"MyPath='../input/future-sales-prediction/'\nCompPath='../input/competitive-data-science-predict-future-sales/'\nTRPath='../input/translated/'\nRawCategories=pd.read_csv(CompPath+'item_categories.csv')\nRawItems=pd.read_csv(CompPath+'items.csv')\nRawSalesTrain=pd.read_csv(CompPath+'sales_train.csv')\nRawSalesTrain['Date']=pd.to_datetime(RawSalesTrain['date'],format='%d.%m.%Y')\nRawShops=pd.read_csv(TRPath+'shopsTR.csv')\nSalesTest1=pd.read_csv(CompPath+'test.csv')\nSalesTest=SalesTest1.set_index(['shop_id','item_id'])\nprint(\"Expected Number of Predictions:\"+str(SalesTest.size))\nShopIds=np.sort(SalesTest1.shop_id.unique())\nprint(\"Expected Shops for prediction(\"+str(len(ShopIds))+\" shops):\"+  str(ShopIds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.EDA and Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def LowerTypes(df,coltypes):\n    for col in coltypes.keys():\n        if col in df.columns:\n            df[col]=df[col].astype(coltypes[col])\n    return df\n\ndef delifexists(var):\n    if var in globals():\n        del var\n        \ndef delifexistslist(varlist):\n    for var in varlist:\n        if var in globals():\n            del var\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We are expected to predict for the below month"},{"metadata":{"trusted":true},"cell_type":"code","source":"M=RawSalesTrain.Date.max().month+1\nY=RawSalesTrain.Date.max().year\nTargetPeriod=str(Y)+str(M)\nTargetMonthId=RawSalesTrain.date_block_num.max()+1\nprint(\"Month \"+ str(M)+\" of \"+ str(Y)+\",Period:\"+TargetPeriod+\",date_block_num:\"+str(TargetMonthId))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets look at outliers and duplicates"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2)\n#plt.figure(figsize=(6,2))\nsns.boxplot(x=RawSalesTrain.item_cnt_day,ax=ax[0])\nsns.boxplot(x=RawSalesTrain.item_price,ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove outliers, returns (item_cnt_day<0) and date column"},{"metadata":{"trusted":true},"cell_type":"code","source":"SalesTrain=RawSalesTrain[(RawSalesTrain.item_cnt_day>0) & (RawSalesTrain.item_cnt_day<1000)& (RawSalesTrain.item_price>0) & (RawSalesTrain.item_price<100000)]\nSalesTrain=SalesTrain.drop(columns=['date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Shops"},{"metadata":{},"cell_type":"markdown","source":"Now we can seperate features from Shop Name field"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove ! character before we slice\nRawShops.shop_name_en=RawShops.shop_name_en.str.replace('!','').str.strip()\n#Get City out\nRawShops[['City','Type','NL']]=RawShops.shop_name_en.str.lower().str.split(' ',2,expand=True)\nRawShops.loc[RawShops.shop_id==9,'City']=''\nRawShops.loc[RawShops.shop_id==12,'City']=''\nRawShops.loc[RawShops.shop_id==55,'City']=''\nShopIdx=[0,6,10,11,22,57]\nRawShops.loc[RawShops.index.isin(ShopIdx),'Type']='shop'\nMallIdx=[2,4,13,29,30,32,39,40,46]\nRawShops.loc[RawShops.index.isin(MallIdx),'Type']='mall'\nOnlineIdx=[12,55]\nRawShops.loc[RawShops.index.isin(OnlineIdx),'Type']='online'\nRawShops.loc[RawShops.index.isin(OnlineIdx),'City']='online'\nRawShops.loc[RawShops.City=='rostovnadonu','City']='rostov-on-don'\nRawShops.loc[RawShops.City=='chekhov','City']='moscow'\nRawShops1=RawShops.loc[ RawShops.shop_id.isin(ShopIds),['shop_id','City','Type']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is important to note that although shop Id's 0,1,10 seem duplicates, they are stripped from predictions and can be safely ignored."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Populations taken from Google searches\nPopulationsK={\n            'adygea':440,\n            'balashikha':229,\n            'volzhsky':330,\n            'vologda':304,\n            'voronezh':998,\n            'zhukovsky':107,\n            'online':0,\n            'kazan':1170,\n            'kaluga':329,\n            'kolomna':145,\n            'krasnoyarsk':1007,\n            'kursk':426,\n            'moscow':1192,\n            'n.novgorod':1257,\n            'novosibirsk':1511,\n            'omsk':1159,\n            'rostov-on-don':1100,\n            'spb':4991,\n            'samara':1170,\n            'sergiev':109,\n            'surgut':321,\n            'tomsk':544,\n            'tyumen':622,\n            'ufa':1075,\n            'yakutsk':282,\n            'yaroslavl':597\n            }\n#Convert to Dataframe with index\nPop=pd.DataFrame({'City':list(PopulationsK.keys()),'Population':list(PopulationsK.values())}).set_index('City')\nPop.Population=(Pop.Population.astype('float32')/1000).astype(np.float32)\nShops1=RawShops1.set_index('City').join(Pop).reset_index().set_index('shop_id')\n#le1 = prep.LabelEncoder()\n#le2 = prep.LabelEncoder()\n#le1.fit(Shops1.City)\nShops1['CityEn']=prep.LabelEncoder().fit_transform(Shops1.City).astype(np.int8)\n#le2.fit(Shops1.Type)\nShops1['TypeEn']=prep.LabelEncoder().fit_transform(Shops1.Type).astype(np.int8)\nShops=Shops1.drop(columns=['City','Type'])\nShops.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets analyze test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets calculate number of items and shops\nNumber_Of_Shops=Shops.index.size\nNumber_Of_Items=RawItems.item_id.size\nTotal_Shop_Item_Combinations=Number_Of_Items*Number_Of_Shops\nNumber_Of_TestIDs=SalesTest.ID.size\nprint('Test IDs to submit='+str(Number_Of_TestIDs)+\",Combinations=\"+str(Total_Shop_Item_Combinations))\n# Not all shop/item combinations are requested!!!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Items"},{"metadata":{},"cell_type":"markdown","source":"Lets clean Item data (thanks Mykyta Minenko since I have no info on Russian and couldnt trasilate)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def name_correction(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9А-Яа-я]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x\n\nRawItems[\"name1\"], RawItems[\"name2\"] = RawItems.item_name.str.split(\"[\", 1).str\nRawItems[\"name1\"],RawItems[\"name3\"] = RawItems.item_name.str.split(\"(\", 1).str\n\nRawItems[\"name2\"] = RawItems.name2.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\nRawItems[\"name3\"] = RawItems.name3.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\nRawItems = RawItems.fillna(\"0\")\n\nRawItems[\"item_name\"] = RawItems[\"item_name\"].apply(lambda x: name_correction(x))\nRawItems.name2 = RawItems.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")\nRawItems[\"type\"] = RawItems.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\nRawItems.loc[(RawItems.type == \"x360\") | (RawItems.type == \"xbox360\") | (RawItems.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nRawItems.loc[ RawItems.type == \"\", \"type\"] = \"mac\"\nRawItems.type = RawItems.type.apply( lambda x: x.replace(\" \", \"\") )\nRawItems.loc[ (RawItems.type == 'pc' )| (RawItems.type == 'pс') | (RawItems.type == \"pc\"), \"type\" ] = \"pc\"\nRawItems.loc[ RawItems.type == 'рs3' , \"type\"] = \"ps3\"\nGroupedItems = RawItems.groupby([\"type\"]).agg({\"item_id\": \"count\"}).reset_index()\nto_drop = []\nfor cat in GroupedItems.type.unique():\n    if GroupedItems.loc[(GroupedItems.type == cat), \"item_id\"].values[0] <40:\n        to_drop.append(cat)\nRawItems.name2 = RawItems.name2.apply( lambda x: \"etc\" if (x in to_drop) else x )\nRawItems = RawItems.drop([\"type\"], axis = 1)\nRawItems.name2 = prep.LabelEncoder().fit_transform(RawItems.name2)\nRawItems.name3 = prep.LabelEncoder().fit_transform(RawItems.name3)\nItems=RawItems.drop(['item_name','name1'],axis=1)\ncoltypes={'item_id':np.int16,'item_category_id':np.int8,'name2':np.int8,'name3':np.int8}\nItems=LowerTypes(Items,coltypes)\nItems.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Item Category"},{"metadata":{"trusted":true},"cell_type":"code","source":"RawCategories['item_category_name_2']=RawCategories[\"item_category_name\"].apply(lambda x: x.split()[0])\nRawCategories[\"item_category_name_2\"]=prep.LabelEncoder().fit_transform(RawCategories[\"item_category_name_2\"]).astype(np.int8)\nCategories=RawCategories.drop(columns=['item_category_name'])\nCategories.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Agregate to months"},{"metadata":{},"cell_type":"markdown","source":"Lets group sales based on shop_id and item_id"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Group by month, shop_id, item_id\nSalesPerPeriodPerItem1=SalesTrain.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day':'sum','item_price':'mean'}).reset_index().rename(columns={'item_cnt_day':'item_cnt_month','item_price':'avg_shopitem_price'})\nShopItemCombinationsFor33=SalesPerPeriodPerItem1[SalesPerPeriodPerItem1.date_block_num==33].shape[0]\nprint('Train data for month 33 is '+str(ShopItemCombinationsFor33)+\", where for month 34 we predict for \"+str(Number_Of_TestIDs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to expand train data to cover all shop-item pairs and pad empty values for zeros"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nSalesTemp=SalesTest.drop(columns=['ID']).reset_index()\nSalesTable=pd.DataFrame({'date_block_num':[0],'shop_id':[0],'item_id':[0]})\nfor i in range(34):  \n    #print(i)\n    SalesTemp['date_block_num']=i\n    SalesTemp2=SalesTemp.set_index(['date_block_num','shop_id','item_id'])\n    SalesPPX=SalesPerPeriodPerItem1[SalesPerPeriodPerItem1.date_block_num==i].set_index(['date_block_num','shop_id','item_id'])\n    SalesTemp2=SalesTemp2.join(SalesPPX).reset_index().fillna(0)\n    SalesTable=SalesTable.append(SalesTemp2)\nSalesTable=SalesTable.iloc[1:]\nSalesTable.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Group by month, item_id\nSalesPerPeriodPerItem2=SalesTrain.groupby(['date_block_num','item_id']).agg({'item_cnt_day':'sum','item_price':'mean'}).reset_index().rename(columns={'item_cnt_day':'item_cnt_month','item_price':'avg_item_price'})\nSalesPerPeriodPerItem3=SalesTable.set_index(['date_block_num','item_id']).join(SalesPerPeriodPerItem2.set_index(['date_block_num','item_id']).drop(columns=['item_cnt_month'])).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate price difference for each item in each shop to mean item price\nSalesPerPeriodPerItem3['price_diff']=(SalesPerPeriodPerItem3['avg_shopitem_price']-SalesPerPeriodPerItem3['avg_item_price'])/SalesPerPeriodPerItem3['avg_item_price']\nSalesPerPeriodPerItem4=SalesPerPeriodPerItem3.drop(columns=['avg_shopitem_price','avg_item_price']).reset_index()\n\nSalesPerPeriodPerItem4.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets expand our train data to accomodate our test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_target1=SalesTest.reset_index().drop(columns=['ID'])\nX_target1['date_block_num']=34\nX_target1['item_cnt_month']=0\nX_target1['price_diff']=0\nSalesPerPeriodPerItem=SalesPerPeriodPerItem4.append(X_target1).fillna(0)\ncoltypes={'date_block_num':np.int8,'shop_id':np.int8,'item_id':np.int16,'price_diff':np.float32,'item_cnt_month':np.float32}\nSalesPerPeriodPerItem=LowerTypes(SalesPerPeriodPerItem,coltypes)\n#if (SalesPerPeriodPerItem[SalesPerPeriodPerItem.date_block_num==34].shape[0])>214200:\n#    print('!!!ERROR, row number larger than 214200')\ndelifexistslist(['SalesPerPeriodPerItem1','SalesPerPeriodPerItem2','SalesPerPeriodPerItem3','SalesPerPeriodPerItem4'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets find how most busy shops are doing"},{"metadata":{"trusted":true},"cell_type":"code","source":"MostBusyShopIds=np.argsort(SalesPerPeriodPerItem.groupby('shop_id').item_cnt_month.mean(),axis=None)[-3:]\nBusyShopActivity=SalesPerPeriodPerItem[SalesPerPeriodPerItem.shop_id.isin(MostBusyShopIds)]\nplt.figure(figsize=(14,4))\nsns.lineplot(data=BusyShopActivity,x='date_block_num',y='item_cnt_month',hue='shop_id')\n#There is a decreasing trend in sales of shops\n#There is a seasonality to the data with peaks happening on december (due to christmas).","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets do a seasonal decompose"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://stackoverflow.com/questions/45184055/how-to-plot-multiple-seasonal-decompose-plots-in-one-figure\ndef plotseasonal(res, axes, title):\n    res.observed.plot(ax=axes[0], legend=False)\n    axes[0].set_ylabel('Observed')\n    res.trend.plot(ax=axes[1], legend=False)\n    axes[1].set_ylabel('Trend')\n    res.seasonal.plot(ax=axes[2], legend=False)\n    axes[2].set_ylabel('Seasonal')\n    res.resid.plot(ax=axes[3], legend=False)\n    axes[3].set_ylabel('Residual')\n    axes[0].set_title(title)\n    \nfig, axes1 = plt.subplots(ncols=2, nrows=4, sharex=True, figsize=(12,5))    \nShopActivity19=BusyShopActivity.loc[BusyShopActivity.shop_id==19,['date_block_num','item_cnt_month']].set_index('date_block_num')\nShopActivity18=BusyShopActivity.loc[BusyShopActivity.shop_id==18,['date_block_num','item_cnt_month']].set_index('date_block_num')\nresult19 = seasonal_decompose(ShopActivity19, model='additive', period=12)\nresult18 = seasonal_decompose(ShopActivity18, model='additive', period=12)\nplotseasonal(result19,axes1[:,0], title = 'Sales decomposition for Store 19')\nplotseasonal(result18,axes1[:,1], title = 'Sales decomposition for Store 8')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.Feature Extraction"},{"metadata":{},"cell_type":"markdown","source":"We add City and Type features from shops"},{"metadata":{"trusted":true},"cell_type":"code","source":"SalesPerPeriodPerItem1=SalesPerPeriodPerItem.set_index('item_id').join(Items.set_index('item_id')).reset_index()\nSalesPerPeriodPerItem2=SalesPerPeriodPerItem1.set_index('item_category_id').join(Categories.set_index('item_category_id')).reset_index()\nEnrichedSales1=SalesPerPeriodPerItem2.set_index('shop_id').join(Shops).reset_index()\nEnrichedSales1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add month day, week day and weekend count\n\ndef last_day_of_month(date_block_num):\n    addyear=date_block_num//12\n    addmonth=date_block_num%12+1\n    date=dt.datetime(year=2013+addyear,month=addmonth,day=28)\n    next_month = date + dt.timedelta(days=4)\n    return (next_month - dt.timedelta(days=next_month.day)).day\n\ndef count_holidays(date_block_num):\n    m = 1 + date_block_num % 12\n    if m == 1:\n        return 1\n    elif m == 2:\n        return 1\n    elif m == 3:\n        return 1\n    elif m == 5:\n        return 2\n    elif m == 6:\n        return 1\n    elif m == 11:\n        return 1\n    elif m == 12:\n        return 2\n    else:\n        return 0\n    \ndef count_weekdays(date_block_num):\n    try:\n        y = 2013 + date_block_num // 12\n        m = 1 + date_block_num % 12\n        if m < 9:\n            return np.busday_count(f'{y}-0{m}', f'{y}-0{m+1}')\n        elif m ==9:\n            return np.busday_count(f'{y}-0{m}', f'{y}-10')\n        elif m != 12:\n             return np.busday_count(f'{y}-{m}', f'{y}-{m+1}')\n        else:\n            return np.busday_count(f'{y}-{m}', f'{y+1}-01')\n    except ValueError:\n        print(m,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nEnrichedSales1['days_in_month']=EnrichedSales1.date_block_num.apply(lambda x:last_day_of_month(x)).astype(np.int8)\nEnrichedSales1['holidays']=EnrichedSales1.date_block_num.apply(lambda x:count_holidays(x)).astype(np.int8)\nEnrichedSales1['weekdays']=EnrichedSales1.date_block_num.apply(lambda x:count_weekdays(x)).astype(np.int8)\nEnrichedSales1.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets look at moving averages to seperate trends and seasonalities"},{"metadata":{"trusted":true},"cell_type":"code","source":"ShopActivity19['zdata']=ShopActivity19.item_cnt_month-ShopActivity19.item_cnt_month.rolling(window=12).mean()/ShopActivity19.item_cnt_month.rolling(window=12).std()\nfig, ax = plt.subplots(2, figsize=(12,6))\nax[0].plot(ShopActivity19.index,ShopActivity19.item_cnt_month,label='Raw Data')\nax[0].plot(ShopActivity19.item_cnt_month.rolling(window=12).mean(),label='Rolling Mean')\nax[0].plot(ShopActivity19.item_cnt_month.rolling(window=12).std(),label='Rolling Std')\nax[0].legend()\nax[1].plot(ShopActivity19.index,ShopActivity19.zdata,label='Untrended Data')\nax[1].plot(ShopActivity19.zdata.rolling(window=12).mean(),label='Rolling Mean')\nax[1].plot(ShopActivity19.zdata.rolling(window=12).std(),label='Rolling Std')\nax[1].legend()\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, figsize=(12,6))\nax[0] = plot_acf(ShopActivity19.zdata.dropna(), ax=ax[0], lags=10)\nax[1] = plot_pacf(ShopActivity19.zdata.dropna(), ax=ax[1], lags=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add item introduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nFirstIntroShop=EnrichedSales1.groupby(['item_id','shop_id']).date_block_num.min().reset_index().rename(columns={'date_block_num':'item_first_intro_shop'})\nEnrichedSales1=EnrichedSales1.reset_index().set_index(['item_id','shop_id'])\nEnrichedSales2=EnrichedSales1.join(FirstIntroShop.set_index(['item_id','shop_id'])).reset_index()\nEnrichedSales2['item_first_intro_shop']=EnrichedSales2['date_block_num']-EnrichedSales2['item_first_intro_shop']\n\nFirstIntroCity=EnrichedSales2.groupby(['item_id','CityEn']).date_block_num.min().reset_index().rename(columns={'date_block_num':'item_first_intro_city'})\nEnrichedSales2=EnrichedSales2.reset_index().set_index(['item_id','CityEn'])\nEnrichedSales3=EnrichedSales2.join(FirstIntroCity.set_index(['item_id','CityEn'])).reset_index()\nEnrichedSales3['item_first_intro_city']=EnrichedSales3['date_block_num']-EnrichedSales3['item_first_intro_city']\n\nFirstIntro=EnrichedSales3.groupby(['item_id']).date_block_num.min().reset_index().rename(columns={'date_block_num':'item_first_intro'})\nEnrichedSales3=EnrichedSales3.set_index(['item_id'])\nEnrichedSales4=EnrichedSales3.join(FirstIntro.set_index(['item_id'])).reset_index()\nEnrichedSales4['item_first_intro']=EnrichedSales4['date_block_num']-EnrichedSales4['item_first_intro']\n\nEnrichedSales4['item_first_intro_shopCity']=EnrichedSales4['item_first_intro_shop']-EnrichedSales4['item_first_intro_city']\nEnrichedSales4['item_first_intro_shopGen']=EnrichedSales4['item_first_intro_shop']-EnrichedSales4['item_first_intro']\n\nEnrichedSales4.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean memory\nEnrichedSales1=EnrichedSales4.copy()\ndelifexistslist(['Enriched2','Enriched3','Enriched4','FirstIntroShop','FirstIntroCity','FirstIntro'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add lagged features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def laggedFeature(df, lags, col,ntype):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        NewcolumnName=col+'_lag_' + str(i)\n        shifted=shifted.rename(columns={col:NewcolumnName})\n        shifted['date_block_num'] += i\n        shifted=shifted.set_index(['date_block_num','shop_id','item_id'])\n        df=df.set_index(['date_block_num','shop_id','item_id']).join(shifted).reset_index()\n        df[NewcolumnName]=df[NewcolumnName].astype(ntype)\n    return df.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nEnrichedSales2=laggedFeature(EnrichedSales1,[1,2,3],'item_cnt_month',np.float32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check if it was populated for test part"},{"metadata":{"trusted":true},"cell_type":"code","source":"ESL1=EnrichedSales2[EnrichedSales2.date_block_num==34].set_index(['shop_id','item_id']).item_cnt_month_lag_1\nESL2=EnrichedSales2[EnrichedSales2.date_block_num==33].set_index(['shop_id','item_id']).item_cnt_month\ntestdf=pd.DataFrame(ESL1)\ntestdf=testdf.join(ESL2).fillna(0)\ntestdf['diff']=testdf.item_cnt_month_lag_1-testdf.item_cnt_month\nif testdf['diff'].unique().size==1:\n    print('Good')\nelse:\n    print('There must be an error')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets add the change of prices as a feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nEnrichedSales3=laggedFeature(EnrichedSales2,[1,2,3],'price_diff',np.float32).fillna(0).drop(columns=['price_diff'])\nEnrichedSales3.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add previous Target Encodings to EnrichedSales"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Item-item_cnt_month Encoding\nItemTargetEncoding=EnrichedSales3.groupby(['date_block_num','item_id'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_enc\"})\nEnrichedSales4=EnrichedSales3.set_index(['date_block_num','item_id']).join(ItemTargetEncoding.set_index(['date_block_num','item_id'])).reset_index()\n#Item-item+Shop_id_cnt_month Encoding\nShopItemTargetEncoding=EnrichedSales4.groupby(['date_block_num','shop_id','item_id'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"shopitem_target_enc\"})\nEnrichedSales5=EnrichedSales4.set_index(['date_block_num','shop_id','item_id']).join(ShopItemTargetEncoding.set_index(['date_block_num','shop_id','item_id'])).reset_index()\n#Item+item_category_id-item_cnt_month Encoding\nItemCategoryTargetEncoding=EnrichedSales5.groupby(['date_block_num','item_id','item_category_id'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_cat_enc\"})\nEnrichedSales6=EnrichedSales5.set_index(['date_block_num','item_id','item_category_id']).join(ItemCategoryTargetEncoding.set_index(['date_block_num','item_id','item_category_id'])).reset_index()\n\n#Item+item_category_name_2-item_cnt_month Encoding\nItemCategoryTargetEncoding=EnrichedSales6.groupby(['date_block_num','item_id','item_category_name_2'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_cat_enc2\"})\nEnrichedSales7=EnrichedSales6.set_index(['date_block_num','item_id','item_category_name_2']).join(ItemCategoryTargetEncoding.set_index(['date_block_num','item_id','item_category_name_2'])).reset_index()\n\n#Item+CityEn-item_cnt_month Encoding\nItemCategoryCityTargetEncoding=EnrichedSales7.groupby(['date_block_num','item_id','CityEn'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_city_enc\"})\nEnrichedSales8=EnrichedSales7.set_index(['date_block_num','item_id','CityEn']).join(ItemCategoryCityTargetEncoding.set_index(['date_block_num','item_id','CityEn'])).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean memory\ndelifexistslist(['ItemTargetEncoding','ShopItemTargetEncoding','ItemCategoryTargetEncoding','ItemCategoryTargetEncoding','ItemCategoryCityTargetEncoding','EnrichedSales3','EnrichedSales4','EnrichedSales5','EnrichedSales6','EnrichedSales7'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nEnrichedSales9=laggedFeature(EnrichedSales8,[1,2,3],'item_target_enc',np.float32)\nEnrichedSales10=laggedFeature(EnrichedSales9,[1,2,3],'shopitem_target_enc',np.float32)\nEnrichedSales11=laggedFeature(EnrichedSales10,[1,2,3],'item_target_cat_enc',np.float32)\nEnrichedSales12=laggedFeature(EnrichedSales11,[1,2,3],'item_target_cat_enc2',np.float32)\nEnrichedSales13=laggedFeature(EnrichedSales12,[1,2,3],'item_target_city_enc',np.float32)\n#*************************\n# It is extremely important to delete below columns after introducing lagged values \n# as they are a source of target leakage\n#*************************\nEnrichedSales13=EnrichedSales13.drop(columns=['item_target_enc','shopitem_target_enc','item_target_cat_enc','item_target_cat_enc2','item_target_city_enc'])\nEnrichedSales13=EnrichedSales13[EnrichedSales11['date_block_num']>2]\nEnrichedSales13.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert data and lower mem usage"},{"metadata":{"trusted":true},"cell_type":"code","source":"coltypes={'date_block_num':np.int8,'shop_id':np.int8,'item_id':np.int16,'CityEn':np.int8,'item_category_id':np.int8}\nEnrichedSales13=LowerTypes(EnrichedSales13,coltypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"save and clear data"},{"metadata":{"trusted":true},"cell_type":"code","source":"EnrichedSales=EnrichedSales13\npickle.dump(EnrichedSales, open(\"EnrichedSales.pkl\", \"wb\"))\ndelifexistslist(['RawShops','RawItems','Items','Categories']) \ndelifexistslist(['SalesTrain','SalesTable','SalesPerPeriodPerItem1','SalesPerPeriodPerItem2','SalesPerPeriodPerItem3','SalesPerPeriodPerItem'])\ndelifexistslist(['EnrichedSales1','EnrichedSales2','EnrichedSales3','EnrichedSales4','EnrichedSales5'])\ndelifexistslist(['EnrichedSales6','EnrichedSales7','EnrichedSales8','EnrichedSales9','EnrichedSales10'])\ndelifexistslist(['EnrichedSales11','EnrichedSales12'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets look at the correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"corrMatrix1=EnrichedSales.corr()\nsns.heatmap(corrMatrix1, annot=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.Create and Tune Model"},{"metadata":{},"cell_type":"markdown","source":"First lets group the sales data last available month (2015-09) as a baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"MyPath='../input/future-sales-prediction/'\nCompPath='../input/competitive-data-science-predict-future-sales/'\nTRPath='../input/translated/'\nModelData = pickle.load(open(MyPath+\"EnrichedSales.pkl\", \"rb\"))\nSalesTest1=pd.read_csv(CompPath+'test.csv')\nSalesTest=SalesTest1.set_index(['shop_id','item_id'])\nModelData.item_cnt_month[ModelData.item_cnt_month>20]=20\nModelData201509=ModelData[ModelData.date_block_num==32]\nModelData201509y=SalesTest.join(ModelData201509.set_index(['shop_id','item_id']).item_cnt_month).fillna(0).item_cnt_month.values\nModelData201510=ModelData[ModelData.date_block_num==33]\nModelData201510y=SalesTest.join(ModelData201510.set_index(['shop_id','item_id']).item_cnt_month).fillna(0).item_cnt_month.values\n#Lets check RSME value if 201509 is used as prediction for 201510\nrms = mean_squared_error(ModelData201510y, ModelData201509y, squared=False)\nprint('RMSE for 201509->201510 Regession:'+str(rms))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Train, Validation Set\nX_train=ModelData[ModelData.date_block_num<33].drop(columns=['item_cnt_month'])\ny_train=ModelData[(ModelData.date_block_num<33)]['item_cnt_month']\nX_valid=ModelData[ModelData.date_block_num==33].drop(columns=['item_cnt_month'])\ny_valid=ModelData[ModelData.date_block_num==33]['item_cnt_month'].values\nX_target=ModelData[ModelData.date_block_num==34].drop(columns=['item_cnt_month'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will train a linear regression model and test it on validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1=LinearRegression()\nmodel1.fit(X_train,y_train)\ny_pred1=model1.predict(X_valid)\ny_pred=np.rint(y_pred1)\ny_pred[y_pred>20]=20\nrms = mean_squared_error(y_valid, y_pred, squared=False)\nprint('RMSE for Linear Regession:'+str(rms))\nplt.style.use('seaborn-whitegrid')\nplt.plot(y_valid,'.',color='red')\nplt.plot(y_pred,'.',color='blue')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will train catboostregressor model and test it on validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncategorical=['date_block_num','shop_id','item_id','CityEn','item_category_id','name2','name3','TypeEn']\nmodel2 = CatBoostRegressor(\n            iterations=50,\n            learning_rate=1,\n            depth=2)\n\nmodel2.fit(\n    X_train, y_train,\n    cat_features=categorical,\n    eval_set=(X_valid, y_valid),\n    logging_level='Silent'\n)\nprint('Model is fitted: ' + str(model2.is_fitted()))\nmodel2.get_best_score()\ny_pred1=model2.predict(X_valid)\ny_pred=np.rint(y_pred1)\ny_pred[y_pred>20]=20\nrms = mean_squared_error(y_valid, y_pred, squared=False)\nprint('RMSE for Catboost:'+str(rms))\nplt.plot(y_valid,'.',color='red')\nplt.plot(y_pred,'.',color='blue')\nplt.show()\npickle.dump(model2, open(\"model2.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will train randomforest model and test it on validation set, please note that we used GridSearch for HYPERPARAMETER tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#params_rf = {\n#    'n_estimators': [20,30,40,50], \n#    'max_depth':[2,4,6], \n#    'max_features': ['auto', 'sqrt'], \n#}\n#n_estimators larger than 20 does not improve score considerably\n#max_depth is 6\n# Below paramaters results better score\n#RFR=ens.RandomForestRegressor()\n#model3 = GridSearchCV(RFR, params_rf, cv=None, refit=True,n_jobs=-1)\nmodel3 = ens.RandomForestRegressor(n_estimators=20,max_depth=6, random_state=0,n_jobs=-1)\nmodel3.fit(X_train, y_train)\n#print(model3.best_params_)\ny_pred1=model3.predict(X_valid)\ny_pred=np.rint(y_pred1)\ny_pred[y_pred>20]=20\nrms = mean_squared_error(y_valid, y_pred, squared=False)\nprint('RMSE for RandomForest Regession:'+str(rms))\nplt.plot(y_valid,'.',color='red')\nplt.plot(y_pred,'.',color='blue')\nplt.show()\npickle.dump(model3, open(\"model3.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets also try xgboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel4 = XGBRegressor(\n    max_depth=10,\n    booster='gbtree',\n    n_estimators=100,\n    min_child_weight=0.5, \n    subsample=0.8,\n    sampling_method=\"uniform\",\n    colsample_bynode=1,\n    colsample_bytree=0.8, \n    eta=0.1,\n    seed=0)\n\nmodel4.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_valid, y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ny_pred1=model4.predict(X_valid)\ny_pred=np.rint(y_pred1)\ny_pred[y_pred>20]=20\nrms = mean_squared_error(y_valid, y_pred, squared=False)\nprint('RMSE for RandomForest Regession:'+str(rms))\nplt.plot(y_valid,'.',color='red')\nplt.plot(y_pred,'.',color='blue')\nplt.show()\npickle.dump(model4, open(\"model4.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1,figsize=(15,20))\nplot_importance(booster=model4, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.Make Predictions and submit"},{"metadata":{},"cell_type":"markdown","source":"Use the most promising method, train it on whole data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_all=ModelData.drop(columns=['item_cnt_month'])\ny_all=ModelData['item_cnt_month'].values\nmodelFinal = ens.RandomForestRegressor(n_estimators=20,max_depth=6, random_state=0)\n\nmodelFinal=pickle.load(open(\"./model4.pkl\", \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict Final Model Output\nModelOutput1=modelFinal.predict(X_target)\nPredictions=np.rint(ModelOutput1)\n#Clip values between [0:20]\nPredictions[Predictions>20]=20\nPredictions[Predictions<0]=0\ndf=pd.DataFrame(Predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SalesTest1=pd.read_csv(CompPath+'test.csv')\nSalesTest=SalesTest1.set_index(['shop_id','item_id'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Organize output data for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"PredsDF=X_target[['shop_id','item_id']]\nPredsDF['item_cnt_month']=Predictions\nSubmission=SalesTest.join(PredsDF.set_index(['shop_id','item_id'])).reset_index().drop(columns=['shop_id','item_id'])\nplt.plot(Submission.item_cnt_month,'.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For submission, encapsulate data to create csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"Submission.to_csv('submission10.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}