{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nl = []\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        l.append(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Import data"},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\")\nshops_data = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\")\nsales_train = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv\")\nitem_category = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **2. Exploratory data analysis** "},{"metadata":{},"cell_type":"markdown","source":"If you translate parts of the shops names it looks like the characters before the \" are cities in russia with a set of letters at the end, which can be some type of each shop. In this case, I found 32 cities with shops and 10 types of shop.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cities = []\ntypes = []\n\nfor i in shops_data.shop_name.str.split('\"'):\n    j = i[0].split(' ')\n    cities.append(j[0])\n    types.append(j[1])\ncities = np.array(cities)\ntypes = np.array(types)\n\nnon = np.array(['', '(Плехановская,', 'Магазин', 'Орджоникидзе,', 'Посад'])\ntypes = np.where(np.isin(types,non), 'None', types)\nshops_data['city'] = cities\nshops_data['type_shop'] = types ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the item_category we found that the first word of each category is repeated several times. In this case, I found 15 unique values, which can be used to reduce the number of categories for each item"},{"metadata":{"trusted":true},"cell_type":"code","source":"first = []\nfor i in item_category.item_category_name.str.split(\" \"):\n    first.append(i[0])\nitem_category['new_category'] = first\nitems = items.merge(item_category, on='item_category_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Graphs of distribution between test and train data set (test, sales_train)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,5))\n\nplt.subplot(121)\nplt.scatter(sales_train.shop_id, sales_train.item_id)\nplt.xlabel('shop_id')\nplt.ylabel('item_id')\nplt.title('train_data')\n\n\nplt.subplot(122)\nplt.scatter(test.shop_id, test.item_id)\nplt.xlabel('shop_id')\nplt.ylabel('item_id')\nplt.title('test_data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,10))\n\nplt.subplot(221)\nplt.hist(sales_train.shop_id, bins=25)\nplt.xlabel('shop_id')\nplt.ylabel('counts')\nplt.title('train_data')\n\nplt.subplot(222)\nplt.hist(test.shop_id, bins=25)\nplt.ylabel('counts')\nplt.xlabel('shop_id')\nplt.title('test_data')\n\nplt.subplot(223)\nplt.hist(sales_train.item_id, bins=25)\nplt.xlabel('item_id')\nplt.ylabel('counts')\nplt.title('train_data')\n\nplt.subplot(224)\nplt.hist(test.item_id, bins=25)\nplt.ylabel('counts')\nplt.xlabel('item_id')\nplt.title('test_data')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see how many unique values both data set have"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The aoumnt of values for the test set is equal to all the possibl ecombinations between all the shops and all the items that this data set have. For this reason, I should take into account all the possible combinations for shop_id and item_id per month in the train data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train['date'] = pd.to_datetime(sales_train.date, format='%d.%m.%Y')\nsales_train['year'] = sales_train.date.dt.year\nsales_train['month'] = sales_train.date.dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = sales_train[sales_train.shop_id.isin(test.shop_id.unique())]\nsales_train = sales_train[sales_train.item_id.isin(test.item_id.unique())]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I used the new set of categories in order to make graphs easier to read"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = sales_train.merge(items[['item_id', 'item_category_id', 'new_category']], on='item_id', how='left')\nsales_train = sales_train.merge(shops_data[['city', 'type_shop', 'shop_id']], on='shop_id', how='left')\n\nsales_train.item_cnt_day = sales_train.item_cnt_day.fillna(0).clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nshops = sales_train.shop_id.unique()\nshops = np.sort(shops)\npos = 1\nplt.figure(figsize=(20,85))\n\nfor j in shops:\n    k = sales_train[sales_train.shop_id == j].groupby(['new_category', 'date_block_num'], as_index=False).sum()\n\n    for i in k.new_category.unique():\n        x = k.date_block_num[k.new_category == i]\n        y = k.item_cnt_day[k.new_category == i]    \n        plt.subplot(int(len(shops)/3), 3, pos)     \n        if j == 36:\n            plt.scatter(x,y, label=i)\n        else:\n            plt.plot(x,y, label=i)\n    \n    m = sales_train[sales_train.shop_id == j].groupby('date_block_num', as_index=False).sum()\n    y_t = m.item_cnt_day\n    x_t = m.date_block_num.unique()\n    if j == 36:\n        plt.scatter(x_t, y_t, label='total', color='black' )\n    else:\n        plt.plot(x_t, y_t, label='total', color='black' )\n    plt.xlabel('time')\n    plt.ylabel('quantity')\n    plt.xticks(np.arange(0, 33, 3))\n    plt.title(\"shop_id = \"+ str(j))\n    plt.legend()\n    pos += 1\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, there is a graph of sold items per month in each store. In each case, the months closer to the end of the year show an increase in sales. Also we can observe the openning of some stores and a peculiar case, in the trainnig data we only have data for the 33 month of the shop with Id=36. In addition, as it is expected, that the sales show a season dependance, maybe there are products that are more popular in specific parts of the year"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,110))\npos = 1\nfor i in np.sort(sales_train.item_category_id.unique()):\n    plt.subplot(20,3,pos)\n    x = sales_train.date_block_num[sales_train.item_category_id == i].unique()\n    x = np.sort(x)\n    y = sales_train[sales_train.item_category_id == i].groupby('date_block_num').sum().item_cnt_day\n    y /= np.linalg.norm(y)\n    plt.plot(x,y, label='cnt')\n    \n    y = sales_train[sales_train.item_category_id == i].groupby('date_block_num').mean().item_price\n    y_std = sales_train[sales_train.item_category_id == i].groupby('date_block_num').std().item_price.values\n    y_std /= np.linalg.norm(y)\n    y /= np.linalg.norm(y)\n    plt.plot(x,y, label='price')\n    plt.errorbar(x,y, yerr=y_std)\n    plt.xlabel('time')\n    plt.ylabel(' normalized quantity / average price')\n    plt.xticks(np.arange(0, 33, 3))\n    plt.title('item_category_id = ' + str(i))\n    plt.legend()\n    pos += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we evaluate the behaviour of the sales and the price for the 60 categories_id in the training data set. In this case, we observe the releasing of some new categories. In addition, sometimes the change of average prices occurs at the same month of change in sales. In the graph bellow, the error lines are equivalent to the standard deviation of the price for each category per month"},{"metadata":{},"cell_type":"markdown","source":"Now lets make a graph of the change in price and the sales for each category. In this case, the change and the price of each item could be usefull for the models. Because we observe a change in both feauteres at the same time of a change in the sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,110))\npos = 1\nfor i in np.sort(sales_train.item_category_id.unique()):\n    plt.subplot(20,3,pos)\n    x = sales_train.date_block_num[sales_train.item_category_id == i].unique()\n    x = np.sort(x)\n    y = sales_train[sales_train.item_category_id == i].groupby('date_block_num').sum().item_cnt_day\n    y /= np.linalg.norm(y)\n    plt.plot(x,y, label='cnt')\n    \n    y = sales_train[sales_train.item_category_id == i].groupby('date_block_num').mean().item_price\n    y = np.diff(y)\n    y /= np.linalg.norm(y)\n    plt.plot(x[:-1],y, label='price')\n    plt.xlabel('time')\n    plt.ylabel(' normalized quantity /  change on average price')\n    plt.xticks(np.arange(0, 33, 3))\n    plt.title('item_category_id = ' + str(i))\n    plt.legend()\n    pos += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets extract the year, month, day and day of the week for all the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train['day'] = sales_train.date.dt.day\nsales_train['day_week'] = sales_train.date.dt.dayofweek","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets graph the amount of sales for each month per year"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.figure(figsize=(20,110))\np = [1, 0, -1]\npos = 0\nfor i in np.sort(sales_train.year.unique()):\n    x = sales_train.month[sales_train.year == i].unique()\n    x = np.sort(x)\n    y = sales_train[sales_train.year == i].groupby('month').sum().item_cnt_day\n    #plt.barh(x,y, label='year '+ str(i), alpha=0.5)\n    \n    #x = np.arange(len(x))  # the label locations\n    width = 0.4  # the width of the bars\n\n        \n    plt.bar(x + p[pos] * width/2, y, width/2, label='year '+ str(i))\n    pos += 1\n    \n    plt.xlabel('month')\n    plt.ylabel('quantity')\n    plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets plot for each day of the month"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(35,20))\npos = 1\nfor j in np.sort(sales_train.year.unique()):\n    h = sales_train[sales_train.year == j]\n    plt.subplot(2,2,pos)\n    for i in np.sort(sales_train.month.unique()):\n        x = h.day[h.month == i].unique()\n        x = np.sort(x)\n        y = h[h.month == i].groupby('day').sum().item_cnt_day\n        plt.bar(x,y, label='month '+ str(i), alpha=0.5)\n        plt.xlabel('day')\n        plt.ylabel('quantity')\n        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.title('year ' + str(j))\n    pos += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets evaluate the sales per day of week for each month"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(26,22))\np = [1, 0, -1]\npos_ = 1\nlabels = [ 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday' ]\nfor i in np.sort(sales_train.month.unique())[:10]:\n    h = sales_train[sales_train.month == i]\n    plt.subplot(4, 3, pos_)\n    pos = 0\n    for j in np.sort(sales_train.year.unique()):\n        x = h.day_week[h.year == j].unique()\n        x = np.sort(x)\n        y = h[h.year == j].groupby('day_week').sum().item_cnt_day\n        \n        x = np.arange(len(labels))  # the label locations\n        width = 0.4  # the width of the bars\n\n        \n        plt.bar(x + p[pos] * width/2, y, width/2, label='year '+ str(j))\n        pos += 1\n        \n        #plt.barh(x,y, label='year '+ str(j), alpha=0.5)\n    plt.xlabel('day of the week')\n    plt.ylabel('quantity')\n    plt.legend()#bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.xticks(x, labels)\n    plt.title('month ' + str(i))\n    pos_ += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With all the graphs it looks for some months in the weekends are more sales in all the years of the training set. In addition, for the two first years the last days of the month showed an increase in the sales. Finally, at the end of each year the sales increase."},{"metadata":{},"cell_type":"markdown","source":"# 3. Featuring processing, train/validation split & model fitting"},{"metadata":{},"cell_type":"markdown","source":"In this case, I created a lot of lag features with the average of the price, the sales, the average of the sales per combination date/item_id and per combination date/shop_id. In addition, I built a set of features that  independt of time like the average sale per combination city/new_category, per combination city/category_id, per combination type_shop/new_category, per combination type_shop/category_id. Later, I used the last month of the given training set as the validation set and trained a XGBRegressor with early stopping. All this procedure can be seen in [Featuring processing and model fitting](https://www.kaggle.com/dagarzon1/features-and-model)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}