{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-28T13:04:00.853491Z","iopub.execute_input":"2022-01-28T13:04:00.853859Z","iopub.status.idle":"2022-01-28T13:04:00.869823Z","shell.execute_reply.started":"2022-01-28T13:04:00.853817Z","shell.execute_reply":"2022-01-28T13:04:00.869048Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nfrom tqdm import tqdm\nimport time\nimport pickle\nfrom itertools import product\nfrom lightgbm import LGBMRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import RepeatedKFold, cross_val_score, RandomizedSearchCV\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme()\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:04:00.871505Z","iopub.execute_input":"2022-01-28T13:04:00.872116Z","iopub.status.idle":"2022-01-28T13:04:00.885612Z","shell.execute_reply.started":"2022-01-28T13:04:00.872075Z","shell.execute_reply":"2022-01-28T13:04:00.884784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = r'/kaggle/input/competitive-data-science-predict-future-sales/'\n\nitems = pd.read_csv(path + '/items.csv')\nitem_categories = pd.read_csv(path + '/item_categories.csv')\nsales_train = pd.read_csv(path + '/sales_train.csv')\nshops = pd.read_csv(path + '/shops.csv')\n\ntest = pd.read_csv(path + '/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:04:00.886936Z","iopub.execute_input":"2022-01-28T13:04:00.887362Z","iopub.status.idle":"2022-01-28T13:04:03.736562Z","shell.execute_reply.started":"2022-01-28T13:04:00.887322Z","shell.execute_reply":"2022-01-28T13:04:03.735678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# item name cleaning\n\n# split item names by first bracket\nitems[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\nitems[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n# replace special characters and turn to lower case\nitems[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\nitems[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\n\n# fill nulls with '0'\nimport re\ndef name_correction(x):\n    x = x.lower() # all letters lower case\n    x = x.partition('[')[0] # partition by square brackets\n    x = x.partition('(')[0] # partition by curly brackets\n    x = re.sub('[^A-Za-z0-9А-Яа-я]+', ' ', x) # remove special characters\n    x = x.replace('  ', ' ') # replace double spaces with single spaces\n    x = x.strip() # remove leading and trailing white space\n    return x\n\nitems = items.fillna('0')\nitems[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n\n# return all characters except the last if name 2 is not \"0\" - the closing bracket\nitems.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")\n\n# clean item type\nitems[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\nitems.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nitems.loc[ items.type == \"\", \"type\"] = \"mac\"\nitems.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\nitems.loc[ (items.type == 'pc' )| (items.type == 'pс') | (items.type == \"pc\"), \"type\" ] = \"pc\"\nitems.loc[ items.type == 'рs3' , \"type\"] = \"ps3\"\n\ngroup_sum = items.groupby([\"type\"]).agg({\"item_id\": \"count\"})\ngroup_sum = group_sum.reset_index()\ndrop_cols = []\nfor cat in group_sum.type.unique():\n    if group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0] <40:\n        drop_cols.append(cat)\nitems.name2 = items.name2.apply( lambda x: \"other\" if (x in drop_cols) else x )\nitems = items.drop([\"type\"], axis = 1)\n\nitems.name2 = LabelEncoder().fit_transform(items.name2)\nitems.name3 = LabelEncoder().fit_transform(items.name3)\n\nitems.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)\nitems.head(6)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:04:03.739057Z","iopub.execute_input":"2022-01-28T13:04:03.739314Z","iopub.status.idle":"2022-01-28T13:04:04.568229Z","shell.execute_reply.started":"2022-01-28T13:04:03.739284Z","shell.execute_reply":"2022-01-28T13:04:04.567265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning item category \nitem_categories[\"category_type\"] = item_categories.item_category_name.apply(\n    lambda x: x.split(\" \")[0]).astype(str)\nitem_categories.loc[(item_categories.category_type == \"Игровые\") | (\n    item_categories.category_type == \"Аксессуары\"), \"category\"] = \"Игры\"\ncategory = []\nfor cat in item_categories.category_type.unique():\n    if len(item_categories[item_categories.category_type == cat]) >= 5:\n        category.append(cat)\nitem_categories.category_type = item_categories.category_type.apply(\n    lambda x: x if (x in category) else \"etc\")\n\n# Label Encoding\nitem_categories.category_type = LabelEncoder().fit_transform(item_categories.category_type)\nitem_categories[\"split\"] = item_categories.item_category_name.apply(lambda x: x.split(\"-\"))\nitem_categories[\"category_subtype\"] = item_categories.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitem_categories[\"category_subtype\"] = LabelEncoder().fit_transform( item_categories[\"category_subtype\"] )\nitem_categories = item_categories[[\"item_category_id\", \"category_subtype\", \"category_type\"]]\n\nitem_categories.head(6)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:04:04.569647Z","iopub.execute_input":"2022-01-28T13:04:04.569872Z","iopub.status.idle":"2022-01-28T13:04:04.603872Z","shell.execute_reply.started":"2022-01-28T13:04:04.569845Z","shell.execute_reply":"2022-01-28T13:04:04.602999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean up some shop names and add 'city' and 'category' to shops dataset\n# revise duplicated shop names in both shops and test datasets\n\n# Якутск Орджоникидзе, 56\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\nsales_train.loc[sales_train.shop_id == 0, 'shop_id'] = 57\n\n# Якутск ТЦ \"Центральный\"\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\nsales_train.loc[sales_train.shop_id == 1, 'shop_id'] = 58\n\n# Жуковский ул. Чкалова 39м²\ntest.loc[test.shop_id == 10, 'shop_id'] = 11\nsales_train.loc[sales_train.shop_id == 10, 'shop_id'] = 11\n\nshops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"',\n          \"shop_name\"] = 'СергиевПосад ТЦ \"7Я\"'\nshops[\"city\"] = shops.shop_name.str.split(\" \").map(lambda x: x[0])\nshops[\"category\"] = shops.shop_name.str.split(\" \").map(lambda x: x[1])\nshops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n\n# Only keep shop category if there are 5 or more shops of that category, the rest are grouped as \"other\".\ncategory = []\nfor cat in shops.category.unique():\n    if len(shops[shops.category == cat]) >= 5:\n        category.append(cat)\nshops.category = shops.category.apply(\n    lambda x: x if (x in category) else \"other\")\n\n# label encoding\nfrom sklearn.preprocessing import LabelEncoder\nshops[\"shop_category\"] = LabelEncoder().fit_transform( shops.category )\nshops[\"shop_city\"] = LabelEncoder().fit_transform( shops.city )\nshops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]\n\nshops.head(6)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:04:04.605489Z","iopub.execute_input":"2022-01-28T13:04:04.60573Z","iopub.status.idle":"2022-01-28T13:04:04.712408Z","shell.execute_reply.started":"2022-01-28T13:04:04.605702Z","shell.execute_reply":"2022-01-28T13:04:04.71152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add month feature\n\nsales_train['month'] = sales_train['date'].apply(lambda x: x.split('.')[1]).astype('int64')\nblock_month = sales_train[['date_block_num','month']].drop_duplicates().reset_index(drop=True)\nblock_month.loc[34] = [34,11]","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:04:04.715316Z","iopub.execute_input":"2022-01-28T13:04:04.715577Z","iopub.status.idle":"2022-01-28T13:04:06.78772Z","shell.execute_reply.started":"2022-01-28T13:04:04.71555Z","shell.execute_reply":"2022-01-28T13:04:06.786688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## check outliers, shop_id x item_cnt_day\n# check outliers, item_id x item_cnt_day\n# check outliers, item_price x item_cnt_day\n\nfig, ax = plt.subplots(figsize=(20, 14), nrows=2, ncols=2)\n\nax[0][0].scatter(data=sales_train,\n                 x='shop_id',\n                 y='item_cnt_day',)\nax[0][0].set_xlabel('shop_id')\nax[0][0].set_ylabel('item_cnt_day')\n\nax[0][1].scatter(data=sales_train,\n                 x='item_id',\n                 y='item_cnt_day',)\nax[0][1].set_xlabel('item_id')\nax[0][1].set_ylabel('item_cnt_day')\n\nax[1][0].scatter(data=sales_train,\n                 x='item_price',\n                 y='item_cnt_day',)\nax[1][0].set_xlabel('item_price')\nax[1][0].set_ylabel('item_cnt_day')","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:04:06.78895Z","iopub.execute_input":"2022-01-28T13:04:06.789212Z","iopub.status.idle":"2022-01-28T13:04:37.54069Z","shell.execute_reply.started":"2022-01-28T13:04:06.789182Z","shell.execute_reply":"2022-01-28T13:04:37.539707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check outliers, shop_id x item_cnt_day --> two outliers > 2000+, 1000+\n# check outliers, item_id x item_cnt_day --> two outliers > 1000+\n# check outliers, item_price x item_cnt_day --> two outliers > 1000, one item price > 300000, a few items price < 0\n\ndf_train = sales_train[(sales_train['item_cnt_day'] < 1000)\n                       & (sales_train['item_cnt_day'] > 0)]\n\ndf_train = sales_train[(sales_train['item_price'] < 150000)\n                       & (sales_train['item_price'] > 0)]","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:04:37.542108Z","iopub.execute_input":"2022-01-28T13:04:37.543174Z","iopub.status.idle":"2022-01-28T13:04:37.883274Z","shell.execute_reply.started":"2022-01-28T13:04:37.543112Z","shell.execute_reply":"2022-01-28T13:04:37.882079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate monthly sales\n\ndf_train = df_train.groupby(['date_block_num', 'shop_id', 'item_id'], as_index=False).agg(\n    {'item_cnt_day': 'sum'}).rename(columns={'item_cnt_day': 'item_cnt_month'}, inplace=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:04:37.887588Z","iopub.execute_input":"2022-01-28T13:04:37.887983Z","iopub.status.idle":"2022-01-28T13:04:38.921788Z","shell.execute_reply.started":"2022-01-28T13:04:37.887946Z","shell.execute_reply":"2022-01-28T13:04:38.920828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combine df_train and test together\n\ntest.drop(['ID'], axis=1, inplace=True)\ntest['date_block_num'] = 34\ntest['item_cnt_month'] = 0\n\ndf_train_test = pd.concat([df_train,test], axis=0)\ndf_train_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:04:38.923844Z","iopub.execute_input":"2022-01-28T13:04:38.924446Z","iopub.status.idle":"2022-01-28T13:04:38.96536Z","shell.execute_reply.started":"2022-01-28T13:04:38.924395Z","shell.execute_reply":"2022-01-28T13:04:38.964415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# product([num], tmp.shop_id.unique(), tmp.item_id.unique()\n# create maxtrix combinations of shop_id,and item_id for a certain month (date_block_num)\n# matrix factorization\n\nmatrix = []\nfor num in df_train_test['date_block_num'].unique(): \n    tmp = df_train_test[df_train_test.date_block_num==num]\n    matrix.append(np.array(list(product([num], tmp.shop_id.unique(), tmp.item_id.unique())), dtype='int16'))\n    \n# Turn the grid into a dataframe\nmatrix = pd.DataFrame(np.vstack(matrix), columns=['date_block_num', 'shop_id', 'item_id'], dtype=np.int16)\n\n# Add the features from sales data to the matrix\nmatrix = matrix.merge(df_train_test, how='left', on=['date_block_num', 'shop_id', 'item_id']).fillna(0)\n\n#Merge features from shops, items and item_categories:\nmatrix = matrix.merge(shops, how='left', on='shop_id')\nmatrix = matrix.merge(items[['item_id','item_category_id']], how='left', on='item_id')\nmatrix = matrix.merge(item_categories, how='left', on='item_category_id')\n\n# revise month feature\nmatrix['month'] = matrix.date_block_num%12\n# Clip counts\nmatrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0, 20)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:04:38.967109Z","iopub.execute_input":"2022-01-28T13:04:38.96768Z","iopub.status.idle":"2022-01-28T13:05:05.021013Z","shell.execute_reply.started":"2022-01-28T13:04:38.967633Z","shell.execute_reply":"2022-01-28T13:05:05.019637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"# a back-up save\n\nmatrix.to_csv('df_matrix_final.csv', index=False)\ndf_train_test = pd.read_csv('df_matrix_final.csv')","metadata":{}},{"cell_type":"code","source":"# reduce size of each variables to avoid memory error\n\ndf_train_test = matrix.copy()\n\n\ndel items\ndel item_categories\ndel shops\ndel sales_train\ndel matrix\n\ndf_train_test['date_block_num'] = df_train_test['date_block_num'].astype(np.int8)\ndf_train_test['shop_id'] = df_train_test['shop_id'].astype(np.int8)\ndf_train_test['item_id'] = df_train_test['item_id'].astype(np.int16)\ndf_train_test['month'] = df_train_test['month'].astype(np.int8)\ndf_train_test['item_cnt_month'] = df_train_test['item_cnt_month'].astype(np.int32)\ndf_train_test['shop_category'] = df_train_test['shop_category'].astype(np.int8)\ndf_train_test['shop_city'] = df_train_test['shop_city'].astype(np.int8)\ndf_train_test['item_category_id'] = df_train_test['item_category_id'].astype(np.int8)\ndf_train_test['category_type'] = df_train_test['category_type'].astype(np.int8)\ndf_train_test['category_subtype'] = df_train_test['category_subtype'].astype(np.int8)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:05:05.02263Z","iopub.execute_input":"2022-01-28T13:05:05.022881Z","iopub.status.idle":"2022-01-28T13:05:06.645085Z","shell.execute_reply.started":"2022-01-28T13:05:05.022853Z","shell.execute_reply":"2022-01-28T13:05:06.644021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('{0:.2f}'.format(df_train_test.memory_usage(index=False, deep=True).sum()/(2**20)), 'MB')","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:05:06.646608Z","iopub.execute_input":"2022-01-28T13:05:06.646958Z","iopub.status.idle":"2022-01-28T13:05:06.654992Z","shell.execute_reply.started":"2022-01-28T13:05:06.646913Z","shell.execute_reply":"2022-01-28T13:05:06.653726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prep lagged features\n\n# shop_category X date_block_num X item_cnt_month\n\nshop_category_block_cnt = df_train_test.groupby(['date_block_num', 'shop_category'], as_index=False).sum(\n).rename(columns={'item_cnt_month': 'shop_category_block_cnt'})[['shop_category', 'date_block_num', 'shop_category_block_cnt']]\n\nlag_period = [1, 2, 3, 12] #memory not enough on kaggle\n#lag_period = [1, 2, 3, 4, 5, 12]\n\n# add lag features: shop_block_cnt_lag_\n\nshop_category_block_cnt_lag = shop_category_block_cnt.copy()\n\nfor lag in lag_period:\n\n    temp = shop_category_block_cnt.copy()\n\n    lag_fet_name = 'shop_category_block_cnt_lag_' + str(lag)\n    temp['date_block_num'] += lag\n    temp[lag_fet_name] = temp['shop_category_block_cnt']\n    temp.drop('shop_category_block_cnt', axis=1, inplace=True)\n    shop_category_block_cnt_lag = shop_category_block_cnt_lag.merge(\n        temp[['shop_category', 'date_block_num', lag_fet_name]], on=['shop_category', 'date_block_num'], how='left')\n\nshop_category_block_cnt_lag.drop(\n    'shop_category_block_cnt', axis=1, inplace=True)\n\n# shop_id X date_block_num X item_cnt_month\n\nshop_block_cnt = df_train_test.groupby(['date_block_num', 'shop_id'], as_index=False).sum(\n).rename(columns={'item_cnt_month': 'shop_block_cnt'})[['shop_id', 'date_block_num', 'shop_block_cnt']]\n\n# add lag features: shop_block_cnt_lag_\n\nshop_block_cnt_lag = shop_block_cnt.copy()\n\nfor lag in lag_period:\n\n    temp = shop_block_cnt.copy()\n\n    lag_fet_name = 'shop_block_cnt_lag_' + str(lag)\n    temp['date_block_num'] += lag\n    temp[lag_fet_name] = temp['shop_block_cnt']\n    temp.drop('shop_block_cnt', axis=1, inplace=True)\n    shop_block_cnt_lag = shop_block_cnt_lag.merge(\n        temp[['shop_id', 'date_block_num', lag_fet_name]], on=['shop_id', 'date_block_num'], how='left')\n\nshop_block_cnt_lag.drop('shop_block_cnt', axis=1, inplace=True)\n\n# item_id X date_block_num X item_cnt_month\n\nitem_block_cnt = df_train_test.groupby(['date_block_num', 'item_id'], as_index=False).sum(\n).rename(columns={'item_cnt_month': 'item_block_cnt'})[['item_id', 'date_block_num', 'item_block_cnt']]\n\n# add lag features: item_block_cnt_lag_\n\nitem_block_cnt_lag = item_block_cnt.copy()\n\nfor lag in lag_period:\n\n    temp = item_block_cnt.copy()\n\n    lag_fet_name = 'item_block_cnt_lag_' + str(lag)\n    temp['date_block_num'] += lag\n    temp[lag_fet_name] = temp['item_block_cnt']\n    temp.drop('item_block_cnt', axis=1, inplace=True)\n    item_block_cnt_lag = item_block_cnt_lag.merge(\n        temp[['item_id', 'date_block_num', lag_fet_name]], on=['item_id', 'date_block_num'], how='left')\n\nitem_block_cnt_lag.drop('item_block_cnt', axis=1, inplace=True)\n\n# category_type X date_block_num X item_cnt_month\n\ncategory_type_block_cnt = df_train_test.groupby(['date_block_num', 'category_type'], as_index=False).sum(\n).rename(columns={'item_cnt_month': 'category_type_block_cnt'})[['category_type', 'date_block_num', 'category_type_block_cnt']]\n\n# add lag features: category_type_block_cnt_lag_\n\ncategory_type_block_cnt_lag = category_type_block_cnt.copy()\n\nfor lag in lag_period:\n\n    temp = category_type_block_cnt.copy()\n\n    lag_fet_name = 'category_type_block_cnt_lag_' + str(lag)\n    temp['date_block_num'] += lag\n    temp[lag_fet_name] = temp['category_type_block_cnt']\n    temp.drop('category_type_block_cnt', axis=1, inplace=True)\n    category_type_block_cnt_lag = category_type_block_cnt_lag.merge(\n        temp[['category_type', 'date_block_num', lag_fet_name]], on=['category_type', 'date_block_num'], how='left')\n\ncategory_type_block_cnt_lag.drop(\n    'category_type_block_cnt', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:05:06.656442Z","iopub.execute_input":"2022-01-28T13:05:06.657199Z","iopub.status.idle":"2022-01-28T13:05:14.881363Z","shell.execute_reply.started":"2022-01-28T13:05:06.657157Z","shell.execute_reply":"2022-01-28T13:05:14.880475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge three lag dfs into df_train_test\n\ndf_merged = df_train_test.copy()\n\ndf_merged = df_merged.merge(shop_category_block_cnt_lag, on=[\n                            'shop_category', 'date_block_num'], how='left')\ndel shop_category_block_cnt_lag\n\ndf_merged = df_merged.merge(shop_block_cnt_lag, on=[\n                            'shop_id', 'date_block_num'], how='left')\ndel shop_block_cnt_lag\n\ndf_merged = df_merged.merge(item_block_cnt_lag, on=[\n                            'item_id', 'date_block_num'], how='left')\ndel item_block_cnt_lag\n\ndf_merged = df_merged.merge(category_type_block_cnt_lag, on=[\n                            'category_type', 'date_block_num'], how='left')\ndel category_type_block_cnt_lag\n\n# df_merged = df_merged.merge(shop_item_block_cnt_lag, on=[\n#                            'shop_id', 'item_id', 'date_block_num'], how='left')\n\ndf_merged.fillna(0, inplace=True)\ndf_merged['item_cnt_month'] = df_merged['item_cnt_month'].clip(0, 20)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:05:14.8833Z","iopub.execute_input":"2022-01-28T13:05:14.883667Z","iopub.status.idle":"2022-01-28T13:05:26.436221Z","shell.execute_reply.started":"2022-01-28T13:05:14.883619Z","shell.execute_reply":"2022-01-28T13:05:26.435298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mean encode labels\n# take the mean of encoded columns X item_cnt_month\n\nenc_cols = ['item_category_id', 'category_type', 'category_subtype',\n            'shop_category', 'shop_city']\n\nfor col in enc_cols:\n    gb = pd.DataFrame(df_merged[df_merged['date_block_num'] < 34].groupby(    # not use test data to avoid leakage\n        col).mean().reset_index())\n\n    col_mean_enc = gb[['item_cnt_month']]\n\n    minmaxscaler = MinMaxScaler()\n    #stdscaler = StandardScaler()\n\n    col_mean_enc = minmaxscaler.fit_transform(col_mean_enc)\n    #col_mean_enc = stdscaler.fit_transform(col_mean_enc)\n\n    # create a map dict and do the mapping\n    enc_map = {ind: enc[0]\n               for ind, enc in enumerate(col_mean_enc)}\n    df_merged[col] = df_merged[col].map(enc_map)\n    \ndel enc_map","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:05:26.437873Z","iopub.execute_input":"2022-01-28T13:05:26.438387Z","iopub.status.idle":"2022-01-28T13:05:54.147938Z","shell.execute_reply.started":"2022-01-28T13:05:26.43835Z","shell.execute_reply":"2022-01-28T13:05:54.147218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add Item name (Tfidf text feature) --> give it a shot　 --> imrpoved\n\nitems_tfidf = pd.read_csv(path +'/items.csv')\n\nitems_subset = items_tfidf[['item_id', 'item_name']]\nfeature_count = 25\ntfidf = TfidfVectorizer(max_features=feature_count)\nitems_df_item_name_text_features = pd.DataFrame(\n    tfidf.fit_transform(items_subset['item_name']).toarray())\n\ncols = items_df_item_name_text_features.columns\nfor i in range(feature_count):\n    feature_name = 'item_name_tfidf_' + str(i)\n    items_subset[feature_name] = items_df_item_name_text_features[cols[i]]\n    \nitems_subset.drop('item_name', axis=1, inplace=True)\ndf_merged = df_merged.merge(items_subset, on='item_id', how='left')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:05:54.149498Z","iopub.execute_input":"2022-01-28T13:05:54.150224Z","iopub.status.idle":"2022-01-28T13:06:01.77423Z","shell.execute_reply.started":"2022-01-28T13:05:54.150176Z","shell.execute_reply":"2022-01-28T13:06:01.773319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del items_tfidf, tfidf,items_df_item_name_text_features,items_subset\ndel df_train, df_train_test\ndel gb,temp,block_month","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:06:01.775451Z","iopub.execute_input":"2022-01-28T13:06:01.775722Z","iopub.status.idle":"2022-01-28T13:06:01.786527Z","shell.execute_reply.started":"2022-01-28T13:06:01.775692Z","shell.execute_reply":"2022-01-28T13:06:01.785605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove 2013 data: previous 12 months\n\ndf_merged = df_merged[df_merged['date_block_num'] >= 12]\ndf_merged.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:06:01.787848Z","iopub.execute_input":"2022-01-28T13:06:01.78884Z","iopub.status.idle":"2022-01-28T13:06:08.910405Z","shell.execute_reply.started":"2022-01-28T13:06:01.788786Z","shell.execute_reply":"2022-01-28T13:06:08.909394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# independent and target variables \n\nX_train = df_merged[(df_merged['date_block_num'] >= 12) & (\n    df_merged['date_block_num'] < 34)].drop(['date_block_num', 'item_cnt_month'], axis=1)\ny_train = df_merged[(df_merged['date_block_num'] >= 12) & (\n    df_merged['date_block_num'] < 34)]['item_cnt_month']\n\nX_test = df_merged[df_merged['date_block_num']\n                   == 34].drop(['date_block_num', 'item_cnt_month'], axis=1)\n\nprint(X_train.shape)\nprint(y_train.shape)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:06:08.912039Z","iopub.execute_input":"2022-01-28T13:06:08.912281Z","iopub.status.idle":"2022-01-28T13:06:13.32045Z","shell.execute_reply.started":"2022-01-28T13:06:08.912253Z","shell.execute_reply":"2022-01-28T13:06:13.319547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sub = pd.read_csv(path + '/test.csv')\ntest_sub.drop(['shop_id', 'item_id'], axis=1, inplace=True)\n\n# memory not enough\ndel df_merged","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:06:13.321818Z","iopub.execute_input":"2022-01-28T13:06:13.322052Z","iopub.status.idle":"2022-01-28T13:06:13.403051Z","shell.execute_reply.started":"2022-01-28T13:06:13.322024Z","shell.execute_reply":"2022-01-28T13:06:13.402076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plain lgbm model\n\n#lgb_cv = RepeatedKFold(n_repeats=3, n_splits=3)\n\nmodels_candidates = [\n    LGBMRegressor(seed=55, silent=True, metric='rmse'),\n] # lgbm competes over xgb and cat\n\nfor model_reg in tqdm(models_candidates):\n\n    model = model_reg\n\n    start = time.time()\n    scores = cross_val_score(\n        model, X_train, y_train,\n                             scoring='neg_root_mean_squared_error', cv=2, \n                             #n_jobs=-1\n                            ) #cv=2 due to insufficient memory\n    end = time.time()\n\n    print('model type: %s' % str(model_reg)[:3])\n    print('training time: %.2f s' % (end - start))\n    print('mean score: %.3f' % scores.mean())\n    print('score std: %.3f\\n' % scores.std())\n\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    # Clip all your monthly count of training and testing dataset between [0,20].\n    pred = np.clip(pred, 0, 20)\n\n    test_sub['item_cnt_month'] = pd.Series(pred)\n\n    if str(model_reg)[:3] == '<ca':\n        filename = 'submission.csv'\n        model_name = str(model_reg)[1:4] + '_final.sav'\n    else:\n        filename = 'submission.csv'\n        model_name = str(model_reg)[:3] + '_final.sav'\n    test_sub.to_csv(filename, index=False)\n\n    # save models for later use\n    # pickle.dump(model, open(model_name, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:07:07.749191Z","iopub.execute_input":"2022-01-28T13:07:07.749843Z","iopub.status.idle":"2022-01-28T13:08:44.983116Z","shell.execute_reply.started":"2022-01-28T13:07:07.749796Z","shell.execute_reply":"2022-01-28T13:08:44.982011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}