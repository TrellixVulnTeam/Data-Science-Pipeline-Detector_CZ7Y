{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id = \"table_of_contents\"></a>\n# Table of contents\n\n[Import of libraries](#imports)\n\n[Global variables](#global_variables)\n\n[Preprocessing before features generation](#preprocessing_before_fe)\n\n-->[Correct the shop names and id](#correct_shop_names_id)\n\n-->[Generate item_category_features](#generate_item_category_features)\n\n-->[Remove the huge price and item sales outliers](#remove_outliers)\n\n[Generate a full df with all data and records](#generate_full_df_with_all_records)\n\n[Create a groupby df with all the sales for shop_id and item_id grouped by months](#generate_gb_df)\n\n[Join the full_df with gb_df](#join_dfs)\n\n[Add additional features to our full sales df](#add_new_csvs)\n\n[FeatureGenerator class](#fe_generator_class)\n\n[Generate additional features as, mean and total sales for shop_id , item_id, city ... for every month](#create_new_features)\n\n-->[Date and shop_id features](#feature_1)\n\n-->[Date and item_id features](#feature_2)\n\n-->[Date and item_category features](#feature_3)\n\n-->[Datetime features](#feature_5)\n\n-->[Adding holiday and number of weekends data](#feature_6)\n\n-->[City population and mean_income per city](#feature_7)\n\n[Join full sales df with all the features generated](#join_dfs_with_features)\n\n[Basic model train](#basic_model)\n\n[Feature importance](#feature_importance_1)\n\n[Predict and model evaluation](#predict_and_model_evaluation_1)\n\n[To do](#to_do)\n\n-->[Additional feature 1](#new_feature_1)\n\n-->[Additional feature 2](#new_feature_2)\n\n-->[Additional feature 3](#new_feature_3)\n\n-->[Join df's with new features](#join_dfs_with_new_features)\n\n-->[Model training](#new_model)\n\n-->[Feature importance of new model](#feature_importance_2)\n\n-->[Predict and model evaluation of new model](#predict_and_model_evaluation_2)"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"imports\"></a>\n# Import of libraries\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the basic libraries we will use in this kernel\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nimport time\nimport datetime\nfrom datetime import datetime\nimport calendar\n\nfrom sklearn import metrics\nfrom math import sqrt\nimport gc\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport itertools\nimport warnings\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nwarnings.filterwarnings(\"ignore\") # specify to ignore warning messages","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"global_variables\"></a>\n# Global variables\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Resample the sales by this parameter\nPERIOD = \"M\"\n\nSHOPS = [8, 14, 37, 41, 59]\n\n# this is help us change faster between Kaggle and local machine\nLOCAL = False\n\nif LOCAL:\n    PATH = os.getcwd()\n    FULL_DF_PATH = PATH\n    GB_DF_PATH = PATH\n    OUTPUT_PATH = PATH\nelse:\n    PATH = '../input/competitive-data-science-predict-future-sales/'\n    FULL_DF_PATH = \"../input/full-df-only-test-all-features/\"\n    GB_DF_PATH = \"../input/group-by-df/\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"preprocessing_before_fe\"></a>\n# Preprocessing before features generation\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{},"cell_type":"markdown","source":"The idea of this section is very simple. We have seen in our EDA part that there are a lot of missing values.\nOur model will benefit a lot if we can supply it a training data, with the missing values being zero. This way, it can learn from more amount of data.\n\nIn order to do so, we must perform a cartesian operation over dates x shops x items_id to generate all the possible combinations of months x shops and x items sales.\n\nIn this kernel we will only generate this type of features for the items that are present in TEST only.\n\nThis will reduce the amount of calculations required. If you have enough memory, we can do this for all possible combinations."},{"metadata":{"trusted":true},"cell_type":"code","source":"# load all the df we have\nshops_df = pd.read_csv(os.path.join(PATH, \"shops.csv\"))\nitems_df = pd.read_csv(os.path.join(PATH, \"items.csv\"))\nitems_category_df = pd.read_csv(os.path.join(PATH, \"item_categories.csv\"))\nsales_df = pd.read_csv(os.path.join(PATH, \"sales_train.csv\"))\ntest_df = pd.read_csv(os.path.join(PATH, \"test.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_category_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"correct_shop_names_id\"></a>\n## Correct the shop names and id\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we have seen in our EDA that we have some duplicate shops, let's correct them.\nshops_df.loc[shops_df.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops_df['city'] = shops_df['shop_name'].str.split(' ').map(lambda x: x[0])\nshops_df.loc[shops_df.city == '!Якутск', 'city'] = 'Якутск'\nshops_df['city_code'] = LabelEncoder().fit_transform(shops_df['city'])\nshops_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, we have some duplicate shop names, let's manually clean them."},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_df[shops_df[\"shop_id\"].isin([0, 57])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Якутск Орджоникидзе, 56\nsales_df.loc[sales_df.shop_id == 0, 'shop_id'] = 57\ntest_df.loc[test_df.shop_id == 0, 'shop_id'] = 57\n\n# Якутск ТЦ \"Центральный\"\nsales_df.loc[sales_df.shop_id == 1, 'shop_id'] = 58\ntest_df.loc[test_df.shop_id == 1, 'shop_id'] = 58\n\n# Жуковский ул. Чкалова 39м²\nsales_df.loc[sales_df.shop_id == 10, 'shop_id'] = 11\ntest_df.loc[test_df.shop_id == 10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"generate_item_category_features\"></a>\n## Generate item_category_features\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"items_category_df['split'] = items_category_df['item_category_name'].str.split('-')\nitems_category_df['type'] = items_category_df['split'].map(lambda x: x[0].strip())\nitems_category_df['type_code'] = LabelEncoder().fit_transform(items_category_df['type'])\n\n# if subtype is nan then type\nitems_category_df['subtype'] = items_category_df['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitems_category_df['subtype_code'] = LabelEncoder().fit_transform(items_category_df['subtype'])\n\nitems_category_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"remove_outliers\"></a>\n## Remove the huge price and item sales outliers\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we have negative prices and some outlier\n# let's replace the data with the mean value and also filter all the outliers\nmean = sales_df[(sales_df[\"shop_id\"] == 32) & (sales_df[\"item_id\"] == 2973) & (sales_df[\"date_block_num\"] == 4) & (sales_df[\"item_price\"] > 0)][\"item_price\"].mean()\nsales_df.loc[sales_df.item_price < 0, 'item_price'] = mean\n\nsales_df = sales_df[sales_df[\"item_price\"] < np.percentile(sales_df[\"item_price\"], q = 100)]\nsales_df = sales_df[sales_df[\"item_cnt_day\"] < np.percentile(sales_df[\"item_cnt_day\"], q = 100)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"generate_full_df_with_all_records\"></a>\n# Generate a full df with all data and records\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(sales_df[\"date\"].iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to datetime the date column\n# specify the format since otherwise it might give some problems\nsales_df[\"date\"] = pd.to_datetime(sales_df[\"date\"], format = \"%d.%m.%Y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# max date in sales is 31.10.2015.\n# In the Kaggle competition we are asked to predict the sales for the next month\n# this means the sales of November\nmin_date = sales_df[\"date\"].min()\nmax_date_sales = sales_df[\"date\"].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_date_sales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how to createa a new date\nmax_date_test = datetime(2015, 11, 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a date range that beggins with the first sale and ends with the last day from our max_date_test\n# Notice however, that we will train our model only a selection of shops and will test our data on october data.\ndate_range = pd.date_range(min_date, max_date_sales, freq = \"D\")\ndate_range","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(date_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model will benefit a lot if we can train it with the highest granularity (daily sales).\n\nHowever, as we can see doing this on a local machine is almost impossible since we have more than 1.4 BILLION rows.\nIf we add 10 featrues (columns) this means that our total DataFrame will have more than 10.4 BILLIONS instances."},{"metadata":{"trusted":true},"cell_type":"code","source":"shops = sorted(list(shops_df[\"shop_id\"].unique()))\n\n# only items present in test\nitems = sorted(list(items_df[\"item_id\"].unique()))\n\ncartesian_product = pd.MultiIndex.from_product([date_range, shops, items], names = [\"date\", \"shop_id\", \"item_id\"])\nlen(cartesian_product)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to replicate the Kaggle competition, we will create a smaller DataFrame with only selected shops and train the model on a Monthly basis.\n\n\nWe will use only 5 shops since generating a lot of features will consume a lot of memory and we won't be able to train on Kaggle. If you have a more powerful machine, you can run the script with all shops."},{"metadata":{"trusted":true},"cell_type":"code","source":"date_range = pd.date_range(min_date, max_date_sales, freq = PERIOD)\nprint(\"We have a total of {} months\".format(len(date_range)))\ndate_range","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.87 million rows, we CAN work with this on a local machine.\n\nWe have created monthly date_range, if we want to join this with our sales data, we must \"resample\" our data to a monthly date_range aswell."},{"metadata":{"trusted":true},"cell_type":"code","source":"# only items present in test\nitems = sorted(list(test_df[\"item_id\"].unique()))\n\ncartesian_product = pd.MultiIndex.from_product([date_range, SHOPS, items], names = [\"date\", \"shop_id\", \"item_id\"])\nlen(cartesian_product)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"generate_gb_df\"></a>\n# Create a groupby df with all the sales for shop_id and item_id grouped by months\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{},"cell_type":"markdown","source":"We will be working with a DataFrame resampled by Months. We must resample the sales_df."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nst = time.time()\n\n# # set index\nsales_df[\"revenue\"] = sales_df[\"item_cnt_day\"]*sales_df[\"item_price\"]\ngb_df = sales_df.set_index(\"date\")\n\n# # groupby shop_id and item_id\ngb_df = gb_df.groupby([\"shop_id\", \"item_id\"])\n\n# # resample the sales to a weekly basis\ngb_df = gb_df.resample(PERIOD).agg({'item_cnt_day': np.sum, \"item_price\": np.mean, \"revenue\":np.sum})\n\n# # convert to dataframe and save the full dataframe\ngb_df.reset_index(inplace = True)\n\n# # save the groupby dataframe\ngb_df.to_pickle(\"GROUP_BY_DF.pkl\")\n\net = time.time()\n\nprint(\"Total time in minutes to preprocess took {}\".format((et - st)/60))\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the groupby dataframe\ngb_df = pd.read_pickle(os.path.join(GB_DF_PATH, \"GROUP_BY_DF.pkl\"))\n# gb_df = pd.read_pickle(\"GROUP_BY_DF.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"gb_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_df.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"join_dfs\"></a>\n# Join the full_df with gb_df\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{},"cell_type":"markdown","source":"Now that we have the sales_df resampled by months, and we have created a cartesian product (all possible combinations of months, shop_id and item_id), let's merge the df."},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df = pd.DataFrame(index = cartesian_product).reset_index()\n\nfull_df = pd.merge(full_df, gb_df, on = ['date','shop_id', \"item_id\"], how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"add_new_csvs\"></a>\n# Add additional features to our full sales df\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# add shops_df information\nfull_df = pd.merge(full_df, shops_df, on = \"shop_id\")\nfull_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add items_df information\nfull_df = pd.merge(full_df, items_df, on = \"item_id\")\nfull_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add items_category_df information\nfull_df = pd.merge(full_df, items_category_df, on = \"item_category_id\")\nfull_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will clip the value in this line.\n# This means that the values greater than 20, will become 20 and lesser than 20\nfull_df[\"item_cnt_day\"] = np.clip(full_df[\"item_cnt_day\"], 0, 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"fe_generator_class\"></a>\n# FeatureGenerator class\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeatureGenerator(object):\n    \n    '''\n    This is a helper class that takes a df and a list of features and creates sum, mean, \n    lag features and variation (change over month) features.\n    \n    '''\n    \n    def __init__(self, full_df,  gb_list):\n        \n        '''\n        Constructor of the class.\n        gb_list is a list of columns that must be in full_df.\n        '''\n        \n        self.full_df = full_df\n        self.gb_list = gb_list\n        # joins the gb_list, this way we can dinamically create new columns\n        # [\"date, \"shop_id] --> date_shop_id\n        self.objective_column_name = \"_\".join(gb_list)\n            \n    def generate_gb_df(self):\n        \n        '''\n        This function thakes the full_df and creates a groupby df based on the gb_list.\n        It creates 2 columns: \n            1. A sum column for every date and gb_list\n            2. Mean columns for every_date and gb_list\n            \n        The resulting df (gb_df_) is assigned back to the FeatureGenerator class as an attribute.\n        '''\n\n        def my_agg(full_df_, args):\n            \n            '''\n            This function is used to perform multiple operations over a groupby df and returns a df\n            without multiindex.\n            '''\n            \n            names = {\n                # you can put here as many columns as you want \n                '{}_sum'.format(args):  full_df_['item_cnt_day'].sum()\n            }\n\n            return pd.Series(names, index = [key for key in names.keys()])\n        \n        # the args is used to pass additional argument to the apply function\n        gb_df_ = self.full_df.groupby(self.gb_list).apply(my_agg, args = (self.objective_column_name)).reset_index()\n\n        self.gb_df_ = gb_df_\n\n        \n    def return_gb_df(self):  \n        \n        '''\n        This function takes the gb_df_ created in the previous step (generate_gb_df) and creates additional features.\n        We create 3 lag features (values from the past).\n        And 6 variation features: 3 with absolute values and 3 with porcentual change.\n        '''\n        \n        def generate_shift_features(self, suffix):\n            \n            '''\n            This function is a helper function that takes the gb_df_ and a suffix (sum or mean) and creates the\n            additional features.\n            '''\n\n            # dinamically creates the features\n            # date_shop_id --> date_shop_id_sum if suffix is sum\n            # date_shop_id --> date_shop_id_mean if suffix is mean\n            name_ = self.objective_column_name + \"_\" + suffix\n\n            self.gb_df_['{}_shift_1'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(1))\n            \n            self.gb_df_['{}_shift_2'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(2))\n            \n            self.gb_df_['{}_shift_3'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(3))\n\n            self.gb_df_['{}_var_pct_1'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(2))/x.shift(2))\n            \n            self.gb_df_['{}_var_pct_2'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(3))/x.shift(3))\n            \n            self.gb_df_['{}_var_pct_3'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(4))/x.shift(4))\n            \n            self.gb_df_.fillna(-1, inplace = True)\n\n            self.gb_df_.replace([np.inf, -np.inf], -1, inplace = True)\n        \n        # call the generate_shift_featues function with different suffix (sum and mean)\n        generate_shift_features(self, suffix = \"sum\")\n    \n        return self.gb_df_\n        ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"<a id = \"create_new_features\"></a>\n# Generate additional features as, mean and total sales for shop_id , item_id, city ... for every month\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"feature_1\"></a>\n## Date and shop_id features\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"st = time.time()\n\ngb_list = [\"date\", \"shop_id\", \"city\"]\n\nfe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\nshop_sales_features = fe_generator.return_gb_df()\n\n# to avoid city_x and city_y\n\nshop_sales_features.drop(\"city\", axis = 1, inplace = True)\net = time.time()\n\n(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_sales_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_sales_features[shop_sales_features[\"shop_id\"] == 8].head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"feature_2\"></a>\n## Date and item_id features\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"st = time.time()\n\ngb_list = [\"date\", \"item_id\"]\n\nfe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\nitem_sales_features = fe_generator.return_gb_df()\n\net = time.time()\n\n(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_sales_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_sales_features[item_sales_features[\"item_id\"] == 30].head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"feature_3\"></a>\n## Date and item_category features\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"st = time.time()\n\ngb_list = [\"date\", \"item_category_id\"]\n\nfe_generator = FeatureGenerator(full_df = full_df, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\nmonth_item_category_features = fe_generator.return_gb_df()\n\net = time.time()\n\n(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_item_category_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_item_category_features[month_item_category_features[\"item_category_id\"] == 2].head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"feature_5\"></a>\n## Datetime features\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df[\"year\"] = full_df[\"date\"].dt.year\nfull_df[\"month\"] = full_df[\"date\"].dt.month\nfull_df[\"days_in_month\"] = full_df[\"date\"].dt.days_in_month","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"feature_6\"></a>\n## Adding holiday and number of weekends data\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"holidays_next_month = {\n    12:8,\n    1:1,\n    2:1,\n    3:0,\n    4:2,\n    5:1,\n    6:0,\n    7:0,\n    8:0,\n    9:0,\n    10:1,\n    11:0\n}\n\nholidays_this_month = {\n    1:8,\n    2:1,\n    3:1,\n    4:0,\n    5:2,\n    6:1,\n    7:0,\n    8:0,\n    9:0,\n    10:0,\n    11:1,\n    12:0\n}\n\nfull_df[\"holidays_next_month\"] = full_df[\"month\"].map(holidays_next_month)\nfull_df[\"holidays_this_month\"] = full_df[\"month\"].map(holidays_this_month)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_number_weekends(test_month):\n    '''\n    Extracts the number of weekend days in a month.\n    '''\n    saturdays = len([1 for i in calendar.monthcalendar(test_month.year, test_month.month) if i[5] != 0])\n    sundays = len([1 for i in calendar.monthcalendar(test_month.year, test_month.month) if i[6] != 0])\n    \n    return saturdays + sundays\n\nfull_df[\"total_weekend_days\"] = full_df[\"date\"].apply(extract_number_weekends)\n\n# how much time has passed since the last sale?\ndate_diff_df = full_df[full_df[\"item_cnt_day\"] > 0][[\"shop_id\", \"item_id\", \"date\", \"item_cnt_day\"]].groupby([\"shop_id\", \"item_id\"])\\\n[\"date\"].diff().apply(lambda timedelta_: timedelta_.days).to_frame()\ndate_diff_df.columns = [\"date_diff_sales\"]\n\nfull_df = pd.merge(full_df, date_diff_df, how = \"left\", left_index=True, right_index=True)\nfull_df.fillna(-1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"feature_7\"></a>\n## City population and mean_income per city\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"city_population = {\\\n'Якутск':307911, \n'Адыгея':141970,\n'Балашиха':450771, \n'Волжский':326055, \n'Вологда':313012, \n'Воронеж':1047549,\n'Выездная':1228680, \n'Жуковский':107560, \n'Интернет-магазин':1228680, \n'Казань':1257391, \n'Калуга':341892,\n'Коломна':140129,\n'Красноярск':1083865, \n'Курск':452976, \n'Москва':12678079,\n'Мытищи':205397, \n'Н.Новгород':1252236,\n'Новосибирск':1602915 , \n'Омск':1178391, \n'РостовНаДону':1125299, \n'СПб':5398064, \n'Самара':1156659,\n'СергиевПосад':104579, \n'Сургут':373940, \n'Томск':572740, \n'Тюмень':744554, \n'Уфа':1115560, \n'Химки':244668,\n'Цифровой':1228680, \n'Чехов':70548, \n'Ярославль':608353\n}\n\ncity_income = {\\\n'Якутск':70969, \n'Адыгея':28842,\n'Балашиха':54122, \n'Волжский':31666, \n'Вологда':38201, \n'Воронеж':32504,\n'Выездная':46158, \n'Жуковский':54122, \n'Интернет-магазин':46158, \n'Казань':36139, \n'Калуга':39776,\n'Коломна':54122,\n'Красноярск':48831, \n'Курск':31391, \n'Москва':91368,\n'Мытищи':54122, \n'Н.Новгород':31210,\n'Новосибирск':37014 , \n'Омск':34294, \n'РостовНаДону':32067, \n'СПб':61536, \n'Самара':35218,\n'СергиевПосад':54122, \n'Сургут':73780, \n'Томск':43235, \n'Тюмень':72227, \n'Уфа':35257, \n'Химки':54122,\n'Цифровой':46158, \n'Чехов':54122, \n'Ярославль':34675\n}\n\nfull_df[\"city_population\"] = full_df[\"city\"].map(city_population)\nfull_df[\"city_income\"] = full_df[\"city\"].map(city_income)\nfull_df[\"price_over_income\"] = full_df[\"item_price\"]/full_df[\"city_income\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"join_dfs_with_features\"></a>\n# Join full sales df with all the features generated\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape before merge is {}\".format(full_df.shape))\n\nfull_df = pd.merge(full_df, shop_sales_features, on = [\"date\", \"shop_id\"], how = \"left\")\nfull_df = pd.merge(full_df, item_sales_features, on = [\"date\", \"item_id\"], how = \"left\")\nfull_df = pd.merge(full_df, month_item_category_features, on = [\"date\", \"item_category_id\"], how = \"left\")\nfull_df.rename(columns = {\"item_cnt_day\":\"sales\"}, inplace = True)\n\nprint(\"Shape after merge is {}\".format(full_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the file\n\nst = time.time()\n\nfull_df.to_pickle(\"FULL_DF_ONLY_TEST_ALL_FEATURES.pkl\")\n\net = time.time()\n(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"basic_model\"></a>\n# Basic model train\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the preprocessed data\nfull_df = pd.read_pickle(\"FULL_DF_ONLY_TEST_ALL_FEATURES.pkl\")\n\n# select only a few shops\nfull_df = full_df[full_df[\"shop_id\"].isin(SHOPS)]\n\n# delete all the columns where lags features are - 1 (shift(6))\nfull_df = full_df[full_df[\"date\"] > np.datetime64(\"2013-03-31\")]\n\ncols_to_drop = [\n\n'revenue',\n'shop_name',\n\"city\",\n'item_name',\n'item_category_name',\n'split',\n'type',\n'subtype',\n\n'date_item_id_sum',\n\"date_shop_id_city_sum\",\n\"date_item_category_id_sum\",\n    \n]\n\nfull_df.drop(cols_to_drop, inplace = True, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ------------------------------------------------------\n# separate the dates for train, validation and test\n\ntrain_index = sorted(list(full_df[\"date\"].unique()))[:-2]\n\nvalida_index = [sorted(list(full_df[\"date\"].unique()))[-2]]\n\ntest_index = [sorted(list(full_df[\"date\"].unique()))[-1]]\n\n# ------------------------------------------------------\n# split the data into train, validation and test dataset\n# we \"simulate\" the test dataset to be the Kaggle test dataset\n\nX_train = full_df[full_df[\"date\"].isin(train_index)].drop(['sales', \"date\"], axis=1)\nY_train = full_df[full_df[\"date\"].isin(train_index)]['sales']\n\nX_valida = full_df[full_df[\"date\"].isin(valida_index)].drop(['sales', \"date\"], axis=1)\nY_valida = full_df[full_df[\"date\"].isin(valida_index)]['sales']\n\nX_test = full_df[full_df[\"date\"].isin(test_index)].drop(['sales', \"date\"], axis = 1)\nY_test = full_df[full_df[\"date\"].isin(test_index)]['sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st = time.time()\n\nmodel = XGBRegressor(seed = 175)\n\nmodel_name = str(model).split(\"(\")[0]\n\nday = str(datetime.now()).split()[0].replace(\"-\", \"_\")\nhour = str(datetime.now()).split()[1].replace(\":\", \"_\").split(\".\")[0]\nt = str(day) + \"_\" + str(hour)\n\nmodel.fit(X_train, Y_train, eval_metric = \"rmse\", \n    eval_set = [(X_train, Y_train), (X_valida, Y_valida)], \n    verbose = True, \n    early_stopping_rounds = 10)\n\net = time.time()\n\nprint(\"Training took {} minutes!\".format((et - st)/60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(model, open(\"{}_{}.dat\".format(model_name, t), \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{}_{}.dat\".format(model_name, t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = pickle.load(open(\"{}_{}.dat\".format(model_name, t), \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"feature_importance_1\"></a>\n# Feature importance\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = model.get_booster().get_score(importance_type = \"gain\")\n\nimportance = {k: v for k, v in sorted(importance.items(), key = lambda item: item[1])}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 15))\nplot_importance(model, importance_type = \"gain\", ax = ax);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"predict_and_model_evaluation_1\"></a>\n# Predict and model evaluation\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_valida_pred = model.predict(X_valida)\n\nrmse_valida = sqrt(metrics.mean_squared_error(Y_valida, Y_valida_pred))\nrmse_valida","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test_predict = model.predict(X_test)\n\nrmse_test = sqrt(metrics.mean_squared_error(Y_test, Y_test_predict))\nrmse_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"to_do\"></a>\n# To do\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the preprocessed data\nfull_df = pd.read_pickle(\"../input/full-df-only-test-all-features/FULL_DF_ONLY_TEST_ALL_FEATURES.pkl\")\n\n# select only a few shops\nfull_df = full_df[full_df[\"shop_id\"].isin(SHOPS)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"new_feature_1\"></a>\n# Additional feature 1: Google Trends Data\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{},"cell_type":"markdown","source":"I downloaded used data from google trends API and merged into one single datatable"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeatureGenerator2(FeatureGenerator):\n            \n    def generate_gb_df(self):\n        \n        '''\n        This function thakes the full_df and creates a groupby df based on the gb_list.\n        It creates 2 columns: \n            1. A sum column for every date and gb_list\n            2. Mean columns for every_date and gb_list\n            \n        The resulting df (gb_df_) is assigned back to the FeatureGenerator class as an attribute.\n        '''\n\n        def my_agg(full_df_, args):\n            \n            '''\n            This function is used to perform multiple operations over a groupby df and returns a df\n            without multiindex.\n            '''\n            \n            names = {\n                # you can put here as many columns as you want \n                '{}_mean_trend'.format(args):  full_df_['trend'].mean()\n            }\n\n            return pd.Series(names, index = [key for key in names.keys()])\n        \n        # the args is used to pass additional argument to the apply function\n        gb_df_ = self.full_df.groupby(self.gb_list).apply(my_agg, args = (self.objective_column_name)).reset_index()\n\n        self.gb_df_ = gb_df_\n\n        \n    def return_gb_df(self):  \n        \n        '''\n        This function takes the gb_df_ created in the previous step (generate_gb_df) and creates additional features.\n        We create 3 lag features (values from the past).\n        And 6 variation features: 3 with absolute values and 3 with porcentual change.\n        '''\n        \n        def generate_shift_features(self, suffix):\n            \n            '''\n            This function is a helper function that takes the gb_df_ and a suffix (sum or mean) and creates the\n            additional features.\n            '''\n\n            # dinamically creates the features\n            # date_shop_id --> date_shop_id_sum if suffix is sum\n            # date_shop_id --> date_shop_id_mean if suffix is mean\n            name_ = self.objective_column_name + \"_\" + suffix\n\n            self.gb_df_['{}_shift_1'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(1))\n            \n            self.gb_df_['{}_shift_2'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(2))\n            \n            self.gb_df_['{}_shift_3'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: x.shift(3))\n\n            self.gb_df_['{}_var_pct_1'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(2))/x.shift(2))\n            \n            self.gb_df_['{}_var_pct_2'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(3))/x.shift(3))\n            \n            self.gb_df_['{}_var_pct_3'.format(name_)] =\\\n            self.gb_df_.groupby(self.gb_list[1:])[name_].transform(lambda x: (x.shift(1) - x.shift(4))/x.shift(4))\n            \n            self.gb_df_.fillna(-1, inplace = True)\n\n            self.gb_df_.replace([np.inf, -np.inf], -1, inplace = True)\n        \n        # call the generate_shift_featues function with different suffix (sum and mean)\n        generate_shift_features(self, suffix = \"mean_trend\")\n    \n        return self.gb_df_\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trends = pd.read_csv(\"../input/googletrends/busquedasgoogle.csv\", encoding=\"utf-8\", sep=\";\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trends[\"Unnamed: 0\"] = pd.to_datetime(trends[\"Unnamed: 0\"], format = \"%Y-%m-%d\")\n\n#rename columns\ntrends.columns=[\"date\",\"trend\",\"item_category_id\"]\n\n#create month and year column (we will use it when merging with full df)\ntrends[\"year\"]=trends[\"date\"].apply(lambda x: x.year)\ntrends[\"month\"]=trends[\"date\"].apply(lambda x: x.month)\n\n#set date as index\ntrends.set_index(\"date\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add lags\ngb_list = [\"year\",\"month\",\"item_category_id\"]\n\nfe_generator = FeatureGenerator2(full_df = trends, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\ncategory_trend_features = fe_generator.return_gb_df()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id = \"new_feature_3\"></a>\n# Additional feature 2: Sells grouped by shop & category\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataset has changed target name, so I create another child class from FeatureGenerator\n\nclass FeatureGenerator3(FeatureGenerator):\n            \n    def generate_gb_df(self):\n        \n        '''\n        This function thakes the full_df and creates a groupby df based on the gb_list.\n        It creates 2 columns: \n            1. A sum column for every date and gb_list\n            2. Mean columns for every_date and gb_list\n            \n        The resulting df (gb_df_) is assigned back to the FeatureGenerator class as an attribute.\n        '''\n\n        def my_agg(full_df_, args):\n            \n            '''\n            This function is used to perform multiple operations over a groupby df and returns a df\n            without multiindex.\n            '''\n            \n            names = {\n                # you can put here as many columns as you want \n                '{}_sum'.format(args):  full_df_['sales'].sum()\n            }\n\n            return pd.Series(names, index = [key for key in names.keys()])\n        \n        # the args is used to pass additional argument to the apply function\n        gb_df_ = self.full_df.groupby(self.gb_list).apply(my_agg, args = (self.objective_column_name)).reset_index()\n\n        self.gb_df_ = gb_df_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st = time.time()\n\ngb_list = [\"date\", \"item_category_id\", \"shop_id\"]\n\nfe_generator = FeatureGenerator3(full_df = full_df, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\nmonth_item_shop_features = fe_generator.return_gb_df()\n\net = time.time()\n\n(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id = \"new_feature_3\"></a>\n# Additional feature 3: Sells grouped by shop & subtype\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"st = time.time()\n\ngb_list = [\"date\", \"subtype_code\", \"shop_id\"]\n\nfe_generator = FeatureGenerator3(full_df = full_df, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\nmonth_subtype_shop_features = fe_generator.return_gb_df()\n\net = time.time()\n\n(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id = \"new_feature_3\"></a>\n# Additional feature 4: Sells grouped by subtype & item_categoy_id\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"st = time.time()\n\ngb_list = [\"date\",\"subtype_code\", \"item_category_id\"]\n\nfe_generator = FeatureGenerator3(full_df = full_df, gb_list = gb_list)\n\nfe_generator.generate_gb_df()\n\ndate_subtype_category = fe_generator.return_gb_df()\n\net = time.time()\n\n(et - st)/60","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"new_feature_2\"></a>\n# Bonus feature: Price mean diff\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{},"cell_type":"markdown","source":"During EDA we added all days with 0 sells, but we haven't assigned a price. What I will do is assign the price for the last sell or, in cases where the isn't a previous sell the price of the next one.\n\nOnce I replaced prices with 0 value, I get the mean price for each item.\nThen I calculate the proportion of the daily price based on the mean of the product (if it's lower than 1 means that is cheaper than usual)\n\nI discarted this feature from model training because it produces a worse model."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_prices= pd.DataFrame(columns=[\"date\",\"item_id\",\"shop_id\",\"item_price\"])\nfor item in full_df[\"item_id\"].unique():\n    temp_df = full_df[full_df[\"item_id\"]==item]\n    \n    #Don't know why but method ffill doesn't work for float16\n    temp_df[\"item_price\"] = temp_df[\"item_price\"].astype(\"float32\")\n    \n    temp_df[\"item_price\"] = temp_df[\"item_price\"].mask(temp_df[\"item_price\"]==0.0).ffill(downcast=\"infer\")\n    temp_df[\"item_price\"].fillna(method=\"bfill\", inplace=True)\n    \n    temp_df = temp_df[[\"date\",\"shop_id\",\"item_id\",\"item_price\"]]\n    \n    new_prices = pd.concat([new_prices, temp_df])\n    \nmean_prices = new_prices[new_prices[\"item_price\"]>0].groupby(\"item_id\")[[\"item_price\"]].mean()\nmean_prices.columns = [\"item_mean_price\"]\n\nprices = pd.merge(new_prices, mean_prices, on = [\"item_id\"], how = \"left\")\nprices[\"price_mean_prop\"] = prices[\"item_price\"]/prices[\"item_mean_price\"]\n\nprices.drop(\"item_price\", axis=1, inplace=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"join_dfs_with_new_features\"></a>\n# Join df's with new features\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df = pd.merge(full_df, category_trend_features, on = [\"year\",\"month\",\"item_category_id\"], how = \"left\")\nfull_df = pd.merge(full_df, month_item_shop_features, on=[\"date\", \"item_category_id\", \"shop_id\"], how = \"left\")\nfull_df = pd.merge(full_df, month_subtype_shop_features, on=[\"date\", \"subtype_code\", \"shop_id\"], how = \"left\" )\nfull_df = pd.merge(full_df,date_subtype_category, on=[\"date\",\"subtype_code\", \"item_category_id\"], how = \"left\" )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I don't know why but some numeric columns have change to object during merge, so I transform to numeric again"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df[\"shop_id\"] = pd.to_numeric(full_df[\"shop_id\"])\nfull_df[\"item_id\"] = pd.to_numeric(full_df[\"item_id\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I delete useless data (days without lags values and current period variables)"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df = full_df[full_df[\"date\"] > np.datetime64(\"2013-03-31\")]\n\ncols_to_drop = [\n\n'revenue',\n'shop_name',\n\"city\",\n'item_name',\n'item_category_name',\n'split',\n'type',\n'subtype',\n\n'date_item_id_sum',\n\"date_item_category_id_sum\",\n    \n\"year_month_item_category_id_mean_trend\",\n\"date_item_category_id_shop_id_sum\",\n\"date_subtype_code_shop_id_sum\",\n\"date_subtype_code_item_category_id_sum\"\n]\n\nfull_df.drop(cols_to_drop, inplace = True, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"new_model\"></a>\n# Model training\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_data(df):\n    \n    train_index = sorted(list(df[\"date\"].unique()))[:-2]\n\n    valida_index = [sorted(list(df[\"date\"].unique()))[-2]]\n\n    test_index = [sorted(list(df[\"date\"].unique()))[-1]]\n\n    # ------------------------------------------------------\n    # split the data into train, validation and test dataset\n    # we \"simulate\" the test dataset to be the Kaggle test dataset\n\n    X_train = df[df[\"date\"].isin(train_index)].drop(['sales', \"date\"], axis=1)\n    Y_train = df[df[\"date\"].isin(train_index)]['sales']\n\n    X_valida = df[df[\"date\"].isin(valida_index)].drop(['sales', \"date\"], axis=1)\n    Y_valida = df[df[\"date\"].isin(valida_index)]['sales']\n\n    X_test = df[df[\"date\"].isin(test_index)].drop(['sales', \"date\"], axis = 1)\n    Y_test = df[df[\"date\"].isin(test_index)]['sales']\n    \n    return (X_train,Y_train,X_valida,Y_valida,X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_model(X_train, Y_train, X_valida, Y_valida):\n    \n    st = time.time()\n\n    model = XGBRegressor(seed = 175)\n\n    model_name = str(model).split(\"(\")[0]\n\n    day = str(datetime.now()).split()[0].replace(\"-\", \"_\")\n    hour = str(datetime.now()).split()[1].replace(\":\", \"_\").split(\".\")[0]\n    t = str(day) + \"_\" + str(hour)\n\n    model.fit(X_train, Y_train, eval_metric = \"rmse\", \n        eval_set = [(X_train, Y_train), (X_valida, Y_valida)],\n        verbose = False,\n        early_stopping_rounds = 10)\n\n    et = time.time()\n\n    print(\"Training took {} minutes!\".format((et - st)/60))\n    \n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_results(model, X_valida, Y_valida, X_test, Y_test):\n    \n    Y_valida_pred = model.predict(X_valida)\n\n    rmse_valida = sqrt(metrics.mean_squared_error(Y_valida, Y_valida_pred))\n    \n    Y_test_predict = model.predict(X_test)\n\n    rmse_test = sqrt(metrics.mean_squared_error(Y_test, Y_test_predict))\n    \n    print(\"Model has a validation rmse of {} and test rmse of {}\".format(rmse_valida,rmse_test))\n    \n    return rmse_valida, rmse_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, Y_train, X_valida, Y_valida, X_test, Y_test = split_data(full_df)\n    \nmodel = set_model(X_train, Y_train, X_valida, Y_valida)\n    \nrmse_val, rmse_test = model_results(model, X_valida, Y_valida, X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"feature_importance_2\"></a>\n# Feature importance of new model\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = model.get_booster().get_score(importance_type = \"gain\")\n\nimportance = {k: v for k, v in sorted(importance.items(), key = lambda item: item[1])}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 15))\nplot_importance(model, importance_type = \"gain\", ax = ax, max_num_features=50);","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}