{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV files I/O\nimport statsmodels.api as sm\n\n#For visualizations\nimport plotly.offline as ply\nimport plotly.graph_objs as go\nfrom plotly.tools import make_subplots\nimport colorlover as cl\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SMALL_SIZE = 10\nMEDIUM_SIZE = 11\nBIGGER_SIZE = 14\nBIGGEST_SIZE = 18 #Some uniform font sizes\n\nplt.rc('font', size=SMALL_SIZE)          # controls default text sizes\nplt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\nplt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\nplt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\nplt.rc('figure', titlesize=BIGGEST_SIZE)  # fontsize of the figure title","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing all the datasets\n\ntrain_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\n# Table on the Date Of Purchase,Shop ID, Item ID, Price and Quantity(Bought Or Returned)\ncategories_data = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\n# Table on Item Category Name corresponding to the Item Category ID\nitems_data = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\n# Table Of Name Of the Item along with its ID along with the Item Category ID it is associated with\nshops_data = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\n# Table Of Name Of The Shop along with its ID and setting the ID in the file as the ID pandas should use as well\ntest_data = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\").set_index('ID')\n# Table Of Shop ID along with Item ID for prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dimensions of datasets\nprint('train:', train_data.shape, 'test:', test_data.shape)\nprint('items:', items_data.shape, 'item_cats:', categories_data.shape, 'shops:', shops_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()\n# date_block_num is a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., \n# October 2015 is 33 and so on","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(train_data[\"shop_id\"].min(), train_data[\"shop_id\"].max()) #To get the range of values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(categories_data[\"item_category_id\"].min(), categories_data[\"item_category_id\"].max()) #To get the range of values ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To suppress scientific notation in pandas\npd.options.display.float_format = '{:.5f}'.format","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Explore the statistics of the prices of all items in the training dataset\n#Some statistics include max, min, avg and deviation\nprint(train_data['item_price'].describe())\n# NOTE: This includes double calculation for the returned items as well","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Explore the statistics of the count of all items in the training dataset\n#Some statistics include max, min, avg and deviation\nprint(train_data['item_cnt_day'].describe())\n# NOTE: This is a proper count as returned quantity will be subtracted appropriately","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_item_price_entries = train_data[train_data['item_price'] < 0]\n\nnegative_item_cnt_day_entries = train_data[train_data['item_cnt_day'] < 0]\nnegative_item_cnt_day_entries = negative_item_cnt_day_entries.reset_index()\n\n# Filter the entries that have negative price(incorrect due to a data entry error) and negative count(item returned)\n\nnegative_item_price_entries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_item_cnt_day_entries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = train_data[(train_data.shop_id==32) & (train_data.item_id==2973) & (train_data.date_block_num==4) & (train_data.item_price > 0)].item_price.mean()\ntrain_data.loc[train_data.item_price < 0, 'item_price'] = mean\n# Replacing negative item price  entry with the mean of the non-negative prices of the same item from the same shop in the same\n# month for eliminating data entry error\n\ntrain_data.loc[(train_data.shop_id==32) & (train_data.item_id==2973) & (train_data.date_block_num==4) & (train_data.item_price > 0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merging datasets, can be used if required for visualization\ntrain_merged = pd.merge(train_data, items_data, on='item_id', how='inner')\ntrain_merged = pd.merge(train_merged, categories_data, on='item_category_id', how='inner')\ntrain_merged = pd.merge(train_merged, shops_data, on='shop_id', how='inner')\n# Inner join selects only those rows from both the tables which satisfy the join condition of ID","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Snippet of merged dataset\ntrain_merged.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract and add some more features\n\ntrain_merged['total_sales'] = train_merged.item_price * train_merged.item_cnt_day\ntrain_merged['date'] = pd.to_datetime(train_merged.date)\ntrain_merged['Month'] = train_merged['date'].dt.month\ntrain_merged['Year'] = train_merged['date'].dt.year\ntrain_merged['day_of_week'] = train_merged['date'].dt.day_name()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#snippet of train_merged\nprint(train_merged.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop entries with negative item count\ntrain_merged=train_merged[train_merged.item_cnt_day>=0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Design specifications for the plot\nchosen_colors=cl.scales['5']['qual']['Paired']\nply.init_notebook_mode(connected=True)\ntemp_df = train_merged.groupby('date_block_num')[['total_sales']].sum().reset_index()\ntemp_df.astype('float')\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=temp_df.date_block_num, y=temp_df.total_sales,\n                    mode='lines',\n                    line= dict(color='green', width=2)))\n\n# Plot of revenue generated per month\nfig.update_layout(title='Monthly Revenue',\n                   xaxis_title='Month',\n                   yaxis_title='Total sales')\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df2 = train_merged.groupby('date_block_num')[['item_cnt_day']].sum().reset_index()\ntemp_df2.astype('float')\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=temp_df2.date_block_num, y=temp_df2.item_cnt_day,\n                    mode='lines',\n                    line= dict(color='green', width=2)))\n\n# Plot of items bought per month\nfig.update_layout(title='Monthly Item Count',\n                   xaxis_title='Month',\n                   yaxis_title='Item Count')\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df3 = train_merged.groupby('day_of_week')[['total_sales']].sum().reset_index()\nfig = plt.figure(figsize = (10, 5))\n\n# creating the bar plot \nplt.bar(temp_df3['day_of_week'], temp_df3['total_sales'], color ='red',  \n        width = 0.4) \n# Plot of revenue generated per day of the week\nplt.xlabel(\"Day of week\") \nplt.ylabel(\"Revenue Generated\") \nplt.title(\"Sales on each day of the week\") \nplt.show()\n#Friday, Saturday, Sunday highest in terms of sales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df4 = train_merged.groupby('Month')[['total_sales']].sum().reset_index()\nfig = plt.figure(figsize = (10, 5))\n\n# creating the bar plot \nplt.bar(temp_df4['Month'], temp_df4['total_sales'], color ='red',  \n        width = 0.4) \n# Plot of revenue geerated per month\nplt.xlabel(\"Month of year\") \nplt.ylabel(\"Revenue Generated\") \nplt.title(\"Sales for every month of the year\") \nplt.xticks([i for i in range(1, 13)])\nplt.show()\n#Most sales during december festive season","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df5 = train_merged.groupby('shop_id')[['total_sales']].sum().reset_index()\nfig = plt.figure(figsize = (30, 15))\n\n# creating the bar plot \nplt.bar(temp_df5['shop_id'], temp_df5['total_sales'], color ='red',  \n        width = 0.4) \n# Plot of revenue generated per shop\nplt.xlabel(\"Shop Id\") \nplt.ylabel(\"Revenue Generated\") \nplt.title(\"Revenue generated per shop\") \nplt.xticks([i for i in range(60)])\nplt.show()\n#Most sales during december festive season","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df6 = train_merged.groupby('item_category_id')[['item_cnt_day']].sum().reset_index()\nfig = plt.figure(figsize=(30,15))\n\nplt.bar(temp_df6['item_category_id'],temp_df6['item_cnt_day'], color='red',width=0.5)\n\n# Plot of item sold per category\nplt.xlabel(\"Item Category ID\")\nplt.ylabel(\"Total Number Of Items Sold\")\nplt.title(\"Comparison Between Sales Of Item Categories\")\nplt.xticks([i for i in range(84)])\nplt.show()\n# Maximum Sales in Item Category 40, followed by 30 and then 55","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outlier Plot for showing outliers in the item price as well as item count\nsns.jointplot(x=\"item_cnt_day\", y=\"item_price\", data=train_merged, height=5)\nplt.title(\"Outliers In Terms Of Count And Price\")\nplt.show()\n# Thus outliers are price greater than 50,000 and count greater than 500 and can cause significant\n# amount of noise in the computation for prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=train_merged.groupby(['item_category_id']).count()\nx=x.sort_values(by='item_id',ascending=False) # Sort In Descending Order Of The Number Of Items \nx=x.iloc[0:10].reset_index() # Select The Top 10 for the graph\n\n#Plot of total number of items per item category\nplt.figure(figsize=(10,5))\ny= sns.barplot(x.item_category_id, x.item_id, color='red')\nplt.title(\"Number Of Items Per Category\")\nplt.ylabel('Total Number Of Items')\nplt.xlabel('Item Category ID')\nplt.show()\n#Thus total number of sales that were high for item_categories 40,30 and 55 were due to proportionately\n# high number of items in those categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trend analysis through rolling mean and std deviation\nplt.figure(figsize=(16,6))\nplt.plot(temp_df2.rolling(window=12,center=False).mean(),label='Rolling Mean');\nplt.plot(temp_df2.rolling(window=12,center=False).std(),label='Rolling sd');\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Trend, seasonal and residual analysis\nimport statsmodels.api as sm\n# multiplicative\nres = sm.tsa.seasonal_decompose(temp_df2.values,period=12,model=\"additive\")\n#plt.figure(figsize=(16,12))\nfig = res.plot()\n#fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop entries with negative Item_cnt_day\ntrain_data=train_data[train_data['item_cnt_day']>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Joining item,shop and categories tables\ntrain_final= train_data.join(items_data, on='item_id',rsuffix='_')\ntrain_final = train_final.join(shops_data, on='shop_id', rsuffix='_')\ntrain_final = train_final.join(categories_data, on='item_category_id', rsuffix='_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since dataset consumes large amount of data, we reduce size of some datatypes by downcasting\ndef downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\ntrain_final = downcast_dtypes(train_final)\nprint(train_final.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_shop_ids = test_data['shop_id'].unique()\ntest_item_ids = test_data['item_id'].unique()\n# Filtering only shops that exist in test set.\nleak_df = train_final[train_final['shop_id'].isin(test_shop_ids)]\n# Filtering only items(in the filtered shops) that exist in test set.\nleak_df = leak_df[leak_df['item_id'].isin(test_item_ids)]\nprint('Data set size before leaking:', train_final.shape[0])\nprint('Data set size after leaking:', leak_df.shape[0])\ntrain_final = leak_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop these string based columns, that are not required for training purposes\ntrain_final.drop(['item_name','shop_name','item_category_name','item_category_id'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shop_id_ and shop_id are the same so we drop one of them\n#item_id_ and item_id are the same so we drop one of them\ntrain_final.drop(['shop_id_','item_id_'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop rows where item price is less than zero\ntrain_final=train_final[train_final['item_price']>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We group the data by the month block number, shop_id and item_id, so that predictions can be made based on that\ntrain_final = train_final.sort_values('date').groupby(['date_block_num', 'shop_id','item_id'], as_index=False)\n#getting aggregates as extra columns\ntrain_final = train_final.agg({'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']})\n# Rename features.\ntrain_final.columns = ['date_block_num', 'shop_id', 'item_id', 'item_price', 'mitem_price', 'item_cnt', 'mitem_cnt', 'transactions']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting month block number to corresponding year and month\ntrain_final['year'] = train_final['date_block_num'].apply(lambda x: ((x//12) + 2013))\ntrain_final['month'] = train_final['date_block_num'].apply(lambda x: (x % 12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Keep only the data that satisfies the condition as the rest are outliers(Refer outlier plot)\ntrain_final = train_final.query('item_cnt >= 0 and item_cnt <= 1500 and item_price < 400000')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting item count per item per shop on a monthly basis by sorting based on date_block_num\ntrain_final['cnt_m'] = train_final.sort_values('date_block_num').groupby(['shop_id','item_id'])['item_cnt'].shift(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We need to have all combinations of date_block_num,shop_id,item_id\n#So we add the ones that are not already present\n#Then we fill them up with zeros\nimport time\nts = time.time()\nshop_ids = train_final['shop_id'].unique()\nitem_ids = train_final['item_id'].unique()\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])\nprint(time.time()-ts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Left join\ntrain_final = pd.merge(empty_df, train_final, on=['date_block_num','shop_id','item_id'], how='left')\ntrain_final.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the dataset into train, test and validation sets\n#date block no.s 0 to 25 -> train set\n#date block no.s 26 to 32 -> validation set\n#date block no 33 -> test set\ntrain_set = train_final.query('date_block_num >= 0 and date_block_num < 26').copy()\nvalidation_set = train_final.query('date_block_num >= 26 and date_block_num < 33').copy()\ntest_set = train_final.query('date_block_num == 33').copy()\n\nprint('Train set records:', train_set.shape[0])\nprint('Validation set records:', validation_set.shape[0])\nprint('Test set records:', test_set.shape[0])\n\nprint('Percent of train_set:',(train_set.shape[0]/train_final.shape[0])*100,'%')\nprint('Percent of validation_set:',(validation_set.shape[0]/train_final.shape[0])*100,'%')\nprint('Percent of test_set:',(test_set.shape[0]/train_final.shape[0])*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping NA\ntrain_set.dropna(subset=['cnt_m'], inplace=True)\nvalidation_set.dropna(subset=['cnt_m'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating training set\nx_train = train_set.drop(['cnt_m','date_block_num'],axis=1)\ny_train = train_set['cnt_m'].astype(int)\n\n#Creating validation set\nx_val = validation_set.drop(['cnt_m','date_block_num'],axis=1)\ny_val = validation_set['cnt_m'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping duplicates and creating test set in required format\nlatest_records = pd.concat([train_set, validation_set]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\nx_test = pd.merge(test_data, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])\nx_test['year'] = 2015\nx_test['month'] = 9\nx_test.drop('cnt_m', axis=1, inplace=True)\nx_test = x_test[x_train.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replace null value by median of the unique values in that particular column \nsets = [x_train, x_val, x_test]\nfor dataset in sets:\n    for shop_id in dataset['shop_id'].unique():\n        for column in dataset.columns:\n            shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n            dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fill the remaining missing values in test set with the mean\nx_test.fillna(x_test.mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#base model is m1->Linear Regression\n#Another model is m2->Random Forest Regression\nm1 = LinearRegression()\nm2 = RandomForestRegressor(n_estimators=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m1.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m2.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred1=m1.predict(x_train)\ntrain_pred2=m2.predict(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nprint('Linear Regression RMSE error on training set:',np.sqrt(mean_squared_error(y_train, train_pred1)))\nprint('Random Forest Regression RMSE error on training set:',np.sqrt(mean_squared_error(y_train, train_pred2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_pred1=m1.predict(x_val)\nval_pred2=m2.predict(x_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Linear Regression RMSE error on validation set:',np.sqrt(mean_squared_error(y_val, val_pred1)))\nprint('Random Forest Regression RMSE error on validation set:',np.sqrt(mean_squared_error(y_val, val_pred2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions on test set\ntest_pred1=m1.predict(x_test)\ntest_pred2=m2.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices=[i for i in range(test_pred1.shape[0])]\ntest_pred1=test_pred1.clip(0,20)\ntest_pred2=test_pred2.clip(0,20)\n#Normalising output value in the range[0,20] as per submission specifications\n#Creating CSV file for submission to Kaggle Leaderboard\nsubmission = pd.DataFrame({\n    \"ID\": indices, \n    \"item_cnt_month\": test_pred1\n})\nsubmission.to_csv('lin_reg.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating CSV file for submission to Kaggle Leaderboard\nsubmission2 = pd.DataFrame({\n    \"ID\": indices, \n    \"item_cnt_month\": test_pred2\n})\nsubmission2.to_csv('random_forest.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nm3 = DecisionTreeRegressor()\nm3.fit(x_train,y_train)\ntrain_pred3=m3.predict(x_train)\nval_pred3=m3.predict(x_val)\nprint('Decision Tree Regression RMSE error on training set:',np.sqrt(mean_squared_error(y_train, train_pred3)))\nprint('Decision Tree RMSE error on validation set:',np.sqrt(mean_squared_error(y_val, val_pred3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred3=m3.predict(x_test)\ntest_pred3=test_pred3.clip(0,20)\n#Normalising output value in the range[0,20] as per submission specifications\n#Creating CSV file for submission to Kaggle Leaderboard\nsubmission3 = pd.DataFrame({\n    \"ID\": indices, \n    \"item_cnt_month\": test_pred3\n})\nsubmission3.to_csv('dec_tree.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ensemble Model with Max Voting\nfrom sklearn.ensemble import VotingRegressor\nmaj_vot_model = VotingRegressor([('lr', m1), ('rf', m2),('dt', m3)])\nmaj_vot_model.fit(x_train, y_train)\ntrain_pred_ens1 = maj_vot_model.predict(x_train)\nval_pred_ens1 = maj_vot_model.predict(x_val)\nprint('Majority Voting Ensemble Model RMSE error on training set:',np.sqrt(mean_squared_error(y_train, train_pred_ens1)))\nprint('Majority Voting Ensemble Model RMSE error on validation set:',np.sqrt(mean_squared_error(y_val, val_pred_ens1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred4=maj_vot_model.predict(x_test)\ntest_pred4=test_pred4.clip(0,20)\n#Normalising output value in the range[0,20] as per submission specifications\n#Creating CSV file for submission to Kaggle Leaderboard\nsubmission4 = pd.DataFrame({\n    \"ID\": indices, \n    \"item_cnt_month\": test_pred4\n})\nsubmission3.to_csv('maj_vot.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Weighted Average Ensemble Model\nweighted_val=0.75*val_pred1+ 0.25*val_pred2+ 0*val_pred3\nprint('Weighted Averaging Ensemble Model RMSE error on validation set:',np.sqrt(mean_squared_error(y_val, weighted_val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Experimenting with dimension reduction through PCA and checking RMSE for the same\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\n\nlabels=preprocessing.LabelEncoder()\nx_pca_train=train_set.iloc[:,:-1]\ny_pca_train=train_set.iloc[:,-1:]\n\n'''\nx_pca_train=preprocessing.scale(x_pca_train)\nmm_scaler = preprocessing.MinMaxScaler()\nx_pca_train=mm_scaler.fit_transform(x_pca_train)\ny_pca_train=labels.fit_transform(y_pca_train)\n'''\ncomp=8\npca=PCA(n_components=comp)\nprincipalComponent=pca.fit_transform(x_pca_train)\ncols=list()\nfor i in range(comp):\n    cols.append('principal_comp_'+str(i+1))\nprincipalDf=pd.DataFrame(data=principalComponent,columns=cols)\nx_pca_train=principalDf.iloc[:,:-1]\n#pca_rf_model=RandomForestRegressor(n_estimators=10)\n\n#pca_rf_model.fit(x_pca_train,y_pca_train)\n#train_pca_pred=pca_rf_model.predict(x_pca_train)\npca_lr_model=LinearRegression()\npca_lr_model.fit(x_pca_train,y_pca_train)\npca_pred=pca_lr_model.predict(x_pca_train)\nprint('PCA Model RMSE error on training set:',np.sqrt(mean_squared_error(y_train, pca_pred)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}