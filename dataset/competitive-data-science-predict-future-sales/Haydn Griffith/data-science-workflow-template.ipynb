{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read in dataset - create Pandas Training & Test data objects to work with\n\ntrain_df = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ntest_df = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info(null_counts=True) #Shows each Column name, Non-Null Count and Object type for that Column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"HELPFUL HINT: If the \"Non-Null Count\" is equal for all of the columns, then no data is missing. If a small %\nof values from one column is missing (e.g <=25%), then you can estimate/impute the missing values using \nSKLearn's SimpleImputer or another impute tool. If the % of missing values is large, you may need to consider\neither dropping the rows with missing data (if the remaining dataset would still be large-enough for your \nanalysis) or dropping the column altogether."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info(null_counts=True) #Shows each Column name, Non-Null Count and Object type for that Column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze dataset as needed to identify possible corrections (for clearly-incorrect entries), estimates \n# (for missing data i.e. null entries) as well as notable trends that may guide the type of predictor used.\n\ntrain_df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ranges:\ndate_block_num - 0 - 33;\nshop_id - 0 - 59;\nitem_id = 0 - 22169;\nitem_price - -1 - 307,980;\nitem_cnt_day - -22 - 2,169\n\nNotable info:\nAt least 75% of entries have Item Prices of 249+.\nAt least 75% of entries have 1 or less Items Sold that day.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pivot Item Count data based on various inputs, e.g. Item Price to look for patterns\n\ntrain_df[['item_price', 'item_cnt_day']].groupby(['item_price'], as_index=False).mean().sort_values(by='item_cnt_day',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pivot Item Count data based on various inputs, e.g. Item ID to look for patterns\n\ntrain_df[['item_id', 'item_cnt_day']].groupby(['item_id'], as_index=False).mean().sort_values(by='item_cnt_day',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pivot Item Count data based on various inputs, e.g. Shop ID to look for patterns\n\ntrain_df[['shop_id', 'item_cnt_day']].groupby(['shop_id'], as_index=False).mean().sort_values(by='item_cnt_day',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pivot Item Count data based on various inputs, e.g. Shop ID & Item ID to look for patterns\n\ntrain_df[['shop_id','item_id', 'item_cnt_day']].groupby(['shop_id','item_id'], as_index=False).mean().sort_values(by='item_cnt_day',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze data visually to look for patterns\n\n#g = sns.FacetGrid(train_df)\n#g.map(plt.hist, 'item_price', bins=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare dataset as needed; combine Training & Test data if similar formatting changes needed\n#ENSURE CATEGORICAL COLUMNS CONVERTED TO NUMERICAL (OR DROPPED IF NOT NEEDED), USING ONE-HOT OR OTHER ENCODING METHOD!!!\nimport datetime\n\n#combine = [train_df, test_df]\n\n# Use copy of Training data to adjust/convert/etc. so that initial dataset remains intact\ntrain_df_final = train_df.copy()\n\n# Convert 'date' Column into numerical values (Try to find conversion to int equivalent, like MS Excel). Consider dropping 'date_block_num' if not relevant to sales prediction. \nint_date = pd.to_datetime(train_df['date'], format='%d.%m.%Y')\ntrain_df_final['date'] = int_date.astype(int)\n\ntrain_df_final.head()\n#train_df_final.tail()\n#train_df_final.info(null_counts=True) #Shows each Column name, Non-Null Count and Object type for that Column\n#train_df_final.describe() #Shows Count/Mean/STD/Quartile/Min-Max data for Numeric Columns\n#train_df_final.shape #Shows Row-Column counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choose model (or group of models) to test\n\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn.svm import SVC, LinearSVC\n#from sklearn.ensemble import RandomForestClassifier\n#from sklearn.neighbors import KNeighborsClassifier\n#from sklearn.naive_bayes import GaussianNB\n#from sklearn.linear_model import Perceptron\n#from sklearn.linear_model import SGDClassifier\n#from sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import train_test_split\n\n#Set up Training & Test data objects to use for training & testing, including making training/validation splits\n\nX_train, X_val, y_train, y_val = train_test_split(train_df_final.drop(\"item_cnt_day\", axis=1), train_df_final[\"item_cnt_day\"], random_state=1)\nX_test = test_df.copy()\n\nlogreg = LogisticRegression()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit model (if doing supervised learning)\n\n#logreg.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions\n\n#y_pred = logreg.predict(X_val)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Measure model accuracy\n\nfrom sklearn.metrics import mean_absolute_error\n\n#mae = mean_absolute_error(y_pred, y_val)\n#print(mae)\n\n#acc_log = round(logreg.score(X_train, y_train) * 100, 2)\n\n#acc_log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look for ways to improve accuracy w/o overfitting\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}