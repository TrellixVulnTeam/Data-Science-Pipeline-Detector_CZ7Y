{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport math\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import kpss\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.api import ExponentialSmoothing\nfrom statsmodels.tsa.api import SimpleExpSmoothing\nfrom statsmodels.tsa.api import Holt\nfrom pandas.plotting import autocorrelation_plot\nfrom pandas.plotting import lag_plot\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom math import sqrt\n\nfrom plotly.subplots import make_subplots\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We gona change 'date' feature to other format. It's good practice beacause our new format will be used by many functions in the future"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change format of date feature\ndf['date'] = pd.to_datetime(df.date, format = '%d.%m.%Y')\ndf.head().style.set_properties(subset=['date'], **{\n    'background-color': 'dodgerblue'\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is no missing values"},{"metadata":{},"cell_type":"markdown","source":"# Resampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = df.groupby('date')['item_cnt_day'].sum()\n\nfig = go.Figure()\nfig.add_trace(\n    go.Scatter(x=plot.index, y=plot.values)\n)\n\nfig.update_layout(\n    title='Number of sold products every day',\n    yaxis_title='Number of products',\n    xaxis_title='Month'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = df.groupby('date', as_index=False)['item_cnt_day'].sum()\nresampled_plot = plot[['date', 'item_cnt_day']].resample('7D', on='date').sum().reset_index(drop=False)\n\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=resampled_plot.date, y=resampled_plot.item_cnt_day)\n)\n\nfig.update_layout(\n    title='Number of sold products every week',\n    yaxis_title='Number of products',\n    xaxis_title='Month'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = df.groupby('date_block_num')['item_cnt_day'].sum()\n\nfig = go.Figure()\nfig.add_trace(\n    go.Scatter(x=plot.index, y=plot.values)\n)\n\nfig.update_layout(\n    title='Number of sold products every month',\n    yaxis_title='Number of products',\n    xaxis_title='Month'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no necessity to look at the daily data. Considering weekly data seems to be sufficient as well. We gona create new dataframe with number of sold_products ordered by date and we will downsample it"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num_prod = pd.DataFrame(df.groupby('date', as_index=False)['item_cnt_day'].sum()).rename(columns={'item_cnt_day': 'sold_products'})\ndf_downsampled = df_num_prod[[\n    'date',\n    'sold_products'\n]].resample('7D', on='date').sum().reset_index(drop=False)\ndf_downsampled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Autocorrelation"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfor i in range(2):\n    fig.add_trace(\n        go.Scatter(x=df_downsampled.date, y=df_downsampled['sold_products'].shift(-i), name='lag ' + str(i))\n    )\n    \nfig.update_xaxes(title_text='Date')\nfig.update_yaxes(title_text='Sold Products')\nfig.update_layout(title='Lag plot')\n    \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running this function plots the temperature data (t) on the x-axis\nautocorrelation_plot(df_downsampled['sold_products'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf(df_downsampled['sold_products'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pacf(df_downsampled['sold_products'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running this function plots the temperature data (t) on the x-axis against the temperature on the previous data \n# (t-1) on the y-axis\nlag_plot(df_downsampled['sold_products'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When data have a trend, the autocorrelations for small lags tend to be large and positive because observations nearby in time are also nearby in size. So the ACF of trended time series tend to have positive values that slowly decrease as the lags increase.\nWhen data are seasonal, the autocorrelations will be larger for the seasonal lags (at multiples of the seasonal frequency) than for other lags.\nWhen data are both trended and seasonal, you see a combination of these effects.\nIn our case we can see strong trend."},{"metadata":{},"cell_type":"markdown","source":"For white noise series, we expect each autocorrelation to be cose to zero. Of course, they will not be exactly equal to zero as there is some random variation. For a white noise series, we expect 95% of the spikes in the ACF to lie within plus-minus 2/sqrt(T) where T is the lenght of the time series. In our case autocorrelation coefficients are outside of the range that is why our data are not white noise."},{"metadata":{},"cell_type":"markdown","source":"#  Simple forecasting methods"},{"metadata":{},"cell_type":"markdown","source":"Average Method "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, the forecasts of all future values are equal to the average\nrang = 20\nmean = df_downsampled['sold_products'].mean()\nmeans = []\nfor i in range(rang):\n    means.append(mean)\n    \nnew = pd.date_range(df_downsampled.date.iloc[-1], periods=rang, freq='W')\nnew_df = pd.DataFrame({'date': new[1:], 'sold_products': mean})\ncopy_df = df_downsampled.copy()\n\nto_plot = pd.concat([copy_df, new_df])\n\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=to_plot.date, y=to_plot.sold_products, name='basic')\n)\n\nfig.add_trace(\n    go.Scatter(x=new_df.date, y=new_df.sold_products, name='predicted', mode='lines')\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Naïve method"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For naïve forecasts, we simply set all forecasts to be the value of the last observation\nrang = 20\nmean = df_downsampled['sold_products'].iloc[-1]\nmeans = []\nfor i in range(rang):\n    means.append(mean)\n    \nnew = pd.date_range(df_downsampled.date.iloc[-1], periods=rang, freq='W')\nnew_df = pd.DataFrame({'date': new[1:], 'sold_products': mean})\ncopy_df = df_downsampled.copy()\n\nto_plot = pd.concat([copy_df, new_df])\n\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=to_plot.date, y=to_plot.sold_products, name='basic')\n)\n\nfig.add_trace(\n    go.Scatter(x=new_df.date, y=new_df.sold_products, name='predicted', mode='lines')\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decomposition"},{"metadata":{"trusted":true},"cell_type":"code","source":"decomp = seasonal_decompose(df_downsampled.sold_products, freq=10, model='additive', extrapolate_trend='freq')\ndf_downsampled['sold_products_trend'] = decomp.trend\ndf_downsampled['sold_products_seasonal'] = decomp.seasonal\ndf_downsampled['sold_products_residual'] = decomp.resid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(cols=1, rows=4, subplot_titles=(\n    'Basic',\n    'Trend',\n    'Seasonality',\n    'Residual'\n))\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products),\n    row=1,\n    col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_trend),\n    row=2,\n    col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_seasonal),\n    row=3,\n    col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_residual),\n    row=4,\n    col=1\n)\n\nfig.update_layout(height=800, title_text='Eecomposition of Sold Products', showlegend=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Stationarity"},{"metadata":{},"cell_type":"markdown","source":"Plot time series and check for trends or seasonality"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products, name='basic')\n)\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products.rolling(10).mean(), name='rolling mean')\n)\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products.rolling(10).std(), name='rolling std')\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ADF test is used to determine the presense of unit root in the series, and hence helps in understanding if the series is stationary or not. The null and alternate hypothesis of this test are:\nNull Hypothesis: The series has a unit root\nAlternate Hypothesis: The series has no unit root"},{"metadata":{"trusted":true},"cell_type":"code","source":"def adf_test(timeseries):\n    print('Results of Diskey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n    for key, value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KPSS is another test for checking the stationarity of a time series. The null and alternate hypothesis for the KPSS test are opposite that of the ADF test:\nNull Hypothesis: The process is trend stationary\nAlternate Hypothesis: The series has a unit root (series is not stationary)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def kpss_test(timeseries):\n    print('Results of KPSS Test:')\n    kpsstest = kpss(timeseries, regression='c', nlags='auto')\n    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic', 'p-value', 'Lags Used'])\n    for key, value in kpsstest[3].items():\n        kpss_output['Critical Value (%s)'%key] = value\n    print(kpss_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adf_test(df_downsampled.sold_products)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If the test statistic is less than the critical value, we can reject the null hypothesis (aka the series is stationary). When the test statistic is greater than the critical value, we fail to reject the null hypothesis (which means the series is not stationary). In out case, the test statistic > critical value, which implies that the series is not stationary."},{"metadata":{"trusted":true},"cell_type":"code","source":"kpss_test(df_downsampled.sold_products)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If the test statistic is greater than the critical value, we reject the null hypothesis (series is not stationary). If the test statistic is less than the critical value, it fail to reject the null hypothesis (series is stationary). In our case, the test statistic > critical value, which again implies that the series is not stationary."},{"metadata":{},"cell_type":"markdown","source":"In both tests we see that our data are not stationary. That is why we will use differencing to make it stationary for later use"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First Order Differencing\nts_diff = np.diff(df_downsampled.sold_products)\ndf_downsampled['sold_products_diff_1'] = np.append([0], ts_diff)\n\n# Second Order Differencing\nts_diff = np.diff(df_downsampled.sold_products_diff_1)\ndf_downsampled['sold_products_diff_2'] = np.append([0], ts_diff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(cols=1, rows=2, subplot_titles=(\n    'sold_products_diff_1',\n    'sold_products_diff_2'\n))\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_1, legendgroup='basic', line=dict(color = 'blue'), name='basic'),\n    row=1,\n    col=1\n)\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_1.rolling(10).mean(), legendgroup='mean', line=dict(color = 'orange'), name='rolling mean'),\n    row=1,\n    col=1\n)\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_1.rolling(10).std(), legendgroup='std', line=dict(color = 'red'), name='rolling std'),\n    row=1,\n    col=1\n)\nfig.update_yaxes(title_text='Number of Sold Products', row=1, col=1)\n\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_2, legendgroup='basic', line=dict(color = 'blue'), showlegend=False),\n    row=2,\n    col=1\n)\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_2.rolling(10).mean(), legendgroup='mean', line=dict(color = 'orange'), showlegend=False),\n    row=2,\n    col=1\n)\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_2.rolling(10).std(), legendgroup='std', line=dict(color = 'red'), showlegend=False),\n    row=2,\n    col=1\n)\nfig.update_yaxes(title_text='Number of Sold Products', row=2, col=1)\n\nfig.update_layout(showlegend=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adf_test(df_downsampled.sold_products_diff_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After differencing we can see that, the ADF test statistic < critical value, which implies that the series is now stationary."},{"metadata":{"trusted":true},"cell_type":"code","source":"kpss_test(df_downsampled.sold_products_diff_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After differencing we can see that, the KPSS test statistic < critical value, which implies that the series is now stationary."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf(df_downsampled.sold_products_diff_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pacf(df_downsampled.sold_products_diff_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Autoregression Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split dataset\nX = df_downsampled.copy()\nX = X.drop(['date'], axis=1)\nX = X.sold_products_diff_1.values\ntrain, test = X[1:len(X)-7], X[len(X)-7:]\n\n# Train autoregression\nmodel = AutoReg(train, lags=2)\nmodel_fit = model.fit()\nprint('Coefficients %s' % model_fit.params)\nprint()\n\n# Make predictions\npredictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n\nfor i in range(len(predictions)):\n    print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n\nprint()\n    \nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n\n# Plot results\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=np.arange(7), y=test, name='test')\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(7), y=predictions, name='predictions')\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ARIMA Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(df_downsampled.set_index('date').sold_products, order=(5, 1, 0))\nmodel_fit = model.fit()\nprint(model_fit.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_downsampled.set_index('date').sold_products\nsize = int(len(X) * 0.8)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\n\n# walk-forward validation\nfor t in range(len(test)):\n    model = ARIMA(history, order=(2,1,0))\n    model_fit = model.fit()\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    print('predicted=%f, expected=%f' % (yhat, obs))\n\nprint()\n# Evaluate forecast\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n\n# Plot forecast against actual outcomes\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=test, name='test')\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=predictions, name='predictions')\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_arima_model(X, arima_order):\n    # prepare training dataset\n    train_size = int(len(X) * 0.8)\n    train, test = X[0:train_size], X[train_size:]\n    history = [x for x in train]\n    # make predictions\n    predictions = list()\n    for t in range(len(test)):\n        model = ARIMA(history, order=arima_order)\n        model_fit = model.fit()\n        yhat = model_fit.forecast()[0]\n        predictions.append(yhat)\n        history.append(test[t])\n    # calculate out of sample error\n    error = mean_squared_error(test, predictions)\n    return sqrt(error)\n\n# GriSearchCV for ARIMA\ndef evaluate_models(dataset, p_values, d_values, q_values):\n    dataset = dataset.astype('float32')\n    best_score, best_cfg = float('inf'), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p, d, q)\n                try:\n                    rmse = evaluate_arima_model(dataset, order)\n                    if rmse < best_score:\n                        best_score, best_cfg = rmse, order\n                    print('ARIMA%s RMSE=%.3f' % (order, rmse))\n                except:\n                    continue\n    print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple Exponential Smoothing"},{"metadata":{},"cell_type":"markdown","source":"The simplest of the exponentially smoothing methods is naturally called simple exponencial smoothing (SES). This method is suitable for forecasting data with no clear trend or seasonal pattern. Here we gona use it only to show how it is works."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_downsampled.set_index('date').sold_products\nfit1 = SimpleExpSmoothing(X, initialization_method='heuristic').fit(smoothing_level=0.2, optimized=False)\nfcast1 = fit1.forecast(20)\n\n\nfit2 = SimpleExpSmoothing(X, initialization_method='heuristic').fit(smoothing_level=0.4, optimized=False)\nfcast2 = fit1.forecast(20)\n\nfit3 = SimpleExpSmoothing(X, initialization_method='heuristic').fit(smoothing_level=0.6, optimized=False)\nfcast3 = fit1.forecast(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=X.index, y=X.values, name='test')\n)\n\nfig.add_trace(\n    go.Scatter(x=fit1.fittedvalues.index, y=fit1.fittedvalues.values, legendgroup='0.2', name='SES 0.2', line=dict(color = 'red'))\n)\n\nfig.add_trace(\n    go.Scatter(x=fcast1.index, y=fcast1.values, legendgroup='0.2', showlegend=False, line=dict(color = 'red'))\n)\n\nfig.add_trace(\n    go.Scatter(x=fit2.fittedvalues.index, y=fit2.fittedvalues.values, legendgroup='0.4', name='SES 0.4', line=dict(color = 'yellow'))\n)\n\nfig.add_trace(\n    go.Scatter(x=fcast2.index, y=fcast2.values, legendgroup='0.4', showlegend=False, line=dict(color = 'yellow'))\n)\n\nfig.add_trace(\n    go.Scatter(x=fit3.fittedvalues.index, y=fit3.fittedvalues.values, legendgroup='0.6', name='SES 0.6', line=dict(color='brown'))\n)\n\nfig.add_trace(\n    go.Scatter(x=fcast3.index, y=fcast3.values, legendgroup='0.6', showlegend=False, line=dict(color = 'brown'))\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_downsampled.set_index('date').sold_products\nsize = int(len(X) * 0.8)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\n\n# walk-forward validation\nfor t in range(len(test)):\n    model = SimpleExpSmoothing(history, initialization_method='heuristic').fit(smoothing_level=0.2, optimized=False)\n#     model_fit = model.fit()\n    output = model.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    print('predicted=%f, expected=%f' % (yhat, obs))\n\nprint()\n# Evaluate forecast\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n\n# Plot forecast against actual outcomes\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=test, name='test')\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=predictions, name='predictions')\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Holt's Exponential Smoothing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_downsampled.set_index('date').sold_products\nfit1 = Holt(X, initialization_method='estimated').fit(smoothing_level=0.8, smoothing_trend=0.2, optimized=False)\nfcast1 = fit1.forecast(10)\n\nfit2 = Holt(X, initialization_method='estimated').fit(smoothing_level=0.6, smoothing_trend=0.4, optimized=False)\nfcast2 = fit2.forecast(10)\n\nfit3 = Holt(X, initialization_method='estimated').fit(smoothing_level=0.4, smoothing_trend=0.6, optimized=False)\nfcast3 = fit3.forecast(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=X.index, y=X.values, name='test')\n)\n\nfig.add_trace(\n    go.Scatter(x=fit1.fittedvalues.index, y=fit1.fittedvalues.values, legendgroup='HES 0.8 0.2', line=dict(color = 'red'), name='HES 0.8 0.2')\n)\nfig.add_trace(\n    go.Scatter(x=fcast1.index, y=fcast1.values, mode='lines', legendgroup='HES 0.8 0.2', line=dict(color='red'), showlegend=False)\n)\n\nfig.add_trace(\n    go.Scatter(x=fit2.fittedvalues.index, y=fit2.fittedvalues.values, legendgroup='HES 0.6 0.4', line=dict(color = 'yellow'), name='HES 0.6 0.4')\n)\nfig.add_trace(\n    go.Scatter(x=fcast2.index, y=fcast2.values, mode='lines', legendgroup='HES 0.6 0.4', line=dict(color='yellow'), showlegend=False)\n)\n\nfig.add_trace(\n    go.Scatter(x=fit3.fittedvalues.index, y=fit3.fittedvalues.values, legendgroup='HES 0.4 0.6', line=dict(color = 'brown'), name='HES 0.4 0.6')\n)\nfig.add_trace(\n    go.Scatter(x=fcast3.index, y=fcast3.values, mode='lines', legendgroup='HES 0.4 0.6', line=dict(color='brown'), showlegend=False)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_downsampled.set_index('date').sold_products\nsize = int(len(X) * 0.8)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\n\n# walk-forward validation\nfor t in range(len(test)):\n    model = Holt(history, initialization_method='estimated').fit(smoothing_level=0.8, smoothing_trend=0.2, optimized=False)\n    output = model.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    print('predicted=%f, expected=%f' % (yhat, obs))\n\nprint()\n# Evaluate forecast\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n\n# Plot forecast against actual outcomes\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=test, name='test')\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=predictions, name='predictions')\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"simulated = model_fit.simulate(anchor='end', nsimulations=7, repetitions=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfor i in range(len(simulated)):\n    fig.add_trace(\n        go.Scatter(x=np.arange(20), y=simulated[i], line=dict(color = 'gray'), showlegend=False, opacity=0.2)\n    )\n    \nfig.add_trace(\n    go.Scatter(x=np.arange(20), y=test, line=dict(color = 'red'))\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Holt's Winters Seasonal Smoothing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_downsampled.set_index('date').sold_products\n\nfit1 = ExponentialSmoothing(X, seasonal_periods=12, trend='add', seasonal='add', use_boxcox=True, initialization_method='estimated').fit()\nfcast1 = fit1.forecast(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=X.index, y=X.values, name='data')\n)\n\nfig.add_trace(\n    go.Scatter(x=fit1.fittedvalues.index, y=fit1.fittedvalues.values, legendgroup='add add', line=dict(color = 'red'), name='predicted')\n)\n\nfig.add_trace(\n    go.Scatter(x=fcast1.index, y=fcast1.values, legendgroup='add add', line=dict(color = 'red'), showlegend=False)\n)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X = df_downsampled.set_index('date').sold_products\nsize = int(len(X) * 0.8)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\n\n# walk-forward validation\nfor t in range(len(test)):\n    model = ExponentialSmoothing(history, seasonal_periods=7, trend='mul', seasonal='mul', use_boxcox=True, initialization_method='estimated')\n    model_fit = model.fit()\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    print('predicted=%f, expected=%f' % (yhat, obs))\n\nprint()\n# Evaluate forecast\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n\n# Plot forecast against actual outcomes\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=test, name='test')\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=predictions, name='predictions')\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"simulated = model_fit.simulate(anchor='end', nsimulations=7, repetitions=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfor i in range(len(simulated)):\n    fig.add_trace(\n        go.Scatter(x=np.arange(20), y=simulated[i], line=dict(color = 'gray'), showlegend=False, opacity=0.2)\n    )\n    \nfig.add_trace(\n    go.Scatter(x=np.arange(20), y=test, line=dict(color = 'red'))\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}