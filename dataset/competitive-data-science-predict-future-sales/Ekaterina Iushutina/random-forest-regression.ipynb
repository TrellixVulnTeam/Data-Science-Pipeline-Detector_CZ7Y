{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport seaborn as sns\nimport matplotlib.pylab as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"items = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\")\nitems.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv\")\ncategories.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\")\nshops","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv\")\nsales.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_test = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/test.csv\")\nsales_test.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Delete the duplicates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(items.duplicated(['item_name'])))\nprint(sum(categories.duplicated(['item_category_name'])))\nprint(sum(shops.duplicated(['shop_name'])))\n# We can see that the names of shops 10 and 11 differ only by one letter. It is probably the same shop.  \n# Also 0 and 57, 1 and 58, ?39 and 40?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's find out if shops 10,11,0,57,1,58 present in the dataframe for forecasting.\nuniq_shops = sales_test['shop_id'].unique()\nfor shop in list([10,11,0,57,1,58]):\n    print(shop, shop in uniq_shops)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_shop_id = {11: 10, 0: 57, 1: 58}\nshops['shop_id'] = shops['shop_id'].apply(lambda x: new_shop_id[x] if x in new_shop_id.keys() else x)\nsales['shop_id'] = sales['shop_id'].apply(lambda x: new_shop_id[x] if x in new_shop_id.keys() else x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's drop the pairs (shop_id, item_id) that are not represented in the dataframe for forecasting. And merge two dataframes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.merge(sales_test, sales, on = ('shop_id', 'item_id'), how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(sales.duplicated()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the duplicate rows from sales\nsales = sales.drop_duplicates()\nsales.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(sales.duplicated(['ID','date','date_block_num','item_price'])))\nprint(sum(sales.duplicated(['ID','date','date_block_num','item_cnt_day'])))\nsales[sales.duplicated(['ID','date','date_block_num','item_cnt_day'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales[sales.duplicated(['ID','date','date_block_num','item_cnt_day'], keep = 'last')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We should think carefully which row should be dropped. Price will help us. But now we will just keep first duplicate and drop later.\nsales = sales.drop_duplicates(['date','date_block_num','shop_id','item_id','item_cnt_day'])\nsales.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales[sales.duplicated(['ID','date','date_block_num'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales[sales.duplicated(['ID','date','date_block_num'], keep = 'last')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales.drop_duplicates(['ID','date','date_block_num'], keep = 'last')\nsales.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(items.isnull().sum().sum())\nprint(categories.isnull().sum().sum())\nprint(shops.isnull().sum().sum())\nprint(sales.isnull().sum().sum())\n# There are missing values in the data. Most of them corresponds to IDs from the forcast set that doesn't represent in training set.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outliers and negative values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.describe()\n# It is possible that item_price and item_cnt_day has outliers (max >> 0.75-quantile), and item_cnt_day has wrong values (min < 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(sales.item_cnt_day < 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change a sign of negative values\nsales.loc[sales.item_cnt_day < 0, 'item_cnt_day'] = -1. * sales.loc[sales.item_cnt_day < 0, 'item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(x = sales.item_cnt_day)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(x = sales.loc[sales.item_id == sales.item_id[sales.item_cnt_day.idxmax()],'item_cnt_day'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(x = sales.item_price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.loc[sales.item_price > 35000,'item_id'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.loc[items.item_id == 13403,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert to month data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's group the sales by ID and calculate month number of sold items and average price.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_month = sales.sort_values('date_block_num').groupby(['ID', 'date_block_num'], as_index = False).agg({'item_cnt_day': ['sum'], 'item_price': ['mean']})\nsales_month.columns = ['ID', 'date_block_num', 'item_cnt_month', 'item_price']\nsales_month.sample(10)\n# after we grouped and aggregate data we delete all rows corresponding to IDs that don't present in train data set (and preset just in forcasting set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_month.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(sales_month.loc[:,'item_cnt_month'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(sales_month.loc[:,'item_price'], kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(sales_month.item_cnt_month, sales_month.item_price)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look on sales of each shop over time.\n\nWe can see that there are several leader-shops and outsider-shops.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.subplots(figsize=(25, 12))\nax = sns.heatmap(sales.pivot_table(index = 'date_block_num', columns = 'shop_id', values = 'item_cnt_day', aggfunc = 'sum'), cmap=\"YlGnBu\")\nplt.title(\"Items sold by each shop per month\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's calculate how many unique IDs was sold in each month","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.subplots(figsize=(15, 5))\nplt.plot(sales_month.groupby(['date_block_num'], as_index = False).agg({'ID':'count'}).iloc[:,1], 'o')\nplt.title(\"Number of the unique IDs over months\")\nplt.xlabel(\"date_block_num\")\nplt.show()\n# We have over 100,000 unique IDs but less then 30,000 of them was sold in any month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.subplots(figsize=(25, 5))\nplt.plot(sales_month.groupby(['ID'], as_index = False).agg({'date_block_num':'count'}).iloc[:,1], '.')\nplt.title(\"Number of the month in which each ID was sold\")\nplt.xlabel(\"unique ID\")\nplt.show()\n# The most of ID has information about sales less then for 10 months.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's split the data by ID. We will store ID and corresponding data in a list.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_IDs(np_data, col_ID):\n    # np_data - sales converted to numpy array\n    # col_ID - name of ID column\n    sales_by_ID = list() #dict()\n    IDs = np.unique(np_data[:,col_ID])\n    #IDs = np_data[col_ID].unique()\n    for i in IDs:\n        positions = np_data[:,col_ID] == i\n        sales_ID = np_data[positions,:]\n        #positions = np_data[col_ID] == i\n        #sales_by_ID[i] = np_data.loc[positions,:]\n        sales_by_ID.append(sales_ID)\n    return sales_by_ID","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_by_ID = to_IDs(sales_month.values,0)\n#sales_by_ID = to_IDs(sales_month,'ID')\nprint(len(sales_by_ID))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to decrease calculation time during a code debugging we remove IDs that don't have observtions for last 6 months\ndef remove_ID_nan_last_year(np_data):\n    N_IDs = len(np_data)\n    col_date = 1\n    clear_data = list()\n    cut_month = 33 - 6\n    for i in range(N_IDs):\n        ID_data = np_data[i]\n        if len(ID_data[ID_data[:,col_date] >= cut_month,2]) != 0:\n            clear_data.append(ID_data)\n    return clear_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sales_by_ID = remove_ID_nan_last_year(sales_by_ID)\n#len(sales_by_ID)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 34 months of observations. Let's split the data into train (33 months) and test (last month) samples.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#def split_train_test(np_data, col_date = 'date_block_num', last_month = 33):\ndef split_train_test(np_data, col_date = 1, last_month = 33):\n    col_TS = 2 # numbe of item_cnt_month column\n    N_IDs = len(np_data)\n    train = list()\n    test = list()\n    empty_train, empty_test = 0, 0 # we will count train and test sets that have zero length\n    #for ID_data in np_data.values():\n    for i in range(N_IDs):\n        ID_data = np.array(np_data[i])\n        # ID_data = np.array(ID_data)\n        train_rows = ID_data[ID_data[:,col_date] < last_month, :]\n        test_response = ID_data[ID_data[:,col_date] >= last_month, col_TS]\n        #train_rows = ID_data.loc[ID_data[col_date] < last_month, :]\n        #test_rows = ID_data.loc[ID_data[col_date] >= last_month, :]\n        if len(train_rows) == 0: #or (len(train_rows) == 1 and len(test_response) == 0):\n            empty_train = 1 + empty_train\n            continue\n        if len(test_response) == 0:\n            empty_test = 1 + empty_test\n            test.append(np.nan)\n        else:\n            test.append(test_response[0])\n        train.append(train_rows)\n    print(empty_train,' IDs do not have observations in TRAIN set')\n    print(empty_test,' IDs do not have observations in TEST set')\n    return train, np.array(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test_actual = split_train_test(sales_by_ID, 1, 33)\nprint(len(test_actual), 'IDs will be used for modeling')\nprint(len(train))\n# We have a lot of IDs that don't have observations for last month, \n# so these are useless for a metric calculating but we keep it for modeling.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_actual = np.nan_to_num(test_actual, nan = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fill data for missing months","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's find out if there are missing month for any ID.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's fill the missing date_block_num by NaN for paticular ID\ndef missing_months(np_data, col_date, col_TS, N_months = 33):\n    # col_date - index of date_block_num column\n    # col_TS - index of item_price column and item_cnt_month column\n    # at first fill time series by NaN for all months\n    series = [np.nan for _ in range(N_months)]\n    for i in range(len(np_data)):\n        position = int(np_data[i, col_date] - 1)\n        # fill positions that present in data\n        series[position] = np_data[i, col_TS]\n    return series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot time series for particular ID to find out missing months\ndef plot_TS(np_data, n_vars = 1, N_months = 33, flag = 0):\n    # n_vars = 1 or 2 (plot item_cnt OR item_cnt and item_price)\n    plt.figure()\n    if flag == 1:\n        TSs = to_fill_missing(np_data, N_months)\n    for i in range(n_vars):\n        col_plot = i + 2 # index of column to plot\n        if flag == 1:\n            series = TSs[:,col_plot]\n        else:\n            series = missing_months(np_data, 1, col_plot, N_months)\n        ax = plt.subplot(n_vars, 1, i+1)\n        plt.plot(series, 'o')\n        plt.plot(series)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_TS(train[10],2,33, flag = 0)\n# in the up plot there is represented the item_cnt_month\n# in the low -- item_price for ID=11\n# We can use an interpolation to fill missing value for some IDs but not for all","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look on the plots of several IDs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_TS(train[1563],2,33)\n# In this case we can fill the data by 0 or 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(train[1563][:,2], train[1563][:,3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_TS(train[80059],2,33)\n# In this case it seems resonable to fill by 0 (because of high price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_TS(train[30111],2,33)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have totaly different numbers and positions of missing months for different IDs. Let's fill item_cnt_month by 0 and add column of missing flag.\nItem_price will be interpolated.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's fill the missing item_cnt_month and item_price for particular ID\ndef to_fill_missing(np_data, N_months = 33):\n    date_month = pd.DataFrame(range(N_months),columns = ['date_block_num'])\n    col = ['ID','date_block_num','item_cnt_month','item_price']\n    sales_ID = pd.DataFrame(np_data, columns = col)\n    #sales_ID['missing_flag'] = 1\n    if sales_ID.shape[0] < N_months:\n        sales_ID = pd.merge(date_month, sales_ID, on = ('date_block_num'), how = 'left')\n        sales_ID = sales_ID.reindex(columns = col)\n     #   sales_ID['missing_flag'] = sales_ID['item_cnt_month'].isnull().astype('uint')\n        sales_ID['ID'] = sales_ID['ID'].fillna(sales_ID['ID'].loc[sales_ID['ID'].first_valid_index()])\n        sales_ID['item_cnt_month'] = sales_ID['item_cnt_month'].fillna(0)\n        sales_ID['item_price'] = sales_ID['item_price'].interpolate(method ='linear', limit_direction ='both')\n    return sales_ID.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look on graphs of data with missing values and data with filled missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_TS(train[30111],2,33,flag = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_TS(train[30111],2,33,flag = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(train[30111][:,2], train[30111][:,3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By scatter plots It seems that the price of particular ID does not have influence on the count. But we have seen just the scatter plots of three IDs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_TS(train[5127],2,33,flag = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_TS(train[5127],2,33,flag = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ACF and PACF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_acf_pacf(np_data, n_vars = 1, N_months = 33):\n    # n_vars = 1 or 2 (plot item_cnt OR item_cnt and item_price)\n    plt.figure()\n    col_plot = 2 # index of column to plot\n    for i in range(1, 2*n_vars, 2): \n        series = to_fill_missing(np_data, N_months)\n        series = series[:,col_plot]\n        ax = plt.subplot(n_vars, 2, i)\n        plot_acf(series, ax = ax)\n        ax = plt.subplot(n_vars, 2, i+1)\n        plot_pacf(series, ax = ax)\n        col_plot = col_plot + 1\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf_pacf(train[61559],1,33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf_pacf(train[30112])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lot of IDs have 1 lag of acf and 1 lag of pacf, ohers - 0 significant lags. So we will create two more features: 1 lag observations and 2 lag observations.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In order to execute supervised algorithms we have to modify the data this way:\nseveral lags like an input and next lag like an output.\n\nLet's assume n_lag = 3 than modified time series of item_cnt_month will be like the following:\n\n---------------------input--------------- ||         output \n\ndata_month1 data_mont2 data_month3 ||        data_month4 \n\ndata_month2 data_mont3 data_month4 ||        data_month5 \n\ndata_month3 data_mont4 data_month5 ||        data_month6 \n\n.....","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's modify the data of particular ID\ndef to_make_lag_features(np_data, n_lag = 2, N_months=33):\n    # in_out = np.empty((N_months-n_lag, 2*n_lag+1))\n    ID_TS = to_fill_missing(np_data, N_months)\n    N_col = ID_TS.shape[1]\n    in_out = np.empty((N_months-n_lag, n_lag+N_col))\n    count_TS = ID_TS[:,2]\n    for i in range(n_lag, N_months):\n        # input features: n_lags of item_cnt_month and n_lags of item_price\n        # output: item_cnt_month\n        # in_out[i-n_lag,:] = np.concatenate([count_TS[i-n_lag:i], np.array([count_TS[i]])]) # price_TS[i-n_lag:i],  np.array([count_TS[i]])])\n        in_out[i-n_lag,:] = np.concatenate([np.delete(ID_TS[i,:],2,axis=0), count_TS[i-n_lag:i],  np.array([count_TS[i]])])\n    # the last array contains n_lags of item_cnt_month and n_lags of item_price that will be features for prediction of 34th month\n    test_df = np.concatenate([np.array([ID_TS[i,0], N_months, ID_TS[i,3]]), count_TS[i+1-n_lag:i+1]])\n    # test_df = np.concatenate([count_TS[i+1-n_lag:i+1], price_TS[i+1-n_lag:i+1]])\n    # test_df = count_TS[i+1-n_lag:i+1]\n    return in_out, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_preparation(data_IDs, N_months = 33, n_lag = 2):\n    # data_IDs - list of train data, each element of list contains train data for particular ID\n    N_IDs = len(data_IDs)\n    N_col = data_IDs[0].shape[1]\n    N_rows = N_months-n_lag\n    train_data = np.empty((N_IDs*N_rows, n_lag+N_col)) # 2*n_lag+1\n    test_data = np.empty((N_IDs, n_lag+N_col-1)) # 2*n_lag\n    list_IDs = np.empty((1,N_IDs))\n    # col_TS - index of item_cnt_month column\n    col_TS = 2\n    for i in range(N_IDs): # each particular ID\n        ID_data = data_IDs[i]\n        train, test = to_make_lag_features(ID_data, n_lag, N_months)\n        train_data[i*N_rows:(i+1)*N_rows,:] = train\n        test_data[i,:] = test\n       # list_IDs[0,i] = ID_data[0,0]\n    return np.array(train_data), np.array(test_data)#, list_IDs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's transform the data. We will use the Box-Cox transforms in order to stabilize variance and minimize skewness.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#we must transform features if we will use SVM model, KNN model\nfrom sklearn.preprocessing import PowerTransformer\n#from sklearn.preprocessing import QuantileTransformer\nBoxCox = PowerTransformer(method = 'box-cox').fit(train.loc[:,['item_cnt_month','item_price']])\ntrain.loc[:,['item_cnt_month','item_price']] = BoxCox.transform(train.loc[:,['item_cnt_month','item_price']])\ntest.loc[:,['item_cnt_month','item_price']] = BoxCox.transform(test.loc[:,['item_cnt_month','item_price']])\n#sales_month.loc[:,['item_cnt_month','item_price']] = QuantileTransformer(output_distribution='normal').fit_transform(sales_month.loc[:,['item_cnt_month','item_price']])","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's fit model for item_cnt_month of all IDs\ndef to_fit_model(model, train_df):\n    features, label = train_df.drop('item_cnt_month', axis = 1), train_df.loc[:,'item_cnt_month']\n    model.fit(features, label)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#we can use method metrics instead of self-function to_calculate_MAE\ndef to_calculate_RMSE(actual, predictions):\n    mse = 0\n    N_IDs = len(actual)\n    for i in range(N_IDs):\n        if np.isnan(actual[i]):\n            continue\n        error = (predictions[i] - actual[i])**2\n        mse += error\n    mse /= sum(~np.isnan(actual))\n    return np.sqrt(mse)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_lag = 2\ntr, ts = data_preparation(train, n_lag = n_lag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_label = pd.DataFrame(tr, columns = ['ID', 'date_block_num', 'item_price', \n                                                   'item_cnt_month_lag-2', 'item_cnt_month_lag-1', 'item_cnt_month'])\ntest_features = pd.DataFrame(ts, columns = ['ID', 'date_block_num', 'item_price', \n                                                   'item_cnt_month_lag-2', 'item_cnt_month_lag-1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_label.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's attach shop_id, iem_id, item_category_id columns to features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_label = pd.merge(train_features_label, sales_test.loc[:,['ID', 'shop_id', 'item_id']], on = ('ID'), how = 'left')\ntrain_features_label = pd.merge(train_features_label, items.loc[:,['item_id', 'item_category_id']], on = ('item_id'), how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#aa = aa.drop(aa[(aa['item_cnt_month_lag-2'] == 0) & (aa['item_cnt_month_lag-1'] == 0) & (aa['item_cnt_month'] == 0)].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_label = train_features_label.astype({'ID': 'uint32', 'date_block_num': 'uint8', 'item_cnt_month_lag-2': 'uint16', 'item_cnt_month_lag-1': 'uint16',        'item_cnt_month': 'uint16'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features = pd.merge(test_features, sales_test.loc[:,['ID', 'shop_id', 'item_id']], on = ('ID'), how = 'left')\ntest_features = pd.merge(test_features, items.loc[:,['item_id', 'item_category_id']], on = ('item_id'), how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features = test_features.astype({'ID': 'uint32', 'date_block_num': 'uint8', 'item_cnt_month_lag-2': 'uint16', 'item_cnt_month_lag-1': 'uint16'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameters tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"hyper_params = {'n_estimators': [int(x) for x in np.linspace(start = 100, stop = 1000, num = 3)], \n                'max_features': ['auto', 'sqrt'], \n                'min_samples_split': [2, 10, 50]} \n                #'bootstrap': [True, False]}","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#choose randomly n_iter combinations of parameters\nmodels_hp = RandomizedSearchCV(estimator = model, param_distributions = hyper_params, n_iter = 3, cv = 3, verbose = 2, random_state = 42)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"models_hp.fit(aa.drop('item_cnt_month', axis = 1), aa.loc[:,'item_cnt_month'])","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#best_model = models_hp.best_estimator_\nmodels_hp.best_params_","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#Mean cross-validated score of the best_estimator\nmodels_hp.best_score_","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best_random = {'n_estimators': 94, 'min_samples_split': 10, 'max_features': 'sqrt', 'bootstrap': True}\n#{'n_estimators': 1000, 'min_samples_split': 50, 'max_features': 'auto'} # score = 0.6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"evaluate_model(model, train_df, test_df, actual)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor(n_estimators = 500, min_samples_split = 10, criterion = \"mse\", bootstrap = True, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fitt = to_fit_model(model, train_features_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = fitt.predict(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = metrics.mean_squared_error(test_actual, predictions) #to_calculate_RMSE(test_actual, predictions)\nprint(RMSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions[range(20)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_actual[range(20)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = fitt.feature_importances_\nN_features = len(importance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.title(\"Feature importance\")\nbars = plt.bar(range(N_features), importance, align=\"center\")\nx = plt.xticks(range(N_features), list(test_features.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenate the columns\nfeatures_last_month = test_features.copy()\nfeatures_last_month['date_block_num'] = 34\nfeatures_last_month['item_cnt_month_lag-2'] = test_features['item_cnt_month_lag-1']\nfeatures_last_month['item_cnt_month_lag-1'] = test_actual\nfeatures_last_month.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_month_predict = fitt.predict(features_last_month)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        'ID': features_last_month['ID'],\n        'item_cnt_month': last_month_predict\n    })\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.merge(sales_test.ID, submission, on = ('ID'), how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission.fillna(0)\nsubmission.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model fits the data in the which missing months are not filled","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"dataset = sales_month.copy()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def to_make_lag(df, lag):\n    lag_df = pd.DataFrame({'ID': [], 'date_block_num': [], 'item_cnt_month_lag-'+str(lag): []})\n    lag_df['ID'] = df['ID']\n    lag_df['item_cnt_month_lag-'+str(lag)] = df['item_cnt_month']\n    lag_df['date_block_num'] = df['date_block_num'] + lag\n    df = pd.merge(df, lag_df, on = ('ID', 'date_block_num'), how = 'left') #'both'\n    return df","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"lags = [1,2]\nfor ilag in range(len(lags)):\n    dataset = to_make_lag(dataset, lags[ilag])","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"dataset = dataset.fillna(0)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"train_set = dataset.copy()\ntest_set = dataset[dataset.date_block_num == 33]\nactual_label = test_set['item_cnt_month']\ntest_set = test_set.drop('item_cnt_month', axis = 1)\ntrain_set = train_set.drop(test_set.index, axis = 0)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"model = RandomForestRegressor(n_estimators = 100, min_samples_split = 10, criterion = \"mse\", bootstrap = True, verbose = 1)\nfitting = to_fit_model(model, train_set)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"predictions = fitting.predict(test_set)\nmetrics.mean_squared_error(actual_label, predictions)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}