{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom tqdm import tqdm_notebook\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import VarianceThreshold\n\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Useful functions\ndef submit(y_pred, fname='submission'):\n    test = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n    submission = pd.DataFrame({\n        'ID': test['ID'],\n        'item_cnt_month': y_pred.ravel()\n    })\n    submission.to_csv(f'{fname}.csv', index=False)\n    \ndef downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(((y_pred - y_true) ** 2).mean())\n\ndef plot_seasonal_decompose(result, title=''):\n    plt.figure(figsize=(10, 8))\n    plt.subplot(411)\n    plt.plot(result.observed)\n    plt.ylabel('Observed')\n    \n    plt.subplot(412)\n    plt.plot(result.trend)\n    plt.ylabel('Trend')\n    \n    plt.subplot(413)\n    plt.plot(result.seasonal)\n    plt.ylabel('Seasonal')\n    \n    plt.subplot(414)\n    plt.plot(result.resid)\n    plt.ylabel('Residual')\n    plt.suptitle(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data\n\nAll translated Russian stuffs were taken from [here](https://www.kaggle.com/c/competitive-data-science-predict-future-sales/discussion/54949)."},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nitems2 = pd.read_csv('../input/predict-future-sales-russian-translated/items-translated.csv')\n\nitem_categories = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nitem_categories2 = pd.read_csv('../input/predict-future-sales-russian-translated/item_categories-classified.csv')\n\nitem_categories_classes = pd.read_csv('../input/predict-future-sales-russian-translated/item_categories-classes.csv')\n\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\nshops2 = pd.read_csv('../input/predict-future-sales-russian-translated/shops-translated.csv')\n\nsales = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('items :', items.shape)\nprint('item_categories :', item_categories.shape)\nprint('shops :', shops.shape)\nprint('sales :', sales.shape)\nprint('test :', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{},"cell_type":"markdown","source":"**Number of items sold per month**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = sales.groupby('date_block_num')['item_cnt_day'].sum()\n\nplt.figure(figsize=(16, 5))\nplt.subplot(121)\nplt.plot(data)\nplt.xlabel('month')\nplt.ylabel('items sold')\n\n# Rolling statistics\nplt.subplot(122)\nplt.plot(data.rolling(window=12).mean(), label='rolling mean')\nplt.plot(data.rolling(window=12).std(), label='rolling std')\nplt.xlabel('month')\nplt.ylabel('items sold')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. **Quick observations:** There is an obvious \"seasonality\" (Eg: peak sales around a time of year) and a decreasing \"Trend\".\n\nLet's check that with a quick <a href=\"https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/\">decomposition</a> into Trend, seasonality and residuals."},{"metadata":{"trusted":true},"cell_type":"code","source":"result = seasonal_decompose(data.values, freq=12, model='additive')\nplot_seasonal_decompose(result, title='additive')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = seasonal_decompose(data.values, freq=12, model='multiplicative')\nplot_seasonal_decompose(result, title='multiplicative')\ndel data, result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Number of items per category**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = items.groupby('item_category_id')['item_id'].count().sort_values(ascending=False).reset_index(name='item_count')\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x='item_category_id', y='item_count', data=data[:10])\nplt.show()\ndel data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Top sold items**"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_items = sales.groupby('item_id').sum()['item_cnt_day'].sort_values(ascending=False)\ntop_items = pd.merge(top_items, items2, how='left', on='item_id')\n\nsns.barplot(y='item_name_translated', x='item_cnt_day', data=top_items[:10])\ndel top_items","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Top sold item-categories**"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_categories = pd.merge(sales, items, how='left', on='item_id').groupby('item_category_id').sum()['item_cnt_day'].sort_values(ascending=False)\ntop_categories = pd.merge(top_categories, item_categories2, how='left', on='item_category_id')\n\nsns.barplot(y='item_category_name_translated', x='item_cnt_day', data=top_categories[:10])\ndel top_categories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Top sold item-category-classes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_category_classes = pd.merge(sales, items, how='left', on='item_id').groupby('item_category_id').sum()['item_cnt_day'].sort_values(ascending=False)\ntop_category_classes = pd.merge(top_category_classes, pd.merge(item_categories2, item_categories_classes, how='left', on='item_class_id'), how='left', on='item_category_id')\ntop_category_classes = top_category_classes.groupby('item_class_name').sum().sort_values('item_cnt_day', ascending=False).reset_index()\n\nsns.barplot(y='item_class_name', x='item_cnt_day', data=top_category_classes)\ndel top_category_classes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Top sold shops**"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_shops = sales.groupby('shop_id').sum()['item_cnt_day'].sort_values(ascending=False)\ntop_shops = pd.merge(top_shops, shops2, how='left', on='shop_id')\n\nsns.barplot(y='shop_name_translated', x='item_cnt_day', data=top_shops[:10])\ndel top_shops","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Top sold shop-locations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_shops = sales.groupby('shop_id').sum()['item_cnt_day']\ntop_shops = pd.merge(top_shops, shops2, how='left', on='shop_id')\ntop_shop_locations = top_shops.groupby('shop_location').sum().sort_values('item_cnt_day', ascending=False).reset_index()\n\nsns.barplot(y='shop_location', x='item_cnt_day', data=top_shop_locations[:10])\ndel top_shops, top_shop_locations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Outliers**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 4))\nsns.boxplot(sales['item_cnt_day'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 4))\nsns.boxplot(sales['item_price'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aggregate data"},{"metadata":{"trusted":true},"cell_type":"code","source":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in tqdm_notebook(sales['date_block_num'].unique()):\n    cur_shops = sales[sales['date_block_num'] == block_num]['shop_id'].unique()\n    cur_items = sales[sales['date_block_num'] == block_num]['item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])), dtype='int32'))\n\n# turn the grid into pandas dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns=index_cols, dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates\ngb = sales.groupby(index_cols, as_index=False).agg({'item_cnt_day': 'sum', 'item_price': 'mean'})\n# fix column names\ngb.rename(columns={'item_cnt_day': 'target', 'item_price': 'price_target'}, inplace=True)\n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n# Same as above but with shop-month aggregates\ngb = sales.groupby(['shop_id', 'date_block_num'], as_index=False).agg({'item_cnt_day': 'sum', 'item_price': 'mean'})\ngb.rename(columns={'item_cnt_day': 'target_shop', 'item_price': 'price_target_shop'}, inplace=True)\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n# Same as above but with item-month aggregates\ngb = sales.groupby(['item_id', 'date_block_num'], as_index=False).agg({'item_cnt_day': 'sum', 'item_price': 'mean'})\ngb.rename(columns={'item_cnt_day': 'target_item', 'item_price': 'price_target_item'}, inplace=True)\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\n# Downcast dtypes from 64 to 32 bit to save memory\nall_data = downcast_dtypes(all_data)\ndel grid, gb\ngc.collect();\n\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combining <b>all_data</b> with <b>test</b>."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ = test.drop(columns=['ID'])\ntest_['date_block_num'] = 34\nall_data = pd.concat([all_data, test_])\ndel test_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Lag features\nWe will use lags from [1, 2, 3, 4, 5, 6, 11, 12] months ago."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_rename = list(all_data.columns.difference(index_cols))\n\nshift_range = [1, 2, 3, 4, 5, 6, 11, 12]\n\nfor month_shift in tqdm_notebook(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    train_shift = train_shift.rename(columns=lambda c: f'{c}_lag_{month_shift}' if c in cols_to_rename else c)\n    \n    all_data = all_data.merge(train_shift, how='left', on=index_cols).fillna(0)\n    \ndel train_shift\nlag_features = [c for c in all_data.columns if c[-1] in [str(x) for x in shift_range]]\n\n# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Merge <b>all_data</b> with <b>items</b> for <b>item_category_id</b>.\n\n2. Merge <b>all_data</b> with <b>item_categories2</b> for <b>item_class_id</b>.\n\n3. Merge <b>all_data</b> with <b>shops2</b> for <b>shop_location_id</b>."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1\nall_data = pd.merge(all_data, items[['item_id', 'item_category_id']], how='left', on='item_id')\n# 2\nall_data = pd.merge(all_data, item_categories2[['item_category_id', 'item_class_id']], how='left', on='item_category_id')\n# 3\nshops2['shop_location_id'] = pd.factorize(shops2['shop_location'])[0]\nall_data = pd.merge(all_data, shops2[['shop_id', 'shop_location_id']], how='left', on='shop_id')\n\nall_data = downcast_dtypes(all_data)\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(all_data.shape)\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_enc_cv(df, col, target, n_splits=5):\n    df[f'{col}_{target}_mean_enc'] = np.nan\n    \n    kf = KFold(n_splits=n_splits, shuffle=False)\n\n    for train_index, val_index in tqdm_notebook(kf.split(df)):\n        X_train, X_val = df.iloc[train_index], df.iloc[val_index]\n        means = X_train.groupby(col)[target].mean()\n        X_val[f'{col}_{target}_mean_enc'] = X_val[col].map(means)\n        df.iloc[val_index] = X_val\n        \n    # Fill NaNs\n    df[f'{col}_{target}_mean_enc'] = df[f'{col}_{target}_mean_enc'].fillna(df[target].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"mean_enc_cv(all_data, 'item_id', 'target')\nmean_enc_cv(all_data, 'item_id', 'price_target')\nmean_enc_cv(all_data, 'item_id', 'target_shop')\nmean_enc_cv(all_data, 'shop_id', 'price_target')\nmean_enc_cv(all_data, 'item_category_id', 'target')\nmean_enc_cv(all_data, 'item_category_id', 'price_target')\nmean_enc_cv(all_data, 'item_id', 'price_target_item')\nmean_enc_cv(all_data, 'item_class_id', 'target')\nmean_enc_cv(all_data, 'item_class_id', 'target_shop')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bag of Words: TFiDF (item name)"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_features = 25\n\nvectorizer = TfidfVectorizer(max_features=tfidf_features)\nvectors = vectorizer.fit_transform(items['item_name'])\n\nitems_tfidf = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())\nitems_tfidf = items_tfidf.rename(columns=lambda c: 'item_name_tfidf_' + c)\nitems_tfidf['item_id'] = items['item_id']\n\nall_data = pd.merge(all_data, items_tfidf, how='left', on='item_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = downcast_dtypes(all_data)\nprint(all_data.shape)\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train/test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect();\ndates = all_data['date_block_num']\n\n# Drop all target cols\nto_drop_cols = ['target_shop', 'target_item', 'price_target', 'price_target_shop', 'price_target_item']\ntraintest = all_data.drop(columns=to_drop_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_features = [c for c in all_data.columns if c.find('tfidf') != -1]\n\n# Renaming 'traintest' for \"not support non-ASCII characters\" issue\nrename_tfidf = {}\nfor i, col in enumerate(tfidf_features):\n    new_col = ''\n    for s in col.split('_')[:-1]:\n        new_col += s + '_'\n    new_col += str(i)\n    rename_tfidf[col] = new_col\n\ntraintest.rename(columns=rename_tfidf, inplace=True)\ntfidf_features = list(rename_tfidf.values())\ndel rename_tfidf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check for constant features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"selector = VarianceThreshold(threshold=0.0)\nselector.fit(traintest[dates <= 33])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traintest.columns[~selector.get_support()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traintest.columns[traintest.nunique() == 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's none."},{"metadata":{},"cell_type":"markdown","source":"#### Selecting Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\nfeatures_not_used = index_cols + ['item_category_id',\n                                 'item_class_id',\n                                 'shop_location_id']\n\n# Top tfidf features\ntfidf_features = ['item_name_tfidf_24',\n                 'item_name_tfidf_12',\n                 'item_name_tfidf_21',\n                 'item_name_tfidf_16',\n                 'item_name_tfidf_7']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use top 30 features\nfeatures = ['target_item_lag_1', 'item_id_target_mean_enc',\n           'shop_id_price_target_mean_enc', 'target_lag_1',\n           'item_id_target_shop_mean_enc',\n           'item_category_id_price_target_mean_enc',\n           'item_class_id_target_mean_enc', 'item_category_id_target_mean_enc',\n           'item_class_id_target_shop_mean_enc', 'item_id_price_target_mean_enc',\n           'item_id_price_target_item_mean_enc', 'price_target_item_lag_1',\n           'target_lag_2', 'target_item_lag_2', 'target_shop_lag_2',\n           'target_item_lag_3', 'price_target_item_lag_2', 'price_target_lag_1',\n           'target_item_lag_4', 'price_target_shop_lag_1', 'target_lag_3',\n           'target_shop_lag_12', 'price_target_shop_lag_4', 'target_lag_5',\n           'price_target_item_lag_3', 'target_lag_6', 'price_target_shop_lag_2',\n           'target_item_lag_6', 'target_shop_lag_1', 'target_lag_4'] + ['target']\n\nfeatures += tfidf_features\n\nfeatures = set(features).difference(features_not_used)\n\ntraintest = traintest[features]\ntraintest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = traintest[dates <= 33]\nX_test = traintest[dates == 34].drop(columns=['target'])\n\nprint(train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(columns=['target'])\ny = train['target']\n\nprint(X.shape, y.shape)\ndel train, traintest\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validation\n<b>Time based validation</b> works most reliability in this case."},{"metadata":{"trusted":true},"cell_type":"code","source":"def cv_time(X, y, model, date_block_nums):\n    train_scores = []\n    val_scores = []\n    \n    for date_block_num in tqdm_notebook(date_block_nums):\n        X_train, y_train = X[dates < date_block_num], y[dates < date_block_num]\n        X_val, y_val = X[dates == date_block_num], y[dates == date_block_num]\n        \n        y_train = y_train.clip(0, 20)\n        lb, ub = np.percentile(y_val, (0.05, 99.9))\n        y_val = y_val.clip(lb, ub)\n        \n        model.fit(X_train, y_train)\n        \n        train_scores.append(rmse(y_train, model.predict(X_train) ))\n        val_scores.append(rmse(y_val, model.predict(X_val) ))\n    \n    train_scores = np.array(train_scores)\n    val_scores = np.array(val_scores)\n    print('Train scores :', train_scores)\n    print('Mean :', train_scores.mean())\n    print()\n    print('Val scores :', val_scores)\n    print('Mean :', val_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{},"cell_type":"markdown","source":"#### LGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_time(X, y, model=lgb.LGBMRegressor(), date_block_nums=[22, 33])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgb = lgb.LGBMRegressor(n_estimators=200, \n                              reg_alpha=0.01)\nmodel_lgb.fit(X, y.clip(0, 20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model_lgb.predict(X_test)\nsubmit(y_pred, fname='submission-lgb-1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Public and private LB scores were: **0.932847 and 0.939523**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}