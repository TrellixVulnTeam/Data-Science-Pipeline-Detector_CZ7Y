{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-08T15:33:38.725026Z","iopub.execute_input":"2022-05-08T15:33:38.725988Z","iopub.status.idle":"2022-05-08T15:33:39.883214Z","shell.execute_reply.started":"2022-05-08T15:33:38.725867Z","shell.execute_reply":"2022-05-08T15:33:39.882508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"一、数据源读取并检查异常值","metadata":{}},{"cell_type":"code","source":"#read all datas\nos.chdir(r'../input/competitive-data-science-predict-future-sales')\nitem_categories = pd.read_csv('item_categories.csv')\nitems = pd.read_csv('items.csv')#\nsales_train = pd.read_csv('sales_train.csv')\nsample_submission = pd.read_csv('sample_submission.csv')\nshops = pd.read_csv('shops.csv')\ntest = pd.read_csv('test.csv')\ntrain = sales_train.join(items, on='item_id', rsuffix='_').join(shops, on='shop_id', rsuffix='_').join(item_categories, on='item_category_id', rsuffix='_').drop(['item_id_', 'shop_id_', 'item_category_id_'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:33:59.555974Z","iopub.execute_input":"2022-05-08T15:33:59.556505Z","iopub.status.idle":"2022-05-08T15:34:04.307129Z","shell.execute_reply.started":"2022-05-08T15:33:59.556451Z","shell.execute_reply":"2022-05-08T15:34:04.30618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x= train['item_cnt_day']\nplt.boxplot(x)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:34:09.33059Z","iopub.execute_input":"2022-05-08T15:34:09.330934Z","iopub.status.idle":"2022-05-08T15:34:09.874722Z","shell.execute_reply.started":"2022-05-08T15:34:09.330898Z","shell.execute_reply":"2022-05-08T15:34:09.874136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x= train['item_price']\nplt.boxplot(x)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:34:11.95508Z","iopub.execute_input":"2022-05-08T15:34:11.95572Z","iopub.status.idle":"2022-05-08T15:34:12.465741Z","shell.execute_reply.started":"2022-05-08T15:34:11.955679Z","shell.execute_reply":"2022-05-08T15:34:12.464641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.jointplot(x='item_price',y='item_cnt_day',data=train,height=8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:34:14.85054Z","iopub.execute_input":"2022-05-08T15:34:14.850917Z","iopub.status.idle":"2022-05-08T15:35:17.160291Z","shell.execute_reply.started":"2022-05-08T15:34:14.850878Z","shell.execute_reply":"2022-05-08T15:35:17.159341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#1、去除无效多余数据\n#剔除异常数据并按照月份聚合\ntest_shop_ids=test['shop_id'].unique()\ntest_item_ids = test['item_id'].unique()\nlk_train = train[train['shop_id'].isin(test_shop_ids)]\nlk_train = lk_train[lk_train['item_id'].isin(test_item_ids)]\nlk_train=lk_train[(lk_train['item_cnt_day']<=20)&(lk_train['item_cnt_day']>=0)]\nlk_train=lk_train[lk_train.item_price<40000]\ntrain_monthly = lk_train[['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'item_cnt_day']]\ntrain_monthly['date']=pd.to_datetime(train_monthly.date)\ntrain_monthly = train_monthly.sort_values('date').groupby(['date_block_num', 'shop_id', 'item_category_id', 'item_id'], as_index=False)\ntrain_monthly = train_monthly.agg({ 'item_price':['mean'],#当月商店的均值\n    'item_cnt_day':['sum']})\ntrain_monthly.columns = ['date_block_num', 'shop_id', 'item_category_id', 'item_id','item_price','item_cnt']\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:41:59.281837Z","iopub.execute_input":"2022-05-08T15:41:59.282254Z","iopub.status.idle":"2022-05-08T15:42:00.825644Z","shell.execute_reply.started":"2022-05-08T15:41:59.282192Z","shell.execute_reply":"2022-05-08T15:42:00.824819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3、构造补全完整的所有门店34个月里面，月份—商店-商品的组合\nshop_ids = train_monthly['shop_id'].unique()\nitem_ids = train_monthly['item_id'].unique()\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])#更改列名\ntrain_monthly = pd.merge(empty_df, train_monthly, on=['date_block_num','shop_id','item_id'], how='left')#合并\ntrain_monthly.fillna(0, inplace=True)#缺失数据填充\ntrain_monthly['year'] = train_monthly['date_block_num'].apply(lambda x: ((x//12) + 2013))#提取年份\ntrain_monthly['month'] = train_monthly['date_block_num'].apply(lambda x: (x % 12))#提取月份\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"二、EDTA分析销量与各个特征的关联性","metadata":{}},{"cell_type":"code","source":"#2、数据EDTA :进行数据探索，分别按照month,category和shop分组，得到各组的销量(求各个维度下销量和均值)画图查看各个指标的相关性情况\n#销量月月份间关系\ngp_month_mean = train_monthly.groupby(['date_block_num'], as_index=False)['item_cnt'].mean()\ngp_month_sum = train_monthly.groupby(['date_block_num'], as_index=False)['item_cnt'].sum()\nf, axes = plt.subplots(2, 1, figsize=(15, 8), sharex=True)\nsns.lineplot(x=\"date_block_num\", y=\"item_cnt\", data=gp_month_mean, ax=axes[0]).set_title(\"Monthly mean\")\nsns.lineplot(x=\"date_block_num\", y=\"item_cnt\", data=gp_month_sum, ax=axes[1]).set_title(\"Monthly sum\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:38:46.924076Z","iopub.execute_input":"2022-05-08T15:38:46.924524Z","iopub.status.idle":"2022-05-08T15:38:47.052633Z","shell.execute_reply.started":"2022-05-08T15:38:46.924488Z","shell.execute_reply":"2022-05-08T15:38:47.051169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#销量与品类间关系\ngp_category_mean = train_monthly.groupby(['item_category_id'], as_index=False)['item_cnt'].mean()\ngp_category_sum = train_monthly.groupby(['item_category_id'], as_index=False)['item_cnt'].sum()\nf, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\nsns.barplot(x=\"item_category_id\", y=\"item_cnt\", data=gp_category_mean, ax=axes[0], palette=\"rocket\").set_title(\"Monthly mean\")\nsns.barplot(x=\"item_category_id\", y=\"item_cnt\", data=gp_category_sum, ax=axes[1], palette=\"rocket\").set_title(\"Monthly sum\")\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#销量与商店的关系\ngp_shop_mean = train_monthly.groupby(['shop_id'], as_index=False)['item_cnt'].mean()\ngp_shop_sum = train_monthly.groupby(['shop_id'], as_index=False)['item_cnt'].sum()\nf, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\nsns.barplot(x=\"shop_id\", y=\"item_cnt\", data=gp_shop_mean, ax=axes[0], palette=\"rocket\").set_title(\"Monthly mean\")\nsns.barplot(x=\"shop_id\", y=\"item_cnt\", data=gp_shop_sum, ax=axes[1], palette=\"rocket\").set_title(\"Monthly sum\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"三、特征方程构造","metadata":{}},{"cell_type":"code","source":"#4、特征构造\n#①构造预测值\ntrain_monthly['item_cnt_month'] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_id'])['item_cnt'].shift(-1) \n#②价格特征（价格与销量的关系使用价格//销量）#最大值、最小值、增量、减量、平均单价、零售价均值\ntrain_monthly['item_price_unit'] = train_monthly['item_price'] // train_monthly['item_cnt']#当月该商店该商品的总销售价//总销量\ntrain_monthly['item_price_unit'].fillna(0, inplace=True)\nitem_price=lk_train.sort_values('date_block_num').groupby(['item_id'],as_index=False).agg({'item_price':['max','min','mean']})\nitem_price.columns=['item_id','item_price_max','item_price_min','item_price_mean']\ntrain_monthly=pd.merge(train_monthly,item_price,how='left',on=['item_id'])\ntrain_monthly['price_increase']=train_monthly['item_price']-train_monthly['item_price_min']\ntrain_monthly['price_decrease']=train_monthly['item_price_max']-train_monthly['item_price']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#③构造销量的时间窗特征\n#时间窗可以对数据起到平滑的效果，同时也包含了一定的历史信息。这里我们用时窗构造出min,max,mean以及std特征，并对缺失数据进行零填充\n#DataFrame.rolling(window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None)设置窗口为3，偏移量1\n#③-①时间窗口构造\nf_min = lambda x: x.rolling(window=3, min_periods=1).min()\n# Max value\nf_max = lambda x: x.rolling(window=3, min_periods=1).max()\n# Mean value\nf_mean = lambda x: x.rolling(window=3, min_periods=1).mean()\n# Standard deviation\nf_std = lambda x: x.rolling(window=3, min_periods=1).std()\nfunction_name = ['min', 'max', 'mean', 'std']\nfunction_list = [f_min, f_max, f_mean, f_std]\nfor i in range(len(function_list)):\n    train_monthly[('item_cnt_%s' % function_name[i])] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].apply(function_list[i])\n    train_monthly[('item_cnt_%s' % function_name[i])].fillna(0, inplace=True)\n    # Fill the empty std features with 0\n\n##③-②构造滞后历史特征，将历史三个月的数据平移\nlag_list = [1, 2, 3]\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly[ft_name] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].shift(lag)\n    # Fill the empty shifted features with 0\n    train_monthly[ft_name].fillna(0, inplace=True)\n#③-③构造销量变化特征，通过计算滞后历史特征的变化量来得出：即计算当月往前推3个月的平均每个月的销量变化值（可尝试成环比变化率试一试）\ntrain_monthly['item_trend'] = train_monthly['item_cnt']\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly['item_trend'] =train_monthly['item_trend']- train_monthly[ft_name]\ntrain_monthly['item_trend'] = train_monthly['item_trend']/(len(lag_list) + 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"四、数据集合划分并处理","metadata":{}},{"cell_type":"code","source":"#5、划分测试集、验证集、训练集合\n#训练集\ntrain_set = train_monthly[(train_monthly.date_block_num>=3)&(train_monthly.date_block_num<28)]\ntrain_set.dropna(subset=['item_cnt_month'], inplace=True)\ntrain_set.dropna(inplace=True)\n#验证集\nvalidation_set = train_monthly[(train_monthly.date_block_num>=28)&(train_monthly.date_block_num<=33)]\nvalidation_set.dropna(subset=['item_cnt_month'], inplace=True)\nvalidation_set.dropna(inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#6、训练集测试集合增加其他常规特征\n#商店单款销量均值\ngp_shop_mean=train_set.groupby(['shop_id'],as_index=False).agg({'item_cnt_month':'mean'})\ngp_shop_mean.columns = ['shop_id','shop_mean']\n#品类销量均值\ngp_category_mean=train_set.groupby(['item_category_id'],as_index=False).agg({'item_cnt_month':'mean'})\ngp_category_mean.columns = ['item_category_id','category_mean']\n#商店品类销量均值\ngp_shop_category_mean=train_set.groupby(['shop_id','item_category_id'],as_index=False).agg({'item_cnt_month':'mean'})\ngp_shop_category_mean.columns = ['shop_id','item_category_id','shop_category_mean']\n#商品销量均值\ngp_item_mean = train_set.groupby(['item_id'],as_index=False).agg({'item_cnt_month': ['mean']})\ngp_item_mean.columns = ['item_id','item_mean']\n#商品商店\ngp_shop_item_mean = train_set.groupby(['shop_id', 'item_id'],as_index=False).agg({'item_cnt_month': ['mean']})\ngp_shop_item_mean.columns = ['shop_id', 'item_id','shop_item_mean']\n#每年销量均值\ngp_year_mean = train_set.groupby(['year'],as_index=False).agg({'item_cnt_month': ['mean']})\ngp_year_mean.columns = ['year','year_mean']\n#每月销量均值\ngp_month_mean = train_set.groupby(['month'],as_index=False).agg({'item_cnt_month': ['mean']})\ngp_month_mean.columns = ['month','month_mean']\n#每个月品类销量均值\ngp_month_category_mean=train_set.groupby(['month','item_category_id'],as_index=False).agg({'item_cnt_month':'mean'})\ngp_month_category_mean.columns = ['month','item_category_id','month_category_mean']\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#将训练集与验证集合与测试集特征合并\ntrain_set = pd.merge(train_set, gp_shop_mean, on=['shop_id'], how='left')\ntrain_set = pd.merge(train_set, gp_category_mean, on=['item_category_id'], how='left')\ntrain_set = pd.merge(train_set, gp_shop_category_mean, on=['shop_id','item_category_id'], how='left')\ntrain_set = pd.merge(train_set, gp_item_mean, on=['item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_year_mean, on=['year'], how='left')\ntrain_set = pd.merge(train_set, gp_month_mean, on=['month'], how='left')\ntrain_set = pd.merge(train_set, gp_month_category_mean, on=['month','item_category_id'], how='left')\n#验证及与特征集合并\nvalidation_set = pd.merge(validation_set, gp_shop_mean, on=['shop_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_category_mean, on=['item_category_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_shop_category_mean, on=['shop_id','item_category_id'], how='left')\nvalidation_set= pd.merge(validation_set, gp_item_mean, on=['item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_year_mean, on=['year'], how='left')\nvalidation_set= pd.merge(validation_set, gp_month_mean, on=['month'], how='left')\nvalidation_set= pd.merge(validation_set, gp_month_category_mean, on=['month','item_category_id'], how='left')\n#分离出训练集和验证集的输入X和输出Y\n#训练集\nX_train = train_set.drop(['item_cnt_month', 'date_block_num'], axis=1)#删除月份和月份排序列\nY_train = train_set['item_cnt_month'].astype(int)#获取销量为输出Y值\n#测试集\nX_validation = validation_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\nY_validation = validation_set['item_cnt_month'].astype(int)\n#把商店代码、商品代码、月份、年份转换为数值型\nint_features = ['shop_id', 'item_id', 'year', 'month']\nX_train[int_features] = X_train[int_features].astype('int32')\nX_validation[int_features] = X_validation[int_features].astype('int32')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#7、测试集处理\n#取最后一次记录的商店与商品组合的交易记录特征\nlatest_records = pd.concat([train_set, validation_set]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\nlatest_records.drop('item_cnt_month', axis=1, inplace=True)\nX_test = pd.merge(test, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])  \nX_test['year'] = 2015\nX_test['month'] = 9\nX_test[int_features] = X_test[int_features].astype('int32')\nX_test = X_test[X_train.columns]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#8、测试集处理\n#取最后一次记录的商店与商品组合的交易记录特征最为测试集特征\nlatest_records = pd.concat([train_set, validation_set]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\nlatest_records.drop('item_cnt_month', axis=1, inplace=True)\nX_test = pd.merge(test, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])  \nX_test['year'] = 2015\nX_test['month'] = 9\nX_test[int_features] = X_test[int_features].astype('int32')\nX_test = X_test[X_train.columns]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#9、空值处理\n#检查空值并进行填充\nX_test.dtypes\nX_validation.dtypes\nX_train.dtypes\nsets = [X_validation, X_test]          \nfor dataset in sets:\n    for shop_id in dataset['shop_id'].unique():\n        for column in dataset.columns:\n            shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n            dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"五、算法预测并计算得分","metadata":{}},{"cell_type":"code","source":"#算法一、CatBoost\n#此算法优点：不用手动处理类别型特征了、使用了组合类别特征、对称树避免过拟合\nimport xgboost\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport catboost as cb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import recall_score\n# cat_feat表示原始数据的哪几列为类别特征，类别特征需要进行str格式转换\ncat_features = ['shop_id','item_id', 'item_category_id', 'year', 'month']\ncat_X_train=X_train\ncat_X_validation=X_validation\ncat_X_test=X_test\ncat_X_train[cat_features] = cat_X_train[cat_features].astype(str)\ncat_X_validation[cat_features] = cat_X_validation[cat_features].astype(str)\ncat_X_test[cat_features] = cat_X_test[cat_features].astype(str)\n#参数设置\ncatboost_model = cb.CatBoostRegressor(\n    iterations=500,\n    max_ctr_complexity=4,\n    random_seed=0,\n    od_type='Iter',\n    od_wait=25,\n    verbose=50,\n    depth=4)\n#训练模型\ncatboost_model.fit(\n    X_train, Y_train,\n    cat_features=cat_features,\n    eval_set=(X_validation, Y_validation))\n#预测\ncat_train_pred = catboost_model.predict(cat_X_train)\ncat_val_pred = catboost_model.predict(cat_X_validation)\ncat_test_pred = catboost_model.predict(cat_X_test)\n#画图查看特征重要性分布\nfeature_score = pd.DataFrame(list(zip(X_train.columns,catboost_model.feature_importances_)), \n                           columns=['Feature','Score'])\nfeature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False)#排序\nplt.rcParams[\"figure.figsize\"] = (19, 6)\nax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\nrects = ax.patches\nlabels = feature_score['Score'].round(2)\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 0.35, label, ha='center', va='bottom')\nplt.show()\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_train, cat_train_pred))) #测试集均方根误差\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, cat_val_pred)))#验证集均方根误差\nprint('The score of cat prediction is:', catboost_model.score(cat_X_train,Y_train)) # 训练集训练结果得分\nprint('The recall_score of cat prediction is:', recall_score(Y_train, np.rint(cat_train_pred),average='weighted')) # 训练集回召率","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 算法二Use only part of features on XGBoost.\nimport xgboost as xgb\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import plot_importance\nxgb_features = ['item_cnt','item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', \n                'item_cnt_shifted2', 'item_cnt_shifted3', 'shop_mean', \n                'shop_item_mean', 'item_trend', 'month_mean','month_category_mean','item_mean']\n\nxgb_train = X_train[xgb_features]\nxgb_val = X_validation[xgb_features]\nxgb_test = X_test[xgb_features]\nxgb_model = xgb.XGBRegressor(max_depth=8, \n                         n_estimators=500, \n                         min_child_weight=1000,  \n                         colsample_bytree=0.7, \n                         subsample=0.7, \n                         eta=0.3, \n                         seed=0)\nxgb_model.fit(xgb_train, \n              Y_train, \n              eval_metric=\"rmse\", #eval_metric=[\"auc\",“rmse”,\"logloss\"]\n              eval_set=[(xgb_train, Y_train), (xgb_val, Y_validation)], \n              verbose=20, \n              early_stopping_rounds=20)\n\nxgb_train_pred =xgb_model.predict(xgb_train)\nxgb_val_pred =xgb_model.predict(xgb_val)\nxgb_test_pred =xgb_model.predict(xgb_test)\n\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_train, xgb_train_pred))) #测试集均方根误差\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, xgb_val_pred)))#验证集均方根误差\nprint('The score of cat prediction is:', xgb_model.score(xgb_train_pred,Y_train)) # 训练集训练结果得分\nprint('The recall_score of cat prediction is:', recall_score(Y_train, np.rint(xgb_train_pred),average='weighted')) # 训练集回召率\n#画图查看特征重要性分布\nfeature_score = pd.DataFrame(list(zip(X_train.columns,xgb_model.feature_importances_)), \n                           columns=['Feature','Score'])\nfeature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False)#排序\nplt.rcParams[\"figure.figsize\"] = (6, 6)\nax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\nax.set_xlabel('')\nrects = ax.patches\nlabels = feature_score['Score'].round(2)\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width(), height, label, ha='center', va='bottom')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#算法三随机森林RF\nfrom sklearn.ensemble import RandomForestRegressor\n# Use only part of features on random forest.\nrf_features = ['shop_id', 'item_id', 'item_cnt',\n               'item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', \n               'shop_mean', 'item_mean', 'item_trend']\nrf_train = X_train[rf_features]\nrf_val = X_validation[rf_features]\nrf_test = X_test[rf_features]\nrf_model = RandomForestRegressor(n_estimators=50, max_depth=7, random_state=0, n_jobs=-1)\nrf_model.fit(rf_train, Y_train)\nrf_train_pred = rf_model.predict(rf_train)\nrf_val_pred = rf_model.predict(rf_val)\nrf_test_pred = rf_model.predict(rf_test)\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_train, rf_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, rf_val_pred)))\nprint('The score of rf prediction is:', rf_model.score(rf_train,Y_train)) # 训练集训练结果得分\nprint('The recall_score of cat prediction is:', recall_score(Y_train, np.rint(rf_train_pred),average='weighted')) # 训练集回召率","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#算法四：LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nlr_features = ['item_cnt', 'item_cnt_shifted1', 'item_trend', 'item_mean', 'shop_mean']\nlr_train = X_train[lr_features]\nlr_val = X_validation[lr_features]\nlr_test = X_test[lr_features]\nlr_scaler = MinMaxScaler()\nlr_scaler.fit(lr_train)\nlr_train = lr_scaler.transform(lr_train)\nlr_val = lr_scaler.transform(lr_val)\nlr_test = lr_scaler.transform(lr_test)\nlr_model = LinearRegression(n_jobs=-1)\nlr_model.fit(lr_train, Y_train)\nlr_train_pred=lr_model.predict(lr_train)\nlr_val_pred=lr_model.predict(lr_val)\nlr_test_pred=lr_model.predict(lr_test)\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_train, lr_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, lr_val_pred)))\nprint('The score of lr prediction is:', lr_model.score(lr_train,Y_train)) # 训练集训练结果得分\nprint('The recall_score of cat prediction is:', recall_score(Y_train, np.rint(lr_train_pred),average='weighted')) # 训练集回召率","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#算法五：KNN（无监督聚类）\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_features = ['item_cnt', 'item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1',\n                'item_cnt_shifted2', 'shop_mean', 'shop_item_mean', \n                'item_trend', 'item_mean']\n\n# Subsample train set (using the whole data was taking too long).\nknn_train = X_train[knn_features]\nknn_val = X_validation[knn_features]\nknn_test = X_test[knn_features]\n \nknn_scaler = MinMaxScaler()\nknn_scaler.fit(knn_train)\nknn_train = knn_scaler.transform(knn_train)\nknn_val = knn_scaler.transform(knn_val)\nknn_test = knn_scaler.transform(knn_test)\nknn_model = KNeighborsClassifier(n_neighbors=9, leaf_size=13, n_jobs=-1)\n\nknn_model.fit(knn_train, Y_train)\n\nknn_train_pred=lr_model.predict(lr_train)\nknn_val_pred=lr_model.predict(lr_val)\nknn_test_pred=lr_model.predict(lr_test)\n\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_train, knn_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, knn_val_pred)))\nprint('The score of knn prediction is:', knn_model.score(knn_model,Y_train)) # 训练集训练结果得分\nprint('The recall_score of cat prediction is:', recall_score(Y_train, np.rint(knn_train_pred),average='weighted')) # 训练集回召率\n","metadata":{},"execution_count":null,"outputs":[]}]}