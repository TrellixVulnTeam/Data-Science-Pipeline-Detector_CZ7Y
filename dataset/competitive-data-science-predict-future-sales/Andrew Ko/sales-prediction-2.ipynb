{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import os \nprint(os.listdir(\"../input/competitive-data-science-predict-future-sales/\"))\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom matplotlib import pylab as plt\nimport matplotlib.dates as mdates\n\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv', dtype={'ID': 'int32', 'shop_id': 'int32', \n                                                  'item_id': 'int32'})\nitem_categories = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv', \n                              dtype={'item_category_name': 'str', 'item_category_id': 'int32'})\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv', dtype={'item_name': 'str', 'item_id': 'int32', \n                                                 'item_category_id': 'int32'})\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv', dtype={'shop_name': 'str', 'shop_id': 'int32'})\nsales = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv', \n                    dtype={'date': 'str', 'date_block_num': 'int32', 'shop_id': 'int32', \n                          'item_id': 'int32', 'item_price': 'float32', 'item_cnt_day': 'int32'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MLP & TSM\n這次的目的是基於上次的EDA以及xgboost的結果，使用不同的方式進行預測。\n首先在上次的每日750消費筆數的限制再加上金額不得超過300000的限制（有固定高單價品項干擾）\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove outliers\nsales = sales[(sales.item_price < 300000 ) & (sales.item_cnt_day < 750)]\n# sales = sales[(sales.item_price < 300000 ) & (sales.item_cnt_day < 1000)]\ntrain = sales.drop(labels = ['date', 'item_price'], axis = 1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_clean = pd.merge(train, items.drop(columns=['item_name']), on=['item_id'])\ntrain_clean = train_clean.groupby([\"item_id\",\"shop_id\",\"date_block_num\",'item_category_id']).sum().reset_index()\ntrain_clean = train_clean.rename(index=str, columns = {\"item_cnt_day\":\"item_cnt_month\"})\ntrain_clean = train_clean[[\"item_id\",\"shop_id\",\"date_block_num\",'item_category_id',\"item_cnt_month\"]]\ntrain_clean.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 計算每日消費量"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_clean.pivot_table(index = ['shop_id','item_id'],values = ['item_cnt_month'],columns = ['date_block_num'],fill_value = 0,aggfunc='sum')\ntrain_data.reset_index(inplace = True)\ntrain_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.merge(test,train_data,on = ['item_id','shop_id'],how = 'left')\nall_data.fillna(0,inplace = True)\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_sequences(sequences, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        end_ix = i + n_steps\n        if end_ix > len(sequences):\n            break\n        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 和上一份預測走相反方向，僅使用歷史消費數量進行預測"},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_samples = [[0] * 34] * 2\nall_data.drop(['ID','shop_id','item_id'],inplace = True, axis = 1)\nall_data.columns = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33]\n# create dummy_samples dataframe and concatenate with all_data\nall_data = pd.concat([pd.DataFrame(dummy_samples), all_data], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = np.expand_dims(all_data.values[:,:-2],axis = 2)\nvalidation_data = np.expand_dims(all_data.values[:,1:-1],axis = 2)\ntest_data = np.expand_dims(all_data.values[:,2:],axis = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import array\nn_steps = 5\nX_train, y = split_sequences(train_data, n_steps)\nX_val, y_val = split_sequences(validation_data, n_steps)\nX_test, y_test = split_sequences(test_data, n_steps)\nprint(X_train.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_input = X_train.shape[1] * X_train.shape[2]\n# reshape to 2 dimensional input\nX = X_train.reshape((X_train.shape[0], n_input))\nX_val = X_val.reshape((X_val.shape[0], n_input))\nX_test = X_test.reshape((X_test.shape[0], n_input))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 建立基本的3層MLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport  tensorflow.keras.optimizers as optimizers\n# define model\nmodel = Sequential()\nmodel.add(Dense(128, activation= 'relu', input_dim=n_input))\nmodel.add(Dense(64, activation= 'relu' ))\nmodel.add(Dense(10))\nmodel.compile(optimizer=optimizers.Adam(lr=.0001), loss= 'mse', metrics = ['mean_squared_error'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\ncallbacks = [\n    EarlyStopping(patience=5, verbose=1),\n    ReduceLROnPlateau(factor=0.25, patience=2, min_lr=0.000001, verbose=1),\n    ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X, y, epochs=15, callbacks=callbacks, validation_data=(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ALPHA = 0.5\nBETA = 0.0\nN_SESSIONS = 33 \n\ndef create_filtered_prediction(train_ts, alpha, beta):\n    train_time_filtered_ts = np.zeros((train_ts.shape[0], N_SESSIONS), dtype=np.float)\n    train_time_filtered_ts[0, :] = train_ts[0, :]\n    train_memontum_ts = np.zeros((train_ts.shape[0], N_SESSIONS), dtype=np.float)\n    prediction_ts = np.zeros((train_ts.shape[0], N_SESSIONS+1), dtype=np.float)\n    for i in range(1, N_SESSIONS):\n        train_time_filtered_ts[:, i] = (1-alpha) * (train_time_filtered_ts[:, i-1] + \\\n                                                    train_memontum_ts[:, i-1]) + alpha * train_ts[:, i]\n        train_memontum_ts[:, i] = (1-beta) * train_memontum_ts[:, i-1] + \\\n                                  beta * (train_time_filtered_ts[:, i] - train_time_filtered_ts[:, i-1])\n        prediction_ts[:, i+1] = train_time_filtered_ts[:, i] + train_memontum_ts[:, i]\n    return prediction_ts\n\nall_data_x = all_data.iloc[:,:-1]\nall_data_y = all_data.iloc[:,-1:]\n\ntrain_time_series_df = np.clip(all_data_x.values, 0, 20).astype(float)\npredictions = create_filtered_prediction(train_time_series_df, ALPHA, BETA)\n# validate prediction by predicting 34th month \n# all_data_x.size()\n# predictions\n# rmse(predictions, all_data_y)\nprint(\"rmse = \",np.sqrt(np.mean((predictions-all_data_y.to_numpy())**2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 結論\n\n這裡僅利用連續月份的銷售數量測試了兩種方式：\n1. 基本的NN (MLP)測試\n2. 利用time series memontum進行預測\n以結果來說rmse好像不是令人滿意的結果，未來希望將每一個item的類別等詳細資訊帶入以優化表現。\n以資料分布來說，針對特定類別的銷售數量標準化或許有機會得到更好的結果\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}