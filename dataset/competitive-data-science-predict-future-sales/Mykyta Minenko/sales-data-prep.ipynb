{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Disclaimer\n\n#### In this notebook I will be using some of useful functions and ideas which were presented by other kaggle users in the discussions and kernel sections.\n#### In order to make this notebook more readable I commented out all visualizations. If there is a need to see them, feel free to remove those comments. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install workalendar","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# imports\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom workalendar.europe import Russia\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\nimport calendar\nimport seaborn as sns\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nimport gc\nimport re\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\n\ntrain=pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv\")\ntest=pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/test.csv\")\nitems=pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\")\nitem_categories=pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv\")\nshops=pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets plot all columns in train (except index) \n\n\n#for i in train.columns[1:]:\n#    plt.figure(figsize=(10,4));\n#    sns.boxplot(train[i]);\n#    plt.title(i);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# there are some outliers, remove them\n\ntrain = train[(train[\"item_price\"] > 0 ) & (train[\"item_price\"] < 100000 ) & (train[\"item_cnt_day\"] > 0) & (train[\"item_cnt_day\"] < 1000)]\n#train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning shops dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# inspect shops dataset\n#shops","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# several shops are duplicates, remove them from test, train and shops\n\n# Якутск Орджоникидзе, 56\n\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n\n# Якутск ТЦ \"Центральный\"\n\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n\n# Жуковский ул. Чкалова 39м²\n\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11\n\n# remove from shops\n\nshops = shops[(shops[\"shop_id\"] > 1) & (shops[\"shop_id\"] != 10)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add shop city name and population (taken from wikipedia)\n\nshops[\"shop_city_population\"] = [\n    784048, 235336, 327356, 1018790, 1003638, 1003638, 1003638, -1, 106872, -1, 1176187, 1176187, 331351, 144707, 1016385, 1016385, 428741, 11979529,\n 11979529, 11979529, 11979529, 11979529, 11979529, 11979529, 11979529, 11979529, 11979529, 11979529, 11979529, 11979529, 178672, 1259921, 1259921, 1523801, 1523801, 1160670, \n    1103733, 1103733, 1103733, 5028000, 5028000, 1171598, 1171598, 108490, 325511, 547989, 634171, 634171, 634171, 1077719, 1077719, 221084, -1, 65359, 286456, 286456, 599169\n]\n\nshops['shop_name'] = shops['shop_name'].apply(lambda x: x.lower()).str.replace('[^\\w\\s]', '').str.replace('\\d+','').str.strip()\nshops['shop_city'] = shops['shop_name'].str.partition(' ')[0]\nshops.loc[:, \"shop_city\"] = LabelEncoder().fit_transform(shops[\"shop_city\"])\nshops['shop_type'] = shops['shop_name'].apply(lambda x: 'мтрц' if 'мтрц' in x else 'трц' if 'трц' in x else 'трк' if 'трк' in x else 'тц' if 'тц' in x else 'тк' if 'тк' in x else 'NaN')\nshops.loc[:, \"shop_type\"] = LabelEncoder().fit_transform(shops[\"shop_type\"])\n\n# drop text column\n\nshops.drop([\"shop_name\"], axis=1, inplace=True)\n\n#shops","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning item_categories dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# add item_category_name_2 feature\n\nitem_categories[\"item_category_name_2\"] = item_categories[\"item_category_name\"].apply(lambda x: x.split()[0])\nitem_categories[\"item_category_name_2\"] = LabelEncoder().fit_transform(item_categories[\"item_category_name_2\"])\n\n# drop text column\n\nitem_categories.drop([\"item_category_name\"], axis=1, inplace=True)\n\n#item_categories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning items dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef name_correction(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9А-Яа-я]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use item name as a text feature. this idea was suggested by Konstantin Yakovlev and nicely done by lonewolf45\n\nitems[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\nitems[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n\nitems[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\nitems[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\nitems = items.fillna(\"0\")\n\nitems[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\nitems.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")\n\nitems[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\nitems.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nitems.loc[ items.type == \"\", \"type\"] = \"mac\"\nitems.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\nitems.loc[ (items.type == 'pc' )| (items.type == 'pс') | (items.type == \"pc\"), \"type\" ] = \"pc\"\nitems.loc[ items.type == 'рs3' , \"type\"] = \"ps3\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group_sum = items.groupby([\"type\"]).agg({\"item_id\": \"count\"}).reset_index()\nto_drop = []\nfor cat in group_sum.type.unique():\n    if group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0] <40:\n        to_drop.append(cat)\nitems.name2 = items.name2.apply( lambda x: \"etc\" if (x in to_drop) else x )\nitems = items.drop([\"type\"], axis = 1)\nitems.name2 = LabelEncoder().fit_transform(items.name2)\nitems.name3 = LabelEncoder().fit_transform(items.name3)\n\n# drop item name and name1 from item dataset\n\nitems.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating matrix with all combinations of shops, items and months and adding more features"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = []\ncols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\nfor i in range(34):\n    sales = train[train[\"date_block_num\"] == i]\n    matrix.append(np.array(list( product( [i], sales[\"shop_id\"].unique(), sales[\"item_id\"].unique()))))\nmatrix = pd.DataFrame(np.vstack(matrix), columns = cols )\nmatrix.sort_values(cols, inplace = True )\n\n#matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add date_block_num to test\n\ntest[\"date_block_num\"] = 34\n\n# add data from test, train, item_categories, items\n\nmatrix = pd.concat([matrix, test.drop([\"ID\"],axis = 1)], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace = True)\n\nmatrix = pd.merge(matrix, shops, on = [\"shop_id\"], how = \"left\")\nmatrix = pd.merge(matrix, items, on = [\"item_id\"], how = \"left\")\nmatrix = pd.merge(matrix, item_categories, on = [\"item_category_id\"], how = \"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# add month day, week day and weekend count\n\ndef count_days(date_block_num): # function by Arnab Chakraborty\n    y = 2013 + date_block_num // 12\n    m = 1 + date_block_num % 12\n    leap = 0\n    if y% 400 == 0:\n        leap = 1\n    elif y % 100 == 0:\n        leap = 0\n    elif y% 4 == 0:\n        leap = 1\n    if m==2:\n        return 28 + leap\n    list = [1,3,5,7,8,10,12]\n    if m in list:\n        return 31\n    return 30\n\ndef count_holidays(date_block_num):\n    m = 1 + date_block_num % 12\n    if m == 1:\n        return 1\n    elif m == 2:\n        return 1\n    elif m == 3:\n        return 1\n    elif m == 5:\n        return 2\n    elif m == 6:\n        return 1\n    elif m == 11:\n        return 1\n    elif m == 12:\n        return 2\n    else:\n        return 0\n    \ndef count_weekdays(date_block_num):\n    try:\n        y = 2013 + date_block_num // 12\n        m = 1 + date_block_num % 12\n        if m < 9:\n            return np.busday_count(f'{y}-0{m}', f'{y}-0{m+1}')\n        elif m ==9:\n            return np.busday_count(f'{y}-0{m}', f'{y}-10')\n        elif m != 12:\n             return np.busday_count(f'{y}-{m}', f'{y}-{m+1}')\n        else:\n            return np.busday_count(f'{y}-{m}', f'{y+1}-01')\n    except ValueError:\n        print(m,y)\n    \nmatrix[\"days_in_month\"] = matrix[\"date_block_num\"].apply(lambda x: count_days(x))\nmatrix[\"holidays_in_month\"] = matrix[\"date_block_num\"].apply(lambda x: count_holidays(x))\nmatrix[\"weekdays_in_month\"] = matrix[\"date_block_num\"].apply(lambda x: count_weekdays(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add item_cnt_month feature\n\ngroup = train.groupby( [\"date_block_num\", \"shop_id\", \"item_id\"] ).agg( {\"item_cnt_day\": [\"sum\"]} )\ngroup.columns = [\"item_cnt_month\"]\ngroup.reset_index( inplace = True)\nmatrix = pd.merge(matrix, group, on = cols, how = \"left\" )\nmatrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].fillna(0)\nmatrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add first item appearance indicators\n\ngroup1 = matrix.groupby(['item_id'])['date_block_num'].min().reset_index()\ngroup1['item_first_interaction'] = 1\nmatrix = pd.merge(matrix, group1[['item_id', 'date_block_num', 'item_first_interaction']], on=['item_id', 'date_block_num'], how='left')\n\ngroup2 = matrix[matrix['date_block_num'] > 0].groupby(['shop_id', 'item_id'])['date_block_num'].min().reset_index()\ngroup2['first_date_block_num'] = group2['date_block_num']\nmatrix = pd.merge(matrix, group2[['item_id', 'shop_id', 'first_date_block_num']], on=['item_id', 'shop_id'], how='left')\n\nmatrix['first_date_block_num'].fillna(100, inplace=True)\nmatrix['shop_item_sold_before'] = (matrix['first_date_block_num'] < matrix['date_block_num'])\nmatrix.drop(['first_date_block_num'], axis=1, inplace=True)\n\nmatrix['item_first_interaction'].fillna(0, inplace=True)\nmatrix['shop_item_sold_before'].fillna(0, inplace=True)\n\ndel group\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for creating lagged features, used in top tier notebooks\n\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_' + str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n        df[col+'_lag_' + str(i)] = df[col + '_lag_' + str(i)].astype('float16')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# add item_cnt_month lags for last 3 months\n\nmatrix = lag_feature(matrix, [1, 2, 3], 'item_cnt_month')\n\n# add average item price per shop and average item price per shop per month\n\ngroup = train.groupby([\"item_id\", \"date_block_num\"])['item_price'].mean().reset_index().rename(columns={\"item_price\": \"avg_item_price\"})\nmatrix = matrix.merge(group, on = [\"item_id\", \"date_block_num\"], how = \"left\")\n\ngroup = train.groupby([\"shop_id\", \"item_id\", \"date_block_num\"])['item_price'].mean().reset_index().rename(columns={\"item_price\": \"avg_item_price_shop\"})\nmatrix = matrix.merge(group, on = [\"shop_id\", \"item_id\", \"date_block_num\"], how = \"left\")\n\n# add normalized difference between prices\n\nmatrix[\"price_diff\"] = (matrix[\"avg_item_price_shop\"] - matrix[\"avg_item_price\"]) / matrix[\"avg_item_price\"]\n\n# fill NaNs with zeros\n\nmatrix.fillna(0, inplace=True)\n\n# add price_diff lags for last 3 months \n\nmatrix = lag_feature(matrix, [1, 2, 3], 'price_diff')\n\n# remove redundant columns\n\nmatrix.drop(['avg_item_price', 'avg_item_price_shop', 'price_diff'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add sales lags for similar items with nearby ids\n# the idea of using this feature belongs to uladzimirkapeika\n\ndef lag_similar_items(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)+'_sim']\n        shifted['date_block_num'] += i\n        shifted['item_id'] -= 1\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n        df[col+'_lag_'+str(i)+'_sim'] = df[col+'_lag_'+str(i)+'_sim']\n    return df\n\nmatrix = lag_similar_items(matrix, [1, 2, 3], 'item_cnt_month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add target encoding\n\ngroup = matrix.groupby(['date_block_num','item_id'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_enc\"})\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\n\n# add target per shop encoding\n\ngroup = matrix.groupby(['date_block_num','item_id', 'shop_id'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_shop_enc\"})\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id', 'shop_id'], how='left')\n\n# add target per item_category_id encoding\n\ngroup = matrix.groupby(['date_block_num','item_id', 'item_category_id'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_cat1_enc\"})\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id', 'item_category_id'], how='left')\n\n# add target per city encoding\n\ngroup = matrix.groupby(['date_block_num','item_id', 'shop_city'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_city_enc\"})\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id', 'shop_city'], how='left')\n\n# add target per name2 encoding\n\ngroup = matrix.groupby(['date_block_num','item_id', 'name2'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_name2_enc\"})\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id', 'name2'], how='left')\n\n# add avg category sales for last 3 months for new items \n\ngroup = matrix[matrix['item_first_interaction'] < 0].groupby(['date_block_num','item_category_id'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"new_item_cat_enc\"})\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\n\n# fill NaNs with zeros\n\nmatrix.fillna(0, inplace=True)\n\n# lag encodings\n\nmatrix = lag_feature(matrix, [1, 2, 3], 'item_target_enc')\nmatrix = lag_feature(matrix, [1, 2, 3], 'item_target_shop_enc')\nmatrix = lag_feature(matrix, [1, 2, 3], 'item_target_cat1_enc') \nmatrix = lag_feature(matrix, [1, 2, 3], 'item_target_name2_enc')\nmatrix = lag_feature(matrix, [1, 2, 3], 'item_target_city_enc')\nmatrix = lag_feature(matrix, [1, 2, 3], 'new_item_cat_enc')\n\n# drop unlagged encodings\n\nmatrix.drop(['item_target_enc', 'item_target_shop_enc', 'item_target_cat1_enc', 'item_target_city_enc', 'new_item_cat_enc', 'item_target_name2_enc'], axis=1, inplace=True)\n\n# fill Nans with zeros\n\nmatrix.fillna(0, inplace=True)\n\n# remove first 3 months that dont have lag values\n\nmatrix = matrix[(matrix['date_block_num'] > 2)]\nmatrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n# Modified to support timestamp type, categorical type\n# Modified to add option to use float16 or not. feather format does not support float16.\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            # skip datetime type or categorical type\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n# reduce matrix memory usage\n\nmatrix = reduce_mem_usage(matrix, use_float16=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save matrix\n\nmatrix.to_pickle('matrix.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}