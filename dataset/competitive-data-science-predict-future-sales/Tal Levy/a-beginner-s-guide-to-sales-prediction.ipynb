{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overview\nFirs of all,\nGo check out the course - https://www.coursera.org/learn/competitive-data-science.  \nThis is a great course which combines theory with hands on exercises (like this competition).  \nDefinitely worth the price.\n\nNow,\nThe notebook gives us predictions for future sales on the test set and the sections are as follows:  \n\n## EDA\ndata analysis is done on the different available datasets and includes outlier removal  \n\n## Feature Matrix Creation\nWe have sales of different items from different stores on each month.  \nFeature Matrix will be a FULL table of all possible item/store pairs in each month.  \nWe will include the test set in this matrix for later features creation.  \n\n## Feature Creation\n* sales count and price features with lags on past months\n* converting month index to month of year (assuming seasonality, it will be usefull to know the month)\n* mean encoding - `item_id` for example is not a useful feature, encoding that id using sales of the item is.\n* `shop_name` text analysis - can we find keywords in a shop name that correlates with high sales and use it as features?  \n\n## Train/Validation/Test Split\nAs we will be predicting future sales, we have to make sure our validation schema does not cause us to have training data from dates beyond the validation and test sets.  \nFull data has N months.  \nWe use Month N for test. Month N-1 for validation. And the rest for train.  \n\n## Training Different Models and Validating on RMSE\nWe tested different models\n* Linear Regression\n* LightGBM (multiple models tested with different hyperparameters)\n* Ensamble of both  \n\n## Results Submission"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom itertools import product\nimport sklearn\nimport scipy.sparse \nimport lightgbm \nimport gc\n\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import mean_squared_error\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read and Display Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = pd.read_csv('../input/sales_train.csv')\nprint('sales_train')\ndisplay(sales_train.head())\n\ntest = pd.read_csv('../input/test.csv')\nprint('test')\ndisplay(test.head())\n\nitems = pd.read_csv('../input/items.csv')\nprint('items')\ndisplay(items.head())\n\nitem_categories = pd.read_csv('../input/item_categories.csv')\nprint('item_categories')\ndisplay(item_categories.head())\n\nshops = pd.read_csv('../input/shops.csv')\nprint('shops')\ndisplay(shops.head())\n\nsample_submission = pd.read_csv('../input/sample_submission.csv')\nprint('sample_submission')\ndisplay(sample_submission.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('sales_train')\ndisplay(sales_train.describe(include='all').T)\n\nprint('test')\ndisplay(test.describe(include='all').T)\n\nprint('items')\ndisplay(items.describe(include='all').T)\n\nprint('item_categories')\ndisplay(item_categories.describe(include='all').T)\n\nprint('shops')\ndisplay(shops.describe(include='all').T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## *sales_train* EDA"},{"metadata":{},"cell_type":"markdown","source":"*item_price* and *item_cnt_day* from **sales_train** seem to have weird min/max values"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['item_price','item_cnt_day']:\n    plt.figure()\n    plt.title(col)\n    sns.boxplot(x=sales_train[col]);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's remove outliers over the 99th percentile"},{"metadata":{"trusted":true},"cell_type":"code","source":"shape0 = sales_train.shape[0] # train size before dropping values\nfor col in ['item_price','item_cnt_day']:\n    max_val = sales_train[col].quantile(.99) # get 99th percentile value\n    sales_train = sales_train[sales_train[col]<max_val] # drop outliers\n    print(f'{shape0-sales_train.shape[0]} {col} values over {max_val} removed')\n\nprint(f'new training set has {sales_train.shape[0]} records')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's checck which items have item_price<=0"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train[sales_train['item_price']<=0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"one transaction with pric<=0.\nIt's safe to remove that item."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train=sales_train[sales_train['item_price']>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Matrix Creation\nGet all shop/id pairs for each month"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in sales_train['date_block_num'].unique():\n    cur_shops = sales_train.loc[sales_train['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales_train.loc[sales_train['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid.shape)\ngrid.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# append test data to matrix with next month's date_block_num\ntest file predicts next month.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# latest month\ngrid['date_block_num'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# append next month\ntest['date_block_num'] = 34\n# add to grid\ngrid = pd.concat([grid, test[grid.columns]], ignore_index=True, sort=False)\nprint('grid shape: ',grid.shape)\nprint('missing values:',grid.isna().sum())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features Creation"},{"metadata":{},"cell_type":"markdown","source":"## Create monthly features from *sales_train*\nas predictions are per month, we need features aggregated to a monthly level"},{"metadata":{},"cell_type":"markdown","source":"`item_cnt_day` features\n* total items\n* total trips "},{"metadata":{"trusted":true},"cell_type":"code","source":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# Groupby data to get shop-item-month aggregates\ngb = sales_train.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum','trips':'size'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n# Same as above but with shop-month aggregates\ngb = sales_train.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum','trips_shop':'size'}})\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n# Same as above but with item-month aggregates\ngb = sales_train.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum','trips_item':'size'}})\ngb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`item_price` features\n(getting lots of nulls)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# median item monthly price (using median to avoid outliers)\ngb = sales_train.groupby(['date_block_num','item_id'],as_index=False).agg({'item_price':{'median_item_price':'median'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(all_data, gb, how='left', on=['date_block_num','item_id'])\n\n# make sure no na values\nprint('na median_item_price:',all_data['median_item_price'].isna().sum())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"first item appearance feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = all_data.groupby(['item_id'],as_index=False).agg({'date_block_num':{'item_first_month':'min'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(all_data, gb, how='left', on=['item_id'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"use `item_first_month` to create `new_item` feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['new_item'] = (all_data['date_block_num']==all_data['item_first_month']).astype(int)\nall_data['new_item'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Downcast dtypes from 64 to 32 bit to save memory\nall_data = downcast_dtypes(all_data)\ndel grid, gb \ngc.collect();\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lag features from [1,2,3,4,5,12] months ago"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of columns that we will use to create lags\ncols_to_rename = list(all_data.columns.difference(index_cols+['item_first_month'])) \n\nshift_range = [1, 2, 3, 4, 5, 12]\n\nfor month_shift in tqdm_notebook(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n\n    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\ndel train_shift\n\n# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12] \n\n# List of all lagged features\nfit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n# We will drop these at fitting stage\nto_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n# Category for each item\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\nall_data = downcast_dtypes(all_data)\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean Encoding for *item_id*"},{"metadata":{"trusted":true},"cell_type":"code","source":"item_target_enc_na = .3343 # default na replacement\n# Expanding Mean\ncumsum = all_data.groupby('item_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_id')['target'].cumcount()\n\nall_data['item_target_enc'] = cumsum/cumcnt\nall_data['item_target_enc'].fillna(item_target_enc_na,inplace=True)\ncorr = np.corrcoef(all_data['target'].values, all_data['item_target_enc'])[0][1]\nprint(corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean Encoding for *shop_id*"},{"metadata":{"trusted":true},"cell_type":"code","source":"item_target_enc_na = .3343 # default na replacement\n# Expanding Mean\ncumsum = all_data.groupby('shop_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('shop_id')['target'].cumcount()\n\nall_data['shop_id_enc'] = cumsum/cumcnt\nall_data['shop_id_enc'].fillna(item_target_enc_na,inplace=True)\ncorr = np.corrcoef(all_data['target'].values, all_data['shop_id_enc'])[0][1]\nprint(corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"not much correlation heare. we can drop that column"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.drop(columns='shop_id_enc',inplace=True)\nall_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add Month Feature"},{"metadata":{},"cell_type":"markdown","source":"assuming month of year plays a big role in number of items sold (seasonality).\nlet's add month"},{"metadata":{"trusted":true},"cell_type":"code","source":"# figure out difference between month and month\nsales_train[['date','date_block_num']].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rule seems to be date_block_num%12+1\nsales_train['month'] = sales_train['date_block_num']%12+1\nsales_train[['date','month','date_block_num']].sample(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add this to all_data\nall_data['month'] = all_data['date_block_num']%12+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore Items Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"items.groupby('item_category_id',as_index=False)['item_id'].count().rename(columns={'item_id':'total_items'}).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lots of items in each category (median of 43).\nwe can use it as a categorical feature as well as encoded feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['item_category_id'] = all_data['item_id'].map(items.set_index('item_id')['item_category_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['item_category_id'].isna().sum() # no missing categories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"check if encoding item category is beneficial "},{"metadata":{"trusted":true},"cell_type":"code","source":"item_target_enc_na = 0 # default na replacement\n# Expanding Mean\ncumsum = all_data.groupby('item_category_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_category_id')['target'].cumcount()\n\nitem_category_id_enc = cumsum/cumcnt\nitem_category_id_enc.fillna(item_target_enc_na,inplace=True)\nall_data['item_category_id_enc'] = item_category_id_enc\ncorr = np.corrcoef(all_data['target'].values, item_category_id_enc)[0][1]\nprint(corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore Shop Names"},{"metadata":{},"cell_type":"markdown","source":"any useful words in shop names?"},{"metadata":{"trusted":true},"cell_type":"code","source":"words = ' '.join(shops['shop_name']).split(' ')\nfrom collections import Counter\nc = Counter(words)\nc.most_common(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"according to google translate top values are\n* Shopping Center\n* Moscow\n* Mega\n* Dispenser\n* TC\n\nLet's see if more people buy there"},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_by_store = sales_train.groupby('shop_id',as_index=False)['item_cnt_day'].sum()\nshop_by_store = shop_by_store.merge(shops, on='shop_id')\nprint(shop_by_store['shop_name'].isna().sum())\nshop_by_store.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_by_store['name_array'] = shop_by_store['shop_name'].str.split(' ')\ntop_words = [x for x,y in c.most_common(6)] # common words in shop name\nfor w in top_words:\n    shop_by_store[w] = shop_by_store['shop_name'].map(lambda x: 1 if w in x else 0)\n#     shop_by_store[w] = w in shop_by_store['shop_name'].str.split(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for w in top_words:\n    print(shop_by_store.groupby(by=w)['item_cnt_day'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"first four seem to be good features.\nlet's add them to the full table.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"top_words = top_words[0:4] # important shop features\nshops['name_array'] = shops['shop_name'].str.split(' ')\nfor w in top_words:\n    shops[w] = shops['shop_name'].map(lambda x: 1 if w in x else 0)\n\nall_data = pd.merge(all_data,shops[['shop_id']+top_words],on='shop_id',how='left') # merge\n\nprint(all_data[top_words].isna().sum()) # make sure no nulls\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train/Validation/Test Split\nTest set is last `date_block_num` (34).\nWe will validate on the month before that (33) and train on everything else\n* be sure to drop leaking columns such as target_shop,etc..."},{"metadata":{"trusted":true},"cell_type":"code","source":"leaking_columns = ['median_item_price','date_block_num','target','target_shop','target_item','trips','trips_shop','trips_item']\n\nX_train = all_data.loc[all_data['date_block_num'] < 33].drop(leaking_columns, axis=1)\nX_val = all_data.loc[all_data['date_block_num'] == 33].drop(leaking_columns, axis=1)\nX_test = all_data.loc[all_data['date_block_num'] == 34].drop(leaking_columns, axis=1)\n\ny_train = all_data.loc[all_data['date_block_num'] < 33,'target'].values\ny_val = all_data.loc[all_data['date_block_num'] == 33,'target'].values\n\n# save all_data\n# all_data.to_csv('all_data.csv',index=False)\ndel all_data\ngc.collect();\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Validation Function\nvalidation is rmse on True target values are clipped into [0,20] range.  \nLet's create such validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def validation_function(y_pred,y_true):\n    print(f'rmse before [0,20] clipping: {mean_squared_error(y_true, y_pred)}')\n    y_pred = y_pred.clip(0,20)\n    y_true = y_true.clip(0,20)\n    print(f'rmse after [0,20] clipping: {mean_squared_error(y_true, y_pred)}')\n    return \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train and Validate Different Models\n* linear regression\n* lightgbm"},{"metadata":{"trusted":true},"cell_type":"code","source":"# linear regression\n\nlr = LinearRegression()\nlr.fit(X_train.values, y_train)\n# (due to memory issues we train on half the data)\n# lr.fit(X_train[round(X_train.shape[0]/2):-1].values, y_train[round(X_train.shape[0]/2):-1])\npred_lr = lr.predict(X_val.values)\n\nprint('Test R-squared for linreg is %f' % r2_score(y_val, pred_lr))\nvalidation_function(y_val,pred_lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## we will try lightgbm with different parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n\n# specify your initial configurations as a dict\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'root_mean_squared_error'},\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 10\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=15,\n                valid_sets=[lgb_train,lgb_eval],\n                valid_names=['train','val'],\n                early_stopping_rounds=5)\n\nprint(gbm.pandas_categorical)\nlgb.plot_importance(gbm,figsize=(10,10));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Update parameters\n* model did not converge: we can increase boosting rounds\n* try model after explicitly specifying which columns are categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['shop_id','item_id','item_category_id','new_item']+top_words\n\nlgb_train = lgb.Dataset(X_train, y_train,categorical_feature=categorical_features)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train,categorical_feature=categorical_features)\n\ngbm2 = lgb.train(params,\n                lgb_train,\n                num_boost_round=15,\n                valid_sets=[lgb_train,lgb_eval],\n                valid_names=['train','val'],\n                categorical_feature = categorical_features,\n                early_stopping_rounds=5)\n\nprint(gbm2.pandas_categorical)\nlgb.plot_importance(gbm2,figsize=(10,10));\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"test one more set of parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train, y_train,categorical_feature=categorical_features)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train,categorical_feature=categorical_features)\n\nlgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':10 \n              }\n\nmodel = lgb.train(lgb_params,\n                lgb_train,\n                num_boost_round=1000,\n                valid_sets=[lgb_train,lgb_eval],\n                valid_names=['train','val'],\n                categorical_feature = categorical_features,\n                early_stopping_rounds=5)\n\n\nprint(model.pandas_categorical)\nlgb.plot_importance(model,figsize=(10,10));\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using best lgbm - *model*"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lgb = model.predict(X_val)\n\nprint('Test R-squared for LightGBM is %f' % r2_score(y_val, pred_lgb))\nvalidation_function(y_val,pred_lgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Try Ensamble\nwe will build a simple lr on top of the train predictions to see if they improve validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train\nX_train_level2 = np.c_[model.predict(X_train), lr.predict(X_train.values)] \nlr2 = LinearRegression()\nlr2.fit(X_train_level2, y_train)\n\n# predict\nX_val_level2 = np.c_[model.predict(X_val), lr.predict(X_val.values)] \npred_lr2 = lr2.predict(X_val_level2)\n\nvalidation_function(y_val,pred_lr2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"before rmse looks better than lightgbm, but after clipping, we get pretty much the same results"},{"metadata":{},"cell_type":"markdown","source":"# Submit Results on Test Set\nwe will save lightgbm and ensamble resuts on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lightgbm and lr predicitons\npred_lgb = model.predict(X_test).clip(0,20)\npred_lr = lr.predict(X_test.values).clip(0,20)\n\n# ensamble predicitons\nX_test_level2 = np.c_[pred_lgb, pred_lr] \npred_ensamble = lr2.predict(X_test_level2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make sure results are in the same order as the original test set\n(test[['shop_id','item_id']].values == X_test[['shop_id','item_id']].values).all()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(data={'ID':test['ID'],'item_cnt_month':pred_lgb}).to_csv('lgbm_predictions.csv',index=False)\npd.DataFrame(data={'ID':test['ID'],'item_cnt_month':pred_ensamble}).to_csv('ensamble_predictions.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}