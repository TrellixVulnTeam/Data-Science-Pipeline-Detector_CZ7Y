{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Simple Time Series Clustering(KMeans)**\n\n\nIn this notebook, trying basic time series clustering, and some other time series analysis techniques.\n\n\n**References - Thanks for sharing information. ** \n\nhttps://www.kaggle.com/learn/time-series\n\nhttps://www.kaggle.com/bextuychiev/every-pandas-function-to-manipulate-time-series/notebook\n\nhttps://www.kaggle.com/izzettunc/introduction-to-time-series-clustering\n","metadata":{}},{"cell_type":"code","source":"!pip install tslearn","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:01:48.453918Z","iopub.execute_input":"2022-02-27T01:01:48.455094Z","iopub.status.idle":"2022-02-27T01:02:01.136783Z","shell.execute_reply.started":"2022-02-27T01:01:48.454942Z","shell.execute_reply":"2022-02-27T01:02:01.135586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns  # visualization tool\nimport os\n\nfrom tslearn.clustering import KShape\nfrom tslearn.preprocessing import TimeSeriesScalerMeanVariance\nfrom tslearn.utils import to_time_series_dataset\nfrom tslearn.clustering import TimeSeriesKMeans\n\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\n","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:01.139052Z","iopub.execute_input":"2022-02-27T01:02:01.139332Z","iopub.status.idle":"2022-02-27T01:02:04.035954Z","shell.execute_reply.started":"2022-02-27T01:02:01.1393Z","shell.execute_reply":"2022-02-27T01:02:04.034987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load Data**","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\ntests = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\nitem_categories = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:04.0379Z","iopub.execute_input":"2022-02-27T01:02:04.038246Z","iopub.status.idle":"2022-02-27T01:02:06.904669Z","shell.execute_reply.started":"2022-02-27T01:02:04.0382Z","shell.execute_reply":"2022-02-27T01:02:06.903522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the content of the data.\n","metadata":{}},{"cell_type":"code","source":"merged = pd.merge(pd.merge(data, items, on='item_id'), shops, on='shop_id')\n\nmerged = merged.loc[:,['date','shop_id','item_id','item_category_id','item_cnt_day']].copy()\nmerged['datetime'] = pd.to_datetime(merged['date'])\nmerged.drop('date',axis=1)\n\nmerged.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:06.907303Z","iopub.execute_input":"2022-02-27T01:02:06.907758Z","iopub.status.idle":"2022-02-27T01:02:08.952529Z","shell.execute_reply.started":"2022-02-27T01:02:06.907715Z","shell.execute_reply":"2022-02-27T01:02:08.951599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualize Overview Trends**\n\nIt looks like gradual upward trend.\n","metadata":{}},{"cell_type":"code","source":"plot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(11, 4))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n)\n%config InlineBackend.figure_format = 'retina'\n\ndf = merged.groupby(['datetime','item_category_id'],as_index=False ).mean()\ndf.set_index('datetime', inplace=True)\ndf = df.resample('M').mean()\ndf['Time'] = np.arange(len(df.index))\n\n# Training data\nX = df.loc[:, ['Time']]  # features\ny = df.loc[:, 'item_cnt_day']  # target\n\n# Train the model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Store the fitted values as a time series with the same time index as\n# the training data\ny_pred = pd.Series(model.predict(X), index=X.index)\n\nax = y.plot(**plot_params)\nax = y_pred.plot(ax=ax, linewidth=3)\nax.set_title('Time Plot of Monthly sales');","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:08.955351Z","iopub.execute_input":"2022-02-27T01:02:08.956536Z","iopub.status.idle":"2022-02-27T01:02:09.920047Z","shell.execute_reply.started":"2022-02-27T01:02:08.956483Z","shell.execute_reply":"2022-02-27T01:02:09.91897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Lag futures**\n\nI've tried everything, but I can't seem to find an effective lag.","metadata":{}},{"cell_type":"code","source":"df['Lag_1'] = df['item_cnt_day'].shift(1)\ndf['Lag_3'] = df['item_cnt_day'].shift(3)\ndf['Lag_12'] = df['item_cnt_day'].shift(12)\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:09.921659Z","iopub.execute_input":"2022-02-27T01:02:09.922009Z","iopub.status.idle":"2022-02-27T01:02:09.946052Z","shell.execute_reply.started":"2022-02-27T01:02:09.921964Z","shell.execute_reply":"2022-02-27T01:02:09.945101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX = df.loc[:, ['Lag_12']]\nX.dropna(inplace=True)  # drop missing values in the feature set\ny = df.loc[:, 'item_cnt_day']  # create the target\ny, X = y.align(X, join='inner')  # drop corresponding values in target\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\ny_pred = pd.Series(model.predict(X), index=X.index)\n\nfig, ax = plt.subplots()\nax.plot(X['Lag_12'], y, '.', color='0.25')\nax.plot(X['Lag_12'], y_pred)\nax.set_aspect('equal')\nax.set_ylabel('item sale count')\nax.set_xlabel('Lag_12')\nax.set_title('Lag Plot of Monthly sales');","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:09.947668Z","iopub.execute_input":"2022-02-27T01:02:09.948213Z","iopub.status.idle":"2022-02-27T01:02:10.2517Z","shell.execute_reply.started":"2022-02-27T01:02:09.948166Z","shell.execute_reply":"2022-02-27T01:02:10.250907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = y.plot(**plot_params)\nax = y_pred.plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:10.254081Z","iopub.execute_input":"2022-02-27T01:02:10.254604Z","iopub.status.idle":"2022-02-27T01:02:10.703503Z","shell.execute_reply.started":"2022-02-27T01:02:10.254561Z","shell.execute_reply":"2022-02-27T01:02:10.70244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Item Categories**\n\nNext, let's look at sales trends by item category.\n(There are a lot of them, so we'll divide them into 10 at a time.)\nThere is many variation in the time period and unit of sales results, so it is difficult to read trends at a glance.\nI can also see some spikes in some places.\n","metadata":{}},{"cell_type":"code","source":"itemcats = merged.groupby(['datetime','item_category_id'],as_index=False ).mean()\nitemcats.set_index('datetime', inplace=True)\n\nfor categories in range(0,70,10):\n    div = itemcats.query(f'{categories} <= item_category_id < {categories+10}')\n    div.pivot(columns='item_category_id',values='item_cnt_day').resample('M').agg(['mean']).plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:10.70489Z","iopub.execute_input":"2022-02-27T01:02:10.705207Z","iopub.status.idle":"2022-02-27T01:02:16.27543Z","shell.execute_reply.started":"2022-02-27T01:02:10.705171Z","shell.execute_reply":"2022-02-27T01:02:16.274274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Process for Outliers**\n\n\nSome outliers should be removed.For now, I will figure out a way to automatically remove values that are a certain distance from the moving average value.\n","metadata":{}},{"cell_type":"code","source":"# Temporary value\nspan = 30\nthreshold=2\n\ndef plot_outlier(ts):\n    fig, ax = plt.subplots()\n    ewm_mean = ts.ewm(span).mean()\n    ewm_std = ts.ewm(span).std()\n    ax.plot(ts, label='original')\n    ax.plot(ewm_mean, label='ewma')\n\n    outlier = ts[(ts - ewm_mean).abs() > ewm_std * threshold]\n    ax.scatter(outlier.index, outlier, label='outlier')\n    ax.legend()\n    ax.set_title('EWMA and Outlier')\n    return fig\n\ndef subst_outlier(ts):\n    df = ts.copy() \n    ewm_mean = df.ewm(span).mean()\n    ewm_std = df.ewm(span).std()\n    df[(df - ewm_mean).abs() > ewm_std * threshold] = ewm_mean #Replace outliers with moving average values\n    return df\n\ndef plot_items_subst_outlier(df):\n    for sp in range(0,50,10):\n        div = df.query(f'{sp} <= item_category_id < {sp+10}')\n        wd = div.pivot(columns='item_category_id',values='item_cnt_day')\n        wd = wd.resample('M').sum()\n        subst_outlier(wd).plot()\n        \ndef plot_shops_subst_outlier(df):\n    for sp in range(0,50,10):\n        div = df.query(f'{sp} <= shop_id < {sp+10}')\n        wd = div.pivot(columns='shop_id',values='item_cnt_day')\n        wd = wd.resample('M').sum()\n        subst_outlier(wd).plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T04:19:09.493571Z","iopub.execute_input":"2022-02-27T04:19:09.494056Z","iopub.status.idle":"2022-02-27T04:19:09.509229Z","shell.execute_reply.started":"2022-02-27T04:19:09.494019Z","shell.execute_reply":"2022-02-27T04:19:09.50814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outl = merged.groupby(['datetime','item_category_id'],as_index=False ).mean()\noutl.set_index('datetime', inplace=True)\n\n# Sample item category No.8.\noutl = outl.query('item_category_id == 8')\nwd = outl.pivot(columns='item_category_id',values='item_cnt_day').resample('M').agg(['mean'])\n\nplot_outlier(wd)\nax = subst_outlier(wd).plot()\nax.set_title('After Delete Outlier')\n","metadata":{"execution":{"iopub.status.busy":"2022-02-27T04:19:13.097366Z","iopub.execute_input":"2022-02-27T04:19:13.098119Z","iopub.status.idle":"2022-02-27T04:19:14.817333Z","shell.execute_reply.started":"2022-02-27T04:19:13.098084Z","shell.execute_reply":"2022-02-27T04:19:14.816647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Shops**\n\n\nNext, let's see at the sales trends for each store.\n\nIt seems that outliers need to be processed here as well.","metadata":{}},{"cell_type":"code","source":"vd = merged.groupby(['datetime','shop_id'],as_index=False ).sum()\nvd.set_index('datetime', inplace=True)\n\ndef plot_shops(df):\n    for sp in range(0,50,10):\n        div = df.query(f'{sp} <= shop_id < {sp+10}')\n        wd = div.pivot(columns='shop_id',values='item_cnt_day')\n        wd = wd.resample('M').sum()\n        wd.plot()\n\nplot_shops(vd)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:17.696264Z","iopub.execute_input":"2022-02-27T01:02:17.696494Z","iopub.status.idle":"2022-02-27T01:02:21.598924Z","shell.execute_reply.started":"2022-02-27T01:02:17.696466Z","shell.execute_reply":"2022-02-27T01:02:21.597951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Time Series Clustering**\n\n\nIt seems to be a certain degree of similarity in the sales trends for both item categories and shops.\nSo I try classification by time series clustering.\n\nI'm trying out some patterns.Number of clusters and the clustering method may not be the best way.","metadata":{}},{"cell_type":"code","source":"def clustering(df, cl_count):\n    # Normalization\n    scaler = TimeSeriesScalerMeanVariance(mu=0.0, std=1.)\n    scaled = scaler.fit_transform(to_time_series_dataset(df.values.T))\n    # Calculate KMeans\n    km = TimeSeriesKMeans(n_clusters=cl_count, verbose=True, random_state=seed)\n    labels = km.fit_predict(scaled)\n    return labels\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:21.600293Z","iopub.execute_input":"2022-02-27T01:02:21.600825Z","iopub.status.idle":"2022-02-27T01:02:21.607007Z","shell.execute_reply.started":"2022-02-27T01:02:21.600785Z","shell.execute_reply":"2022-02-27T01:02:21.605973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 0\nnp.random.seed(seed)\n\n# Temporary value\nshop_cluster_count = 4\nitem_cluster_count = 3\n\n# Get Shops cluster\nclst = merged.groupby(['datetime','shop_id'],as_index=False ).sum()\nclst.set_index('datetime', inplace=True)\nclst = clst.pivot(columns='shop_id',values='item_cnt_day').resample('M').sum()\nclst = subst_outlier(clst)\nshops['shop_cluster'] = clustering(clst, 4)\n\n# Get Shops cluster\nclst = merged.groupby(['datetime','item_category_id'],as_index=False ).sum()\nclst.set_index('datetime', inplace=True)\nclst = clst.pivot(columns='item_category_id',values='item_cnt_day').resample('M').sum()\nclst = subst_outlier(clst)\nitem_categories['item_category_cluster'] = clustering(clst, 3)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:21.610464Z","iopub.execute_input":"2022-02-27T01:02:21.610955Z","iopub.status.idle":"2022-02-27T01:02:22.52475Z","shell.execute_reply.started":"2022-02-27T01:02:21.610819Z","shell.execute_reply":"2022-02-27T01:02:22.523549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualize Clusters**\n\n\nVisualise the clusters, but unfortunately there is not such a clear difference.","metadata":{}},{"cell_type":"code","source":"def plot_items_cluster(df):\n    for sp in range(item_cluster_count):\n        div = df[df['item_category_cluster'] == sp]\n        wd = div.pivot(columns='item_category_id',values='item_cnt_day').resample('M').sum()\n        ax = subst_outlier(wd).plot()\n        ax.set_title(f'Item category cluster {sp}')\n        ax.get_legend().remove();\n\n        \ndef plot_shops_cluster(df):\n    for sp in range(shop_cluster_count):\n        div = df[df['shop_cluster'] == sp]\n        wd = div.pivot(columns='shop_id',values='item_cnt_day').resample('M').sum()\n        ax = subst_outlier(wd).plot()\n        ax.set_title(f'Shop cluster {sp}')\n        ax.get_legend().remove();","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:22.526146Z","iopub.execute_input":"2022-02-27T01:02:22.526382Z","iopub.status.idle":"2022-02-27T01:02:22.536319Z","shell.execute_reply.started":"2022-02-27T01:02:22.526353Z","shell.execute_reply":"2022-02-27T01:02:22.535187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For shops, the clusters are separated according to the distribution of sales peaks.","metadata":{}},{"cell_type":"code","source":"item_merged = pd.merge(items, item_categories, on='item_category_id')\nmerged = pd.merge(pd.merge(data, item_merged, on='item_id'), shops, on='shop_id')\nmerged = merged.loc[:,['date','shop_id','item_id','item_category_id','shop_cluster','item_category_cluster','item_cnt_day']].copy()\nmerged['datetime'] = pd.to_datetime(merged['date'])\nmerged.drop('date',axis=1)\n\nmerged.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:22.538033Z","iopub.execute_input":"2022-02-27T01:02:22.538313Z","iopub.status.idle":"2022-02-27T01:02:25.417278Z","shell.execute_reply.started":"2022-02-27T01:02:22.53828Z","shell.execute_reply":"2022-02-27T01:02:25.416224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vd = merged.groupby(['datetime','shop_id'],as_index=False ).sum()\nvd.set_index('datetime', inplace=True)\n\nplot_shops_cluster(vd)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:25.418514Z","iopub.execute_input":"2022-02-27T01:02:25.41873Z","iopub.status.idle":"2022-02-27T01:02:28.103671Z","shell.execute_reply.started":"2022-02-27T01:02:25.418703Z","shell.execute_reply":"2022-02-27T01:02:28.102554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Item categories clusters seems obscure.","metadata":{}},{"cell_type":"code","source":"vd = merged.groupby(['datetime','item_category_id'],as_index=False ).sum()\nvd.set_index('datetime', inplace=True)\n\nplot_items_cluster(vd)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:02:28.105472Z","iopub.execute_input":"2022-02-27T01:02:28.105813Z","iopub.status.idle":"2022-02-27T01:02:30.999865Z","shell.execute_reply.started":"2022-02-27T01:02:28.105769Z","shell.execute_reply":"2022-02-27T01:02:30.99917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model building and prediction**\n\n\nIt's a simple prediction using linear regression and the XGB ensemble.","metadata":{}},{"cell_type":"code","source":"# LinearRegression model\n\ndef LinearRegressionPred(data):\n    df = data.copy()\n\n    itemsum = pd.DataFrame(df.loc[:,'item_cnt_day']).resample('M').sum()\n    didx = pd.DataFrame(index=pd.date_range(start=\"2013-01-01\", end=\"2015-10-31\", freq=\"M\"))\n    itemsum = itemsum.merge(didx, how=\"outer\", left_index=True, right_index=True).fillna(0)\n    itemsum['Time'] = np.arange(len(itemsum.index))\n    \n    X = itemsum.loc[:, ['Time']]  # features\n    y = itemsum.loc[:, 'item_cnt_day']  # target\n    \n    model = LinearRegression()\n    model.fit(X, y)\n    \n    return model.predict(pd.DataFrame([itemsum['Time'].max()+1]))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:46:27.622366Z","iopub.execute_input":"2022-02-27T01:46:27.622711Z","iopub.status.idle":"2022-02-27T01:46:27.632533Z","shell.execute_reply.started":"2022-02-27T01:46:27.622674Z","shell.execute_reply":"2022-02-27T01:46:27.631403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = merged.loc[:,['datetime','shop_id','item_id','item_cnt_day']].copy()\nfeatures.set_index('datetime', inplace=True)\nfeatures = features['20130101':'20151031']\n\ntests['pred_line'] = 0\nfor index, row in tests.iterrows():\n    pred = LinearRegressionPred(features.query(f\"shop_id == {row['shop_id']} and item_id == {row['item_id']}\"))\n    row['pred_line'] = pred","metadata":{"execution":{"iopub.status.busy":"2022-02-27T01:46:44.603658Z","iopub.execute_input":"2022-02-27T01:46:44.605053Z","iopub.status.idle":"2022-02-27T03:41:53.635998Z","shell.execute_reply.started":"2022-02-27T01:46:44.605005Z","shell.execute_reply":"2022-02-27T03:41:53.634672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = merged[['date','shop_id','item_id','shop_cluster','item_category_cluster','item_category_id','item_cnt_day']].copy()\nfeatures['datetime'] = pd.to_datetime(features['date'])\nfeatures = features.drop('date',axis=1)\nfeatures.set_index('datetime', inplace=True)\nfeatures['month'] = features.index.month\n\ntarget = features.pop('item_cnt_day')\n\nxgb = XGBRegressor()\nxgb.fit(features, target)\n\ndf_tests = pd.merge(pd.merge(tests, item_merged, on='item_id'), shops, on='shop_id')\nfeatures = df_tests[['shop_id','item_id','shop_cluster','item_category_cluster','item_category_id']].copy()\nfeatures['datetime'] = pd.to_datetime('2015-11-30')\nfeatures.set_index('datetime', inplace=True)\nfeatures['month'] = features.index.month\n\ntests['pred_xgb'] = xgb.predict(features)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T03:42:13.775116Z","iopub.execute_input":"2022-02-27T03:42:13.776229Z","iopub.status.idle":"2022-02-27T03:44:08.175146Z","shell.execute_reply.started":"2022-02-27T03:42:13.776181Z","shell.execute_reply":"2022-02-27T03:44:08.17442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Make Submission**","metadata":{}},{"cell_type":"code","source":"tests['item_cnt_month'] = (tests['pred_xgb'] + tests['pred_line'])/2\ntests[['ID','item_cnt_month']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T03:51:06.053823Z","iopub.execute_input":"2022-02-27T03:51:06.054199Z","iopub.status.idle":"2022-02-27T03:51:06.792717Z","shell.execute_reply.started":"2022-02-27T03:51:06.054166Z","shell.execute_reply":"2022-02-27T03:51:06.791599Z"},"trusted":true},"execution_count":null,"outputs":[]}]}