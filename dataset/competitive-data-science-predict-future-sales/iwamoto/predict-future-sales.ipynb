{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Summary\nI used XGBoost, Linear Regression and ensembling. The most important features were data_block_num, shop_id, item_id and item_category_id. I used python. It took to train our model about an hour.\n\n# Features Selection / Engineering\nThe most important features for XGBoost model are data_block_num, shop_id, item_id and item_category_id. At first I had ignored these features and mainly used mean encodings such as monthly item sales and counts. I couldn't get good results with this strategy, however. I added these features on trial and I could get a score under 1 rmse.\n\n# Training Methods\nI used XGBoost and Linear Regression methods as the 1st level models. I ensembled models with a simple linear regression to combine these models. I was looking forward to be improved score with ensembling. However its improvement was a little and sometimes worse.\n\n# Interesting findings\nI had been using only some features to make prediction process faster in the beginning. These features were selected by the correlations between features and target value. Because I supposed it was the key to select important features. However low correlation features could sometimes improve scores. I found out I needed to provide all features and validate on models.\n\n# Model Execution Time\n- Computer specifications: Mac Book Pro 2.9 GHz i5 / 16 GB\n- EDA time: 5 minutes\n- Training and Validation time: 2 hours\n- Prediction time: 1 hour (a few minutes, if you use the saved XGBoost parameters.)\n- Grid search time: 10 hours"},{"metadata":{},"cell_type":"markdown","source":"# Dependencies"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport xgboost as xgb\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.externals import joblib\n\nimport re\nfrom itertools import product\nimport gc\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Library versions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check library versions\nfor p in [np, pd, sklearn, xgb]:\n    print (p.__name__, p.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load data from csv files\nsalesTrain = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv', dtype={'date': 'str', 'date_block_num': 'int32', 'shop_id': 'int32', 'item_id': 'int32', 'item_price': 'float32', 'item_cnt_day': 'int32'})\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales//test.csv', dtype={'ID': 'int32', 'shop_id': 'int32', 'item_id': 'int32'})\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales//items.csv', dtype={'item_name': 'str', 'item_id': 'int32', 'item_category_id': 'int32'})\nitemCategories = pd.read_csv('../input/competitive-data-science-predict-future-sales//item_categories.csv', dtype={'item_category_name': 'str', 'item_category_id': 'int32'})\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales//shops.csv', dtype={'shop_name': 'str', 'shop_id': 'int32'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25a7c7e3feac5e9032754525d373402d6c65012b","trusted":true},"cell_type":"code","source":"# Join item_category_id to sales_train data\nsales = pd.merge(salesTrain, items, on='item_id', how='left')\nsales = sales.drop('item_name', axis=1)\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Aggregate data"},{"metadata":{},"cell_type":"markdown","source":"Since the competition task is to make a monthly prediction, I need to aggregate the data to montly level before doing any encodings."},{"metadata":{"_uuid":"033a905ca7b4aa9c5d29be2e197e88f9fb1ebfc6","trusted":true},"cell_type":"code","source":"# For every month we create a grid from all shops/items combinations from that month\ngrid = []\nfor blockNum in sales.date_block_num.unique():\n    shopIds = sales.loc[sales.date_block_num == blockNum, 'shop_id'].unique()\n    itemIds = sales.loc[sales.date_block_num == blockNum, 'item_id'].unique()\n    grid.append(np.array(list(product(*[[blockNum], shopIds, itemIds])), dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns=['date_block_num', 'shop_id', 'item_id'], dtype='int32')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e08a7cff40d30a2a6c4c49ca483869331fea8b5b","trusted":true},"cell_type":"code","source":"grid.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3b555da3e07038d9564f393d39e77a421cbcce4","scrolled":true,"trusted":true},"cell_type":"code","source":"# Get aggregated values for (shop_id, item_id, month)\n#   The count and average price of sold items in each shop for a month\nsalesMean = sales.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_day': 'sum', 'item_price':'mean'}).reset_index()\nsalesMean = pd.merge(grid, salesMean, how='left', on=['date_block_num', 'shop_id', 'item_id']).fillna(0)\nsalesMean = pd.merge(salesMean, items, how='left', on=['item_id'])\nsalesMean.columns = ['date_block_num', 'shop_id', 'item_id', 'shop_item_count_sum', 'shop_item_price_mean', 'item_name', 'item_category_id']\nsalesMean.drop(['item_name'], axis=1, inplace=True)\nsalesMean.shop_item_count_sum = salesMean.shop_item_count_sum.astype('int32')\n\nsalesMean.head(10).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clip outliers"},{"metadata":{},"cell_type":"markdown","source":"The target values are clipped into [0, 20] range. so I'll clip values into the range and remove outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.ylim(0,200)\nplt.hist(salesMean.shop_item_count_sum, bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clip target values into [0, 20] range\nsalesMean.shop_item_count_sum = salesMean.shop_item_count_sum.clip(0, 20)\n\nplt.figure(figsize=(15, 5))\nplt.ylim(0,100000)\nplt.hist(salesMean.shop_item_count_sum, bins=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.ylim(0,10)\n# plt.xlim(0, 50000)\nplt.hist(salesMean.shop_item_price_mean, bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clip item prices into [0, 40000] range\nsalesMean.shop_item_price_mean = salesMean.shop_item_price_mean.clip(0, 40000)\n\nplt.figure(figsize=(15, 5))\nplt.ylim(0,1000)\nplt.hist(salesMean.shop_item_price_mean, bins=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d97150cf40ed6a059900cbdf88e82ed465164fe"},"cell_type":"markdown","source":"# Mean Encodings"},{"metadata":{},"cell_type":"markdown","source":"Shop, item and category ID are encoded in 3 ways:\n- Average price\n- Average count of sold items\n- Total count of sold items"},{"metadata":{"_uuid":"91c6985c5892fad337f669aeae7d7b37c6163edc","trusted":true},"cell_type":"code","source":"def encodeMean(groupColumns, tarnsformColumn, outputColumn):\n    gb = salesMean.groupby(groupColumns)\n    salesMean[outputColumn + '_mean'] = gb[tarnsformColumn].transform('mean').astype('float32')\n\ndef encodeMeanSum(groupColumns, tarnsformColumn, outputColumn):\n    gb = salesMean.groupby(groupColumns)\n    salesMean[outputColumn + '_mean'] = gb[tarnsformColumn].transform('mean').astype('float32')\n    salesMean[outputColumn + '_sum'] = gb[tarnsformColumn].transform('sum').astype('float32')\n\nencodeMean   (['date_block_num', 'shop_id'],          'shop_item_price_mean', 'shop_price')\nencodeMeanSum(['date_block_num', 'shop_id'],          'shop_item_count_sum',  'shop_count')\n\nencodeMean   (['date_block_num', 'item_id'],          'shop_item_price_mean', 'item_price')\nencodeMeanSum(['date_block_num', 'item_id'],          'shop_item_count_sum',  'item_count')\n\nencodeMean   (['date_block_num', 'item_category_id'], 'shop_item_price_mean', 'category_price')\nencodeMeanSum(['date_block_num', 'item_category_id'], 'shop_item_count_sum',  'category_count')\n\nsalesMean.head(10).T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"914cb68788011c6786521e82c6bf911364251cca"},"cell_type":"markdown","source":"# Lag Features"},{"metadata":{},"cell_type":"markdown","source":"The lag value of all mean encodings are created."},{"metadata":{"_uuid":"aee75fffdadbf01e528f3e354d34ea61102e93b2","trusted":true},"cell_type":"code","source":"lags = [1, 2, 3]\n\nbaseColumns = ['date_block_num', 'shop_id', 'item_id', 'item_category_id']\nlagColumns = ['shop_item_count_sum', 'shop_item_price_mean', 'shop_price_mean', 'shop_count_mean', 'shop_count_sum', 'item_price_mean',\n              'item_count_mean', 'item_count_sum', 'category_price_mean', 'category_count_mean', 'category_count_sum']\n\ndef addLags(salesOrigin, salesMerged):\n    for lag in lags:\n        s = salesOrigin.copy()\n        s.date_block_num += lag\n        s = s[baseColumns + lagColumns]\n        s.columns = baseColumns + [c + '_' + str(lag) for c in lagColumns]\n        salesMerged = pd.merge(salesMerged, s, how='left', on=baseColumns)\n    return salesMerged","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"551c89fc0f51114e970de69d5d896cde3b21af11"},"cell_type":"markdown","source":"# Fill NaN values"},{"metadata":{},"cell_type":"markdown","source":"NaN values are filled with zero and median."},{"metadata":{"_uuid":"93bf3fdf37893ee43c83e9f0b95b9dacf076a7d2","trusted":true},"cell_type":"code","source":"medians = salesMean.median()\n\ndef fillOutNan(df):\n    for column in df.columns:\n        if 'count' in column:\n            df[column] = df[column].fillna(0)\n        elif 'price' in column:\n            c = re.sub(r'_[0-9]+$', \"\", column)\n            df[column] = df[column].fillna(medians[c])\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create train/validate data"},{"metadata":{"_uuid":"9a2f9585ea59086c4ec3de28faa4ee504e8cda94","trusted":true},"cell_type":"code","source":"salesMeanLags = salesMean[baseColumns + ['shop_item_count_sum']]\n\n# Create the lag value of all mean encodings\nsalesMeanLags = addLags(salesMean, salesMeanLags)\n\n# Remove values having no lag valuses\nsalesMeanLags = salesMeanLags[salesMeanLags.date_block_num >= max(lags)]\n\n# Fill NaN with zero and median\nsalesMeanLags = fillOutNan(salesMeanLags)\n\nsalesMeanLags.head(10).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the correlation of all features for shop_item_count_sum\ncorr = salesMeanLags.corr()\nplt.figure(figsize=(25,1))\npc = pd.DataFrame([corr.loc['shop_item_count_sum', :]], columns=corr.index)\nsns.heatmap(pc, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validBlock = salesMeanLags.date_block_num.max()\n\nxAll = salesMeanLags.loc[:, salesMeanLags.columns != 'shop_item_count_sum']\nyAll = salesMeanLags.loc[:, salesMeanLags.columns == 'shop_item_count_sum']\n\nxTrain = xAll.loc[xAll.date_block_num <  validBlock]\nxValid = xAll.loc[xAll.date_block_num == validBlock]\n\nyTrain = yAll.loc[xAll.date_block_num <  validBlock]\nyValid = yAll.loc[xAll.date_block_num == validBlock]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create test data"},{"metadata":{"_uuid":"6276776e003fd922d2c2a3b7a5976dc35a6f8f69","trusted":true},"cell_type":"code","source":"# Join item_category_id and mean encodings to test data\nxTest = pd.merge(test, items, on='item_id', how='left')\nxTest = xTest.drop(['ID', 'item_name'], axis=1)\nxTest['date_block_num'] = salesMean.date_block_num.max() + 1\nxTest.date_block_num = xTest.date_block_num.astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f3276ce857e6772dea4a70c55c699f03bc2fb21","trusted":true},"cell_type":"code","source":"xTest = xTest[baseColumns]\n\n# Create the lag value of all mean encodings\nxTest = addLags(salesMean[salesMean.date_block_num > salesMean.date_block_num.max() - max(lags)], xTest)\n\n# Fill NaN with zero and median\nxTest = fillOutNan(xTest)\n\nxTest.head(10).T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d98523fce97aa37a4e0ce925b0d6e21b6da7766c","trusted":true},"cell_type":"code","source":"# Check the columns of test data\nassert sum(xTrain.columns != xTest.columns) == 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{},"cell_type":"markdown","source":"Linear regression and xgboost model are used as 1st level model to predcit."},{"metadata":{"trusted":true},"cell_type":"code","source":"# If true, eanble Grid Searh\ngridSearch = False\n\n# Prediction or Train/Valid\nprediction = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Free memory area"},{"metadata":{},"cell_type":"markdown","source":"I'll free memory area which are no longer needed to make calculation faster."},{"metadata":{"trusted":true},"cell_type":"code","source":"del salesTrain\ndel items\ndel itemCategories\ndel shops\ndel sales\ndel grid\ndel salesMean\ndel salesMeanLags\n\n# del gb\ndel corr\ndel pc\n\nif not prediction:\n    del xTrain\n    del xValid\n    del yTrain\n    del yValid\n\n# garbage collect\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression"},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"if not prediction:\n    lr = LinearRegression()\n    lr.fit(xTrain.values, yTrain.values)\n\n    predTrainLr = lr.predict(xTrain.values)\n\n    print('Train R-squared for LinearRegression is %f' % r2_score(yTrain, predTrainLr))\n    print('Train Mean Squared Error for LinearRegression is %f' % np.sqrt(mean_squared_error(yTrain, predTrainLr)))\n\n    predValidLr = lr.predict(xValid.values)\n\n    print('Valid R-squared for LinearRegression is %f' % r2_score(yValid, predValidLr))\n    print('Valid Mean Squared Error for LinearRegression is %f' % np.sqrt(mean_squared_error(yValid, predValidLr)))\n\n    # show coeficients\n    plt.figure(figsize=(15,3))\n    plt.bar(np.arange(lr.coef_.shape[1]), lr.coef_[0], tick_label=xTrain.columns)\n    plt.xticks(rotation='vertical')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"if prediction:\n    lr = LinearRegression()\n    lr.fit(xAll.values, yAll)\n\n    predAllLr = lr.predict(xAll.values)\n\n    print('All R-squared for LinearRegression is %f' % r2_score(yAll, predAllLr))\n    print('All Mean Squared Error for LinearRegression is %f' % np.sqrt(mean_squared_error(yAll, predAllLr)))\n\n    predTestLr = lr.predict(xTest.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show coeficients\nplt.figure(figsize=(15,3))\nplt.bar(np.arange(lr.coef_.shape[1]), lr.coef_[0], tick_label=xTest.columns)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_round': 100,\n          'eta': 0.3,\n          'seed': 123,\n          'silent': 1,\n          'eval_metric': 'rmse'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid search"},{"metadata":{},"cell_type":"markdown","source":"I'll find the best parameters for XGBoost."},{"metadata":{"trusted":true},"cell_type":"code","source":"if gridSearch:\n    gridParams = {'max_depth': [5, 8, 10], \n                  'min_child_weight': [0.5, 0.75, 1],\n                  'subsample': [0.5, 0.75, 1]}\n\n    cvCount = 30000 # xValid.shape[0]\n    x = xValid.values[: cvCount]\n    y = yValid.values[: cvCount, 0]\n\n    gs = GridSearchCV(xgb.XGBClassifier(**params), gridParams, cv=5)\n    gs.fit(x, y)\n\n    print('The best score is %f' % gs.best_score_)\n    print('The best parameters are %s' % gs.best_params_)\n\n    params.update(gs.best_params_)\nelse:\n    # Use parameters which are searched on the latest calculation\n    params.update({'max_depth': 10,\n                   'min_child_weight': 0.5,\n                   'subsample': 0.5})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d229849109f3a5a6bd071bf32510ed88fa1dcc3"},"cell_type":"markdown","source":"## Train"},{"metadata":{"_uuid":"08f1b9b022214194abc36aa0794a0e91df49a816","trusted":true},"cell_type":"code","source":"if not prediction:\n    bst = xgb.XGBClassifier(**params)\n    bst.fit(xTrain.values, yTrain.values[:, 0])\n\n    predTrainXgb = bst.predict(xTrain.values)\n    \n    print('Train R-squared for XGB is %f' % r2_score(yTrain, predTrainXgb))\n    print('Train Mean Squared Error for XGB is %f' % np.sqrt(mean_squared_error(yTrain, predTrainXgb)))\n\n    predValidXgb = bst.predict(xValid.values)\n\n    print('Valid R-squared for XGB is %f' % r2_score(yValid, predValidXgb))\n    print('Valid Mean Squared Error for XGB is %f' % np.sqrt(mean_squared_error(yValid, predValidXgb)))\n\n    # Show feature importances\n    bst.get_fscore()\n    mapper = {'f{0}'.format(i): v for i, v in enumerate(xTrain.columns)}\n    mapped = {mapper[k]: v for k, v in bst.get_fscore().items()}\n    fig, ax = plt.subplots(figsize=(10, 15))\n    xgb.plot_importance(mapped, ax=ax)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e579b7f863b5b7937f9b2c3f1dffdfa30d6a9cd"},"cell_type":"markdown","source":"## Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"if prediction:\n    dAll = xgb.DMatrix(xAll.values, label = yAll.values)\n    dTest = xgb.DMatrix(xTest.values)\n    \n#     bst = xgb.train(params, dAll)\n    \n    # Save model to file\n#     joblib.dump(bst, \"xgb.dat\")\n    \n    # Load model to file\n    bst = joblib.load(\"../input/xgbdat/xgb.dat\")\n    \n    predAllXgb = bst.predict(dAll)\n    \n    print('All R-squared for XGB is %f' % r2_score(yAll, predAllXgb))\n    print('All Mean Squared Error for XGB is %f' % np.sqrt(mean_squared_error(yAll, predAllXgb)))\n\n    predTestXgb = bst.predict(dTest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show feature importances\nbst.get_fscore()\nmapper = {'f{0}'.format(i): v for i, v in enumerate(xTrain.columns)}\nmapped = {mapper[k]: v for k, v in bst.get_fscore().items()}\nfig, ax = plt.subplots(figsize=(10, 15))\nxgb.plot_importance(mapped, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensembling"},{"metadata":{},"cell_type":"markdown","source":"I'll use a simple linear regression to combine the 1st level models."},{"metadata":{"trusted":true},"cell_type":"code","source":"if not prediction:\n    predTrainLv1 = pd.DataFrame()\n    predTrainLv1['lr']  = predTrainLr[:,0]\n    predTrainLv1['xgb'] = predTrainXgb\n\n    predValidLv1 = pd.DataFrame()\n    predValidLv1['lr']  = predValidLr[:,0]\n    predValidLv1['xgb'] = predValidXgb\n    \n    predTrainLv1.head(20).T\nelse:\n    predAllLv1 = pd.DataFrame()\n    predAllLv1['lr']  = predAllLr[:,0]\n    predAllLv1['xgb'] = predAllXgb\n\n    predTestLv1 = pd.DataFrame()\n    predTestLv1['lr']  = predTestLr[:,0]\n    predTestLv1['xgb'] = predTestXgb\n\n    predTestLv1.head(20).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"lrLv2 = LinearRegression()\n\nif not prediction:\n    lrLv2.fit(predTrainLv1, yTrain.values)\n\n    predTrainLv2 = lrLv2.predict(predTrainLv1)\n    predValidLv2 = lrLv2.predict(predValidLv1)\n\n    print('Train R-squared for Ensembling is %f' % r2_score(yTrain, predTrainLv2))\n    print('Train Mean Squared Error for Ensembling is %f' % np.sqrt(mean_squared_error(yTrain, predTrainLv2)))\n\n    print('Valid R-squared for Ensembling is %f' % r2_score(yValid, predValidLv2))\n    print('Valid Mean Squared Error for Ensembling is %f' % np.sqrt(mean_squared_error(yValid, predValidLv2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not prediction:\n    # Show coefitients\n    plt.figure(figsize=(5,3))\n    plt.bar(np.arange(lrLv2.coef_.shape[1]), lrLv2.coef_[0], tick_label=predTrainLv1.columns)\n    plt.xticks(rotation='vertical')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"if prediction:\n    lrLv2.fit(predAllLv1, yAll.values)\n\n    predAllLv2 = lrLv2.predict(predAllLv1)\n\n    print('All R-squared for Ensembling is %f' % r2_score(yAll, predAllLv2))\n    print('All Mean Squared Error for Ensembling is %f' % np.sqrt(mean_squared_error(yAll, predAllLv2)))\n\n    predTestLv2 = lrLv2.predict(predTestLv1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if prediction:\n    # Show coefitients\n    plt.figure(figsize=(5,3))\n    plt.bar(np.arange(lrLv2.coef_.shape[1]), lrLv2.coef_[0], tick_label=predTestLv1.columns)\n    plt.xticks(rotation='vertical')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create submission file"},{"metadata":{"_uuid":"302e0afbbbb8c4a96e73ec7b33042078446365b1","trusted":true},"cell_type":"code","source":"if prediction:\n#     pred = predTestLv2[:,0]\n    pred = predTestXgb\n    \n    pred = pred.clip(0, 20)\n    submission = pd.DataFrame({'ID': test.index, 'item_cnt_month': pred})\n    submission.to_csv('submission.csv',index=False)\n\n    print(submission.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if prediction:\n    print(submission.describe())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":1}