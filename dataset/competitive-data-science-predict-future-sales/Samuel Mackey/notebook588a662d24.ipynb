{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nimport keras\nimport pandas as pd\nimport lightgbm as lgb\nimport time\nimport numpy as np\nimport pickle\nimport sklearn\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nfor package in [pd, keras, np, lgb, sklearn]:\n    print(package.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load required data into dataframe"},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train    = pd.read_csv('sales_train.csv')\nitems           = pd.read_csv('items_english.csv')\nitem_categories = pd.read_csv('categories_english.csv')\nshops           = pd.read_csv('shops_english.csv')\ntest           = pd.read_csv('test.csv')\n\nsales_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract features from shops and categories. We see the first part of shops is usually a town. We can use this as a feature. Since there are not many shops, we can define a function to take care of edge cases"},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_town(shop_name):\n    pre = shop_name.split()[0]\n    if pre == 'St.':\n        return 'St Petersburg'\n    elif pre == '!':\n        return 'Yakutsk'\n    elif 'Rostov' in pre:\n        return 'Rostov'\n    elif 'Moscow' in pre:\n        return 'Moscow'\n    elif 'Internet' in pre or 'Digital' in pre:\n        return 'Online'\n    else:\n        return pre\n\nshops['town'] = shops['shop_name'].apply(get_town)\nshops.head(60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perform similar preprocessing on categories. In most cases, the category is divided into \"{category} - {subcategory}\". Can use this as a feature"},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_subcategories(category_name):\n    s = category_name.split(' - ')\n    if len(s) == 2:\n        return s[0], s[1]\n    else:\n        return s[0], None\n    \nitem_categories['category_1'], item_categories['category_2'] = zip(*item_categories['category_name'].apply(get_subcategories))\nitem_categories.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get unique items, duplicate rows and info on nulls"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f'{len(sales_train)} total sales {len(items)} items {len(item_categories)} categories in train')\nprint(f'{len(test)} pairs {test[\"shop_id\"].nunique()} shops {test[\"item_id\"].nunique()} items in test')\nsales_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No null rows to remove. Check duplicates"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f'{sales_train.duplicated().sum()} duplicates found and removed')\nsales_train.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"item_price should not be negative (returns are signified by item_cnt_day being negative). Check for these rows and remove"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f'{(sales_train[\"item_price\"] < 0).sum()} item_price < 0 removed')\nsales_train = sales_train[sales_train['item_price'] >= 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Substitute shop id's as noted above"},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train[sales_train['shop_id'] == 57]['shop_id'] = 0\nsales_train[sales_train['shop_id'] == 58]['shop_id'] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sales over time"},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train[['date_block_num','item_cnt_day']].groupby('date_block_num').sum().plot()\nplt.show()\nsales_train[['item_price']].boxplot(vert=False)\nplt.show()\nsales_train[['item_cnt_day']].boxplot(vert=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sales seem to peak in November and December and steadily drop over time. We are almost certainly safe to remove the maximum value in both series. "},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train.drop(sales_train['item_price'].idxmax(), inplace=True)\nsales_train.drop(sales_train['item_cnt_day'].idxmax(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Month and year are also useful as features"},{"metadata":{"trusted":false},"cell_type":"code","source":"dates = sales_train[['date','date_block_num']]\ndates['date'] = pd.to_datetime(dates['date'],format='%d.%m.%Y')\ndates['month'] = dates['date'].dt.month\ndates['year'] = dates['date'].dt.year\ndates.drop('date',axis=1,inplace=True)\ndates.drop_duplicates(inplace=True)\n\n#add test block num\ndates = dates.append({'date_block_num' : 34, 'month' : 11, 'year' : 2015},ignore_index=True)\ndates.head(50)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now start aggregating the data into the correct form for the task"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = sales_train[['date_block_num','shop_id','item_id','item_cnt_day']]. \\\n    groupby(['date_block_num','shop_id','item_id'], as_index=False).agg({'item_cnt_day' : 'sum'})\ndf['item_cnt_month'] = df['item_cnt_day']\ndf.drop(['item_cnt_day'],inplace=True, axis=1)\ndf.tail(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only concerned with pairs in the test set, so find these for every month. For each date block, join with the test data on shop_id and item_id. If a row has ID null, it doesn't appear in the test data and may therefore be dropped. If item_cnt_month is null, it indicates that the pair had no sales in the date block and this should be filled with 0. If a row has no null values, that indicates the pair appeared in both the train and test data."},{"metadata":{"trusted":false},"cell_type":"code","source":"data = pd.DataFrame()\nfor date_block_num in df['date_block_num'].unique():\n    block = df[df['date_block_num'] == date_block_num]\n    new = block.merge(test, on=['shop_id','item_id'],how='outer')\n    new.dropna(subset=['ID'],inplace=True)\n\n    new['date_block_num'].fillna(date_block_num,inplace=True)\n    new['item_cnt_month'].fillna(0 ,inplace=True)\n    \n    data = pd.concat([data, new])\n\n#add test data as date block num 34, but leave item_cnt_month as null (target value)\ndata.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test['date_block_num'] = 34\ndata = pd.concat([data, test])\ndata.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sanity check. Each pair should appear exactly 35 times (one for each date block)"},{"metadata":{"trusted":false},"cell_type":"code","source":"pair = data['shop_id'].astype(str) + '-' + data['item_id'].astype(str)\n\nassert len(pair.value_counts().unique()) == 1\nassert pair.value_counts().unique() == 35","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have our training data. The first 34 blocks will be used to fit several time series models with the final one for testing. Merge item and shop names into our data for feature engineering"},{"metadata":{"trusted":false},"cell_type":"code","source":"item_data = data.merge(items, on=['item_id'])\ncat_data = item_data.merge(item_categories, on=['category_id'])\nfull_data = cat_data.merge(shops, on=['shop_id'])\nfull_data_dates = full_data.merge(dates, on=['date_block_num'])\nfull_data_dates.sort_values('date_block_num',inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have done EDA and feature preprocessing (may do more in notebook depending on model). Save the resulting sheet for use later. We don't need to save shop category and item names as these correlate perfectly with their respective ids and we have already extracted features"},{"metadata":{"trusted":false},"cell_type":"code","source":"full_data_dates.drop(columns=['shop_name','category_name','item_name'],inplace=True)\nfull_data_dates.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clip sales to acceptable range and add 1,2,3,6 and 12 month lag for each pair"},{"metadata":{"trusted":false},"cell_type":"code","source":"data = full_data_dates\ndata['category_1'] = data['category_1'].astype('category').cat.codes\ndata['category_2'] = data['category_2'].astype('category').cat.codes\ndata['town'] = data['town'].astype('category').cat.codes\ndata['item_cnt_month'] = data['item_cnt_month'].clip(0, 20)\n\nshift_range = [1,2,3,6,12]\nfor shift in shift_range:\n    \n    train_shift = data[['ID','date_block_num','item_cnt_month']]\n    train_shift['date_block_num'] = train_shift['date_block_num'] + shift\n    train_shift[f'item_cnt_month_lag_{shift}'] = train_shift['item_cnt_month']\n    train_shift.drop('item_cnt_month',inplace=True,axis=1)\n    \n    data = data.merge(train_shift, on = ['ID','date_block_num'],how='left')\n    data[f'item_cnt_month_lag_{shift}'] = data[f'item_cnt_month_lag_{shift}'].fillna(0)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add mean encoded features for 6 and 12 month intervals for relevant columns. Discard old data (from 2013) but still use it for mean encoding."},{"metadata":{"trusted":false},"cell_type":"code","source":"mae_cols = ['shop_id','item_id','category_id','category_1','category_2','town']\ndates = data['date_block_num']\nvalidation_block = 33\n\nts = time.time()\n#for date_block_num in dates.unique():\n\nres = pd.DataFrame()\n\nfor date_block_num in dates.unique():\n    \n    subset = data.loc[dates == date_block_num]\n    \n    for col in mae_cols:\n\n        df1 = data.loc[(dates < date_block_num) & (dates > date_block_num - 6), ['date_block_num', col, 'item_cnt_month']]\n        df2 = data.loc[(dates < date_block_num) & (dates > date_block_num - 12), ['date_block_num', col, 'item_cnt_month']]\n\n        matrix1 = df1.drop('date_block_num',axis=1)\n        avg1 = matrix1.groupby(col, as_index=False).mean()\n        avg1['date_block_num'] = date_block_num\n        avg1.rename(columns={'item_cnt_month' : f'{col}_cnt_6_month_average'}, inplace=True)\n        \n        matrix2 = df2.drop('date_block_num',axis=1)\n        avg2 = matrix2.groupby(col, as_index=False).mean()\n        avg2['date_block_num'] = date_block_num\n        avg2.rename(columns={'item_cnt_month' : f'{col}_cnt_12_month_average'}, inplace=True)\n\n        subset = subset.merge(avg1, on=[col,'date_block_num'],how='left')\n        subset = subset.merge(avg2, on=[col,'date_block_num'],how='left')\n        \n    res = pd.concat([res, subset])\n\nres.fillna(0, inplace=True)\nprint(f'calcualting mae features took {time.time() - ts} seconds')\n\n#discard old data\nres = res[res.date_block_num > 11]\nres = res.sort_values(by = ['date_block_num','ID'], ascending = [True, True])\nprint(res.columns)\nprint(len(res))\n\n#check ordering is the same for two date blocks as this property will be used later\nassert (res[res.date_block_num == 15]['ID'] == res[res.date_block_num == 12]['ID']).all()\n\nX_train = res[res.date_block_num < validation_block].drop(['item_cnt_month','date_block_num'], axis=1)\ny_train = res[res.date_block_num < validation_block]['item_cnt_month']\nX_valid = res[res.date_block_num == validation_block].drop(['item_cnt_month','date_block_num'], axis=1)\ny_valid = res[res.date_block_num == validation_block]['item_cnt_month']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train lightGBM"},{"metadata":{"trusted":false},"cell_type":"code","source":"ts = time.time()\nlgb_params = {\n    'feature_fraction': 0.75,\n    'metric': 'rmse',\n    'bagging_fraction': 0.75, \n    'learning_rate': 0.03, \n    'objective': 'mse',\n    'num_leaves': 1000,\n    'max_depth' : 20,\n    'bagging_freq':1,\n    'verbose':-1,\n}\n\ntrain_data = lgb.Dataset(X_train, label = y_train)\nvalid_data = lgb.Dataset(X_valid, label = y_valid)\n\nlgb_model = lgb.train(\n    lgb_params,\n    train_data,\n    valid_sets=valid_data,\n    num_boost_round=1000,\n    verbose_eval=False\n)\nt = time.time() - ts\n\n#save lgb model\nwith open('lgb_model.pkl','wb') as f:\n    pickle.dump(lgb_model, f)\n\ny_pred_train = lgb_model.predict(X_train)\n\nr2 = r2_score(y_train, y_pred_train)\nmse = mean_squared_error(y_train, y_pred_train)\n\nprint(f'R-squared for training data is {r2}')\nprint(f'MSE for training data is {mse}')\nprint(f'Fitting time is {t} seconds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neural network"},{"metadata":{"trusted":false},"cell_type":"code","source":"ts = time.time()\n\nnn_model = Sequential()\n\nnn_model.add(Dense(100, input_dim=26, activation='relu'))\nnn_model.add(Dense(100, activation='relu'))\nnn_model.add(Dense(1, activation='linear'))\n\nnn_model.compile(loss='mse', optimizer='adam')\nnn_model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_valid, y_valid))\n\nt = time.time() - ts\n\n#save lgb model\nwith open('nn_model.pkl','wb') as f:\n    pickle.dump(nn_model, f)\n\ny_pred_train = nn_model.predict(X_train)\n\nr2 = r2_score(y_train, y_pred_train)\nmse = mean_squared_error(y_train, y_pred_train)\n\nprint(f'R-squared for training data is {r2}')\nprint(f'MSE for training data is {mse}')\nprint(f'Fitting time is {t} seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_lgb = lgb_model.predict(X_valid)\nr2 = r2_score(y_valid, y_pred_lgb)\nmse = mean_squared_error(y_valid, y_pred_lgb)\n\nprint(f'Test R-squared for block {validation_block} is {r2} for lgb')\nprint(f'Test MSE for block {validation_block} is {mse} for lgb')\n\ny_pred_nn = nn_model.predict(X_valid)\nr2 = r2_score(y_valid, y_pred_nn)\nmse = mean_squared_error(y_valid, y_pred_nn)\n\nprint(f'Test R-squared for block {validation_block} is {r2} for nn')\nprint(f'Test MSE for block {validation_block} is {mse} for nn')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit meta model. Note that in this case, linear regression has no hyper parameters to tune, so can fit on the validation block and then use the resulting model on the test data"},{"metadata":{"trusted":false},"cell_type":"code","source":"meta_features = np.stack([y_pred_nn.squeeze(axis=1), y_pred_lgb], axis=1)\n\nmeta_model = LinearRegression()\nmeta_model.fit(meta_features, y_valid)\n\ny_pred_meta = meta_model.predict(meta_features)\nr2 = r2_score(y_valid, y_pred_meta)\nmse = mean_squared_error(y_valid, y_pred_meta)\n\n#save lgb model\nwith open('meta_model.pkl','wb') as f:\n    pickle.dump(meta_model, f)\n    \nprint(f'R-squared for training is {r2} for meta')\nprint(f'MSE for training is {mse} for meta')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run this cell to load models to avoid training time"},{"metadata":{"trusted":false},"cell_type":"code","source":"with open('nn_model.pkl','rb') as f:\n    nn_model = pickle.load(f)\nwith open('lgb_model.pkl','rb') as f:\n    lgb_model = pickle.load(f)\nwith open('meta_model.pkl','rb') as f:\n    meta_model = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make predictions for submission file"},{"metadata":{"trusted":false},"cell_type":"code","source":"test_block = 34\n\nX_test = res[res.date_block_num == test_block].drop(['item_cnt_month','date_block_num'], axis=1)\ny_test = res[res.date_block_num == test_block]['item_cnt_month']\n\nmeta_nn = nn_model.predict(X_test).squeeze(axis=1)\nmeta_lgb = lgb_model.predict(X_test)\nmeta_features = np.stack([meta_nn, meta_lgb], axis=1)\n\ny_test = meta_model.predict(meta_features).clip(0, 20)\nX_test['item_cnt_month'] = y_test\n\nsub = X_test[['ID','item_cnt_month']]\n\nsub['ID'] = sub['ID'].astype(int)\nsub = sub.sort_values('ID')\n\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}