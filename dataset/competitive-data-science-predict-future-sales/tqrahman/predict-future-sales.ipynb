{"cells":[{"metadata":{},"cell_type":"markdown","source":"The goal of this notebook is to figure out how to format the data so that we can tackle the task at hand. The task was to predict the number of items sold for each store given the month. The data was not set-up to answer this question. We needed to organize/format the data to match the task."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Imports\n\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## sales_train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading in sales_train\n\nsales_train = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\nsales_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing the shape of the dataframe\n\nsales_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing the date into a pandas datatime\n\nsales_train['date'] = pd.to_datetime(sales_train['date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing basic stats on data\n\nsales_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing the info for each column\n\nsales_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing the number of unique items and shops in training data\n\n# Viewing the number of items\nprint(\"Number of unique items: {}\".format(len(sales_train[\"item_id\"].unique()))) # 21807 items in total))\n\n# Viewing the number of shops\nprint(\"Number of unique shops: {}\".format(len(sales_train[\"shop_id\"].unique()))) # 60 shops in total )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the number of items sold per month\n\n## Grouping by month and viewing number of items sold in each month\nsales_per_month = sales_train.groupby(['date_block_num'])['item_cnt_day'].sum()\n\n## Plotting\nfig = px.bar(sales_per_month, title=\"Number of Items Sold per Month\", labels={\"date_block_num\":\"Month\", \"value\":\"Count\"})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## items"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading in items\n\nitems = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing the number of items and number of categories\n\n# Viewing the number of items\nprint(\"Number of unique items: {}\".format(len(items[\"item_id\"].unique())))\n\n# Viewing the number of categories\nprint(\"Number of unique categories: {}\".format(len(items[\"item_category_id\"].unique()))) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## item_categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading in item_categories\n\nitem_categories = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nitem_categories.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting sub categories from item_category_name\n\n## Spliting the category by '-'\nitem_categories['categories'] = item_categories['item_category_name'].str.split('-')\n\n## Extracting the first element from split\nitem_categories['type'] = item_categories['categories'].apply(lambda x: x[0].strip())\n\n## Extracting second element if there is a second element, else return first element \nitem_categories['sub_type'] = item_categories['categories'].apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping unnecessary columns\n\nitem_categories.drop(['item_category_name', 'categories'], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Creating dummy variables\n\n# ## Creating dummies for type \n# item_categories = pd.concat([item_categories, pd.get_dummies(item_categories['type'], drop_first=True)], axis=1)\n\n# ## Creating dummies for sub_type \n# item_categories = pd.concat([item_categories, pd.get_dummies(item_categories['sub_type'], drop_first=True)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting \n\nfig = px.bar(item_categories.groupby('type')['sub_type'].count(), title=\"Number of Sub_categories in Each Category\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## shops"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading in shops\n\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\nshops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing shop ids with other duplicate shop_ids\n\n## Shopnames for 0 and 57 are the same so changing shop_id 0 to 57\nshops.loc[shops['shop_id']==0, 'shop_id'] = 57\n\n## Shopnames for 1 and 58 are the same so changing shop_id 1 to 58\nshops.loc[shops['shop_id']==1, 'shop_id'] = 58\n\n## Shopnames for 10 and 11 are the same so changing shop_id 00 to 11\nshops.loc[shops['shop_id']==10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Collecting the duplicate names just in case \n\nduplicate_shop_names = {\n    shops.loc[shops['shop_id']==57, 'shop_name'].values[0]:shops.loc[shops['shop_id']==57, 'shop_name'].values[1],\n    shops.loc[shops['shop_id']==58, 'shop_name'].values[0]:shops.loc[shops['shop_id']==58, 'shop_name'].values[1],\n    shops.loc[shops['shop_id']==11, 'shop_name'].values[0]:shops.loc[shops['shop_id']==11, 'shop_name'].values[1] \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops = shops.drop_duplicates(subset='shop_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieving city from shop_name\n\nshops['city'] = shops['shop_name'].str.split(' ').apply(lambda x: x[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Small corrections\n\n## Removing space\nshops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n\n## Removing ! from '!Якутск'\nshops.loc[shops['city'] == '!Якутск', 'city'] = 'Якутск'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the number of shops in each city\n\nfig = px.bar(shops.groupby('city')['shop_id'].count(), title='Number of Stores in a City')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing the number of unique shops\n\nprint(\"Number of unique shops: {}\".format(len(shops['shop_id'].unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing shop ids in sales_train\n\n## Shopnames for 0 and 57 are the same so changing shop_id 0 to 57\nsales_train.loc[sales_train['shop_id']==0, 'shop_id'] = 57\n\n## Shopnames for 1 and 58 are the same so changing shop_id 1 to 58\nsales_train.loc[sales_train['shop_id']==1, 'shop_id'] = 58\n\n## Shopnames for 10 and 11 are the same so changing shop_id 00 to 11\nsales_train.loc[sales_train['shop_id']==10, 'shop_id'] = 11","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The task is to **predict the sales for each product in a store given month**. Therefore we have to downsample the data to represent *monthly sales per item per store*."},{"metadata":{},"cell_type":"markdown","source":"## Combining Data into one Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the dataframes\n\n## Merging sales_train with item\ndf = pd.merge(sales_train, items, how=\"left\", on=\"item_id\")\n\n## Merging df and item_categories\ndf = pd.merge(df, item_categories, how=\"left\", on=\"item_category_id\")\n\n## Merging df and shops\ndf = pd.merge(df, shops, how=\"left\", on=\"shop_id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping data by month, shop_id, item_id to get total sales\n\ndata = df.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_price':'mean', 'item_cnt_day':'sum'}).reset_index()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the dataframes\n\n## Merging sales_train with item\ndata = pd.merge(data, items, how=\"left\", on=\"item_id\")\n\n## Merging df and item_categories\ndata = pd.merge(data, item_categories, how=\"left\", on=\"item_category_id\")\n\n## Merging df and shops\ndata = pd.merge(data, shops, how=\"left\", on=\"shop_id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting sales by month\n\ndata['total_sales'] = data['item_price'] * data['item_cnt_day']\n\nfig = px.line(data.groupby('date_block_num')['total_sales'].sum(), \n              title=\"Sales by Month\", \n              labels={\"date_block_num\":\"Month\",\n                      \"value\":\"Total Sales\"})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data shows that the sales are seasonal. The spikes represents monthDecember which makes sense why there are a lot of sales (Christmas)."},{"metadata":{},"cell_type":"markdown","source":"## Data Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting\n\ndata['month'] = data['date_block_num'].apply(lambda month: (month+1)%12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dummifying the categorical columns\n\n# ## Creating dummies and concatenating\ndata = pd.concat([data, pd.get_dummies(data['shop_id'], drop_first=True, prefix='shop_')], axis=1)\n\n## Creating dummies and concatenating\ndata = pd.concat([data, pd.get_dummies(data['type'], drop_first=True, prefix='type')], axis=1)\n\n## Creating dummies and concatenating\ndata = pd.concat([data, pd.get_dummies(data['sub_type'], drop_first=True, prefix='sub_type')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the names of the feature columns\n\n# Collecting shop feature names\nshop_columns = [col for col in data.columns if 'shop__' in col]\n\n## Collecting type feature names\ntype_columns = [col for col in data.columns if 'type_' in col]\n\n## Collecting sub_type feature names\nsub_type_columns = [col for col in data.columns if 'sub_type_' in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting the feature and target variables\n\nfeatures = ['month', 'shop_id','item_id', 'item_price'] + type_columns + sub_type_columns\ntarget = ['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline Model using Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing data for modeling\n\n## Import for splitting data\nfrom sklearn.model_selection import train_test_split\n\n## Setting feature and target variables\nX = data[features].fillna(value=0)\ny = data[target].fillna(value=0)\n\n## Splitting train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Linear Regression\n\n## Getting LR function\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\n## Fitting on training data\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading in test\n\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding data_block_num\n\ntest['date_block_num'] = 34\ntest['month'] = 11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_price = data[['item_id', 'item_price']].groupby('item_id')['item_price'].mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the test data with dataframes\n\n## Merging sales_train with item\ntest = pd.merge(test, item_price, how=\"left\", on=\"item_id\")\n\n## Merging items with test\ntest = pd.merge(test, items, how=\"left\", on=\"item_id\")\n\n## Merging df and item_categories\ntest = pd.merge(test, item_categories, how=\"left\", on=\"item_category_id\")\n\n## Merging df and shops\ntest = pd.merge(test, shops, how=\"left\", on=\"shop_id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dummifying the categorical columns\n\n# ## Creating dummies and concatenating\ntest = pd.concat([test, pd.get_dummies(test['shop_id'], drop_first=True, prefix='shop_')], axis=1)\n\n## Creating dummies and concatenating\ntest = pd.concat([test, pd.get_dummies(test['type'], drop_first=True, prefix='type')], axis=1)\n\n## Creating dummies and concatenating\ntest = pd.concat([test, pd.get_dummies(test['sub_type'], drop_first=True, prefix='sub_type')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are getting this issue because there are columns in training data that is not in the test data. We need to create a set of features that are in both training data and test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the names of the feature columns\n\n# Collecting shop feature names\ntest_shop_columns = [col for col in test.columns if 'shop__' in col]\n\n## Collecting type feature names\ntest_type_columns = [col for col in test.columns if 'type_' in col]\n\n## Collecting sub_type feature names\ntest_sub_type_columns = [col for col in test.columns if 'sub_type_' in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting the feature and target variables\n\ntest_features = ['month', 'item_id' + 'shop_id', 'item_price'] + test_type_columns + test_sub_type_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparing the features in train and test data\n\nprint(f\"Number of predictors in train data: {len(features)}\")\nprint(f\"Number of predictors in test data: {len(test_features)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are predictors in the training data that is not in the test data. Why? It is because there are items that were in the training data that were not in the test data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"common_features = list(set(features) & set(test_features)) \nprint(f\"Number of common features: {len(common_features)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'item_price' in common_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are items in training data that is not in test data and vice versa. We are extracting the common features in both training and test data to fit the model."},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression using Common Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing data for modeling\n\n## Import for splitting data\nfrom sklearn.model_selection import train_test_split\n\n## Setting feature and target variables\nX = data[common_features].fillna(value=0)\ny = data[target].fillna(value=0)\n\n## Splitting train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Linear Regression\n\n## Getting LR function\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\n## Fitting on training data\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'item_price' in test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.predict(test[common_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['preds'] = lr.predict(test[common_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the submission dataframe\n\npreds = test[['ID', 'preds']]\npreds.columns = ['ID', 'item_cnt_month']\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving the submission dataframe\npreds.to_csv('my_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The goal of thie notebook was to format the data. We just applied a Linear Regression to see if the format was correct, which it was. The next steps are to:\n1. Apply times-series related models\n2. Make better fill-in choices\n3. Make better feature engineering choices"},{"metadata":{},"cell_type":"markdown","source":"# ARIMA"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.bar(data, x='month', y='item_cnt_day')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}