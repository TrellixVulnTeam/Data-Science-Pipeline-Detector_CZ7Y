{"cells":[{"metadata":{},"cell_type":"markdown","source":"The Part-1 of this competition which deals with data preparation can be found @ https://www.kaggle.com/jatinmittal0001/predict-future-sales-part-1"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport gc\nimport pickle\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom itertools import product\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import plot_importance\nfrom matplotlib.pyplot import figure\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\nimport os\nprint(os.listdir(\"../input/predict-future-sales-part-1\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Importing datasets\nsales_train = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\ntest.drop('ID',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In previous part we made shop_id 10 equal to shop_id 11 for train part, doing same for test part now."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_shop_item_pair = test[(test.shop_id==10)]\ntest_shop_item_pair.loc[test_shop_item_pair.shop_id == 10, 'shop_id']= 11\ntest.loc[test.shop_id == 10, 'shop_id']= 11","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining function to reduce size of dataframe by downcasting data types."},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_size(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to open the dataset from previous kernel\nall_data1 = pickle.load(open(\"../input/predict-future-sales-part-1/all_data1\",\"rb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{},"cell_type":"markdown","source":"I ran model multiple times with different features and now keeping features which came out to be important."},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_to_keep = ['date_block_num', 'shop_id', 'item_id', 'item_cnt_month',\n       'item_category_id', 'subtype', 'db_avg_items_sold_lag_1',\n       'db_shop_cat_avg_items_sold_lag_1', 'db_item_id_items_sold_lag_1',\n       'db_item_id_items_sold_lag_2', 'db_item_id_items_sold_lag_3',\n       'db_item_id_items_sold_lag_6','city_target_enc',\n       'item_id_target_enc', 'month_target_enc','db_shop_city_avg_items_sold_lag_1',\n       'db_shop_city_avg_items_sold_lag_2', 'db_city_avg_items_sold_lag_1',\n       'month', 'item_months_since_first_sale','item_shop_last_sale',\n       'db_item_avg_price_lag_1','delta_price_lag', 'delta_price_lag_1',\n        'delta_price_lag_3','max_cnt_lag_1', 'max_cnt_lag_3','max_cnt_lag_6',  'revenue_shop_lag_2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data1 = all_data1.loc[:,feat_to_keep]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = all_data1[all_data1.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = all_data1[all_data1.date_block_num < 33]['item_cnt_month']\nX_valid = all_data1[all_data1.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = all_data1[all_data1.date_block_num == 33]['item_cnt_month']\nX_test = all_data1[all_data1.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\ndel all_data1\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Why is standardization/normalization required?\nTo make our features of same range so that our gradient descent converges faster.\n\nHere we do not require above techniques, for following reasons:\n1. We will try our model on XGBoost which does not use Gradient Boosting per se.\n2. Even if we use other models that use GB, our features are almost of similar range expect item_id feature, so we convergence of GB won't be affected much. Item_id feature is like a categorical feature which makes no sense to be numerically scaled.\n\nRight now we are not using One hot encoding, that area can also be explored.\nBut we are using other kind of encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining function to calculate RMSE, for manual evaluation\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\ndef rmse(y_pred, y_test):\n    rmse = sqrt(mean_squared_error(y_test,y_pred))\n    return print(rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining function to predict and make final submission, useful if you want to try out multiple models."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model,name):\n    Y_test = model.predict(X_test).clip(0., 20.)\n    results = X_test.loc[:,['shop_id', 'item_id']]\n    results['prediction'] = Y_test\n    if len(test.columns)==3:\n        test.drop('ID',axis=1,inplace=True)\n    sub = pd.merge(test, results, on = ['shop_id', 'item_id'], how='left')\n    submission = pd.DataFrame({\n        \"ID\": test.index, \n        \"item_cnt_month\": sub['prediction']\n    })\n    file_name = str(name) + '_submission.csv'\n    submission.to_csv(file_name, index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBOOST"},{"metadata":{},"cell_type":"markdown","source":"Apart from XGBoost, I tried other individual models as marked in comments, but they were not giving improvement in performance.\nI also tried Ensembling (Stacking) but it was also not giving improvement in performance as compared to only XGBoost. So I am only using XGBoost. However, I am marking those models under comments for you to try and learn.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#base model-1\n\nxgb = XGBRegressor(\n    max_depth=8,\n    n_estimators=45,\n    min_child_weight=300, \n    colsample_bytree=0.9, \n    subsample=0.8, \n    eta=0.3,    \n    seed=4)\n\nxgb.fit(X_train, Y_train, eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n        eval_metric = 'rmse', early_stopping_rounds =10,verbose=True)\n\npredict(xgb,'xgb')\n\nplot_features(xgb, (10,30))\n\n'''\n\n\n\nmodel1_train = xgb.predict(X_train)\nmodel1_valid = xgb.predict(X_valid)\nmodel1_test = xgb.predict(X_test)\n\n\ndel xgb\ngc.collect();\n\n#base model-2\nsvr_model= SVR(kernel='rbf', degree=10, verbose=True, max_iter = 50)\n\nsvr_model.fit(X_train, Y_train)\nmodel2_train = svr_model.predict(X_train)\nmodel2_valid = svr_model.predict(X_valid)\nmodel2_test = svr_model.predict(X_test)\n\n\n\ndel svr_model\ngc.collect();\n\n#base model-3\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha=0.05, max_iter=100)\nlasso.fit(X_train, Y_train)\nmodel3_train = lasso.predict(X_train)\nmodel3_valid = lasso.predict(X_valid)\nmodel3_test = lasso.predict(X_test)\n\n\ndel lasso\ngc.collect();\n\n#stacking\nbase_predictions_train = pd.DataFrame( {'XGBoost': model1_train.ravel(),\n     'SVR model': model2_train.ravel(),\n     'lasso': model3_train.ravel()\n     #'SVM': model4_train.ravel()\n    })\nbase_predictions_test = pd.DataFrame( {'XGBoost': model1_test.ravel(),\n     'SVR model': model2_test.ravel(),\n     'lasso': model3_test.ravel()\n     #'SVM': model4_test.ravel()\n    })\n\nbase_predictions_valid = pd.DataFrame( {'XGBoost': model1_valid.ravel(),\n     'SVR model': model2_valid.ravel(),\n     'lasso': model3_valid.ravel()\n     #'SVM': model4_valid.ravel()\n    })\n\nX_new_train = base_predictions_train.as_matrix()\nX_new_valid = base_predictions_valid.as_matrix()\nX_new_test = base_predictions_test.as_matrix()\nbase_predictions_train.head()\n\n#heatmap to see correlation between different predictions\nsns.heatmap(base_predictions_train.astype(float).corr(),\n            linewidths=0.1,vmax=1.0, square=True, linecolor='white', annot=True)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n# META Model\nlm = LinearRegression()\nlm.fit(X_new_train,Y_train)\ny_valid_pred = lm.predict(X_new_valid)\nplt.plot(y_valid_pred, '.', Y_valid, 'x')\n#y_test_pred = lm.predict(X_new_test)\n\nY_test = lm.predict(X_new_test).clip(0., 20.)\nresults = X_test.loc[:,['shop_id', 'item_id']]\nresults['prediction'] = Y_test\nif len(test.columns)==3:\n    test.drop('ID',axis=1,inplace=True)\nsub = pd.merge(test, results, on = ['shop_id', 'item_id'], how='left')\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": sub['prediction']\n})\nsubmission.to_csv('stack_submission.csv', index=False)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfrom catboost import CatBoostRegressor\n\nmodel = CatBoostRegressor(\n    random_seed=63,\n    iterations=200,\n    learning_rate=0.12,\n    depth=4,\n    loss_function='RMSE',\n    rsm = 0.85,\n    od_type='Iter',\n    od_wait=20,\n)\nmodel.fit(\n    X_train, Y_train,\n    logging_level='Silent',\n    eval_set=(X_valid, Y_valid),\n    plot=True\n)\n\nimportances = model.get_feature_importance(prettified=True)\nfeature_labels = []\nfeature_value = []\nfor i in range(0,len(importances)):\n    feature_labels.append(importances[i][0])\n    feature_value.append(importances[i][1])\n    \nfig, ax = plt.subplots(1,1,figsize=(10,20))\nsns.barplot(y = feature_labels,  x=feature_value, ax=ax)\n\npredict(model,'catboost')\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}