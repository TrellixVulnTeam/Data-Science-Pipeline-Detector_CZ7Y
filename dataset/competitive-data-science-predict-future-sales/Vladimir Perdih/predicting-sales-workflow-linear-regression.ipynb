{"cells":[{"metadata":{},"cell_type":"markdown","source":"Table of contents:\n\n1. [READ, TRANSLATE AND MERGE TABLES](#section-1)\n    - [Shops](#subsection-1-1)\n    - [Categories](#subsection-1-2)\n    - [Data merge](#subsection-1-3)\n2. [DATA UNDERSTANDING AND CLEANING](#section-2)\n    - [Data overview](#subsection-2-1)\n    - [Managing date columns](#subsection-2-2)\n    - [Data Exploration and Visualisation](#subsection-2-3)\n        - [Sales quantity](#subsection-2-3-1)\n        - [Item price](#subsection-2-3-2)\n        - [Cronological sales](#subsection-2-3-3)\n        - [Monthly sales - 2013 & 2014](#subsection-2-3-4)\n        - [Weekday sales](#subsection-2-3-5)\n        - [Shop sales](#subsection-2-3-6)\n        - [Category sales](#subsection-2-3-7)\n3. [FEATURE ENGINEERING](#section-3)\n    - [Average prices calculation](#subsection-3-1)\n    - [First / last sale, medians and modes](#subsection-3-2)\n    - [Aggregating train data](#subsection-3-3)\n    - [Stacking train data](#subsection-3-4)\n    - [Price features](#subsection-3-5)\n    - [Mean quantity features](#subsection-3-6)\n    - [Lag features](#subsection-3-7)\n    - [Items features](#subsection-3-8)\n    - [Test data enginnering](#subsection-3-9)\n    - [Calendar related features](#subsection-3-10)\n    - [Final steps](#subsection-3-11)\n4. [FEATURE SELECTION](#section-4)\n    - [Feature correlation](#subsection-4-1)\n    - [Best feature selection with SelectKBest](#subsection-4-2)\n    - [Best feature selection with RFECV](#subsection-4-3)\n5. [MODELING](#section-5)\n    - [Spliting the train data](#subsection-5-1)\n    - [Selecting features to train](#subsection-5-2)\n    - [Training and evaluating](#subsection-5-3)\n    - [Predictions](#subsection-5-4)\n6. [CREATING SUBMISSION FILE](#section-6)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-1\"></a>\n# 1. READ, TRANSLATE AND MERGE TABLES #"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime as dt\n\n%matplotlib inline\n\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 50)\n\nitems = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nitem_categories = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\nsample_submission = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sample_submission.csv\")\n\ntrain_o = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\ntest_o = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Translate shop and category names to English."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install googletrans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from googletrans import Translator\n\ntranslator = Translator()\n\ndef translate(df, feature, src, dest):\n    return df[feature].apply(translator.translate, src=src, dest=dest).apply(getattr, args=('text',))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories[\"item_category_name_en\"] = translate(item_categories, \"item_category_name\", \"ru\", \"en\")\nshops[\"shop_name_en\"] = translate(shops, \"shop_name\", \"ru\", \"en\")\n\nitem_categories.drop(\"item_category_name\", axis=1, inplace=True)\nshops.drop(\"shop_name\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-1-1\"></a>\n## Shops ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"shops[\"shop_name_en\"].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that most of the shop names begin with the city name. Let's isolate the first string from names and insert it in the separate column named \"city\""},{"metadata":{"trusted":true},"cell_type":"code","source":"shops[\"city\"] = shops[\"shop_name_en\"].str.replace(\"[!,?,²]\", \"\").str.lower().str.strip().str.split(\" \").str.get(0)\nshops[\"city\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's isolate the names which are not cities (spd == St Petersburg) replace them with \"other\":\n- digital\n- offsite\n- emergency"},{"metadata":{"trusted":true},"cell_type":"code","source":"not_shops = [\"digital\", \"offsite\", \"emergency\"]\nshops[\"city\"] = shops[\"city\"].apply(lambda x: \"other\" if x in not_shops else x)\nshops.sort_values(\"city\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also have some potential duplicates, which we will explore later:\n- Zhukovsky st. Chkalov 39m² (id 10 and 11)\n- Moscow TC \"Budenovskiy\" (id 23 and 24)\n- Yakutsk Ordzhonikidze (id 0 and 57)\n- Yakutsk TC \"Central\" (id 1 and 58)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-1-2\"></a>\n## Categories ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories[\"item_category_name_en\"].head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Category names begin with the \"master category\". Let's isolate the text before \"-\" and insert it in the separate column named \"master_category\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories[\"master_category\"] = item_categories[\"item_category_name_en\"].str.replace(\"[!,?,²]\", \"\").str.lower().str.strip().str.split(\"-\").str.get(0).str.strip()\nitem_categories[\"master_category\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More grouping"},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories[\"master_category\"] = item_categories[\"master_category\"].apply(lambda x: \"payment cards\" if \"payment cards\" in x else x)\nitem_categories[\"master_category\"] = item_categories[\"master_category\"].apply(lambda x: \"games\" if \"games\" in x else x)\nitem_categories[\"master_category\"] = item_categories[\"master_category\"].apply(lambda x: \"blank media\" if \"blank media\" in x else x)\nitem_categories[\"master_category\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-1-3\"></a>\n## Data merge ##\n\nMerge item categories, items and shops to train/test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.merge(items, item_categories, how=\"left\", on='item_category_id')\ntrain_o = pd.merge(train_o, items, how=\"left\", on='item_id')\ntrain_o = pd.merge(train_o, shops, how=\"left\", on='shop_id')\n\ntest_o = pd.merge(test_o, items, how=\"left\", on='item_id')\ntest_o = pd.merge(test_o, shops, how=\"left\", on='shop_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Downgrade numeric data types to save memory.\n\nThank you Konstantin Yakovlev (kyakovlev) for this trick!\nhttps://www.kaggle.com/kyakovlev/1st-place-solution-part-1-hands-on-data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def downgrade_dtypes(df):\n    float_cols = list(df.dtypes[df.dtypes == \"float64\"].index)\n    int_cols = list(df.dtypes[(df.dtypes == \"int64\") | (df.dtypes == \"int32\")].index)\n\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_s = downgrade_dtypes(train_o)\ntest_s = downgrade_dtypes(test_o)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-2\"></a>\n# 2. DATA UNDERSTANDING AND CLEANING#"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2-1\"></a>\n## Data overview ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_s.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_features_overview(df):\n    df_info = pd.DataFrame()\n    df_info[\"type\"] = df.dtypes\n    df_info[\"missing_count\"] = df.isna().sum()\n    df_info[\"missing_perc\"] = (df_info[\"missing_count\"] / len(df) * 100).astype(int)\n    df_info = pd.concat([df_info, df.describe(include='all').T], axis=1)\n    \n    return df_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"info_df = generate_features_overview(train_s)\ninfo_df_string = info_df.dropna(subset=[\"unique\"], axis=0).dropna(axis=1)\ninfo_df_numeric = info_df.dropna(subset=[\"mean\"], axis=0).dropna(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"info_df_string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"info_df_numeric","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusions:\n- No missing values\n- Unique dates is equal to all days in the timeframe from 01.01.2013 to 31.10.2015, which means that we have sales every day\n- We have 21807 different items\n- We have 84 different categories\n- We have 60 different shops\n- We have some outliers in item_price and item_cnt_day columns"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2-2\"></a>\n## Managing date columns ##"},{"metadata":{},"cell_type":"markdown","source":"The \"date\" column should first be converted to type date and the following columns should be added, for easier understanding of data:\n- day\n- month\n- year\n- weekday"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c1 = train_s.copy()\ntrain_c1[\"date\"] = pd.to_datetime(train_s[\"date\"], format=\"%d.%m.%Y\")\ntrain_c1[\"month\"] = train_c1[\"date\"].dt.month\ntrain_c1[\"year\"] = train_c1[\"date\"].dt.year\ntrain_c1[\"weekday\"] = train_c1[\"date\"].dt.weekday\ntrain_c1[\"day\"] = train_c1[\"date\"].dt.day","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2-3\"></a>\n## Data Exploration and Visualisation ##"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def lineplot(df, X, Y, title):\n    fig, ax = plt.subplots(1, 1, figsize=(20, 7), sharex=True)\n    sns.lineplot(x=X, y=Y, data=df[Y], ax=ax[0]).set_title(title)\n    plt.show()\n    \n    \ndef lineplot_multiple(df, X, Y, title):\n    sns.lineplot(X, 'value', hue='variable', \n             data=pd.melt(df, X))\n    \ndef barplot(df, feature_1, label_1, x_ticks, title=\"\", width=0.4):\n    fig = plt.figure(figsize =(20, 7))\n    ax = fig.add_subplot()\n\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n        \n    plt.bar(range(len(df)), df[feature_1], align='center', label=label_1, color=\"blue\")\n    plt.xticks(range(len(df)), df[x_ticks], size='small')\n    plt.title(title)\n    plt.grid(False)\n    \n    plt.show()\n    \ndef barplot_double(df, features, labels, x_ticks, width=0.4):\n    fig = plt.figure(figsize =(20, 7))\n    ax = fig.add_subplot()\n\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n\n    plt.bar(range(len(df)), df[features[0]], align='center', label=labels[0])\n    plt.bar(range(len(df)), df[features[1]], align='center', label=labels[1])\n        \n    plt.xticks(range(len(df)), df[x_ticks], size='small')\n    plt.grid(False)\n    ax.legend(loc='upper right')\n    plt.show()\n\ndef barplot_sns(df, X, Y, hue, title):\n    fig, ax = plt.subplots(1, 1, figsize=(22, 7), sharex=True)\n    tidy = df.melt(id_vars=X).rename(columns=str.title)\n    sns.barplot(x=X, y=Y, hue=hue, data=tidy, ax=ax[0], palette=\"rocket\").set_title(title)\n    plt.show()\n    \ndef barplot_double_axis(df, feature_1, feature_2, label_1, label_2, x_ticks, width=0.4):\n    fig = plt.figure(figsize =(20, 7))\n    ax1 = fig.add_subplot()\n    ax2 = ax1.twinx()\n    \n    ax1.spines['top'].set_visible(False)\n    ax1.spines['right'].set_visible(False)\n    ax1.spines['left'].set_visible(False)\n    ax1.spines['bottom'].set_visible(False)\n    ax1.grid(False)\n\n        \n    ax2.spines['top'].set_visible(False)\n    ax2.spines['right'].set_visible(False)\n    ax2.spines['left'].set_visible(False)\n    ax2.spines['bottom'].set_visible(False)\n    ax2.grid(False)\n\n    ax1.bar(np.arange(len(df)) + (width / 2), df[feature_1], width=width, color=\"red\", label=label_1)\n    ax2.bar(np.arange(len(df)) - (width / 2), df[feature_2], width=width, color=\"blue\", label=label_2)\n    \n    ax1.legend(loc='upper left', frameon=False)\n    ax2.legend(loc='upper right', frameon=False)\n    \n    plt.xticks(range(len(df)), df[x_ticks], size='small')\n    plt.grid(False)\n\n    plt.show()\n    \n\ndef box_plot(df, features):\n    fig = plt.figure(figsize =(20, 7))\n\n    # Creating axes instance \n    ax = fig.add_subplot()\n\n    # Creating plot\n    data = []\n    for col in features:\n        data.append(df[col])\n        \n    bp = ax.boxplot(data)\n\n    # show plot\n    plt.show()\n    \ndef box_plot_sns(df, X, Y):\n    fig = plt.figure(figsize =(20, 7))\n    ax = fig.add_subplot()\n    sns.boxplot(x = X, y = Y, ax=ax, data = df)\n    \ndef histogram(df, features, bins=10):\n    df[features].plot.hist(bins=bins)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2-3-1\"></a>\n### Sales quantity ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c1[\"item_cnt_day\"].value_counts(bins=10).sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm = train_c1[\"item_cnt_day\"].value_counts(normalize=True)*100\nnorm.cumsum().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot_sns(train_c1, \"shop_id\", \"item_cnt_day\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the vast majority (99.59%) of quantities is in top 10 most represented values (-1 to 9). We also see some outliers - especially at shop 12.\n\nLets take a look at the items with the sales higher or equal to 1000 per day."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"q_outliers = train_c1[train_c1[\"item_cnt_day\"] >= 1000]\nq_outliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove the top outlier."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c1 = train_c1[train_c1[\"item_cnt_day\"] <= 1000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2-3-2\"></a>\n### Item price ###"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_c1[\"item_price\"].value_counts(bins=20).sort_index()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"norm = train_c1[\"item_price\"].value_counts(normalize=True)*100\nnorm.cumsum().head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot_sns(train_c1, \"shop_id\", \"item_price\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that top 20 prices are in the range from 99 to 2599. Setting the price with the 99 ending seems very popular. We can also see 1 outlier with the price far above the others. There is also 1 price below zero. Let's eliminate them."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c2 = train_c1[(train_c1[\"item_price\"] > 0) & (train_c1[\"item_price\"] < 100000)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2-3-3\"></a>\n### Cronological sales ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"def aggregate(df, group_by, aggfunc, features=[]):\n    #agg_types = np.mean, np.max, np.min, np.count_nonzero, np.sum\n    \n    grouped_df = df.groupby(group_by, as_index=False)\n    \n    if len(features) > 0:\n        grouped_df = grouped_df[features]\n\n    df = grouped_df.agg(aggfunc)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_1 = aggregate(train_c2, [\"date_block_num\"], np.sum)\n\nbarplot(agg_1, \"item_cnt_day\", \"Quantity\", \"date_block_num\", \"Cronological sales\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are seeing a decline in sales over the range we are training on, there is also a seasonal effect."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2-3-4\"></a>\n### Monthly sales - 2013 & 2014 ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_2 = aggregate(train_c2[train_c2[\"year\"] <= 2014], [\"month\"], np.sum)\n\nbarplot(agg_2, \"item_cnt_day\", \"Quantity\", \"month\", \"SUM of sales by month (2013 + 2014)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are only comparing the years 2013 and 2014 since we don't have full data for 2015. We can see a massive seasonality effect in winter months, especially december."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2-3-5\"></a>\n### Weekday sales ###"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"agg_3 = aggregate(train_c2, [\"weekday\"], np.sum)\nagg_3_m = agg_3.sort_values(\"item_cnt_day\", ascending=False)\nagg_3_m[\"cumsum\"] = agg_3_m[\"item_cnt_day\"].cumsum()\nagg_3_m[\"cumperc\"] = agg_3_m[\"cumsum\"] / agg_3_m[\"item_cnt_day\"].sum()\nagg_3_m[[\"weekday\",\"cumperc\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"barplot(agg_3, \"item_cnt_day\", \"Quantity\", \"weekday\", \"Sales quantity by day in week\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most sales occur on weekend (52%), which we'll consider later."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2-3-6\"></a>\n\n### Shops sales ###\n\nFirst let's explore some potential duplicates:\n- Zhukovsky st. Chkalov 39m² (id 10 and 11)\n- Moscow TC \"Budenovskiy\" (id 23 and 24)\n- Yakutsk Ordzhonikidze (id 0 and 57)\n- Yakutsk TC \"Central\" (id 1 and 58)\n\nWe will plot shop sales pairs on the barplot."},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_4 = aggregate(train_c2, [\"shop_id\", \"date_block_num\"], np.sum).sort_values(\"date_block_num\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def comparison_barplot(ids, feature):\n    compare_ids = ids\n    shop_1 = agg_4[agg_4[feature] == compare_ids[0]]\n    shop_2 = agg_4[agg_4[feature] == compare_ids[1]]\n\n    shop_comp = pd.merge(shop_1, shop_2, how=\"outer\", on='date_block_num')\n    shop_comp.fillna(0, inplace=True)\n\n    shop_comp[\"stacked\"] = shop_comp[\"item_cnt_day_x\"] + shop_comp[\"item_cnt_day_y\"]\n\n    fig, ax1 = plt.subplots(figsize=(20, 7))\n    sns.barplot(x='date_block_num', y='stacked', data=shop_comp, ax=ax1, color=\"red\", label=\"{0}: {1}\".format(feature, compare_ids[1]))\n    sns.barplot(x='date_block_num', y='item_cnt_day_x', data=shop_comp, ax=ax1, color=\"blue\", label=\"{0}: {1}\".format(feature, compare_ids[0]))\n    plt.legend()\n    sns.despine(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comparison_barplot([10, 11], \"shop_id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comparison_barplot([23, 24], \"shop_id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comparison_barplot([0, 57], \"shop_id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comparison_barplot([1, 58], \"shop_id\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like the following shops are duplicated:\n- Zhukovsky st. Chkalov 39m² (id 10 and 11)\n- Yakutsk Ordzhonikidze (id 0 and 57)\n- Yakutsk TC \"Central\" (id 1 and 58)\n\nLet's make the folowing shop id replacements:\n- id 11 => id 10\n- id 0 => id 57\n- id 1 => 58"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train_c2.loc[train_c2[\"shop_id\"] == 11, 'shop_id'] = 10\ntrain_c2.loc[train_c2[\"shop_id\"] == 0, 'shop_id'] = 57\ntrain_c2.loc[train_c2[\"shop_id\"] == 1, 'shop_id'] = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_5 = aggregate(train_c2, [\"shop_id\"], np.sum).sort_values(\"item_cnt_day\", ascending=False)\nagg_5[\"cumsum\"] = agg_5[\"item_cnt_day\"].cumsum()\nagg_5[\"cumperc\"] = agg_5[\"cumsum\"] / agg_5[\"item_cnt_day\"].sum()\n\nbarplot(agg_5, \"item_cnt_day\", \"Quantity\", \"shop_id\", \"Sales quantity by shop\")\nbarplot(agg_5, \"cumperc\", \"Quantity\", \"shop_id\", \"Sales quantity by shop - cumulative percentage\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_6 = aggregate(train_c2, [\"city\"], np.sum).sort_values(\"item_cnt_day\", ascending=False)\nagg_6[\"cumsum\"] = agg_6[\"item_cnt_day\"].cumsum()\nagg_6[\"cumperc\"] = agg_6[\"cumsum\"] / agg_6[\"item_cnt_day\"].sum()\nbarplot(agg_6, \"item_cnt_day\", \"Quantity\", \"city\", \"Sales quantity by city\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2-3-7\"></a>\n### Categories sales ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_7 = aggregate(train_c2, [\"item_category_id\"], np.sum).sort_values(\"item_cnt_day\", ascending=False)\nagg_7[\"cumsum\"] = agg_7[\"item_cnt_day\"].cumsum()\nagg_7[\"cumperc\"] = agg_7[\"cumsum\"] / agg_7[\"item_cnt_day\"].sum()\n\nbarplot(agg_7, \"item_cnt_day\", \"Quantity\", \"item_category_id\", \"Sales quantity by item category\")\nbarplot(agg_7, \"cumperc\", \"Quantity\", \"item_category_id\", \"Sales quantity by item category - cumulative percentage\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_8 = aggregate(train_c2, [\"master_category\"], np.sum).sort_values(\"item_cnt_day\", ascending=False)\n\nbarplot(agg_8, \"item_cnt_day\", \"Quantity\", \"master_category\", \"Sales quantity by master category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that top 5 master categories form the vast majority of sales."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-3\"></a>\n# 3. FEATURE ENGINEERING #\n\nWe are predicting the monthly sales data for november 2015, however our train data consists of daily sales. Therefore we will have to aggregate the data by item/shop/month, but before that we should create some useful features with non-aggregated data."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3-1\"></a>\n### Average prices calculation ###"},{"metadata":{},"cell_type":"markdown","source":"New features:\n- mean price for item/shop/date_block combo\n- mean price for item/shop combo\n- mean price for item\n- mean price item_category"},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_price_item_shop_month = train_c2.groupby(['item_id', 'shop_id', 'date_block_num']).agg({\"item_price\": \"mean\"})\navg_price_item_shop = train_c2.groupby(['item_id', 'shop_id']).agg({\"item_price\": \"mean\"})\navg_price_item = train_c2.groupby('item_id').agg({\"item_price\": \"mean\"})\navg_price_category = train_c2.groupby('item_category_id').agg({\"item_price\": \"mean\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3-2\"></a>\n### First / last sale, medians and modes ###\n\nNew features:\n- first and last sale of item/shop combo\n- first and last sale of item\n- first and last sale of shop\n\n- weekday, day and month median of item/shop combo sales count\n- weekday, day and month median of item sales count\n- weekday, day and month median of shop sales count\n\n- weekday, day and month mode of item sales count\n- weekday, day and month mode of shop sales count"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\nitem_shop_sales_detail = aggregate(train_c2, [\"item_id\", \"shop_id\"], {\"date_block_num\": [\"min\", \"max\"], \"weekday\": \"median\", \"day\": \"median\", \"month\": \"median\"})\n#item_shop_sales_detail = train_c2.groupby([\"item_id\", \"shop_id\"])[[\"day\", \"weekday\", \"month\"]].agg(lambda x: scipy.stats.mode(x)[0])\nitem_shop_sales_detail.set_index([\"item_id\", \"shop_id\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_sales_detail = aggregate(train_c2, \"item_id\", {\"date_block_num\": [\"min\", \"max\"], \"weekday\": \"median\", \"day\": \"median\", \"month\": \"median\"})\nitem_sales_modes = train_c2.groupby(\"item_id\")[[\"day\", \"weekday\", \"month\"]].agg(lambda x: scipy.stats.mode(x)[0])\nitem_sales_detail.set_index(\"item_id\", inplace=True)\nitem_sales_detail[(\"weekday\", \"mode\")] = item_sales_modes[\"weekday\"]\nitem_sales_detail[(\"day\", \"mode\")] = item_sales_modes[\"day\"]\nitem_sales_detail[(\"month\", \"mode\")] = item_sales_modes[\"month\"]\nitem_sales_detail.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_sales_detail = aggregate(train_c2, \"shop_id\", {\"date_block_num\": [\"min\", \"max\"], \"weekday\": \"median\", \"day\": \"median\", \"month\": \"median\"})\nshop_sales_modes = train_c2.groupby(\"shop_id\")[[\"day\", \"weekday\", \"month\"]].agg(lambda x: scipy.stats.mode(x)[0])\nshop_sales_detail.set_index(\"shop_id\", inplace=True)\nshop_sales_detail[(\"weekday\", \"mode\")] = shop_sales_modes[\"weekday\"]\nshop_sales_detail[(\"day\", \"mode\")] = shop_sales_modes[\"day\"]\nshop_sales_detail[(\"month\", \"mode\")] = shop_sales_modes[\"month\"]\nshop_sales_detail.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3-3\"></a>\n### Aggregating the data ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c3 = train_c2.copy()\ntrain_c3.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should first drop some columns that we won't need for modeling."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop_train = [\"date\", \"item_name\", \"item_category_name_en\", \"shop_name_en\", \"weekday\", \"day\"]\ntrain_c3.drop(cols_to_drop_train, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aggregation, rename column from item_cnt_day to item_cnt_month."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c3 = aggregate(train_c3, [\"date_block_num\", \"shop_id\", \"item_id\"], {\"item_cnt_day\": np.sum})\ntrain_c3.rename(columns={\"item_cnt_day\": \"item_cnt_month\"}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3-4\"></a>\n### Stacking the train data ###\n\nFirst step:\n- Remove the data not in the test set from train\n- Fill the data with zero sales for all item/shop/date_block combo"},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_ids = test_s['shop_id'].unique()\nitem_ids = test_s['item_id'].unique()\n\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c4 = pd.merge(empty_df, train_c3, on=['date_block_num','shop_id','item_id'], how='left')\ntrain_c4.fillna(value=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Append all available data. Clean shops data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c4 = pd.merge(train_c4, items[[\"item_id\", \"item_category_id\", \"master_category\"]], on=\"item_id\", how=\"left\")\ntrain_c4 = pd.merge(train_c4, shops[[\"shop_id\", \"city\"]], on=\"shop_id\", how=\"left\")\ntrain_c4[\"year\"] = train_c4[\"date_block_num\"] // 12 + 2013\ntrain_c4[\"month\"] = train_c4[\"date_block_num\"] % 12 + 1\ntrain_c4.loc[train_c4[\"shop_id\"] == 11, 'shop_id'] = 10\ntrain_c4.loc[train_c4[\"shop_id\"] == 0, 'shop_id'] = 57\ntrain_c4.loc[train_c4[\"shop_id\"] == 1, 'shop_id'] = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add first and last sales, weekday and month medians."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c4.set_index([\"item_id\", \"shop_id\"], inplace=True)\n\ntrain_c4[\"item_shop_date_block_min\"] = item_shop_sales_detail[(\"date_block_num\", \"min\")]\ntrain_c4[\"item_shop_date_block_max\"] = item_shop_sales_detail[(\"date_block_num\", \"max\")]\ntrain_c4[\"item_shop_weekday_median\"] = item_shop_sales_detail[(\"weekday\", \"median\")] + 1\ntrain_c4[\"item_shop_month_median\"] = item_shop_sales_detail[(\"month\", \"median\")]\n\ntrain_c4 = train_c4.reset_index().set_index(\"item_id\")\ntrain_c4[\"item_date_block_min\"] = item_sales_detail[(\"date_block_num\", \"min\")]\ntrain_c4[\"item_date_block_max\"] = item_sales_detail[(\"date_block_num\", \"max\")]\ntrain_c4[\"item_weekday_median\"] = item_sales_detail[(\"weekday\", \"median\")] + 1\ntrain_c4[\"item_month_median\"] = item_sales_detail[(\"month\", \"median\")]\ntrain_c4[\"item_weekday_mode\"] = item_sales_detail[(\"weekday\", \"mode\")] + 1\ntrain_c4[\"item_month_mode\"] = item_sales_detail[(\"month\", \"mode\")]\n\ntrain_c4 = train_c4.reset_index().set_index(\"shop_id\")\ntrain_c4[\"shop_date_block_min\"] = shop_sales_detail[(\"date_block_num\", \"min\")]\ntrain_c4[\"shop_date_block_max\"] = shop_sales_detail[(\"date_block_num\", \"max\")]\ntrain_c4[\"shop_weekday_median\"] = shop_sales_detail[(\"weekday\", \"median\")] + 1\ntrain_c4[\"shop_month_median\"] = shop_sales_detail[(\"month\", \"median\")]\ntrain_c4[\"shop_weekday_mode\"] = shop_sales_detail[(\"weekday\", \"mode\")] + 1\ntrain_c4[\"shop_month_mode\"] = shop_sales_detail[(\"month\", \"mode\")]\n\ntrain_c4.reset_index(inplace=True)\ntrain_c4.fillna(value=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3-5\"></a>\n### Price features ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c5 = train_c4.set_index(['item_id', 'shop_id', 'date_block_num'])\ntrain_c5[\"item_price\"] = avg_price_item_shop_month[\"item_price\"]\ntrain_c5[\"item_price\"].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c5 = train_c5.reset_index().set_index(['item_id', 'shop_id'])\ntrain_c5.loc[train_c5['item_price'].isna(), 'item_price'] = avg_price_item_shop[\"item_price\"]\ntrain_c5[\"item_price\"].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c5 = train_c5.reset_index().set_index('item_id')\ntrain_c5.loc[train_c5['item_price'].isna(), 'item_price'] = avg_price_item[\"item_price\"]\ntrain_c5[\"item_price\"].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also add average item price in a separate column for potential features such as discounts."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c5[\"avg_item_price\"] = avg_price_item[\"item_price\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c5 = train_c5.reset_index().set_index('item_category_id')\ntrain_c5.loc[train_c5['item_price'].isna(), 'item_price'] = avg_price_category[\"item_price\"]\ntrain_c5[\"item_price\"].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets calculate a new feature - shop item price percentage of average price in all shops."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c5[\"avg_item_price_perc\"] = (train_c5['item_price'] - train_c5[\"avg_item_price\"]) / train_c5[\"avg_item_price\"]\ntrain_c5.fillna(value=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3-6\"></a>\n### Mean quantity features ###\n\nMean quantity features in relation to date_block_num.\n\nIt is very important to filter train data so that it is similar to test data:\n- Clip the sales between 0 and 20"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c5[\"item_cnt_month\"] = train_c5[\"item_cnt_month\"].clip(0., 20.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_q_month = train_c5.groupby(['date_block_num']).agg({\"item_cnt_month\": \"mean\"})\navg_q_month_item = train_c5.groupby(['date_block_num', 'item_id']).agg({\"item_cnt_month\": \"mean\"})\navg_q_month_shop = train_c5.groupby(['date_block_num', 'shop_id']).agg({\"item_cnt_month\": \"mean\"})\navg_q_month_category = train_c5.groupby(['date_block_num', 'item_category_id']).agg({\"item_cnt_month\": \"mean\"})\navg_q_month_city = train_c5.groupby(['date_block_num', 'city']).agg({\"item_cnt_month\": \"mean\"})\navg_q_month_master_category = train_c5.groupby(['date_block_num', 'master_category']).agg({\"item_cnt_month\": \"mean\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c5 = train_c5.reset_index().set_index('date_block_num')\ntrain_c5[\"avg_month_sales\"] = avg_q_month[\"item_cnt_month\"]\n\ntrain_c5 = train_c5.reset_index().set_index(['date_block_num', 'item_id'])\ntrain_c5[\"avg_month_item_sales\"] = avg_q_month_item[\"item_cnt_month\"]\n\ntrain_c5 = train_c5.reset_index().set_index(['date_block_num', 'shop_id'])\ntrain_c5[\"avg_month_shop_sales\"] = avg_q_month_shop[\"item_cnt_month\"]\n\ntrain_c5 = train_c5.reset_index().set_index(['date_block_num', 'item_category_id'])\ntrain_c5[\"avg_month_category_sales\"] = avg_q_month_category[\"item_cnt_month\"]\n\ntrain_c5 = train_c5.reset_index().set_index(['date_block_num', 'city'])\ntrain_c5[\"avg_month_city_sales\"] = avg_q_month_city[\"item_cnt_month\"]\n\ntrain_c5 = train_c5.reset_index().set_index(['date_block_num', 'master_category'])\ntrain_c5[\"avg_month_master_category_sales\"] = avg_q_month_master_category[\"item_cnt_month\"]\n\ntrain_c5 = train_c5.reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3-7\"></a>\n### Lag features ###\n\nWe still don't have comparisons of sales against previous months. We should add some, since previous sales are one of the most important features in sales analytics.\n\nWe will add features using a great function from Denis Larionov => https://www.kaggle.com/dlarionov/feature-engineering-xgboost:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c6 = train_c5.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c6 = lag_feature(train_c6, [1, 2, 3], 'item_cnt_month')\ntrain_c6 = lag_feature(train_c6, [1, 2, 3], 'avg_month_item_sales')\ntrain_c6 = lag_feature(train_c6, [1, 2, 3], 'avg_month_shop_sales')\ntrain_c6 = lag_feature(train_c6, [1], 'avg_month_sales')\ntrain_c6 = lag_feature(train_c6, [1], 'avg_month_category_sales')\ntrain_c6 = lag_feature(train_c6, [1], 'avg_month_city_sales')\ntrain_c6 = lag_feature(train_c6, [1], 'avg_month_master_category_sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also create price lag features, so that we can add them to the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c6 = lag_feature(train_c6, [1], 'item_price')\ntrain_c6 = lag_feature(train_c6, [1], 'avg_item_price_perc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3-8\"></a>\n### Items features ###\n\nWe will create the item type feature:\n- Old item - no sales in last 6 months\n- New item - first sales in last 6 months\n- Regular items - the rest"},{"metadata":{"trusted":true},"cell_type":"code","source":"def itemTypes(block_min, block_max):\n    if block_min >= 27:\n        return 1\n    elif block_max < 27:\n        return 2\n    else:\n        return 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c6[\"item_type\"] = np.vectorize(itemTypes)(train_c6['item_date_block_min'], train_c6['item_date_block_max'])\ntrain_c6[\"item_type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3-9\"></a>\n### Test data enginnering ###\n\nLet's fill the missing data in our test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_c1 = test_s.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_c1[\"month\"] = 11\ntest_c1[\"year\"] = 2015\ntest_c1[\"date_block_num\"] = 34\n\ntest_c1.set_index([\"item_id\", \"shop_id\"], inplace=True)\ntest_c1[\"item_shop_date_block_min\"] = item_shop_sales_detail[(\"date_block_num\", \"min\")]\ntest_c1[\"item_shop_date_block_max\"] = item_shop_sales_detail[(\"date_block_num\", \"max\")]\ntest_c1[\"item_shop_weekday_median\"] = item_shop_sales_detail[(\"weekday\", \"median\")] + 1\ntest_c1[\"item_shop_month_median\"] = item_shop_sales_detail[(\"month\", \"median\")]\n\ntest_c1 = test_c1.reset_index().set_index(\"item_id\")\ntest_c1[\"item_date_block_min\"] = item_sales_detail[(\"date_block_num\", \"min\")]\ntest_c1[\"item_date_block_max\"] = item_sales_detail[(\"date_block_num\", \"max\")]\ntest_c1[\"item_weekday_median\"] = item_sales_detail[(\"weekday\", \"median\")] + 1\ntest_c1[\"item_month_median\"] = item_sales_detail[(\"month\", \"median\")]\ntest_c1[\"item_weekday_mode\"] = item_sales_detail[(\"weekday\", \"mode\")] + 1\ntest_c1[\"item_month_mode\"] = item_sales_detail[(\"month\", \"mode\")]\n\ntest_c1 = test_c1.reset_index().set_index(\"shop_id\")\ntest_c1[\"shop_date_block_min\"] = shop_sales_detail[(\"date_block_num\", \"min\")]\ntest_c1[\"shop_date_block_max\"] = shop_sales_detail[(\"date_block_num\", \"max\")]\ntest_c1[\"shop_weekday_median\"] = shop_sales_detail[(\"weekday\", \"median\")] + 1\ntest_c1[\"shop_month_median\"] = shop_sales_detail[(\"month\", \"median\")]\ntest_c1[\"shop_weekday_mode\"] = shop_sales_detail[(\"weekday\", \"mode\")] + 1\ntest_c1[\"shop_month_mode\"] = shop_sales_detail[(\"month\", \"mode\")]\n\ntest_c1.reset_index(inplace=True)\ntest_c1.fillna(value=0, inplace=True)\n\ncols_to_drop_test = [\"item_name\", \"item_category_name_en\", \"shop_name_en\"]\ntest_c1.drop(cols_to_drop_test, axis=1, inplace=True)\n\ntest_c2 = test_c1.reset_index().set_index(\"index\")\ntest_c2.fillna(value=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = train_c6.reset_index().set_index(['item_id', 'shop_id'])\nsales_lag_1 = sales[sales[\"date_block_num\"] == 33]\nsales_lag_2 = sales[sales[\"date_block_num\"] == 32]\nsales_lag_3 = sales[sales[\"date_block_num\"] == 31]\nsales_lag_12 = sales[sales[\"date_block_num\"] == 22]\ntest_c2 = test_c2.reset_index().set_index(['item_id', 'shop_id'])\n\ntest_c2[\"item_cnt_month_lag_1\"] = sales_lag_1[\"item_cnt_month\"]\ntest_c2[\"item_price_lag_1\"] = sales_lag_1[\"item_price\"]\ntest_c2[\"avg_item_price_perc_lag_1\"] = sales_lag_1[\"avg_item_price_perc\"]\ntest_c2[\"avg_month_sales_lag_1\"] = sales_lag_1[\"avg_month_sales\"]\ntest_c2[\"avg_month_item_sales_lag_1\"] = sales_lag_1[\"avg_month_item_sales\"]\ntest_c2[\"avg_month_shop_sales_lag_1\"] = sales_lag_1[\"avg_month_shop_sales\"]\ntest_c2[\"avg_month_category_sales_lag_1\"] = sales_lag_1[\"avg_month_category_sales\"]\ntest_c2[\"avg_month_city_sales_lag_1\"] = sales_lag_1[\"avg_month_city_sales\"]\ntest_c2[\"avg_month_master_category_sales_lag_1\"] = sales_lag_1[\"avg_month_master_category_sales\"]\n\ntest_c2[\"item_cnt_month_lag_2\"] = sales_lag_2[\"item_cnt_month\"]\ntest_c2[\"avg_month_item_sales_lag_2\"] = sales_lag_2[\"avg_month_item_sales\"]\ntest_c2[\"avg_month_shop_sales_lag_2\"] = sales_lag_2[\"avg_month_shop_sales\"]\n\ntest_c2[\"item_cnt_month_lag_3\"] = sales_lag_3[\"item_cnt_month\"]\ntest_c2[\"avg_month_item_sales_lag_3\"] = sales_lag_3[\"avg_month_item_sales\"]\ntest_c2[\"avg_month_shop_sales_lag_3\"] = sales_lag_3[\"avg_month_shop_sales\"]\n\ntest_c2[\"item_type\"] = np.vectorize(itemTypes)(test_c2['item_date_block_min'], test_c2['item_date_block_max'])\n\ntest_c2 = test_c2.reset_index().set_index(\"index\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3-10\"></a>\n### Calendar related features ###"},{"metadata":{},"cell_type":"markdown","source":"Let's add the number of weekend days (friday included) for every month in our data. Also we calculate the number of days in month."},{"metadata":{"trusted":true},"cell_type":"code","source":"import calendar\n\ndef calculateWeekendDays(month, year):\n    weekend_days = 0\n    for week in calendar.monthcalendar(year, month):\n        for day in week[4:]:\n            if day != 0:\n                weekend_days +=1\n                \n    return weekend_days\n\ndef calculateMonthDays(month, year):\n    month_days = 0\n    for week in calendar.monthcalendar(year, month):\n        for day in week:\n            if day != 0:\n                month_days +=1\n                \n    return month_days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar_dict = {\"date_block_num\": [], \"weekend_days\": [], \"month_days\": []}\n\nfor year in range (2013, 2016):\n    for month in range(1, 13):\n        calendar_dict[\"date_block_num\"].append((year - 2013)*12 + month - 1)\n        calendar_dict[\"weekend_days\"].append(calculateWeekendDays(month, year))\n        calendar_dict[\"month_days\"].append(calculateMonthDays(month, year))\n\nweekend_days_df = pd.DataFrame(calendar_dict)\nweekend_days_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c6 = pd.merge(train_c6, weekend_days_df, how=\"left\", on='date_block_num')\ntest_c2 = pd.merge(test_c2, weekend_days_df, how=\"left\", on='date_block_num')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3-11\"></a>\n### Final steps ###\n\nWe also need to eliminate first three months from training data since it has a lot of missing data in lagged features."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c6 = train_c6[train_c6[\"date_block_num\"] > 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_c6.drop([\"item_price\", \"avg_item_price\", \"avg_item_price_perc\"], axis=1, inplace=True)\ntrain_c6.drop([\"avg_month_sales\", \"avg_month_item_sales\", \"avg_month_shop_sales\", \"avg_month_category_sales\", \"avg_month_city_sales\", \"avg_month_master_category_sales\"], axis = 1, inplace=True)\n\ntrain_f = train_c6.copy()\ntest_f = test_c2.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert object to numeric columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dummies(df,features):\n    for col in features:\n        dummies = pd.get_dummies(df[col],prefix=col)\n        df = pd.concat([df,dummies],axis=1)\n        df = df.drop(col, axis=1)\n    return df\n\ndef categorize_column(df, features):\n    for col in features:\n        df[col] = df[col].astype('category')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_f = create_dummies(train_f, [\"master_category\"])\n# train_f = create_dummies(train_f, [\"city\"])\n\n# test_f = create_dummies(test_f, [\"master_category\"])\n# test_f = create_dummies(test_f, [\"city\"])\n\ntrain_f = categorize_column(train_f, [\"master_category\"])\ntrain_f = categorize_column(train_f, [\"city\"])\n\ntest_f = categorize_column(test_f, [\"master_category\"])\ntest_f = categorize_column(test_f, [\"city\"])\n\ntrain_f[\"master_category\"] = train_f[\"master_category\"].cat.codes\ntrain_f[\"city\"] = train_f[\"city\"].cat.codes\n\ntest_f[\"master_category\"] = test_f[\"master_category\"].cat.codes\ntest_f[\"city\"] = test_f[\"city\"].cat.codes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Downgrade numeric types for faster calculations."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_f = downgrade_dtypes(train_f)\ntrain_f.info()\ntest_f = downgrade_dtypes(test_f)\ntest_f.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-4\"></a>\n# 4. FEATURE SELECTION #"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-4-1\"></a>\n### Feature correlation ###\n\nMethod types:\n- pearson => numerical input - numerical output\n- spearman => numerical input - numerical output\n- kendall => categorical input - numerical output, numerical input - categorical output"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_corr_heatmap(df, method):\n    corr = df.corr(method)\n    \n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    \n    return corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = show_corr_heatmap(train_f, \"pearson\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_correlation_features = list(correlations[\"item_cnt_month\"].sort_values(ascending=False)[:30].index)\n\ntop = show_corr_heatmap(train_f[top_correlation_features], \"pearson\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-4-2\"></a>\n### Best feature selection with SelectKBest ###\n\nSearching for best features using SelectKBest.\n\nRegression methods: f_regression, mutual_info_regression\n\nClassification methods: chi2, f_classif, mutual_info_classif"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, chi2, f_regression, mutual_info_regression\nfrom sklearn.feature_selection import f_classif, mutual_info_classif\n\ndef get_best_features(df, features, target, function, num_of_features=-1):\n    \n    # Select all features if number is not passed\n    if num_of_features == -1:\n        num_of_features = len(features)\n    \n    # Create the model and fit it with data\n    kBest=SelectKBest(score_func=function,k=num_of_features)\n    kBest.fit(df[features],df[target])\n    \n    # Get columns to keep and create new dataframe with those only\n    cols = kBest.get_support(indices=True)\n    features_df_new = df[features].iloc[:,cols]\n        \n    # Create a dataframe of feature names and scores\n    names = df[features].columns.values[kBest.get_support()]\n    scores = kBest.scores_\n    names_scores = list(zip(names, scores))\n    feature_scores_df = pd.DataFrame(data = names_scores, columns=['feature', 'score'])\n    \n    #Sort the dataframe for better visualization\n    feature_scores_df_sorted = feature_scores_df.sort_values(['score', 'feature'], ascending = [False, True])\n\n    return feature_scores_df_sorted","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import chi2, f_regression, mutual_info_regression, f_classif, mutual_info_classif\n\ntarget_feature = \"item_cnt_month\"\nbest_train_features = list(train_f.columns)\nbest_train_features.remove(target_feature)\n\nmethods = [f_regression]\nfor method in methods:\n    best_features_kBest = get_best_features(train_f, best_train_features, target_feature, method)\n    print(best_features_kBest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-4-3\"></a>\n\n### Best feature selection with RFECV ###\n\nSearching for best features using RFECV.\n\nWarning: it is a very time consuming process - in my case it took 6 minutes."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom xgboost.sklearn import XGBRegressor\n\nimport numpy as np\n\ndef select_features_RFECV(df, target):\n    df.dropna(axis=0, inplace=True)\n    \n    df = df.select_dtypes([np.number])\n    \n    all_X = df.drop([target], axis=1)\n    all_y = df[target]\n    \n    clf = LinearRegression()\n    selector = RFECV(clf, cv=5, min_features_to_select=25, scoring='neg_root_mean_squared_error')\n    selector.fit(all_X, all_y)\n\n    optimized_columns = all_X.columns[selector.support_]\n\n    return optimized_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_features_RFECV = select_features_RFECV(train_f, \"item_cnt_month\")\nprint(best_features_RFECV)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-5\"></a>\n# 5. MODELING #"},{"metadata":{},"cell_type":"markdown","source":"First we create a context manager to manage calculation times."},{"metadata":{"trusted":true},"cell_type":"code","source":"import contextlib\nimport time\n\n@contextlib.contextmanager\ndef timer():\n    start = time.time()\n    \n    yield\n\n    end = time.time()\n    runtime = '{:.2f}s \\n'.format(end - start)\n    print(runtime)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hyperparameters optimization with the function below using GridSearchCV or RandomizedSearchCV."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom xgboost.sklearn import XGBRegressor\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\ndef select_regression_model(df, features, target, typ):\n    \n    all_X = df[features]\n    all_y = df[target]\n    \n    models = [\n        {\n            \"name\": \"LinearRegression\",\n            \"estimator\": LinearRegression(),\n            \"hyperparameters\":\n                {\n                    \"normalize\": [True, False]\n                }\n         }\n    ]\n    \n    for model in models:\n        with timer():\n            if typ == \"grid\":\n                search = GridSearchCV(model[\"estimator\"], param_grid=model[\"hyperparameters\"], cv=5, scoring='neg_root_mean_squared_error')\n            elif typ == \"random\":\n                search = RandomizedSearchCV(model[\"estimator\"], param_distributions=model[\"hyperparameters\"], n_iter = 1, cv=5, scoring='neg_root_mean_squared_error')\n\n            search.fit(all_X, all_y)\n            model[\"best_params\"] = search.best_params_\n            model[\"best_score\"] = search.best_score_\n            model[\"best_model\"] = search.best_estimator_\n\n    return models","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-5-1\"></a>\n### Split the train data ###\n\nSplit our train data into train_train and train_test. We will use the last month to evaluate our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_f_tr = train_f[(train_f[\"date_block_num\"] < 33)]\ntrain_f_t = train_f[train_f[\"date_block_num\"] == 33]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-5-2\"></a>\n### Selecting features to train ###"},{"metadata":{},"cell_type":"markdown","source":"Best correlation features"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations[\"item_cnt_month\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best features using SelectKBest."},{"metadata":{"trusted":true},"cell_type":"code","source":"list(best_features_kBest[\"feature\"][:30].to_numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best features using RFECV."},{"metadata":{"trusted":true},"cell_type":"code","source":"list(best_features_RFECV.to_numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target and train features selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_feature = \"item_cnt_month\"\nall_train_features = list(train_f.columns)\nall_train_features.remove(target_feature)\nprint(\"Available features: {}\".format(all_train_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features = ['date_block_num',\n 'year',\n 'month',\n 'item_shop_date_block_min',\n 'item_shop_date_block_max',\n 'item_shop_weekday_median',\n 'item_weekday_median',\n 'item_month_mode',\n 'shop_weekday_median',\n 'shop_month_median',\n 'item_cnt_month_lag_1',\n 'item_cnt_month_lag_2',\n 'item_cnt_month_lag_3',\n 'avg_month_item_sales_lag_1',\n 'avg_month_item_sales_lag_2',\n 'avg_month_item_sales_lag_3',\n 'avg_month_shop_sales_lag_1',\n 'avg_month_shop_sales_lag_2',\n 'avg_month_shop_sales_lag_3',\n 'avg_month_sales_lag_1',\n 'avg_month_category_sales_lag_1',\n 'avg_month_city_sales_lag_1',\n 'avg_item_price_perc_lag_1',\n 'item_type',\n 'month_days']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model evaluation function"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndef evaluate(result):\n    best_rf_model = result[0][\"best_model\"]\n\n    predictions_tr = best_rf_model.predict(train_f_tr[selected_features])\n    predictions_t = best_rf_model.predict(train_f_t[selected_features])\n\n    rmse_tr = (mean_squared_error(train_f_tr[\"item_cnt_month\"].to_numpy(), predictions_tr.clip(0., 20.))) ** (1/2)\n    rmse_t = (mean_squared_error(train_f_t[\"item_cnt_month\"].to_numpy(), predictions_t.clip(0., 20.))) ** (1/2)\n    \n    data.append({\"best_model\": best_rf_model, \"best_score\": result[0][\"best_score\"], \"features\": selected_features,\n                 \"rmse_train\": rmse_tr, \"rmse_test\": rmse_t})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-5-3\"></a>\n### Training and evaluating ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = []\n\nresult = select_regression_model(train_f_tr, selected_features, target_feature, \"random\")\nevaluate(result)\n\nprint(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_rf_model = result[0][\"best_model\"]\nbest_rf_model.fit(train_f[selected_features], train_f[target_feature])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-5-4\"></a>\n### Predictions ###\n\nWe first select the model which performed optimal and make predictions on actual test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = best_rf_model.predict(test_f[selected_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predictions fast check."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_f[\"predictions\"] = predictions\ntest_f[\"predictions\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-6\"></a>\n# 6. CREATING SUBMISSION FILE #"},{"metadata":{},"cell_type":"markdown","source":"Creating a submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_submission_file(data, filename=\"submission_13.csv\"):\n    test_ids = data.index\n    predictions = data[\"predictions\"].clip(0., 20.)\n    \n    submission_df = {\"ID\": test_ids,\n                 \"item_cnt_month\": predictions}\n    \n    submission = pd.DataFrame(submission_df)\n    submission.to_csv(filename,index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_submission_file(test_f)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}