{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------------- \n# PYTHON IMPORTS\n# --------------------------------------- ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nfrom scipy import stats\nimport numpy as np\n\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost.sklearn import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------------- \n# LOAD COMPETITION DATA\n# --------------------------------------- ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_FOLDER = '/kaggle/input/competitive-data-science-predict-future-sales'\n\ntrain           = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv'))\ntest            = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))\nsubmission      = pd.read_csv(os.path.join(DATA_FOLDER, 'sample_submission.csv'))\nitems           = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\nitem_cats       = pd.read_csv(os.path.join(DATA_FOLDER, 'item_categories.csv'))\nshops           = pd.read_csv(os.path.join(DATA_FOLDER, 'shops.csv'))\n\n#convert data to dataTime format\ntrain['dateTime'] = pd.to_datetime(train['date'], format='%d.%m.%Y')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------------- \n# DATA EXPLORATION\n# --------------------------------------- ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The competition is about predicting november 2015 sales, per item-shop pairs.\n\n# The training set however describe all sales for a few years back (from jan-2013 to oct-2015).\n# let's take a closer look at the range of the time serie we have as a train set.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(train[\"dateTime\"].dt.year).count().plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#per year chart\ntrain15 = train[(train['dateTime'].dt.year == 2015)]\ntrain14 = train[(train['dateTime'].dt.year == 2014)]\ntrain13 = train[(train['dateTime'].dt.year == 2013)]\n\ntrain15.groupby(train15[\"dateTime\"].dt.month).count().plot(kind=\"bar\")\ntrain14.groupby(train14[\"dateTime\"].dt.month).count().plot(kind=\"bar\")\ntrain13.groupby(train13[\"dateTime\"].dt.month).count().plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's now see what the test set is about.\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As test set do not contain any date and train test stop at october 2015 we can assume that we are tasked to predict november 2015.\n#and test is a mapping from ID to [shop_id,item_id] usefull to submit prediction as `submission` is indexed by ID.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's explore a bit more, around the pair from test set:\ntransactionsWithCategory = pd.merge(train, items, on='item_id', how='left')\n\ntransactionsWithCategory = transactionsWithCategory.drop(columns='item_name') # dropping item_name it is redundant with item_id.\ntransactionsWithCategory = transactionsWithCategory.drop(columns='date')      # dropping data it is redundant dateTime.\ntransactionsWithCategory = transactionsWithCategory.drop(columns='date_block_num') # dropping date_block_num it is probably usefull but let's select our fight to gain some time.\n\ntransactionsWithCategory.hist(column='shop_id')\ntransactionsWithCategory.hist(column='item_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('num shops', transactionsWithCategory.groupby('shop_id')['shop_id'].nunique().shape)\nprint('num items', transactionsWithCategory.groupby('item_id')['item_id'].nunique().shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Both 'shop_id' and 'item_id' are categorical feature, we got 60 unique shop and 21807 unique item.\n\n#Thus it make sense to one hot encode shop_id (we will do that latter).\n#However item_id would require more care for memory and performance reasons and won't be adress in this notebook for time reasons.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's now look at prices and volumes of sales\ntransactionsWithCategory.hist(column='item_price')\ntransactionsWithCategory.hist(column='item_cnt_day')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#It appear that the distribution is quite spiky in both case:\n#- A lot of item are in the range of 0-50000 price while few goes up to 300000, this could be either bad data or very rare case.\n#--> It might be interresting to use a log scale, run two different network or simply remove the outlier here.\n\n#- A lot of item are sold less than 200 time a day while very few 2000 time a day, this could be either bad data or very rare case.\n#--> It might be interresting to see if separating the outliers would have a positive impact on prediction here, if so 2 network is a posibility.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's define a function to clean transaction outlier based on Z-score.\ndef removeOutlierUsingZScore(dataToClean, zScoreThreshold, columNameToClean):\n    z = np.abs(stats.zscore(dataToClean[columNameToClean]))\n    dataCleaned = dataToClean[(z < zScoreThreshold)]\n    removedItems = dataToClean[(z >= zScoreThreshold)]\n    return dataCleaned, removedItems","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zScorePrice = 5\nzScoreCount = 50\n\n#Let's clean the data using some heuristic (manually chosen z-score)\n#The heuristic is arbitrary and might create problem for the items we remove, however we will see latter that it is beneficial overall.\ntransactionCleanPrice, transactionOutlierPrice = removeOutlierUsingZScore(transactionsWithCategory, zScorePrice, 'item_price')\ntransactionCleanCount, transactionOutlierCount = removeOutlierUsingZScore(transactionsWithCategory, zScoreCount, 'item_cnt_day')\ntransactionClean, transactionOutlier = removeOutlierUsingZScore(transactionCleanPrice, zScoreCount, 'item_cnt_day')\n\ntransactionOutlier = pd.concat([transactionOutlier, transactionOutlierCount])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see what we did:\nprint('Source shape:',transactionsWithCategory.shape)\n\nprint('------------------------------')\nprint('Cleaned shape:',transactionClean.shape)\ntransactionClean.hist(column='item_price')\ntransactionClean.hist(column='item_cnt_day')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Outlier shape:',transactionOutlier.shape)\ntransactionOutlier.hist(column='item_price')\ntransactionOutlier.hist(column='item_cnt_day')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This look like much better distributions, great!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------------- \n# FEATURES GENERATION - STEP1 - Generating 10months laggy feature for item count and price.\n# --------------------------------------- ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We have seen that we need to predict november 2015 sales (at least we assume so based on data exploration).\n\n#An option would thus be to train a model to predict november sales using stats from current year from january to october.\n#this mean that we will train on jan-oct for year 2013 and 2014 were we have november sales and evaluate on jan-oct year 2015 were we only have jan-oct data.\n\n#We will see latter in the notebook that generating more laggy data (aka 4 months, 6 months etc) is also gonna be helpfull.\n#PS: It would probably be interesting to try to capture the trend from years to years to extrapolate better from 2013-2014 to 215 however i did not tied.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transactionClean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's collect all data for each year from 1st janurary to end of october sales.\n\n#PS: nomenclature here is train_<year>_<c for clean/o for outlier>_<numberOfMonthForLaggyData>\n\ntrain_15_c_10 = transactionClean[(transactionClean['dateTime'].dt.year == 2015) & (transactionClean['dateTime'].dt.month <= 10)]\ntrain_14_c_10 = transactionClean[(transactionClean['dateTime'].dt.year == 2014) & (transactionClean['dateTime'].dt.month <= 10)]\ntrain_13_c_10 = transactionClean[(transactionClean['dateTime'].dt.year == 2013) & (transactionClean['dateTime'].dt.month <= 10)]\ntrain_15_o_10 = transactionOutlier[(transactionOutlier['dateTime'].dt.year == 2015) & (transactionOutlier['dateTime'].dt.month <= 10)]\ntrain_14_o_10 = transactionOutlier[(transactionOutlier['dateTime'].dt.year == 2014) & (transactionOutlier['dateTime'].dt.month <= 10)]\ntrain_13_o_10 = transactionOutlier[(transactionOutlier['dateTime'].dt.year == 2013) & (transactionOutlier['dateTime'].dt.month <= 10)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here is an helper function to generate laggy feature (aka how many item per shop were sold for a given time range, for example jan-oct)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepareLaggyData(dataName, dataOnPeriod, testData, submissionData):\n    print('-------------------------')\n    print(dataName)\n    print('dataOnPeriod', dataOnPeriod.shape, '\\n')\n    \n    #Dropping date we will agregate on the period and loss it anyway.\n    dataOnPeriod = dataOnPeriod.drop(columns='dateTime')\n    \n    #Only keep ['item_id','shop_id'] pairs that exist in the testData, rest are probably items not sold anymore or\n    #not existing shop, and we have no way to map them to an ID for submission.\n    dataOnPeriodWithExistingID = pd.merge(dataOnPeriod, testData, on=['item_id','shop_id'])\n    print('dataOnPeriodWithExistingID', dataOnPeriodWithExistingID.shape)\n    \n    #Now let's generate our laggy features -->\n    \n    #Total amount of sell (per ID)\n    pairCnt = dataOnPeriodWithExistingID[['ID','item_cnt_day']]\n    pairCntSummed = pairCnt.groupby('ID').sum().astype(int)\n    pairCntSummed = pairCntSummed.rename(columns={\"item_cnt_day\": \"item_cnt_laggy\"})\n    print('pairCntSummed', pairCntSummed.shape)\n    numNan = pairCntSummed.isna().sum()[\"item_cnt_laggy\"]\n    if (numNan != 0):\n        print('NAN FOUND! THIS NEED TO BE ADRESSED')\n        \n    #Avg price on period (per ID)\n    pairPrice = dataOnPeriodWithExistingID[['ID','item_price']]\n    pairPriceAvg = pairPrice.groupby('ID').mean().astype(float)\n    pairPriceAvg = pairPriceAvg.rename(columns={\"item_price\": \"item_price_laggy\"})\n    print('pairPriceAvg', pairPriceAvg.shape)\n    numNan = pairPriceAvg.isna().sum()[\"item_price_laggy\"]\n    if (numNan != 0):\n        print('NAN FOUND! THIS NEED TO BE ADRESSED')\n        \n    #Create laggy feature so it all ID from submission (fill NaNs from missing rows with 0s as int)\n    submissionID = submissionData['ID'].to_frame(name='ID')\n    laggyFeatures = submissionID.merge(pairCntSummed, on = 'ID', how='left')\n    laggyFeatures = laggyFeatures.merge(pairPriceAvg, on = 'ID', how='left')\n    numNan2 = laggyFeatures.isna().sum()['item_cnt_laggy']\n    print('source data covered:',100-(int)(100*numNan2/submissionID.shape[0]), '% of submission IDs, rest will be filled with 0s.')\n    finalLaggyFeatures = laggyFeatures.fillna(0.0).astype(int)\n    print('finalLaggyFeatures', finalLaggyFeatures.shape)\n    return finalLaggyFeatures\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PS: nomenclature here is trainLaggy_<year>_<c for clean/o for outlier>_<numberOfMonthForLaggyData>\ntrainLaggy_15_c_10 = prepareLaggyData('train_15_c_10', train_15_c_10, test, submission)\ntrainLaggy_14_c_10 = prepareLaggyData('train_14_c_10', train_14_c_10, test, submission)\ntrainLaggy_13_c_10 = prepareLaggyData('train_13_c_10', train_13_c_10, test, submission)\ntrainLaggy_15_o_10 = prepareLaggyData('train_15_o_10', train_15_o_10, test, submission)\ntrainLaggy_14_o_10 = prepareLaggyData('train_14_o_10', train_14_o_10, test, submission)\ntrainLaggy_13_o_10 = prepareLaggyData('train_13_o_10', train_13_o_10, test, submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Few really interresting thing here:\n#* We don't have data for all items\n#--> A lot of them were probably neither sold by for the [item_id,shop_id] combination.\n#PS: An interresting experiment would be to see if each year cover a different part of the submission IDs and use this \n#    info to generate better prediction using coverage based logic when stacking models, we won't do this in this notebook however.\n\n#* We can also see that year 2015 cover more IDs than 2014, itself having a better cover than 2013.\n#--> It is possible that some item/shop pair we are trying to predict in 2015 were not existing in the past.\n\n#* We can see that removed outliers account for a very low percent of the submission ideas\n#--> We can probably discard this data completely.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def countCategoricalFeatureUniqueItems(dataName, data):\n    print('---------', dataName, '----------')\n    print('num shops', data.groupby('shop_id')['shop_id'].nunique().shape)\n    print('num category', data.groupby('item_category_id')['item_category_id'].nunique().shape)\n    print('num item', data.groupby('item_id')['item_id'].nunique().shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def enrichLaggyData(data, test, items):\n    tmpData = data.merge(test, on = 'ID', how='left')\n    tmpData = tmpData.merge(items, on='item_id', how='left')\n    tmpData = tmpData.drop(columns=\"item_name\")\n    return tmpData","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's add back some data to those laggy features.\ntrainLaggy_15_c_10_tmp = enrichLaggyData(trainLaggy_15_c_10, test, items)\ntrainLaggy_14_c_10_tmp = enrichLaggyData(trainLaggy_14_c_10, test, items)\ntrainLaggy_13_c_10_tmp = enrichLaggyData(trainLaggy_13_c_10, test, items)\n\n#Let's see it if make sense to convert categorical features using simple one hot encoding.\ncountCategoricalFeatureUniqueItems('trainLaggy_15_c_10', trainLaggy_15_c_10_tmp)\ncountCategoricalFeatureUniqueItems('trainLaggy_14_c_10', trainLaggy_14_c_10_tmp)\ncountCategoricalFeatureUniqueItems('trainLaggy_13_c_10', trainLaggy_13_c_10_tmp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ok it does for shop and category, as seen before item_id would require more care and i'm letting it as an int for time reasons.\n#Let's define an helper for this:\ndef oneHotEncodeCategoricalFeature(data, categoryName):\n    onehot = pd.get_dummies(data[categoryName], prefix=categoryName)\n    dataWithOneHot = pd.concat([data, onehot], 1)\n    dataWithOneHot = dataWithOneHot.drop(columns=categoryName)\n    return dataWithOneHot\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainLaggy_15_c_10_tmp = oneHotEncodeCategoricalFeature(trainLaggy_15_c_10_tmp, 'shop_id')\ntrainLaggy_14_c_10_tmp = oneHotEncodeCategoricalFeature(trainLaggy_14_c_10_tmp, 'shop_id')\ntrainLaggy_13_c_10_tmp = oneHotEncodeCategoricalFeature(trainLaggy_13_c_10_tmp, 'shop_id')\ntrainLaggy_15_c_10_final = oneHotEncodeCategoricalFeature(trainLaggy_15_c_10_tmp, 'item_category_id')\ntrainLaggy_14_c_10_final = oneHotEncodeCategoricalFeature(trainLaggy_14_c_10_tmp, 'item_category_id')\ntrainLaggy_13_c_10_final = oneHotEncodeCategoricalFeature(trainLaggy_13_c_10_tmp, 'item_category_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see what we have now:\ntrainLaggy_15_c_10_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------------- \n# PREDICTION/SUBMISSION - STEP1/2 - Using 10 month laggy data.\n# --------------------------------------- ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's add november target predictions to 2013 and 2014 laggy data, so we can fit a model on them.\ntransactionNov14 = train[(train['dateTime'].dt.year == 2014) & (train['dateTime'].dt.month == 11)]\ntransactionNov13 = train[(train['dateTime'].dt.year == 2013) & (train['dateTime'].dt.month == 11)]\ntargetPredictionNov14 = prepareLaggyData('transactionNov14', transactionNov14, test, submission).drop(columns='item_price_laggy')\ntargetPredictionNov13 = prepareLaggyData('transactionNov13', transactionNov13, test, submission).drop(columns='item_price_laggy')\n\ntargetPredictionNov14 = targetPredictionNov14.rename(columns={'item_cnt_laggy':'item_cnt_month'})\ntargetPredictionNov13 = targetPredictionNov13.rename(columns={'item_cnt_laggy':'item_cnt_month'})\n\ntargetPredictionNov14.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can see that a lot of item were not sold on november, however this is fine as we are tasked into predicting exactly that (aka november sales).","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For now let's have two models one for year 13 and one for year 14.\nmodel13_10months = XGBRegressor()\nmodel14_10months = XGBRegressor()\n\nmodel13_10months.fit(trainLaggy_13_c_10_final, targetPredictionNov13['item_cnt_month'])\nmodel14_10months.fit(trainLaggy_14_c_10_final, targetPredictionNov14['item_cnt_month'])\n\nprediction15From13_10months = model13_10months.predict(trainLaggy_15_c_10_final)\nprediction15From14_10months = model14_10months.predict(trainLaggy_15_c_10_final)\n\nprint(prediction15From13_10months.shape)\nprint(prediction15From14_10months.shape)\n\n#For now final prediction for 2015 will be the average from both model(one trained on 2013 and one trained on 2014 data).\nprediction15_10months = (prediction15From13_10months + prediction15From14_10months) / 2\n\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's try a submission with the above.\nsubmission['item_cnt_month'] = prediction15_10months\nsubmission.head()\nsubmission.to_csv(\"prediction15_10months.csv\", index= False)\n#score of 1.95","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's try a submission with clamp value from the course / the evaluation page of the competition.\nsubmissionClipped = submission\nsubmissionClipped['item_cnt_month'] = submissionClipped['item_cnt_month'].clip(0,20)\nsubmissionClipped.to_csv(\"prediction15_10months_clipped.csv\", index= False)\n#score of 1.21, it is thus very important to always clip the result to match what we are tested on.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------------- \n# CROSS VALIDATION STEP 1/3 - Linear stacking of model from 2013 and 2014 datas\n# --------------------------------------- ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To do cross validation we will try to do exactly as above (aka 10month laggy on november), but instead evaluate on october 2015.\n#the reason is that we can have exactly the same process however we actually have october data for 2015. Also this should prevent\n#overfitting to the public test set from the competition submissions as we don't actually use those value but rather optimize for\n#the general scheme of the competition (well on october so we can still overfit to october, but well time is limited so i will take this risk).\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#target prediction for 2013-2015\ntransactionOct15 = train[(train['dateTime'].dt.year == 2015) & (train['dateTime'].dt.month == 10)]\ntransactionOct14 = train[(train['dateTime'].dt.year == 2014) & (train['dateTime'].dt.month == 10)]\ntransactionOct13 = train[(train['dateTime'].dt.year == 2013) & (train['dateTime'].dt.month == 10)]\ntargetPredictionOct15 = prepareLaggyData('transactionOct15', transactionOct15, test, submission).drop(columns='item_price_laggy')\ntargetPredictionOct14 = prepareLaggyData('transactionOct14', transactionOct14, test, submission).drop(columns='item_price_laggy')\ntargetPredictionOct13 = prepareLaggyData('transactionOct13', transactionOct13, test, submission).drop(columns='item_price_laggy')\ntargetPredictionOct15 = targetPredictionOct15.rename(columns={'item_cnt_laggy':'item_cnt_month'})\ntargetPredictionOct14 = targetPredictionOct14.rename(columns={'item_cnt_laggy':'item_cnt_month'})\ntargetPredictionOct13 = targetPredictionOct13.rename(columns={'item_cnt_laggy':'item_cnt_month'})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's first define an helper method based on the above work:\ndef prepareTrainData(name, year, monthStart, monthStop):\n    trainDataTmp = transactionClean[(transactionClean['dateTime'].dt.year == year) & (transactionClean['dateTime'].dt.month <= monthStop) & (transactionClean['dateTime'].dt.month >= monthStart)]\n    trainDataTmp = prepareLaggyData(name, trainDataTmp, test, submission)\n    trainDataTmp = enrichLaggyData(trainDataTmp, test, items)\n    trainDataTmp = oneHotEncodeCategoricalFeature(trainDataTmp, 'shop_id')\n    trainDataTmp = oneHotEncodeCategoricalFeature(trainDataTmp, 'item_category_id')\n    return trainDataTmp\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crossTrain_15_c_9 = prepareTrainData('crossTrain_15_c_9',2015,1,9)\ncrossTrain_14_c_9 = prepareTrainData('crossTrain_14_c_9',2014,1,9)\ncrossTrain_13_c_9 = prepareTrainData('crossTrain_13_c_9',2013,1,9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's also have a bit more helper methods:\ndef fitXGBR(data, target):\n    modelTmp = XGBRegressor()\n    modelTmp.fit(data, target['item_cnt_month'])\n    return modelTmp\n\ndef testSubmission(prediction, target):\n    targetClipped = target\n    targetClipped['item_cnt_month'] = targetClipped['item_cnt_month'].clip(0,20)\n    \n    submissionClipped = submission\n    submissionClipped['item_cnt_month'] = prediction\n    submissionClipped['item_cnt_month'] = submissionClipped['item_cnt_month'].clip(0,20)\n   \n    rmse = ((submission - targetClipped) ** 2).mean() ** .5\n    return rmse['item_cnt_month']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oct_model14_9months = fitXGBR(crossTrain_14_c_9, targetPredictionOct14)\noct_model13_9months = fitXGBR(crossTrain_13_c_9, targetPredictionOct13)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oct_prediction15From13_9months = oct_model14_9months.predict(crossTrain_15_c_9)\noct_prediction15From14_9months = oct_model13_9months.predict(crossTrain_15_c_9)\n\noct_prediction15_9months = (oct_prediction15From13_9months + oct_prediction15From14_9months) / 2\n\nrmse = testSubmission(oct_prediction15_9months, targetPredictionOct15)\nprint(\"RMSE = \", rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's try to see if we can find better than avg of year 2013 and 2014.\nrmse13 = testSubmission(oct_prediction15From13_9months, targetPredictionOct15)\nrmse14 = testSubmission(oct_prediction15From14_9months, targetPredictionOct15)\nprint(\"1.0 / 2014 -> RMSE = \", rmse14)\nprint(\"0.0 / 2013 -> RMSE = \", rmse13)\n\nprint(\"----------\")\nfor i in range(0,10):\n    w = i/10.0\n    testpred  = oct_prediction15From13_9months * w\n    testpred += oct_prediction15From14_9months * (1-w)\n    rmse = testSubmission(testpred, targetPredictionOct15)\n    print(w,\"-> RMSE = \", rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"---------- (range 0.7-0.9)\")\nfor i in range(0,20):\n    w = i/100.0 + 0.7\n    testpred  = oct_prediction15From13_9months * w\n    testpred += oct_prediction15From14_9months * (1-w)\n    rmse = testSubmission(testpred, targetPredictionOct15)\n    print(w,\"-> RMSE = \", rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So the best way to linearly interpolate between those two model seems to be : 2013*w+2014*(1-w) with W = 0.85.\n#Let's define a fast helper then:\ndef getFinalPred(prediction15From13,prediction15From14):\n    w = 0.85\n    return prediction15From13 * w + prediction15From14 * (1-w)\n\nprint(testSubmission(getFinalPred(oct_prediction15From13_9months,oct_prediction15From14_9months), targetPredictionOct15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------------- \n# FEATURES GENERATION - STEP2/2 - Adding 7months, 4months and 1 month laggy price and count\n# --------------------------------------- ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generateExtraLaggyFeature(name, year, monthStart, monthStop):\n    trainDataTmp = transactionClean[(transactionClean['dateTime'].dt.year == year) & (transactionClean['dateTime'].dt.month <= monthStop) & (transactionClean['dateTime'].dt.month >= monthStart)]\n    trainDataTmp = prepareLaggyData(name, trainDataTmp, test, submission)\n    trainDataTmp = trainDataTmp.rename(columns={'item_cnt_laggy':'item_cnt_'+name})\n    trainDataTmp = trainDataTmp.rename(columns={'item_price_laggy':'item_price_'+name})\n    return trainDataTmp\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def addExtraLaggyFeatures(trainDataWithLaggy10month, year, monthStop):\n\n    laggyFeature_7months = generateExtraLaggyFeature('7months',year, monthStop-7,monthStop)\n    laggyFeature_4months = generateExtraLaggyFeature('4months',year, monthStop-4,monthStop)\n    laggyFeature_1month  = generateExtraLaggyFeature('1month' ,year, monthStop,monthStop)\n\n    trainAllLaggy = trainDataWithLaggy10month.merge(laggyFeature_7months, on = 'ID', how='left')\n    trainAllLaggy = trainAllLaggy.merge(laggyFeature_4months, on = 'ID', how='left')\n    trainAllLaggy = trainAllLaggy.merge(laggyFeature_1month, on = 'ID', how='left')\n    return trainAllLaggy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's try to have more generated feature for the models, aka various laggy length.\n#before we had for 10 month: jan-oct aka 1-10\n#let's add 7 months: avril-oct aka 4-10\n#let's add 4 months: july-oct aka 7-10\n#let's add 1 month: october aka 10\n\ntrainAllLaggy_13 = addExtraLaggyFeatures(trainLaggy_13_c_10_final, 2013, 10)\ntrainAllLaggy_14 = addExtraLaggyFeatures(trainLaggy_14_c_10_final, 2014, 10)\ntrainAllLaggy_15 = addExtraLaggyFeatures(trainLaggy_15_c_10_final, 2015, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clipPrediction(pred):\n    return pd.DataFrame(pred, columns=[\"item_cnt_month\"]).clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's fit those new training sets, and predict on clipped target value (as anyway we want to predict in that range).\ntargetPredictionNov14Clipped = clipPrediction(targetPredictionNov14) \ntargetPredictionNov13Clipped = clipPrediction(targetPredictionNov13) \nmodel14_AllLaggy = fitXGBR(trainAllLaggy_13, targetPredictionNov14Clipped)\nmodel13_AllLaggy = fitXGBR(trainAllLaggy_14, targetPredictionNov13Clipped)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#And predict 2015 from the 2 new models.\nprediction15From13_AllLaggy = model13_AllLaggy.predict(trainAllLaggy_15)\nprediction15From14_AllLaggy = model14_AllLaggy.predict(trainAllLaggy_15)\nprediction15From13_AllLaggyClipped = clipPrediction(prediction15From13_AllLaggy)\nprediction15From14_AllLaggyClipped = clipPrediction(prediction15From14_AllLaggy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predAllLaggy = getFinalPred(prediction15From13_AllLaggyClipped,prediction15From14_AllLaggyClipped)\nsubmission['item_cnt_month'] = predAllLaggy\nsubmission.to_csv(\"allLagSubTest.csv\", index= False)\n# score 1.20790","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dropNonItemCountFeature(data):\n    dataSimple = data.drop(columns='ID')\n    dataSimple = dataSimple.drop(columns=list(dataSimple.filter(regex='item_id')))\n    dataSimple = dataSimple.drop(columns=list(dataSimple.filter(regex='shop_id')))\n    dataSimple = dataSimple.drop(columns=list(dataSimple.filter(regex='item_category_id')))\n    dataSimple = dataSimple.drop(columns=list(dataSimple.filter(regex='item_price')))\n    return dataSimple","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see if a simpler dataset could give better result.\ntrainAllLaggy_15_simple = dropNonItemCountFeature(trainAllLaggy_15)\ntrainAllLaggy_14_simple = dropNonItemCountFeature(trainAllLaggy_14)\ntrainAllLaggy_13_simple = dropNonItemCountFeature(trainAllLaggy_13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model14_AllLaggySimple = fitXGBR(trainAllLaggy_13_simple, targetPredictionNov13Clipped)\nmodel13_AllLaggySimple = fitXGBR(trainAllLaggy_14_simple, targetPredictionNov14Clipped)\nprediction15From13_AllLaggySimple = model13_AllLaggySimple.predict(trainAllLaggy_15_simple)\nprediction15From14_AllLaggySimple = model14_AllLaggySimple.predict(trainAllLaggy_15_simple)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction15From13_AllLaggySimpleClipped = clipPrediction(prediction15From13_AllLaggySimple)\nprediction15From14_AllLaggySimpleClipped = clipPrediction(prediction15From14_AllLaggySimple)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predAllLaggySimple = getFinalPred(prediction15From13_AllLaggySimpleClipped,prediction15From14_AllLaggySimpleClipped)\nsubmission['item_cnt_month'] = predAllLaggySimple\nsubmission.to_csv(\"allLaggySimpleSubTest.csv\", index= False)\n#Score 1.03724  --> something in the extra data is preventing the model to perform well, interresting!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------------- \n# CROSS VALIDATION STEP 2/3 - Selecting the best features set.\n# ---------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainLaggy_13_c_10_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's use cross validation to find what is the best features to use, all of them is not great but a subset is probably good.\ncrossTrain_AllLaggy_15 = addExtraLaggyFeatures(crossTrain_15_c_9, 2015, 9)\ncrossTrain_AllLaggy_14 = addExtraLaggyFeatures(crossTrain_14_c_9, 2014, 9)\ncrossTrain_AllLaggy_13 = addExtraLaggyFeatures(crossTrain_13_c_9, 2013, 9)\ntargetPredictionOct15Clipped = clipPrediction(targetPredictionOct15)\ntargetPredictionOct14Clipped = clipPrediction(targetPredictionOct14)\ntargetPredictionOct13Clipped = clipPrediction(targetPredictionOct13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crossModel14_AllLaggy = fitXGBR(crossTrain_AllLaggy_14, targetPredictionOct14Clipped)\ncrossMode13_AllLaggy = fitXGBR(crossTrain_AllLaggy_13, targetPredictionOct13Clipped)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trainPredictAndEvalute(ID=True, item_id=True, item_category=True, shop_id=True):\n    #prepare data\n    testData15 = crossTrain_AllLaggy_15\n    testData14 = crossTrain_AllLaggy_14\n    testData13 = crossTrain_AllLaggy_13\n    if ID==False:\n        testData15 = testData15.drop(columns=list(testData15.filter(regex='ID')))\n        testData14 = testData14.drop(columns=list(testData14.filter(regex='ID')))\n        testData13 = testData13.drop(columns=list(testData13.filter(regex='ID')))    \n    if item_id==False:\n        testData15 = testData15.drop(columns=list(testData15.filter(regex='item_id')))\n        testData14 = testData14.drop(columns=list(testData14.filter(regex='item_id')))\n        testData13 = testData13.drop(columns=list(testData13.filter(regex='item_id')))    \n    if item_category==False:\n        testData15 = testData15.drop(columns=list(testData15.filter(regex='item_category')))\n        testData14 = testData14.drop(columns=list(testData14.filter(regex='item_category')))\n        testData13 = testData13.drop(columns=list(testData13.filter(regex='item_category')))    \n    if shop_id==False:\n        testData15 = testData15.drop(columns=list(testData15.filter(regex='shop_id')))\n        testData14 = testData14.drop(columns=list(testData14.filter(regex='shop_id')))\n        testData13 = testData13.drop(columns=list(testData13.filter(regex='shop_id')))    \n    \n    #print(testData13.columns)\n    #train\n    testModel14 = fitXGBR(testData14, targetPredictionOct14Clipped)\n    testModel13 = fitXGBR(testData13, targetPredictionOct13Clipped)\n    #predict\n    pred13 = testModel14.predict(testData15)\n    pred14 = testModel13.predict(testData15)\n    pred = getFinalPred(pred13, pred14)\n    #rmse\n    rmse = testSubmission(pred, targetPredictionOct15Clipped)\n    print(\"ID=\", ID, \"/ item_id=\", item_id, \"/ item_category=\", item_category, \"/ shop_id=\", shop_id, \" ==> RMSE: \", rmse)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#With all extra feature \n#trainPredictAndEvalute()\n\n#^ Commented as quite slow to compute, output is:\n#ID= True / item_id= True / item_category= True / shop_id= True  ==> RMSE:  0.9492638952203369","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#No extra data\n#trainPredictAndEvalute(ID=False, item_id=False, item_category=False, shop_id=False)\n\n#Commented as quite slow to compute, output is:\n#ID= False / item_id= False / item_category= False / shop_id= False  ==> RMSE:  0.9273928270147149","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One extra feature\n#trainPredictAndEvalute(ID=True , item_id=False, item_category=False, shop_id=False)\n#trainPredictAndEvalute(ID=False, item_id=True , item_category=False, shop_id=False)\n#trainPredictAndEvalute(ID=False, item_id=False, item_category=True , shop_id=False)\n#trainPredictAndEvalute(ID=False, item_id=False, item_category=False, shop_id=True )\n\n#^ Commented as quite slow to compute, output is:\n#ID= True / item_id= False / item_category= False / shop_id= False  ==> RMSE:  0.9168895917744182\n#ID= False / item_id= True / item_category= False / shop_id= False  ==> RMSE:  0.9573129684418232\n#ID= False / item_id= False / item_category= True / shop_id= False  ==> RMSE:  0.9128995565549166\n#ID= False / item_id= False / item_category= False / shop_id= True  ==> RMSE:  0.9149498618412979","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Two extra features\n#trainPredictAndEvalute(ID=False, item_id=False, item_category=True , shop_id=True )\n#trainPredictAndEvalute(ID=False, item_id=True , item_category=False, shop_id=True )\n#trainPredictAndEvalute(ID=True , item_id=False, item_category=False, shop_id=True )\n#trainPredictAndEvalute(ID=False, item_id=True , item_category=True , shop_id=False)\n#trainPredictAndEvalute(ID=True , item_id=False, item_category=True , shop_id=False)\n#trainPredictAndEvalute(ID=True , item_id=True , item_category=False, shop_id=False)\n\n#^ Commented as quite slow to compute, output is:\n#ID= False / item_id= False / item_category= True / shop_id= True  ==> RMSE:  0.9037315880761034\n#ID= False / item_id= True / item_category= False / shop_id= True  ==> RMSE:  0.9473394881388407\n#ID= True / item_id= False / item_category= False / shop_id= True  ==> RMSE:  0.9136952566849936\n#ID= False / item_id= True / item_category= True / shop_id= False  ==> RMSE:  0.957341665651518\n#ID= True / item_id= False / item_category= True / shop_id= False  ==> RMSE:  0.9072651151695039\n#ID= True / item_id= True / item_category= False / shop_id= False  ==> RMSE:  0.95130676158712","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3 extra features\n#trainPredictAndEvalute(ID=True , item_id=True , item_category=True , shop_id=False)\n#trainPredictAndEvalute(ID=True , item_id=True , item_category=False, shop_id=True )\n#trainPredictAndEvalute(ID=True , item_id=False, item_category=True , shop_id=True )\n#trainPredictAndEvalute(ID=False, item_id=True , item_category=True , shop_id=True )\n\n#^ Commented as quite slow to compute, output is:\n#ID= True / item_id= True / item_category= True / shop_id= False  ==> RMSE:  0.9567579927032015\n#ID= True / item_id= True / item_category= False / shop_id= True  ==> RMSE:  0.9415430168587506\n#ID= True / item_id= False / item_category= True / shop_id= True  ==> RMSE:  0.9123035362411145\n#ID= False / item_id= True / item_category= True / shop_id= True  ==> RMSE:  0.9527056930511119","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#It appears that the best configuration we can have is using `shop_id` and `item_category` features (ie dropping `ID` and `item_id`).\n#That make sense as the kept features are the one that were one hot encoded properly!\n\n#An interresting fact is that `ID` alone give quite a good result, this might be a sign that ID is not random and could potentially be investigated for data leakage (maybe category can be infer from it or something else?)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testData15 = crossTrain_AllLaggy_15\ntestData14 = crossTrain_AllLaggy_14\ntestData13 = crossTrain_AllLaggy_13\ntestData15 = testData15.drop(columns=list(testData15.filter(regex='ID')))\ntestData14 = testData14.drop(columns=list(testData14.filter(regex='ID')))\ntestData13 = testData13.drop(columns=list(testData13.filter(regex='ID')))    \ntestData15 = testData15.drop(columns=list(testData15.filter(regex='item_id')))\ntestData14 = testData14.drop(columns=list(testData14.filter(regex='item_id')))\ntestData13 = testData13.drop(columns=list(testData13.filter(regex='item_id')))    \n\ncrossTrain_AllLaggy_15_final = testData15\ncrossTrain_AllLaggy_14_final = testData14\ncrossTrain_AllLaggy_13_final = testData13\n\ncrossTrain_AllLaggy_13_final.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Before moving further let's do the same check with our laggy data features:","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trainPredictAndEvaluateLaggyData(Count = True, Price=True):\n    #prepare data\n    testData15 = crossTrain_AllLaggy_15_final\n    testData14 = crossTrain_AllLaggy_14_final\n    testData13 = crossTrain_AllLaggy_13_final\n    if Count==False:\n        testData15 = testData15.drop(columns=list(testData15.filter(regex='cnt')))\n        testData14 = testData14.drop(columns=list(testData14.filter(regex='cnt')))\n        testData13 = testData13.drop(columns=list(testData13.filter(regex='cnt')))    \n    if Price==False:\n        testData15 = testData15.drop(columns=list(testData15.filter(regex='price')))\n        testData14 = testData14.drop(columns=list(testData14.filter(regex='price')))\n        testData13 = testData13.drop(columns=list(testData13.filter(regex='price')))    \n    \n    #train\n    testModel14 = fitXGBR(testData14, targetPredictionOct14Clipped)\n    testModel13 = fitXGBR(testData13, targetPredictionOct13Clipped)\n    #predict\n    pred13 = testModel14.predict(testData15)\n    pred14 = testModel13.predict(testData15)\n    pred = getFinalPred(pred13, pred14)\n    #rmse\n    rmse = testSubmission(pred, targetPredictionOct15Clipped)\n    print(\"Cnt=\", Count, \"/ Price=\", Price,\"==> RMSE: \", rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trainPredictAndEvaluateLaggyData(Count = True , Price = True )\n#trainPredictAndEvaluateLaggyData(Count = True , Price = False)\n#trainPredictAndEvaluateLaggyData(Count = False, Price = True )\n#trainPredictAndEvaluateLaggyData(Count = False, Price = False)\n\n#Commented as quite slow to compute, output is:\n#Cnt= True  / Price= True  ==> RMSE:  0.9037315880761034\n#Cnt= True  / Price= False ==> RMSE:  0.911286786899765\n#Cnt= False / Price= True  ==> RMSE:  1.0285608166824878\n#Cnt= False / Price= False ==> RMSE:  1.0031688013402924\n\n#Witch prove that both `count` and `price` laggy feature are usefull to the model. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------------- \n# CROSS VALIDATION STEP 3/3 - Hyper parameters tunning.\n# ---------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's now try to optimize model hyper parameters on this data. For simplicity and speed we will only use testData13 as anyway final result is 85% of it.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#code snippet adapted from https://www.kaggle.com/jayatou/xgbregressor-with-gridsearchcv\n#one could/should definitively push grid search for greater effect, time is the limiting factor here.\n#regressorModel = XGBRegressor()\n#parameters = {'learning_rate': [0.1, 0.2, 0.3],#aka `eta`, default is 0.3\n#              'max_depth': [6, 7],# default is 6\n#              'min_child_weight': [1,15],# default is 1\n#              'subsample': [0.8, 1.0], # default is 1.0\n#              'colsample_bytree': [0.8,1.0],# default is 1\n#              'colsample_bylevel': [0.8,1.0]}# default is 1\n#xgb_grid = GridSearchCV(regressorModel,\n#                        parameters,\n#                        n_jobs = 5,\n#                        verbose=True)\n#xgb_grid.fit(crossTrain_AllLaggy_13_final, targetPredictionOct13Clipped)\n#\n#print(xgb_grid.best_score_)\n#print(xgb_grid.best_params_)\n\n#Commented as quite slow to compute, output is:\n#Fitting 5 folds for each of 96 candidates, totalling 480 fits\n#[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n#[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed: 17.7min\n#[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed: 89.3min\n#[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed: 228.3min\n#[Parallel(n_jobs=5)]: Done 480 out of 480 | elapsed: 253.4min finished\n#0.6109913473710371\n#{'colsample_bylevel': 0.8, 'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 6, 'min_child_weight': 15, 'subsample': 1.0}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------------- \n# PREDICTION/SUBMISSION STEP 2/2 - Final prediction and submission\n# ---------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fitXGBROptimizedParameters(data, target):\n    modelTmp = XGBRegressor(learning_rate=0.2, max_depth=6, min_child_weight=15, subsample=1.0, colsample_bytree=1.0, colsample_bylevel=0.8)\n    modelTmp.fit(data, target['item_cnt_month'])\n    return modelTmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def onlyKeepBestFeatures(data):\n    bestData = data.drop(columns=list(data.filter(regex='item_category_id')))\n    bestData = bestData.drop(columns=list(bestData.filter(regex='item_price')))\n    bestData = bestData.drop(columns=list(bestData.filter(regex='ID')))\n    bestData = bestData.drop(columns=list(bestData.filter(regex='item_id')))\n    return bestData","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainBestFeatures_13 = onlyKeepBestFeatures(trainAllLaggy_13)\ntrainBestFeatures_14 = onlyKeepBestFeatures(trainAllLaggy_14)\ntrainBestFeatures_15 = onlyKeepBestFeatures(trainAllLaggy_15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model13_bestFeatures = fitXGBROptimizedParameters(trainBestFeatures_13, targetPredictionNov13Clipped)\nmodel14_bestFeatures = fitXGBROptimizedParameters(trainBestFeatures_14, targetPredictionNov14Clipped)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction15From13_bestFeatures = model13_bestFeatures.predict(trainBestFeatures_15)\nprediction15From14_bestFeatures = model14_bestFeatures.predict(trainBestFeatures_15)\nprediction15From13_bestFeatures_clipped = clipPrediction(prediction15From13_bestFeatures)\nprediction15From14_bestFeatures_clipped = clipPrediction(prediction15From14_bestFeatures)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predBestFeatures = getFinalPred(prediction15From13_bestFeatures_clipped,prediction15From14_bestFeatures_clipped)\nsubmission['item_cnt_month'] = predBestFeatures\nsubmission.to_csv(\"finalPredictionNoID.csv\", index= False)\n#Score 1.03345.\n\n#I hope you enjoyed the read :)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#---------------------------------------------------------\n# ANNEX/BONUS - Let's add more features and try other libs\n#---------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance\nplt.rcParams[\"figure.figsize\"] = (15, 6)\nplot_importance(model13_bestFeatures)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def addMoreExtraLaggyFeatures(trainDataToEnrich, year, monthStop):\n\n    laggyFeature_1month1  = generateExtraLaggyFeature('01' ,year, monthStop-9,monthStop-9)\n    laggyFeature_1month2  = generateExtraLaggyFeature('02' ,year, monthStop-8,monthStop-8)\n    laggyFeature_1month3  = generateExtraLaggyFeature('03' ,year, monthStop-7,monthStop-7)\n    laggyFeature_1month4  = generateExtraLaggyFeature('04' ,year, monthStop-6,monthStop-6)\n    laggyFeature_1month5  = generateExtraLaggyFeature('05' ,year, monthStop-5,monthStop-5)\n    laggyFeature_1month6  = generateExtraLaggyFeature('06' ,year, monthStop-4,monthStop-4)\n    laggyFeature_1month7  = generateExtraLaggyFeature('07' ,year, monthStop-3,monthStop-3)\n    laggyFeature_1month8  = generateExtraLaggyFeature('08' ,year, monthStop-2,monthStop-2)\n    laggyFeature_1month9  = generateExtraLaggyFeature('09' ,year, monthStop-1,monthStop-1)\n    \n    trainDataToEnrich = trainDataToEnrich.merge(laggyFeature_1month1, on = 'ID', how='left')\n    trainDataToEnrich = trainDataToEnrich.merge(laggyFeature_1month2, on = 'ID', how='left')\n    trainDataToEnrich = trainDataToEnrich.merge(laggyFeature_1month3, on = 'ID', how='left')\n    trainDataToEnrich = trainDataToEnrich.merge(laggyFeature_1month4, on = 'ID', how='left')\n    trainDataToEnrich = trainDataToEnrich.merge(laggyFeature_1month5, on = 'ID', how='left')\n    trainDataToEnrich = trainDataToEnrich.merge(laggyFeature_1month6, on = 'ID', how='left')\n    trainDataToEnrich = trainDataToEnrich.merge(laggyFeature_1month7, on = 'ID', how='left')\n    trainDataToEnrich = trainDataToEnrich.merge(laggyFeature_1month8, on = 'ID', how='left')\n    trainDataToEnrich = trainDataToEnrich.merge(laggyFeature_1month9, on = 'ID', how='left')\n    \n    return trainDataToEnrich","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainAllLaggy_13 = addMoreExtraLaggyFeatures(trainAllLaggy_13,2013,11)\ntrainAllLaggy_14 = addMoreExtraLaggyFeatures(trainAllLaggy_14,2014,11)\ntrainAllLaggy_15 = addMoreExtraLaggyFeatures(trainAllLaggy_15,2015,11)\n\ntrainBestFeatures_13 = onlyKeepBestFeatures(trainAllLaggy_13)\ntrainBestFeatures_14 = onlyKeepBestFeatures(trainAllLaggy_14)\ntrainBestFeatures_15 = onlyKeepBestFeatures(trainAllLaggy_15)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model13_bestFeatures = fitXGBROptimizedParameters(trainBestFeatures_13, targetPredictionNov13Clipped)\nmodel14_bestFeatures = fitXGBROptimizedParameters(trainBestFeatures_14, targetPredictionNov14Clipped)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainBestFeatures_13.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance\nplt.rcParams[\"figure.figsize\"] = (15, 6)\nplot_importance(model13_bestFeatures)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction15From13_bestFeatures = model13_bestFeatures.predict(trainBestFeatures_15)\nprediction15From14_bestFeatures = model14_bestFeatures.predict(trainBestFeatures_15)\nprediction15From13_bestFeatures_clipped = clipPrediction(prediction15From13_bestFeatures)\nprediction15From14_bestFeatures_clipped = clipPrediction(prediction15From14_bestFeatures)\n\npredBestFeatures = getFinalPred(prediction15From13_bestFeatures_clipped,prediction15From14_bestFeatures_clipped)\nsubmission['item_cnt_month'] = predBestFeatures\nsubmission.to_csv(\"finalPredictionMoreLaggy.csv\", index= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's try catboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import Pool\nfrom catboost import CatBoostRegressor\n\ndef fitCatBoost(data, target):\n    modelTmp = CatBoostRegressor(iterations=500, max_ctr_complexity=4, random_seed=0, od_type='Iter', od_wait=25, verbose=50, depth=4)\n    modelTmp.fit(data, target['item_cnt_month'])\n    return modelTmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model13_CatBoost = fitCatBoost(trainBestFeatures_13, targetPredictionNov13Clipped)\nmodel14_CatBoost = fitCatBoost(trainBestFeatures_14, targetPredictionNov14Clipped)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction15From13_catboost = model13_CatBoost.predict(trainBestFeatures_15)\nprediction15From14_catboost = model14_CatBoost.predict(trainBestFeatures_15)\nprediction15From13_catboost_clipped = clipPrediction(prediction15From13_catboost)\nprediction15From14_catboost_clipped = clipPrediction(prediction15From14_catboost)\n\npredBestFeatures = getFinalPred(prediction15From13_catboost_clipped,prediction15From14_catboost_clipped)\nsubmission['item_cnt_month'] = predBestFeatures\nsubmission.to_csv(\"finalPredictionCatBoost.csv\", index= False)\n# Score 1.03543","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's try randomforest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\ndef fitRF(data, target):\n    modelTmp = RandomForestRegressor(n_estimators=50, max_depth=7, random_state=0, n_jobs=-1)\n    modelTmp.fit(data, target['item_cnt_month'])\n    return modelTmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model13_RF = fitRF(trainBestFeatures_13, targetPredictionNov13Clipped)\nmodel14_RF = fitRF(trainBestFeatures_14, targetPredictionNov14Clipped)\n\nprediction15From13_rf = model13_RF.predict(trainBestFeatures_15)\nprediction15From14_rf = model14_RF.predict(trainBestFeatures_15)\nprediction15From13_rf_clipped = clipPrediction(prediction15From13_rf)\nprediction15From14_rf_clipped = clipPrediction(prediction15From14_rf)\n\npredBestFeatures = getFinalPred(prediction15From13_rf_clipped,prediction15From14_rf_clipped)\nsubmission['item_cnt_month'] = predBestFeatures\nsubmission.to_csv(\"finalPredictionRF.csv\", index= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}