{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import product\nfrom tqdm import tqdm, tqdm_notebook\n\nimport sklearn\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nimport tensorflow as tf\nimport keras\nimport pickle\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for p in [np, pd, sklearn, lgb]:\n    print (p.__name__, p.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load Dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\nitems = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nitems_categories = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\nsample_submission = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')\ntest_data = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exploratory Data Analysis**\n\n\nItems per category\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items_per_category = items.item_category_id.value_counts().iloc[:10]\nitems_per_category = items_per_category.sort_index(sort_remaining=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(items_per_category.index, items_per_category.values)\nplt.xlabel(\"Item Category\")\nplt.ylabel(\"Number of items\")\nplt.title(\"Number of items per category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get the popular shops cause that might contain a pattern","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"popular_shops = sales.groupby('shop_id')['item_cnt_day'].sum()\npopular_shops = popular_shops.sort_values(ascending=False)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(popular_shops.index, popular_shops.values)\nplt.xlabel(\"Shop ID\")\nplt.ylabel(\"Count\")\nplt.title(\"Popular shops\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get popular items","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"popular_items = sales.groupby('item_id')['item_cnt_day'].sum()\npopular_items = popular_items.sort_values(ascending=False)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(popular_items.index, popular_items.values)\nplt.xlabel(\"Item ID\")\nplt.ylabel(\"Count\")\nplt.title(\"Popular items\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"20949 is pretty popular huh!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Items sold per month","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"item_cnt_month = sales.groupby('date_block_num')['item_cnt_day'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.barplot(item_cnt_month.index, item_cnt_month.values)\nplt.plot(item_cnt_month.index, item_cnt_month.values)\nplt.xlabel(\"Month number\")\nplt.ylabel(\"Count\")\nplt.title(\"Number of items sold in each month\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are spikes at November months but the overall trend is decreasing sales","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Total sales per month is groos amount","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"price_cnt_month = sales.groupby('date_block_num')['item_price'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.barplot(price_cnt_month.index, price_cnt_month.values)\nplt.plot(price_cnt_month.index, price_cnt_month.values)\nplt.xlabel(\"Month number\")\nplt.ylabel(\"Count\")\nplt.title(\"Total price of items sold in each month\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation between \"Number of items sold\" and \"Total price of all items\" in each month","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.corrcoef(item_cnt_month.values, price_cnt_month.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Roughly 0.735","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_block = sales['date_block_num'].max() + 1\ntest_data['date_block_num'] = test_block\ntest_data = test_data.drop(columns=['ID'])\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create grid from all shops/items combinations from that monthÂ¶","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = []\nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols, dtype=np.int32)\ngrid = pd.concat([grid, test_data])\ngrid.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create features by grouping shops, items and month","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Groupby data to get shop-item-month aggregates\ngb = sales.groupby(index_cols, as_index=False)['item_cnt_day'].sum()\ngb = gb.rename(columns={'item_cnt_day': 'target'})\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n# Same as above but with shop-month aggregates\ngb = sales.groupby(['shop_id', 'date_block_num'], as_index=False)['item_cnt_day'].sum()\ngb = gb.rename(columns={'item_cnt_day': 'target_shop'})\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n# Same as above but with item-month aggregates\ngb = sales.groupby(['item_id', 'date_block_num'], as_index=False)['item_cnt_day'].sum()\ngb = gb.rename(columns={'item_cnt_day': 'target_item'})\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\n# Downcast dtypes from 64 to 32 bit to save memory\nall_data = downcast_dtypes(all_data)\ndel grid, gb \ngc.collect();\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"features using previous months data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of columns that we will use to create lags\ncols_to_rename = list(all_data.columns.difference(index_cols))\nshift_range = [1, 2, 3, 4, 5, 12]\n\nfor month_shift in tqdm_notebook(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n\n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n\n    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n    \ndel train_shift\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add \"item category id\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12]\n\n# List of all lagged features\nfit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]]\n# We will drop these at fitting stage\nto_drop_cols = ['target_item', 'target_shop', 'target', 'date_block_num']\nto_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num']\n\n# Category for each item\nitem_category_mapping = items[['item_id', 'item_category_id']].drop_duplicates()\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\nall_data = downcast_dtypes(all_data)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Final Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create train test split**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dates = all_data['date_block_num']\n\ndates_train  = dates[dates <  test_block]\ndates_test  = dates[dates == test_block]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = all_data.loc[dates <  test_block].drop(to_drop_cols, axis=1)\nX_test =  all_data.loc[dates == test_block].drop(to_drop_cols, axis=1)\n\ny_train = all_data.loc[dates <  test_block, 'target'].values\ny_test =  all_data.loc[dates == test_block, 'target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target range is 0 to 20","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target_range = [0, 20]\ntarget_range","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Models**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_train.values, y_train)\npred_lr = lr.predict(X_test.values).clip(*target_range)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'ID': sample_submission.ID, 'item_cnt_month': pred_lr})\nsubmission.to_csv('submission_linear_regression.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LightGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7,\n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }\n\nmodel = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\npred_lgb = model.predict(X_test).clip(*target_range)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'ID': sample_submission.ID, 'item_cnt_month': pred_lgb})\nsubmission.to_csv('submission_lgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nConcatenate test predictions\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_level2 = np.c_[pred_lr, pred_lgb]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_level2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Validation L2**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32, 33])]\n\n# That is how we get target for the 2nd level dataset\ny_train_level2 = y_train[dates_train.isin(dates_train_level2)]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time series data - Use 2nd level validation scheme accordingly","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# And here we create 2nd level feeature matrix, init it with zeros first\nX_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n\n# Now fill `X_train_level2` with metafeatures\nfor cur_block_num in [27, 28, 29, 30, 31, 32, 33]:\n    print(cur_block_num)\n    '''\n        1. Split `X_train` into parts\n           Remember, that corresponding dates are stored in `dates_train` \n        2. Fit linear regression \n        3. Fit LightGBM and put predictions          \n        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n           You can use `dates_train_level2` for it\n           Make sure the order of the meta-features is the same as in `X_test_level2`\n    '''      \n    X_train_cur = all_data.loc[dates <  cur_block_num].drop(to_drop_cols, axis=1)\n    X_test_cur =  all_data.loc[dates == cur_block_num].drop(to_drop_cols, axis=1)\n\n    y_train_cur = all_data.loc[dates <  cur_block_num, 'target'].values\n    y_test_cur =  all_data.loc[dates == cur_block_num, 'target'].values\n    \n    lr.fit(X_train_cur.values, y_train_cur)\n    pred_lr = lr.predict(X_test_cur.values)\n    X_train_level2[dates_train_level2 == cur_block_num, 0] = pred_lr.clip(*target_range)\n    \n    model = lgb.train(lgb_params, lgb.Dataset(X_train_cur, label=y_train_cur), 100)\n    pred_lgb = model.predict(X_test_cur)\n    X_train_level2[dates_train_level2 == cur_block_num, 1] = pred_lgb.clip(*target_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlation between prediction methods**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X_train_level2[:, 0], X_train_level2[:, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ensembling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas_to_try = np.linspace(0, 1, 1001)\n\nr2_scores = []\nfor alpha in alphas_to_try:\n    mix = alpha*X_train_level2[:,0] + (1-alpha)*X_train_level2[:,1]\n    r2_scores.append(r2_score(y_train_level2, mix))\n\nr2_scores = np.array(r2_scores)\n\n# YOUR CODE GOES HERE\nbest_alpha = alphas_to_try[r2_scores.argmax()]\nr2_train_simple_mix = r2_scores.max()\n\nprint('Best alpha: %f; Corresponding r2 score on train: %f' % (best_alpha, r2_train_simple_mix))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_simple_averaging = best_alpha*X_test_level2[:,0] + (1-best_alpha)*X_test_level2[:,1]\nsubmission = pd.DataFrame({'ID': sample_submission.ID, 'item_cnt_month': pred_simple_averaging})\nsubmission.to_csv('submission_simple_averaging.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Stacking**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train_level2, y_train_level2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_stacking = lr.predict(X_test_level2).clip(*target_range)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'ID': sample_submission.ID, 'item_cnt_month': pred_stacking})\nsubmission.to_csv('submission_stacking.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_stacking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}