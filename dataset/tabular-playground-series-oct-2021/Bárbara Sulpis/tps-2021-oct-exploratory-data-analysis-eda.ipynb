{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploratory Data Analisys for Tabular Playground Series (Oct 2021)","metadata":{}},{"cell_type":"code","source":"# =======================================================\n# TPS October 2021 - EDA\n# =======================================================\n# Name: BÃ¡rbara Sulpis\n# Date: 11-oct-2021\n# Description: I will analyze TPS data to have an idea of following steps...\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy.stats as st # statistical functions\nimport os\n\nfrom sklearn.model_selection import train_test_split\n\n#Lgbm\nimport lightgbm as lgb\n\n# roc\nimport sklearn.metrics as metrics   # Para la curva ROC\nimport matplotlib.pyplot as plt     # Para la curva ROC\n\n# for hystograms\nimport seaborn as sns\n\n\n# ---------------------------\n# Input data:\n# Go to file -> add or upload data -> \"Competition\" data tab and select the commpetition which you want to add the csv data data \"\n# files are available in the read-only \"../input/\" directory\n# ---------------------------\n\nlist =  os. getcwd()\nprint(list) # shoud be in \"kaggle\" directory\n\n# I left this commented if you want to check that the files are there\n# i = 0\n# for subdir, dirs, files in os.walk('./'):\n#     for file in files:\n#         print(file)\n#         i+= 1\n#         if i>20: \n#             break\n\n\ndata = pd.read_csv(\"../input/tabular-playground-series-oct-2021/train.csv\")        \nsubm = pd.read_csv(\"../input/tabular-playground-series-oct-2021/test.csv\")  \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-12T03:19:47.090312Z","iopub.execute_input":"2021-10-12T03:19:47.090651Z","iopub.status.idle":"2021-10-12T03:21:17.877898Z","shell.execute_reply.started":"2021-10-12T03:19:47.090558Z","shell.execute_reply":"2021-10-12T03:21:17.877113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Size of the dataset\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-11T19:11:16.358788Z","iopub.execute_input":"2021-10-11T19:11:16.359086Z","iopub.status.idle":"2021-10-11T19:11:16.370964Z","shell.execute_reply.started":"2021-10-11T19:11:16.359044Z","shell.execute_reply":"2021-10-11T19:11:16.370334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# With this setting we can see all rows of the dataset\npd.set_option(\"display.max_columns\", 300)\n# We have a look to the data\ndata","metadata":{"execution":{"iopub.status.busy":"2021-10-11T19:12:07.783199Z","iopub.execute_input":"2021-10-11T19:12:07.784183Z","iopub.status.idle":"2021-10-11T19:12:08.061841Z","shell.execute_reply.started":"2021-10-11T19:12:07.784134Z","shell.execute_reply":"2021-10-11T19:12:08.061044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Before working with the data, we reduce the use of memory, so we can improve performance\n# REFERENCE: https://www.kaggle.com/smiles28/tps10-optuna-xgb-catb-lgbm-stacking\n\n# What the function does is to deduce the data types and cast each column to its most performant type\n\ndef reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n    \n            # test if column can be converted to an integer\n            asint = props[col].astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return props\n","metadata":{"execution":{"iopub.status.busy":"2021-10-12T03:21:17.880154Z","iopub.execute_input":"2021-10-12T03:21:17.880406Z","iopub.status.idle":"2021-10-12T03:21:17.893727Z","shell.execute_reply.started":"2021-10-12T03:21:17.880379Z","shell.execute_reply":"2021-10-12T03:21:17.892944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = reduce_mem_usage(data)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T03:21:17.895235Z","iopub.execute_input":"2021-10-12T03:21:17.8957Z","iopub.status.idle":"2021-10-12T03:23:17.070402Z","shell.execute_reply.started":"2021-10-12T03:21:17.895669Z","shell.execute_reply":"2021-10-12T03:23:17.069592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm = reduce_mem_usage(subm)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T03:23:17.072325Z","iopub.execute_input":"2021-10-12T03:23:17.072542Z","iopub.status.idle":"2021-10-12T03:24:16.857102Z","shell.execute_reply.started":"2021-10-12T03:23:17.072518Z","shell.execute_reply":"2021-10-12T03:24:16.855852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------------------------------------------------\n#   Search for MISSING values\n# ------------------------------------------------------------\n# First we make a dataframe with the number of not-null values for each column\ncount = pd.DataFrame(data.count(), columns=['count'])\n# Then we get the fields that has a number smaller than 1M (the number of rows in train set)\ncount.query(\"count < 1000000\")\n\n# As we can see there are not null values in the dataset. \n# That's good because we don't have to spend time working with missing values","metadata":{"execution":{"iopub.status.busy":"2021-10-12T01:51:35.832805Z","iopub.execute_input":"2021-10-12T01:51:35.833754Z","iopub.status.idle":"2021-10-12T01:51:36.341205Z","shell.execute_reply.started":"2021-10-12T01:51:35.833712Z","shell.execute_reply":"2021-10-12T01:51:36.340399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can make the same check for the submission dataset (\"test.csv dataset\")\ncount = pd.DataFrame(subm.count(), columns=['count'])\ncount.query(\"count < 500000\")\n\n# As expected, there are not null values in test dataset neither.","metadata":{"execution":{"iopub.status.busy":"2021-10-12T01:57:49.110775Z","iopub.execute_input":"2021-10-12T01:57:49.111096Z","iopub.status.idle":"2021-10-12T01:57:49.403554Z","shell.execute_reply.started":"2021-10-12T01:57:49.111059Z","shell.execute_reply":"2021-10-12T01:57:49.40264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------------------------------------------------\n#   Variable CORRELATION\n# ------------------------------------------------------------\n# Correlation matrix\n# --------------------\n# We make a correlation matrix to check if there are relations between the different fields.\ncorrmat = data.corr()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T19:18:43.427198Z","iopub.execute_input":"2021-10-11T19:18:43.427582Z","iopub.status.idle":"2021-10-11T19:22:32.126554Z","shell.execute_reply.started":"2021-10-11T19:18:43.427539Z","shell.execute_reply":"2021-10-11T19:22:32.125406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's draw the corrmat\nf, ax = plt.subplots(figsize =(40, 40))\nsns.heatmap(corrmat, ax = ax, cmap =\"YlGnBu\", linewidths = 0.1)\n# Explanation of the graph: The blue diagonal \"line\" represents a 100% of correlation between each feature and itself\n#   the other points, as the right vertical correlation rule indicates, seems not to have correlation with other features except of itself. \n# f22 seems to have no correlation at all with target, that could leat to the feature removal","metadata":{"execution":{"iopub.status.busy":"2021-10-11T19:22:32.128794Z","iopub.execute_input":"2021-10-11T19:22:32.129252Z","iopub.status.idle":"2021-10-11T19:22:37.88853Z","shell.execute_reply.started":"2021-10-11T19:22:32.12921Z","shell.execute_reply":"2021-10-11T19:22:37.88744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of the target:\ndata.groupby('target').count()\n# 499515\n# 500485\n# The data is quite perfectly balanced","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------------------------------------------------\n#   Variable DISTRIBUTIONS\n# ------------------------------------------------------------\n# I will draw the hystograms for all variables\ndata.hist(grid = False, figsize=(25,80), layout=(29, 10), bins=50)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T19:46:07.488803Z","iopub.execute_input":"2021-10-11T19:46:07.489646Z","iopub.status.idle":"2021-10-11T19:47:24.658169Z","shell.execute_reply.started":"2021-10-11T19:46:07.489607Z","shell.execute_reply":"2021-10-11T19:47:24.657097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------------------------------------------------------------------\n#  CARDINALITY OF VARIABLES\n# ------------------------------------------------------------------------------\n# After watching the output we can appreciate that there are plenty of features that seems to be binaries\n# So, let's see theyr cardinality\npd.set_option(\"display.max_rows\", 300)\n\ndata.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-10-12T01:36:47.33003Z","iopub.execute_input":"2021-10-12T01:36:47.330683Z","iopub.status.idle":"2021-10-12T01:36:53.57331Z","shell.execute_reply.started":"2021-10-12T01:36:47.330636Z","shell.execute_reply":"2021-10-12T01:36:53.572382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What we can see below is that there are 45 binary features . \n# This is useful for example in case we use a LightGBM algorithm, we can specify the categorical features in order to improve results\n\n# Example usage (LightGBM): \n# categorical = ['f22', 'f43', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248',\n#                'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257',\n#                'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266',\n#                'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275',\n#                'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284']\n\n# fit_params={... \n#             'categorical_feature': categorical\n#             ...\n#            }  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can find handy this other histogram plot, that makes two plots overlapped\n# Superposition of the two graphs: target==1 and target==0\n# We will only plot the first 10 features\n\n# REFERENCE: https://stackoverflow.com/questions/37911731/seaborn-histogram-with-4-panels-2-x-2-in-python\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndata_hist = pd.melt(data[['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'target']], \"target\", var_name=\"target distributions\")\ng = sns.FacetGrid(data_hist, hue=\"target\", col=\"target distributions\", col_wrap=5, sharex=False, sharey=False)\ng.map(plt.hist, \"value\", bins=20, alpha=.4)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-12T01:54:29.620846Z","iopub.execute_input":"2021-10-12T01:54:29.621183Z","iopub.status.idle":"2021-10-12T01:54:53.912886Z","shell.execute_reply.started":"2021-10-12T01:54:29.621151Z","shell.execute_reply":"2021-10-12T01:54:53.911954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------------------------------------------------\n#  Checking SKEWNESS for continuous data\n# ------------------------------------------------------------\n# Last of all, the following code is to calculate the skewed data. In this example left skewed data.\n# This could be used to correct skewness with log or exponential transformations \n\ndata_skewed = pd.concat([pd.DataFrame(data.columns), pd.DataFrame(st.skew(data))], axis=1)\ndata_skewed.columns = ['names', 'skewness']\n# I only get fields that has a skewness bigger than 3\nskewed = data_skewed.query('skewness > 3')['names']","metadata":{"execution":{"iopub.status.busy":"2021-10-12T03:25:15.127657Z","iopub.execute_input":"2021-10-12T03:25:15.128675Z","iopub.status.idle":"2021-10-12T03:25:29.994683Z","shell.execute_reply.started":"2021-10-12T03:25:15.128631Z","shell.execute_reply":"2021-10-12T03:25:29.993752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skewed","metadata":{"execution":{"iopub.status.busy":"2021-10-12T03:25:40.6318Z","iopub.execute_input":"2021-10-12T03:25:40.63224Z","iopub.status.idle":"2021-10-12T03:25:40.643887Z","shell.execute_reply.started":"2021-10-12T03:25:40.632191Z","shell.execute_reply":"2021-10-12T03:25:40.642784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------------------------------------------------\n#  Best performing algorithms\n# ------------------------------------------------------------\n# As part of the EDA I can add the output of the LazyPredict used in other of my notebooks.\n# REFERENCE: https://www.kaggle.com/brbarasulpis/tps-2021-oct-automl-lazypredict-lazyclassifier\n\n# In the ouptut we can see that the best option got is LightGBM Classifier.\n# If we perform an ensemble algorithm, we could use for example LightGBM+BernoulliNB+AdaBoostClassifier.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](http://storage.googleapis.com/kagglesdsdata/datasets/1642245/2696798/LazyClassifier_Output.JPG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20211011%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211011T220846Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=36028cc47ab8ddc360a12680533209103048444e19d16eb41c8996032e865b1f9c28b5cac8432bcc9ca25c2333a710e25c651844ea159e0790b4f7bd780bc9d28eea421e2a9c3850c83ba20c921048dd25845f904ae90603e0bc226eceab6b9b141d66ea9d588ec88d64ceb5f3e25064e147bdb930fea38ba3841c3839d76a0a7f539b01fb3ab49ccfa49e1b007e64139500c00586fd494d98a4d0cb451869b5193e36262075617bfbbcf3a922a3d593542c911a259ef46c51ae45eaa66b71c1620131fbbb6ac544ade9204ea65e2fabc352c244a42a4336b721e2ecf94d6b70993ebd1e25c99603f64aeba92384c1ce88dac421a2a1a185e618e0aa8666b484)","metadata":{}},{"cell_type":"markdown","source":"# CONCLUSIONS\nAfter this exploratory data analisys we now know that:\n* There are no missing values in the dataset\n* The target is balanced (nearly half values in 1 and half in 0)\n* There is no correlations between the variables\n* There are 45 categorical features\n* Many continuous features are left skewed\n* Best algorithm suggested for the problem is LightGBM","metadata":{}}]}