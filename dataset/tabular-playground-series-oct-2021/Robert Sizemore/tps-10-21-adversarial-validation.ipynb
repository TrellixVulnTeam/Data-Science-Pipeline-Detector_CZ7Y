{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Adversarial Validation\n\nIn this notebook, we use adversarial validation to test the similarity of the training and test data. I made a [post](https://www.kaggle.com/c/tabular-playground-series-oct-2021/discussion/276712) about this on the discussion boards and figured I should make a notebook for others to inspect.\n\nThe idea behind adversarial validation is that if there are differences in the training and test data distributions an algorithm like XGBoost should be able to find these differences and use them to distinguish the two sets. So we create a classification problem where we predict whether a sampling of the data comes from the training or test sets. Ideally, we hope to see ~.5 AUC which would mean that our algorithm couldn't find meaningful distinctions between the test and training data.","metadata":{}},{"cell_type":"code","source":"# Global variables for testing changes to this notebook quickly\nRANDOM_SEED = 0\nNUM_FOLDS = 3\nSAMPLES = 100000","metadata":{"execution":{"iopub.status.busy":"2022-01-08T20:56:30.385981Z","iopub.execute_input":"2022-01-08T20:56:30.386314Z","iopub.status.idle":"2022-01-08T20:56:30.468283Z","shell.execute_reply.started":"2022-01-08T20:56:30.386223Z","shell.execute_reply":"2022-01-08T20:56:30.467571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport os\nimport gc\n\n# Hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\n# Plotting\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Model and Evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport shap","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T20:56:30.469995Z","iopub.execute_input":"2022-01-08T20:56:30.470487Z","iopub.status.idle":"2022-01-08T20:56:33.396961Z","shell.execute_reply.started":"2022-01-08T20:56:30.47045Z","shell.execute_reply":"2022-01-08T20:56:33.3962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data\n\nHelper function which samples the training and test sets in preparation for adversarial validation.","metadata":{}},{"cell_type":"code","source":"def get_data(n_samples = 100000):\n    start = time.time()\n    \n    # Load and sample training data\n    train = pd.read_csv('../input/tabular-playground-series-oct-2021/train.csv')\n    train.drop(['id', 'target'], axis = 'columns', inplace = True)\n    train = train.sample(n = n_samples, random_state = RANDOM_SEED)\n    gc.collect()\n\n    # Load and sample test data\n    test = pd.read_csv('../input/tabular-playground-series-oct-2021/test.csv')\n    test.drop('id', axis = 'columns', inplace = True)\n    test = test.sample(n = n_samples, random_state = RANDOM_SEED)\n    gc.collect()\n    \n    data = train.append(test).reset_index(drop = True)\n    data['test'] = [0] * len(train) + [1] * len(test)\n    data.reset_index(drop = True, inplace = True)\n    \n    for col, dtype in data.dtypes.iteritems():\n        if dtype.name.startswith('int'):\n            data[col] = pd.to_numeric(data[col], downcast ='integer')\n        elif dtype.name.startswith('float'):\n            data[col] = pd.to_numeric(data[col], downcast ='float')\n    \n    train, valid = train_test_split(data, stratify = data['test'])\n    \n    end = time.time()\n    print(f\"Data retrieved in {round(end-start,2)}s.\")\n    return train, valid","metadata":{"execution":{"iopub.status.busy":"2022-01-08T20:56:33.398247Z","iopub.execute_input":"2022-01-08T20:56:33.398745Z","iopub.status.idle":"2022-01-08T20:56:33.410258Z","shell.execute_reply.started":"2022-01-08T20:56:33.398705Z","shell.execute_reply":"2022-01-08T20:56:33.409544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adversarial Validation\n\nWe use [XGBoost](https://xgboost.readthedocs.io/en/latest/python/index.html)  and a random sampling of 100,000 rows from both the training and test data (200k total):","metadata":{}},{"cell_type":"code","source":"# Load data\ntrain, valid = get_data(n_samples = SAMPLES)\nfeatures = [x for x in train.columns if x != 'test']\n\n# Define and train model\nstart = time.time()\nmodel = XGBClassifier(\n    random_state = RANDOM_SEED,\n    tree_method='gpu_hist',\n    gpu_id=0,\n    predictor=\"gpu_predictor\",\n)\nmodel.fit(\n    train[features], train['test'], \n    eval_metric = 'auc',\n    verbose = False\n)\n\n# Get AUC from validation set\nvalid_auc = roc_auc_score(\n    y_true = valid['test'], \n    y_score = model.predict_proba(valid[features])[:,1],\n)\nend = time.time()\n\nprint(f'Validation AUC: {round(valid_auc, 6)} in {round(end-start, 2)}s.')","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-08T20:56:33.41238Z","iopub.execute_input":"2022-01-08T20:56:33.412885Z","iopub.status.idle":"2022-01-08T20:57:44.756228Z","shell.execute_reply.started":"2022-01-08T20:56:33.412848Z","shell.execute_reply":"2022-01-08T20:57:44.755444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see a validation AUC of `> .95` which seems to suggest significant differences in the training and test distributions.","metadata":{}},{"cell_type":"markdown","source":"# XGBoost Feature Importance\n\nIt may be useful to examine these features which XGBoost found useful for distinguishing the training and test data. We use consider the following importance types:\n\n* `weight` is the number of times a feature appears in a tree\n* `gain` is the average gain of splits which use the feature\n* `cover` is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split\n\nAbove descriptions taken from the XGBoost Plotting API [documentation](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.plotting).","metadata":{}},{"cell_type":"markdown","source":"## 1. Importance Type: Weight","metadata":{}},{"cell_type":"code","source":"# Top 10 features by importance type \"Weight\"\nfig, ax = plt.subplots(figsize = (9,6))\nxgb.plot_importance(\n    model,\n    ax = ax,\n    importance_type = \"weight\",\n    max_num_features = 10,\n)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T20:57:44.757435Z","iopub.execute_input":"2022-01-08T20:57:44.757702Z","iopub.status.idle":"2022-01-08T20:57:45.065413Z","shell.execute_reply.started":"2022-01-08T20:57:44.757665Z","shell.execute_reply":"2022-01-08T20:57:45.064704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Importance Type - Gain","metadata":{}},{"cell_type":"code","source":"# Top 10 features by importance type \"Gain\"\nfig, ax = plt.subplots(figsize = (9,6))\nxgb.plot_importance(\n    model,\n    ax = ax,\n    importance_type = \"gain\",\n    max_num_features = 10,\n    show_values = False, # looks bad\n)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T20:57:45.066495Z","iopub.execute_input":"2022-01-08T20:57:45.066753Z","iopub.status.idle":"2022-01-08T20:57:45.618346Z","shell.execute_reply.started":"2022-01-08T20:57:45.066718Z","shell.execute_reply":"2022-01-08T20:57:45.617617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Importance Type - Cover","metadata":{}},{"cell_type":"code","source":"# Top 10 features by importance type \"Cover\"\nfig, ax = plt.subplots(figsize = (9,6))\nxgb.plot_importance(\n    model,\n    ax = ax,\n    importance_type = \"cover\",\n    max_num_features = 10,\n    show_values = False, # looks bad\n)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T20:57:45.622201Z","iopub.execute_input":"2022-01-08T20:57:45.624384Z","iopub.status.idle":"2022-01-08T20:57:46.047441Z","shell.execute_reply.started":"2022-01-08T20:57:45.624343Z","shell.execute_reply":"2022-01-08T20:57:46.046688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that different features are considered important based on which importance type we consider.","metadata":{}},{"cell_type":"markdown","source":"# XGBoost SHAP Values\n\nFinally, we can use our fitted model to calculate feature importances using SHAP values:","metadata":{}},{"cell_type":"code","source":"# Top 10 features by SHAP values\nbooster_xgb = model.get_booster()\nshap_values_xgb = booster_xgb.predict(\n    xgb.DMatrix(train[features], train['test']), \n    pred_contribs=True\n)\n\nshap.summary_plot(\n    shap_values_xgb[:, :-1], train[features], \n    feature_names=features, \n    plot_type=\"bar\",\n    max_display = 10,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T20:57:46.048773Z","iopub.execute_input":"2022-01-08T20:57:46.049228Z","iopub.status.idle":"2022-01-08T20:57:48.155827Z","shell.execute_reply.started":"2022-01-08T20:57:46.049186Z","shell.execute_reply":"2022-01-08T20:57:48.155028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see there are several features that are consistently considered important for distinguishing the test set from the training set. It may be worth examining these features.","metadata":{}}]}