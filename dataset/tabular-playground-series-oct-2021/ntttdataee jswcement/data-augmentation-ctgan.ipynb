{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Credit to the original notebook: https://www.kaggle.com/gogo827jz/data-augmentation-ctgan","metadata":{}},{"cell_type":"code","source":"import warnings; warnings.filterwarnings(\"ignore\")\n\nimport os\nimport gc\nimport sys\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom time import time\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\n\ndir = '../input/tabular-playground-series-oct-2021'","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","executionInfo":{"elapsed":37319,"status":"ok","timestamp":1602683205255,"user":{"displayName":"Yirun Zhang","photoUrl":"","userId":"05891579764514658952"},"user_tz":-60},"id":"kw1VW6DCvgSq","papermill":{"duration":0.787435,"end_time":"2020-10-17T18:09:32.480149","exception":false,"start_time":"2020-10-17T18:09:31.692714","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-15T06:43:34.974433Z","iopub.execute_input":"2021-10-15T06:43:34.974763Z","iopub.status.idle":"2021-10-15T06:43:35.770709Z","shell.execute_reply.started":"2021-10-15T06:43:34.97468Z","shell.execute_reply":"2021-10-15T06:43:35.769787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{"id":"dSVuPpi2vgSv","papermill":{"duration":0.01041,"end_time":"2020-10-17T18:09:32.502627","exception":false,"start_time":"2020-10-17T18:09:32.492217","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train = pd.read_csv(dir + '/train.csv')\ntest = pd.read_csv(dir + '/test.csv')\n\nss = pd.read_csv(dir + '/sample_submission.csv')\ncols = [c for c in ss.columns.values]","metadata":{"executionInfo":{"elapsed":46960,"status":"ok","timestamp":1602683214909,"user":{"displayName":"Yirun Zhang","photoUrl":"","userId":"05891579764514658952"},"user_tz":-60},"id":"CEGqdrS47CBz","papermill":{"duration":6.726335,"end_time":"2020-10-17T18:09:39.274945","exception":false,"start_time":"2020-10-17T18:09:32.54861","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-15T06:43:35.772279Z","iopub.execute_input":"2021-10-15T06:43:35.772552Z","iopub.status.idle":"2021-10-15T06:45:02.425193Z","shell.execute_reply.started":"2021-10-15T06:43:35.772517Z","shell.execute_reply":"2021-10-15T06:45:02.424467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = train.sample(1500) # dont forget to change this! this is for memory consumptopn issues..\ndata.head()","metadata":{"executionInfo":{"elapsed":46947,"status":"ok","timestamp":1602683214909,"user":{"displayName":"Yirun Zhang","photoUrl":"","userId":"05891579764514658952"},"user_tz":-60},"id":"3R4zvq0B7iAp","outputId":"65f34cf4-a0dc-4a8f-eb8a-354a8b4bba22","papermill":{"duration":0.115913,"end_time":"2020-10-17T18:09:39.401898","exception":false,"start_time":"2020-10-17T18:09:39.285985","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-15T06:45:02.426515Z","iopub.execute_input":"2021-10-15T06:45:02.426786Z","iopub.status.idle":"2021-10-15T06:45:02.462103Z","shell.execute_reply.started":"2021-10-15T06:45:02.42675Z","shell.execute_reply":"2021-10-15T06:45:02.46125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discrete_cols = []\nprint(len(discrete_cols))","metadata":{"executionInfo":{"elapsed":46934,"status":"ok","timestamp":1602683214910,"user":{"displayName":"Yirun Zhang","photoUrl":"","userId":"05891579764514658952"},"user_tz":-60},"id":"kJ7tCTMw73uq","outputId":"3ae5774c-02fa-4f4b-efda-454bf1985124","papermill":{"duration":0.021513,"end_time":"2020-10-17T18:09:39.436158","exception":false,"start_time":"2020-10-17T18:09:39.414645","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-15T06:45:02.464346Z","iopub.execute_input":"2021-10-15T06:45:02.464645Z","iopub.status.idle":"2021-10-15T06:45:02.470873Z","shell.execute_reply.started":"2021-10-15T06:45:02.464593Z","shell.execute_reply":"2021-10-15T06:45:02.46976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CTGAN","metadata":{"id":"HfAdEYwj4bC2","papermill":{"duration":0.012098,"end_time":"2020-10-17T18:09:39.491559","exception":false,"start_time":"2020-10-17T18:09:39.479461","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nfrom torch import optim\nfrom torch.nn import functional\nfrom torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.mixture import BayesianGaussianMixture\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.utils._testing import ignore_warnings\n\nclass ConditionalGenerator(object):\n    def __init__(self, data, output_info, log_frequency):\n        self.model = []\n\n        start = 0\n        skip = False\n        max_interval = 0\n        counter = 0\n        for item in output_info:\n            if item[1] == 'tanh':\n                start += item[0]\n                skip = True\n                continue\n\n            elif item[1] == 'softmax':\n                if skip:\n                    skip = False\n                    start += item[0]\n                    continue\n\n                end = start + item[0]\n                max_interval = max(max_interval, end - start)\n                counter += 1\n                self.model.append(np.argmax(data[:, start:end], axis=-1))\n                start = end\n\n            else:\n                assert 0\n\n        assert start == data.shape[1]\n\n        self.interval = []\n        self.n_col = 0\n        self.n_opt = 0\n        skip = False\n        start = 0\n        self.p = np.zeros((counter, max_interval))\n        for item in output_info:\n            if item[1] == 'tanh':\n                skip = True\n                start += item[0]\n                continue\n            elif item[1] == 'softmax':\n                if skip:\n                    start += item[0]\n                    skip = False\n                    continue\n                end = start + item[0]\n                tmp = np.sum(data[:, start:end], axis=0)\n                if log_frequency:\n                    tmp = np.log(tmp + 1)\n                tmp = tmp / np.sum(tmp)\n                self.p[self.n_col, :item[0]] = tmp\n                self.interval.append((self.n_opt, item[0]))\n                self.n_opt += item[0]\n                self.n_col += 1\n                start = end\n            else:\n                assert 0\n\n        self.interval = np.asarray(self.interval)\n\n    def random_choice_prob_index(self, idx):\n        a = self.p[idx]\n        r = np.expand_dims(np.random.rand(a.shape[0]), axis=1)\n        return (a.cumsum(axis=1) > r).argmax(axis=1)\n\n    def sample(self, batch):\n        if self.n_col == 0:\n            return None\n\n        batch = batch\n        idx = np.random.choice(np.arange(self.n_col), batch)\n\n        vec1 = np.zeros((batch, self.n_opt), dtype='float32')\n        mask1 = np.zeros((batch, self.n_col), dtype='float32')\n        mask1[np.arange(batch), idx] = 1\n        opt1prime = self.random_choice_prob_index(idx)\n        opt1 = self.interval[idx, 0] + opt1prime\n        vec1[np.arange(batch), opt1] = 1\n\n        return vec1, mask1, idx, opt1prime\n\n    def sample_zero(self, batch):\n        if self.n_col == 0:\n            return None\n\n        vec = np.zeros((batch, self.n_opt), dtype='float32')\n        idx = np.random.choice(np.arange(self.n_col), batch)\n        for i in range(batch):\n            col = idx[i]\n            pick = int(np.random.choice(self.model[col]))\n            vec[i, pick + self.interval[col, 0]] = 1\n\n        return vec\n\nclass Discriminator(Module):\n\n    def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n\n        alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n        alpha = alpha.repeat(1, pac, real_data.size(1))\n        alpha = alpha.view(-1, real_data.size(1))\n\n        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n\n        disc_interpolates = self(interpolates)\n\n        gradients = torch.autograd.grad(\n            outputs=disc_interpolates, inputs=interpolates,\n            grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n            create_graph=True, retain_graph=True, only_inputs=True\n        )[0]\n\n        gradient_penalty = ((\n            gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1\n        ) ** 2).mean() * lambda_\n\n        return gradient_penalty\n\n    def __init__(self, input_dim, dis_dims, pack=10):\n        super(Discriminator, self).__init__()\n        dim = input_dim * pack\n        self.pack = pack\n        self.packdim = dim\n        seq = []\n        for item in list(dis_dims):\n            seq += [Linear(dim, item), LeakyReLU(0.2), Dropout(0.5)]\n            dim = item\n\n        seq += [Linear(dim, 1)]\n        self.seq = Sequential(*seq)\n\n    def forward(self, input):\n        assert input.size()[0] % self.pack == 0\n        return self.seq(input.view(-1, self.packdim))\n\n\nclass Residual(Module):\n    def __init__(self, i, o):\n        super(Residual, self).__init__()\n        self.fc = Linear(i, o)\n        self.bn = BatchNorm1d(o)\n        self.relu = ReLU()\n\n    def forward(self, input):\n        out = self.fc(input)\n        out = self.bn(out)\n        out = self.relu(out)\n        return torch.cat([out, input], dim=1)\n\n\nclass Generator(Module):\n    def __init__(self, embedding_dim, gen_dims, data_dim):\n        super(Generator, self).__init__()\n        dim = embedding_dim\n        seq = []\n        for item in list(gen_dims):\n            seq += [Residual(dim, item)]\n            dim += item\n        seq.append(Linear(dim, data_dim))\n        self.seq = Sequential(*seq)\n\n    def forward(self, input):\n        data = self.seq(input)\n        return data\n\nclass Sampler(object):\n    \"\"\"docstring for Sampler.\"\"\"\n\n    def __init__(self, data, output_info):\n        super(Sampler, self).__init__()\n        self.data = data\n        self.model = []\n        self.n = len(data)\n\n        st = 0\n        skip = False\n        for item in output_info:\n            if item[1] == 'tanh':\n                st += item[0]\n                skip = True\n            elif item[1] == 'softmax':\n                if skip:\n                    skip = False\n                    st += item[0]\n                    continue\n\n                ed = st + item[0]\n                tmp = []\n                for j in range(item[0]):\n                    tmp.append(np.nonzero(data[:, st + j])[0])\n\n                self.model.append(tmp)\n                st = ed\n            else:\n                assert 0\n\n        assert st == data.shape[1]\n\n    def sample(self, n, col, opt):\n        if col is None:\n            idx = np.random.choice(np.arange(self.n), n)\n            return self.data[idx]\n\n        idx = []\n        for c, o in zip(col, opt):\n            idx.append(np.random.choice(self.model[c][o]))\n\n        return self.data[idx]\n\nclass DataTransformer(object):\n    \"\"\"Data Transformer.\n    Model continuous columns with a BayesianGMM and normalized to a scalar\n    [0, 1] and a vector.\n    Discrete columns are encoded using a scikit-learn OneHotEncoder.\n    Args:\n        n_cluster (int):\n            Number of modes.\n        epsilon (float):\n            Epsilon value.\n    \"\"\"\n\n    def __init__(self, n_clusters=10, epsilon=0.005):\n        self.n_clusters = n_clusters\n        self.epsilon = epsilon\n\n    @ignore_warnings(category=ConvergenceWarning)\n    def _fit_continuous(self, column, data):\n        gm = BayesianGaussianMixture(\n            self.n_clusters,\n            weight_concentration_prior_type='dirichlet_process',\n            weight_concentration_prior=0.001,\n            n_init=1\n        )\n        gm.fit(data)\n        components = gm.weights_ > self.epsilon\n        num_components = components.sum()\n\n        return {\n            'name': column,\n            'model': gm,\n            'components': components,\n            'output_info': [(1, 'tanh'), (num_components, 'softmax')],\n            'output_dimensions': 1 + num_components,\n        }\n\n    def _fit_discrete(self, column, data):\n        ohe = OneHotEncoder(sparse=False)\n        ohe.fit(data)\n        categories = len(ohe.categories_[0])\n\n        return {\n            'name': column,\n            'encoder': ohe,\n            'output_info': [(categories, 'softmax')],\n            'output_dimensions': categories\n        }\n\n    def fit(self, data, discrete_columns=tuple()):\n        self.output_info = []\n        self.output_dimensions = 0\n\n        if not isinstance(data, pd.DataFrame):\n            self.dataframe = False\n            data = pd.DataFrame(data)\n        else:\n            self.dataframe = True\n\n        self.meta = []\n        for column in data.columns:\n            column_data = data[[column]].values\n            if column in discrete_columns:\n                meta = self._fit_discrete(column, column_data)\n            else:\n                meta = self._fit_continuous(column, column_data)\n\n            self.output_info += meta['output_info']\n            self.output_dimensions += meta['output_dimensions']\n            self.meta.append(meta)\n\n    def _transform_continuous(self, column_meta, data):\n        components = column_meta['components']\n        model = column_meta['model']\n\n        means = model.means_.reshape((1, self.n_clusters))\n        stds = np.sqrt(model.covariances_).reshape((1, self.n_clusters))\n        features = (data - means) / (4 * stds)\n\n        probs = model.predict_proba(data)\n\n        n_opts = components.sum()\n        features = features[:, components]\n        probs = probs[:, components]\n\n        opt_sel = np.zeros(len(data), dtype='int')\n        for i in range(len(data)):\n            pp = probs[i] + 1e-6\n            pp = pp / pp.sum()\n            opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n\n        idx = np.arange((len(features)))\n        features = features[idx, opt_sel].reshape([-1, 1])\n        features = np.clip(features, -.99, .99)\n\n        probs_onehot = np.zeros_like(probs)\n        probs_onehot[np.arange(len(probs)), opt_sel] = 1\n        return [features, probs_onehot]\n\n    def _transform_discrete(self, column_meta, data):\n        encoder = column_meta['encoder']\n        return encoder.transform(data)\n\n    def transform(self, data):\n        if not isinstance(data, pd.DataFrame):\n            data = pd.DataFrame(data)\n\n        values = []\n        for meta in self.meta:\n            column_data = data[[meta['name']]].values\n            if 'model' in meta:\n                values += self._transform_continuous(meta, column_data)\n            else:\n                values.append(self._transform_discrete(meta, column_data))\n\n        return np.concatenate(values, axis=1).astype(float)\n\n    def _inverse_transform_continuous(self, meta, data, sigma):\n        model = meta['model']\n        components = meta['components']\n\n        u = data[:, 0]\n        v = data[:, 1:]\n\n        if sigma is not None:\n            u = np.random.normal(u, sigma)\n\n        u = np.clip(u, -1, 1)\n        v_t = np.ones((len(data), self.n_clusters)) * -100\n        v_t[:, components] = v\n        v = v_t\n        means = model.means_.reshape([-1])\n        stds = np.sqrt(model.covariances_).reshape([-1])\n        p_argmax = np.argmax(v, axis=1)\n        std_t = stds[p_argmax]\n        mean_t = means[p_argmax]\n        column = u * 4 * std_t + mean_t\n\n        return column\n\n    def _inverse_transform_discrete(self, meta, data):\n        encoder = meta['encoder']\n        return encoder.inverse_transform(data)\n\n    def inverse_transform(self, data, sigmas):\n        start = 0\n        output = []\n        column_names = []\n        for meta in self.meta:\n            dimensions = meta['output_dimensions']\n            columns_data = data[:, start:start + dimensions]\n\n            if 'model' in meta:\n                sigma = sigmas[start] if sigmas else None\n                inverted = self._inverse_transform_continuous(meta, columns_data, sigma)\n            else:\n                inverted = self._inverse_transform_discrete(meta, columns_data)\n\n            output.append(inverted)\n            column_names.append(meta['name'])\n            start += dimensions\n\n        output = np.column_stack(output)\n        if self.dataframe:\n            output = pd.DataFrame(output, columns=column_names)\n\n        return output\n\nclass CTGANSynthesizer(object):\n    \"\"\"Conditional Table GAN Synthesizer.\n    This is the core class of the CTGAN project, where the different components\n    are orchestrated together.\n    For more details about the process, please check the [Modeling Tabular data using\n    Conditional GAN](https://arxiv.org/abs/1907.00503) paper.\n    Args:\n        embedding_dim (int):\n            Size of the random sample passed to the Generator. Defaults to 128.\n        gen_dim (tuple or list of ints):\n            Size of the output samples for each one of the Residuals. A Residual Layer\n            will be created for each one of the values provided. Defaults to (256, 256).\n        dis_dim (tuple or list of ints):\n            Size of the output samples for each one of the Discriminator Layers. A Linear Layer\n            will be created for each one of the values provided. Defaults to (256, 256).\n        l2scale (float):\n            Wheight Decay for the Adam Optimizer. Defaults to 1e-6.\n        batch_size (int):\n            Number of data samples to process in each step.\n    \"\"\"\n\n    def __init__(self, embedding_dim=128, gen_dim=(256, 256), dis_dim=(256, 256),\n                 l2scale=1e-6, batch_size=500):\n\n        self.embedding_dim = embedding_dim\n        self.gen_dim = gen_dim\n        self.dis_dim = dis_dim\n\n        self.l2scale = l2scale\n        self.batch_size = batch_size\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.trained_epoches = 0\n\n    def _apply_activate(self, data):\n        data_t = []\n        st = 0\n        for item in self.transformer.output_info:\n            if item[1] == 'tanh':\n                ed = st + item[0]\n                data_t.append(torch.tanh(data[:, st:ed]))\n                st = ed\n            elif item[1] == 'softmax':\n                ed = st + item[0]\n                data_t.append(functional.gumbel_softmax(data[:, st:ed], tau=0.2))\n                st = ed\n            else:\n                assert 0\n\n        return torch.cat(data_t, dim=1)\n\n    def _cond_loss(self, data, c, m):\n        loss = []\n        st = 0\n        st_c = 0\n        skip = False\n        for item in self.transformer.output_info:\n            if item[1] == 'tanh':\n                st += item[0]\n                skip = True\n\n            elif item[1] == 'softmax':\n                if skip:\n                    skip = False\n                    st += item[0]\n                    continue\n\n                ed = st + item[0]\n                ed_c = st_c + item[0]\n                tmp = functional.cross_entropy(\n                    data[:, st:ed],\n                    torch.argmax(c[:, st_c:ed_c], dim=1),\n                    reduction='none'\n                )\n                loss.append(tmp)\n                st = ed\n                st_c = ed_c\n\n            else:\n                assert 0\n\n        loss = torch.stack(loss, dim=1)\n\n        return (loss * m).sum() / data.size()[0]\n\n    def fit(self, train_data, discrete_columns=tuple(), epochs=300, log_frequency=True):\n        \"\"\"Fit the CTGAN Synthesizer models to the training data.\n        Args:\n            train_data (numpy.ndarray or pandas.DataFrame):\n                Training Data. It must be a 2-dimensional numpy array or a\n                pandas.DataFrame.\n            discrete_columns (list-like):\n                List of discrete columns to be used to generate the Conditional\n                Vector. If ``train_data`` is a Numpy array, this list should\n                contain the integer indices of the columns. Otherwise, if it is\n                a ``pandas.DataFrame``, this list should contain the column names.\n            epochs (int):\n                Number of training epochs. Defaults to 300.\n            log_frequency (boolean):\n                Whether to use log frequency of categorical levels in conditional\n                sampling. Defaults to ``True``.\n        \"\"\"\n\n        if not hasattr(self, \"transformer\"):\n            self.transformer = DataTransformer()\n            self.transformer.fit(train_data, discrete_columns)\n        train_data = self.transformer.transform(train_data)\n\n        data_sampler = Sampler(train_data, self.transformer.output_info)\n\n        data_dim = self.transformer.output_dimensions\n\n        if not hasattr(self, \"cond_generator\"):\n            self.cond_generator = ConditionalGenerator(\n                train_data,\n                self.transformer.output_info,\n                log_frequency\n            )\n\n        if not hasattr(self, \"generator\"):\n            self.generator = Generator(\n                self.embedding_dim + self.cond_generator.n_opt,\n                self.gen_dim,\n                data_dim\n            ).to(self.device)\n\n        if not hasattr(self, \"discriminator\"):\n            self.discriminator = Discriminator(\n                data_dim + self.cond_generator.n_opt,\n                self.dis_dim\n            ).to(self.device)\n\n        if not hasattr(self, \"optimizerG\"):\n            self.optimizerG = optim.Adam(\n                self.generator.parameters(), lr=2e-4, betas=(0.5, 0.9),\n                weight_decay=self.l2scale\n            )\n\n        if not hasattr(self, \"optimizerD\"):\n            self.optimizerD = optim.Adam(\n                self.discriminator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n\n        assert self.batch_size % 2 == 0\n        mean = torch.zeros(self.batch_size, self.embedding_dim, device=self.device)\n        std = mean + 1\n\n        steps_per_epoch = max(len(train_data) // self.batch_size, 1)\n        for i in range(epochs):\n            self.trained_epoches += 1\n            for id_ in range(steps_per_epoch):\n                fakez = torch.normal(mean=mean, std=std)\n\n                condvec = self.cond_generator.sample(self.batch_size)\n                if condvec is None:\n                    c1, m1, col, opt = None, None, None, None\n                    real = data_sampler.sample(self.batch_size, col, opt)\n                else:\n                    c1, m1, col, opt = condvec\n                    c1 = torch.from_numpy(c1).to(self.device)\n                    m1 = torch.from_numpy(m1).to(self.device)\n                    fakez = torch.cat([fakez, c1], dim=1)\n\n                    perm = np.arange(self.batch_size)\n                    np.random.shuffle(perm)\n                    real = data_sampler.sample(self.batch_size, col[perm], opt[perm])\n                    c2 = c1[perm]\n\n                fake = self.generator(fakez)\n                fakeact = self._apply_activate(fake)\n\n                real = torch.from_numpy(real.astype('float32')).to(self.device)\n\n                if c1 is not None:\n                    fake_cat = torch.cat([fakeact, c1], dim=1)\n                    real_cat = torch.cat([real, c2], dim=1)\n                else:\n                    real_cat = real\n                    fake_cat = fake\n\n                y_fake = self.discriminator(fake_cat)\n                y_real = self.discriminator(real_cat)\n\n                pen = self.discriminator.calc_gradient_penalty(\n                    real_cat, fake_cat, self.device)\n                loss_d = -(torch.mean(y_real) - torch.mean(y_fake))\n\n                self.optimizerD.zero_grad()\n                pen.backward(retain_graph=True)\n                loss_d.backward()\n                self.optimizerD.step()\n\n                fakez = torch.normal(mean=mean, std=std)\n                condvec = self.cond_generator.sample(self.batch_size)\n\n                if condvec is None:\n                    c1, m1, col, opt = None, None, None, None\n                else:\n                    c1, m1, col, opt = condvec\n                    c1 = torch.from_numpy(c1).to(self.device)\n                    m1 = torch.from_numpy(m1).to(self.device)\n                    fakez = torch.cat([fakez, c1], dim=1)\n\n                fake = self.generator(fakez)\n                fakeact = self._apply_activate(fake)\n\n                if c1 is not None:\n                    y_fake = self.discriminator(torch.cat([fakeact, c1], dim=1))\n                else:\n                    y_fake = self.discriminator(fakeact)\n\n                if condvec is None:\n                    cross_entropy = 0\n                else:\n                    cross_entropy = self._cond_loss(fake, c1, m1)\n\n                loss_g = -torch.mean(y_fake) + cross_entropy\n\n                self.optimizerG.zero_grad()\n                loss_g.backward()\n                self.optimizerG.step()\n\n            l_average = (loss_g.item() + loss_d.item()) / 2\n            print(\"Epoch %d, Loss G: %.4f, Loss D: %.4f, Loss A: %.4f\" %\n                  (self.trained_epoches, loss_g.detach().cpu(), loss_d.detach().cpu(), l_average),\n                  flush=True)\n\n    def sample(self, n, condition_column=None, condition_value=None):\n        \"\"\"Sample data similar to the training data.\n        Args:\n            n (int):\n                Number of rows to sample.\n        Returns:\n            numpy.ndarray or pandas.DataFrame\n        \"\"\"\n\n        if condition_column is not None and condition_value is not None:\n            condition_info = self.transformer.covert_column_name_value_to_id(\n                condition_column, condition_value)\n            global_condition_vec = self.cond_generator.generate_cond_from_condition_column_info(\n                condition_info, self.batch_size)\n        else:\n            global_condition_vec = None\n\n        steps = n // self.batch_size + 1\n        data = []\n        for i in range(steps):\n            mean = torch.zeros(self.batch_size, self.embedding_dim)\n            std = mean + 1\n            fakez = torch.normal(mean=mean, std=std).to(self.device)\n\n            if global_condition_vec is not None:\n                condvec = global_condition_vec.copy()\n            else:\n                condvec = self.cond_generator.sample_zero(self.batch_size)\n\n            if condvec is None:\n                pass\n            else:\n                c1 = condvec\n                c1 = torch.from_numpy(c1).to(self.device)\n                fakez = torch.cat([fakez, c1], dim=1)\n\n            fake = self.generator(fakez)\n            fakeact = self._apply_activate(fake)\n            data.append(fakeact.detach().cpu().numpy())\n\n        data = np.concatenate(data, axis=0)\n        data = data[:n]\n\n        return self.transformer.inverse_transform(data, None)\n\n    def save(self, path):\n        assert hasattr(self, \"generator\")\n        assert hasattr(self, \"discriminator\")\n        assert hasattr(self, \"transformer\")\n\n        # always save a cpu model.\n        device_bak = self.device\n        self.device = torch.device(\"cpu\")\n        self.generator.to(self.device)\n        self.discriminator.to(self.device)\n\n        torch.save(self, path)\n\n        self.device = device_bak\n        self.generator.to(self.device)\n        self.discriminator.to(self.device)\n\n    @classmethod\n    def load(cls, path):\n        model = torch.load(path)\n        model.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        model.generator.to(model.device)\n        model.discriminator.to(model.device)\n        return model","metadata":{"_kg_hide-input":true,"executionInfo":{"elapsed":53375,"status":"ok","timestamp":1602683221359,"user":{"displayName":"Yirun Zhang","photoUrl":"","userId":"05891579764514658952"},"user_tz":-60},"id":"kAqDD5VDdhrD","papermill":{"duration":1.661557,"end_time":"2020-10-17T18:09:41.165274","exception":false,"start_time":"2020-10-17T18:09:39.503717","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-15T06:45:02.47299Z","iopub.execute_input":"2021-10-15T06:45:02.473543Z","iopub.status.idle":"2021-10-15T06:45:04.149051Z","shell.execute_reply.started":"2021-10-15T06:45:02.473504Z","shell.execute_reply":"2021-10-15T06:45:04.148342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train CTGAN","metadata":{"id":"ZUNEqX4G4eno","papermill":{"duration":0.01209,"end_time":"2020-10-17T18:09:41.190005","exception":false,"start_time":"2020-10-17T18:09:41.177915","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model = CTGANSynthesizer()\nmodel.fit(data, discrete_cols, epochs = 1)","metadata":{"id":"3JiGnAnfn_RU","outputId":"87409823-5bc2-4ab2-d1bf-2aa3030212fa","papermill":{"duration":29758.256122,"end_time":"2020-10-18T02:25:39.458416","exception":false,"start_time":"2020-10-17T18:09:41.202294","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-15T06:45:04.150159Z","iopub.execute_input":"2021-10-15T06:45:04.150396Z","iopub.status.idle":"2021-10-15T06:48:59.21978Z","shell.execute_reply.started":"2021-10-15T06:45:04.150365Z","shell.execute_reply":"2021-10-15T06:48:59.217908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samples = model.sample(10000)\nsamples.to_csv('ctgan_aug.csv', index = False)","metadata":{"id":"K0Xfoc1nI0RX","papermill":{"duration":28.163172,"end_time":"2020-10-18T02:26:07.737598","exception":false,"start_time":"2020-10-18T02:25:39.574426","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-15T06:48:59.221039Z","iopub.status.idle":"2021-10-15T06:48:59.221464Z","shell.execute_reply.started":"2021-10-15T06:48:59.22123Z","shell.execute_reply":"2021-10-15T06:48:59.221253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from joblib import dump, load\n\ndump(model, 'ctgan_dump')\nmodel = load('ctgan_dump')","metadata":{"id":"Bm-D0dfmcMO0","papermill":{"duration":4.191537,"end_time":"2020-10-18T02:26:12.05483","exception":false,"start_time":"2020-10-18T02:26:07.863293","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-15T06:48:59.22292Z","iopub.status.idle":"2021-10-15T06:48:59.223329Z","shell.execute_reply.started":"2021-10-15T06:48:59.223108Z","shell.execute_reply":"2021-10-15T06:48:59.22313Z"},"trusted":true},"execution_count":null,"outputs":[]}]}