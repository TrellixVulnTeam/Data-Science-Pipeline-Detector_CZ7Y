{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-23T19:46:53.081776Z","iopub.execute_input":"2021-10-23T19:46:53.082124Z","iopub.status.idle":"2021-10-23T19:46:53.09269Z","shell.execute_reply.started":"2021-10-23T19:46:53.08209Z","shell.execute_reply":"2021-10-23T19:46:53.091726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This notebook is in response to https://www.kaggle.com/c/tabular-playground-series-oct-2021/discussion/280986\nIt is not supposed to a notebook dedicated to modelling data for the TPS Oct \n\n","metadata":{}},{"cell_type":"markdown","source":"From the docmentation  https://contrib.scikit-learn.org/category_encoders/targetencoder.html\n\n\"For the case of categorical target: features are replaced with a blend of posterior probability of the target given particular categorical value and the prior probability of the target over all the training data.\"\n\nBut we know that the correlation of features to target is very low with this dataset as ALL the EDA (and SHAP) notebooks came to the same conclusion. Actually this makes sense for what we see TargetEncoding actually do. \nThe new values it comes up with are very close to 0.5 suggesting that correlation of feature to target is very very low.\n","metadata":{}},{"cell_type":"markdown","source":"So what was a sequence of 0's and 1's now becomes a sequence of 0.4999 and 0.5001\nEither way it is simply 2 possible values , which my head still reads binary and as such it should not make any difference to the scores. But it is since the LB score is definetly changing.\n\nWait. The golden rule is trust in CV. I need to check and see how the CV score is changing.\n\nWithout target encoding www.kaggle.com/mohammadkashifunique/tsp-single-xgboost-model it is \nOverall Validation Score: 0.8572129946017066\nWith target encoding  www.kaggle.com/mohammadkashifunique/single-xgboost-model-featureengineering it is  \nOverall Validation Score: 0.8572199414082429\n\nThat seems very close. A bit of a change in the 6th decimal place of CV score is producing a 2 point change in the 5th decimal place in the LB score.\nThat is food for thought!! Is the 30% of scoring data being used somehow biased on any boolean features.\n\nI might need to think about this point some more...\nAlso I don't trust the caluclation of the CV score becuase it is based on the average over the 5 folds and not calculated once on the entire set of OOF precitions.","metadata":{}},{"cell_type":"markdown","source":"For now though we need to expand on exactly what is changing when we run TargetEncoding. At the most basic level we are changing the variance of that feature. More specifically of every feature that we run TargetEncoding on. \nSo a quick recap on variance.\n\nVariance is a measure of dispersion, meaning it is a measure of how far a set of numbers are spread out from their average value. ","metadata":{}},{"cell_type":"code","source":"np_joe1 = [0, 0, 0, 0, 0, 1, 1, 1, 1]\nprint (\"Var joe1 : \", np.var(np_joe1))\nnp_joe2 = [0.49, 0.49, 0.49, 0.49, 0.49, 0.51, 0.51, 0.51, 0.51]\nprint (\"Var joe2: \", np.var(np_joe2))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:47:31.162123Z","iopub.execute_input":"2021-10-23T19:47:31.162761Z","iopub.status.idle":"2021-10-23T19:47:31.17019Z","shell.execute_reply.started":"2021-10-23T19:47:31.162721Z","shell.execute_reply":"2021-10-23T19:47:31.169319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I don't know all the technical details of how each model interacts with the variance of an indivual feature, but I do know that they do.\n\nFrom the StandardScaler documenation http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n    \n    Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or \n    less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n    \nThe StandardScaler sorts this out by \"Standardize features by removing the mean and scaling to unit variance.\"\n\n\nReading ths above again, I'm thinkning that it is not that easy to change a bimodal distribution to a Gaussian one. So model sensitivity is likely to be more on variance and less on dsitribution. \n\n","metadata":{}},{"cell_type":"markdown","source":"So to test this theory I have modified my copy of the original notebook \nhttps://www.kaggle.com/joecooper/tsp-single-xgboost-model\n\nThe difference between v1 and v3 is mainly the inclusion of the standard scaler. It largely confirms what the above is saying : having reduced or even better, having 'managed' variance for features will improve the models estimators and performance.","metadata":{}}]}