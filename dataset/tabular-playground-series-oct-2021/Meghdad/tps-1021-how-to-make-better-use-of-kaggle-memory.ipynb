{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Abstract\nThe resources that Kaggle allocates to execute code are limited. For example 13 or 16 GB of RAM (depending on the account). The TPS-October data is more than 2 GB and is not easy to work with despite Google restrictions. In this article we will see how to make better use of the amount of dedicated RAM.\n\nDue to the high capabilities of pandas, most data scientists are accustomed to using pandas. This is a good choice but not when working with big data .\nAnother issue is not observing the data type.  In the following, we will deal with these two points","metadata":{}},{"cell_type":"code","source":"import time\nimport pandas as pd\nimport datatable as dt\nimport psutil\nimport gc","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pandas load data slowly. One alternative is to use `datatable` and convert theme to `Pandas`. [Link](https://www.kaggle.com/bextuychiev/how-to-work-w-million-row-datasets-like-a-pro#Read-in-the-massive-dataset)","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_pd = pd.read_csv('/kaggle/input/tabular-playground-series-oct-2021/train.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_dt =dt.fread('/kaggle/input/tabular-playground-series-oct-2021/train.csv').to_pandas()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can delete the columns that we do not need while reading the data and save time and resources ","metadata":{}},{"cell_type":"code","source":"del train_dt\ngc.collect()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_dt =dt.fread('/kaggle/input/tabular-playground-series-oct-2021/train.csv', columns=lambda cols:[col.name not in ('id') for col in cols]).to_pandas()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"memory_pd_1 = train_pd.memory_usage(index = True).sum() / 1e9\nmemory_dt_1 = train_dt.memory_usage(index = True).sum() / 1e9","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Memory consumed through Pandas: {:0.2f} GB'.format(memory_pd_1))\nprint('Memory consumed through DataTable: {:0.2f} GB'.format(memory_dt_1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The reason for this difference is in the type of data ","metadata":{}},{"cell_type":"code","source":"print(f'{3*\"=\"} For Pandas {10*\"=\"}\\n{(train_pd.dtypes).value_counts()}')\nprint(f'\\n{3*\"=\"} For Datatable {7*\"=\"}\\n{(train_dt.dtypes).value_counts()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting Type\n\nDoes converting data change their values? **Depends on the data type**\n\nIs it possible to do this easily using the existing functions? **No**\n\nIn the following, we will deal with these two points\n\n","metadata":{}},{"cell_type":"code","source":"## clear memory\nprint('{:0.2f} gb memory used so we empty some of it'.format(psutil.virtual_memory()[3] // 1e9))\ndel train_dt\ngc.collect()\nprint('clear memory')\nprint('{:0.2f} gb memory used'.format(psutil.virtual_memory()[3] // 1e9))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a copy for comparison \ntrain_pd_copy = train_pd.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-danger\">\n<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"32\" height=\"32\" viewBox=\"0 0 16 16\" fill=\"currentColor\">\n  <path d=\"M8.982 1.566a1.13 1.13 0 0 0-1.96 0L.165 13.233c-.457.778.091 1.767.98 1.767h13.713c.889 0 1.438-.99.98-1.767L8.982 1.566zM8 5c.535 0 .954.462.9.995l-.35 3.507a.552.552 0 0 1-1.1 0L7.1 5.995A.905.905 0 0 1 8 5zm.002 6a1 1 0 1 1 0 2 1 1 0 0 1 0-2z\"/>\n</svg>\n<b style=\"font-size: x-large;\">ATTENTION</b><br>\n    Running this line will overflow the memory. For this reason, I could not compare the existing functions with the manual method \n    \n</div>","metadata":{}},{"cell_type":"code","source":"# Running this line will overflow the memory \n# train_pd[train_pd.select_dtypes(include='float64') ] = train_pd.select_dtypes(include='float64').astype('float32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_type(col, new_type):\n    return train_pd[col].astype(new_type)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in train_pd.columns:\n    if train_pd[c].dtypes == 'float64':\n        train_pd[c] = convert_type(c, 'float32')\n    elif train_pd[c].dtypes == 'int64':\n        train_pd[c] = convert_type(c, 'bool')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"memory_pd_2 = train_pd.memory_usage(index = True).sum() / 1e9\nmemory_pd_percent = (memory_pd_2 * 100 ) / memory_pd_1\nprint('Memory consumed befor converting was {:0.2f} GB and after converting is {:0.2f} GB so {:0.2f}% savings'.format(memory_pd_1, memory_pd_2, memory_pd_percent))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I will publish the comparison section soon **","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}