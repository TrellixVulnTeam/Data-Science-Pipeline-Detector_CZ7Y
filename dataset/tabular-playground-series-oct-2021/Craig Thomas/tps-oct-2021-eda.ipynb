{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1 Introduction\n\nThis EDA explores the data available for the Tabular Playground Series - October 2021 competition. Simple data exploration is performed, as well as preliminary modeling.\n\n## 1.1 Evaluation Criteria\n\nThe goal for this competition is to maximize ROC AUC score. This means generating classifiers or regressions that predict the probability of the class target variable based on the features included.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\n\ntrain = pd.read_csv(\"../input/tabular-playground-series-oct-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-oct-2021/test.csv\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cat_column_info(column):\n    num_categories = train[column].nunique()\n    print(\"------> {} <------\".format(column))\n    print(\"--: train - type {}\".format(train[column].dtype))\n    print(\"--: test  - type {}\".format(test[column].dtype))\n    print(\"--: train - # categories {}\".format(train[column].nunique()))\n    print(\"--: test  - # categories {}\".format(test[column].nunique()))\n    if num_categories < 10:\n        if train[column].dtype == \"int64\":\n            print(\"--: train - values {}\".format(np.sort(train[column].unique())))\n            print(\"--: test  - values {}\".format(np.sort(test[column].unique())))\n        else:\n            print(\"--: train - values {}\".format(train[column].unique()))\n            print(\"--: test  - values {}\".format(test[column].unique()))\n    print(\"--: train - NaN count {}\".format(train[column].isnull().values.sum()))\n    print(\"--: test  - NaN count {}\".format(test[column].isnull().values.sum()))\n    print(\"\")\n\ndef cont_column_info(column):\n    print(\"------> {} <------\".format(column))\n    print(\"--: train - type {}\".format(train[column].dtype))\n    print(\"--: test  - type {}\".format(test[column].dtype))\n    print(\"--: train - min {}\".format(train[column].min()))\n    print(\"--: test  - min {}\".format(test[column].min()))\n    print(\"--: train - max {}\".format(train[column].max()))\n    print(\"--: test  - max {}\".format(test[column].max()))    \n    print(\"--: train - NaN count {}\".format(train[column].isnull().values.sum()))\n    print(\"--: test  - NaN count {}\".format(test[column].isnull().values.sum()))\n    print(\"\")\n    \nprint(\": Train shape {}\".format(train.shape))\nprint(\": Test shape {}\".format(test.shape))\nprint(\"\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Training and Testing Files\n\nOur input data consists of:\n\n* `train.csv` - 2.2 GB in size, containing 287 columns and 1,000,000 rows\n* `test.csv` - 1.1 GB in size, containing 286 columns and 500,000 rows\n\nOne main observation here is the sheer size of the data we are looking at. While 2.2 GB fits in memory, model training may exert pressure on the Kaggle 16 GB CPU memory and GPU memory limitations. We should definitely explore what column formats are at play, and whether running functions to [reduce memory usage](https://www.kaggle.com/gemartin/load-data-reduce-memory-usage) on Pandas dataframes can ease pressure on memory.","metadata":{}},{"cell_type":"markdown","source":"# 2 Features\n\n## 2.1 `id` Column\n\nThe `id` column is a `int64` integer column that contains unique record indicators ranging from 0 to 999,999. Like most Tabular Series, this is simply an identifier for the record and is likely not going to be of use for modelling purposes.\n\n## 2.2 `target` Column\n\nThe `target` column contains the class targets we are attempting to predict. We should look first to see what class breakdown we have.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\nsns_params = {\"palette\": \"bwr_r\"}\n\ncounts = pd.DataFrame(train[\"target\"].value_counts())\nax = sns.barplot(x=counts.index, y=counts.target, **sns_params)\nfor p in ax.patches:\n    ax.text(x=p.get_x()+(p.get_width()/2), y=p.get_height(), s=\"{:,d}\".format(round(p.get_height())), ha=\"center\")\n_ = ax.set_title(\"Class Balance\", fontsize=15)\n_ = ax.set_ylabel(\"Number of Records\", fontsize=15)\n_ = ax.set_xlabel(\"Class\", fontsize=15)\n\ndel(counts)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The predicted class is well balanced, with little to no skew. This is interesting as it gives us a lot of training data per class to look at.","metadata":{}},{"cell_type":"markdown","source":"## 2.3 `fX` Columns\n\nThere are 285 feature columns named `f0` through `f284`. \n\n\n## 2.4 Null Values\n\nFirst things first, we should check to see if we are missing any values in the columns, as this was an interesting feature from the competition last month.","metadata":{}},{"cell_type":"code","source":"# Count the number of null values that occur in each row\ntrain[\"null_count\"] = train.isnull().sum(axis=1)\n\n# Group the null counts\ncounts = train.groupby(\"null_count\")[\"target\"].count().to_dict()\nnull_data = {\"{} Null Value(s)\".format(k) : v for k, v in counts.items() if k < 6}\n\n# Plot the null count results\npie, ax = plt.subplots(figsize=[20, 10])\ncolors = sns.color_palette(\"bwr_r\")[0:5]\nplt.pie(x=null_data.values(), autopct=\"%.2f%%\", explode=[0.05]*len(null_data.keys()), labels=null_data.keys(), pctdistance=0.5, colors=colors)\n_ = plt.title(\"Percentage of Null Values Per Row (Train Data)\", fontsize=14)\n\ndel(counts)\ndel(null_data)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count the number of null values that occur in each row\ntest[\"null_count\"] = test.isnull().sum(axis=1)\n\n# Group the null counts\ncounts = test.groupby(\"null_count\")[\"null_count\"].count().to_dict()\nnull_data = {\"{} Null Value(s)\".format(k) : v for k, v in counts.items() if k < 6}\n\n# Plot the null count results\npie, ax = plt.subplots(figsize=[20, 10])\nplt.pie(x=null_data.values(), autopct=\"%.2f%%\", explode=[0.05]*len(null_data.keys()), labels=null_data.keys(), pctdistance=0.5, colors=colors)\n_ = plt.title(\"Percentage of Null Values Per Row (Test Data)\", fontsize=14)\n\ndel(counts)\ndel(null_data)\ndel(test)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this competition, we're not seeing any missing values. This means we don't have to worry about imputing or creating new features based on null values.","metadata":{}},{"cell_type":"markdown","source":"## 2.5 Continuous Columns\n\nColumns `f0` through `f241` (with the exception of `f22` and `f43`) are all of type `float64`. All columns are scaled between 0 and 1. This is interesting, since the data is already pre-scaled for use with a neural network. ","metadata":{}},{"cell_type":"markdown","source":"## 2.6 Categorical Columns\n\nColumns `f22`, `f43`, and `f242` through `f284` are all of type `int64`. Value counts suggest that these columns are likely one-hot encoded variables of some form. Let's take a look at how they line up with the target variable.","metadata":{}},{"cell_type":"code","source":"cat_features = [\"f22\", \"f43\"]\ncat_features.extend([\"f{}\".format(x) for x in range(242, 285)])","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(11, 4, figsize=(4*4, 11*3), squeeze=False, sharey=True)\n\nptr = 0\nfor row in range(11):\n    for col in range(4):  \n        x = train[[cat_features[ptr], \"target\"]].value_counts().sort_index().to_frame().rename({0: \"# of Samples\"}, axis=\"columns\").reset_index()\n        sns.barplot(x=cat_features[ptr], y=\"# of Samples\", hue=\"target\", data=x, ax=axs[row][col], **sns_params)\n        plt.xlabel(cat_features[ptr])\n        ptr += 1\n        del(x)\nplt.tight_layout()    \nplt.show()\n\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we start to see some interesting information. Most of the categorical variables aren't very informative when it comes to discriminating the target variable. The one exception is feature `f22`, where we see a value of `0` is strongly correlated with a `target` of `1`, while a value of `1` is strongly correlated with a target of `0`. The remaining categorical features do not have any strong indicators of the target variable. This would suggest as a baseline we are probably best to drop most of the other feature variables if dimensionality is a problem.","metadata":{}},{"cell_type":"markdown","source":"## 2.7 Categorical Feature Relationships\n\nGiven that most of the categorical features are `0` or `1`, it is reasonable to hypothesize that these may be one-hot encoded categoricals. The question is whether we can determine what feature columns were broken out into their one-hot counterparts, and if we could reasonably recombine them into a single categorical field instead. The reason this may be a good idea is due to the plethora of categorical features we see. If they all belong to a single category, then we may want to recreate that category and check to see what other types of categorical encodings we could use in place of one-hot. To check for one-hot dependencies, we need to look at the dataframe categorical columns and check for instances where two columns contain mutually exclusive information. For example, if column `f242` and `f243` were broken out from a categorical column where there were values `A` and `B` into two new columns such as `A_present` and `B_present`, then we would never expect to see both `A_present` and `B_present` having the value `1` at the same time. ","metadata":{}},{"cell_type":"code","source":"temp_features = cat_features.copy()\ntemp_features.remove(\"f22\")\ntemp_features.remove(\"f43\")\n\ndf = pd.DataFrame(train[temp_features])\nrelated_features = []\nfor x in range(242, 285):\n    for y in range(x+1, 285):\n        if len(df[(df[\"f{}\".format(x)] == 1) & (df[\"f{}\".format(y)] == 1)]) == 0:\n            related_features.append((\"f{}\".format(x), \"f{}\".format(y)))\n\nif len(related_features) == 0:\n    print(\"-> Found no one-hot dependencies between categorical features\")\nelse:\n    print(\"-> The following features may be dependent on one another:\")\n    for (x, y) in related_features:\n        print(\"---> f{} and f{}\".format(x, y))","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, there appear to be no links between categorical columns, which means that the binary columns are not categorically related to each other.","metadata":{}},{"cell_type":"markdown","source":"## 2.8 Sum of Binary Features Per Row\n\nOne other aspect to look at is if sum of the binary features has some information when related to `target`.","metadata":{}},{"cell_type":"code","source":"train[\"binary_count\"] = train[cat_features].sum(axis=1)\n\nx = train[[\"binary_count\", \"target\"]].value_counts().sort_index().to_frame().rename({0: \"# of Samples\"}, axis=\"columns\").reset_index()\nfig, ax = plt.subplots(figsize=(20, 20))\n_ = sns.barplot(x=\"binary_count\", y=\"# of Samples\", hue=\"target\", data=x, **sns_params)\n_ = plt.xlabel(\"Number of Binary Features = 1\", fontsize=15)\n\ndel(x)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are starting to see a little separation based on target value. For example, we see that there is a higher likelihood of a `target` value of `1` if there are 11 binary features set to `1`. We may want to include this information in our models. It looks as though the probability of having a `target` value of `1` is higher if there are fewer than 15 binary features set. Let's zoom on this a little more.","metadata":{}},{"cell_type":"code","source":"train[\"binary_15_16\"] = train[\"binary_count\"].apply(lambda x: 15 if x <= 15 else 16)\n\nx = train[[\"binary_15_16\", \"target\"]].value_counts().sort_index().to_frame().rename({0: \"# of Samples\"}, axis=\"columns\").reset_index()\nfig, ax = plt.subplots(figsize=(20, 20))\n_ = sns.barplot(x=\"binary_15_16\", y=\"# of Samples\", hue=\"target\", data=x, **sns_params)\n_ = plt.xlabel(\"Number of Binary Features\", fontsize=15)\n\ndel(x)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There isn't a huge amount of distinction between them. If we bin them a little more, perhaps there may be more differentiation.","metadata":{}},{"cell_type":"code","source":"def bin_count(x):\n    if x <= 5:\n        return 5\n    if x > 5 and x <= 10:\n        return 10\n    if x > 10 and x <= 15:\n        return 15\n    if x > 15 and x <= 20:\n        return 20\n    if x > 20 and x <= 25:\n        return 25\n    return 30\n\ntrain[\"binary_5_10_15_20_25_30\"] = train[\"binary_count\"].apply(lambda x: bin_count(x))\n\nx = train[[\"binary_5_10_15_20_25_30\", \"target\"]].value_counts().sort_index().to_frame().rename({0: \"# of Samples\"}, axis=\"columns\").reset_index()\nfig, ax = plt.subplots(figsize=(20, 20))\n_ = sns.barplot(x=\"binary_5_10_15_20_25_30\", y=\"# of Samples\", hue=\"target\", data=x, **sns_params)\n_ = plt.xlabel(\"Number of Binary Features\", fontsize=15)\n\ndel(x)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, it looks like generating bins isn't going to help us any more than simply counting the number of binary values that occur in each row.","metadata":{}},{"cell_type":"markdown","source":"## 2.9 P-Value Testing\n\nWhile looking at features visually will tell us some interesting information, we can also use p-value testing to see if a feature has a net impact on a simple regression model. ","metadata":{}},{"cell_type":"code","source":"from statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools.tools import add_constant\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport gc\n\ncont_features = [\"f{}\".format(x) for x in range(242)]\ncont_features.remove(\"f22\")\ncont_features.remove(\"f43\")\n\nfeatures = []\nfeatures.extend(cat_features)\nfeatures.extend(cont_features)\nx = add_constant(train[features])\nmodel = OLS(train[\"target\"], x).fit()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pvalues = pd.DataFrame(model.pvalues)\npvalues.reset_index(inplace=True)\npvalues.rename(columns={0: \"pvalue\", \"index\": \"feature\"}, inplace=True)\npvalues.style.background_gradient(cmap='YlOrRd')","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del(model)\ndel(x)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The null hypothesis is that the feature impacts the target variable of `target`. In this case, anything with a p-value greater than 0.05 means we reject that hypothesis. This means that there are features above we can remove that will not impact the overall model. Let's iterate them below.","metadata":{}},{"cell_type":"code","source":"features_to_drop = []\nfor index, row in pvalues.iterrows():\n    if row[\"pvalue\"] > 0.05:\n        features_to_drop.append(row[\"feature\"])\nfeatures_to_drop","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 Simple Models\n\nGiven we know a little about the distribution of data, we should establish a set of baseline models to understand what kind of performance we can get from models.","metadata":{}},{"cell_type":"markdown","source":"## 3.1 LightGBM\n\nWe'll start with a simple LightGBM model and see how our features work out from there.","metadata":{}},{"cell_type":"code","source":"cont_features = [\"f{}\".format(x) for x in range(242)]\ncont_features.remove(\"f22\")\ncont_features.remove(\"f43\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\n\ntarget = train[\"target\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\nfeatures = []\nfeatures.extend(cat_features)\nfeatures.extend(cont_features)\n\ntrain_preds = np.zeros(len(train.index), )\ntrain_probas = np.zeros(len(train.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"auc\",\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        early_stopping_rounds=50,\n        verbose=0,\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_oof_probas = model.predict_proba(x_valid)[:, -1]\n    train_preds[test_index] = train_oof_preds\n    train_probas[test_index] = train_oof_probas\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- ROC AUC: {}\".format(roc_auc_score(target, train_probas)))\n\ntrain[\"unmodified_preds\"] = train_preds\ntrain[\"unmodified_probas\"] = train_probas\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"target\"], train[\"unmodified_preds\"])\nax = sns.heatmap(confusion, annot=True, fmt=\",d\")\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (Unmodified Dataset)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(train_probas)\ndel(confusion)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking across folds, we are seeing stability, which is good. Our overall precision and recall metrics are fairly high between the positive and negative class, although the model is having problems capturing more instances of the positive class. ","metadata":{}},{"cell_type":"markdown","source":"## 3.2 LightGBM Dropping Uninformative Categorical Features\n\nGiven what we know about categorical features, we know that only `f22` is the only informative feature available. We can rebuild our model and drop the other categoricals.","metadata":{}},{"cell_type":"code","source":"target = train[\"target\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\nfeatures = [\"f22\"]\nfeatures.extend(cont_features)\n\ntrain_preds = np.zeros(len(train.index), )\ntrain_probas = np.zeros(len(train.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"auc\",\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        early_stopping_rounds=50,\n        verbose=0,\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_oof_probas = model.predict_proba(x_valid)[:, -1]\n    train_preds[test_index] = train_oof_preds\n    train_probas[test_index] = train_oof_probas\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- ROC AUC: {}\".format(roc_auc_score(target, train_probas)))\n\ntrain[\"drop_cat_features_preds\"] = train_preds\ntrain[\"drop_cat_features_probas\"] = train_probas\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"target\"], train[\"drop_cat_features_preds\"])\nax = sns.heatmap(confusion, annot=True, fmt=\",d\")\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (Dropping Most Categoricals)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(train_probas)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We're seeing a slight decrease in our ROC AUC score without some of the categoricals. However, as expected, the impact is quite minimal, suggesting that `f22` has a lot of pull.","metadata":{}},{"cell_type":"markdown","source":"## 3.3 Using Binary Counts\n\nThe next interesting feature we should look at is using binary count data across each row. The data discovery portion above found that there may be some correlation between the number of binary features being `1` in the row and it's relation to `target`.","metadata":{}},{"cell_type":"code","source":"target = train[\"target\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\nfeatures = []\nfeatures.extend(cat_features)\nfeatures.extend(cont_features)\nfeatures.append(\"binary_count\")\n\ntrain_preds = np.zeros(len(train.index), )\ntrain_probas = np.zeros(len(train.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"auc\",\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        early_stopping_rounds=50,\n        verbose=0,\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_oof_probas = model.predict_proba(x_valid)[:, -1]\n    train_preds[test_index] = train_oof_preds\n    train_probas[test_index] = train_oof_probas\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- ROC AUC: {}\".format(roc_auc_score(target, train_probas)))\n\ntrain[\"bincount_preds\"] = train_preds\ntrain[\"bincount_probas\"] = train_probas\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"target\"], train[\"bincount_preds\"])\nax = sns.heatmap(confusion, annot=True, fmt=\",d\")\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (Binary Counts)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(train_probas)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While we see a small amount of lift using binary counts, it doesn't impact ROC AUC scores by a large amount when compared to the default model.","metadata":{}},{"cell_type":"markdown","source":"## 3.4 Removing Features\n\nLet's take a look at what happens when we remove features as indicated by their p-value.","metadata":{}},{"cell_type":"code","source":"target = train[\"target\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\nfeatures = []\nfeatures.extend(cat_features)\nfeatures.extend(cont_features)\nfeatures.append(\"binary_count\")\n\nfor feature in features_to_drop:\n    features.remove(feature)\n\ntrain_preds = np.zeros(len(train.index), )\ntrain_probas = np.zeros(len(train.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"auc\",\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        early_stopping_rounds=50,\n        verbose=0,\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_oof_probas = model.predict_proba(x_valid)[:, -1]\n    train_preds[test_index] = train_oof_preds\n    train_probas[test_index] = train_oof_probas\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- ROC AUC: {}\".format(roc_auc_score(target, train_probas)))\n\ntrain[\"bincount_remove_preds\"] = train_preds\ntrain[\"bincount_remove_probas\"] = train_probas\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"target\"], train[\"bincount_remove_preds\"])\nax = sns.heatmap(confusion, annot=True, fmt=\",d\")\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (Feature Removal + Binary Counts)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(train_probas)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.5 Comparison of Approaches","metadata":{}},{"cell_type":"code","source":"bar, ax = plt.subplots(figsize=(20, 10))\nax = sns.barplot(\n    x=[\"Unmodified\", \"Dropping Features\", \"Binary Counts\", \"Binary Counts + Removed\"],\n    y=[\n        float(roc_auc_score(target, train[\"unmodified_probas\"])),\n        roc_auc_score(target, train[\"drop_cat_features_probas\"]),\n        roc_auc_score(target, train[\"bincount_probas\"]),\n        roc_auc_score(target, train[\"bincount_remove_probas\"]),\n    ],\n    **sns_params\n)\n_ = ax.set_title(\"ROC AUC Score Based on Approach\", fontsize=15)\n_ = ax.set_xlabel(\"Approach\")\n_ = ax.set_ylabel(\"ROC AUC Score\")\n_ = ax.set(ylim=(0.84, 0.86))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(\n        x=p.get_x()+(p.get_width()/2),\n        y=height,\n        s=\"{:.4f}\".format(height),\n        ha=\"center\"\n    )","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe a few interesting features. First, dropping categorical features blindly in favor of `f22` results in a drop in our ROC AUC score. Binary counts on the other hand, fail to provide a significant amount of lift to our scores. Finally, removing features based on p-values leaves the overall score unchanged. ","metadata":{}},{"cell_type":"markdown","source":"# More to come...","metadata":{}}]}