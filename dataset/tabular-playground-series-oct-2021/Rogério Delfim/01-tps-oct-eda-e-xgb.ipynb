{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 div class='alert alert-success'><center> TPS-Oct: ponto de partida (EDA, linha de base XGB)</center></h1>\n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/26480/logos/header.png?t=2021-04-09-00-57-05)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"}},{"cell_type":"markdown","source":"# Descrição de dados\n\nPara esta competição, você vai prever se um cliente fez uma reclamação sobre uma apólice de seguro. A verdade fundamental claimtem valor binário, mas uma previsão pode ser qualquer número de 0.0 para 1.0, representando a probabilidade de uma reclamação. Os recursos neste conjunto de dados foram tornados anônimos e podem conter valores ausentes.\narquivos\n\n- `train.csv`: os dados de treinamento com o alvo claimcoluna\n- `test.csv`: o conjunto de teste; você estará prevendo o claimpara cada linha neste arquivo\n- `sample_submission.csv`:  um arquivo de envio de amostra no formato correto","metadata":{}},{"cell_type":"markdown","source":"# <div class=\"alert alert-success\">  1. IMPORTAÇÕES </div> \n","metadata":{}},{"cell_type":"markdown","source":"## 1.1. Bibliotecas ","metadata":{"ExecuteTime":{"end_time":"2021-10-01T23:17:10.385375Z","start_time":"2021-10-01T23:17:10.373409Z"}}},{"cell_type":"code","source":"import warnings\nimport random\nimport os\nimport gc\n#import cudf","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:55:33.905519Z","start_time":"2021-10-03T20:55:33.898538Z"},"execution":{"iopub.status.busy":"2021-10-03T21:27:51.672926Z","iopub.execute_input":"2021-10-03T21:27:51.673686Z","iopub.status.idle":"2021-10-03T21:27:51.762533Z","shell.execute_reply.started":"2021-10-03T21:27:51.673594Z","shell.execute_reply":"2021-10-03T21:27:51.761763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas            as pd\nimport numpy             as np\nimport matplotlib.pyplot as plt \nimport seaborn           as sns\nimport joblib            as jb","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:55:35.842338Z","start_time":"2021-10-03T20:55:33.943418Z"},"execution":{"iopub.status.busy":"2021-10-03T21:27:51.764648Z","iopub.execute_input":"2021-10-03T21:27:51.765115Z","iopub.status.idle":"2021-10-03T21:27:52.074829Z","shell.execute_reply.started":"2021-10-03T21:27:51.765058Z","shell.execute_reply":"2021-10-03T21:27:52.074053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.preprocessing   import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, QuantileTransformer\nfrom sklearn.impute          import SimpleImputer\nfrom sklearn                 import metrics","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:55:36.531507Z","start_time":"2021-10-03T20:55:35.844333Z"},"execution":{"iopub.status.busy":"2021-10-03T21:27:52.076161Z","iopub.execute_input":"2021-10-03T21:27:52.07645Z","iopub.status.idle":"2021-10-03T21:27:52.138245Z","shell.execute_reply.started":"2021-10-03T21:27:52.076413Z","shell.execute_reply":"2021-10-03T21:27:52.137578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost               as xgb\nimport catboost              as ctb","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:55:36.591335Z","start_time":"2021-10-03T20:55:36.533489Z"},"execution":{"iopub.status.busy":"2021-10-03T21:27:52.139503Z","iopub.execute_input":"2021-10-03T21:27:52.139795Z","iopub.status.idle":"2021-10-03T21:27:52.178951Z","shell.execute_reply.started":"2021-10-03T21:27:52.139759Z","shell.execute_reply":"2021-10-03T21:27:52.178274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Funções\nAqui centralizamos todas as funções desenvolvidas durante o projeto para melhor organização do código.","metadata":{}},{"cell_type":"code","source":"def jupyter_setting():\n    \n    %matplotlib inline\n      \n    #os.environ[\"WANDB_SILENT\"] = \"true\" \n    #plt.style.use('bmh') \n    #plt.rcParams['figure.figsize'] = [20,15]\n    #plt.rcParams['font.size']      = 13\n     \n    pd.options.display.max_columns = None\n    #pd.set_option('display.expand_frame_repr', False)\n\n    warnings.filterwarnings(action='ignore')\n    warnings.simplefilter('ignore')\n    warnings.filterwarnings('ignore')\n    #warnings.filterwarnings(category=UserWarning)\n\n    warnings.filterwarnings('ignore', category=DeprecationWarning)\n    warnings.filterwarnings('ignore', category=FutureWarning)\n    warnings.filterwarnings('ignore', category=RuntimeWarning)\n    warnings.filterwarnings('ignore', category=UserWarning)\n    #warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n    pd.set_option('display.max_rows', 150)\n    pd.set_option('display.max_columns', 500)\n    pd.set_option('display.max_colwidth', None)\n\n    icecream = [\"#00008b\", \"#960018\",\"#008b00\", \"#00468b\", \"#8b4500\", \"#582c00\"]\n    #sns.palplot(sns.color_palette(icecream))\n    \n    return icecream\n\nicecream = jupyter_setting()\n\n# Colors\ndark_red = \"#b20710\"\nblack    = \"#221f1f\"\ngreen    = \"#009473\"\nmyred    = '#CD5C5C'\nmyblue   = '#6495ED'\nmygreen  = '#90EE90'\n\ncols= [myred, myblue,mygreen]","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:55:36.607292Z","start_time":"2021-10-03T20:55:36.592332Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2021-10-03T21:27:52.181233Z","iopub.execute_input":"2021-10-03T21:27:52.181497Z","iopub.status.idle":"2021-10-03T21:27:52.193699Z","shell.execute_reply.started":"2021-10-03T21:27:52.181463Z","shell.execute_reply":"2021-10-03T21:27:52.192784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:55:36.623249Z","start_time":"2021-10-03T20:55:36.60829Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2021-10-03T21:27:52.19519Z","iopub.execute_input":"2021-10-03T21:27:52.195458Z","iopub.status.idle":"2021-10-03T21:27:52.208052Z","shell.execute_reply.started":"2021-10-03T21:27:52.195424Z","shell.execute_reply":"2021-10-03T21:27:52.207324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def missing_zero_values_table(df):\n        mis_val         = df.isnull().sum()\n        mis_val_percent = round(df.isnull().mean().mul(100), 2)\n        mz_table        = pd.concat([mis_val, mis_val_percent], axis=1)\n        mz_table        = mz_table.rename(columns = {df.index.name:'col_name', \n                                                     0 : 'Valores ausentes', \n                                                     1 : '% de valores totais'})\n        \n        mz_table['Tipo de dados'] = df.dtypes\n        mz_table                  = mz_table[mz_table.iloc[:,1] != 0 ]. \\\n                                     sort_values('% de valores totais', ascending=False)\n        \n        msg = \"Seu dataframe selecionado tem {} colunas e {} \" + \\\n              \"linhas. \\nExistem {} colunas com valores ausentes.\"\n            \n        print (msg.format(df.shape[1], df.shape[0], mz_table.shape[0]))\n        \n        return mz_table.reset_index()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:55:36.639206Z","start_time":"2021-10-03T20:55:36.625244Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2021-10-03T21:27:52.209455Z","iopub.execute_input":"2021-10-03T21:27:52.209767Z","iopub.status.idle":"2021-10-03T21:27:52.217777Z","shell.execute_reply.started":"2021-10-03T21:27:52.209732Z","shell.execute_reply":"2021-10-03T21:27:52.216944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def describe(df):\n    var = df.columns\n\n    # Medidas de tendência central, média e mediana \n    ct1 = pd.DataFrame(df[var].apply(np.mean)).T\n    ct2 = pd.DataFrame(df[var].apply(np.median)).T\n\n    # Dispensão - str, min , max range skew, kurtosis\n    d1 = pd.DataFrame(df[var].apply(np.std)).T\n    d2 = pd.DataFrame(df[var].apply(min)).T\n    d3 = pd.DataFrame(df[var].apply(max)).T\n    d4 = pd.DataFrame(df[var].apply(lambda x: x.max() - x.min())).T\n    d5 = pd.DataFrame(df[var].apply(lambda x: x.skew())).T\n    d6 = pd.DataFrame(df[var].apply(lambda x: x.kurtosis())).T\n    d7 = pd.DataFrame(df[var].apply(lambda x: (3 *( np.mean(x) - np.median(x)) / np.std(x) ))).T\n\n    # concatenete \n    m = pd.concat([d2, d3, d4, ct1, ct2, d1, d5, d6, d7]).T.reset_index()\n    m.columns = ['attrobutes', 'min', 'max', 'range', 'mean', 'median', 'std','skew', 'kurtosis','coef_as']\n    \n    return m","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:55:36.655164Z","start_time":"2021-10-03T20:55:36.640209Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2021-10-03T21:27:52.219361Z","iopub.execute_input":"2021-10-03T21:27:52.219681Z","iopub.status.idle":"2021-10-03T21:27:52.232017Z","shell.execute_reply.started":"2021-10-03T21:27:52.219648Z","shell.execute_reply":"2021-10-03T21:27:52.23129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def graf_bar_churn(df, col, title, xlabel, ylabel, tol = 0):\n    \n    #ax    = df.groupby(['churn_cat'])['churn_cat'].count()\n    ax     = df    \n    colors = cols\n    \n    if tol == 0: \n        total  = sum(ax)\n        ax = (ax).plot(kind    ='bar',\n                   stacked = True,\n                    width = .5,\n                   rot     = 0,\n                   color   = colors)\n    else:\n        total  = tol     \n        \n        ax = (ax).plot(kind    ='bar',\n                       stacked = True,\n                       width = .5,\n                       rot     = 0,figsize = (10,6),\n                       color   = colors)\n\n    #ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n    \n    #y_fmt = tick.FormatStrFormatter('%.0f') \n    #ax.yaxis.set_major_formatter(y_fmt)\n\n    title   = title + ' \\n'\n    xlabel  = '\\n ' + xlabel \n    ylabel  = ylabel + ' \\n'\n    \n    ax.set_title(title  , fontsize=22)\n    ax.set_xlabel(xlabel, fontsize=13)\n    ax.set_ylabel(ylabel, fontsize=13)    \n\n    min = [0,23000000]\n    #ax.set_ylim(min)\n    \n    for i in ax.patches:\n        # get_width pulls left or right; get_y pushes up or down\n        width, height = i.get_width(), i.get_height()\n        x, y = i.get_xy()        \n        \n        ax.annotate(str(round((i.get_height() * 100.0 / total), 1) )+'%', \n                    (i.get_x()+.3*width, \n                     i.get_y()+.5*height),\n                     color   = 'white',\n                     weight = 'bold',\n                     size   = 14)","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:55:36.671121Z","start_time":"2021-10-03T20:55:36.657159Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2021-10-03T21:27:52.233485Z","iopub.execute_input":"2021-10-03T21:27:52.233845Z","iopub.status.idle":"2021-10-03T21:27:52.245361Z","shell.execute_reply.started":"2021-10-03T21:27:52.233792Z","shell.execute_reply":"2021-10-03T21:27:52.244444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:55:36.686082Z","start_time":"2021-10-03T20:55:36.673117Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2021-10-03T21:27:52.246799Z","iopub.execute_input":"2021-10-03T21:27:52.247401Z","iopub.status.idle":"2021-10-03T21:27:52.262536Z","shell.execute_reply.started":"2021-10-03T21:27:52.247365Z","shell.execute_reply":"2021-10-03T21:27:52.261812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_memory_usage(df, verbose=True):\n    \n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    \n    for col in df.columns:\n        \n        col_type = df[col].dtypes\n        \n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n        \n    return df","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:55:36.702038Z","start_time":"2021-10-03T20:55:36.687079Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2021-10-03T21:27:52.263927Z","iopub.execute_input":"2021-10-03T21:27:52.264391Z","iopub.status.idle":"2021-10-03T21:27:52.279759Z","shell.execute_reply.started":"2021-10-03T21:27:52.264353Z","shell.execute_reply":"2021-10-03T21:27:52.279016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc_curve(fpr, tpr, label=None):\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, \"r-\", label=label)\n    ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('ROC curve for TPS 09')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:55:36.717996Z","start_time":"2021-10-03T20:55:36.703036Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2021-10-03T21:27:52.281332Z","iopub.execute_input":"2021-10-03T21:27:52.281629Z","iopub.status.idle":"2021-10-03T21:27:52.291762Z","shell.execute_reply.started":"2021-10-03T21:27:52.281591Z","shell.execute_reply":"2021-10-03T21:27:52.290918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir Data\n!mkdir Data/pkl\n!mkdir Data/submission\n!mkdir model\n!mkdir model/preds\n!mkdir model/optuna\n\n!mkdir model/preds/test\n!mkdir model/preds/test/n1\n!mkdir model/preds/test/n2\n!mkdir model/preds/test/n3\n\n!mkdir model/preds/train\n!mkdir model/preds/train/n1\n!mkdir model/preds/train/n2\n!mkdir model/preds/train/n3\n!mkdir model/preds/param","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:55:37.045121Z","start_time":"2021-10-03T20:55:36.718993Z"},"execution":{"iopub.status.busy":"2021-10-03T21:27:52.293381Z","iopub.execute_input":"2021-10-03T21:27:52.293648Z","iopub.status.idle":"2021-10-03T21:28:02.864755Z","shell.execute_reply.started":"2021-10-03T21:27:52.293614Z","shell.execute_reply":"2021-10-03T21:28:02.863745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3. Carregar Dados\nExistem 2 conjuntos de dados que são usados na análise, eles são o conjunto de dados de treinamento e de teste. O principal uso do conjunto de dados de treino é treinar os modelos e usá-lo para prever o conjunto de dados de teste. \n\nEnquanto o arquivo de envio de amostra é usado para informar os participantes sobre a inscrição prevista para a competição. ","metadata":{"execution":{"iopub.execute_input":"2021-10-01T11:08:17.914988Z","iopub.status.busy":"2021-10-01T11:08:17.914236Z","iopub.status.idle":"2021-10-01T11:08:17.922569Z","shell.execute_reply":"2021-10-01T11:08:17.921266Z","shell.execute_reply.started":"2021-10-01T11:08:17.914943Z"}}},{"cell_type":"code","source":"path = '../input/tabular-playground-series-oct-2021/'\n#path = 'Data/'","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:55:37.061078Z","start_time":"2021-10-03T20:55:37.047116Z"},"execution":{"iopub.status.busy":"2021-10-03T21:28:02.870912Z","iopub.execute_input":"2021-10-03T21:28:02.871174Z","iopub.status.idle":"2021-10-03T21:28:02.87513Z","shell.execute_reply.started":"2021-10-03T21:28:02.871143Z","shell.execute_reply":"2021-10-03T21:28:02.874356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndf1_train  = pd.read_csv(path + 'train.csv', index_col='id')\ndf1_test   = pd.read_csv(path + 'test.csv',index_col='id')\ndf_submission = pd.read_csv(path + 'sample_submission.csv')\n\ndf1_train.shape, df1_test.shape, df_submission.shape","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:30.826342Z","start_time":"2021-10-03T20:55:37.062076Z"},"execution":{"iopub.status.busy":"2021-10-03T21:28:02.876554Z","iopub.execute_input":"2021-10-03T21:28:02.877144Z","iopub.status.idle":"2021-10-03T21:29:02.26732Z","shell.execute_reply.started":"2021-10-03T21:28:02.877102Z","shell.execute_reply":"2021-10-03T21:29:02.26667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div class=\"alert alert-success\">  1.0. EDA  </div> ","metadata":{}},{"cell_type":"markdown","source":"### 1.1.2. Dimensão do DataSet","metadata":{}},{"cell_type":"code","source":"print('TREINO')\nprint('Number of Rows: {}'.format(df1_train.shape[0]))\nprint('Number of Columns: {}'.format(df1_train.shape[1]), end='\\n\\n')\n\nprint('TESTE')\nprint('Number of Rows: {}'.format(df1_test.shape[0]))\nprint('Number of Columns: {}'.format(df1_test.shape[1]))","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:30.857255Z","start_time":"2021-10-03T20:56:30.84326Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:02.288832Z","iopub.execute_input":"2021-10-03T21:29:02.291215Z","iopub.status.idle":"2021-10-03T21:29:02.302981Z","shell.execute_reply.started":"2021-10-03T21:29:02.291177Z","shell.execute_reply":"2021-10-03T21:29:02.302233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.3. Tipo de Dados","metadata":{}},{"cell_type":"code","source":"df1_train.info()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:30.88917Z","start_time":"2021-10-03T20:56:30.859218Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:02.306642Z","iopub.execute_input":"2021-10-03T21:29:02.307942Z","iopub.status.idle":"2021-10-03T21:29:02.34456Z","shell.execute_reply.started":"2021-10-03T21:29:02.307906Z","shell.execute_reply":"2021-10-03T21:29:02.343932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_test.info()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:30.92009Z","start_time":"2021-10-03T20:56:30.890134Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:02.348219Z","iopub.execute_input":"2021-10-03T21:29:02.350218Z","iopub.status.idle":"2021-10-03T21:29:02.385611Z","shell.execute_reply.started":"2021-10-03T21:29:02.350182Z","shell.execute_reply":"2021-10-03T21:29:02.384974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <BR>\n    \n- O dataset de treiro tem 2.1 GB com 1000000 de registros e 286 columas; \n- O dataset de teste tem 1.1 GB com 500000 de registros e 285 columas\n    \nVamos fazer uma redução desses dataset, primeiro vamos identificar os tipos de dados que temos nos datasets.\n\n</div>","metadata":{}},{"cell_type":"code","source":"print(f'{3*\"=\"} For Pandas {10*\"=\"}\\n{(df1_train.dtypes).value_counts()}')\nprint(f'\\n{3*\"=\"} For Datatable {7*\"=\"}\\n{(df1_test.dtypes).value_counts()}')","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:30.935039Z","start_time":"2021-10-03T20:56:30.92306Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:02.389404Z","iopub.execute_input":"2021-10-03T21:29:02.391355Z","iopub.status.idle":"2021-10-03T21:29:02.404719Z","shell.execute_reply.started":"2021-10-03T21:29:02.391319Z","shell.execute_reply":"2021-10-03T21:29:02.403966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Temos dois tipos de dados nos datasets, sendo vamos criar duas variáveis para podermos entender o conteúdo das informações armazenadas. ","metadata":{"ExecuteTime":{"end_time":"2021-10-02T22:30:43.420847Z","start_time":"2021-10-02T22:30:43.355024Z"}}},{"cell_type":"code","source":"feature_cat   = df1_test.select_dtypes(np.int64).columns.to_list()\nfeature_float = df1_test.select_dtypes(np.float64).columns.to_list()\n\ndf1_train[feature_cat].head()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:30.980893Z","start_time":"2021-10-03T20:56:30.937034Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:02.408496Z","iopub.execute_input":"2021-10-03T21:29:02.410371Z","iopub.status.idle":"2021-10-03T21:29:02.462652Z","shell.execute_reply.started":"2021-10-03T21:29:02.410335Z","shell.execute_reply":"2021-10-03T21:29:02.462008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_train[feature_float].head()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:31.155457Z","start_time":"2021-10-03T20:56:30.981916Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:02.466565Z","iopub.execute_input":"2021-10-03T21:29:02.468578Z","iopub.status.idle":"2021-10-03T21:29:02.689686Z","shell.execute_reply.started":"2021-10-03T21:29:02.468541Z","shell.execute_reply":"2021-10-03T21:29:02.688926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <BR>\n    \nNo primeiro dataset, temos variáveis categóricas que sofreram transformações para dammy e no segundo dataset temos variáveis quantitativas continuas, então agora vamos transformar os tipos dessas variávieis para fazermos a redução. \n    \n</div>","metadata":{}},{"cell_type":"code","source":"for col in feature_cat: \n    df1_train[col] = df1_train[col].astype(np.int8)\n    df1_test[col] = df1_test[col].astype(np.int8)\n    \nfor col in feature_float: \n    df1_train[col] = df1_train[col].astype(np.float32)\n    df1_test[col] = df1_test[col].astype(np.float32)","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:31.969941Z","start_time":"2021-10-03T20:56:31.15742Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:02.69105Z","iopub.execute_input":"2021-10-03T21:29:02.691314Z","iopub.status.idle":"2021-10-03T21:29:03.021496Z","shell.execute_reply.started":"2021-10-03T21:29:02.69128Z","shell.execute_reply":"2021-10-03T21:29:03.020728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_train.info()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:32.001093Z","start_time":"2021-10-03T20:56:31.972169Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:03.022964Z","iopub.execute_input":"2021-10-03T21:29:03.023245Z","iopub.status.idle":"2021-10-03T21:29:03.158482Z","shell.execute_reply.started":"2021-10-03T21:29:03.023204Z","shell.execute_reply":"2021-10-03T21:29:03.157654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_test.info()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:32.033007Z","start_time":"2021-10-03T20:56:32.003087Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:03.161381Z","iopub.execute_input":"2021-10-03T21:29:03.161756Z","iopub.status.idle":"2021-10-03T21:29:03.185595Z","shell.execute_reply.started":"2021-10-03T21:29:03.161716Z","shell.execute_reply":"2021-10-03T21:29:03.184628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Uma redução de 46.37% no dataset de treino e 43.91% no dataset de test. ","metadata":{}},{"cell_type":"markdown","source":"### 1.1.4. Idenficar Variáveis Ausentes (NA)\nVamos verificar os valores ausentes em cada variável conjunto de treinono e teste.","metadata":{}},{"cell_type":"code","source":"missing = missing_zero_values_table(df1_train)\nmissing[:].style.background_gradient(cmap='Reds')","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:32.249428Z","start_time":"2021-10-03T20:56:32.035002Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:03.186916Z","iopub.execute_input":"2021-10-03T21:29:03.187384Z","iopub.status.idle":"2021-10-03T21:29:03.303346Z","shell.execute_reply.started":"2021-10-03T21:29:03.187342Z","shell.execute_reply":"2021-10-03T21:29:03.302684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing = missing_zero_values_table(df1_test)\nmissing[:].style.background_gradient(cmap='Reds')","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:32.358137Z","start_time":"2021-10-03T20:56:32.250426Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:03.304388Z","iopub.execute_input":"2021-10-03T21:29:03.305058Z","iopub.status.idle":"2021-10-03T21:29:03.398904Z","shell.execute_reply.started":"2021-10-03T21:29:03.30502Z","shell.execute_reply":"2021-10-03T21:29:03.397964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n`NOTA:` <br>\n\nNão temos dados faltantes.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"### 1.1.6. Estatística Descritiva\nAbaixo estão as estatísticas básicas para cada variável que contém informações sobre contagem, média, desvio padrão, mínimo, 1º quartil, mediana, 3º quartil e máximo.","metadata":{}},{"cell_type":"code","source":"df_num = df1_train.select_dtypes(np.number)\ndf_cat = df1_train.select_dtypes(exclude=[np.number])\n\ndf_num.shape, df_cat.shape\n\nprint('Temos {} variávies numéricas e {} categóricas.'.format(df_num.shape[1], df_cat.shape[1]))","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:32.390052Z","start_time":"2021-10-03T20:56:32.360133Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:03.40014Z","iopub.execute_input":"2021-10-03T21:29:03.400615Z","iopub.status.idle":"2021-10-03T21:29:03.420203Z","shell.execute_reply.started":"2021-10-03T21:29:03.40057Z","shell.execute_reply":"2021-10-03T21:29:03.419517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.1.6.1. Atributos Numéricos","metadata":{}},{"cell_type":"markdown","source":"- Train","metadata":{}},{"cell_type":"code","source":"df1_train[feature_float].describe().style.background_gradient(cmap='YlOrRd')","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:33.154723Z","start_time":"2021-10-03T20:56:32.392047Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:03.421495Z","iopub.execute_input":"2021-10-03T21:29:03.421779Z","iopub.status.idle":"2021-10-03T21:29:04.314916Z","shell.execute_reply.started":"2021-10-03T21:29:03.421745Z","shell.execute_reply":"2021-10-03T21:29:04.313298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Test","metadata":{"ExecuteTime":{"end_time":"2021-10-02T00:20:34.613927Z","start_time":"2021-10-02T00:19:33.661Z"}}},{"cell_type":"code","source":"df1_test[feature_float].describe().style.background_gradient(cmap='YlOrRd')","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:33.875249Z","start_time":"2021-10-03T20:56:33.156718Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:04.31667Z","iopub.execute_input":"2021-10-03T21:29:04.31701Z","iopub.status.idle":"2021-10-03T21:29:05.199636Z","shell.execute_reply.started":"2021-10-03T21:29:04.31696Z","shell.execute_reply":"2021-10-03T21:29:05.19875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.1.6.1. Atributos Categóricos","metadata":{"ExecuteTime":{"end_time":"2021-10-02T00:27:06.23192Z","start_time":"2021-10-02T00:27:06.22494Z"}}},{"cell_type":"code","source":"df1_train[feature_cat].columns","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:33.891173Z","start_time":"2021-10-03T20:56:33.877211Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:05.201025Z","iopub.execute_input":"2021-10-03T21:29:05.201381Z","iopub.status.idle":"2021-10-03T21:29:05.209921Z","shell.execute_reply.started":"2021-10-03T21:29:05.201341Z","shell.execute_reply":"2021-10-03T21:29:05.209238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Análise Gráfica","metadata":{}},{"cell_type":"markdown","source":"### 1.2.1. Correlação\nVamos examinar a correlação entre as variáveis.","metadata":{}},{"cell_type":"code","source":"df = df1_train.corr().round(5)\n\n# Máscara para ocultar a parte superior direita do gráfico, pois é uma duplicata\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\n\nax.set_title(\"Mapa de calor de correlação das variável\", fontsize=17)\n\nplt.setp(ax.get_xticklabels(), \n         rotation      = 90, \n         ha            = \"right\",\n         rotation_mode = \"anchor\", \n         weight        = \"normal\")\n\nplt.setp(ax.get_yticklabels(), \n         weight        = \"normal\",\n         rotation_mode = \"anchor\", \n         rotation      = 0, \n         ha            = \"right\");","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:36.045444Z","start_time":"2021-10-03T20:56:33.893177Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:05.211068Z","iopub.execute_input":"2021-10-03T21:29:05.211946Z","iopub.status.idle":"2021-10-03T21:29:08.059374Z","shell.execute_reply.started":"2021-10-03T21:29:05.21191Z","shell.execute_reply":"2021-10-03T21:29:08.058487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <br>\n    \nComo podemos ver, a correlação está entre ~-0,02 e 0, o que é muito pequeno. Portanto, as variáveis são fracamente correlacionados e negativas\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"### 1.2.2. Distribuição","metadata":{}},{"cell_type":"markdown","source":"#### 1.2.2.1. Train / Test","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5, 5))\n\npie = ax.pie([len(df1_train), len(df1_test)],\n             labels   = [\"Train dataset\", \"Test dataset\"],\n             colors   = [\"salmon\", \"teal\"],\n             textprops= {\"fontsize\": 15},\n             autopct  = '%1.1f%%')\n\nax.axis(\"equal\")\nax.set_title(\"Comparação de comprimento do conjunto de dados \\n\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:36.18711Z","start_time":"2021-10-03T20:56:36.047418Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:08.060937Z","iopub.execute_input":"2021-10-03T21:29:08.061284Z","iopub.status.idle":"2021-10-03T21:29:08.164544Z","shell.execute_reply.started":"2021-10-03T21:29:08.061244Z","shell.execute_reply":"2021-10-03T21:29:08.163663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2.2.2. Proporção das variáveis","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5, 5))\n\nplt.pie([len(feature_cat), len(feature_float)], \n        labels=['Categorical', 'Continuos'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\n\n#ax.axis(\"equal\")\nax.set_title(\"Comparação variáveis continuas/categóricas \\n Dataset Treino/Teste\", fontsize=18)\nfig.set_facecolor('white')\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:36.281822Z","start_time":"2021-10-03T20:56:36.188071Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:08.16591Z","iopub.execute_input":"2021-10-03T21:29:08.166294Z","iopub.status.idle":"2021-10-03T21:29:08.264293Z","shell.execute_reply.started":"2021-10-03T21:29:08.166254Z","shell.execute_reply":"2021-10-03T21:29:08.263441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2.2.1. Target\nVamos ver as ocorrências de números individuais do conjunto de dados de treino.","metadata":{}},{"cell_type":"code","source":"L    = len(df1_train.columns[0:60])\nnrow = int(np.ceil(L/6))\nncol = 6\n\nremove_last= (nrow * ncol) - L\n\nfig, ax = plt.subplots(nrow, ncol,figsize=(24, 30))\nfig.subplots_adjust(top=0.95)\ni = 1\n\nfor feature in df1_train.columns[0:60]:\n    \n    plt.subplot(nrow, ncol, i)\n    \n    ax = sns.kdeplot(df1_train[feature], shade=True, color='salmon',  alpha=0.5, label='train')\n    ax = sns.kdeplot(df1_test[feature], shade=True, color='teal',  alpha=0.5, label='test')\n    plt.xlabel(feature, fontsize=9)\n    plt.legend()\n    \n    i += 1\n    \nplt.suptitle('DistPlot: train & test data', fontsize=20)\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:43.467857Z","start_time":"2021-10-03T20:56:36.283836Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:08.265809Z","iopub.execute_input":"2021-10-03T21:29:08.26611Z","iopub.status.idle":"2021-10-03T21:29:18.399559Z","shell.execute_reply.started":"2021-10-03T21:29:08.266067Z","shell.execute_reply":"2021-10-03T21:29:18.398721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <br>\n\n- Os conjuntos de treinamento e teste têm aproximadamente as mesmas distribuições em termos de variáveis; <br>\n- Temos poucas variáveis com distribuições normais; <br>\n- A maioria das variáveis tem distribuições distorcidas. <br>\n\nPrecisamos pensar em como fazer tudo isso normalmente distribuído se decidirmos usar modelos não baseados em árvore. <br>\n\n> A verificação da correlação não revelou relações significativas entre as características (a maioria estava entre -0.02  e 0).\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### 1.2.2.1. Detecção de Outlier","metadata":{}},{"cell_type":"markdown","source":"##### 1.2.2.1.1. Data Train ","metadata":{}},{"cell_type":"code","source":"df_plot = ((df1_train - df1_train.min())/(df1_train.max() - df1_train.min()))\nfig, ax = plt.subplots(4, 1, figsize = (25,25))\n\nfor i, (x) in enumerate([(1,30), (30,60), (60,90), (90,120)]): \n    sns.boxplot(data = df_plot.iloc[:, x[0]:x[1] ], ax = ax[i]);","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:45.590179Z","start_time":"2021-10-03T20:56:43.469851Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:18.400866Z","iopub.execute_input":"2021-10-03T21:29:18.401286Z","iopub.status.idle":"2021-10-03T21:29:21.834471Z","shell.execute_reply.started":"2021-10-03T21:29:18.401248Z","shell.execute_reply":"2021-10-03T21:29:21.833608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 1.2.2.1.2. Data Test","metadata":{}},{"cell_type":"code","source":"df_plot = ((df1_test - df1_test.min())/(df1_test.max() - df1_test.min()))\nfig, ax = plt.subplots(4, 1, figsize = (25,25))\n\nfor i, (x) in enumerate([(1,30), (30,60), (60,90), (90,120)]): \n    sns.boxplot(data = df_plot.iloc[:, x[0]:x[1] ], ax = ax[i]);","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:47.545948Z","start_time":"2021-10-03T20:56:45.592175Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:21.837929Z","iopub.execute_input":"2021-10-03T21:29:21.840061Z","iopub.status.idle":"2021-10-03T21:29:25.062968Z","shell.execute_reply.started":"2021-10-03T21:29:21.840005Z","shell.execute_reply":"2021-10-03T21:29:25.062274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <br>\n    \nAcima observamos que temos muitos outliers em ambos conjunto de dados, no processamento vamos trartá-los.\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### 1.2.2.3. Target\nA variável alvo tem os valores 0 e 1, vamos verificar a distribuição da target.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nax = sns.countplot(x=df1_train['target'], palette='viridis')\nax.set_title('Distribuição da variável Target', fontsize=20, y=1.05)\n\nsns.despine(right=True)\nsns.despine(offset=10, trim=True)","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:56:47.654839Z","start_time":"2021-10-03T20:56:47.547944Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:25.064262Z","iopub.execute_input":"2021-10-03T21:29:25.06501Z","iopub.status.idle":"2021-10-03T21:29:25.258962Z","shell.execute_reply.started":"2021-10-03T21:29:25.06497Z","shell.execute_reply":"2021-10-03T21:29:25.258212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <br>\n    \nPodemos observar no gráfico acima que não temos desbalanceamento nos dados. \n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"#### 1.2.2.4. Variáveis preditoras  vs Target.","metadata":{}},{"cell_type":"code","source":"L    = len(df1_train.columns[0:60])\nnrow = int(np.ceil(L/6))\nncol = 6\ni    = 1\nremove_last= (nrow * ncol) - L\n\nfig, ax = plt.subplots(nrow, ncol,figsize=(24, 30))\nfig.subplots_adjust(top=0.95)\n\nfor feature in df1_train.columns[0:60]:\n    \n    plt.subplot(nrow, ncol, i)\n    \n    ax = sns.kdeplot(df1_train[feature], \n                     shade    = True, \n                     palette  = 'viridis',  \n                     alpha    = 0.5, \n                     hue      = df1_train['target'], \n                     multiple = \"stack\")\n    \n    plt.xlabel(feature, fontsize=9)\n        \n    i += 1\n    \n    gc.collect()\n    \nplt.suptitle('DistPlot: Variável de treino vs target', fontsize=20)\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:57:00.294725Z","start_time":"2021-10-03T20:56:47.656653Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:25.260235Z","iopub.execute_input":"2021-10-03T21:29:25.260483Z","iopub.status.idle":"2021-10-03T21:29:44.232511Z","shell.execute_reply.started":"2021-10-03T21:29:25.26045Z","shell.execute_reply":"2021-10-03T21:29:44.231837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div class=\"alert alert-success\">  2. Split Train/Test </div> <br>","metadata":{}},{"cell_type":"code","source":"gc.collect()\n\nX      = df1_train.drop('target', axis=1)\ny      = df1_train['target']\nX_test = df1_test\ncols   = X.columns\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size    = 0.2,\n                                                      shuffle      = True, \n                                                      stratify     = y,\n                                                      random_state = 0)\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape , X_test.shape","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:57:00.420601Z","start_time":"2021-10-03T20:57:00.29672Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:44.233491Z","iopub.execute_input":"2021-10-03T21:29:44.233746Z","iopub.status.idle":"2021-10-03T21:29:44.442272Z","shell.execute_reply.started":"2021-10-03T21:29:44.233711Z","shell.execute_reply":"2021-10-03T21:29:44.441282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nesta etapa, treinaremos nosso modelo **XGBClassifier** de linha de base simples, em relação ao tratamento dos dados, vamos fazer apenas o scaler nesta etapa.","metadata":{}},{"cell_type":"code","source":"X_test.head()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:57:00.6081Z","start_time":"2021-10-03T20:57:00.426618Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:44.447311Z","iopub.execute_input":"2021-10-03T21:29:44.447536Z","iopub.status.idle":"2021-10-03T21:29:44.649788Z","shell.execute_reply.started":"2021-10-03T21:29:44.44751Z","shell.execute_reply":"2021-10-03T21:29:44.648961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div class=\"alert alert-success\">  2. Modelo Baseline XGB </div> <br>","metadata":{}},{"cell_type":"code","source":"%%time \nseed   = 12359\nparams = {'random_state'  : seed,          \n          'predictor'     : 'gpu_predictor',\n          'tree_method'   : 'gpu_hist',\n          'eval_metric'   : 'auc'}\n\nmodel_baseline = xgb.XGBClassifier(**params)\n\nscalers = [None, StandardScaler(), RobustScaler(), MinMaxScaler(), \n           MaxAbsScaler(), QuantileTransformer(output_distribution='normal', random_state=0)]\n\nfor scaler in scalers: \n    \n    if scaler!=None:\n        X_train_s = scaler.fit_transform(X_train)\n        X_valid_s = scaler.fit_transform(X_valid)\n    else:\n        X_train_s = X_train\n        X_valid_s = X_valid\n                \n    model_baseline.fit(X_train_s, y_train, verbose = False)\n    y_hat = model_baseline.predict_proba(X_valid_s)[:, 1]\n    \n    fpr, tpr, thresholds = metrics.roc_curve(y_valid, y_hat)\n    \n    auc = metrics.auc(fpr, tpr)\n    print('Validaçao AUC: {:2.5f} => {}'.format(auc, scaler))\n    \nprint()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:57:23.511091Z","start_time":"2021-10-03T20:57:00.612091Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:44.65107Z","iopub.execute_input":"2021-10-03T21:29:44.651429Z","iopub.status.idle":"2021-10-03T21:29:53.912464Z","shell.execute_reply.started":"2021-10-03T21:29:44.651389Z","shell.execute_reply":"2021-10-03T21:29:53.911881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <br>\n    \nCom scaler RobustScaler obtivemos uma AUC de 0.85144, como estamos fazer apenas uma validação (treino/validação), neste caso a pontuação do score pode ser afetada por aleatoriedade dos dados, sendo assim, vamos fazer uma validação cruzada para termos uma estimativa robusta.  <br>\n\nPara o treinamento do modelo foi criado a função abaixo que tem a finalidade de treinar um conjunto de scalers para um determinado modelo, durante o treinamento será exibido os resultados e no final será retornado o melhor modelo com as variáveis e seus scores de importância.\n    \n</div>","metadata":{}},{"cell_type":"code","source":"path='Data/'\n\ndef cross_val_model(model, scalers, name_file_submission, FOLDS=5, verbose=False, seed=59): \n    \n    mdl_train   = []\n    feature_imp = 0 \n    \n    for scaler in scalers: \n\n        df_submission.claim = 0\n        auc_best            = 0   \n        feature_imp_best    = 0       \n        auc                 = []\n        lloss               = []\n        f1                  = []\n        kfold               = KFold(n_splits=FOLDS, random_state=seed, shuffle=True)\n\n        if scaler!=None:\n            X_ts = scaler.fit_transform(X_test.copy())\n        else:\n            X_ts = X_test.copy()\n\n        print('='*80)\n        print('Scaler: {}'.format(scaler))\n        print('='*80)\n\n        for i, (train_idx, test_idx) in enumerate(kfold.split(X_train)):\n\n            i+=1\n\n            X_tr, y_tr = X_train.iloc[train_idx], y_train.iloc[train_idx]\n            X_vl, y_vl = X_train.iloc[test_idx], y_train.iloc[test_idx]\n\n            # Scaler\n            if scaler!=None:    \n                X_tr = scaler.fit_transform(X_tr)\n                X_vl = scaler.fit_transform(X_vl)                \n\n            eval_set     = [(X_tr,y_tr), (X_vl,y_vl)]            \n            n_estimators = model.get_params()['n_estimators'] \n\n            if n_estimators==100: \n                early_stopping_rounds = 20\n            else: \n                early_stopping_rounds = 200\n\n            model = model.fit(X_tr, y_tr, \n                              eval_set              = eval_set,\n                              early_stopping_rounds = early_stopping_rounds, \n                              verbose               = verbose,\n                             )\n            \n            best_ntree = model.best_ntree_limit\n            \n            y_hat_prob = model.predict_proba(X_vl, ntree_limit=best_ntree)[:, 1] #   \n            y_hat      = (y_hat_prob >.5).astype(int) \n\n            fpr, tpr, thresholds = metrics.roc_curve(y_vl, y_hat_prob)\n\n            log_loss_     = metrics.log_loss(y_vl, y_hat_prob)                \n            f1_score_     = metrics.f1_score(y_vl, y_hat)        \n            auc_          = metrics.auc(fpr, tpr)    \n\n            stop = ''\n            \n            if n_estimators > best_ntree: \n                stop = '*'\n                \n            msg = '[Fold {}] AUC: {:.5f} - F1: {:.5f} - L. LOSS: {:.5f} {}'\n            print(msg.format(i, auc_, f1_score_,log_loss_, stop))\n\n            # Getting mean feature importances (i.e. devided by number of splits)\n            feature_imp  += model.feature_importances_ / FOLDS\n            \n            df_submission['target'] += model.predict_proba(X_ts)[:, 1] / FOLDS\n\n            f1.append(f1_score_)\n            lloss.append(log_loss_)\n            auc.append(auc_)\n                        \n        auc_mean   = np.mean(auc)\n        auc_std    = np.std(auc)\n        lloss_mean = np.mean(lloss)\n        f1_mean    = np.mean(f1)\n        \n        if auc_mean > auc_best: \n            auc_best          = auc_mean\n            f1_best           = f1_mean\n            lloss_best        = lloss_mean\n            model_best        = model\n            feature_imp_best  = feature_imp\n            scaler_best       = scaler\n                                    \n        print('-'*80)\n        msg = '[Mean Fold] AUC: {:.5f}(Std:{:.5f}) - F1: {:.5f} - L. LOSS: {:.5f}'\n        print(msg.format(auc_mean,auc_std, f1_mean, lloss_mean))\n        print('='*80)\n        print('')\n\n        # Gerar o arquivo de submissão \n        name_file_sub = 'submission/' + name_file_submission + '_' + str(scaler).lower()[:4] + '.csv'\n        df_submission.to_csv(path + name_file_sub, index = False)\n\n        gc.collect()\n        \n    print()\n    print('='*80)\n    print('Scaler Best: {}'.format(scaler_best))\n    print('AUC        : {:2.5f}'.format(auc_best))\n    print('F1-Score   : {:2.5f}'.format(f1_best))\n    print('L. Loss    : {:2.5f}'.format(lloss_best))\n    print('='*80)\n    print()\n            \n    gc.collect()  \n    \n    return model_best, feature_imp_best ","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:59:28.778949Z","start_time":"2021-10-03T20:59:28.750027Z"},"code_folding":[],"run_control":{"marked":false},"execution":{"iopub.status.busy":"2021-10-03T21:29:53.915787Z","iopub.execute_input":"2021-10-03T21:29:53.917501Z","iopub.status.idle":"2021-10-03T21:29:53.93637Z","shell.execute_reply.started":"2021-10-03T21:29:53.917465Z","shell.execute_reply":"2021-10-03T21:29:53.935622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ngc.collect()\n\nseed    = 12359\nscalers = [None, StandardScaler(), RobustScaler(), MinMaxScaler(), \n           MaxAbsScaler(), QuantileTransformer(output_distribution='normal', random_state=0)]\n\nmodel_best, feat_imp_best = cross_val_model(xgb.XGBClassifier(**params), \n                                            scalers, \n                                            '001_xgb_baseline', \n                                            FOLDS = 5, \n                                            seed  = seed\n                                            )","metadata":{"ExecuteTime":{"end_time":"2021-10-03T21:06:21.765578Z","start_time":"2021-10-03T20:59:29.190767Z"},"execution":{"iopub.status.busy":"2021-10-03T21:29:53.937889Z","iopub.execute_input":"2021-10-03T21:29:53.938162Z","iopub.status.idle":"2021-10-03T21:30:29.481266Z","shell.execute_reply.started":"2021-10-03T21:29:53.938128Z","shell.execute_reply":"2021-10-03T21:30:29.480509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\">\n\nNa validação cruzada obtivemos uma `AUC` de `0.85093` com um desvio padrão de `0.00088` sem fazer o scaler, vamos gerar dois arquivos com as melhores pontuação de AUC para submissão. <br>\n    \n`NOTA:` <br>\n   \nNa validação cruzada que foi realizada, mostra que não fazer a normalização temos a melhor AUC, e isso é confirmado na submissão da competição, abaixo as duas submissões:   <br>\n    \n- AUC: 0.85383 => Sem normalização; <br> \n- AUC: 0.85246 => RobustScaler <br>\n    \nVamos treinar três modelos, sendo que vamos acrescentar o parametros `n_estimators` que indica o número de arvores para o treinamento do modelo, próximos notebooks vamos fazer ajuste em diversos parametros para ajusdar no score.\n    \n</div>","metadata":{"ExecuteTime":{"end_time":"2021-10-02T01:40:42.824181Z","start_time":"2021-10-02T01:40:42.813183Z"}}},{"cell_type":"code","source":"%%time\n    \nseed   = 112359    \nparams = {'random_state'  : seed,         \n          'n_estimators'  : 1000,\n          'predictor'     : 'gpu_predictor',\n          'tree_method'   : 'gpu_hist',\n          'eval_metric'   : 'auc'}\n\nscalers = [None, \n           RobustScaler(), \n           QuantileTransformer(output_distribution='normal', random_state=0)]\n\nmodel_best, feat_imp_best  = cross_val_model(xgb.XGBClassifier(**params), \n                                             scalers, \n                                             '002_xgb_bl_n_estimators_1000', \n                                             FOLDS = 5, \n                                             seed  = seed\n                                            )","metadata":{"ExecuteTime":{"end_time":"2021-10-03T21:11:16.420074Z","start_time":"2021-10-03T21:06:42.375988Z"},"code_folding":[],"execution":{"iopub.status.busy":"2021-10-03T21:30:29.482675Z","iopub.execute_input":"2021-10-03T21:30:29.482959Z","iopub.status.idle":"2021-10-03T21:31:28.874021Z","shell.execute_reply.started":"2021-10-03T21:30:29.482923Z","shell.execute_reply":"2021-10-03T21:31:28.873277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <br>\nNa submissão na competição obtivemos uma AUC de `0.85407` com o QuantileTransformer em relação ao treinamento do modelo, vamos fazer uma pequena análise do comportamento do modelo na próxima seção.\n   \n</div>","metadata":{}},{"cell_type":"markdown","source":"## 2.1. Análise do Modelo \n","metadata":{"ExecuteTime":{"end_time":"2021-10-03T01:22:04.358763Z","start_time":"2021-10-03T01:22:04.353777Z"}}},{"cell_type":"code","source":"model_best","metadata":{"ExecuteTime":{"end_time":"2021-10-03T21:11:27.889051Z","start_time":"2021-10-03T21:11:27.85908Z"},"execution":{"iopub.status.busy":"2021-10-03T21:31:28.875487Z","iopub.execute_input":"2021-10-03T21:31:28.875765Z","iopub.status.idle":"2021-10-03T21:31:28.885539Z","shell.execute_reply.started":"2021-10-03T21:31:28.875728Z","shell.execute_reply":"2021-10-03T21:31:28.884598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1.1. Número de Estimadores","metadata":{"ExecuteTime":{"end_time":"2021-10-03T03:30:07.804545Z","start_time":"2021-10-03T03:30:07.793576Z"}}},{"cell_type":"code","source":"results     = model_best.evals_result()\nntree_limit = model_best.best_ntree_limit\n\nplt.figure(figsize=(7,5))\nplt.plot(results[\"validation_0\"][\"auc\"], label=\"Treinamento\")\nplt.plot(results[\"validation_1\"][\"auc\"], label=\"Validação\")\n\n\nplt.axvline(ntree_limit, \n            color=\"gray\", \n            label=\"N. de árvore ideal {}\".format(ntree_limit))\n\nplt.xlabel(\"Número de árvores\")\nplt.ylabel(\"AUC\")\nplt.legend();","metadata":{"ExecuteTime":{"end_time":"2021-10-03T21:11:32.577881Z","start_time":"2021-10-03T21:11:31.910664Z"},"execution":{"iopub.status.busy":"2021-10-03T21:31:28.887238Z","iopub.execute_input":"2021-10-03T21:31:28.8878Z","iopub.status.idle":"2021-10-03T21:31:29.149391Z","shell.execute_reply.started":"2021-10-03T21:31:28.887764Z","shell.execute_reply":"2021-10-03T21:31:29.148703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n`NOTA:` <br>\nAcima recuperamos as informações de treinamento do nosso modelo, podemos observar que o número de 1000 estimadores é mais que suficiente para o treinamento do modelo, o ideal é que fique em torno de 75 à 95 para esses dados e com a utilização dos parametros padrões que devem ser ajustados para o `XGB`. <br>\n    \n    \nVamos agora utilizar o modelo treinado que foi retornado pela função e vamos fazer a previsão para novos dados que o modelo não viu no treinamento, para termos uma ideia da generalização do modelo, lembrando que o modelo que foi treinado utiliza 1000 estimadores (arvores), sendo assim, vamos utilizar na previssão 86 estimadores utilizando o parametro `ntree_limit` ao fazermos as previsões.\n\n</div>","metadata":{"ExecuteTime":{"end_time":"2021-10-03T17:20:26.997973Z","start_time":"2021-10-03T17:20:26.984011Z"}}},{"cell_type":"code","source":"%%time\nthreshold =.5\n\ny_pred_prob = model_best.predict_proba(X_valid, ntree_limit=ntree_limit)[:, 1] \ny_pred      = (y_pred_prob > threshold).astype(int)\n\nf1_    = metrics.f1_score(y_valid, y_pred)\nauc_   = metrics.roc_auc_score(y_valid, y_pred)\nlloss_ = metrics.log_loss(y_valid, y_pred_prob) \n    \nprint('AUC     : {:2.5f}'.format(auc_))\nprint('AUC     : {:2.5f}'.format(auc))\nprint('F1-Score: {:2.5f}'.format(f1_))\nprint('L. Loss : {:2.5f}'.format(lloss_))\nprint()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T21:11:57.061306Z","start_time":"2021-10-03T21:11:57.001475Z"},"execution":{"iopub.status.busy":"2021-10-03T21:31:29.150658Z","iopub.execute_input":"2021-10-03T21:31:29.150936Z","iopub.status.idle":"2021-10-03T21:31:29.182128Z","shell.execute_reply.started":"2021-10-03T21:31:29.150902Z","shell.execute_reply":"2021-10-03T21:31:29.180999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n`NOTA:` <br>\n    \nObservando o resultado acima nos dados de validação, o nosso modelo parece que está com um pequeno overfitting em relação ao treinamento, mesmo assim é um modelo bom, pois está generalizando pelo resultado na submissão na competição.      \n    \n</div>","metadata":{"ExecuteTime":{"end_time":"2021-10-03T19:00:53.888508Z","start_time":"2021-10-03T19:00:53.868545Z"}}},{"cell_type":"markdown","source":"### 2.1.2. Matriz de Confusão","metadata":{}},{"cell_type":"code","source":"metrics.plot_confusion_matrix(model_best, X_valid, y_valid, cmap='inferno')\nplt.title('Confusion matrix')\nplt.grid(False)\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T21:12:01.759026Z","start_time":"2021-10-03T21:12:01.431211Z"},"execution":{"iopub.status.busy":"2021-10-03T21:31:29.183279Z","iopub.execute_input":"2021-10-03T21:31:29.183543Z","iopub.status.idle":"2021-10-03T21:31:29.422945Z","shell.execute_reply.started":"2021-10-03T21:31:29.183509Z","shell.execute_reply":"2021-10-03T21:31:29.422061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold    = .5\ny_pred_valid = (y_pred_prob > threshold).astype(int)\nf1_          = metrics.f1_score (y_valid, y_pred_valid)\nauc_         = metrics.roc_auc_score(y_valid, y_pred_prob)\n\nprint(metrics.classification_report(y_valid, y_pred_valid))\nprint('')\nprint('AUC     : {:2.5f}'.format(auc_))\nprint('F1-score: {:2.5f}'.format(f1_))\n","metadata":{"ExecuteTime":{"end_time":"2021-10-03T21:12:02.904845Z","start_time":"2021-10-03T21:12:02.878916Z"},"execution":{"iopub.status.busy":"2021-10-03T21:31:29.424099Z","iopub.execute_input":"2021-10-03T21:31:29.424345Z","iopub.status.idle":"2021-10-03T21:31:29.440965Z","shell.execute_reply.started":"2021-10-03T21:31:29.424312Z","shell.execute_reply":"2021-10-03T21:31:29.440316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n**`NOTA:`** <br>\n   \n    \n</div>","metadata":{"ExecuteTime":{"end_time":"2021-10-03T19:48:12.103818Z","start_time":"2021-10-03T19:48:12.089857Z"}}},{"cell_type":"markdown","source":"### 2.1.3 Curva ROC","metadata":{}},{"cell_type":"code","source":"fpr, tpr, thresholds = metrics. roc_curve(y_valid, y_pred)\n\nplot_roc_curve(fpr, tpr, label=\"XGB\")\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T21:12:08.546526Z","start_time":"2021-10-03T21:12:08.39593Z"},"execution":{"iopub.status.busy":"2021-10-03T21:31:29.442245Z","iopub.execute_input":"2021-10-03T21:31:29.442505Z","iopub.status.idle":"2021-10-03T21:31:29.645067Z","shell.execute_reply.started":"2021-10-03T21:31:29.442472Z","shell.execute_reply":"2021-10-03T21:31:29.644305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n`NOTA:` <br>\n   \n    \n</div>\n\n### 2.2.4. Feature Importances  ","metadata":{"ExecuteTime":{"end_time":"2021-10-03T20:05:43.225662Z","start_time":"2021-10-03T20:05:43.208709Z"}}},{"cell_type":"code","source":"feature_imp_ = feat_imp_best","metadata":{"ExecuteTime":{"end_time":"2021-10-03T21:12:13.468354Z","start_time":"2021-10-03T21:12:13.45934Z"},"execution":{"iopub.status.busy":"2021-10-03T21:31:29.646391Z","iopub.execute_input":"2021-10-03T21:31:29.646651Z","iopub.status.idle":"2021-10-03T21:31:29.650622Z","shell.execute_reply.started":"2021-10-03T21:31:29.646618Z","shell.execute_reply":"2021-10-03T21:31:29.649703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df               = pd.DataFrame()\ndf[\"Feature\"]    = X.columns\ndf[\"Importance\"] = feature_imp_ / feature_imp_.sum()\n\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)","metadata":{"ExecuteTime":{"end_time":"2021-10-03T21:12:14.409034Z","start_time":"2021-10-03T21:12:14.39708Z"},"execution":{"iopub.status.busy":"2021-10-03T21:31:29.651944Z","iopub.execute_input":"2021-10-03T21:31:29.652532Z","iopub.status.idle":"2021-10-03T21:31:29.66272Z","shell.execute_reply.started":"2021-10-03T21:31:29.652386Z","shell.execute_reply":"2021-10-03T21:31:29.661937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13, 70))\nbars    = ax.barh(df[\"Feature\"], \n                  df[\"Importance\"], \n                  height    = 0.4,\n                  color     = \"mediumorchid\", \n                  edgecolor = \"black\")\n\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature name\", fontsize=20, labelpad=15)\n#ax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(df[\"Feature\"])\nax.set_yticklabels(df[\"Feature\"], fontsize=13)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\n\n# Adicionando rótulos na parte superior\nax2 = ax.secondary_xaxis('top')\n#ax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=13)\nax2.tick_params(axis=\"x\", labelsize=15)\nax.margins(0.05, 0.01)\n\n# Inverter a direção do eixo y \nplt.gca().invert_yaxis()","metadata":{"ExecuteTime":{"end_time":"2021-10-03T21:12:18.060472Z","start_time":"2021-10-03T21:12:15.516733Z"},"execution":{"iopub.status.busy":"2021-10-03T21:31:29.663707Z","iopub.execute_input":"2021-10-03T21:31:29.663979Z","iopub.status.idle":"2021-10-03T21:31:33.697551Z","shell.execute_reply.started":"2021-10-03T21:31:29.663946Z","shell.execute_reply":"2021-10-03T21:31:33.6969Z"},"trusted":true},"execution_count":null,"outputs":[]}]}