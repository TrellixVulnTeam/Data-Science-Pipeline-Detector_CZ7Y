{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cudf\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n%matplotlib inline\nimport plotly.express as px\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-22T11:02:45.478426Z","iopub.execute_input":"2021-10-22T11:02:45.479177Z","iopub.status.idle":"2021-10-22T11:02:50.614066Z","shell.execute_reply.started":"2021-10-22T11:02:45.479086Z","shell.execute_reply":"2021-10-22T11:02:50.611165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"%%time\ntrain = cudf.read_csv('../input/tabular-playground-series-oct-2021/train.csv', index_col=0)\ntest = cudf.read_csv('../input/tabular-playground-series-oct-2021/test.csv', index_col=0)\n\nsample_submission = cudf.read_csv(\"../input/tabular-playground-series-oct-2021/sample_submission.csv\").to_pandas()\n\nmemory_usage = train.memory_usage(deep=True) / 1024 ** 2\nstart_mem = memory_usage.sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:02:50.61567Z","iopub.execute_input":"2021-10-22T11:02:50.615931Z","iopub.status.idle":"2021-10-22T11:03:32.119739Z","shell.execute_reply.started":"2021-10-22T11:02:50.615896Z","shell.execute_reply":"2021-10-22T11:03:32.118967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_cols = test.columns.tolist()\n\ncnt_features =[]\ncat_features =[]\n\nfor col in feature_cols:\n    if train[col].dtype=='float64':\n        cnt_features.append(col)\n    else:\n        cat_features.append(col)\n        \n\ntrain[cnt_features] = train[cnt_features].astype('float32')\ntrain[cat_features] = train[cat_features].astype('uint8')\n\ntest[cnt_features] = test[cnt_features].astype('float32')\ntest[cat_features] = test[cat_features].astype('uint8')\n\nmemory_usage = train.memory_usage(deep=True) / 1024 ** 2\nend_mem = memory_usage.sum()\n\ntrain = train.to_pandas()\ntest = test.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:03:32.121116Z","iopub.execute_input":"2021-10-22T11:03:32.121634Z","iopub.status.idle":"2021-10-22T11:03:34.176141Z","shell.execute_reply.started":"2021-10-22T11:03:32.12159Z","shell.execute_reply":"2021-10-22T11:03:34.175383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classic Feature Importance\n\nThe easiest way to **determine the magnitude of importance of each feature**, is to remove each feature, and then train a model to see how much accuracy drops without that feature. A big shortcoming of this solution is that you have to train a model for each feature and this is a resource and time consuming task. \n\nHowever I have provided the results of training for a simple **XGBoost** model (with default parameter values) with removing each feature. You can find the results [here](https://www.kaggle.com/kavehshahhosseini/tpsoctclassicfeatureimportance).\n\nThe results may be different from other ways such as Shapely values, permutation importance, model feature importance and...\n\nI've provided a sample code of how it's been done. You can see it by clicking on \"Show Hidden Cell\".","metadata":{}},{"cell_type":"code","source":"# %%time\n# # Here is a sample code for training a simple xgboost model with removing each feature one by one.\n# x_train, x_valid, y_train, y_valid = train_test_split(train[feature_cols], train[\"target\"], test_size=0.2, random_state=42)\n# scores = {}\n# feature_cols.insert(0,\"all\")\n\n# for col in feature_cols:\n#     feat = feature_cols.copy()\n#     feat.remove(col)\n#     if \"all\" in feat:\n#         feat.remove(\"all\")\n#     x_t = x_train[feat]\n#     x_v = x_valid[feat]\n\n#     xgb_params = {\n#         'eval_metric': 'auc',\n#         'objective': 'binary:logistic', \n#         'tree_method': 'gpu_hist', \n#         'predictor': 'gpu_predictor', \n#         'seed': 42, \n#         'use_label_encoder': False,\n#     }\n    \n#     xgb_model = XGBClassifier(**xgb_params)\n#     xgb_model.fit(x_t, y_train, eval_set=[(x_v, y_valid)], verbose=False)\n    \n#     preds_valid = xgb_model.predict_proba(x_v)[:,1]\n#     auc = roc_auc_score(y_valid, preds_valid)\n#     print(f\"{col},{auc}\", end=\"\\t\")\n#     scores.update({col:auc})\n    \n# df = pd.Series(scores, name=\"xgb_scores\")\n# df.to_csv(\"xgboost.csv\", index_label=\"feature\")\n# print(\"AVG AUC:\",np.mean(df.values))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-10-22T11:03:34.185523Z","iopub.execute_input":"2021-10-22T11:03:34.185976Z","iopub.status.idle":"2021-10-22T11:03:34.194821Z","shell.execute_reply.started":"2021-10-22T11:03:34.185939Z","shell.execute_reply":"2021-10-22T11:03:34.193765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi = pd.read_csv(\"../input/tpsoctclassicfeatureimportance/xgboost.csv\").set_index(\"feature\")\nfi[\"importance\"] = fi.loc[\"all\",\"xgb_scores\"] - fi[\"xgb_scores\"]\nfi = fi.sort_values(ascending=False, by=\"importance\")\nfi.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:03:34.1961Z","iopub.execute_input":"2021-10-22T11:03:34.19648Z","iopub.status.idle":"2021-10-22T11:03:34.249398Z","shell.execute_reply.started":"2021-10-22T11:03:34.196443Z","shell.execute_reply":"2021-10-22T11:03:34.248475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results\n* We can see some features have **negative effect** on the model and some of them have no effect.  ","metadata":{}},{"cell_type":"code","source":"fig = px.bar(fi, y=fi[\"importance\"], x=fi.index)\nfig.update_layout(\n    title=f\"Feature Importance\",\n    xaxis_title=\"Features\",\n    yaxis_title=\"Importance\",\n    yaxis={'categoryorder':'total descending'},\n    colorway=[\"blue\"]\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:03:34.250811Z","iopub.execute_input":"2021-10-22T11:03:34.251417Z","iopub.status.idle":"2021-10-22T11:03:35.360154Z","shell.execute_reply.started":"2021-10-22T11:03:34.25137Z","shell.execute_reply":"2021-10-22T11:03:35.358054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which features made accuracy worse?","metadata":{}},{"cell_type":"code","source":"neg_features = fi[fi.importance < 0].index\nprint(neg_features.tolist())","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:03:35.361528Z","iopub.execute_input":"2021-10-22T11:03:35.361915Z","iopub.status.idle":"2021-10-22T11:03:35.36854Z","shell.execute_reply.started":"2021-10-22T11:03:35.361872Z","shell.execute_reply":"2021-10-22T11:03:35.367659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(fi, y=fi[fi.importance < 0][\"importance\"], x=fi[fi.importance < 0].index)\nfig.update_layout(\n    title=f\"Feature Importance\",\n    xaxis_title=\"Features\",\n    yaxis_title=\"Importance\",\n    yaxis={'categoryorder':'total descending'},\n    colorway=[\"blue\"]\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:11:19.018115Z","iopub.execute_input":"2021-10-22T11:11:19.018376Z","iopub.status.idle":"2021-10-22T11:11:19.088648Z","shell.execute_reply.started":"2021-10-22T11:11:19.018349Z","shell.execute_reply":"2021-10-22T11:11:19.087936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which features made accuracy better?","metadata":{}},{"cell_type":"code","source":"pos_features = fi[fi.importance > 0].index\nprint(pos_features.tolist())","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:03:35.369706Z","iopub.execute_input":"2021-10-22T11:03:35.370638Z","iopub.status.idle":"2021-10-22T11:03:35.380477Z","shell.execute_reply.started":"2021-10-22T11:03:35.3706Z","shell.execute_reply":"2021-10-22T11:03:35.379592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(fi, y=fi[fi.importance > 0][\"importance\"], x=fi[fi.importance > 0].index)\nfig.update_layout(\n    title=f\"Feature Importance\",\n    xaxis_title=\"Features\",\n    yaxis_title=\"Importance\",\n    yaxis={'categoryorder':'total descending'},\n    colorway=[\"blue\"]\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:10:39.39925Z","iopub.execute_input":"2021-10-22T11:10:39.399511Z","iopub.status.idle":"2021-10-22T11:10:39.464748Z","shell.execute_reply.started":"2021-10-22T11:10:39.399482Z","shell.execute_reply":"2021-10-22T11:10:39.463949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the order of importance may remain the same with different models, but the magnitude may change. For example with a good model configuration all features with negative effect may have a positive effect, but they will impact less, in comparison with the positive columns. ","metadata":{}}]}