{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello There,\n\nin this TPS I combined a lot of notebooks i've read and code from previous TPSs to create this.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:22:59.756503Z","iopub.execute_input":"2021-10-04T08:22:59.756819Z","iopub.status.idle":"2021-10-04T08:22:59.849038Z","shell.execute_reply.started":"2021-10-04T08:22:59.756735Z","shell.execute_reply":"2021-10-04T08:22:59.848321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I load the data using pandas read_csv method. A better way would be to create a feather dataset... TBC","metadata":{}},{"cell_type":"code","source":"%%time\n# read dataframe\ndf_train = pd.read_csv(\"../input/tabular-playground-series-oct-2021/train.csv\")\ndf_test  = pd.read_csv(\"../input/tabular-playground-series-oct-2021/test.csv\")\n\nsample_submission = pd.read_csv(\"../input/tabular-playground-series-oct-2021/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:23:00.271189Z","iopub.execute_input":"2021-10-04T08:23:00.271889Z","iopub.status.idle":"2021-10-04T08:24:22.539393Z","shell.execute_reply.started":"2021-10-04T08:23:00.271853Z","shell.execute_reply":"2021-10-04T08:24:22.538602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\n\nI add some mean, std, min, max columns. Useless features will be removed by the feature selection step later.","metadata":{}},{"cell_type":"code","source":"num_cols = [col for col in df_test.columns]\n\ndf_train[\"mean\"] = df_train[num_cols].mean(axis=1)\ndf_train[\"std\"]  = df_train[num_cols].std(axis=1)\ndf_train[\"min\"]  = df_train[num_cols].min(axis=1)\ndf_train[\"max\"]  = df_train[num_cols].max(axis=1)\n\ndf_test[\"mean\"] = df_test[num_cols].mean(axis=1)\ndf_test[\"std\"]  = df_test[num_cols].std(axis=1)\ndf_test[\"min\"]  = df_test[num_cols].min(axis=1)\ndf_test[\"max\"]  = df_test[num_cols].max(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:24:22.540871Z","iopub.execute_input":"2021-10-04T08:24:22.541576Z","iopub.status.idle":"2021-10-04T08:24:42.33409Z","shell.execute_reply.started":"2021-10-04T08:24:22.54154Z","shell.execute_reply":"2021-10-04T08:24:42.333311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the dataset is verrrrry large I tend to use this function a lot. It converts each column to the best fitting datatype for its range.","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n    \n            # test if column can be converted to an integer\n            asint = props[col].astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return props","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:24:42.335524Z","iopub.execute_input":"2021-10-04T08:24:42.335792Z","iopub.status.idle":"2021-10-04T08:24:42.349874Z","shell.execute_reply.started":"2021-10-04T08:24:42.335741Z","shell.execute_reply":"2021-10-04T08:24:42.349133Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ndf_train = reduce_mem_usage(df_train)\ndf_test  = reduce_mem_usage(df_test)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:24:42.352012Z","iopub.execute_input":"2021-10-04T08:24:42.352253Z","iopub.status.idle":"2021-10-04T08:26:29.141123Z","shell.execute_reply.started":"2021-10-04T08:24:42.352223Z","shell.execute_reply":"2021-10-04T08:26:29.14047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare dataframe for modeling\nX = df_train.drop(columns=[\"id\", \"target\"]).copy()\ny = df_train[\"target\"].copy()\n\ntest_data = df_test.drop(columns=[\"id\"]).copy()","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:26:29.142437Z","iopub.execute_input":"2021-10-04T08:26:29.142682Z","iopub.status.idle":"2021-10-04T08:26:31.118552Z","shell.execute_reply.started":"2021-10-04T08:26:29.142652Z","shell.execute_reply":"2021-10-04T08:26:31.117771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here i find the columns with less than 5 unique values and add them to the list of categorical columns.","metadata":{}},{"cell_type":"code","source":"cat_cols = X.columns[(X.nunique() < 5)]\ncon_cols = X.columns[(X.nunique() >= 5)]\ncat_cols_indices = [X.columns.get_loc(col) for col in cat_cols]\nprint(f\"cat_cols: {cat_cols}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:26:31.119749Z","iopub.execute_input":"2021-10-04T08:26:31.120029Z","iopub.status.idle":"2021-10-04T08:26:47.469231Z","shell.execute_reply.started":"2021-10-04T08:26:31.119996Z","shell.execute_reply":"2021-10-04T08:26:47.467832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scale all non-categorical columns","metadata":{}},{"cell_type":"code","source":"import gc\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX[con_cols] = reduce_mem_usage(pd.DataFrame(columns=con_cols, data=scaler.fit_transform(X[con_cols])))\ntest_data[con_cols] = reduce_mem_usage(pd.DataFrame(columns=con_cols, data=scaler.transform(test_data[con_cols])))\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:26:47.470598Z","iopub.execute_input":"2021-10-04T08:26:47.470864Z","iopub.status.idle":"2021-10-04T08:28:39.299455Z","shell.execute_reply.started":"2021-10-04T08:26:47.470831Z","shell.execute_reply":"2021-10-04T08:28:39.298759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ncal_X_train, cal_X_val, cal_y_train, cal_y_val = train_test_split(X, y, random_state=0, stratify=y, test_size=.95)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:28:39.30093Z","iopub.execute_input":"2021-10-04T08:28:39.30141Z","iopub.status.idle":"2021-10-04T08:28:41.304609Z","shell.execute_reply.started":"2021-10-04T08:28:39.301374Z","shell.execute_reply":"2021-10-04T08:28:41.303883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna Studies","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ncal_X_train, cal_X_val, cal_y_train, cal_y_val = train_test_split(X, y, random_state=0, stratify=y, test_size=.75)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:28:41.306087Z","iopub.execute_input":"2021-10-04T08:28:41.306399Z","iopub.status.idle":"2021-10-04T08:28:43.549793Z","shell.execute_reply.started":"2021-10-04T08:28:41.30636Z","shell.execute_reply":"2021-10-04T08:28:43.549059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM\n\nI use the lgbm integration to do this study. Nothing special here. Remove my fixed parameters to tune yourself.","metadata":{}},{"cell_type":"code","source":"import json\nimport optuna\nimport lightgbm as lgbm\nimport optuna.integration.lightgbm as lgbo\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nlgbm_params_0 = {\n  \"objective\": \"binary\",\n  \"metric\": \"auc\",\n  \"learning_rate\": 0.08,\n  \"device\": \"gpu\",\n  \"verbose\": 0, \n  \"feature_pre_filter\": False, \n  \"lambda_l1\": 9.314037635261775, \n  \"lambda_l2\": 0.10613573572440353,\n  \"num_leaves\": 7,\n  \"feature_fraction\": 0.4, \n  \"bagging_fraction\": 0.8391963650875751, \n  \"bagging_freq\": 5, \n  \"min_child_samples\": 100,\n  \"num_iterations\": 10000,\n  \"n_estimators\": 20000,\n  \"random_state\": 42\n}\n\nif lgbm_params_0 is None:\n    lgb_train = lgbm.Dataset(cal_X_train, cal_y_train, categorical_feature=cat_cols_indices)\n    lgb_valid = lgbm.Dataset(cal_X_val,   cal_y_val, categorical_feature=cat_cols_indices)\n\n    model = lgbo.train(\n        {\n            \"objective\": \"binary\",\n            \"metric\": \"auc\", \n            \"categorical_feature\": cat_cols_indices,\n            \"n_estimators\": 10_000, \n            \"learning_rate\": 0.08, \n            \"device\": \"gpu\", \n            \"verbose\": 0\n        }, \n        lgb_train, \n        valid_sets=[lgb_valid], \n        verbose_eval=False, \n        num_boost_round=100, \n        verbosity=0, \n        early_stopping_rounds=5, \n        optuna_seed=42\n    )\n\n    lgbm_params_0 = model.params\n\n    lgbm_params_0[\"n_estimators\"] = 20_000\n    lgbm_params_0[\"random_state\"] = 42\n\n    del lgbm_params_0[\"early_stopping_round\"]\n\nwith open(\"lgbm_params_0.json\".format(), \"w\") as file:\n    file.write(json.dumps(lgbm_params_0, indent=4))","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:28:43.552286Z","iopub.execute_input":"2021-10-04T08:28:43.552536Z","iopub.status.idle":"2021-10-04T08:28:45.804429Z","shell.execute_reply.started":"2021-10-04T08:28:43.552505Z","shell.execute_reply":"2021-10-04T08:28:45.803625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I create another params preset with different seed, i plan on using another study here in the future, with different boosting or something like that.","metadata":{}},{"cell_type":"code","source":"import json\n\nlgbm_params_1 = lgbm_params_0.copy()\n\nlgbm_params_1[\"random_state\"] = 187\n\n\nwith open(\"lgbm_params_1.json\".format(), \"w\") as file:\n    file.write(json.dumps(lgbm_params_1, indent=4))","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:28:45.805799Z","iopub.execute_input":"2021-10-04T08:28:45.806216Z","iopub.status.idle":"2021-10-04T08:28:45.811817Z","shell.execute_reply.started":"2021-10-04T08:28:45.806177Z","shell.execute_reply":"2021-10-04T08:28:45.811116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nlgbm_params_2 = lgbm_params_0.copy()\n\nlgbm_params_2[\"random_state\"] = 256\n\n\nwith open(\"lgbm_params_2.json\".format(), \"w\") as file:\n    file.write(json.dumps(lgbm_params_2, indent=4))","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:51:34.402654Z","iopub.execute_input":"2021-10-04T12:51:34.403212Z","iopub.status.idle":"2021-10-04T12:51:34.408726Z","shell.execute_reply.started":"2021-10-04T12:51:34.403175Z","shell.execute_reply":"2021-10-04T12:51:34.407725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Catboost\n\nThe barriers for the searchspace of this study where narrowed down by previous searches. I used the plot below to determine promising barriers. Remove my fixed parameters to tune yourself.","metadata":{}},{"cell_type":"code","source":"import json\nimport optuna\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier, Pool\n\ndef objective(trial):\n    param = {\n        \"objective\": \"CrossEntropy\",\n        \"eval_metric\" : \"AUC\",\n        \"task_type\": \"GPU\",\n        \"grow_policy\": \"SymmetricTree\",\n        \"use_best_model\" : True,\n        \"learning_rate\": 0.08,\n        \"n_estimators\":  10_000,\n        \"random_strength\" : trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n        \"max_bin\": 128,\n        \"cat_features\": cat_cols_indices,\n        \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-5, 1.0),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 300),\n        \"random_state\": 42,\n    }\n    \n    model = CatBoostClassifier(**param)  \n    \n    model.fit(\n        cal_train_pool,\n        eval_set=[cal_val_pool],\n        early_stopping_rounds=5,\n        verbose=False\n    )\n    \n    preds = model.predict_proba(cal_X_val)[:,-1]\n    \n    return roc_auc_score(cal_y_val, preds)\n\n\ncatb_params_0 = {\n    \"objective\": \"CrossEntropy\",\n    \"eval_metric\" : \"AUC\",\n    \"task_type\": \"GPU\",\n    \"grow_policy\": \"SymmetricTree\",\n    \"use_best_model\" : True,\n    \"learning_rate\": 0.01,\n    \"n_estimators\":  20_000,\n    \"random_strength\" : 1.0,\n    \"max_bin\": 128,\n    \"l2_leaf_reg\": 0.0007202715557592255,\n    \"max_depth\": 4,\n    \"min_data_in_leaf\": 103,\n    \"random_state\": 42,\n}\n\nif catb_params_0 is None:\n    cal_train_pool = Pool(cal_X_train, cal_y_train, cat_features=cat_cols_indices)\n    cal_val_pool   = Pool(cal_X_val, cal_y_val, cat_features=cat_cols_indices)\n\n    cal_train_pool.quantize(max_bin=128)\n    cal_val_pool.quantize(max_bin=128)\n    \n    study = optuna.create_study(\n        direction=\"maximize\",\n        pruner=optuna.pruners.MedianPruner(\n            n_startup_trials=5, n_warmup_steps=10, interval_steps=5\n        ),\n    )\n    study.optimize(objective, n_trials=20)\n    print(\"Number of finished trials:\", len(study.trials))\n    print(\"Best trial:\", study.best_trial.params)\n\n    catb_params_0 = study.best_trial.params\n\n    catb_params_0[\"n_estimators\"] = 20_000\n    catb_params_0[\"learning_rate\"] = 0.01\n    catb_params_0[\"max_bin\"] = 128\n    catb_params_0[\"random_strength\"] = 1.0\n    catb_params_0[\"random_state\"] = 42\n    catb_params_0[\"use_best_model\"] = True\n    catb_params_0[\"objective\"] = \"CrossEntropy\"\n    catb_params_0[\"grow_policy\"] = \"SymmetricTree\"\n    catb_params_0[\"eval_metric\"] = \"AUC\"\n    catb_params_0[\"task_type\"] = \"GPU\"\n    \n    del cal_train_pool\n    del cal_val_pool\n    \n    fig = optuna.visualization.plot_parallel_coordinate(study)\n    fig.show()\n\nwith open(\"catb_params_0.json\".format(), \"w\") as file:\n    file.write(json.dumps(catb_params_0, indent=4))","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:28:45.813206Z","iopub.execute_input":"2021-10-04T08:28:45.813829Z","iopub.status.idle":"2021-10-04T08:28:45.994143Z","shell.execute_reply.started":"2021-10-04T08:28:45.813797Z","shell.execute_reply":"2021-10-04T08:28:45.993372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I try to collect the pools here:","metadata":{}},{"cell_type":"code","source":"import gc\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:28:45.99534Z","iopub.execute_input":"2021-10-04T08:28:45.995689Z","iopub.status.idle":"2021-10-04T08:28:46.117342Z","shell.execute_reply.started":"2021-10-04T08:28:45.995654Z","shell.execute_reply":"2021-10-04T08:28:46.116678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I create another params preset with diffrent seed, i plan on using another study here in the future, with different boosting or something like that.","metadata":{}},{"cell_type":"code","source":"import json\n\ncatb_params_1 = catb_params_0.copy()\n\ncatb_params_1[\"random_state\"] = 187\n\n\nwith open(\"catb_params_1.json\".format(), \"w\") as file:\n    file.write(json.dumps(catb_params_1, indent=4))","metadata":{"execution":{"iopub.status.busy":"2021-10-04T08:28:46.118673Z","iopub.execute_input":"2021-10-04T08:28:46.119149Z","iopub.status.idle":"2021-10-04T08:28:46.126102Z","shell.execute_reply.started":"2021-10-04T08:28:46.119116Z","shell.execute_reply":"2021-10-04T08:28:46.125288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\ncatb_params_2 = catb_params_0.copy()\n\ncatb_params_2[\"random_state\"] = 256\n\n\nwith open(\"catb_params_2.json\".format(), \"w\") as file:\n    file.write(json.dumps(catb_params_2, indent=4))","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:51:30.140056Z","iopub.execute_input":"2021-10-04T12:51:30.140682Z","iopub.status.idle":"2021-10-04T12:51:30.145997Z","shell.execute_reply.started":"2021-10-04T12:51:30.140648Z","shell.execute_reply":"2021-10-04T12:51:30.144973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost\n\nAs with Catboost, the barriers for the searchspace of this study where also narrowed down by previous searches. I will look into more tunable params and diffrent boosting methods. Remove my fixed parameters to tune yourself.","metadata":{}},{"cell_type":"code","source":"import json\nfrom xgboost import XGBClassifier\n\ndef objective(trial):\n    param = {\n        \"objective\": \"binary:logistic\",\n        \"eval_metric\": \"auc\",\n        \"tree_method\": \"gpu_hist\",\n        \"learning_rate\": 0.02,\n        \"n_estimators\": 10_000,\n        \"random_state\": 42,\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.5, 0.8),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 2e-3, 0.008),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.51, 0.55),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 0.65),\n        \"max_depth\": 17, # trial.suggest_int(\"max_depth\", 17, 18, 1),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 150, 200),\n    }\n    model = XGBClassifier(**param)  \n    \n    model.fit(\n        cal_X_train, \n        cal_y_train,\n        eval_set=[(cal_X_val, cal_y_val)],\n        early_stopping_rounds=5,\n        verbose=False\n    )\n    \n    preds = model.predict_proba(cal_X_val)[:,-1]\n    \n    return roc_auc_score(cal_y_val, preds)\n\nxgb_params_0 = {\n    \"lambda\": 0.5397422302447832,\n    \"alpha\": 0.007483070716022332,\n    \"colsample_bytree\": 0.5400956175261262,\n    \"subsample\": 0.50044109494562,\n    \"min_child_weight\": 200,\n    \"n_estimators\": 20_000,\n    \"random_state\": 42,\n    \"learning_rate\": 0.005,\n    \"tree_method\": \"gpu_hist\",\n    \"eval_metric\": \"auc\",\n    \"objective\": \"binary:logistic\"\n}\n\nif xgb_params_0 is None:\n    study = optuna.create_study(\n        direction=\"maximize\",\n        pruner=optuna.pruners.MedianPruner(\n            n_startup_trials=5, n_warmup_steps=10, interval_steps=5\n        ),\n    )\n    study.optimize(objective, n_trials=50)\n    print(\"Number of finished trials:\", len(study.trials))\n    print(\"Best trial:\", study.best_trial.params)\n\n    xgb_params_0 = study.best_trial.params\n\n    xgb_params_0[\"n_estimators\"] = 20_000\n    xgb_params_0[\"random_state\"] = 42\n    xgb_params_0[\"learning_rate\"] = 0.005\n    xgb_params_0[\"tree_method\"] = \"gpu_hist\"\n    xgb_params_0[\"eval_metric\"] = \"auc\"\n    xgb_params_0[\"objective\"] = \"binary:logistic\"\n    \n    fig = optuna.visualization.plot_parallel_coordinate(study)\n    fig.show()\n\nwith open(\"xgb_params_0.json\".format(), \"w\") as file:\n    file.write(json.dumps(xgb_params_0, indent=4))","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:05:04.555646Z","iopub.execute_input":"2021-10-04T12:05:04.556296Z","iopub.status.idle":"2021-10-04T12:44:27.925742Z","shell.execute_reply.started":"2021-10-04T12:05:04.55625Z","shell.execute_reply":"2021-10-04T12:44:27.925002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nxgb_params_1 = xgb_params_0.copy()\n\nxgb_params_1[\"random_state\"] = 187\n\n\nwith open(\"xgb_params_1.json\".format(), \"w\") as file:\n    file.write(json.dumps(xgb_params_1, indent=4))","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:51:21.116268Z","iopub.execute_input":"2021-10-04T12:51:21.11708Z","iopub.status.idle":"2021-10-04T12:51:21.123151Z","shell.execute_reply.started":"2021-10-04T12:51:21.117037Z","shell.execute_reply":"2021-10-04T12:51:21.122381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params_2 = {\n    \"objective\": \"binary:logistic\",\n    \"learning_rate\": 8e-3,\n    \"seed\": 42,\n    \"subsample\": 0.6,\n    \"colsample_bylevel\": 0.9,\n    \"colsample_bytree\": 0.4,\n    \"n_estimators\": 20_000,\n    \"max_depth\": 8,\n    \"alpha\": 64,\n    \"lambda\": 32,\n    \"min_child_weight\": 8,\n    \"importance_type\": \"total_gain\",\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n}","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:51:50.544488Z","iopub.execute_input":"2021-10-04T12:51:50.544754Z","iopub.status.idle":"2021-10-04T12:51:50.551628Z","shell.execute_reply.started":"2021-10-04T12:51:50.544726Z","shell.execute_reply":"2021-10-04T12:51:50.549536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Level 1 - LGBM/CatB/XGB\n\nI do an KFold on all models with the previously tuned hyperparams.","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nmodels = [\n    (\"lgbm0\", LGBMClassifier(**lgbm_params_0)),\n    (\"lgbm1\", LGBMClassifier(**lgbm_params_1)),\n    (\"lgbm2\", LGBMClassifier(**lgbm_params_2)),\n    (\"catb0\", CatBoostClassifier(**catb_params_0)),\n    (\"catb1\", CatBoostClassifier(**catb_params_1)),\n    (\"catb2\", CatBoostClassifier(**catb_params_2)),\n    (\"xgb0\", XGBClassifier(**xgb_params_0)),\n    (\"xgb1\", XGBClassifier(**xgb_params_1)),\n    (\"xgb2\", XGBClassifier(**xgb_params_2)),\n]\n\noof_pred_tmp = dict()\ntest_pred_tmp = dict()\nscores_tmp = dict()\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    for name, model in models:\n        if name not in scores_tmp:\n            oof_pred_tmp[name] = list()\n            oof_pred_tmp[\"y_valid\"] = list()\n            test_pred_tmp[name] = list()\n            scores_tmp[name] = list()\n     \n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_valid,y_valid)],\n            early_stopping_rounds=500,\n            verbose=0\n        )\n        \n        pred_valid = model.predict_proba(X_valid)[:, -1]\n        score = roc_auc_score(y_valid, pred_valid)\n        \n        scores_tmp[name].append(score)\n        oof_pred_tmp[name].extend(pred_valid)\n        \n        print(f\"Fold: {fold + 1} Model: {name} Score: {score}\")\n        print(\"--\"*20)\n        \n        y_hat = model.predict_proba(test_data)[:, -1]\n        test_pred_tmp[name].append(y_hat)\n    oof_pred_tmp[\"y_valid\"].extend(y_valid)\n\nfor name, model in models:\n    print(f\"Overall Validation Score | {name}: {np.mean(scores_tmp[name])}\")\n    print(\"::\"*20)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T13:00:20.602305Z","iopub.execute_input":"2021-10-04T13:00:20.602858Z","iopub.status.idle":"2021-10-04T13:06:14.569169Z","shell.execute_reply.started":"2021-10-04T13:00:20.602817Z","shell.execute_reply":"2021-10-04T13:06:14.568463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create df with base predictions on test_data\nbase_test_predictions = pd.DataFrame(\n    {name: np.mean(np.column_stack(test_pred_tmp[name]), axis=1) \n    for name in test_pred_tmp.keys()}\n)\n\n# save csv checkpoint\nbase_test_predictions.to_csv(\"./base_test_predictions.csv\", index=False)\n\n# create simple average blend \nbase_test_predictions[\"simple_avg\"] = base_test_predictions.mean(axis=1)\n\n# create submission file with simple blend average\nsimple_blend_submission = sample_submission.copy()\nsimple_blend_submission[\"claim\"] = base_test_predictions[\"simple_avg\"]\nsimple_blend_submission.to_csv(\"./simple_blend_submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create training set for meta learner based on the oof_predictions of the base models\noof_predictions = pd.DataFrame(\n    {name:oof_pred_tmp[name] for name in oof_pred_tmp.keys()}\n)\n\n# save csv checkpoint\noof_predictions.to_csv(\"./oof_predictions.csv\", index=False)\n\n# get simple blend validation score\ny_valid = oof_predictions[\"y_valid\"].copy()\ny_hat_blend = oof_predictions.drop(columns=[\"y_valid\"]).mean(axis=1)\nscore = roc_auc_score(y_valid, y_hat_blend)\n\nprint(f\"Overall Validation Score | Simple Blend: {score}\")\nprint(\"::\"*20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lvl 2 - Logistic Regression","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.linear_model import LogisticRegression\n\n# prepare meta_training set\nX_meta = oof_predictions.drop(columns=[\"y_valid\"]).copy()\ny_meta = oof_predictions[\"y_valid\"].copy()\ntest_meta = base_test_predictions.drop(columns=[\"simple_avg\"]).copy()\n\nmeta_pred_tmp = []\nscores_tmp = []\n\n# create cv\nkf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X_meta, y_meta)):\n    # create train, validation sets\n    X_train, y_train = X_meta.iloc[idx_train], y_meta.iloc[idx_train]\n    X_valid, y_valid = X_meta.iloc[idx_valid], y_meta.iloc[idx_valid]\n\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # validation prediction\n    pred_valid = model.predict_proba(X_valid)[:,1]\n    score = roc_auc_score(y_valid, pred_valid)\n    scores_tmp.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print(\"--\"*20)\n    \n    # test prediction based on oof_set\n    y_hat = model.predict_proba(test_meta)[:,1]\n    meta_pred_tmp.append(y_hat)\n    \n# print overall validation scores\nprint(f\"Overall Validation Score | Meta: {np.mean(scores_tmp)}\")\nprint(\"::\"*20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# average meta predictions over each fold\nmeta_predictions = np.mean(np.column_stack(meta_pred_tmp), axis=1)\n\n# create submission file\nstacked_submission = sample_submission.copy()\nstacked_submission[\"target\"] = meta_predictions\nstacked_submission.to_csv(\"./stacked_submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}