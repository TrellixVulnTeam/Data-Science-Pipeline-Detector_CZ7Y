{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ”¥ðŸ”¥TPS Oct 2021 - ðŸ”¥ðŸ”¥EDA and Analysis ðŸ”¥ðŸ”¥\n\n# Biological Molecules response to Chemicals \n.         |  ..\n:-------------------------:|:-------------------------:\n![](https://miro.medium.com/max/1400/0*hQG_y2xkkj4cjexx)  |  ![DNA](https://upload.wikimedia.org/wikipedia/commons/1/16/DNA_orbit_animated.gif)\n![Key Bio Proteins](https://upload.wikimedia.org/wikipedia/commons/d/d3/0322_DNA_Nucleotides.jpg)  | ![Meta Genomic Methods](https://upload.wikimedia.org/wikipedia/commons/4/47/Overview_of_metagenomic_methods.jpg)\n\n> Chemical biology is a scientific discipline spanning the fields of chemistry and biology. The discipline involves the application of chemical techniques, analysis, and often small molecules produced through synthetic chemistry, to the study and manipulation of biological systems. In contrast to biochemistry, which involves the study of the chemistry of biomolecules and regulation of biochemical pathways within and between cells, chemical biology deals with chemistry applied to biology (synthesis of biomolecules, simulation of biological systems etc.).\n\n## Excited? \n\n\n##### Some References on Biology and Chemistry correlation \n###### Thanks to Tensor Girl for posting on BMS competition\n- [Deep learning and generative methods in cheminformatics and chemical biology: navigating small molecule space intelligently](https://portlandpress.com/biochemj/article/477/23/4559/227194/Deep-learning-and-generative-methods-in)\n- [Learning Drug Functions from Chemical Structures with Convolutional Neural Networks and Random Forests](https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236)\n-[Deep Learning of Atomically Resolved Scanning Transmission Electron Microscopy Images: Chemical Identification and Tracking Local Transformations](https://www.osti.gov/servlets/purl/1427646)\n-[Chemception: Deep Learning from 2D Chemical Structure Images](https://depth-first.com/articles/2019/02/04/chemception-deep-learning-from-2d-chemical-structure-images/)\n-[Molecular Structure Extraction From Documents Using Deep Learning](https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf)\n-[CheMixNet: Mixed DNN Architectures for Predicting Chemical Properties using Multiple Molecular Representations](http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf)\n\n### One more source to check for good info on molecules is [PubChem](https://pubchem.ncbi.nlm.nih.gov/)\n> PubChem is the world's largest collection of freely accessible chemical information. Search chemicals by name, molecular formula, structure, and other identifiers. Find chemical and physical properties, biological activities, safety and toxicity information, patents, literature citations and more.\n\n### Let us check the analysis","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents\n<a id=\"table-of-contents\"></a>\n- [1 Introduction](#1)\n- [2 Preparations](#2)\n- [3 Datasets Overview](#3)\n    - [3.1 Train dataset](#3.1)\n    - [3.2 Test dataset](#3.2)\n    - [3.3 Submission](#3.3)\n- [4 Features](#4)\n    - [4.1 Missing values](#4.1)\n       - [4.1.1 Preparation](#4.1.1)\n       - [4.1.2 Individual features](#4.1.2)\n       - [4.1.3 Individual rows](#4.1.3)\n       - [4.1.3 Dealing with missing values (reference)](#4.1.4)\n    - [4.2 Distribution](#4.2)","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"1\"></a>\n# 1 Introduction\n\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, Kaggle have launched many Playground competitions that are more approachable than Featured competition, and thus more beginner-friendly.\n\nThe goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a [CTGAN](https://github.com/sdv-dev/CTGAN). The original dataset deals with *predicting the **biological response of molecules** given various chemical properties*. Although the features are anonymized, they have properties relating to real-world features.\n\nThis competition will asked to predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values.\n\nSubmissions are evaluated on **area under the ROC curve** between the predicted probability and the observed target.","metadata":{}},{"cell_type":"code","source":"\n# import packages\nimport os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\n# setting up options\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n# import datasets\ntrain_file = '../input/tabular-playground-series-oct-2021/train.csv'\ntest_file = '../input/tabular-playground-series-oct-2021/test.csv'\nsub_file = '../input/tabular-playground-series-oct-2021/sample_submission.csv'","metadata":{"execution":{"iopub.status.busy":"2021-10-01T06:41:40.314587Z","iopub.execute_input":"2021-10-01T06:41:40.314917Z","iopub.status.idle":"2021-10-01T06:41:41.088256Z","shell.execute_reply.started":"2021-10-01T06:41:40.314826Z","shell.execute_reply":"2021-10-01T06:41:41.087548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(train_file)\ntest_df = pd.read_csv(test_file)\nsubmission = pd.read_csv(sub_file)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T06:41:41.089854Z","iopub.execute_input":"2021-10-01T06:41:41.090184Z","iopub.status.idle":"2021-10-01T06:43:02.631704Z","shell.execute_reply.started":"2021-10-01T06:41:41.09015Z","shell.execute_reply":"2021-10-01T06:43:02.630968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"2\"></a>\n# 2 Preparations\nPreparing packages and data that will be used in the analysis process. Packages that will be loaded are mainly for data manipulation, data visualization and modeling. There are 2 datasets that are used in the analysis, they are train and test dataset. The main use of train dataset is to train models and use it to predict test dataset. While sample submission file is used to informed participants on the expected submission for the competition. *(to see the details, please expand)*","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T06:43:02.632856Z","iopub.execute_input":"2021-10-01T06:43:02.633155Z","iopub.status.idle":"2021-10-01T06:43:02.765397Z","shell.execute_reply.started":"2021-10-01T06:43:02.633121Z","shell.execute_reply":"2021-10-01T06:43:02.764745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3\"></a>\n# 3 Dataset Overview\nThe intend of the overview is to get a feel of the data and its structure in train, test and submission file. An overview on train and test datasets will include a quick analysis on missing values and basic statistics, while sample submission will be loaded to see the expected submission.\n\n<a id=\"3.1\"></a>\n## 3.1 Train dataset\nAs stated before, train dataset is mainly used to train predictive model as there is an available target variable in this set. This dataset is also used to explore more on the data itself including find a relation between each predictors and the target variable.\n\n**Observations:**\n- `target` column is the target variable which is only available in the `train` dataset.\n- There are `287` columns: `285` are features, `1` target variable `target` and `1` column of `id`.\n- `train` dataset contain `1,000,000`  data, and the test consists of 500000 data.\n- f0~f241 : continuous feature (242)\n- f242 ~ f284 : binary feature (43)\n\n### 3.1.1 Quick view\nBelow is the first 5 rows of train dataset:","metadata":{}},{"cell_type":"code","source":"print(f'Number of rows: {train_df.shape[0]}')\nprint(f'Number of columns: {train_df.shape[1]}')\nprint(f'No of missing values: {sum(train_df.isna().sum())}')","metadata":{"execution":{"iopub.status.busy":"2021-10-01T06:48:06.498994Z","iopub.execute_input":"2021-10-01T06:48:06.499717Z","iopub.status.idle":"2021-10-01T06:48:06.65941Z","shell.execute_reply.started":"2021-10-01T06:48:06.499604Z","shell.execute_reply":"2021-10-01T06:48:06.657845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.2 Basic statistics\nBelow is the basic statistics for each variables which contain information on `count`, `mean`, `standard deviation`, `minimum`, `1st quartile`, `median`, `3rd quartile` and `maximum`.","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T06:43:03.215312Z","iopub.execute_input":"2021-10-01T06:43:03.21564Z","iopub.status.idle":"2021-10-01T06:43:11.910998Z","shell.execute_reply.started":"2021-10-01T06:43:03.215603Z","shell.execute_reply":"2021-10-01T06:43:11.910307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T06:43:11.911939Z","iopub.execute_input":"2021-10-01T06:43:11.912194Z","iopub.status.idle":"2021-10-01T06:43:12.030697Z","shell.execute_reply.started":"2021-10-01T06:43:11.91216Z","shell.execute_reply":"2021-10-01T06:43:12.029956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3.3\"></a>\n## 3.3 Submission\nThe submission file is expected to have an `id` and `target` columns.\n\nBelow is the first 5 rows of submission file:","metadata":{}},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T06:43:12.031927Z","iopub.execute_input":"2021-10-01T06:43:12.032255Z","iopub.status.idle":"2021-10-01T06:43:12.040812Z","shell.execute_reply.started":"2021-10-01T06:43:12.03222Z","shell.execute_reply":"2021-10-01T06:43:12.040131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-10-01T06:43:12.04199Z","iopub.execute_input":"2021-10-01T06:43:12.042558Z","iopub.status.idle":"2021-10-01T06:43:12.050414Z","shell.execute_reply.started":"2021-10-01T06:43:12.042522Z","shell.execute_reply":"2021-10-01T06:43:12.049701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"4\"></a>\n# 4 Features\nNumber of features available to be used to create a prediction model are `285`.\n\n<a id=\"4.1\"></a>\n## 4.1 Missing values\nCounting number of missing value and it's relative with their respective observations between train & test dataset.\n\n<a id=\"4.1.1\"></a>\n### 4.1.1 Preparation\nPrepare train and test dataset for data analysis and visualization. *(to see the details, please expand)*","metadata":{}},{"cell_type":"code","source":"train_df.loc[:, 'f0':'f284'].describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Greens')\\\n                            .background_gradient(subset=['25%'], cmap='Spectral')\\\n                            .background_gradient(subset=['50%'], cmap='seismic')\\\n                            .background_gradient(subset=['75%'], cmap='viridis')\\\n                            .background_gradient(subset=['mean'], cmap='cubehelix')\\\n                            .background_gradient(subset=['min'], cmap='Reds')\\\n                            .background_gradient(subset=['max'], cmap='Blues')\n","metadata":{"execution":{"iopub.status.busy":"2021-10-01T06:43:12.051639Z","iopub.execute_input":"2021-10-01T06:43:12.05198Z","iopub.status.idle":"2021-10-01T06:43:22.331633Z","shell.execute_reply.started":"2021-10-01T06:43:12.051933Z","shell.execute_reply":"2021-10-01T06:43:22.330838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.1.2 Memory Reduction","metadata":{}},{"cell_type":"markdown","source":"Let us use datatable to reduce the memory\n\n##### Source Credit: https://towardsdatascience.com/how-to-work-with-million-row-datasets-like-a-pro-76fb5c381cdd","metadata":{}},{"cell_type":"markdown","source":"### Missing Data Analysis ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"missing_train_df = pd.DataFrame(train_df.isna().sum())\nmissing_train_df = missing_train_df.drop(['id', 'target']).reset_index()\nmissing_train_df.columns = ['feature', 'count']\n\nmissing_train_percent_df = missing_train_df.copy()\nmissing_train_percent_df['count'] = missing_train_df['count']/train_df.shape[0]\n\nmissing_test_df = pd.DataFrame(test_df.isna().sum())\nmissing_test_df = missing_test_df.drop(['id']).reset_index()\nmissing_test_df.columns = ['feature', 'count']\n\nmissing_test_percent_df = missing_test_df.copy()\nmissing_test_percent_df['count'] = missing_test_df['count']/test_df.shape[0]\n\nfeatures = [feature for feature in train_df.columns if feature not in ['id', 'target']]\nmissing_train_row = train_df[features].isna().sum(axis=1)\nmissing_train_row = pd.DataFrame(missing_train_row.value_counts()/train_df.shape[0]).reset_index()\nmissing_train_row.columns = ['no', 'count']\n\nmissing_test_row = test_df[features].isna().sum(axis=1)\nmissing_test_row = pd.DataFrame(missing_test_row.value_counts()/test_df.shape[0]).reset_index()\nmissing_test_row.columns = ['no', 'count']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_features =[]\nnum_features =[]\n\nfor col in train_df.columns:\n    if train_df[col].dtype=='float64':\n        num_features.append(col)\n    else:\n        cat_features.append(col)\nprint('Catagoric features: ', cat_features)\ndisplay(len(cat_features))\nprint('Numerical features: ', num_features)\ndisplay(len(num_features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"L = len(num_features[175:202])\nnrow= 7\nncol= 4\n\nremove_last= (nrow * ncol) - L\n\nfig, ax = plt.subplots(nrow, ncol,figsize=(16, 20))\nfig.subplots_adjust(top=0.95)\ni = 1\nfor feature in num_features[0:28]:\n    plt.subplot(nrow, ncol, i)\n    ax = sns.kdeplot(train_df[feature], shade=True,  color='lime',  alpha=0.9, label='Train DS')\n    ax = sns.kdeplot(test_df[feature], shade=True, color='fuchsia',  alpha=0.9, label='Test DS')\n    plt.xlabel(feature, fontsize=7)\n    plt.legend()\n    i += 1\nplt.suptitle('Distribution of some numerical features ==> Training & Test datasets ', fontsize=18)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.1.2\"></a>\n### 4.1.2 Individual features\nCount how many missing values in each features on `train` and `test` dataset to see if there any similiarity between them.\n\n**Observations:**\n- Every features in `train` and `test` dataset has a missing value of around `0.0%`.\n","metadata":{}},{"cell_type":"markdown","source":"## 4.1.3 - Analysis using Autoviz","metadata":{}},{"cell_type":"code","source":"#!pip install sweetviz autoviz xlrd","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nfrom autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()\ndf = AV.AutoViz(train_file, depVar='target',verbose = 1, \n                lowess = False, chart_format ='png', \n                max_rows_analyzed = 150000)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#More to come! Thanks to the notebook outline from TPS SEP 2021 from Sharito Cope","metadata":{"execution":{"iopub.status.busy":"2021-10-01T01:00:25.470619Z","iopub.execute_input":"2021-10-01T01:00:25.47093Z","iopub.status.idle":"2021-10-01T01:00:26.015945Z","shell.execute_reply.started":"2021-10-01T01:00:25.470894Z","shell.execute_reply":"2021-10-01T01:00:26.014841Z"}}},{"cell_type":"markdown","source":"# 5 - Simple LGBM model for evaluation","metadata":{}},{"cell_type":"code","source":"#test_df = dt.fread(test_file).to_pandas()\n#train_df = dt.fread(train_file).to_pandas()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop the Target Column\ntraining_data = train_df.drop(\"target\", axis=1)\n#Save the value of Target for usage\ntraining_label = train_df[\"target\"].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nlgbm_pipeline = Pipeline([\n    ('imputer',SimpleImputer(strategy=\"most_frequent\")),\n    ('std_scaler', StandardScaler()),\n])\ntraining_prepared = lgbm_pipeline.fit_transform(training_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nlgbm_cls = lgb.LGBMClassifier()\nlgbm_cls.fit(training_prepared,training_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_prepared = lgbm_pipeline.fit_transform(test_df)\ntest_predictions = lgbm_cls.predict(testing_prepared)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(sub_file)\nsubmission['target'] = list(map(float, test_predictions))\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}