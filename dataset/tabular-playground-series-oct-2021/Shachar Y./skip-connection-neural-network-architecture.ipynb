{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-19T04:51:02.648685Z","iopub.execute_input":"2021-10-19T04:51:02.649165Z","iopub.status.idle":"2021-10-19T04:51:02.659409Z","shell.execute_reply.started":"2021-10-19T04:51:02.649114Z","shell.execute_reply":"2021-10-19T04:51:02.658667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport time\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, KBinsDiscretizer # standartization of vars\nfrom sklearn.decomposition import PCA\nfrom collections import Counter\nimport tensorflow as tf\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras import backend as K # for the definition of the bce metrics\nfrom sklearn.metrics import roc_auc_score\nimport datatable as dt\nfrom sklearn.feature_selection import VarianceThreshold","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:02.66083Z","iopub.execute_input":"2021-10-19T04:51:02.661403Z","iopub.status.idle":"2021-10-19T04:51:09.860154Z","shell.execute_reply.started":"2021-10-19T04:51:02.661362Z","shell.execute_reply":"2021-10-19T04:51:09.859297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dt_read_data(path,file_type):\n    return dt.fread(f'{path}/{file_type}.csv').to_pandas()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:09.861225Z","iopub.execute_input":"2021-10-19T04:51:09.861438Z","iopub.status.idle":"2021-10-19T04:51:09.866392Z","shell.execute_reply.started":"2021-10-19T04:51:09.861414Z","shell.execute_reply":"2021-10-19T04:51:09.865597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the boolian variables to 0/1\ndef convert_boolian(df, excluded_vars):\n    bool_vars = df[np.setdiff1d(df.columns, excluded_vars)].select_dtypes(include='bool').columns\n    for i in bool_vars:\n        df[i]=df[i].astype(int)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:09.868128Z","iopub.execute_input":"2021-10-19T04:51:09.868477Z","iopub.status.idle":"2021-10-19T04:51:09.876676Z","shell.execute_reply.started":"2021-10-19T04:51:09.86845Z","shell.execute_reply":"2021-10-19T04:51:09.875904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def memory_reduction(df):\n    mem = df.memory_usage().sum() / 1024 ** 2\n    print(f'start memory size is {mem}')\n    x,y=[],[]\n    for i in ['int','float']:\n        if i == 'int':\n            for j in [8,16,32,64]:\n                x.append(i+str(j))\n                y.append(i)\n        else:\n            for j in [16,32,64]:\n                x.append(i+str(j))\n                y.append(i)\n\n    p={}\n    for i in x:\n        if i.startswith('int'):\n            p[i]=[np.iinfo(i).min, np.iinfo(i).max]\n        else:\n            p[i]=[np.finfo(i).min, np.finfo(i).max]\n\n    p_df={}\n    for i in df.columns:\n        if df[i].dtype in ['int','float']:\n            if df[i].dtype == 'int':\n                p_df[i]=['int', np.min(df[i]),np.max(df[i])]\n            else:\n                p_df[i]=['float', np.min(df[i]),np.max(df[i])]\n\n    dtype_min_array = np.array([i[0] for i in p.values()])\n    dtype_max_array = np.array([i[1] for i in p.values()])\n\n    dtype_required_type={}\n    for k in p_df.keys():\n        s0 = np.where(np.array(y) == p_df.get(k)[0])\n        s1 = np.where(dtype_min_array < p_df.get(k)[1])\n        s2 = np.where(dtype_max_array > p_df.get(k)[2])\n        dtype_required_type[k] = x[np.min(np.intersect1d(s2, np.intersect1d(s0, s1)))]\n\n    # convert the variable types into their appropriate type\n    for k,v in dtype_required_type.items():\n        df[k] = df[k].astype(v)\n\n    mem_new = df.memory_usage().sum() / 1024 ** 2\n    print(f'memory post reduction is {np.round(mem_new,2)}MG-reduced by {np.round((1-mem_new/mem)*100,2)}%')\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:09.878003Z","iopub.execute_input":"2021-10-19T04:51:09.878518Z","iopub.status.idle":"2021-10-19T04:51:09.897499Z","shell.execute_reply.started":"2021-10-19T04:51:09.878475Z","shell.execute_reply":"2021-10-19T04:51:09.896494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def missing_values(df, excluded_vars):\n    return {k:df[k].isna().sum() for k in np.setdiff1d(df.columns, excluded_vars) if df[k].isna().sum() > 0}","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:09.898676Z","iopub.execute_input":"2021-10-19T04:51:09.898896Z","iopub.status.idle":"2021-10-19T04:51:09.913365Z","shell.execute_reply.started":"2021-10-19T04:51:09.898871Z","shell.execute_reply":"2021-10-19T04:51:09.91244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def IQR_scores(df, excluded_vars, IQR_treshhold=1.5, outlier_treshhold = 0.15):\n    outlier = []\n    length_vars = len(df.columns)\n    for i in np.setdiff1d(df.columns, excluded_vars):\n        Q1 = np.quantile(df[i], 0.25)\n        Q3 = np.quantile(df[i], 0.75)\n        IQR = Q3-Q1\n        oulier_index = np.where((df[i] < (Q1 - IQR_treshhold * IQR)) | (df[i] > (Q3 + IQR_treshhold * IQR)))\n        outlier.extend(oulier_index[0].tolist())\n    outlier_indices0 = dict(Counter(outlier))\n    outlier_indices1 = {k:v/length_vars for k,v in outlier_indices0.items()}\n    outlier = {k:v for k,v in outlier_indices1.items() if v > outlier_treshhold}\n    return list(outlier.keys())","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:09.914825Z","iopub.execute_input":"2021-10-19T04:51:09.91534Z","iopub.status.idle":"2021-10-19T04:51:09.925367Z","shell.execute_reply.started":"2021-10-19T04:51:09.915298Z","shell.execute_reply":"2021-10-19T04:51:09.924578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create three features per row: sd, min, max (for the non-boolian variables\ndef create_features(df, excluded_vars):\n    df['sd_var'] = df[np.setdiff1d(df.columns, excluded_vars)].std(axis=1)\n    df['min_var'] = df[np.setdiff1d(df.columns, excluded_vars)].min(axis=1)\n    df['max_var'] = df[np.setdiff1d(df.columns, excluded_vars)].max(axis=1)\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop columns with low variance threshhold\ndef drop_columns_with_low_var(df, threshold):\n    var_thr = VarianceThreshold(threshold=threshold)\n    var_thr.fit(df)\n    columns_to_keep = df.columns[var_thr.get_support()]\n    return columns_to_keep.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:09.945596Z","iopub.execute_input":"2021-10-19T04:51:09.946161Z","iopub.status.idle":"2021-10-19T04:51:09.954014Z","shell.execute_reply.started":"2021-10-19T04:51:09.946115Z","shell.execute_reply":"2021-10-19T04:51:09.95334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# standardized the variables\ndef standartization(df, excluded_vars=None):\n    if excluded_vars is None:\n        var_names = list(df.columns)\n        scaler = StandardScaler()\n        df_scaled = scaler.fit_transform(df)\n        df_scaled = pd.DataFrame(df_scaled)\n        df_scaled.columns = var_names\n        return df_scaled\n    else:\n        vars_to_transform = np.setdiff1d(df.columns, excluded_vars)\n        df_tmp = df[excluded_vars]\n        scaler = StandardScaler()\n        df_scaled = scaler.fit_transform(df[vars_to_transform])\n        df_scaled = pd.DataFrame(df_scaled)\n        df_scaled.columns = vars_to_transform\n        return pd.concat([df_tmp, df_scaled], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:09.955547Z","iopub.execute_input":"2021-10-19T04:51:09.955992Z","iopub.status.idle":"2021-10-19T04:51:09.966387Z","shell.execute_reply.started":"2021-10-19T04:51:09.955949Z","shell.execute_reply":"2021-10-19T04:51:09.965365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function that create random noise for denoising autoencoders\ndef denoise(df, noise_type, noise_fill, noise_percent, seed):\n    np.random.seed(seed)\n    if type(df) is np.ndarray:\n        pass\n    else:\n        df = np.array(df)\n    if noise_type == 'random':\n        indices = np.random.choice(\n            range(\n                np.multiply(\n                    df.shape[0], df.shape[1])), int(noise_percent * df.shape[1]), replace=False)\n        if noise_fill == 'zero':\n            np.put(df, indices, 0)\n        elif noise_fill == 'random_normal':\n            df=df.astype(float)\n            np.put(df, indices, np.random.normal(0,1,len(indices)))\n    elif noise_type == 'row_wise':\n        indices = np.array(\n            [np.random.choice(\n                range(df.shape[1]), int(noise_percent * df.shape[1]), replace=False) + i\n             for i in range(0, np.multiply(df.shape[0], df.shape[1]), df.shape[1])])\n        if noise_fill == 'zero':\n            np.put(df, indices.flatten(), 0)\n        elif noise_fill == 'random_normal':\n            df = df.astype(float)\n            np.put(df, indices.flatten(), np.random.normal(0, 1, len(indices.flatten())))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:09.984654Z","iopub.execute_input":"2021-10-19T04:51:09.985158Z","iopub.status.idle":"2021-10-19T04:51:09.996985Z","shell.execute_reply.started":"2021-10-19T04:51:09.985126Z","shell.execute_reply":"2021-10-19T04:51:09.996388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the denoise model\ndef denoise_architecture(train_denoise, train, epochs, batch_size, seed):\n    tf.random.set_seed(seed)\n    visible = tf.keras.layers.Input(shape=(train_denoise.shape[1],), name='input_denoise')\n    # define the encoder layers\n    encode_layer = tf.keras.layers.Dense(units=64, activation='relu', name='encoding_layer')(visible)\n    # The output layer will have the same number of nodes\n    # as there are columns in the denoising input data and will use\n    # a linear activation function to output numeric values\n    output = tf.keras.layers.Dense(units=train.shape[1], activation='linear', name='output_layer')(encode_layer)\n    # define the model\n    model = tf.keras.models.Model(inputs=visible, outputs=output)\n    model.compile(loss='mse',\n                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    model.fit(train_denoise,\n              train,\n              validation_split=0.2,\n              epochs=epochs,\n              batch_size=batch_size,\n              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss',patience=3,min_delta=0.0001,restore_best_weights=True),\n                         tf.keras.callbacks.ReduceLROnPlateau(factor=0.7,patience=3,min_delta=0.00001)],\n              verbose=1)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-19T05:15:29.6722Z","iopub.execute_input":"2021-10-19T05:15:29.673525Z","iopub.status.idle":"2021-10-19T05:15:29.686492Z","shell.execute_reply.started":"2021-10-19T05:15:29.67347Z","shell.execute_reply":"2021-10-19T05:15:29.684923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract the denoising new features\ndef denoise_features(denoising_model, encoding_layer_name, df):\n    encoding_model = tf.keras.models.Model(inputs=denoising_model.input,\n                                           outputs=denoising_model.get_layer(encoding_layer_name).output)\n    encoding_layer_output = pd.DataFrame(encoding_model.predict(df))\n    encoding_layer_output.columns = ['_'.join(['dae',str(i)]) for i in encoding_layer_output.columns]\n    return encoding_layer_output","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:10.015944Z","iopub.execute_input":"2021-10-19T04:51:10.016335Z","iopub.status.idle":"2021-10-19T04:51:10.030649Z","shell.execute_reply.started":"2021-10-19T04:51:10.016301Z","shell.execute_reply":"2021-10-19T04:51:10.02997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bin_partition(df,excluded_vars,n_bins,random_state,output_distribution='uniform'):\n    # store the excluded vars in a tmp data\n    excluded_vars_data = df[excluded_vars]\n    # drop the excluded vars from the data prior to bin partition\n    df_tmp = df.drop(excluded_vars, axis=1)\n    # store the column names of the transformed data to assign them at the end of the process\n    df_tmp_columns = df_tmp.columns\n    # set the seed\n    np.random.seed(random_state)\n    # fit_transform the bin data\n    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=output_distribution)\n    df_tmp = pd.DataFrame(est.fit_transform(df_tmp), columns=df_tmp_columns)\n    df_tmp = df_tmp.apply(lambda x: x.astype(int))\n    df_tmp = pd.concat([excluded_vars_data,df_tmp], axis=1)\n    return df_tmp","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:10.031939Z","iopub.execute_input":"2021-10-19T04:51:10.032785Z","iopub.status.idle":"2021-10-19T04:51:10.042725Z","shell.execute_reply.started":"2021-10-19T04:51:10.032751Z","shell.execute_reply":"2021-10-19T04:51:10.042069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create callbacks\ndef callbacks():\n    return [tf.keras.callbacks.EarlyStopping(monitor='loss',patience=3,min_delta=0.0001,restore_best_weights=True),\n            tf.keras.callbacks.ModelCheckpoint('model.h5',save_weights_only=True),\n            tf.keras.callbacks.ReduceLROnPlateau(factor=0.7,patience=3,min_delta=0.00001)]","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:10.043948Z","iopub.execute_input":"2021-10-19T04:51:10.044322Z","iopub.status.idle":"2021-10-19T04:51:10.058028Z","shell.execute_reply.started":"2021-10-19T04:51:10.044294Z","shell.execute_reply":"2021-10-19T04:51:10.056898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# residual block architecture\ndef residual_architecture(df, number_of_blocks, input_dim, output_dim, output_shape, seed):\n    block = {}\n    tf.random.set_seed(seed)\n    auc = tf.keras.metrics.AUC()\n    input = tf.keras.layers.Input(shape=(df.shape[1],), name='input_layer')\n\n    for i in range(number_of_blocks):\n        if i == 0:\n            embed = tf.keras.layers.Embedding(input_dim=input_dim,\n                                              output_dim=output_dim,\n                                              input_length=1,\n                                              name='embedding')(input)\n            emb_flat = tf.keras.layers.Flatten()(embed)\n            emb_norm = tf.keras.layers.BatchNormalization()(emb_flat)\n            emb_drop = tf.keras.layers.Dropout(0.2)(emb_norm)\n            block[i] = tf.keras.layers.Dense(units=64, activation='relu', name=f'block_{i}')(emb_drop)\n\n        else:\n            norm = tf.keras.layers.BatchNormalization()(block[i-1])\n            drop = tf.keras.layers.Dropout(0.2)(norm)\n            block[i] = tf.keras.layers.Dense(units=64, activation='relu', name=f'block_{i}')(drop)\n            block[i] = tf.keras.layers.concatenate([block[i], block[i-1]])\n    norm = tf.keras.layers.BatchNormalization()(block[number_of_blocks-1])\n    output = tf.keras.layers.Dense(units=output_shape, activation='sigmoid')(norm)\n\n    model = tf.keras.models.Model(inputs=input, outputs=output)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.08),\n                  metrics=[auc])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:10.059567Z","iopub.execute_input":"2021-10-19T04:51:10.059913Z","iopub.status.idle":"2021-10-19T04:51:10.075222Z","shell.execute_reply.started":"2021-10-19T04:51:10.059873Z","shell.execute_reply":"2021-10-19T04:51:10.074198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build the prediction pipeline\ndef predict_residual_model(df, test_df, splits,\n                           seeds, number_of_blocks,\n                           input_dim, output_dim,\n                           output_shape, epochs, batch_size):\n    n_splits = splits\n    pred_test = 0\n    general_prediction = 0\n    label = df['target']\n    _df = df.drop(['id','target'], axis=1)\n    test_ids = test_df['id']\n    _test = test_df.drop('id', axis=1)\n    # run across the seeds\n    for seed in seeds:\n        np.random.seed(seed)\n        tf.random.set_seed(seed)\n        skf = StratifiedKFold(n_splits=n_splits, random_state=seed, shuffle=True)\n        score = []\n        for i, (train_index, test_index) in enumerate(skf.split(_df, label)):\n            tr, ts = np.array(_df.iloc[train_index]), np.array(_df.iloc[test_index])\n            tr_label, ts_label = np.array(label.iloc[train_index]), np.array(label.iloc[test_index])\n            model = residual_architecture(tr,\n                                          number_of_blocks,\n                                          input_dim,\n                                          output_dim,\n                                          output_shape,\n                                          seed)\n            hist = model.fit(tr,\n                             tr_label,\n                             validation_data=(ts, ts_label),\n                             epochs=epochs,\n                             batch_size=batch_size,\n                             callbacks=callbacks(),\n                             verbose=0)\n            # collect the loss value results to compute the average loss value\n            model.load_weights('model.h5')\n            pred_ts = roc_auc_score(ts_label, model.predict(ts))\n            hist_scores = [i for i in hist.history.values()]\n            print(f'roc_auc_score value for seed {seed} at fold {i} at step {pd.Series(hist_scores[0]).idxmin()} is {pred_ts}')\n            pred_test += model.predict(_test)/n_splits\n            score.append(pred_ts)\n        print(f'\"\\n\"the average roc_auc_score value for seed value {seed} is {np.mean(score)}\"\\n\"')\n    general_prediction += pred_test/len(seeds)\n    p = pd.DataFrame({'id': test_ids, 'target': general_prediction.flatten()})\n    return p","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:10.076685Z","iopub.execute_input":"2021-10-19T04:51:10.077399Z","iopub.status.idle":"2021-10-19T04:51:10.097721Z","shell.execute_reply.started":"2021-10-19T04:51:10.077348Z","shell.execute_reply":"2021-10-19T04:51:10.097075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/tabular-playground-series-oct-2021'\nt1 = time.time()\nprint('reading the train and test data')\ntrain = dt_read_data(path, 'train')\ntest = dt_read_data(path, 'test')\ntrain['target']=train['target'].astype(int)\nprint(f'reading the train and test data in {time.time()-t1} seconds with {len(train)} rows in the train data')","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:10.099142Z","iopub.execute_input":"2021-10-19T04:51:10.100284Z","iopub.status.idle":"2021-10-19T04:51:46.88322Z","shell.execute_reply.started":"2021-10-19T04:51:10.100237Z","shell.execute_reply":"2021-10-19T04:51:46.882237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a list of boolian variables for later use\nbool_vars = train[np.setdiff1d(train.columns, ['id','target'])].select_dtypes(include='bool').columns.to_list()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:46.884818Z","iopub.execute_input":"2021-10-19T04:51:46.885473Z","iopub.status.idle":"2021-10-19T04:51:47.651613Z","shell.execute_reply.started":"2021-10-19T04:51:46.885439Z","shell.execute_reply":"2021-10-19T04:51:47.650877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('convert boolian vars to int (0/1)')\ntrain = convert_boolian(train, ['id','target'])\ntest = convert_boolian(test, ['id'])","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:47.652978Z","iopub.execute_input":"2021-10-19T04:51:47.653401Z","iopub.status.idle":"2021-10-19T04:51:49.43094Z","shell.execute_reply.started":"2021-10-19T04:51:47.653353Z","shell.execute_reply":"2021-10-19T04:51:49.429992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for missing values\ndictionary_missing = missing_values(train, ['id','target'])\nprint(f'the number of variables that have missing values is {len(dictionary_missing)}')","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:49.432328Z","iopub.execute_input":"2021-10-19T04:51:49.432584Z","iopub.status.idle":"2021-10-19T04:51:50.038499Z","shell.execute_reply.started":"2021-10-19T04:51:49.432553Z","shell.execute_reply":"2021-10-19T04:51:50.037873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('dropping outliers based on IQR index')\noutlier_indices = IQR_scores(train, ['id','target'])\nprint('drop rows with high outlier occurrences')\ntrain = train.iloc[np.setdiff1d(range(len(train)), outlier_indices)].reset_index(drop=True)\nprint(f'the number of rows in the train data is {len(train)}')","metadata":{"execution":{"iopub.status.busy":"2021-10-19T04:51:50.039438Z","iopub.execute_input":"2021-10-19T04:51:50.040172Z","iopub.status.idle":"2021-10-19T04:52:11.396263Z","shell.execute_reply.started":"2021-10-19T04:51:50.040134Z","shell.execute_reply":"2021-10-19T04:52:11.395323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create new features\ntrain = create_features(train, excluded_vars=['id','target']+bool_vars)\ntest = create_features(test, excluded_vars=['id','target']+bool_vars)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run the denoise function\nprint('running dae analysis on the train and test data')\ncolumns_for_dae = np.setdiff1d(train.columns, ['id','target']+bool_vars)\ntrain_denoise = denoise(train[columns_for_dae], noise_type='row_wise', noise_fill='zero', noise_percent=0.5, seed=1974)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T05:15:47.551596Z","iopub.execute_input":"2021-10-19T05:15:47.55195Z","iopub.status.idle":"2021-10-19T05:17:35.968434Z","shell.execute_reply.started":"2021-10-19T05:15:47.551913Z","shell.execute_reply":"2021-10-19T05:17:35.967353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run the denoise model\nmodel_dae = denoise_architecture(train_denoise, train[columns_for_dae], epochs=100, batch_size=512, seed=1974)\ndel train_denoise\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T05:08:34.43141Z","iopub.execute_input":"2021-10-19T05:08:34.43185Z","iopub.status.idle":"2021-10-19T05:12:44.790329Z","shell.execute_reply.started":"2021-10-19T05:08:34.431805Z","shell.execute_reply":"2021-10-19T05:12:44.789121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract the features from the encoding layer\ndenoise_train = denoise_features(model_dae, 'encoding_layer', train[columns_for_dae])\ndenoise_test = denoise_features(model_dae, 'encoding_layer', test[columns_for_dae])","metadata":{"execution":{"iopub.status.busy":"2021-10-19T05:29:26.119192Z","iopub.execute_input":"2021-10-19T05:29:26.119861Z","iopub.status.idle":"2021-10-19T05:30:07.779976Z","shell.execute_reply.started":"2021-10-19T05:29:26.119817Z","shell.execute_reply":"2021-10-19T05:30:07.778909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keep columns with variance greater than 0.05\ncolumns_to_keep = drop_columns_with_low_var(denoise_train, 0.05)\nprint(columns_to_keep)\ndenoise_train = denoise_train[columns_to_keep]\ndenoise_test = denoise_test[columns_to_keep]","metadata":{"execution":{"iopub.status.busy":"2021-10-19T05:32:17.176179Z","iopub.execute_input":"2021-10-19T05:32:17.176868Z","iopub.status.idle":"2021-10-19T05:32:18.987436Z","shell.execute_reply.started":"2021-10-19T05:32:17.176823Z","shell.execute_reply":"2021-10-19T05:32:18.986449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add the new features to the train and test data\ntrain = pd.concat([train,denoise_train], axis=1)\ntest = pd.concat([test,denoise_test], axis=1)\ndel [denoise_train,denoise_test]\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('standardized the train and test data')\ntrain = standartization(train,['id','target']+bool_vars)\ntest = standartization(test,['id']+bool_vars)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# devide the continous variables into 128 bins\ntrain = bin_partition(train,\n                      excluded_vars=['id','target'],\n                      n_bins=128,\n                      random_state=1974,\n                      output_distribution='uniform')\n\ntest = bin_partition(test,\n                     excluded_vars=['id'],\n                     n_bins=128,\n                     random_state=1974,\n                     output_distribution='uniform')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('memory reduction function')\ntrain = memory_reduction(train)\ntest = memory_reduction(test)\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_prediction = predict_residual_model(train,\n                                         test,\n                                         splits = 5,\n                                         seeds = [1974],\n                                         number_of_blocks = 4,\n                                         input_dim = 128,\n                                         output_dim = 11,\n                                         output_shape = 1,\n                                         epochs = 100,\n                                         batch_size = 512)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_prediction.to_csv('resnet.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}