{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-09T13:56:36.738448Z","iopub.execute_input":"2021-10-09T13:56:36.739037Z","iopub.status.idle":"2021-10-09T13:56:36.839056Z","shell.execute_reply.started":"2021-10-09T13:56:36.738946Z","shell.execute_reply":"2021-10-09T13:56:36.838207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to read cvs faster (uses GPU)\nimport cudf\n\n# to split dataset\nfrom sklearn.model_selection import train_test_split\n# model\nfrom xgboost import XGBClassifier\n# to split dataset in folds for cross-validation preserving the percentage of samples for each class\nfrom sklearn.model_selection import StratifiedKFold\n# to perform a randomized search for cross-validation\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# to perform a hyperparameter scan using Bayesian Optimization\nfrom skopt import BayesSearchCV\n# parameter ranges are specified by one of below\nfrom skopt.space import Real, Categorical, Integer\n\n# to calculate the score\nfrom sklearn.metrics import roc_auc_score\n\n# garbage collector: to free-up memory when needed\nimport gc\n\n# to keep track of time\nimport time\n\n# Standard Scaling\nfrom sklearn.preprocessing import StandardScaler\n# Preprocessing\nfrom sklearn.compose import ColumnTransformer\n# Pipeline\nfrom sklearn.pipeline import Pipeline","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:56:36.840947Z","iopub.execute_input":"2021-10-09T13:56:36.841255Z","iopub.status.idle":"2021-10-09T13:56:40.639194Z","shell.execute_reply.started":"2021-10-09T13:56:36.841219Z","shell.execute_reply":"2021-10-09T13:56:40.638415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Loading data sets using cudf (faster) and coverts to pandas (DataFrame)\ntrain = cudf.read_csv('../input/tabular-playground-series-oct-2021/train.csv', index_col = 'id').to_pandas()\n\ntrain.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:56:40.64126Z","iopub.execute_input":"2021-10-09T13:56:40.641495Z","iopub.status.idle":"2021-10-09T13:57:22.982793Z","shell.execute_reply.started":"2021-10-09T13:56:40.64147Z","shell.execute_reply":"2021-10-09T13:57:22.982131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:57:22.984158Z","iopub.execute_input":"2021-10-09T13:57:22.984501Z","iopub.status.idle":"2021-10-09T13:57:31.642899Z","shell.execute_reply.started":"2021-10-09T13:57:22.984463Z","shell.execute_reply":"2021-10-09T13:57:31.641189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking to make sure all columns are normalised\noutlmax = [col for col in train.columns if train[col].max()>1]\noutlmin = [col for col in train.columns if train[col].max()<0]\n\nprint(outlmax, outlmin)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:57:31.645185Z","iopub.execute_input":"2021-10-09T13:57:31.645457Z","iopub.status.idle":"2021-10-09T13:57:32.244901Z","shell.execute_reply.started":"2021-10-09T13:57:31.645424Z","shell.execute_reply":"2021-10-09T13:57:32.243828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how many entries are missing per column, how many unique entries per column\ntrain.isnull().sum().values, train.dtypes.values, train.nunique().values","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:57:32.2468Z","iopub.execute_input":"2021-10-09T13:57:32.247473Z","iopub.status.idle":"2021-10-09T13:57:40.71296Z","shell.execute_reply.started":"2021-10-09T13:57:32.247405Z","shell.execute_reply":"2021-10-09T13:57:40.711244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****From the results above, we see that there are no missing values and no categorical entries.\nThere are features with only 2 unique entries.****\n\n****Let us preprocess the features with several entries with StandardScaler****","metadata":{}},{"cell_type":"code","source":"# First let us separate X from y and divide the sets into search set - to perform a randomized search -\n# and opt set - to be fitted by the model with the best \"optimized hyperparameter set\" from the search.\nX = train.drop(columns='target')\ny = train.target\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size = 0.4, random_state=7)\n\n# since the datasets below won't be used here, we free up memory space by removing them\ndel train\ndel X\ndel y\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:57:40.714425Z","iopub.execute_input":"2021-10-09T13:57:40.714703Z","iopub.status.idle":"2021-10-09T13:57:44.145576Z","shell.execute_reply.started":"2021-10-09T13:57:40.714668Z","shell.execute_reply":"2021-10-09T13:57:44.144904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing\n# First, we separate the binary and non-binary features (columns)\nnb_cols = [col for col in X_train.columns if X_train[col].nunique() > 2]\n\n# Using StandardScaler on the columns with real entries (nb_cols)\n#sc = StandardScaler()\n#prepr = ColumnTransformer(transformers=[('stdscaler', sc, nb_cols)], remainder='passthrough')\n\n# Defining the model: 'gpu_hist' is important to run it faster with GPU\nmodel_search = XGBClassifier(tree_method='gpu_hist', use_label_encoder=False, eval_metric='auc', random_state=7)\n\n# Defining the pipeline to apply prepr and model\n#pipe = Pipeline(steps=[('preprocessing', prepr), ('model', model)])","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:57:44.149602Z","iopub.execute_input":"2021-10-09T13:57:44.151682Z","iopub.status.idle":"2021-10-09T13:57:47.277538Z","shell.execute_reply.started":"2021-10-09T13:57:44.151641Z","shell.execute_reply":"2021-10-09T13:57:47.276798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Now, we proceed to perform a ramdomized scan within \"interesting\" hyperparameter regions****","metadata":{}},{"cell_type":"code","source":"# Let us vary through the XGBoost hyperparamenters to see which setup gives the best result (score)\n\n# define the hyperparameters and the ranges to perform the scan\nsearch_spaces = {'n_estimators': Integer(100, 1000),'learning_rate': Real(0.01, 0.31, 'log-uniform'), 'max_depth': Integer(3, 14), \n                 'subsample': Real(0.1, 1, 'log-uniform'), 'colsample_bytree': Real(0.1, 1, 'log-uniform'),\n                 'colsample_bylevel':Real(0.1, 1, 'log-uniform'), 'min_child_weight': Integer(0, 10), 'reg_alpha': Integer(0, 15), \n                 'reg_lambda': Integer(0, 30)}\n\n# search_spaces = {'n_estimators': Integer(100, 1000),'learning_rate': Real(0.01, 0.31, 'log-uniform'), 'max_depth': Integer(3, 14), \n#                  'subsample': Real(0.1, 1, 'log-uniform')}\n\n\n\n# for cross validation with 5 splits, using StratifiedKFold to keep the same percentage of sample per each class\nskfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n\n# defining the Bayesian scan, n_iter=7 (picks 7 scenarios), using 'roc_auc' and the scoring method\nb_search = BayesSearchCV(model_search, search_spaces, n_iter=7, scoring='roc_auc', cv=skfold, random_state=7)\n\nprint(b_search.total_iterations)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:57:47.278875Z","iopub.execute_input":"2021-10-09T13:57:47.279134Z","iopub.status.idle":"2021-10-09T13:57:47.293267Z","shell.execute_reply.started":"2021-10-09T13:57:47.279104Z","shell.execute_reply":"2021-10-09T13:57:47.292175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\n# the model should fit the search set\nb_result = b_search.fit(X_train, y_train)\n\n# print the best score in the search and the corresponding best parameters\nprint(\"Best: %f using %s\" % (b_result.best_score_, b_result.best_params_))\n\n# print the total time...\nelapsed = time.time() - start\nprint(\"Seconds to run the scan: %f\" % (elapsed))\n\n# Search datasets won't be used anymore, so we remove them from memory\ndel X_train\ndel y_train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:57:47.294737Z","iopub.execute_input":"2021-10-09T13:57:47.295Z","iopub.status.idle":"2021-10-09T14:21:17.418537Z","shell.execute_reply.started":"2021-10-09T13:57:47.294968Z","shell.execute_reply":"2021-10-09T14:21:17.417882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Having found the best parameter set in the scan, we use these hyperparameters to fit the remaining dataset, i.e. X_opt and y_opt****","metadata":{}},{"cell_type":"code","source":"# now, using the tuned hyperparameters, we fit and test the model on the \"opt\" set \n# Defining the model\nmodel_opt = XGBClassifier(**b_result.best_params_, tree_method='gpu_hist', use_label_encoder=False, \n                         eval_metric='auc', random_state=7)\n\n\n# Optimal hyperparameters\n#opt_params = {'learning_rate': 0.0335660337575312, 'max_depth': 9, 'n_estimators': 963, 'subsample': 0.9883632916174595}\n#model_opt = XGBClassifier( n_estimators = 963, learning_rate = 0.0335660337575312, max_depth = 9, subsample= 0.9883632916174595, tree_method='gpu_hist', use_label_encoder=False, eval_metric='auc', random_state=7)\n\n# Put it inside a new pipeline... use the same preprocessing as before: prepr\n#pipe_opt = Pipeline(steps=[('preprocessing', prepr), ('model', model_opt)])\n\n\n#Let us split X_valid and y_valid into trainning and validation sets again to make use of the early_stopping_rounds feature\nX_train_2, X_valid_2, y_train_2, y_valid_2 = train_test_split(X_valid, y_valid, test_size=0.2, random_state=7)\n\n\n# To use eval_set, early_stopping rounds in the pipeline, we need to preprocess the eval_set beforehand \n# otherwise we get an error\n# Let's fit the prepr to the training set and transform the validation set to pass it to the pipeline\n# X_train_2_prepr = prepr.fit_transform(X_train_2)\n# X_valid_2_prepr = prepr.transform(X_valid_2)\n# eval_set_prepr = [(X_valid_2_prepr, y_valid_2)]\n\ndel X_valid\ndel y_valid\n# del X_valid_2\n# del X_train_2_prepr\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:21:17.419796Z","iopub.execute_input":"2021-10-09T14:21:17.420125Z","iopub.status.idle":"2021-10-09T14:21:18.760379Z","shell.execute_reply.started":"2021-10-09T14:21:17.420088Z","shell.execute_reply":"2021-10-09T14:21:18.759652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pipeline\n#pipe_opt.fit(X_train_2, y_train_2, model__eval_set=eval_set_prepr, model__early_stopping_rounds=15)\n# pipe_opt.fit(X_valid, y_valid)\n\nmodel_opt.fit(X_train_2, y_train_2, eval_set=[(X_valid_2, y_valid_2)],early_stopping_rounds=10)\n\ndel X_train_2\ndel y_train_2\n# del X_valid_2_prepr\n# del y_valid\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:21:18.761931Z","iopub.execute_input":"2021-10-09T14:21:18.762277Z","iopub.status.idle":"2021-10-09T14:23:21.650307Z","shell.execute_reply.started":"2021-10-09T14:21:18.762227Z","shell.execute_reply":"2021-10-09T14:23:21.649629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****With the model determined, we use it to make predictions based on the X_test set and save them in a csv file to submit for the competition****","metadata":{}},{"cell_type":"code","source":"# Loading the test set\nX_test = cudf.read_csv('../input/tabular-playground-series-oct-2021/test.csv', index_col = 'id').to_pandas()\n\n### We calculate and store the probability of the positive prediction\npred_test = model_opt.predict_proba(X_test)[:,1]\n\n\noutput = pd.DataFrame({'id': X_test.index,\n                       'target': pred_test})\noutput.to_csv('submission_TPSOct21.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:23:21.651624Z","iopub.execute_input":"2021-10-09T14:23:21.651902Z","iopub.status.idle":"2021-10-09T14:24:01.222991Z","shell.execute_reply.started":"2021-10-09T14:23:21.65187Z","shell.execute_reply":"2021-10-09T14:24:01.222272Z"},"trusted":true},"execution_count":null,"outputs":[]}]}