{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## About Tabular Playground Series - Oct 2021\n\nThe dataset used for this competition is synthetic, but based on a real dataset and generated using a CTGAN.\n\nThe dataset deals with predicting the biological response of molecules given various chemical properties. Although the features are anonymized, they have properties relating to real-world features.\n\n## About this notebook\n\nThis notebook is a work in progress and will be regularly updated. It is my first notebook of this competition and will concentrate on data exploration and creating a baseline for future notebooks that will concentrate on modelling different solutions. This is a beginner level notebook meant for my own use and not indended to be a training aid or tutorial. Some of the code will be taken from other public notebooks, sources will be creditted at the bottom.\n\n## First thoughts on this months project\n\n* This months Tabular Playground Dataset is once again quite large, so managing both cpu usage and ram is going to be an important element of the project.\n* It looks like another classification problem.\n* There is no missing data, so imputing values will not be required.\n* There a both categorical and continuous features. The categorical data is all binary and some of the continuous data appears to be category like. It may be possible to reduce the memory requirements by redefining data types in order to minimize memory use without lossing any meaningful information.\n* Data engineering and feature importance may be important.\n* Its likely that model selection and hyper parameter tuning will be important.\n* Staking, blending and ensambles are likely to be important to get higher scores.","metadata":{}},{"cell_type":"markdown","source":"## Set up environment","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport os, psutil\nimport gc\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import cross_validate,cross_val_score,train_test_split, KFold, GridSearchCV\nfrom sklearn.metrics import classification_report, accuracy_score, log_loss, roc_auc_score\nfrom sklearn import ensemble,metrics,model_selection,neighbors,preprocessing, svm, tree\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# machine learning tools\nimport h2o\nfrom h2o.estimators import H2OGeneralizedLinearEstimator, H2ORandomForestEstimator, H2OGradientBoostingEstimator\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-06T07:40:33.040381Z","iopub.execute_input":"2021-10-06T07:40:33.041284Z","iopub.status.idle":"2021-10-06T07:40:35.709039Z","shell.execute_reply.started":"2021-10-06T07:40:33.04121Z","shell.execute_reply":"2021-10-06T07:40:35.708082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create functions","metadata":{}},{"cell_type":"code","source":"%%time\n# taken from https://www.kaggle.com/ryanholbrook/getting-started-september-2021-tabular-playground\n\ndef cpu_stats():\n    pid = os.getpid()\n    py = psutil.Process(pid)\n    memory_use = py.memory_info()[0] / 2. ** 30\n    return 'memory GB:' + str(np.round(memory_use, 2))\n\ndef score(X, y, model, cv):\n    scoring = [\"roc_auc\"]\n    scores = cross_validate(\n        model, X_train, y_train, scoring=scoring, cv=cv, return_train_score=True\n    )\n    scores = pd.DataFrame(scores).T\n    return scores.assign(\n        mean = lambda x: x.mean(axis=1),\n        std = lambda x: x.std(axis=1),\n    )\nprint('Function built')\n","metadata":{"execution":{"iopub.status.busy":"2021-10-06T07:40:35.710585Z","iopub.execute_input":"2021-10-06T07:40:35.711307Z","iopub.status.idle":"2021-10-06T07:40:35.720539Z","shell.execute_reply.started":"2021-10-06T07:40:35.711242Z","shell.execute_reply":"2021-10-06T07:40:35.719752Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get data","metadata":{}},{"cell_type":"code","source":"%%time\n# Get data\ntrain=pd.read_csv('../input/tabular-playground-series-oct-2021/train.csv')\ntest=pd.read_csv('../input/tabular-playground-series-oct-2021/test.csv')\nprint(\"Data imported\")\n\n## from: https://www.kaggle.com/bextuychiev/how-to-work-w-million-row-datasets-like-a-pro\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df\n\ntrain = reduce_memory_usage(train, verbose=True)\ntest = reduce_memory_usage(test, verbose=True)\nprint(cpu_stats())\nprint('Memory reduced')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-06T07:40:35.721666Z","iopub.execute_input":"2021-10-06T07:40:35.722427Z","iopub.status.idle":"2021-10-06T07:43:07.676282Z","shell.execute_reply.started":"2021-10-06T07:40:35.722399Z","shell.execute_reply":"2021-10-06T07:43:07.675629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get features","metadata":{}},{"cell_type":"code","source":"%%time\nfeatures=[]\ncat_features=[]","metadata":{"execution":{"iopub.status.busy":"2021-10-06T07:45:38.496565Z","iopub.execute_input":"2021-10-06T07:45:38.49681Z","iopub.status.idle":"2021-10-06T07:45:38.945119Z","shell.execute_reply.started":"2021-10-06T07:45:38.496785Z","shell.execute_reply":"2021-10-06T07:45:38.944044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_features=[]\nfor feature in test.columns:\n    features.append(feature)\n    if test.dtypes[feature]=='int8':\n        cat_features.append(feature)\n    if test.dtypes[feature]=='float16':\n        cont_features.append(feature)\n    #print(test.dtypes[feature])\nprint('features obtained')\n\nplt.pie([len(cat_features), len(cont_features)], \n        labels=['Categorical', 'Continuos'],\n        colors=['skyblue', 'blue'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-06T07:45:38.496565Z","iopub.execute_input":"2021-10-06T07:45:38.49681Z","iopub.status.idle":"2021-10-06T07:45:38.945119Z","shell.execute_reply.started":"2021-10-06T07:45:38.496785Z","shell.execute_reply":"2021-10-06T07:45:38.944044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## About imported data","metadata":{}},{"cell_type":"code","source":"%%time\n# Get shape of data\nprint('*'*40, '\\nHow much data was imported?')\nprint('*'*40)\nprint('Training data :', train.shape)\nprint('Test data :', test.shape)\nprint('*'*40,\"\\n\")\n\n# missing data\nprint('*'*40,'\\nHow much data is missing?')\nprint('*'*40)\ntraining_missing_val_count_by_column = (train.isnull().values.sum())\ntest_missing_val_count_by_column = (test.isnull().values.sum())\nprint('Missing training data :  {:.2f} ({:.1f})%'.format (training_missing_val_count_by_column,training_missing_val_count_by_column/train.shape[0]))\nprint('Missing test data :  {:.2f} ({:.1f})%'.format (test_missing_val_count_by_column,test_missing_val_count_by_column/test.shape[0]))\nprint('*'*40,\"\\n\")\n\n# categorical data\nprint('*'*40,'\\nFeature types?')\nprint('*'*40)\nprint('Categorical features : ', (len(cat_features)))\nprint('Continuous features : ', (len(cont_features)))\nprint('*'*40,'\\n')\n\n# get info\nprint('*'*40,'\\nInfo on datasets')\nprint('*'*40)\nprint(train.info(),'\\n')\nprint(test.info(),'\\n')\nprint('*'*40)\n\nprint('\\noverview complete')  \n\nplt.pie([len(train), len(test)], \n        labels=['train', 'test'],\n        colors=['skyblue', 'blue'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()\n\ndel training_missing_val_count_by_column,test_missing_val_count_by_column","metadata":{"execution":{"iopub.status.busy":"2021-10-06T02:48:32.040317Z","iopub.execute_input":"2021-10-06T02:48:32.041366Z","iopub.status.idle":"2021-10-06T02:48:33.794778Z","shell.execute_reply.started":"2021-10-06T02:48:32.041323Z","shell.execute_reply":"2021-10-06T02:48:33.793944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[](http://)","metadata":{}},{"cell_type":"markdown","source":"## Sample Data","metadata":{}},{"cell_type":"code","source":"%%time\n# Get column titles\n#print('*'*40,'\\nColumn Names')\n#print('*'*40)\n#print(features)\n#print('*'*40)\n\n# Get sample data\nprint('*'*40,'\\nSample Training Data')\nprint('*'*40)\nprint(train.head(),'\\n')\n#print('*'*40,'\\n')\n#print('*'*40,'\\nSample Test Data')\n#print('*'*40)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-05T10:21:04.976798Z","iopub.execute_input":"2021-10-05T10:21:04.97751Z","iopub.status.idle":"2021-10-05T10:21:05.080995Z","shell.execute_reply.started":"2021-10-05T10:21:04.977466Z","shell.execute_reply":"2021-10-05T10:21:05.080154Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distibution of Categorical Data","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_outliers = ((train[cat_features] - train[cat_features] .min())/(train[cat_features] .max() - train[cat_features] .min()))\ntest_outliers = ((test[cat_features] - test[cat_features].min())/(test[cat_features].max() - test[cat_features].min()))\ntry:\n    train_outliers.drop(['claim'], axis=1, inplace=True)\nexcept:\n    print('Already separated')\nprint('Training data (blue), Test Data (red)')\nfig = plt.figure(figsize = (20, 40))\nfor idx, i in enumerate(train_outliers.columns):\n    fig.add_subplot(np.ceil(len(train_outliers.columns)/4), 4, idx+1)\n    train_outliers.iloc[:, idx].hist(bins = 2,color='b',alpha=0.5)\n    test_outliers.iloc[:, idx].hist(bins = 2,color='r',alpha=0.5)\n    plt.title(i)\n#plt.text(9, -20000, caption, size = 12)\nplt.show()\ndel train_outliers, test_outliers","metadata":{"execution":{"iopub.status.busy":"2021-10-06T01:49:06.059332Z","iopub.execute_input":"2021-10-06T01:49:06.060263Z","iopub.status.idle":"2021-10-06T01:49:15.175301Z","shell.execute_reply.started":"2021-10-06T01:49:06.060214Z","shell.execute_reply":"2021-10-06T01:49:15.174174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distibution of Categorical Data by target value","metadata":{}},{"cell_type":"code","source":"%%time\n# category features by target value\n\n# separate data by target value\ntrain_0=train[train.target==0]\ntrain_1=train[train.target==1]\n\n# separate cat features\ntrain_outliers = ((train[cat_features] - train[cat_features] .min())/(train[cat_features] .max() - train[cat_features] .min()))\ntrain_outliers_0 = ((train_0[cat_features] - train_0[cat_features] .min())/(train_0[cat_features] .max() - train_0[cat_features] .min()))\ntrain_outliers_1 = ((train_1[cat_features] - train_1[cat_features].min())/(train_1[cat_features].max() - train_1[cat_features].min()))\ntry:\n    train_outliers_0.drop(['claim'], axis=1, inplace=True)\nexcept:\n    print('Already separated')\ntry:\n    train_outliers_1.drop(['claim'], axis=1, inplace=True)\nexcept:\n    print('Already separated')\nprint('Distribution for category features, red=target 0, blue=target 1')\nfig = plt.figure(figsize = (20, 40))\nfor idx, i in enumerate(train_outliers_0.columns):\n    fig.add_subplot(np.ceil(len(train_outliers_0.columns)/4), 4, idx+1)\n    train_outliers_0.iloc[:, idx].hist(bins = 2,color='skyblue',alpha=0.3)\n    train_outliers_1.iloc[:, idx].hist(bins = 2,color='blue',alpha=0.3)\n    plt.title(i)\nplt.show()\ndel train_outliers_0, train_outliers_1,train_0, train_1","metadata":{"execution":{"iopub.status.busy":"2021-10-06T03:27:11.502042Z","iopub.execute_input":"2021-10-06T03:27:11.50243Z","iopub.status.idle":"2021-10-06T03:27:20.214594Z","shell.execute_reply.started":"2021-10-06T03:27:11.502389Z","shell.execute_reply":"2021-10-06T03:27:20.213757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of Continuous data","metadata":{}},{"cell_type":"code","source":"%%time\n#train_outliers = ((train[cont_features] - train[cont_features].min())/(train[cont_features].max() - train[cont_features].min()))\n#train_test = ((test[cont_features] - test[cont_features].min())/(test[cont_features].max() - test[cont_features].min()))\ntrain_outliers=train[cont_features]\ntest_outliers=test[cont_features]\ntry:\n    train_outliers.drop(['claim'], axis=1, inplace=True)\nexcept:\n    print('Already separated')\nprint('Training data (blue), Test Data (red)')\nfig = plt.figure(figsize = (40, 140))\nfor idx, i in enumerate(train_outliers.columns):\n    fig.add_subplot(np.ceil(len(train_outliers.columns)/4), 4, idx+1)\n    train_outliers.iloc[:, idx].hist(bins=20,color='b',alpha=0.5)\n    test_outliers.iloc[:, idx].hist(bins = 20,color='r',alpha=0.5)    \n    plt.subplots_adjust(hspace=0.2)\n    plt.title(i)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T10:21:20.626895Z","iopub.execute_input":"2021-10-05T10:21:20.627372Z","iopub.status.idle":"2021-10-05T10:22:42.628227Z","shell.execute_reply.started":"2021-10-05T10:21:20.627331Z","shell.execute_reply":"2021-10-05T10:22:42.627359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# continuous features by target value\n\n# separate data by target value\ntrain_0=train[train.target==0]\ntrain_1=train[train.target==1]\n\ntrain_outliers_0=train_0[cont_features]\ntrain_outliers_1=train_1[cont_features]\ntry:\n    train_outliers_0.drop(['claim'], axis=1, inplace=True)\nexcept:\n    print('Already separated')\ntry:\n    train_outliers_1.drop(['claim'], axis=1, inplace=True)\nexcept:\n    print('Already separated')\nprint('Distribution of continuous features, red=target0, blue=target1')\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(18, 150), facecolor='#EAEAF2')\nfor idx, i in enumerate(train_outliers_0.columns):\n    fig.add_subplot(np.ceil(len(train_outliers_0.columns)/4), 4, idx+1)\n    train_outliers_0.iloc[:, idx].hist(bins=20,color='b',alpha=0.5)\n    train_outliers_1.iloc[:, idx].hist(bins = 20,color='r',alpha=0.5)  \n    plt.subplots_adjust(hspace=0.2)\n    plt.title(i)\nplt.show()\n\ndel train_0,train_1,train_outliers_0,train_outliers_1","metadata":{"execution":{"iopub.status.busy":"2021-10-06T04:12:25.00525Z","iopub.execute_input":"2021-10-06T04:12:25.005486Z","iopub.status.idle":"2021-10-06T04:14:12.179673Z","shell.execute_reply.started":"2021-10-06T04:12:25.005459Z","shell.execute_reply":"2021-10-06T04:14:12.178826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I thaught for one glorious moment that i'd hit upon some great secret within the data,where features like 196 and 202 had separatable distibutions of feature values directly correlated to the target value, however it looks like this is mearly down to the way that data is scaling to produce the histograms. The original histograms or kdeplot of the same data don't show the same separation of the data.   ","metadata":{}},{"cell_type":"markdown","source":"## Kde plot of feature distribution colour coded by target value","metadata":{}},{"cell_type":"code","source":"%%time\n# continuous features by target value\n\n# separate data by target value\ntrain_0=train[train.target==0]\ntrain_1=train[train.target==1]\n\ntrain_outliers_0=train_0[cont_features]\ntrain_outliers_1=train_1[cont_features]\ntry:\n    train_outliers_0.drop(['claim'], axis=1, inplace=True)\nexcept:\n    print('Already separated')\ntry:\n    train_outliers_1.drop(['claim'], axis=1, inplace=True)\nexcept:\n    print('Already separated')\nprint('Distribution of continuous features, red=target0, blue=target1')\n\nprint(\"Feature distribution of continous features: \")\nncols = 5\nnrows = int(len(cont_features) / ncols )#+ (len(features) % ncols > 0))\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(18, 150), facecolor='#EAEAF2')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = cont_features[r*ncols+c]\n        sns.kdeplot(x=train_outliers_0[col], ax=axes[r, c], color='blue', label='target=0')\n        sns.kdeplot(x=train_outliers_1[col], ax=axes[r, c], color='orange', label='target=1')\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8, fontweight='bold')\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(4)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:26:27.189858Z","iopub.execute_input":"2021-10-06T08:26:27.190103Z","iopub.status.idle":"2021-10-06T08:57:42.86206Z","shell.execute_reply.started":"2021-10-06T08:26:27.19008Z","shell.execute_reply":"2021-10-06T08:57:42.861069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Boxplots of continuous features","metadata":{}},{"cell_type":"code","source":"%%time\n# generate box plots\ntrain_outliers = ((train[cont_features] - train[cont_features].min())/(train[cont_features].max() - train[cont_features].min()))\nfig, ax = plt.subplots(8, 1, figsize = (25,25))\nsns.boxplot(data = train_outliers.iloc[:, 1:30], ax = ax[0])\nsns.boxplot(data = train_outliers.iloc[:, 30:60], ax = ax[1])\nsns.boxplot(data = train_outliers.iloc[:, 60:90], ax = ax[2])\nsns.boxplot(data = train_outliers.iloc[:, 90:120], ax = ax[3])\nsns.boxplot(data = train_outliers.iloc[:, 120:150], ax = ax[4])\nsns.boxplot(data = train_outliers.iloc[:, 150:180], ax = ax[5])\nsns.boxplot(data = train_outliers.iloc[:, 180:210], ax = ax[6])\nsns.boxplot(data = train_outliers.iloc[:, 210:240], ax = ax[7])\nplt.show()\ndel train_outliers","metadata":{"execution":{"iopub.status.busy":"2021-10-05T10:22:42.629855Z","iopub.execute_input":"2021-10-05T10:22:42.630299Z","iopub.status.idle":"2021-10-05T10:23:54.896013Z","shell.execute_reply.started":"2021-10-05T10:22:42.63026Z","shell.execute_reply":"2021-10-05T10:23:54.895172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature correlation of categogical data","metadata":{}},{"cell_type":"code","source":"%%time\n# Generate correlations in categorical data\ncorr=train[cat_features].corr()\n\n# create heatmap\nmask = np.triu(np.ones_like(corr, dtype = bool))\nplt.figure(figsize = (15, 15))\nplt.title('Correlation matrix for categorigal features of Training data')\nsns.heatmap(corr,cmap='coolwarm', mask = mask,annot=False, linewidths = .5,square=True,cbar_kws={\"shrink\": .60})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T10:21:13.655494Z","iopub.execute_input":"2021-10-05T10:21:13.655744Z","iopub.status.idle":"2021-10-05T10:21:20.625375Z","shell.execute_reply.started":"2021-10-05T10:21:13.655717Z","shell.execute_reply":"2021-10-05T10:21:20.624545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature correlation of continuous data","metadata":{}},{"cell_type":"code","source":"%%time\n# get list of columns with high correlations\ncorr = train[cont_features].corr().abs()\nhigh_corr=np.where(corr>0.02)\nhigh_corr=[(corr.columns[x],corr.columns[y]) for x,y in zip(*high_corr) if x!=y and x<y]\nprint('initial correlations calculated')\n#print(\"high correlation \\n\",high_corr)\nhigh_corr_features=[]\nfor x in high_corr:\n    for item in x:\n        if item not in high_corr_features:\n            high_corr_features.append(item)\ncorr_matrix = train[high_corr_features].corr()\nprint('highest correlations calculated')\n\n# create heatmap\nmask = np.triu(np.ones_like(corr_matrix, dtype = bool))\nplt.figure(figsize = (15, 15))\nplt.title('Correlation matrix for continuous features of training data')\nsns.heatmap(corr_matrix, cmap='coolwarm',mask = mask,annot=False, linewidths = .5,square=True,cbar_kws={\"shrink\": .60})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T10:23:54.897219Z","iopub.execute_input":"2021-10-05T10:23:54.897817Z","iopub.status.idle":"2021-10-05T10:26:42.300658Z","shell.execute_reply.started":"2021-10-05T10:23:54.897782Z","shell.execute_reply":"2021-10-05T10:26:42.299691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature correlation to target","metadata":{}},{"cell_type":"code","source":"%%time\n# correlations to target\n# idea/code taken https://www.kaggle.com/rahullalu/tps-oct-2021-eda-and-baseline\n\ncorr_cat=pd.DataFrame()\ncorr_cat['target'] = train[cat_features].corrwith(train['target'])\ndf_cat=corr_cat.sort_values(by='target', ascending=False)\n\ncorr_cont=pd.DataFrame()\ncorr_cont['target'] = train[high_corr_features].corrwith(train['target'])\ndf_cont=corr_cont.sort_values(by='target', ascending=False)\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 10))\nfig.suptitle('Correlation to Target')\n\nheatmap = sns.heatmap(ax=axes[0],data=df_cat,annot=True,cmap='tab20c',linewidth=0.5,xticklabels=df_cat.columns,yticklabels=df_cat.index)\nheatmap = sns.heatmap(ax=axes[1],data=df_cont,annot=True,cmap='tab20c',linewidth=0.5,xticklabels=df_cont.columns,yticklabels=df_cont.index)\nplt.show()\n\ncorr_cont=pd.DataFrame()\ncorr_cont['target'] = train[cont_features].corrwith(train['target'])\ndf_cont=corr_cont.sort_values(by='target', ascending=False)\n\ndel df_cont, df_cat, corr_cat, corr_cont","metadata":{"execution":{"iopub.status.busy":"2021-10-05T10:26:42.301861Z","iopub.execute_input":"2021-10-05T10:26:42.302153Z","iopub.status.idle":"2021-10-05T10:26:51.35974Z","shell.execute_reply.started":"2021-10-05T10:26:42.302117Z","shell.execute_reply":"2021-10-05T10:26:51.359038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature importance - categorical features","metadata":{}},{"cell_type":"code","source":"# Feature importance lgbm\nX=train[cat_features]\ny=train['target']\n# Split data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,train_size=0.8,test_size = 0.2,random_state = 0)\n\n# instanciate and fit model\nlgbm_checker = LGBMClassifier(learning_rate=0.05,\n                      n_estimators=1000,\n                      reg_lambda = 1)\n\nlgbm_checker.fit(X_train, y_train)\n\n# put feature impoartance into table\nimportances_df = pd.DataFrame(lgbm_checker.feature_importances_, columns=['Feature_Importance'],\n                              index=X_train.columns)\nimportances_df.sort_values(by=['Feature_Importance'], ascending=False, inplace=True)\n#print(importances_df)\n\n# plot importance as bar chart\nimportances=importances_df.index\nimportances_df = importances_df.sort_values(['Feature_Importance'])\ny_pos = np.arange(len(importances_df))\nplt.figure(figsize=(8,10))\nplt.barh(y_pos,importances_df['Feature_Importance'])\nplt.yticks(y_pos, importances,fontsize=10)\nplt.ylabel('importance')\nplt.title('Feature importance for Lgbm classifier')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T10:26:51.360759Z","iopub.execute_input":"2021-10-05T10:26:51.361592Z","iopub.status.idle":"2021-10-05T10:27:34.799485Z","shell.execute_reply.started":"2021-10-05T10:26:51.361546Z","shell.execute_reply":"2021-10-05T10:27:34.798463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature importance - continuous features","metadata":{}},{"cell_type":"code","source":"# Feature importance lgbm\nX=train[cont_features]\ny=train['target']\n# Split data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,train_size=0.8,test_size = 0.2,random_state = 0)\n\n# instanciate and fit model\nlgbm_checker = LGBMClassifier(learning_rate=0.05,\n                      n_estimators=1000,\n                      reg_lambda = 1)\n\nlgbm_checker.fit(X_train, y_train)\n\n# put feature impoartance into table\nimportances_df = pd.DataFrame(lgbm_checker.feature_importances_, columns=['Feature_Importance'],\n                              index=X_train.columns)\nimportances_df.sort_values(by=['Feature_Importance'], ascending=False, inplace=True)\n#print(importances_df)\n\n# plot importance as bar chart\nimportances=importances_df.index\nimportances_df = importances_df.sort_values(['Feature_Importance'])\ny_pos = np.arange(len(importances_df))\nplt.figure(figsize=(8,25))\nplt.barh(y_pos,importances_df['Feature_Importance'])\nplt.yticks(y_pos, importances,fontsize=10)\nplt.ylabel('importance')\nplt.title('Feature importance for Lgbm classifier')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T10:27:34.801225Z","iopub.execute_input":"2021-10-05T10:27:34.801546Z","iopub.status.idle":"2021-10-05T10:36:01.311416Z","shell.execute_reply.started":"2021-10-05T10:27:34.801508Z","shell.execute_reply":"2021-10-05T10:36:01.30912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SHAP analysis\n\nThe goal of SHAP is to explain the prediction of an instance by computing the contribution of each feature to the prediction. The SHAP explanation method computes Shapley values from coalitional game theory. The feature values of a data instance act as players in a coalition. Shapley values tell us how to fairly distribute the “payout” among the features.\n\nThe idea behind SHAP feature importance is simple: Features with large absolute Shapley values are important. Since we want the global importance, we sum the absolute Shapley values per feature across the data:","metadata":{}},{"cell_type":"code","source":"%%time\n#taken from https://www.kaggle.com/docxian/tabular-playground-10-first-glance-baseline/notebook#Model \n\n# extract list of features\nfeatures = train.columns.tolist()\nfeatures.remove('id')\nfeatures.remove('target')\n\n# select predictors\npredictors = features\nprint('Number of predictors: ', len(predictors))\n\n# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # define maximum memory usage and number of cores\n\n# upload data in H2O environment\n# let's start with a SUBSET of training data only to reduce RAM use!\nn_sub = 50000\ntrain_sub = train.sample(n=n_sub, random_state=42)\ntrain_hex = h2o.H2OFrame(train_sub)\n\n# force categorical target\ntrain_hex['target'] = train_hex['target'].asfactor()\n\n# fit Gradient Boosting model\nn_cv = 5 # 5 folds\n\nfit_GBM = H2OGradientBoostingEstimator(ntrees=250,\n                                       max_depth=6,\n                                       min_rows=10,\n                                       learn_rate=0.1, # default: 0.1\n                                       sample_rate=1,\n                                       col_sample_rate=0.5,\n                                       nfolds=n_cv,\n                                       score_each_iteration=True,\n                                       stopping_metric='auc',\n                                       stopping_rounds=5,\n                                       stopping_tolerance=0.0001*0.5,\n                                       seed=999)\n# train model\nfit_GBM.train(x=predictors,\n              y='target',\n              training_frame=train_hex)\n\n# variable importance\nfit_GBM.varimp_plot()\n\n# alternative variable importance using SHAP => see direction as well as severity of feature impact\nfit_GBM.shap_summary_plot(train_hex);","metadata":{"execution":{"iopub.status.busy":"2021-10-05T11:09:22.021324Z","iopub.execute_input":"2021-10-05T11:09:22.021613Z","iopub.status.idle":"2021-10-05T11:14:37.592646Z","shell.execute_reply.started":"2021-10-05T11:09:22.021561Z","shell.execute_reply":"2021-10-05T11:14:37.591896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation between feature f22 and target\n\nFeature f22 produced some unusual results; It appears to have a high negative correlation to the target value but a very low correlation to other features and it scores poorly in terms of feature importance using basic models. For this reason i've singled it out to look at the distribution compared to target distribution and to produce a mossaic plot of the values against target.","metadata":{}},{"cell_type":"code","source":"%%time\n# feature 'f22' ditribution v's target\nsns.distplot(train['f22'], kde=True, hist=False, color='blue', label='f22')\nsns.distplot(train['target'], kde=True, hist=False, color='red', label='target')\nplt.legend()\n","metadata":{"execution":{"iopub.status.busy":"2021-10-05T10:36:06.20404Z","iopub.execute_input":"2021-10-05T10:36:06.204408Z","iopub.status.idle":"2021-10-05T10:36:15.805279Z","shell.execute_reply.started":"2021-10-05T10:36:06.204369Z","shell.execute_reply":"2021-10-05T10:36:15.804388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mosaic plot for f22 versus target values","metadata":{}},{"cell_type":"code","source":"# plot target vs binary features using mosaic plot\n# taken from https://www.kaggle.com/docxian/tabular-playground-10-first-glance-baseline/notebook#Model\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nfor f in ['f22']:\n    plt.rcParams['figure.figsize'] = (6,4) # increase plot size for mosaics\n    mosaic(train, [f, 'target'], title='Target vs ' + f)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T11:14:37.60149Z","iopub.execute_input":"2021-10-05T11:14:37.602204Z","iopub.status.idle":"2021-10-05T11:14:37.822233Z","shell.execute_reply.started":"2021-10-05T11:14:37.602166Z","shell.execute_reply":"2021-10-05T11:14:37.821681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like approximately 25-30% of the values for f22 are either (0,0) or (1,1) meaning that if all you wanted was to be right 70-75% of the time this would work really well,but because there is little correlation between f22 and the other features if you want to score above 70-75% you shouldn't rely too heavily on this feature. ","metadata":{}},{"cell_type":"markdown","source":"# Baseline lgbm submission\n\nThis notebook is mainly about exploring the data, but we'll also produce a basic model with the default parameters to get a baseline on how well the basic model performs without any parameter tuning or feature engineering.","metadata":{}},{"cell_type":"code","source":"%%time\nX=train[cont_features]\ny=train['target']\n# Split data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,train_size=0.8,test_size = 0.2,random_state = 0)\nprint('data split')\n\n# instanciate model\nlgbmmodel = LGBMClassifier(learning_rate=0.05,\n                      n_estimators=1000,\n                      reg_lambda = 1)\n# fit model\nlgbmmodel.fit(X_train, y_train)\nprint('model fit')\n\n# score model\ny_preds = lgbmmodel.predict(X_valid)#[:, 1]\nlgbm_score=roc_auc_score(y_valid, y_preds)\nprint(\"Area under the curve score ; \", lgbm_score)\n\n# evaluate model\ny_preds = lgbmmodel.predict(X_valid)\nlgbscores=roc_auc_score(y_valid, y_preds)\nprint(\"Area under the curve score (binary) ; \",lgbscores)\n\nlgbscore = score(X_train, y_train, lgbmmodel, cv=4)\ndisplay(lgbscore)\nprint('scoring completed')\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(lgbmmodel, X_valid, y_valid)\nplt.title('Confusion matrix')\nplt.grid(False)\nplt.show()\n\n# predict test\nlgbm_preds = lgbmmodel.predict((test)[cont_features])\nprint('predictions complete')\n\n# Save the predictions to a CSV file\nlgbm_submission = pd.read_csv(\"../input/tabular-playground-series-oct-2021/sample_submission.csv\")\nlgbm_submission.target = lgbm_preds\nlgbm_submission.to_csv(\"lgbm_pbaseline_submission.csv\",index=False)\nprint('lgbm submission complete')   \n\nsns.distplot(lgbm_submission['target'], kde=True, hist=False, color='blue', label='prediction')\nsns.distplot(train['target'], kde=True, hist=False,color='red', label='target')","metadata":{"execution":{"iopub.status.busy":"2021-10-05T10:36:16.038409Z","iopub.execute_input":"2021-10-05T10:36:16.038918Z","iopub.status.idle":"2021-10-05T11:09:22.019259Z","shell.execute_reply.started":"2021-10-05T10:36:16.03888Z","shell.execute_reply":"2021-10-05T11:09:22.018119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Total memory usage","metadata":{}},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"execution":{"iopub.status.busy":"2021-10-05T11:14:37.593867Z","iopub.execute_input":"2021-10-05T11:14:37.594627Z","iopub.status.idle":"2021-10-05T11:14:37.598978Z","shell.execute_reply.started":"2021-10-05T11:14:37.594567Z","shell.execute_reply":"2021-10-05T11:14:37.598438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Observation on Data\n\n* The test dataset is approx 1/2 the size of the training dataset\n* The training dataset is highly representative of the test dataset\n* There is no missing data\n* Approx 1/6th of features are binary features\n* Approx 5/6th of features are continuous features\n* There is relatively low correlation between features\n* There appears to be a relatively high correlation between f22 and target value.\n* The majority of categorical features have a negative correlation to target classification.\n* Continuous feature have show both positive and negative correlations to target classification.\n* feature importance indicates that there are a number of both categorical and continuous  features of importance.\n* feature importance doesn't indicate f22 as an important feature.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Next Steps\n\nThe next steps would be to build some basic models and see how they perform.\nThis is covered in my second notebook where i compare results for 20 classification models. This can be found here https://www.kaggle.com/davidcoxon/20-model-comparison-oct-tabular-playground","metadata":{}},{"cell_type":"markdown","source":"## Credit where credits due\n\nFirst up thanks to the Kaggle team for the tireless work putting the tabular plaground together. Many thanks to the kaggle and stackoverflow communities and the folks that contitbutor to the various documents for the various python modules, without whom finding solutions to these problems would be so much rougher.  \n\nSpecific thanks this month go to:\n\nhttps://www.kaggle.com/rahullalu for https://www.kaggle.com/rahullalu/tps-oct-2021-eda-and-baseline on correlation to target\n\nhttps://www.kaggle.com/docxian for https://www.kaggle.com/docxian/tabular-playground-10-first-glance-baseline/notebook#Model on mosaic plots\n\nhttps://www.kaggle.com/docxian for https://www.kaggle.com/docxian/tabular-playground-10-first-glance-baseline/notebook#Model on SHAP\n\nhttps://www.kaggle.com/pourchot for comments on notebook.\n\nIf you found by notebook useful or you have comments please upvote / comment here. If you found any of the code from other sources useful (or used it in your own projects) please take the time to upvote their notebooks)","metadata":{}}]}