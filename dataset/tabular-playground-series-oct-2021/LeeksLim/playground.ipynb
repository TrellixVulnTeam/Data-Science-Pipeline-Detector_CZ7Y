{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \ndf = pd.read_csv('../input/tabular-playground-series-oct-2021/train.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-25T00:17:26.641923Z","iopub.execute_input":"2022-02-25T00:17:26.642202Z","iopub.status.idle":"2022-02-25T00:18:08.937901Z","shell.execute_reply.started":"2022-02-25T00:17:26.642173Z","shell.execute_reply":"2022-02-25T00:18:08.937055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Some Basic EDA","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T19:59:34.974478Z","iopub.execute_input":"2022-02-24T19:59:34.97474Z","iopub.status.idle":"2022-02-24T19:59:35.005473Z","shell.execute_reply.started":"2022-02-24T19:59:34.974706Z","shell.execute_reply":"2022-02-24T19:59:35.004471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- Columns are anonymized, but there's a mix of numerical and binary features. The last column is the label and id should be dropped before training.","metadata":{}},{"cell_type":"code","source":"df.isna().any().any()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T19:59:35.006897Z","iopub.execute_input":"2022-02-24T19:59:35.007277Z","iopub.status.idle":"2022-02-24T19:59:35.235132Z","shell.execute_reply.started":"2022-02-24T19:59:35.007236Z","shell.execute_reply":"2022-02-24T19:59:35.234351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- Above shows no missing values were found","metadata":{}},{"cell_type":"code","source":"# dirty way of forcing kaggle to display all columns, transposed\ndf.iloc[:,1:287].describe(include = 'all').T.style","metadata":{"execution":{"iopub.status.busy":"2022-02-24T19:59:35.237115Z","iopub.execute_input":"2022-02-24T19:59:35.237526Z","iopub.status.idle":"2022-02-24T19:59:44.358483Z","shell.execute_reply.started":"2022-02-24T19:59:35.23749Z","shell.execute_reply":"2022-02-24T19:59:44.357781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- Target is supposed to be a binary label and above shows target column has a \"mean\" close to 0.5, so it's a balanced set\n- All features seem to have range 0-1, with a subset of the later features binary. This also means the numerical features do not need to be scaled","metadata":{}},{"cell_type":"code","source":"# take a quick look at features with the highest correlation with each other\nstg = df.iloc[:,1:287]\n\n# using corr method on dataframe creates a symmetrical matrix where about half the values are redundant since corr(a,b) = corr(b,a) for this case\npairs_to_drop = set()\ncols = stg.columns\nfor i in range(0, stg.shape[1]):\n    for j in range(0, i+1):\n        pairs_to_drop.add((cols[i], cols[j]))\n\nstg = stg.corr().abs()\nstg = stg.unstack()\nstg = stg.drop(labels=pairs_to_drop).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:37:34.33827Z","iopub.execute_input":"2022-02-24T21:37:34.33856Z","iopub.status.idle":"2022-02-24T21:40:50.063117Z","shell.execute_reply.started":"2022-02-24T21:37:34.33853Z","shell.execute_reply":"2022-02-24T21:40:50.062406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stg.head(100)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:46:13.360647Z","iopub.execute_input":"2022-02-24T21:46:13.361282Z","iopub.status.idle":"2022-02-24T21:46:13.373538Z","shell.execute_reply.started":"2022-02-24T21:46:13.361243Z","shell.execute_reply":"2022-02-24T21:46:13.372552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- The features are not very correlated, with the highest values less than 0.10\n- Also interesting to see f22 is highly correlated with the target","metadata":{}},{"cell_type":"markdown","source":"# Modeling - Variations of Logistic Regression","metadata":{}},{"cell_type":"code","source":"# drop id column, and split into features x and target y\nx = df.iloc[:,1:286]\ny = df.iloc[:,286]","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:18:08.939785Z","iopub.execute_input":"2022-02-25T00:18:08.94007Z","iopub.status.idle":"2022-02-25T00:18:09.579993Z","shell.execute_reply.started":"2022-02-25T00:18:08.940024Z","shell.execute_reply":"2022-02-25T00:18:09.579053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# just to reduce mem usage\nimport gc\ndel df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:18:09.581189Z","iopub.execute_input":"2022-02-25T00:18:09.582593Z","iopub.status.idle":"2022-02-25T00:18:09.680199Z","shell.execute_reply.started":"2022-02-25T00:18:09.582554Z","shell.execute_reply":"2022-02-25T00:18:09.679329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\n\n# Since the competition explicitly scores by the Area Under ROC curve, we will use it as the performance metric for every model\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import roc_curve # for more custom plotting\nfrom sklearn.metrics import roc_auc_score\n\n# for saving and loading models\nfrom joblib import dump, load","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:18:09.682259Z","iopub.execute_input":"2022-02-25T00:18:09.682495Z","iopub.status.idle":"2022-02-25T00:18:10.521487Z","shell.execute_reply.started":"2022-02-25T00:18:09.682464Z","shell.execute_reply":"2022-02-25T00:18:10.520669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:18:10.522672Z","iopub.execute_input":"2022-02-25T00:18:10.523814Z","iopub.status.idle":"2022-02-25T00:18:13.055806Z","shell.execute_reply.started":"2022-02-25T00:18:10.523774Z","shell.execute_reply":"2022-02-25T00:18:13.054884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del x, y\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:18:13.057092Z","iopub.execute_input":"2022-02-25T00:18:13.057367Z","iopub.status.idle":"2022-02-25T00:18:13.164211Z","shell.execute_reply.started":"2022-02-25T00:18:13.057331Z","shell.execute_reply":"2022-02-25T00:18:13.163473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first, we benchmark using a basic bare-bones logistic regression\n# sklearn's base logistic regression uses L2 regularization by default\nlogreg = LogisticRegression(solver='liblinear', random_state = 42)\nlogreg.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keep track of the train and test scores for comparing later\nmodel_brief_desc = []\ntrain_scores = []\ntest_scores = []","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:52:24.672898Z","iopub.execute_input":"2022-02-24T21:52:24.673175Z","iopub.status.idle":"2022-02-24T21:52:24.678385Z","shell.execute_reply.started":"2022-02-24T21:52:24.673145Z","shell.execute_reply":"2022-02-24T21:52:24.677639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_brief_desc.append(\"logistic\")\n# train score\ny_train_pred = logreg.predict_proba(X_train)\ntrain_scores.append(roc_auc_score(y_train, y_train_pred[:,1]))\n# test score\ny_pred = logreg.predict_proba(X_test)\ntest_scores.append(roc_auc_score(y_test, y_pred[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:52:26.404016Z","iopub.execute_input":"2022-02-24T21:52:26.404553Z","iopub.status.idle":"2022-02-24T21:52:27.88771Z","shell.execute_reply.started":"2022-02-24T21:52:26.404518Z","shell.execute_reply":"2022-02-24T21:52:27.886951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0, len(train_scores)):\n    print(model_brief_desc[i], \" : \", train_scores[i], \", \", test_scores[i])","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:52:29.67193Z","iopub.execute_input":"2022-02-24T21:52:29.672501Z","iopub.status.idle":"2022-02-24T21:52:29.679275Z","shell.execute_reply.started":"2022-02-24T21:52:29.672458Z","shell.execute_reply":"2022-02-24T21:52:29.678561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump(logreg, 'logistic.sav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(logreg, X_test, y_test, name = 'Logistic Regression')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:53:55.623337Z","iopub.execute_input":"2022-02-24T21:53:55.623923Z","iopub.status.idle":"2022-02-24T21:53:56.383612Z","shell.execute_reply.started":"2022-02-24T21:53:55.623884Z","shell.execute_reply":"2022-02-24T21:53:56.382918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- Try out a method for seeing if more training samples would improve score\n- Doing this just for practice since outside of shifting the train_test_split, there isn't a way to get more training data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n# the following is a wrapper to learning curve\n\ndef plot_learning_curves_classification(model, features, labels, model_str_name):\n    train_sizes = np.round(np.linspace(1000, 0.7 * features.shape[0], 10)).astype(int)\n    \n    train_sizes, train_scores, validation_scores = learning_curve(\n        estimator = model,\n        X = features,\n        y = labels, train_sizes=train_sizes, cv=5,\n        scoring='roc_auc')\n    train_scores_mean = train_scores.mean(axis=1)\n    validation_scores_mean = validation_scores.mean(axis=1)\n\n    plt.style.use('seaborn')\n    plt.plot(train_sizes, 1 - train_scores_mean, label='Training error')\n    plt.plot(train_sizes, 1 - validation_scores_mean, label='Validation error')\n    plt.xlabel(\"Training Set Size\")\n    plt.ylabel(\"Error (1 - ROC AUC)\")\n    plt.title(\"Learning Curve for \" + model_str_name)\n    plt.legend()\n    plt.savefig(\"learning_curve.png\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:54:00.868908Z","iopub.execute_input":"2022-02-24T21:54:00.869199Z","iopub.status.idle":"2022-02-24T21:54:00.877208Z","shell.execute_reply.started":"2022-02-24T21:54:00.869168Z","shell.execute_reply":"2022-02-24T21:54:00.876088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = LogisticRegression(solver='liblinear', random_state = 42)\nplot_learning_curves_classification(m, X_train, y_train, 'logistic')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:54:03.022479Z","iopub.execute_input":"2022-02-24T21:54:03.023063Z","iopub.status.idle":"2022-02-24T22:20:31.35451Z","shell.execute_reply.started":"2022-02-24T21:54:03.023026Z","shell.execute_reply":"2022-02-24T22:20:31.352902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- Above plot shows that the training and validation scores (or errors) converge towards 0.7 of our total train set\n- Since our train set is even higher than that, this suggests more training data will not improve the score significantly","metadata":{}},{"cell_type":"markdown","source":"# Trying out Dimensionality Reduction, Feature Selection Techniques\n- Basic logistic regression (with L2 regularization) model produced similar train and validation scores\n- This suggests that the model is generalizing well (i.e. it's not overfitting)\n- Although from EDA we saw that the columns are not very correlated, want to try some dimensionality reduction and feature selection techniques\n- We expect that these might lead to lower performance","metadata":{}},{"cell_type":"code","source":"# we first try PCA\n# first do a quick plot of new features explained variance and decide how many I want to keep\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=None)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:23:07.656553Z","iopub.execute_input":"2022-02-24T22:23:07.65684Z","iopub.status.idle":"2022-02-24T22:23:07.670391Z","shell.execute_reply.started":"2022-02-24T22:23:07.656811Z","shell.execute_reply":"2022-02-24T22:23:07.669669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca.fit(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:23:09.315496Z","iopub.execute_input":"2022-02-24T22:23:09.316277Z","iopub.status.idle":"2022-02-24T22:23:44.415032Z","shell.execute_reply.started":"2022-02-24T22:23:09.316237Z","shell.execute_reply":"2022-02-24T22:23:44.414304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.cumsum(pca.explained_variance_ratio_ * 100))\nplt.xlabel(\"Number of components (Dimensions)\")\nplt.ylabel(\"Explained variance (%)\")","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:23:44.416823Z","iopub.execute_input":"2022-02-24T22:23:44.417091Z","iopub.status.idle":"2022-02-24T22:23:44.619005Z","shell.execute_reply.started":"2022-02-24T22:23:44.417056Z","shell.execute_reply":"2022-02-24T22:23:44.618327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- choosing 100, which is after the inflection point and explains close to ~90% of the variance","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=100)\npca.fit(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_pca=pca.fit_transform(X_train)\nX_train_pca=pd.DataFrame(X_train_pca)\nprint(X_train_pca)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:23:52.340684Z","iopub.execute_input":"2022-02-24T22:23:52.340949Z","iopub.status.idle":"2022-02-24T22:24:30.547456Z","shell.execute_reply.started":"2022-02-24T22:23:52.340916Z","shell.execute_reply":"2022-02-24T22:24:30.546665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump(pca, 'pca.sav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg2 = LogisticRegression(solver='liblinear')\nlogreg2.fit(X_train_pca, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_pca=pca.fit_transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:24:51.649912Z","iopub.execute_input":"2022-02-24T22:24:51.650176Z","iopub.status.idle":"2022-02-24T22:25:09.892665Z","shell.execute_reply.started":"2022-02-24T22:24:51.650149Z","shell.execute_reply":"2022-02-24T22:25:09.891842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_brief_desc.append(\"logistic(w/PCA)\")\n# train score\ny_train_pred = logreg2.predict_proba(X_train_pca)\ntrain_scores.append(roc_auc_score(y_train, y_train_pred[:,1]))\n# test score\ny_pred = logreg2.predict_proba(X_test_pca)\ntest_scores.append(roc_auc_score(y_test, y_pred[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:25:22.600861Z","iopub.execute_input":"2022-02-24T22:25:22.601146Z","iopub.status.idle":"2022-02-24T22:25:23.286776Z","shell.execute_reply.started":"2022-02-24T22:25:22.601118Z","shell.execute_reply":"2022-02-24T22:25:23.285923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's see the scores again\nfor i in range(0, len(train_scores)):\n    print(model_brief_desc[i], \" : \", train_scores[i], \", \", test_scores[i])","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:25:26.410665Z","iopub.execute_input":"2022-02-24T22:25:26.411121Z","iopub.status.idle":"2022-02-24T22:25:26.417881Z","shell.execute_reply.started":"2022-02-24T22:25:26.411085Z","shell.execute_reply":"2022-02-24T22:25:26.417176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump(logreg2, 'logistic_pca.sav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(logreg2, X_test_pca, y_test, name = 'Logistic Regression PCA')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:26:33.869208Z","iopub.execute_input":"2022-02-24T22:26:33.869513Z","iopub.status.idle":"2022-02-24T22:26:34.475287Z","shell.execute_reply.started":"2022-02-24T22:26:33.869481Z","shell.execute_reply":"2022-02-24T22:26:34.474578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- PCA before linear regression has lowered the AUC score, suggesting that some of the features dropped that, while highly correlated with others, are seperately highly correlated with the target and so now the model is performing worse\n- try a different method for feature selection next : recursive feature elimination","metadata":{}},{"cell_type":"code","source":"# previously ran models taking up a lot of memory, so we drop them to try and free memory\ndel m, X_test_pca, X_train_pca, y_train_pred, y_pred, logreg, logreg2\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit simple logistic regression and recursively take away features\nfrom sklearn.feature_selection import RFE\nrfe_selector = RFE(estimator=LogisticRegression(solver='liblinear'),n_features_to_select = 50, step = 2)\nrfe_selector.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump(rfe_selector, 'rfe.sav')\n# brief look at which columns were selected\nX_train.columns[rfe_selector.get_support()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_rfe = rfe_selector.transform(X_train)\nlogreg3 = LogisticRegression(solver='liblinear')\nlogreg3.fit(X_train_rfe, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_brief_desc.append(\"logistic(w/RFE)\")\n# train score\ny_train_pred = logreg3.predict_proba(X_train_rfe)\ntrain_scores.append(roc_auc_score(y_train, y_train_pred[:,1]))\n\n# test score\nX_test_rfe = rfe_selector.transform(X_test)\ny_pred = logreg3.predict_proba(X_test_rfe)\ntest_scores.append(roc_auc_score(y_test, y_pred[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:27:49.143155Z","iopub.execute_input":"2022-02-24T22:27:49.143982Z","iopub.status.idle":"2022-02-24T22:27:49.96478Z","shell.execute_reply.started":"2022-02-24T22:27:49.14394Z","shell.execute_reply":"2022-02-24T22:27:49.963994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's see the scores again\nfor i in range(0, len(train_scores)):\n    print(model_brief_desc[i], \" : \", train_scores[i], \", \", test_scores[i])","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:27:52.266392Z","iopub.execute_input":"2022-02-24T22:27:52.267119Z","iopub.status.idle":"2022-02-24T22:27:52.273666Z","shell.execute_reply.started":"2022-02-24T22:27:52.267085Z","shell.execute_reply":"2022-02-24T22:27:52.272862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump(logreg3, 'logistic_rfe.sav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(logreg3, X_test_rfe, y_test, name = 'RFE Logistic')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:27:59.625661Z","iopub.execute_input":"2022-02-24T22:27:59.626433Z","iopub.status.idle":"2022-02-24T22:28:00.009264Z","shell.execute_reply.started":"2022-02-24T22:27:59.626388Z","shell.execute_reply":"2022-02-24T22:28:00.008555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- RFE took much longer, performed much better on the test set, but does not beat the baseline Logistic Regression w/ regularization\n- As suspected might happen, dimensionality reduction and feature selection actually worsened performance by removing information (features) that were related to the target\n","metadata":{}},{"cell_type":"markdown","source":"# Modeling - DTEs\n- Now try classification using common decision tree methods","metadata":{}},{"cell_type":"code","source":"# gc memory or Kaggle will reset\ndel rfe_selector, submit, y_submit, X_submit, X_submit_rfe, test_df, X_test_rfe, y_pred_rfe, logreg3\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run this cell in case kaggle restarts the notebook for memory or timeout reasons\n# this simply repeats the short preprocessing required and imports all general libraries\nimport numpy as np\nimport pandas as pd \ndf = pd.read_csv('../input/tabular-playground-series-oct-2021/train.csv')\n\nx = df.iloc[:,1:286]\ny = df.iloc[:,286]\n\nimport gc\ndel df\ngc.collect()\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nfrom joblib import dump, load\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n\ndel x, y\ngc.collect()\n\n# we know the scores based on previous runs on the same seed\nmodel_brief_desc = ['logistic','logistic(w/PCA)','logistic(w/RFE)']\ntrain_scores = [0.8402472449462012, 0.8066444198959453, 0.8328801836407288]\ntest_scores = [0.8394841897582251, 0.7612871702176366, 0.8324074476989]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will try a simple catboost first\nimport catboost as catb","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:29:00.39732Z","iopub.execute_input":"2022-02-24T22:29:00.398154Z","iopub.status.idle":"2022-02-24T22:29:00.574761Z","shell.execute_reply.started":"2022-02-24T22:29:00.398107Z","shell.execute_reply":"2022-02-24T22:29:00.574015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catboost = catb.CatBoostClassifier(loss_function='Logloss',verbose=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catboost.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_brief_desc.append(\"catboost\")\n# CatBoost has an inbuilt function for calculating AUC, but we can keep this for consistency/convenience\n# train score\ny_train_pred = catboost.predict_proba(X_train)\ntrain_scores.append(roc_auc_score(y_train, y_train_pred[:,1]))\n\n# test score\ny_pred = catboost.predict_proba(X_test)\ntest_scores.append(roc_auc_score(y_test, y_pred[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:29:28.741959Z","iopub.execute_input":"2022-02-24T22:29:28.742734Z","iopub.status.idle":"2022-02-24T22:29:31.349908Z","shell.execute_reply.started":"2022-02-24T22:29:28.742684Z","shell.execute_reply":"2022-02-24T22:29:31.349146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's see the scores again\nfor i in range(0, len(train_scores)):\n    print(model_brief_desc[i], \" : \", train_scores[i], \", \", test_scores[i])","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:29:32.136337Z","iopub.execute_input":"2022-02-24T22:29:32.137182Z","iopub.status.idle":"2022-02-24T22:29:32.145696Z","shell.execute_reply.started":"2022-02-24T22:29:32.137141Z","shell.execute_reply":"2022-02-24T22:29:32.144879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump(catboost, 'catboost.sav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(catboost, X_test, y_test, name = 'catboost')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:13:15.143139Z","iopub.execute_input":"2022-02-25T00:13:15.143749Z","iopub.status.idle":"2022-02-25T00:13:16.071038Z","shell.execute_reply.started":"2022-02-25T00:13:15.143709Z","shell.execute_reply":"2022-02-25T00:13:16.07034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- CatBoost with mostly default parameters performs very well compared to any of the logistic regression models\n- However, the validation score is worse than the train score, suggesting there might be a small amount of overfitting\n- Next, will try different hyperparameter tuning methods\n- In real setting, would want to try different methods on the same classifier but to cover as much as possible, will just do random search on catboost and leave bayesian search for xgboost later.\n- Leaving out grid search as it is basically brute force and takes a long time, susceptible to Kaggle memory and timeout restarts","metadata":{}},{"cell_type":"code","source":"catboost_rs = catb.CatBoostClassifier(loss_function='Logloss',verbose=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"space = {'iterations' : [10, 100, 1000], # default is 1000\n         'learning_rate': [0.03, 0.1, 0.3], # default generated 0.16\n         'depth': [4, 6, 10], # 6 is default\n         'l2_leaf_reg': [1, 3, 5, 7, 9]} # default is 3, with higher penalities we might reduce overfitting","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# catboost has inbuilt randomized search function, that defaults to using a 3-fold cross-validation\nsearch_results = catboost_rs.randomized_search(space, X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_brief_desc.append(\"catboost(w/RandomSearch)\")\n# according to catboost doc, model is already trained/fitted after running the search\n\n# train score\ny_train_pred = catboost_rs.predict_proba(X_train)\ntrain_scores.append(roc_auc_score(y_train, y_train_pred[:,1]))\n\n# test score\ny_pred = catboost_rs.predict_proba(X_test)\ntest_scores.append(roc_auc_score(y_test, y_pred[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:30:43.462708Z","iopub.execute_input":"2022-02-24T22:30:43.463271Z","iopub.status.idle":"2022-02-24T22:30:45.654042Z","shell.execute_reply.started":"2022-02-24T22:30:43.463234Z","shell.execute_reply":"2022-02-24T22:30:45.653224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's see the scores again\nfor i in range(0, len(train_scores)):\n    print(model_brief_desc[i], \" : \", train_scores[i], \", \", test_scores[i])","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:30:47.160207Z","iopub.execute_input":"2022-02-24T22:30:47.160991Z","iopub.status.idle":"2022-02-24T22:30:47.169276Z","shell.execute_reply.started":"2022-02-24T22:30:47.16094Z","shell.execute_reply":"2022-02-24T22:30:47.16848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump(catboost_rs, 'catboost_rs.sav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(catboost_rs, X_test, y_test, name = 'catboost(w/RandomSearch)')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:13:21.194174Z","iopub.execute_input":"2022-02-25T00:13:21.194674Z","iopub.status.idle":"2022-02-25T00:13:22.124957Z","shell.execute_reply.started":"2022-02-25T00:13:21.194634Z","shell.execute_reply":"2022-02-25T00:13:22.124253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- In-built random search has lowered the train score but improved the test (validation) score slightly\n- Now we will try a basic xgboost, as well as xgboost with bayesian search and see how they perform","metadata":{}},{"cell_type":"code","source":"# next we try xgboost, again gc for memory\ndel catboost, catboost_rs, search_results, space, y_pred_proba, X_submit, y_submit, submit, test_df\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run this cell in case kaggle restarts the notebook for memory or timeout reasons\n# this simply repeats the short preprocessing required and imports all general libraries\nimport numpy as np\nimport pandas as pd \ndf = pd.read_csv('../input/tabular-playground-series-oct-2021/train.csv')\n\nx = df.iloc[:,1:286]\ny = df.iloc[:,286]\n\nimport gc\ndel df\ngc.collect()\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nfrom joblib import dump, load\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n\ndel x, y\ngc.collect()\n\n# we know the scores based on previous runs on the same seed\nmodel_brief_desc = ['logistic','logistic(w/PCA)','logistic(w/RFE)', 'catboost', 'catboost(w/RandomSearch)']\ntrain_scores = [0.8402472449462012, 0.8066444198959453, 0.8328801836407288, 0.8819991775525523, 0.8719932449630302]\ntest_scores = [0.8394841897582251, 0.7612871702176366, 0.8324074476989, 0.8545129516316827, 0.8557815686724553]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start with basic xgboost classifier\n# this is the sklearn wrapper version so we can pass parameters as arguments\nimport xgboost as xgb\nxgboost = xgb.XGBClassifier(objective = 'binary:logistic')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:18:13.165873Z","iopub.execute_input":"2022-02-25T00:18:13.166408Z","iopub.status.idle":"2022-02-25T00:18:13.257234Z","shell.execute_reply.started":"2022-02-25T00:18:13.166368Z","shell.execute_reply":"2022-02-25T00:18:13.256165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgboost.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_brief_desc.append(\"xgboost\")\n# train score\ny_train_pred = xgboost.predict_proba(X_train)\ntrain_scores.append(roc_auc_score(y_train, y_train_pred[:,1]))\n\n# test score\ny_pred = xgboost.predict_proba(X_test)\ntest_scores.append(roc_auc_score(y_test, y_pred[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:31:36.681809Z","iopub.execute_input":"2022-02-24T22:31:36.682343Z","iopub.status.idle":"2022-02-24T22:31:43.155027Z","shell.execute_reply.started":"2022-02-24T22:31:36.682305Z","shell.execute_reply":"2022-02-24T22:31:43.154186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's see the scores again\nfor i in range(0, len(train_scores)):\n    print(model_brief_desc[i], \" : \", train_scores[i], \", \", test_scores[i])","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:31:43.156796Z","iopub.execute_input":"2022-02-24T22:31:43.157072Z","iopub.status.idle":"2022-02-24T22:31:43.166867Z","shell.execute_reply.started":"2022-02-24T22:31:43.157038Z","shell.execute_reply":"2022-02-24T22:31:43.166138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump(xgboost, 'xgb.sav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(xgboost, X_test, y_test, name = 'xgboost')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:13:39.13822Z","iopub.execute_input":"2022-02-25T00:13:39.138757Z","iopub.status.idle":"2022-02-25T00:13:41.119015Z","shell.execute_reply.started":"2022-02-25T00:13:39.138718Z","shell.execute_reply":"2022-02-25T00:13:41.118312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- Basic XGBoost score was quite good considering the best scores from previous models\n- Now let's try hyperparameter tuning using bayesian search","metadata":{}},{"cell_type":"code","source":"# this library is what I could find online for running a bayesian search\nfrom hyperopt import hp\nfrom hyperopt import fmin\nfrom hyperopt import tpe","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:18:13.258679Z","iopub.execute_input":"2022-02-25T00:18:13.259182Z","iopub.status.idle":"2022-02-25T00:18:13.686859Z","shell.execute_reply.started":"2022-02-25T00:18:13.259143Z","shell.execute_reply":"2022-02-25T00:18:13.686047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del xgboost\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:18:13.687982Z","iopub.execute_input":"2022-02-25T00:18:13.68823Z","iopub.status.idle":"2022-02-25T00:18:13.791825Z","shell.execute_reply.started":"2022-02-25T00:18:13.688199Z","shell.execute_reply":"2022-02-25T00:18:13.790935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instead of interacting with the sickit-learn wrapper, we will use the xgboost python api directly, \n# in order to send it a set of \"params\" found by the hyperopt fmin function later\n# first, create a DMatrix which xgb accepts\ntrain_dm = xgb.DMatrix(X_train, label = y_train)\n\n# define hyperparameter space using dictionary\n# for bayesian search we give a distribution\nspace = {\n    'objective' : 'binary:logistic', # this is a required parameter even though we dont search over it\n    'max_depth' : hp.choice('max_depth', np.arange(3, 14, dtype=int)), # the default is 3\n    'min_child_weight' : hp.uniform('min_child_weight', 5, 8), # the default is 1\n    'subsample' : hp.uniform('subsample', 0.8, 1.0), # the default is 1\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0), # the default is 1\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)), # the default is 0.1\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:18:13.794424Z","iopub.execute_input":"2022-02-25T00:18:13.794998Z","iopub.status.idle":"2022-02-25T00:18:17.152602Z","shell.execute_reply.started":"2022-02-25T00:18:13.794963Z","shell.execute_reply":"2022-02-25T00:18:17.151952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(params):\n    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n    \n    # Perform n_fold cross validation with hyperparameters\n    # Use early stopping and evalute based on ROC AUC\n    cv_results = xgb.cv(params = params, dtrain = train_dm, nfold = 3, num_boost_round = 10, \n                        early_stopping_rounds = 2, metrics = 'auc', seed = 42)\n  \n    # Extract the best score\n    best_score = max(cv_results['train-auc-mean'])\n    \n    # Loss must be minimized\n    loss = 1 - best_score\n    \n    # Dictionary with information for evaluation\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:19:01.569264Z","iopub.execute_input":"2022-02-25T00:19:01.570007Z","iopub.status.idle":"2022-02-25T00:19:01.578043Z","shell.execute_reply.started":"2022-02-25T00:19:01.569968Z","shell.execute_reply":"2022-02-25T00:19:01.577211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best = fmin(objective, space, algo=tpe.suggest, max_evals=20)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:19:04.004352Z","iopub.execute_input":"2022-02-25T00:19:04.004949Z","iopub.status.idle":"2022-02-25T04:08:21.830434Z","shell.execute_reply.started":"2022-02-25T00:19:04.004913Z","shell.execute_reply":"2022-02-25T04:08:21.828294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get classifier fitted on the train set using best hyperparameters\n# there might be a more efficient way of giving the params, but I'll stick to what's intuitive with sklearn\nxgboost_bs = xgb.XGBClassifier(objective = 'binary:logistic',\n                               colsample_bytree = best['colsample_bytree'],\n                               learning_rate = best['learning_rate'],\n                               max_depth = best['max_depth'],\n                               min_child_weight = best['min_child_weight'],\n                               subsample = best['subsample'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump(xgboost_bs, 'stg_xgb_bs.sav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgboost_bs.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_brief_desc.append(\"xgboost(w/BayesianSearch)\")\n# train score\ny_train_pred = xgboost_bs.predict_proba(X_train)\ntrain_scores.append(roc_auc_score(y_train, y_train_pred[:,1]))\n\n# test score\ny_pred = xgboost_bs.predict_proba(X_test)\ntest_scores.append(roc_auc_score(y_test, y_pred[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:33:07.785202Z","iopub.execute_input":"2022-02-24T22:33:07.785494Z","iopub.status.idle":"2022-02-24T22:33:16.096559Z","shell.execute_reply.started":"2022-02-24T22:33:07.78546Z","shell.execute_reply":"2022-02-24T22:33:16.095758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's see the scores again\nfor i in range(0, len(train_scores)):\n    print(model_brief_desc[i], \" : \", train_scores[i], \", \", test_scores[i])","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:33:16.098286Z","iopub.execute_input":"2022-02-24T22:33:16.098578Z","iopub.status.idle":"2022-02-24T22:33:16.108097Z","shell.execute_reply.started":"2022-02-24T22:33:16.098543Z","shell.execute_reply":"2022-02-24T22:33:16.107375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump(xgboost_bs, 'xgb_bayesian.sav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(xgboost_bs, X_test, y_test, name = 'xgboost(w/BayesianSearch)')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:14:01.094708Z","iopub.execute_input":"2022-02-25T00:14:01.094985Z","iopub.status.idle":"2022-02-25T00:14:03.94553Z","shell.execute_reply.started":"2022-02-25T00:14:01.094957Z","shell.execute_reply":"2022-02-25T00:14:03.944829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- it looks like hp tuning has increased the AUC train score, but it took much longer, and did not improve the test score\n- this suggests the model is overfitting relative to the default xgboost model","metadata":{}},{"cell_type":"markdown","source":"# Other Considerations\n- In a real-world setting, the tasks may not be as straightforward, and the features may require more cleaning (e.g. need to impute missing values or remove outliers), or are more interpretable, allowing for more feature engineering\n- Another important step may be determining a good peformance metric to evaluate models by\n- In this section, briefly go through exercise of looking at ROC curve and confusion matrix","metadata":{}},{"cell_type":"code","source":"# first, let's reload libraries and models, if kaggle has restarted multiple times at this point\nimport numpy as np\nimport pandas as pd \ndf = pd.read_csv('../input/tabular-playground-series-oct-2021/train.csv')\n\nx = df.iloc[:,1:286]\ny = df.iloc[:,286]\n\nimport gc\ndel df\ngc.collect()\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nfrom joblib import dump, load\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n\ndel x, y\ngc.collect()\n\nimport catboost as catb\nimport xgboost as xgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# change directory depending on if this is a reupload or from output\nlogreg = load('../input/playground-models/logistic.sav')\nlogreg2 = load('../input/playground-models/logistic_pca.sav')\nlogreg3 = load('../input/playground-models/logistic_rfe.sav')\ncatboost = load('../input/playground-models/catboost.sav')\ncatboost_rs = load('../input/playground-models/catboost_rs.sav')\nxgboost = load('../input/playground-models/xgb.sav')\nxgboost_bs = load('../input/playground-models/xgb_bayesian.sav')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparing ROC curves and confusion matrices\n- The ROC curve plots true and false positive rate of a model based on different probability thresholds\n- In a real-world setting, we may care more about one or the other (or care about other things such as the F-1 score)\n- For example, this dataset may be a predictor for a serious health condition (positive meaning having the condition) where early detection is crucial.\n- In such a case, we would care much more about the (and would want a higher) true positive rate because we want to make sure to detect the condition if the patient has it. We may even want to measure performance based on the true positive rate entirely\n- A false positive is less harmful because they are likely to do further testing/go through more consultations and realize later it was a false alarm, but it still bears cost because the patient may be needlessly taking up medical resources or their own money from future tests","metadata":{}},{"cell_type":"code","source":"# plot ROC curve of best performing models on the same axes to compare test score\n\ny_pred1 = catboost_rs.predict_proba(X_test)\ny_pred2 = xgboost.predict_proba(X_test)\n\nplt.figure(0).clf()\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nfpr, tpr, thresh = roc_curve(y_test, y_pred1[:,1])\nplt.plot(fpr,tpr,label = 'catboost(w/RandomSearch)')\nfpr, tpr, thresh = roc_curve(y_test, y_pred2[:,1])\nplt.plot(fpr,tpr,label = 'xgboost')\n\n\nplt.legend(loc=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T23:48:41.672816Z","iopub.execute_input":"2022-02-24T23:48:41.673316Z","iopub.status.idle":"2022-02-24T23:48:44.785006Z","shell.execute_reply.started":"2022-02-24T23:48:41.67328Z","shell.execute_reply":"2022-02-24T23:48:44.784329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- The models' ROC curves are not very distinguishable and seem to perform similarly at all probability thresholds (Catboost maybe edging out slightly)\n- What may be important in real-world is adjusting the probability threshold for predicting a positive case depending on the cost/benefit of false positives/negatives and true positives/negatives respectively\n- In the case outlined above, we might want to lower the probability threshold (toward the right of the ROC curve) so we make sure to catch all cases of the serious health condition\n- Now quickly compare confusion matrices at 50% probability threshold (defualt)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ny_pred1 = catboost_rs.predict(X_test)\ny_pred2 = xgboost.predict(X_test)\n\nprint('ref: tn, fp, fn, tp')\nprint('catboost(w/RandomSearch)\\n', confusion_matrix(y_test, y_pred1).ravel())\nprint('xgboost\\n', confusion_matrix(y_test, y_pred2).ravel())","metadata":{"execution":{"iopub.status.busy":"2022-02-24T23:23:17.615008Z","iopub.execute_input":"2022-02-24T23:23:17.615557Z","iopub.status.idle":"2022-02-24T23:23:20.968905Z","shell.execute_reply.started":"2022-02-24T23:23:17.615519Z","shell.execute_reply":"2022-02-24T23:23:20.968177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n- The results are again very similar\n- Given the example outlined earlier where the cost of a false negative was higher than the cost of a false positive, we would choose catboost(w/RandomSearch) if we didn't want to adjust the probability threshold","metadata":{}},{"cell_type":"code","source":"# run these only for competition\n# test data and predict\ntest_df=pd.read_csv('../input/tabular-playground-series-oct-2021/test.csv')\nX_submit = test_df.iloc[:,1:]\n# classifier can be changed to submit different predictions\ny_submit = catboost_rs.predict_proba(X_submit)\nsubmit = np.c_[test_df.iloc[:,0],y_submit[:,1]]\nsubmit = pd.DataFrame(submit, columns = ['id','target'])\nsubmit.to_csv('catb_rs.csv')","metadata":{},"execution_count":null,"outputs":[]}]}