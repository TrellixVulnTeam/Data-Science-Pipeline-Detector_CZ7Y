{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://lightgbm.readthedocs.io/en/latest/_images/LightGBM_logo_black_text.svg)","metadata":{}},{"cell_type":"markdown","source":"# Summary\n\nWith only 5 days left in the competition, I am sharing my best models so that you can incorporate these into your stacks or work on further improving them as standalone models. Considering the public subset is only 20% of the entire test set, I find it essential to train models that are likely to do well on unseen data. Thus, I set conservative hyperparameters - more on this in the Model training section - and trained each of the 15 models on 20 different seeds and averaged them. Typically, these models scored 0.85647 on the public test set. Adding the last 5 to my existing stack still improved the result, but not as much as the first 10.","metadata":{}},{"cell_type":"markdown","source":"# Dataset\n\nYou can find the 15 out-of-fold and test predictions [here](https://www.kaggle.com/adamwurdits/tps-10-2021-lightgbm-predictions). Individual model performances are listed in the dataset description.","metadata":{}},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.metrics import roc_auc_score\nimport optuna\n# from optuna.integration import LightGBMPruningCallback\nfrom lightgbm import LGBMClassifier\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/tabular-playground-series-oct-2021/train.csv')\ndf_test = pd.read_csv('../input/tabular-playground-series-oct-2021/test.csv')\nsample_submission = pd.read_csv('../input/tabular-playground-series-oct-2021/sample_submission.csv')","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n\nI tried adding various features and experimented with different scalers in earlier versions (15-19). Adding the sum of the continuous and binary features and using a standard scaler gave me the best results. I didnâ€™t try transforming the numbers in any way.","metadata":{}},{"cell_type":"code","source":"features = [c for c in df_test.columns if 'f' in c]\nbin_features = [c for c in df_test[features] if df_test[c].dtype=='int64']\ncont_features = [c for c in df_test[features] if df_test[c].dtype=='float64']\n\ndf_train['bin_count'] = df_train[bin_features].sum(axis=1)\ndf_test['bin_count'] = df_test[bin_features].sum(axis=1)\nfeatures.append('bin_count')\n\ndf_train['cont_sum'] = df_train[cont_features].sum(axis=1)\ndf_test['cont_sum'] = df_test[cont_features].sum(axis=1)\nfeatures.append('cont_sum')\n\nscaler = preprocessing.StandardScaler()\ndf_train[features] = scaler.fit_transform(df_train[features])\ndf_test[features] = scaler.transform(df_test[features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating folds\n\nStratified KFold cross-validation using 5 splits. I decided to go with 5 folds as this will keep training times short and allow me to train more models on more seeds.","metadata":{}},{"cell_type":"code","source":"df_train['kfold'] = -1\n\ny_train = df_train.target\nX_train = df_train.drop('target', axis=1)\n\nskf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor fold, (i_train, i_valid) in enumerate (skf.split(X_train, y_train)):\n    df_train.loc[i_valid, 'kfold'] = fold\n    \ndel X_train, y_train\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter optimization with Optuna\n\nFor the first 10 days of the competition I focused more on finding the ideal k in KFold, making the most of feature engineering and scaling, and getting a ballpark idea of the hyperparameters I am going to be using. Later on, I ran Optuna studies in batches of 12 trials. For the most part these notebooks could finish running in less than 6 hours which was ideal for overnight work.","metadata":{}},{"cell_type":"code","source":"# seed = 0\n\n# def objective(trial):\n#     fold = 0\n#     params = {\n#         'num_leaves': trial.suggest_int('num_leaves', 16, 16),\n#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 2000, 6700),\n#         'max_depth': trial.suggest_int('max_depth', 0, 0),\n#         'max_bin': trial.suggest_int('max_bin', 200, 400),\n#         'learning_rate': trial.suggest_float('learning_rate', 0.0071, 0.0076),\n#         'lambda_l1': trial.suggest_loguniform('lambda_l1', 0.00001, 8),\n#         'lambda_l2': trial.suggest_loguniform('lambda_l2', 0.00001, 100),\n#         'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0, 4),\n#         'feature_fraction': trial.suggest_float('feature_fraction', 0.22, 0.35),\n#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.49, 0.52),\n#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 1)        \n#     }\n\n#     X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n#     X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n        \n#     y_train = X_train.target\n#     y_valid = X_valid.target\n    \n#     X_train = X_train[features]\n#     X_valid = X_valid[features]\n    \n#     model = LGBMClassifier(\n#             objective='binary',\n#             tree_learner='serial',\n#             seed=seed,\n#             n_estimators=20000,\n#             **params)\n    \n#     model.fit(X_train,\n#               y_train,\n#               early_stopping_rounds=500,\n#               eval_set=[(X_valid, y_valid)],\n#               eval_metric='auc',\n# #               callbacks=[LightGBMPruningCallback(trial, 'auc')],\n#               verbose=1000)\n    \n#     valid_pred = model.predict_proba(X_valid)[:,1]\n        \n#     auc = roc_auc_score(y_valid, valid_pred)\n    \n#     del X_train, X_valid, y_train, y_valid\n#     gc.collect()\n    \n#     return auc\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model training\n\nI wanted to use conservative models so I set max_depth to 0, num_leaves to half the default amount and used low learning rates. The models were trained on roughly half of all the samples and used only a quarter of all the features. Binning and regularization I used were very different from model to model.\n\nI set up 10 notebooks to run the same models on seeds 0 to 9 and 10 to 19 as this helped to keep everything organized.","metadata":{}},{"cell_type":"code","source":"%%time\n\nm = 15\ns = 0\n\nvalid_preds = {}\ntest_preds = []\nscores = []\n\nfor fold in range(5):\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n\n    X_test = df_test[features].copy()\n\n    valid_ids = X_valid.id.values.tolist()\n\n    y_train = X_train.target\n    y_valid = X_valid.target\n\n    X_train = X_train[features]\n    X_valid = X_valid[features]\n\n    params = {'num_leaves': 16,\n              'min_data_in_leaf': 4980,\n              'max_depth': 0,\n              'max_bin': 399,\n              'learning_rate': 0.007443215095336714,\n              'lambda_l1': 1.6181770821331433e-05,\n              'lambda_l2': 1.696656500639349e-05,\n              'min_gain_to_split': 1.335756684660189,\n              'feature_fraction': 0.2550512769849608,\n              'bagging_fraction': 0.5143697950295731,\n              'bagging_freq': 1}\n\n    model = LGBMClassifier(\n        objective='binary',\n        importance_type='split',\n        boosting_type='gbdt',\n        tree_learner='serial',\n        num_threads=-1,\n        random_state=s,\n        n_estimators=20000,\n        **params)\n\n    model.fit(X_train,\n              y_train,\n              early_stopping_rounds=500,\n              eval_set=[(X_valid, y_valid)],\n              eval_metric='auc',\n              verbose=1000)\n\n    valid_pred = model.predict_proba(X_valid)[:,1]\n    test_pred = model.predict_proba(X_test)[:,1]\n\n    valid_preds.update(dict(zip(valid_ids, valid_pred)))\n    test_preds.append(test_pred)\n\n    score = roc_auc_score(y_valid, valid_pred)    \n    scores.append(score)\n\nprint(f'Mean auc {np.mean(scores)}, std {np.std(scores)}')\n\nvalid_preds = pd.DataFrame.from_dict(valid_preds, orient='index').reset_index()\nvalid_preds.columns = ['id', f'm{m}s{s}_pred']\nvalid_preds.to_csv(f'm{m}s{s}_valid_pred.csv', index=False)\n\nsample_submission.target = np.mean(np.column_stack(test_preds), axis=1)\nsample_submission.columns = ['id', f'm{m}s{s}_pred']\nsample_submission.to_csv(f'm{m}s{s}_test_pred.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the same model as the one above, except it is trained on another seed. In this notebook I trained on seeds 0 and 10, in others I trained on seeds 1 and 11, 2 and 12 and so on.","metadata":{}},{"cell_type":"code","source":"%%time\n\ns = 10\n\nvalid_preds = {}\ntest_preds = []\nscores = []\n\nfor fold in range(5):\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n\n    X_test = df_test[features].copy()\n\n    valid_ids = X_valid.id.values.tolist()\n\n    y_train = X_train.target\n    y_valid = X_valid.target\n\n    X_train = X_train[features]\n    X_valid = X_valid[features]\n\n    params = {'num_leaves': 16,\n              'min_data_in_leaf': 4980,\n              'max_depth': 0,\n              'max_bin': 399,\n              'learning_rate': 0.007443215095336714,\n              'lambda_l1': 1.6181770821331433e-05,\n              'lambda_l2': 1.696656500639349e-05,\n              'min_gain_to_split': 1.335756684660189,\n              'feature_fraction': 0.2550512769849608,\n              'bagging_fraction': 0.5143697950295731,\n              'bagging_freq': 1}\n\n    model = LGBMClassifier(\n        objective='binary',\n        importance_type='split',\n        boosting_type='gbdt',\n        tree_learner='serial',\n        num_threads=-1,\n        random_state=s,\n        n_estimators=20000,\n        **params)\n\n    model.fit(X_train,\n              y_train,\n              early_stopping_rounds=500,\n              eval_set=[(X_valid, y_valid)],\n              eval_metric='auc',\n              verbose=1000)\n\n    valid_pred = model.predict_proba(X_valid)[:,1]\n    test_pred = model.predict_proba(X_test)[:,1]\n\n    valid_preds.update(dict(zip(valid_ids, valid_pred)))\n    test_preds.append(test_pred)\n\n    score = roc_auc_score(y_valid, valid_pred)    \n    scores.append(score)\n\nprint(f'Mean auc {np.mean(scores)}, std {np.std(scores)}')\n\nvalid_preds = pd.DataFrame.from_dict(valid_preds, orient='index').reset_index()\nvalid_preds.columns = ['id', f'm{m}s{s}_pred']\nvalid_preds.to_csv(f'm{m}s{s}_valid_pred.csv', index=False)\n\nsample_submission.target = np.mean(np.column_stack(test_preds), axis=1)\nsample_submission.columns = ['id', f'm{m}s{s}_pred']\nsample_submission.to_csv(f'm{m}s{s}_test_pred.csv', index=False)","metadata":{"jupyter":{"source_hidden":true},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Considerations for stacking\n\nAdding similar boosting algorithms to my stack didn't improve my CV or LB scores. Adding non-boosting type models, however - even ones that had way worse scores - improved my LB score by around 0.00010. Interestingly, tuning these non-boosting models decreased my overall scores again.\n\nThank you for reading my notebook! Let me know if these models helped you in any way or if you have suggestion for improving them. I am still experimenting with different combinations and hoping to increase my final results to around 0.85665.","metadata":{}}]}