{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datatable as dt\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nimport optuna\n\nprint (xgb.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-23T19:54:03.077777Z","iopub.execute_input":"2021-10-23T19:54:03.078282Z","iopub.status.idle":"2021-10-23T19:54:04.726764Z","shell.execute_reply.started":"2021-10-23T19:54:03.078173Z","shell.execute_reply":"2021-10-23T19:54:04.726016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In response to https://www.kaggle.com/c/tabular-playground-series-oct-2021/discussion/280986\n\n","metadata":{}},{"cell_type":"markdown","source":"# Memory Reduction\n\n* This memory reduction part taken from https://www.kaggle.com/azzamradman/tps-10-single-xgboost/notebook\n  amazing notebook. Please upvote it if you like this part.","metadata":{}},{"cell_type":"code","source":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:54:04.728481Z","iopub.execute_input":"2021-10-23T19:54:04.728742Z","iopub.status.idle":"2021-10-23T19:54:04.742032Z","shell.execute_reply.started":"2021-10-23T19:54:04.728709Z","shell.execute_reply":"2021-10-23T19:54:04.741229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n### Changed this from DataTable to basic pandas read_csv since I don't want any cols to be of type boolean\ntrain = pd.read_csv('../input/tabular-playground-series-oct-2021/train.csv')\ntrain = reduce_memory_usage(train)\ntest = pd.read_csv('../input/tabular-playground-series-oct-2021/test.csv')\ntest = reduce_memory_usage(test)\n\nss = dt.fread('../input/tabular-playground-series-oct-2021/sample_submission.csv').to_pandas()\nss = reduce_memory_usage(ss)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:54:04.743276Z","iopub.execute_input":"2021-10-23T19:54:04.743675Z","iopub.status.idle":"2021-10-23T19:57:10.884334Z","shell.execute_reply.started":"2021-10-23T19:54:04.743641Z","shell.execute_reply":"2021-10-23T19:57:10.883772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Based on the way this is loaded I'm expecting these to be blank\n\nbool_cols_train = []\nfor i, col in enumerate(train.columns):\n    if train[col].dtypes == bool:\n        bool_cols_train.append(i)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:57:10.887864Z","iopub.execute_input":"2021-10-23T19:57:10.889447Z","iopub.status.idle":"2021-10-23T19:57:10.89633Z","shell.execute_reply.started":"2021-10-23T19:57:10.889413Z","shell.execute_reply":"2021-10-23T19:57:10.895682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Just checking that it was empty\nprint (bool_cols_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:57:10.89964Z","iopub.execute_input":"2021-10-23T19:57:10.901576Z","iopub.status.idle":"2021-10-23T19:57:10.908907Z","shell.execute_reply.started":"2021-10-23T19:57:10.901503Z","shell.execute_reply":"2021-10-23T19:57:10.90822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Based on the way this is loaded I'm expecting these to be blank\n\nbool_cols_test = []\nfor i, col in enumerate(test.columns):\n    if train[col].dtypes == bool:\n        bool_cols_test.append(i)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:57:10.912654Z","iopub.execute_input":"2021-10-23T19:57:10.914114Z","iopub.status.idle":"2021-10-23T19:57:10.920281Z","shell.execute_reply.started":"2021-10-23T19:57:10.914085Z","shell.execute_reply":"2021-10-23T19:57:10.919622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Meaningless code for this test\n# train.iloc[:, bool_cols_train] = train.iloc[:, bool_cols_train].astype(int)\n# test.iloc[:, bool_cols_test] = test.iloc[:, bool_cols_test].astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:57:10.92146Z","iopub.execute_input":"2021-10-23T19:57:10.921805Z","iopub.status.idle":"2021-10-23T19:57:10.929438Z","shell.execute_reply.started":"2021-10-23T19:57:10.921771Z","shell.execute_reply":"2021-10-23T19:57:10.928665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train set shape\", train.shape, \"\\n\", \"Test set shape\", test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:57:10.931874Z","iopub.execute_input":"2021-10-23T19:57:10.932152Z","iopub.status.idle":"2021-10-23T19:57:10.938936Z","shell.execute_reply.started":"2021-10-23T19:57:10.932109Z","shell.execute_reply":"2021-10-23T19:57:10.938084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()\n### I really just want to looks at f276 through to f284","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:57:10.941063Z","iopub.execute_input":"2021-10-23T19:57:10.941433Z","iopub.status.idle":"2021-10-23T19:57:10.974688Z","shell.execute_reply.started":"2021-10-23T19:57:10.941399Z","shell.execute_reply":"2021-10-23T19:57:10.97404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Will need some slight changes here based on pd.read_csv and still having \"id\"\nX = train.drop(columns=[\"id\", \"target\"]).copy()\ny = train[\"target\"].copy()\n\nX_test = test.drop(columns=[\"id\"]).copy()\n\n\n# X = train.drop('target', axis=1).copy()\n# y = train['target'].copy()\n# X_test = test.copy()\n\ndel train\ndel test","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:57:10.977057Z","iopub.execute_input":"2021-10-23T19:57:10.977308Z","iopub.status.idle":"2021-10-23T19:57:12.222376Z","shell.execute_reply.started":"2021-10-23T19:57:10.977277Z","shell.execute_reply":"2021-10-23T19:57:12.221632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### I talk about this in the introduction section of version 4 of https://www.kaggle.com/joecooper/tps-oct-joes-sandpit\n### It is relevant here as well. \n## To be more specific I think it is also relevant to the perceived benefits of Target Encoding potentially showing improvments\n\n# For now I leave this section of code in so we can do a like for like comparison of v1 of this notebook\n\nX['std'] = X.std(axis=1)\nX['min'] = X.min(axis=1)\nX['max'] = X.max(axis=1)\n\nX_test['std'] = X_test.std(axis=1)\nX_test['min'] = X_test.min(axis=1)\nX_test['max'] = X_test.max(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:57:12.223562Z","iopub.execute_input":"2021-10-23T19:57:12.22448Z","iopub.status.idle":"2021-10-23T19:57:41.015593Z","shell.execute_reply.started":"2021-10-23T19:57:12.224446Z","shell.execute_reply":"2021-10-23T19:57:41.014865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## My new section of code for this notebook is to add StandardScaler\n## Again I point to my commentary in  https://www.kaggle.com/joecooper/tps-oct-joes-sandpit where I run the scaler. I'm thinkning that the logic is similar to min/max/std FE\n\n## Is running a scaler as useful as TargetEncoding?\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX = pd.DataFrame (data=scaler.fit_transform(X), columns=X.columns)\nX_test = pd.DataFrame (data=scaler.transform(X_test), columns=X_test.columns)\n\n\n### And now I'm changing my mind again. I think there are a few benefits to running Scalers !\n\n### Some thought will have to go into the inclusion of the std/min/max features being but through a scaler \n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:57:41.016976Z","iopub.execute_input":"2021-10-23T19:57:41.017252Z","iopub.status.idle":"2021-10-23T19:58:07.457348Z","shell.execute_reply.started":"2021-10-23T19:57:41.017219Z","shell.execute_reply":"2021-10-23T19:58:07.456562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:58:07.458728Z","iopub.execute_input":"2021-10-23T19:58:07.458986Z","iopub.status.idle":"2021-10-23T19:58:07.490325Z","shell.execute_reply.started":"2021-10-23T19:58:07.458954Z","shell.execute_reply":"2021-10-23T19:58:07.489456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'max_depth': 6,\n    'n_estimators': 9500,\n    'subsample': 0.7,\n    'colsample_bytree': 0.2,\n    'colsample_bylevel': 0.6000000001,\n    'min_child_weight': 56.419807345,\n    'reg_lambda': 75.5665191,\n    'reg_alpha': 0.11766876,\n    'gamma': 0.64078232264,\n    'booster': 'gbtree',\n    'eval_metric': 'auc',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    'use_label_encoder': False\n    }","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:58:07.491642Z","iopub.execute_input":"2021-10-23T19:58:07.492091Z","iopub.status.idle":"2021-10-23T19:58:07.499847Z","shell.execute_reply.started":"2021-10-23T19:58:07.492055Z","shell.execute_reply":"2021-10-23T19:58:07.499135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nkf = StratifiedKFold(n_splits=6, shuffle=True, random_state=13)\n\npreds = []\nscores = []\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n#     params['learning_rate']=0.007\n    params['learning_rate']=0.0011\n    model1 = XGBClassifier(**params)\n    \n    model1.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_valid,y_valid)],\n              early_stopping_rounds=200,\n              verbose=False)\n    \n#     params['learning_rate']=0.01\n    params['learning_rate']=0.021012\n    model2 = XGBClassifier(**params)\n    \n    model2.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_valid,y_valid)],\n              early_stopping_rounds=200,\n              verbose=False,\n              xgb_model=model1)\n    \n#     params['learning_rate']=0.05\n    params['learning_rate']=0.007\n    model3 = XGBClassifier(**params)\n    \n    model3.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_valid,y_valid)],\n              early_stopping_rounds=200,\n              verbose=False,\n              xgb_model=model2)\n    \n    pred_valid = model3.predict_proba(X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('||'*40)\n    \n    test_preds = model3.predict_proba(X_test)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:58:07.501399Z","iopub.execute_input":"2021-10-23T19:58:07.501663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = np.mean(np.column_stack(preds),axis=1)\n\nss['target'] = predictions\nss.to_csv('./xgb.csv', index=False)\nss.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}