{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Extreme Gradient Boosting model\nAim of this notebook is to produce a Baseline Model to perform initial discovery","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-16T09:04:49.338752Z","iopub.execute_input":"2021-10-16T09:04:49.339161Z","iopub.status.idle":"2021-10-16T09:04:49.348953Z","shell.execute_reply.started":"2021-10-16T09:04:49.339125Z","shell.execute_reply":"2021-10-16T09:04:49.34817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import modules for model analysis\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n# Import xgb modules\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:04:49.350273Z","iopub.execute_input":"2021-10-16T09:04:49.350741Z","iopub.status.idle":"2021-10-16T09:04:49.355867Z","shell.execute_reply.started":"2021-10-16T09:04:49.350706Z","shell.execute_reply":"2021-10-16T09:04:49.355171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in the data\ntrain = pd.read_csv('../input/tabular-playground-series-oct-2021/train.csv',index_col=0)\ntest  = pd.read_csv('../input/tabular-playground-series-oct-2021/test.csv', index_col=0)\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:04:49.357019Z","iopub.execute_input":"2021-10-16T09:04:49.357434Z","iopub.status.idle":"2021-10-16T09:05:50.754342Z","shell.execute_reply.started":"2021-10-16T09:04:49.357398Z","shell.execute_reply":"2021-10-16T09:05:50.753609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:05:50.755748Z","iopub.execute_input":"2021-10-16T09:05:50.756009Z","iopub.status.idle":"2021-10-16T09:05:50.761632Z","shell.execute_reply.started":"2021-10-16T09:05:50.755974Z","shell.execute_reply":"2021-10-16T09:05:50.760824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the memory consumed by the DataFrame\ntrain.info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:05:50.763054Z","iopub.execute_input":"2021-10-16T09:05:50.763333Z","iopub.status.idle":"2021-10-16T09:05:50.791915Z","shell.execute_reply.started":"2021-10-16T09:05:50.763297Z","shell.execute_reply":"2021-10-16T09:05:50.791176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets understand the range of values prior to improving memory performance\ntrain.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:05:50.793184Z","iopub.execute_input":"2021-10-16T09:05:50.793428Z","iopub.status.idle":"2021-10-16T09:06:00.664656Z","shell.execute_reply.started":"2021-10-16T09:05:50.793398Z","shell.execute_reply":"2021-10-16T09:06:00.663865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review cardinality distribution by feature\ntrain.nunique().value_counts()\n# Looks like 46 features could be converted to categorical","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:06:00.666162Z","iopub.execute_input":"2021-10-16T09:06:00.666427Z","iopub.status.idle":"2021-10-16T09:06:09.026185Z","shell.execute_reply.started":"2021-10-16T09:06:00.666391Z","shell.execute_reply":"2021-10-16T09:06:09.02538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> NOTE: check for co-linearity between the categorical features (excluding target)","metadata":{}},{"cell_type":"code","source":"# Check for the max value by feature\ntrain.max().value_counts()\n# It appears that scaling will not be required","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:06:09.027325Z","iopub.execute_input":"2021-10-16T09:06:09.027592Z","iopub.status.idle":"2021-10-16T09:06:09.315747Z","shell.execute_reply.started":"2021-10-16T09:06:09.027558Z","shell.execute_reply":"2021-10-16T09:06:09.314829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets take a sample of the training dataset to perform some model development analysis\n# Use only 25% of the training data in this example\ntrain_data      = train.sample(frac=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:06:09.317134Z","iopub.execute_input":"2021-10-16T09:06:09.317499Z","iopub.status.idle":"2021-10-16T09:06:10.070239Z","shell.execute_reply.started":"2021-10-16T09:06:09.317447Z","shell.execute_reply":"2021-10-16T09:06:10.069403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the original train dataset to conserve space within the environment\ndel train","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:06:10.071694Z","iopub.execute_input":"2021-10-16T09:06:10.071959Z","iopub.status.idle":"2021-10-16T09:06:10.07624Z","shell.execute_reply.started":"2021-10-16T09:06:10.071925Z","shell.execute_reply":"2021-10-16T09:06:10.075416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets reduce the memory usage of the features\n# First - check the integer values and downcast\ndef int_downcast(df):\n    int_cols = df.select_dtypes(include=['int64'])\n\n    for col in int_cols.columns:\n#         print(col, 'min:',df[col].min(),'; max:',df[col].max())\n        df[col] = pd.to_numeric(df[col], downcast ='integer')\n    return df\n\nint_downcast(train_data)\nint_downcast(test)\ntrain_data.memory_usage(deep=True) * 1e-6","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:06:10.077656Z","iopub.execute_input":"2021-10-16T09:06:10.077915Z","iopub.status.idle":"2021-10-16T09:06:11.895303Z","shell.execute_reply.started":"2021-10-16T09:06:10.077882Z","shell.execute_reply":"2021-10-16T09:06:11.89457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:06:11.896583Z","iopub.execute_input":"2021-10-16T09:06:11.896833Z","iopub.status.idle":"2021-10-16T09:06:11.906771Z","shell.execute_reply.started":"2021-10-16T09:06:11.896799Z","shell.execute_reply":"2021-10-16T09:06:11.905984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Second - check the float values and downcast. Method will have to be applied to the train and test DataFrames\ndef float_downcast(df):\n    float_cols = df.select_dtypes(include=['float64'])\n\n    for col in float_cols.columns:\n#         print(col, 'min:',df[col].min(),'; max:',df[col].max())\n        df[col] = pd.to_numeric(df[col], downcast ='float')\n    return df\n\nfloat_downcast(train_data)\nfloat_downcast(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:06:11.908289Z","iopub.execute_input":"2021-10-16T09:06:11.908724Z","iopub.status.idle":"2021-10-16T09:06:57.489296Z","shell.execute_reply.started":"2021-10-16T09:06:11.908685Z","shell.execute_reply":"2021-10-16T09:06:57.488559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review the memory usage by DataFrame\ntrain_data.info(memory_usage='deep')\ntest.info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:06:57.490593Z","iopub.execute_input":"2021-10-16T09:06:57.49098Z","iopub.status.idle":"2021-10-16T09:06:57.535076Z","shell.execute_reply.started":"2021-10-16T09:06:57.490938Z","shell.execute_reply":"2021-10-16T09:06:57.534215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The train dataset has now halved in size so this has helped with the memory aspect. We are now able to store these datasets and can use these going forward. ","metadata":{}},{"cell_type":"code","source":"# Check for missing values\nprint(f'Train df has missing value: {train_data.isnull().sum().value_counts()}')\nprint(f'Test df has missing value: {test.isnull().sum().value_counts()}')","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:06:57.536341Z","iopub.execute_input":"2021-10-16T09:06:57.536703Z","iopub.status.idle":"2021-10-16T09:06:57.900408Z","shell.execute_reply.started":"2021-10-16T09:06:57.536667Z","shell.execute_reply":"2021-10-16T09:06:57.899681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature analysis","metadata":{}},{"cell_type":"code","source":"# Correlation matrix\n# corr = train_data.corr()\n# # Mask the upper triangle\n# mask = np.triu(np.ones_like(corr, dtype=bool))\n# Add the mask to the heatmap\n# sns.heatmap(corr, mask=mask, center=0, linewidths=1, annot=True, fmt=\".2f\")\n# plt.show()\n\n# Remove highly correlated features\ncorr_matrix = train_data.corr().abs()\n\n# Create a True/False mask and apply it\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\ntri_df = corr_matrix.mask(mask)\n\n# List column names of highly correlated features (r > 0.25)\nto_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.25)]\nto_drop","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:07:31.139652Z","iopub.execute_input":"2021-10-16T09:07:31.139905Z","iopub.status.idle":"2021-10-16T09:08:20.161412Z","shell.execute_reply.started":"2021-10-16T09:07:31.139877Z","shell.execute_reply":"2021-10-16T09:08:20.16073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix['f22']","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:08:26.453327Z","iopub.execute_input":"2021-10-16T09:08:26.453991Z","iopub.status.idle":"2021-10-16T09:08:26.461334Z","shell.execute_reply.started":"2021-10-16T09:08:26.45395Z","shell.execute_reply":"2021-10-16T09:08:26.460571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multicollinearity","metadata":{}},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:08:31.14094Z","iopub.execute_input":"2021-10-16T09:08:31.141647Z","iopub.status.idle":"2021-10-16T09:08:31.252235Z","shell.execute_reply.started":"2021-10-16T09:08:31.141608Z","shell.execute_reply":"2021-10-16T09:08:31.251538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review the multicollinearity for the features\n# X_feat = train_data.drop('target', axis=1)\n# vif_data = pd.DataFrame()\n# vif_data['feature'] = X_feat.columns\n# vif_data['VIF'] = [variance_inflation_factor(X_feat.values, i) for i in range(len(X_feat.columns))]","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:53:42.19437Z","iopub.execute_input":"2021-10-16T09:53:42.195071Z","iopub.status.idle":"2021-10-16T09:53:42.198911Z","shell.execute_reply.started":"2021-10-16T09:53:42.195035Z","shell.execute_reply":"2021-10-16T09:53:42.197743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Analysis","metadata":{}},{"cell_type":"code","source":"# Features and label\nX = train_data.drop('target', axis=1)\ny = train_data['target']","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:53:45.211225Z","iopub.execute_input":"2021-10-16T09:53:45.211605Z","iopub.status.idle":"2021-10-16T09:53:45.312539Z","shell.execute_reply.started":"2021-10-16T09:53:45.211565Z","shell.execute_reply":"2021-10-16T09:53:45.311814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare the data to be used within the model. Make use of the lgb.Dataset() method to optimise the memory usage\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=6, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:53:56.545787Z","iopub.execute_input":"2021-10-16T09:53:56.5466Z","iopub.status.idle":"2021-10-16T09:53:56.999846Z","shell.execute_reply.started":"2021-10-16T09:53:56.54654Z","shell.execute_reply":"2021-10-16T09:53:56.999095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline model","metadata":{}},{"cell_type":"code","source":"# Instantiate the XGBClassifier\nxg_cl = xgb.XGBClassifier(objective='binary:logistic', \n                          n_estimators=10, \n                          seed=123, \n                          use_label_encoder=False, \n                          eval_metric='auc', \n                          tree_method='gpu_hist')\n\n# Fit the classifier to the training set\nxg_cl.fit(X_train, y_train)\n\n# Predict the labels of the test set: preds\npreds = xg_cl.predict(X_test)\n\n# Compute the accuracy: accuracy\naccuracy = float(np.sum(preds==y_test))/y_test.shape[0]\nprint(\"accuracy: %f\" % (accuracy))","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:54:03.360992Z","iopub.execute_input":"2021-10-16T09:54:03.361252Z","iopub.status.idle":"2021-10-16T09:54:09.182948Z","shell.execute_reply.started":"2021-10-16T09:54:03.361223Z","shell.execute_reply":"2021-10-16T09:54:09.182375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets use the boosting and inbuild CV methods\n\n# Create the DMatrix from X and y: churn_dmatrix\nd_train = xgb.DMatrix(data=X_train, label=y_train)\nd_test = xgb.DMatrix(data=X_test, label=y_test)\nxgd_test = xgb.DMatrix(data=test)\n\n# Create the parameter dictionary: params. NOTE: have to explicitly provide the objective param\nparams = {\"objective\":\"binary:logistic\", \n          \"max_depth\":3,\n#           \"use_label_encoder\":False, \n          \"eval_metric\":'auc', \n          \"tree_method\":'gpu_hist'\n         }\n\n# Reviewing the AUC metric\n# Perform cross_validation: cv_results\ncv_results = xgb.cv(dtrain=d_train, params=params,\n                  nfold=3, num_boost_round=10, \n                  metrics=\"auc\", as_pandas=True, seed=123)\n\n# Print cv_results\nprint(cv_results)\n\n# Print the AUC\nprint((cv_results[\"test-auc-mean\"]).iloc[-1])","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:54:09.18623Z","iopub.execute_input":"2021-10-16T09:54:09.188011Z","iopub.status.idle":"2021-10-16T09:54:16.061717Z","shell.execute_reply.started":"2021-10-16T09:54:09.187977Z","shell.execute_reply":"2021-10-16T09:54:16.059998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review the train method\nparams = {\n    \"objective\": \"binary:logistic\", \n    \"max_depth\": 3,\n    \"eval_metric\": 'auc', \n    \"tree_method\": 'gpu_hist'\n}\n\n# train - verbose_eval option switches off the log outputs\nxgb_clf = xgb.train(\n    params,\n    d_train,\n    num_boost_round=5000,\n    evals=[(d_train, 'train'), (d_test, 'test')],\n    early_stopping_rounds=100,\n    verbose_eval=0\n)\n\n# predict\ny_pred = xgb_clf.predict(d_test)\n# Compute and print metrics\nprint(f\"AUC : {roc_auc_score(y_test, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:54:16.063522Z","iopub.execute_input":"2021-10-16T09:54:16.063781Z","iopub.status.idle":"2021-10-16T09:54:20.216991Z","shell.execute_reply.started":"2021-10-16T09:54:16.063745Z","shell.execute_reply":"2021-10-16T09:54:20.216194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dimensionlity Reduction","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-10-14T16:25:11.023423Z","iopub.execute_input":"2021-10-14T16:25:11.02371Z","iopub.status.idle":"2021-10-14T16:25:11.030449Z","shell.execute_reply.started":"2021-10-14T16:25:11.023681Z","shell.execute_reply":"2021-10-14T16:25:11.029725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. First model - Lasso Regressor\nfrom sklearn.linear_model import LassoCV\n\n# Create and fit the LassoCV model on the training set\nlcv = LassoCV()\nlcv.fit(X_train, y_train)\nprint('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n\n# Calculate R squared on the test set\nr_squared = lcv.score(X_test, y_test)\nprint('The model explains {0:.1%} of the test set variance'.format(r_squared))\n\n# Create a mask for coefficients not equal to zero\nlcv_mask = lcv.coef_ != 0\nprint('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T16:25:18.8236Z","iopub.execute_input":"2021-10-14T16:25:18.82434Z","iopub.status.idle":"2021-10-14T16:25:22.748186Z","shell.execute_reply.started":"2021-10-14T16:25:18.824302Z","shell.execute_reply":"2021-10-14T16:25:22.747392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n# Create the RFE with a LogisticRegression estimator and 3 features to select\nrfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n\n# Fits the eliminator to the data\nrfe.fit(X_train, y_train)\n\n# Print the features and their ranking (high = dropped early on)\nprint(dict(zip(X.columns, rfe.ranking_)))\n\n# Print the features that are not eliminated\nprint(X.columns[rfe.support_])\n\n# Calculates the test set accuracy\nacc = accuracy_score(y_test, rfe.predict(X_test))\nprint(\"{0:.1%} accuracy on test set.\".format(acc)) ","metadata":{"execution":{"iopub.status.busy":"2021-10-14T16:27:03.084444Z","iopub.execute_input":"2021-10-14T16:27:03.085161Z","iopub.status.idle":"2021-10-14T16:27:07.72025Z","shell.execute_reply.started":"2021-10-14T16:27:03.085125Z","shell.execute_reply":"2021-10-14T16:27:07.718286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-10-11T16:48:52.896015Z","iopub.execute_input":"2021-10-11T16:48:52.89659Z","iopub.status.idle":"2021-10-11T16:48:52.902641Z","shell.execute_reply.started":"2021-10-11T16:48:52.896541Z","shell.execute_reply":"2021-10-11T16:48:52.901464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the random forest model to the training data\n# rf = RandomForestClassifier(random_state=0)\n# rf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T16:45:43.295685Z","iopub.execute_input":"2021-10-11T16:45:43.296078Z","iopub.status.idle":"2021-10-11T16:48:09.501377Z","shell.execute_reply.started":"2021-10-11T16:45:43.296048Z","shell.execute_reply":"2021-10-11T16:48:09.498416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the accuracy\n# acc = accuracy_score(y_test, rf.predict(X_test))\n\n# # Print the importances per feature\n# print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n\n# # Print accuracy\n# print(\"{0:.1%} accuracy on test set.\".format(acc))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T16:49:05.807231Z","iopub.execute_input":"2021-10-11T16:49:05.80811Z","iopub.status.idle":"2021-10-11T16:49:07.027726Z","shell.execute_reply.started":"2021-10-11T16:49:05.808076Z","shell.execute_reply":"2021-10-11T16:49:07.026676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a mask for features importances above the threshold\n# mask = rf.feature_importances_ > 0.01\n\n# # Apply the mask to the feature dataset X\n# reduced_X = X.loc[:, mask]\n\n# # prints out the selected column names\n# print(reduced_X.columns)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T16:49:43.714569Z","iopub.execute_input":"2021-10-11T16:49:43.714859Z","iopub.status.idle":"2021-10-11T16:49:43.756368Z","shell.execute_reply.started":"2021-10-11T16:49:43.714829Z","shell.execute_reply":"2021-10-11T16:49:43.755355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.feature_selection import RFE\n# from sklearn.linear_model import LogisticRegression\n\n# # Create the RFE with a LogisticRegression estimator and 3 features to select\n# rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n\n# # Fits the eliminator to the data\n# rfe.fit(X_train, y_train)\n\n# # Print the features and their ranking (high = dropped early on)\n# print(dict(zip(X.columns, rfe.ranking_)))\n\n# # Print the features that are not eliminated\n# print(X.columns[rfe.support_])\n\n# # Calculates the test set accuracy\n# acc = accuracy_score(y_test, rfe.predict(X_test))\n# print(\"{0:.1%} accuracy on test set.\".format(acc)) ","metadata":{"execution":{"iopub.status.busy":"2021-10-11T16:52:54.269515Z","iopub.execute_input":"2021-10-11T16:52:54.270465Z","iopub.status.idle":"2021-10-11T16:53:00.692923Z","shell.execute_reply.started":"2021-10-11T16:52:54.270398Z","shell.execute_reply":"2021-10-11T16:53:00.688796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make submission","metadata":{}},{"cell_type":"code","source":"def submission_sample(model, df_test, model_name):\n    sample = pd.read_csv('../input/tabular-playground-series-oct-2021/sample_submission.csv')\n    sample['target'] = model.predict(df_test)\n    return sample.to_csv(f'submission_{model_name}.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:54:54.618773Z","iopub.execute_input":"2021-10-16T09:54:54.619486Z","iopub.status.idle":"2021-10-16T09:54:54.62488Z","shell.execute_reply.started":"2021-10-16T09:54:54.619433Z","shell.execute_reply":"2021-10-16T09:54:54.623671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Baseline submission - original code versions\nsubmission_sample(xgb_clf, xgd_test, 'xgb_base')","metadata":{"execution":{"iopub.status.busy":"2021-10-16T09:54:55.653573Z","iopub.execute_input":"2021-10-16T09:54:55.654145Z","iopub.status.idle":"2021-10-16T09:54:58.699124Z","shell.execute_reply.started":"2021-10-16T09:54:55.6541Z","shell.execute_reply":"2021-10-16T09:54:58.69825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}