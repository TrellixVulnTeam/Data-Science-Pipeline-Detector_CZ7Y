{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I never used the Stochastic Gradient Descent classifier before. \nIt's being capable of handling verly large datasets efficiently. \n\nSo let's pick this one and train it.","metadata":{}},{"cell_type":"code","source":"VERBOSE = True","metadata":{"execution":{"iopub.status.busy":"2021-10-18T07:54:30.647384Z","iopub.execute_input":"2021-10-18T07:54:30.64842Z","iopub.status.idle":"2021-10-18T07:54:30.651756Z","shell.execute_reply.started":"2021-10-18T07:54:30.648281Z","shell.execute_reply":"2021-10-18T07:54:30.651167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport gc\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-18T07:54:30.653128Z","iopub.execute_input":"2021-10-18T07:54:30.653971Z","iopub.status.idle":"2021-10-18T07:54:30.667484Z","shell.execute_reply.started":"2021-10-18T07:54:30.653924Z","shell.execute_reply":"2021-10-18T07:54:30.666668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load datasets and memory reduction","metadata":{}},{"cell_type":"code","source":"## from: https://www.kaggle.com/bextuychiev/how-to-work-w-million-row-datasets-like-a-pro\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T07:54:30.668626Z","iopub.execute_input":"2021-10-18T07:54:30.669369Z","iopub.status.idle":"2021-10-18T07:54:30.685948Z","shell.execute_reply.started":"2021-10-18T07:54:30.669338Z","shell.execute_reply":"2021-10-18T07:54:30.684851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-oct-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-oct-2021/test.csv\")\nsubmission = pd.read_csv(\"../input/tabular-playground-series-oct-2021/sample_submission.csv\")\n\ntrain = reduce_memory_usage(train, VERBOSE)\ntest = reduce_memory_usage(test, VERBOSE)\n\nTARGET = 'target'","metadata":{"execution":{"iopub.status.busy":"2021-10-18T07:54:30.687175Z","iopub.execute_input":"2021-10-18T07:54:30.687692Z","iopub.status.idle":"2021-10-18T07:57:06.067813Z","shell.execute_reply.started":"2021-10-18T07:54:30.687655Z","shell.execute_reply":"2021-10-18T07:57:06.066831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get features","metadata":{}},{"cell_type":"code","source":"continous_cols = ['f'+str(i) for i in range(242)]\ncontinous_cols.remove('f22')\ncontinous_cols.remove('f43')\ncategorical_cols = ['f'+str(i) for i in range(242,285)]+['f22','f43']","metadata":{"execution":{"iopub.status.busy":"2021-10-18T07:57:06.069758Z","iopub.execute_input":"2021-10-18T07:57:06.07Z","iopub.status.idle":"2021-10-18T07:57:06.075534Z","shell.execute_reply.started":"2021-10-18T07:57:06.069972Z","shell.execute_reply":"2021-10-18T07:57:06.074704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"mean\"] = train[continous_cols].mean(axis=1)\ntrain[\"std\"] = train[continous_cols].std(axis=1)\ntrain[\"min\"] = train[continous_cols].min(axis=1)\ntrain[\"max\"] = train[continous_cols].max(axis=1)\n\ntest[\"mean\"] = test[continous_cols].mean(axis=1)\ntest[\"std\"] = test[continous_cols].std(axis=1)\ntest[\"min\"] = test[continous_cols].min(axis=1)\ntest[\"max\"] = test[continous_cols].max(axis=1)\n\ncontinous_cols.append('mean')\ncontinous_cols.append('std')\ncontinous_cols.append('min')\ncontinous_cols.append('max')\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T07:57:06.076814Z","iopub.execute_input":"2021-10-18T07:57:06.077261Z","iopub.status.idle":"2021-10-18T07:57:39.850212Z","shell.execute_reply.started":"2021-10-18T07:57:06.077231Z","shell.execute_reply":"2021-10-18T07:57:39.849354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_cols = continous_cols + categorical_cols","metadata":{"execution":{"iopub.status.busy":"2021-10-18T07:57:39.85144Z","iopub.execute_input":"2021-10-18T07:57:39.851692Z","iopub.status.idle":"2021-10-18T07:57:39.855801Z","shell.execute_reply.started":"2021-10-18T07:57:39.851664Z","shell.execute_reply":"2021-10-18T07:57:39.854889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scale data\n\nStochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. ","metadata":{}},{"cell_type":"code","source":"scaler = RobustScaler()\ntrain[continous_cols] = scaler.fit_transform(train[continous_cols])\ntest[continous_cols] = scaler.transform(test[continous_cols])","metadata":{"execution":{"iopub.status.busy":"2021-10-18T07:57:39.857068Z","iopub.execute_input":"2021-10-18T07:57:39.857305Z","iopub.status.idle":"2021-10-18T07:58:08.138655Z","shell.execute_reply.started":"2021-10-18T07:57:39.857278Z","shell.execute_reply":"2021-10-18T07:58:08.136179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Automatic hyper-parameter search\n\nTo find the best parameters I'm using GridSearchCV, with 10% of data. ","metadata":{}},{"cell_type":"code","source":"ResultOfPreviousRun = True\n\nparams = {\n    \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\", \"perceptron\"], #[\"hinge\"]\n    \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n    \"penalty\" : [\"l2\", \"l1\", \"elasticnet\"],    \n    \"random_state\":[2021],\n    \"class_weight\":[\"balanced\"]\n}\n\nif ResultOfPreviousRun and VERBOSE:\n    print(\"best score: 0.7562200000000001\")\n    print(\"best estimator: SGDClassifier(alpha=0.1, class_weight='balanced', penalty='l1',random_state=2021)\") \nelif not PrintResultOfPreviousRun:\n    tmptrain = train.copy()\n\n    tmptrain = tmptrain.sample(frac=0.10, replace=True, random_state=999)\n\n    model = SGDClassifier(max_iter=1000)\n    clf = GridSearchCV(model, cv=5, param_grid=params, verbose=10)\n\n    clf.fit(tmptrain[feature_cols],tmptrain[TARGET])\n\n    if VERBOSE:\n        print(\"best score: \",clf.best_score_)\n        print(\"best estimator: \",clf.best_estimator_)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T07:58:08.142799Z","iopub.execute_input":"2021-10-18T07:58:08.145408Z","iopub.status.idle":"2021-10-18T07:58:08.170101Z","shell.execute_reply.started":"2021-10-18T07:58:08.145231Z","shell.execute_reply":"2021-10-18T07:58:08.167977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling with SGDClassifier\n\nSGDClassifier(loss = 'hinge') does not have probability by default.\n\nYou have to pass SGDclassifier(loss = 'hinge') to CalibratedClassifierCV() which will calculate the probability values of SGDclassifier(loss = 'hinge').        \n  ","metadata":{}},{"cell_type":"code","source":"params = {\n    \"loss\" : \"hinge\",\n    \"alpha\" : 0.1, \n    \"random_state\":2021   \n}\n\npreds = []\nscores = []\n              \nkf = StratifiedKFold(n_splits=50, shuffle=True, random_state=13)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(train[feature_cols],train[TARGET])):\n    \n    X_train, y_train = train[feature_cols].iloc[idx_train], train[TARGET].iloc[idx_train]\n    X_valid, y_valid = train[feature_cols].iloc[idx_valid], train[TARGET].iloc[idx_valid]\n    \n    model = SGDClassifier(**params)    \n    clf = model.fit(X_train, y_train)\n    calibrator = CalibratedClassifierCV(clf, cv='prefit')\n    model = calibrator.fit(X_train, y_train)     \n    \n    pred_valid = model.predict_proba(X_valid)[:,1]\n    score = roc_auc_score(y_valid, pred_valid)\n    scores.append(score)\n    \n    if VERBOSE:\n        print(f\"Fold: {fold + 1} score auc: {score}\")\n    \n    y_hat = model.predict_proba(test[feature_cols])[:,1]\n    preds.append(y_hat)\n\nif VERBOSE:\n    print(f\"Overall Validation Score : {np.mean(scores)}\")\n              \ndel model\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T07:58:08.173606Z","iopub.execute_input":"2021-10-18T07:58:08.174276Z","iopub.status.idle":"2021-10-18T08:01:22.066286Z","shell.execute_reply.started":"2021-10-18T07:58:08.174168Z","shell.execute_reply":"2021-10-18T08:01:22.065359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create submission","metadata":{}},{"cell_type":"code","source":"submission[TARGET] = np.mean(np.column_stack(preds), axis=1)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T08:01:22.067551Z","iopub.execute_input":"2021-10-18T08:01:22.06851Z","iopub.status.idle":"2021-10-18T08:01:23.311038Z","shell.execute_reply.started":"2021-10-18T08:01:22.068476Z","shell.execute_reply":"2021-10-18T08:01:23.310113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nI had better results with LGBM classifier but not bad for the first time.\n\nThis is the first time I use Stochastic Gradient Descent. Your comments and suggestions are welcome.","metadata":{}}]}