{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn â‰¥0.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\ntry:\n    tensorflow.__version__ >= \"2.0\"\nexcept Exception:\n    pass\n\nimport tensorflow as tf\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Common imports\n# from pathlib import Path\nfrom google.cloud import bigquery\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\n\nimport seaborn as sns\n\nimport descartes\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_log_error\n\nimport os\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"covid19-global-forecasting-week-1 database provides geographical information (latitude and longitude) and number of Fatalities and Confirmed Cases in 284 location from 163 countries across the world from 22-Jan-2020 to 24-March-2020 (once a day for each lat and lon position).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv( '/kaggle/input/covid19-global-forecasting-week-1/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.rename(columns={\"Lat\": \"lat\", \"Long\": \"lon\"})\nmo = train['Date'].apply(lambda x: x[5:7])\nda = train['Date'].apply(lambda x: x[8:10])\ntrain['day_from_jan_first'] = (da.apply(int)\n                               + 31*(mo=='02') \n                               + 60*(mo=='03')\n                               + 91*(mo=='04')  \n                              )\n\ntrain= train.dropna(subset = ['lat', 'lon'])\ntrain = train.reset_index(drop=True)\ntrain.lon= train.lon.astype(int)\ntrain.lat= train.lat.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Realized that locations with same latitude, longitude could be located in 2 different countries\ntrain=train.rename(columns={'Country/Region': 'Country'})\ncolumns_train = [\"Country\", \"lat\", \"lon\"]\ncountLONLATCountry= train.groupby(columns_train, as_index=False).agg({'Id': np.count_nonzero, 'ConfirmedCases': np.mean})\n#infact at -122, 37 there are 126 datapoints instead of 63\ncountLONLAT= train['Id'].groupby([train['lon'], train['lat']]).agg(['count'])\n\n# with lat, lon AND country they are univoquely identified, except for 1 location\ncountLONLATCountry= train.groupby(columns_train, as_index=False).agg(['count'])\ncountLONLATCountry[countLONLATCountry> 63].agg(['count'])\n(countLONLATCountry > 63).agg('sum')\n#that location is \ncountLONLATCountry= train.groupby(columns_train, as_index=False).agg({'Id': np.count_nonzero, 'ConfirmedCases': np.mean})\ncountLONLATCountry[(countLONLATCountry['Id'] > 63)==True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a New dataset without that location: this is the easiest to have different countries with same number of timesteps\ntrain_new= train[train.lon !=-64]\ntrain_new.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **BigQuery Public Data offers the NOAA_GSOD database with weather information (GSOD2020 table) recorded in 29745 stations worldwide (stations table). Among the GSOD measures I selected dewpoint that came an intereting predicting factor in my previous analysis in kaggle notebook called covid_week1_weather. **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"PROJECT_ID = 'your-google-cloud-project'\nfrom google.cloud import bigquery\nclient = bigquery.Client(project=PROJECT_ID)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table1_stations = bigquery.TableReference.from_string(\n    \"bigquery-public-data.noaa_gsod.stations\"\n)\n\ndataframe_stations = client.list_rows(\n    table1_stations,\n    selected_fields=[\n        bigquery.SchemaField(\"usaf\", \"STRING\"), #station number, world metherorological org\n        bigquery.SchemaField(\"wban\", \"STRING\"), #wban number, weather bureau army\n        bigquery.SchemaField(\"country\", \"STRING\"),\n        bigquery.SchemaField(\"lat\", \"FLOAT\"),\n        bigquery.SchemaField(\"lon\", \"FLOAT\"),\n    ],\n).to_dataframe()\n\ndataframe_stations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table1_gsod2020 = bigquery.TableReference.from_string(\n    \"bigquery-public-data.noaa_gsod.gsod2020\"\n)\n\ndataframe_gsod2020= client.list_rows(table1_gsod2020,\n    selected_fields=[\n        bigquery.SchemaField(\"stn\", \"STRING\"), #station number\n        bigquery.SchemaField(\"wban\", \"STRING\"), #station number\n        bigquery.SchemaField(\"year\", \"INTEGER\"),\n        bigquery.SchemaField(\"mo\", \"INTEGER\"),\n        bigquery.SchemaField(\"da\", \"INTEGER\"),\n        bigquery.SchemaField(\"temp\", \"FLOAT\"), #mean temp of the day\n        bigquery.SchemaField(\"dewp\", \"FLOAT\"), #mean_dew_point\n        bigquery.SchemaField(\"slp\", \"FLOAT\"), #mean_sealevel_pressure\n        bigquery.SchemaField(\"wdsp\", \"FLOAT\"), #mean_wind_speed\n        bigquery.SchemaField(\"prcp\", \"FLOAT\"), #total_precipitation\n        bigquery.SchemaField(\"sndp\", \"FLOAT\"), #snow_depth\n    ],).to_dataframe()\n\ndataframe_gsod2020","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stations_df= dataframe_stations\ntwenty_twenty_df= dataframe_gsod2020","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">**I merged the two NOAA_GSOD database tables on the common columns (usaf and wban, which are stations identifiers)\n>I will use the new column 'day_from_jan_first' to merge the new weather table with the Covid dataset.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stations_df['STN'] = stations_df['usaf'] + '-' + stations_df['wban']\ntwenty_twenty_df['STN'] = twenty_twenty_df['stn'] + '-' + twenty_twenty_df['wban']\ncols_1= list(twenty_twenty_df.columns)\ncols_2= list(stations_df.columns)\nweather_df = twenty_twenty_df[cols_1].join(stations_df[cols_2].set_index('STN'), on='STN',  how='left', lsuffix='_left', rsuffix='_right')\n\nweather_df['temp'] = weather_df['temp'].apply(lambda x: np.nan if x==9999.9 else x)\nweather_df['slp'] = weather_df['slp'].apply(lambda x: np.nan if x==9999.9 else x)\nweather_df['dewp'] = weather_df['dewp'].apply(lambda x: np.nan if x==9999.9 else x)\nweather_df['wdsp'] = weather_df['wdsp'].apply(lambda x: np.nan if x==999.9 else x)\nweather_df['prcp'] = weather_df['prcp'].apply(lambda x: np.nan if x==999.9 else x)\nweather_df['sndp'] = weather_df['sndp'].apply(lambda x: np.nan if x==999.9 else x)\n\n# convert everything into celsius\ntemp = (weather_df['temp'] - 32) / 1.8\ndewp = (weather_df['dewp'] - 32) / 1.8\n\nweather_df['month']= weather_df['mo']\nweather_df['day']= weather_df['da']\nweather_df['Date']=pd.to_datetime(weather_df[['year','month','day']])\nweather_df['Date2']= weather_df['Date']\nweather_df['Date2']= weather_df['Date2'].astype('str')\nmo2 = weather_df['Date2'].apply(lambda x: x[5:7])\nda2 = weather_df['Date2'].apply(lambda x: x[8:10])\nweather_df['day_from_jan_first'] = (da2.apply(int)\n                               + 31*(mo2=='02') \n                               + 60*(mo2=='03')\n                               + 91*(mo2=='04')  \n                              )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **The level of geospatial granularity for the Covid table is not as fine as the weather table. \nTherefore, I am selecting from the weather table only the locations in the Covid database: THIS APPROACH MIGHT LEAD TO INACCURATE PREDICTIONS WHEN THE COVID AREAS ARE VERY BROAD (COVERING DIFFERENT WEATHER REGIONS).\n> Also I am extracting from the weather table only the COVID days **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_df= weather_df.dropna(subset = ['lat', 'lon'])\nweather_df = weather_df.reset_index(drop=True)\nweather_df.lon= weather_df.lon.astype(int)\nweather_df.lat= weather_df.lat.astype(int)\n\nCovidWeatherTrain=train_new.merge(weather_df, how='left', on=['lat', 'lon', 'day_from_jan_first'])\n# list(CovidWeatherTrain.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TO PLOT COORDINATES THE COORD NEED TO BE TRANSFORMED BACK TO INT\n\nCovidWeatherTrain['lon']= CovidWeatherTrain['lon'].astype(int) \nCovidWeatherTrain['lat']= CovidWeatherTrain['lat'].astype(int)\n\nCovidWeather=CovidWeatherTrain\n# There is a lot of missing data in dew point\ngeom= [Point(xy) for xy in zip(CovidWeather['lon'], CovidWeather['lat'])]\ncrs={'init': 'epsg:4326'}\ngeo_df= gpd.GeoDataFrame(CovidWeather, crs=crs, geometry= geom)\nfig, ax= plt.subplots(figsize = (15,15))\ngeo_df.plot(ax= ax, markersize=CovidWeather.dewp, marker= \"o\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> because some countries have more datapoints (weather stations) than others, there might be a bias in the data\n> I decided to calculate average between stations in the same lat, lon, Country rather than sampling the data to reduce bias","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#TO DO GROUPBY THE COLUMNS NEED TO BE STR\nCovidWeatherTrain['lon']= CovidWeatherTrain['lon'].astype(str) \nCovidWeatherTrain['lat']= CovidWeatherTrain['lat'].astype(str)\nCovidWeatherTrain['Location'] = CovidWeatherTrain['lon'] + '-' + CovidWeatherTrain['lat'] + '-' + CovidWeatherTrain['Country']\nCovidWeatherTrain['Location'][1]\n\ncolumns_train= [\"Country\", \"lat\", \"lon\" , 'day_from_jan_first', \"Location\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multiple Parallel Series or multivariate forecasting\n> First we need to reorganize the time series: individual time series (countries) are in different columns, time variable is rows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"CovidWeather=CovidWeatherTrain\n\ncolumns_Aggregate= [\"Location\", \"day_from_jan_first\" ]\n\nCovidWeatherAggregated= CovidWeather.groupby(columns_Aggregate, as_index=False).agg({ 'dewp': np.nanmean, \"Fatalities\": np.nanmean, \"ConfirmedCases\": np.nanmean}) \nCovidWeatherAggregated = CovidWeatherAggregated.dropna() \nlen(CovidWeatherAggregated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_X = [\"Location\", \"day_from_jan_first\", \"dewp\" ]\ncolumns_y_Cases= [ \"Location\", \"day_from_jan_first\", \"ConfirmedCases\"]\ncolumns_y_Fatalities= [ \"Location\", \"day_from_jan_first\", \"Fatalities\" ]\n\nCovidWeatherAggregated.data= CovidWeatherAggregated[columns_X]\nCovidWeatherAggregated.target1= CovidWeatherAggregated[columns_y_Cases]\nCovidWeatherAggregated.target2= CovidWeatherAggregated[columns_y_Fatalities]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences)-1:\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n        X.append(seq_x)\n        y.append(seq_y)\n    return np.array(X), np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_build(Pivoted):\n    Train, Test  = Pivoted[:round(len(Pivoted)/2)], Pivoted[round(len(Pivoted)/2):]\n    ArrayTrain, ArrayTest = np.array(Train), np.array(Test)\n    \n    #in time series that are not cyclical (like Fatalities and Confirmed Cases in our data) \n    #we cannot split the data in train and test in cronological order, because the underlying function is \n    #exponential\n    \n#     Array = np.array(Pivoted)\n#     tscv = TimeSeriesSplit()\n#     TimeSeriesSplit(max_train_size=None, n_splits=9)\n#     for train_index, test_index in tscv.split(Array):\n#         print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n#         ArrayTrain, ArrayTest = Array[train_index], Array[test_index]\n\n    imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n    imputer=imputer.fit(ArrayTrain)\n    Train_imputed = imputer.transform(ArrayTrain)\n    #this subsitite the nan with the average per column (from previuos or following days. separately for each country)\n\n    imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n    imputer=imputer.fit(ArrayTest)\n    Test_imputed = imputer.transform(ArrayTest)\n    \n    scaler = StandardScaler()\n    Train_scaled = scaler.fit_transform(Train_imputed)\n    Test_scaled = scaler.transform(Test_imputed)\n    \n    n_steps=3\n    Xtrain, ytrain = split_sequences(Train_imputed, n_steps)\n    Xtest, ytest = split_sequences(Test_imputed, n_steps)\n    \n    n_features= Xtrain.shape[2]\n    \n    model =  keras.models.Sequential([\n    #Return_sequence tells whether to return the last output in the output sequence or the full sequence\n    #True is used to return the hidden state output for each input time step.\n    #if Stacking LSTM Return_sequence must be set to True\n    keras.layers.LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)),\n    keras.layers.LSTM(100, activation='relu'),\n    #the number of neurons in the Dense layer is one because we want a single value per country\n    keras.layers.Dense(n_features)\n    ])\n\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    \n    history= model.fit(Xtrain, ytrain, epochs= 30)\n    \n    yhat = model.predict(Xtest, verbose=0)\n    \n    Columns= list(Pivoted.columns)\n    \n\n    Test_reset= Test.reset_index()\n    TestReduced= Test_reset[1:len(Test)-1]\n    Test_mean= TestReduced.mean(axis=1)\n\n    yhat_df= pd.DataFrame(yhat, columns=  Columns)\n    yhat_df['day_from_jan_first']= Test_reset['day_from_jan_first']\n    yhat_mean= yhat_df.mean(axis=1)\n\n    fig = plt.figure()\n    fig.suptitle('Prediction and Loss', fontsize=14, fontweight='bold')\n    ax = fig.add_subplot(1,2,1)\n    yhat_mean.plot(ax= ax, x='day_from_jan_first', y= Columns, color='red', label= 'predicted')\n    Test_mean.plot(ax= ax, x='day_from_jan_first', y= Columns, color='black', label= 'real values')\n    ax.set_xlabel('days')\n    ax.set_title('Prediction')\n    ax.legend(loc=\"upper right\")\n    \n    ax2 = fig.add_subplot(1,2,2)\n    loss= history.history['loss']\n    epochs=range(len(loss))\n    ax2.plot(epochs, loss, 'bo', label='Training Loss')\n    ax2.set_xlabel('epochs')\n    ax2.set_title('Training Loss')\n    plt.show()\n    \n    return Train, Test, yhat_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CovidWeatherXPivoted = CovidWeatherAggregated.data.pivot(index='day_from_jan_first',columns='Location',values='dewp')\nXtrain1, Xtest1, yhat_df1= model_build(CovidWeatherXPivoted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CovidWeatherYCasesPivoted = CovidWeatherAggregated.target1.pivot(index='day_from_jan_first',columns='Location',values='ConfirmedCases')\nXtrain2, Xtest2, yhat_df2= model_build(CovidWeatherYCasesPivoted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CovidWeatherYFatalitiesPivoted = CovidWeatherAggregated.target2.pivot(index='day_from_jan_first',columns='Location',values='Fatalities')\nXtrain3, Xtest3, yhat_df3= model_build(CovidWeatherYFatalitiesPivoted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#The difference between the predicted and the actual data is given by the fact that the train was performed on the first half of the data and the test on the second half , since the trend is exponential the prediction is underestimating.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}