{"cells":[{"metadata":{},"cell_type":"markdown","source":"# StepFunction Team COVID Global Forecast\n\nIn the context of the global COVID-19 pandemic, Kaggle has launched several challenges in order to provide useful insights that may answer some of the open scientific questions about the virus. This is the case of the [COVID19 Global Forecasting](https://www.kaggle.com/c/covid19-global-forecasting-week-1), in which participants are encouraged to fit worldwide data in order to predict the pandemic evolution, hopefully helping to determine factors that impact the transmission rate of COVID-19.\n\n\nChose this notebook as the starting point for the model. Great EDA and approach in this one\nhttps://www.kaggle.com/saga21/covid-global-forecast-sir-model-ml-regressions"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport time\nfrom datetime import datetime\nfrom google.cloud import bigquery\n\n# ML libraries\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nimport  xgboost as xgb\nfrom sklearn.metrics import mean_squared_log_error\npd.options.display.max_columns= None\nfrom sklearn.ensemble import ExtraTreesRegressor\nimport  xgboost as xgb\nfrom sklearn.metrics import mean_squared_log_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Get DATA <a id=\"section1\"></a>\n\nFirst of all, let's take a look on the data structure:"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"#submission = pd.read_csv(\"../input/covid19-global-forecasting-week-1/submission.csv\")\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-1/test.csv\")\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-1/train.csv\")\ndisplay(train.head(5))\ndisplay(train.describe())\nprint(\"Number of Country/Region: \", train['Country/Region'].nunique())\nprint(\"Dates go from day\", max(train['Date']), \"to day\", min(train['Date']), \", a total of\", train['Date'].nunique(), \"days\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Country/Region: \", test['Country/Region'].nunique())\nprint(\"Dates go from day\", max(test['Date']), \"to day\", min(test['Date']), \", a total of\", test['Date'].nunique(), \"days\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset covers 163 countries and almost 2 full months from 2020, which is enough data to get some clues about the pandemic. Let's see some plots of the worldwide tendency to see if we can extract some insights:"},{"metadata":{},"cell_type":"markdown","source":"# 2. Data enrichment <a id=\"section3\"></a>\n\n\nMain workflow of this section:\n1. Join data, filter dates and clean missing values + feature engineering\n2. Add country details (external datasets)\n3. Add Weather Data\n4. Compute lags and trends\n5. Target Encoding\n\n**Disclaimer**: this data enrichment is not mandatory and we could end up not using all of the new features in our models. However I consider it a didactical step that will surely add some value, for example in an in-depth exploratory analysis."},{"metadata":{},"cell_type":"markdown","source":"## 2.1. Join data, filter dates and clean missings\n\nFirst of all, we perform some pre-processing prepare the dataset, consisting on:\n\n* **Join data**. Join train/test to facilitate data transformations\n* **Filter dates**. According to the challenge conditions, remove ConfirmedCases and Fatalities post 2020-03-12\n* **Missings**. Analyze and fix missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge train and test, exclude overlap from test set so that the forecast id are correct\n#dates_overlap = ['2020-03-12','2020-03-13','2020-03-14','2020-03-15','2020-03-16','2020-03-17','2020-03-18','2020-03-19','2020-03-20','2020-03-21','2020-03-22']\ndates=set(test.Date)\ndates_overlap=set(train[train['Date'].isin(dates)]['Date'])\ndates_prediction=dates-dates_overlap\n\n# train2 = train.loc[~train['Date'].isin(dates_overlap)]\ntest2 = test.loc[~test['Date'].isin(dates_overlap)]\nall_data = pd.concat([train, test2], axis = 0, sort=False)\nall_data=all_data.sort_values(by=['Country/Region','Province/State','Date'])\n\n#update the forecastid from test set\nall_data.loc[all_data['Date']>='2020-03-12','ForecastId']=test['ForecastId'].to_list()\n\n# Double check that there are no informed ConfirmedCases and Fatalities after 2020-03-11\n# all_data.loc[all_data['Date'] >= '2020-03-12', 'ConfirmedCases'] = np.nan\n# all_data.loc[all_data['Date'] >= '2020-03-12', 'Fatalities'] = np.nan\nall_data['Date'] = pd.to_datetime(all_data['Date'])\n\n# Create column Day, label encoding Date\nle = preprocessing.LabelEncoder()\nall_data['Day'] = le.fit_transform(all_data.Date)\n\n# # Country wise days since 1st case\nall_data['Province/State'].fillna(\"None\", inplace=True)\nall_data['country/state']=all_data['Country/Region']+\"_\"+all_data['Province/State']\n\ncountrydate = all_data[all_data['ConfirmedCases']>0].groupby('country/state').agg({\"Date\":'min'}).reset_index()\ncountrydate.columns=['country/state','Dayof1stcase']\nall_data=all_data.merge(countrydate, left_on='country/state', right_on='country/state', how='left')\nall_data['Dayofcases'] = (all_data['Date']-all_data['Dayof1stcase']).dt.days\nall_data.loc[~(all_data['Dayofcases']>=0),'Dayofcases']=-1\nall_data=all_data.drop(columns=['Dayof1stcase'])\n\n# # Country wise days since 1st fatality\ncountrydate = all_data[all_data['Fatalities']>0].groupby('country/state').agg({\"Date\":'min'}).reset_index()\ncountrydate.columns=['country/state','Dayof1stfatality']\nall_data=all_data.merge(countrydate, left_on='country/state', right_on='country/state', how='left')\nall_data['Dayoffatalities'] = (all_data['Date']-all_data['Dayof1stfatality']).dt.days\nall_data.loc[~(all_data['Dayoffatalities']>=0),'Dayoffatalities']=-1\nall_data=all_data.drop(columns=['Dayof1stfatality'])\n\n# Aruba has no Lat nor Long. Inform it manually\nall_data.loc[all_data['Lat'].isna()==True, 'Lat'] = 12.510052\nall_data.loc[all_data['Long'].isna()==True, 'Long'] = -70.009354\n\n# Fill null values given that we merged train-test datasets\nall_data['Province/State'].fillna(\"None\", inplace=True)\nall_data['ConfirmedCases'].fillna(0, inplace=True)\nall_data['Fatalities'].fillna(0, inplace=True)\nall_data['Id'].fillna(-1, inplace=True)\nall_data['ForecastId'].fillna(-1, inplace=True)\n\n#Add day of week and month\nall_data['dayofweek'] = all_data['Date'].dt.dayofweek\n# all_data['month'] = all_data['Date'].dt.month\n# all_data['dayofyear'] = all_data['Date'].dt.dayofyear\n\n# display(all_data)\ndisplay(all_data.loc[all_data['Date'] == '2020-03-12'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**: \n* The dataset includes all countries and dates, which is required for the lag/trend step\n* Missing values for \"ConfirmedCases\" and \"Fatalities\" have been replaced by 0, which may be dangerous if we do not remember it at the end of the process. However, since we will train only on dates previous to 2020-03-12, this won't impact our prediction algorithm\n* A new column \"Day\" has been created, as a day counter starting from the first date\n\nDouble-check that there are no remaining missing values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"missings_count = {col:all_data[col].isnull().sum() for col in all_data.columns}\nmissings = pd.DataFrame.from_dict(missings_count, orient='index')\nprint(missings.nlargest(30, 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. Add country details\n\nVariables like the total population of a country, the average age of citizens or the fraction of peoople living in cities may strongly impact on the COVID-19 transmission behavior. Hence, it's important to consider these factors. \n\nDatasets used:\nhttps://www.kaggle.com/tanuprabhu/population-by-country-2020) \nhttps://www.kaggle.com/koryto/countryinfo\nhttps://www.kaggle.com/lewisduncan93/the-economic-freedom-index\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"world_population = pd.read_csv(\"/kaggle/input/population-by-country-2020/population_by_country_2020.csv\")\ncountry_data = pd.read_csv(\"/kaggle/input/countryinfo/covid19countryinfo.csv\")\ncontinent = pd.read_csv(\"/kaggle/input/country-to-continent/countryContinent.csv\", encoding = 'ISO-8859-1')\neconomy = pd.read_csv(\"/kaggle/input/the-economic-freedom-index/economic_freedom_index2019_data.csv\", encoding = 'ISO-8859-1')\n\n\n# Select desired columns and rename some of them\nworld_population = world_population[['Country (or dependency)', 'Population (2020)', 'Density (P/Km²)', 'Land Area (Km²)', 'Med. Age', 'Urban Pop %']]\nworld_population.columns = ['country', 'Population (2020)', 'Density', 'Land Area', 'Med Age', 'Urban Pop']\n\ncountry_data = country_data[['country','quarantine', 'schools', 'restrictions', 'hospibed','smokers']]\ncountry_data.columns = ['country', 'Quarantine', 'Schools', 'Restrictions', 'Hospibed','Smokers']\n\ncontinent = continent[['country','continent','sub_region']]\ncontinent.columns = ['country', 'Continent','Sub_Region']\n\neconomy = economy[['Country Name', 'World Rank','Region Rank', '2019 Score', 'Property Rights', 'Judical Effectiveness',\n       'Government Integrity', 'Tax Burden', \"Gov't Spending\", 'Fiscal Health',\n       'Business Freedom', 'Labor Freedom', 'Monetary Freedom',\n       'Trade Freedom', 'Investment Freedom ', 'Financial Freedom',\n       'Tariff Rate (%)', 'Income Tax Rate (%)', 'Corporate Tax Rate (%)',\n       'Tax Burden % of GDP', \"Gov't Expenditure % of GDP \",  'GDP (Billions, PPP)', 'GDP Growth Rate (%)',\n       '5 Year GDP Growth Rate (%)', 'GDP per Capita (PPP)',\n       'Unemployment (%)', 'Inflation (%)', 'FDI Inflow (Millions)',\n       'Public Debt (% of GDP)']]\n\neconomy.columns=['country', 'World Rank','Region Rank', '2019 Score', 'Property Rights', 'Judical Effectiveness',\n       'Government Integrity', 'Tax Burden', \"Gov't Spending\", 'Fiscal Health',\n       'Business Freedom', 'Labor Freedom', 'Monetary Freedom',\n       'Trade Freedom', 'Investment Freedom ', 'Financial Freedom',\n       'Tariff Rate (%)', 'Income Tax Rate (%)', 'Corporate Tax Rate (%)',\n       'Tax Burden % of GDP', \"Gov't Expenditure % of GDP \",  'GDP (Billions, PPP)', 'GDP Growth Rate (%)',\n       '5 Year GDP Growth Rate (%)', 'GDP per Capita (PPP)',\n       'Unemployment (%)', 'Inflation (%)', 'FDI Inflow (Millions)',\n       'Public Debt (% of GDP)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Replace United States by US\nworld_population.loc[world_population['country']=='United States', 'country'] = 'US'\nworld_population.loc[world_population['country']==\"Gambia\",'country']='The Gambia'\nworld_population.loc[world_population['country']==\"Bahamas\",'country']='The Bahamas'\nworld_population.loc[world_population['country']==\"Réunion\",'country']='Reunion'\nworld_population.loc[world_population['country']==\"Czech Republic (Czechia)\",'country']='Czechia'\nworld_population.loc[world_population['country']==\"DR Congo\",'country']='Congo (Kinshasa)'\nworld_population.loc[world_population['country']==\"Congo\",'country']='Congo (Brazzaville)'\nworld_population.loc[world_population['country']==\"Côte d'Ivoire\",'country']=\"Cote d'Ivoire\"\nworld_population.loc[world_population['country']==\"South Korea\",'country']=\"Korea, South\"\nworld_population.loc[world_population['country']==\"St. Vincent & Grenadines\",'country']='Saint Vincent and the Grenadines'\n\n# continent\ncontinent.loc[continent['country']==\"United States of America\",'country']='US'\ncontinent.loc[continent['country']==\"Bolivia (Plurinational State of)\",'country']='Bolivia'\ncontinent.loc[continent['country']==\"Brunei Darussalam\" ,'country'] = 'Brunei'\ncontinent.loc[continent['country']==\"Gambia\",'country']='The Gambia'\ncontinent.loc[continent['country']==\"Bahamas\",'country']='The Bahamas'\ncontinent.loc[continent['country']==\"Réunion\",'country']='Reunion'\ncontinent.loc[continent['country']==\"Congo (Democratic Republic of the)\",'country']='Congo (Kinshasa)'\ncontinent.loc[continent['country']==\"Congo\",'country']='Congo (Brazzaville)'\ncontinent.loc[continent['country']==\"Czech Republic\",'country']='Czechia'\ncontinent.loc[continent['country']==\"Côte d'Ivoire\",'country']=\"Cote d'Ivoire\"\ncontinent.loc[continent['country']==\"Macedonia (the former Yugoslav Republic of)\",'country']=\"North Macedonia\"\ncontinent.loc[continent['country']==\"Viet Nam\",'country']='Vietnam'\ncontinent.loc[continent['country']==\"Venezuela (Bolivarian Republic of)\",'country']='Venezuela'\ncontinent.loc[continent['country']==\"United Kingdom of Great Britain and Northern Ireland\",'country']='United Kingdom'\ncontinent.loc[continent['country']==\"Tanzania, United Republic of\",'country']='Tanzania'\ncontinent.loc[continent['country']==\"Russian Federation\",'country']='Russia'\ncontinent.loc[continent['country']==\"Moldova (Republic of)\",'country']='Moldova'\ncontinent.loc[continent['country']==\"Korea (Republic of)\",'country']='Korea, South'\ncontinent.loc[continent['country']==\"Iran (Islamic Republic of)\",'country']='Iran'\ncontinent.loc[continent['country']==\"Iran (Islamic Republic of)\",'country']='Iran'\ncontinent.loc[continent['country']==\"Iran (Islamic Republic of)\",'country']='Iran'\n\neconomy.loc[economy['country']==\"United States of America\",'country']='US'\neconomy.loc[economy['country']==\"Bolivia (Plurinational State of)\",'country']='Bolivia'\neconomy.loc[economy['country']==\"Brunei Darussalam\" ,'country'] = 'Brunei'\neconomy.loc[economy['country']==\"Gambia\",'country']='The Gambia'\neconomy.loc[economy['country']==\"Bahamas\",'country']='The Bahamas'\neconomy.loc[economy['country']==\"Réunion\",'country']='Reunion'\neconomy.loc[economy['country']==\"Congo (Democratic Republic of the)\",'country']='Congo (Kinshasa)'\neconomy.loc[economy['country']==\"Congo\",'country']='Congo (Brazzaville)'\neconomy.loc[economy['country']==\"Czech Republic\",'country']='Czechia'\neconomy.loc[economy['country']==\"Côte d'Ivoire\",'country']=\"Cote d'Ivoire\"\neconomy.loc[economy['country']==\"Macedonia (the former Yugoslav Republic of)\",'country']=\"North Macedonia\"\neconomy.loc[economy['country']==\"Viet Nam\",'country']='Vietnam'\neconomy.loc[economy['country']==\"Venezuela (Bolivarian Republic of)\",'country']='Venezuela'\neconomy.loc[economy['country']==\"United Kingdom of Great Britain and Northern Ireland\",'country']='United Kingdom'\neconomy.loc[economy['country']==\"Tanzania, United Republic of\",'country']='Tanzania'\neconomy.loc[economy['country']==\"Russian Federation\",'country']='Russia'\neconomy.loc[economy['country']==\"Moldova (Republic of)\",'country']='Moldova'\neconomy.loc[economy['country']==\"Korea (Republic of)\",'country']='Korea, South'\neconomy.loc[economy['country']==\"Iran (Islamic Republic of)\",'country']='Iran'\neconomy.loc[economy['country']==\"Iran (Islamic Republic of)\",'country']='Iran'\neconomy.loc[economy['country']==\"Iran (Islamic Republic of)\",'country']='Iran'\n\n# actual data\n# all_data.loc[train['Country/Region']==\"Republic of the Congo\",'country']='Congo (Kinshasa)'\nall_data.loc[all_data['Country/Region']==\"Gambia, The\",'Country/Region']='The Gambia'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DEMOGRAPHICS\n# Remove the % character from Urban Pop values\nworld_population['Urban Pop'] = world_population['Urban Pop'].str.rstrip('%')\n\n# Replace Urban Pop and Med Age \"N.A\" by their respective modes, then transform to int\nworld_population.loc[world_population['Urban Pop']=='N.A.', 'Urban Pop'] = int(world_population.loc[world_population['Urban Pop']!='N.A.', 'Urban Pop'].mode()[0])\nworld_population['Urban Pop'] = world_population['Urban Pop'].astype('int16')\nworld_population.loc[world_population['Med Age']=='N.A.', 'Med Age'] = int(world_population.loc[world_population['Med Age']!='N.A.', 'Med Age'].mode()[0])\nworld_population['Med Age'] = world_population['Med Age'].astype('int16')\n\nprint(\"Cleaned country details dataset\")\ndisplay(world_population)\n\n# Now join the dataset to our previous DataFrame and clean missings (not match in left join)\nprint(\"Enriched dataset\")\nall_data = all_data.merge(world_population, left_on='Country/Region', right_on='country', how='left')\nall_data[['Population (2020)', 'Density', 'Land Area', 'Med Age', 'Urban Pop']] = all_data[['Population (2020)', 'Density', 'Land Area', 'Med Age', 'Urban Pop']].fillna(0)\nall_data=all_data.drop(columns=['country'])\n# display(all_data)\n\n\n# CONTINENT INFO\n# Now join the dataset to our previous DataFrame and clean missings (not match in left join)\nprint(\"Enriched dataset\")\nall_data = all_data.merge(continent, left_on='Country/Region', right_on='country', how='left')\nall_data[['Continent','Sub_Region']] = all_data[['Continent','Sub_Region']].fillna('Other')\nall_data=all_data.drop(columns=['country'])\n# display(all_data)\n\n\n# ECONOMY DATA\n# Remove the $ character from GDP values\neconomy['GDP per Capita (PPP)'] = economy['GDP per Capita (PPP)'].str.strip('$')\neconomy['GDP (Billions, PPP)'] = economy['GDP (Billions, PPP)'].str.strip('$')\n# economy['GDP per Capita (PPP)'] = economy['GDP per Capita (PPP)'].str.split('(').apply(lambda x: x[0]).str.replace(\",\",\"\").astype(float)\n# economy['GDP (Billions, PPP)'] = economy['GDP (Billions, PPP)'].str.split('\\ ').apply(lambda x: x[0]).str.replace(\",\",\"\").astype(float)\n# Now join the dataset to our previous DataFrame and clean missings (not match in left join)\nprint(\"Enriched dataset\")\nall_data = all_data.merge(economy, left_on='Country/Region', right_on='country', how='left')\nall_data[economy.columns] = all_data[economy.columns].fillna(0)\nall_data=all_data.drop(columns=['country'])\n# display(all_data)\n# all_data['GDP per Capita (PPP)']= all_data['GDP per Capita (PPP)'].str.split('(').apply(lambda x: x[0]).str.replace(\",\",\"\").astype(float)\n# all_data['GDP (Billions, PPP)'] = all_data['GDP (Billions, PPP)'].str.split('\\ ').apply(lambda x: x[0]).str.replace(\",\",\"\").astype(float)\n\n\n# Covid Country Hospital Smokers Quarantine Data\ncountry_data['Smokers'].fillna(country_data.Smokers.mode()[0],inplace=True)\ncountry_data['Hospibed'].fillna(country_data.Hospibed.mode()[0],inplace=True)\ncountry_data['Quarantine']=pd.to_datetime(country_data['Quarantine'])\ncountry_data['Schools']=pd.to_datetime(country_data['Schools'])\ncountry_data['Restrictions']=pd.to_datetime(country_data['Restrictions'])\n\n\nprint(\"Enriching country actions dataset\")\nall_data = all_data.merge(country_data, left_on='Country/Region', right_on='country', how='left')\nall_data['Smokers'].fillna(country_data.Smokers.mode()[0],inplace=True)\nall_data['Hospibed'].fillna(country_data.Hospibed.mode()[0],inplace=True)\nall_data=all_data.drop(columns=['country'])\ndisplay(all_data)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quarantine info flags\nall_data['quarantine_flag']=(pd.to_datetime(all_data['Quarantine'])<all_data['Date'])\nall_data['schools_flag']=(pd.to_datetime(all_data['Schools'])<all_data['Date'])\nall_data['restrictions_flag']=(pd.to_datetime(all_data['Restrictions'])<all_data['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a=(pd.to_datetime(all_data['Quarantine'])-pd.to_datetime(all_data['Date'])).dt.days\n# a[~(a>=0)]\n\nall_data['quarantine_days']=(pd.to_datetime(all_data['Quarantine'])-pd.to_datetime(all_data['Date'])).dt.days\nall_data['schools_days']=(pd.to_datetime(all_data['Schools'])-pd.to_datetime(all_data['Date'])).dt.days\nall_data['restrictions_days']=(pd.to_datetime(all_data['Restrictions'])-pd.to_datetime(all_data['Date'])).dt.days\n\nall_data.loc[~(all_data['quarantine_days']<=0),'quarantine_days']=-1\nall_data.loc[~(all_data['schools_days']<=0),'schools_days'] =-1\nall_data.loc[~(all_data['restrictions_days']<=0),'restrictions_days']=-1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Adding Weather & Happiness Index Data\nFor Weather Data, We will be using the technique outlined in the great notebook https://www.kaggle.com/davidbnn92/weather-data?scriptVersionId=30695168\n\n\nDataset used fro Happiness Data\nhttps://www.kaggle.com/londeen/world-happiness-report-2020"},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_data.to_csv('all_data.csv', index=False)\nweather_data= pd.read_csv('../input/data-for-covid19/all_data.csv')\nweather_data['Date']=pd.to_datetime(weather_data['Date'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data=all_data.merge(weather_data[['Lat','Long','Country/Region','Province/State','Date','temp', 'min', 'max', 'stp', 'wdsp', 'prcp', 'fog']],\n                          left_on=['Lat','Long','Country/Region','Province/State', 'Date'], right_on=['Lat','Long','Country/Region','Province/State', 'Date'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_happiness_index = pd.read_csv(\"../input/world-happiness/2019.csv\")\nworld_happiness_index.loc[world_happiness_index['Country or region']=='United States', 'Country or region'] = 'US'\nworld_happiness_grouped = world_happiness_index.groupby('Country or region').nth(-1)\n# world_happiness_grouped.drop(\"Year\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.merge(left=all_data, right=world_happiness_grouped, how='left', left_on='Country/Region', right_index=True)\n# all_data = all_data.drop(columns=['Country or region'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data=all_data.sort_values(by=['Country/Region','Province/State','Date'])\nall_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4. Compute lags and trends\n\nEnriching a dataset is key to obtain good results. In this case we will apply 2 different transformations:\n\n**Lag**. Lags are a way to compute the previous value of a column, so that the lag 1 for ConfirmedCases would inform the this column from the previous day. The lag 3 of a feature X is simply:\n$$X_{lag3}(t) = X(t-3)$$\n\n\n**Trend**. Transformig a column into its trend gives the natural tendency of this column, which is different from the raw value. The definition of trend I will apply is: \n$$Trend_{X} = {X(t) - X(t-1) \\over X(t-1)}$$\n\n\n**Moving Average** Rolling mean of confirmed cases and fatalities calculated on the previous 7 days numbers\n$$Moving_{X} = \\frac{1}{7} * {\\sum_{1}^{8}(X(t-i)} $$\n\nThe backlog of lags I'll apply is 14 days, while for trends is 7 days.  For ConfirmedCases and Fatalities:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_trend(df, lag_list, column):\n    for lag in lag_list:\n        trend_column_lag = \"Trend_\" + column + \"_\" + str(lag)\n        df[trend_column_lag] = (df[column].shift(lag, fill_value=0)-df[column].shift(lag+1, fill_value=-999))/df[column].shift(lag+1, fill_value=0)\n    return df\n\n\ndef calculate_lag(df, lag_list, column):\n    for lag in lag_list:\n        column_lag = \"Lag_\" + column + \"_\" + str(lag)\n        df[column_lag] = df[column].shift(lag, fill_value=0)\n    return df\n\ndef moving_average(df,column):\n    column_ma = \"Moving_\" + column \n    column_lag= \"Lag_\" + column + \"_\" + str(1)\n    df[column_ma] = df[column_lag].rolling(window=7).mean()\n    return df\n\nts = time.time()\nall_data=all_data.sort_values(by=['Country/Region','Province/State','Date'])\n\nall_data = calculate_lag(all_data, range(1,7), 'ConfirmedCases')\nall_data = calculate_lag(all_data, range(1,7), 'Fatalities')\nall_data = calculate_trend(all_data, range(1,7), 'ConfirmedCases')\nall_data = calculate_trend(all_data, range(1,7), 'Fatalities')\nall_data.replace([np.inf, -np.inf], 0, inplace=True)\nall_data.fillna(0, inplace=True)\n\nall_data = moving_average(all_data,'ConfirmedCases')\nall_data = moving_average(all_data,'Fatalities')\nall_data.replace([np.inf, -np.inf], 0, inplace=True)\nall_data.fillna(0, inplace=True)\n\nprint(\"Time spent: \", time.time()-ts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_data[all_data['Country/Region']=='Spain'].iloc[40:50][['Id', 'Province/State', 'Country/Region', 'Lat', 'Long', 'Date',\n#        'ConfirmedCases', 'Fatalities', 'ForecastId', 'Day', 'ConfirmedCases_1',\n#        'ConfirmedCases_2', 'ConfirmedCases_3', 'Fatalities_1', 'Fatalities_2',\n#        'Fatalities_3']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.5 Likelihood Encoding\n\nUsing mean values of target variables (in training data) as encodes for country + province unique combination"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = all_data[all_data['ForecastId'] == -1]\nencoding = train_data.groupby('country/state').agg({'ConfirmedCases': 'max', 'Dayofcases': 'max','Fatalities': 'max', 'Dayoffatalities': 'max'})\nencoding['mean_encoding_confirmedCases'] = abs(encoding['ConfirmedCases']/(encoding['Dayofcases']+1))\nencoding['mean_encoding_deathCases'] = abs(encoding['Fatalities']/(encoding['Dayoffatalities']+1))\nencoding.replace([np.inf, -np.inf, np.NaN], 0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data=all_data.merge(encoding[['mean_encoding_confirmedCases','mean_encoding_deathCases']], left_on='country/state', right_index=True, how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Predictions with machine learning <a id=\"section4\"></a>\n\nOur obective in this section consists on  predicting the evolution of the expansion from a data-centric perspective, like any other regression problem. To do so, remember that the challenge specifies that submissions on the public LB shouldn only contain data previous to 2020-03-12.\n\nModels applied:\n1. RandomForest\n"},{"metadata":{},"cell_type":"markdown","source":"# Prepare data for Fitting model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encode country names\ndata = all_data.copy()\ndata = data.drop(columns=['country/state','Quarantine','Schools', 'Restrictions'])\n\ndata['Country/Region'] = le.fit_transform(data['Country/Region'])\n\n# Save dictionary for exploration purposes\nnumber = data['Country/Region']\ncountries = le.inverse_transform(data['Country/Region'])\ncountry_dict = dict(zip(countries, number)) \n\ndata['Continent'] = le.fit_transform(data['Continent'])\ndata['Sub_Region'] = le.fit_transform(data['Sub_Region'])\ndata['Province/State'] = le.fit_transform(data['Province/State'])\n# data.drop(columns=[''])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates_valid={date for date in dates_overlap if date > '2020-03-18'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_data(data):\n        # Train set\n    new_train=data[data.ForecastId == -1]\n    X_train = new_train.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    Y_train_1 = new_train['ConfirmedCases']\n    Y_train_2 = new_train['Fatalities']\n\n    # Valid set\n    valid=data[(data.ForecastId != -1)&(data.Date.isin(dates_overlap))]\n    X_valid = valid.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    Y_valid_1 = valid['ConfirmedCases']\n    Y_valid_2 = valid['Fatalities']\n\n    # Test set\n    new_test=data[(data.ForecastId != -1)&(data.Date.isin(dates))]\n    X_test = new_test.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n\n    # Test set\n    #X_test = data[data.Day > day_valid].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    \n    X_train.drop(columns=['Id','ForecastId','Date'], inplace=True, errors='ignore')\n    \n    valid_index= X_valid['ForecastId']\n    test_index = X_test['ForecastId']\n    X_valid.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n    X_test.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n#     testdata=X_test.copy()\n#     X_test.drop('Date', inplace=True, errors='ignore')\n\n    return new_train, X_train, Y_train_1, Y_train_2, valid, X_valid, Y_valid_1, Y_valid_2, new_test, X_test, valid_index, test_index\n\ndef split_data_24(data):\n        # Train set\n    new_train=data[data.Date <=max(dates_overlap)]\n    X_train = new_train.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    Y_train_1 = new_train['ConfirmedCases']\n    Y_train_2 = new_train['Fatalities']\n\n    # Valid set\n    valid=data[(data.ForecastId != -1)&(data.Date.isin(dates_overlap))]\n    X_valid = valid.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    Y_valid_1 = valid['ConfirmedCases']\n    Y_valid_2 = valid['Fatalities']\n\n    # Test set\n    new_test=data[(data.ForecastId != -1)&(data.Date.isin(dates))]\n    X_test = new_test.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n\n    # Test set\n    #X_test = data[data.Day > day_valid].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    \n    X_train.drop(columns=['Id','ForecastId','Date'], inplace=True, errors='ignore')\n    \n    valid_index= X_valid['ForecastId']\n    test_index = X_test['ForecastId']\n    X_valid.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n    X_test.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n#     testdata=X_test.copy()\n#     X_test.drop('Date', inplace=True, errors='ignore')\n\n    return new_train, X_train, Y_train_1, Y_train_2, valid, X_valid, Y_valid_1, Y_valid_2, new_test, X_test, valid_index, test_index\n\n\n\n# def split_data_18(data):\n#         # Train set\n#     new_train=data[data.Date <='2020-03-18']\n#     X_train = new_train.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n#     Y_train_1 = new_train['ConfirmedCases']\n#     Y_train_2 = new_train['Fatalities']\n\n#     # Valid set\n#     valid=data[(data.ForecastId != -1)&(data.Date.isin(dates_valid))]\n#     X_valid = valid.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n#     Y_valid_1 = valid['ConfirmedCases']\n#     Y_valid_2 = valid['Fatalities']\n\n#     # Test set\n#     new_test=data[(data.ForecastId != -1)&(data.Date.isin(dates))]\n#     X_test = new_test.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n\n#     # Test set\n#     #X_test = data[data.Day > day_valid].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    \n#     X_train.drop(columns=['Id','ForecastId','Date'], inplace=True, errors='ignore')\n    \n#     valid_index= X_valid['ForecastId']\n#     test_index = X_test['ForecastId']\n#     X_valid.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n#     X_test.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n# #     testdata=X_test.copy()\n# #     X_test.drop('Date', inplace=True, errors='ignore')\n\n#     return new_train, X_train, Y_train_1, Y_train_2, valid, X_valid, Y_valid_1, Y_valid_2, new_test, X_test, valid_index, test_index\n\n\ndef split_data_18(data):\n        # Train set\n    new_train=data[data.Date <='2020-03-18']\n    X_train = new_train.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    Y_train_1 = new_train['ConfirmedCases']\n    Y_train_2 = new_train['Fatalities']\n\n    # Valid set\n    valid=data[(data.ForecastId != -1)&(data.Date.isin(dates_overlap))]\n    X_valid = valid.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    Y_valid_1 = valid['ConfirmedCases']\n    Y_valid_2 = valid['Fatalities']\n\n    # Test set\n    new_test=data[(data.ForecastId != -1)&(data.Date.isin(dates))]\n    X_test = new_test.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n\n    # Test set\n    #X_test = data[data.Day > day_valid].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    \n    X_train.drop(columns=['Id','ForecastId','Date'], inplace=True, errors='ignore')\n    \n    valid_index= X_valid['ForecastId']\n    test_index = X_test['ForecastId']\n    X_valid.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n    X_test.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n#     testdata=X_test.copy()\n#     X_test.drop('Date', inplace=True, errors='ignore')\n\n    return new_train, X_train, Y_train_1, Y_train_2, valid, X_valid, Y_valid_1, Y_valid_2, new_test, X_test, valid_index, test_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recreate_valid_split(data):\n    # Valid set\n    valid   = data[(data.ForecastId != -1)&(data.Date.isin(dates_overlap))]\n    X_valid = valid.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    Y_valid_1 = valid['ConfirmedCases']\n    Y_valid_2 = valid['Fatalities']\n    X_valid.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n#     testdata=X_test.copy()\n#     X_test.drop('Date', inplace=True, errors='ignore')\n    return X_valid#, Y_valid_1, Y_valid_2\n\ndef recreate_submission_split(data):\n    # Valid set\n    sub   = data[(data.ForecastId != -1)]\n    X_sub = sub.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    Y_sub_1 = sub['ConfirmedCases']\n    Y_sub_2 = sub['Fatalities']\n    X_sub.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n#     testdata=X_test.copy()\n#     X_test.drop('Date', inplace=True, errors='ignore')\n    return X_sub#, Y_valid_1, Y_valid_2\n\ndef recalculate_lags(df):\n    df = df.sort_values(by=['Country/Region','Province/State','Date'])\n    df = calculate_lag(df, range(1,7), 'ConfirmedCases')\n    df = calculate_lag(df, range(1,7), 'Fatalities')\n#     df = calculate_trend(df, [1], 'ConfirmedCases')\n#     df = calculate_trend(df, [1], 'Fatalities')\n    df = calculate_trend(df, range(1,7), 'ConfirmedCases')\n    df = calculate_trend(df, range(1,7), 'Fatalities')\n    df.replace([np.inf, -np.inf], 0, inplace=True)\n    df.fillna(0, inplace=True)\n    \n    df = moving_average(df,'ConfirmedCases')\n    df = moving_average(df,'Fatalities')\n    df.replace([np.inf, -np.inf], 0, inplace=True)\n    df.fillna(0, inplace=True)\n    return df\n#     print(\"Time spent: \", time.time()-ts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"economy_columns=['World Rank', 'Region Rank', '2019 Score',\n       'Property Rights', 'Judical Effectiveness', 'Government Integrity',\n       'Tax Burden', \"Gov't Spending\", 'Fiscal Health', 'Business Freedom',\n       'Labor Freedom', 'Monetary Freedom', 'Trade Freedom',\n       'Investment Freedom ', 'Financial Freedom', 'Tariff Rate (%)',\n       'Income Tax Rate (%)', 'Corporate Tax Rate (%)', 'Tax Burden % of GDP',\n       \"Gov't Expenditure % of GDP \", 'GDP (Billions, PPP)',\n       'GDP Growth Rate (%)', '5 Year GDP Growth Rate (%)',\n       'GDP per Capita (PPP)', 'Unemployment (%)', 'Inflation (%)',\n       'FDI Inflow (Millions)', 'Public Debt (% of GDP)']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_columns=[e  for e in data.columns if e.startswith('Trend_')]\nlag_columns=[e for e in data.columns if e.startswith('Lag_')  ]\n\n# for e in data.columns:\n#     if e.str.contains('Lag_'):\n#         print(e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train,X_train, Y_train_1, Y_train_2, valid, \\\nX_valid, Y_valid_1, Y_valid_2, new_test,X_test, valid_index, test_index \\\n= split_data(data.drop(columns=economy_columns ))\n                     #+lag_columns+trend_columns))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Random Forest Regressor \n\nRecalculate lags and trends after each date of predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test1=RandomForestRegressor(n_estimators=150,random_state = 42)\n\ntest1.fit(X_train, Y_train_1)\n\ntest2=RandomForestRegressor(n_estimators=150,random_state = 42)\n\ntest2.fit(X_train, Y_train_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=pd.DataFrame(data=X_train.columns, columns=['col'])\nimportances1=test1.feature_importances_\nimportances2=test2.feature_importances_\ncolumns['importances1']=importances1\ncolumns['importances2']=importances2\n\ndisplay(columns[(~columns.col.str.startswith('Trend'))&(~columns.col.str.startswith('Lag'))].sort_values(by='importances1',ascending=False).head(10))\n\ndisplay(columns[(~columns.col.str.startswith('Trend'))&(~columns.col.str.startswith('Lag'))].sort_values(by='importances2',ascending=False).head(10))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inferences from the model**\n\n1. Feature Engineering on Fatalities and Confirmed Cases (Lags, Trends and Moving Averages) came out to be the most important features for the model\n2. Days since the first confirmed case and days since quarantine also highly impact the predictions\n3. Likelihood encodings for province and country combo helped the model to get better results\n4. School and Restriction Flags didn't show up in the important feature list (probably due to the fact that the dates for these restrictions lie in the validation time period 12th to 25th March)\n5. Weather Data specifically temperature values also impact the performance of the model (but lower feature_importance values)\n"},{"metadata":{},"cell_type":"markdown","source":"## Validation Set and RMSLE Error"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_data=data.copy()\ntemp_x_valid= X_valid.copy()\nx_pred=valid.copy() # Final predictions\nx_pred['Predictions1']=0\nx_pred['Predictions2']=0\nfor date in sorted(dates_overlap):  \n    if date>min(dates_overlap):\n        temp_data.loc[temp_data['ForecastId'].isin(forecastids),'ConfirmedCases']=predictions1\n        temp_data.loc[temp_data['ForecastId'].isin(forecastids),'Fatalities']=predictions2\n        temp_data = recalculate_lags(temp_data)\n#         temp_x_valid = recreate_valid_split(temp_data[important_columns+drop_features+lag_columns+trend_columns])\n        temp_x_valid = recreate_valid_split(temp_data.drop(columns=economy_columns))\n    forecastids=temp_data[temp_data['Date']==date]['ForecastId']\n    valid_dataset=temp_x_valid[temp_x_valid['Date']==date].drop(columns=['Date'])\n    \n    predictions1=test1.predict(valid_dataset)\n#     predictions1=pd.concat((valid_dataset['Lag_ConfirmedCases_1'].reset_index(),pd.Series(predictions1)),axis=1)\n#     predictions1=predictions1.drop(columns=['index'])\n#     predictions1=predictions1.max(axis=1).to_list()\n    predictions1[predictions1 < 0] = 0\n    \n    predictions2=test2.predict(valid_dataset)\n#     predictions2=pd.concat((valid_dataset['Lag_Fatalities_1'].reset_index(),pd.Series(predictions2)),axis=1)\n#     predictions2=predictions2.drop(columns=['index'])\n#     predictions2=predictions2.max(axis=1).to_list()\n    predictions2[predictions2 < 0] = 0\n    x_pred.loc[x_pred['ForecastId'].isin(forecastids),'Predictions1']=predictions1\n    x_pred.loc[x_pred['ForecastId'].isin(forecastids),'Predictions2']=predictions2\n#     print(predictions1)\n#     print(predictions2)\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(np.sqrt(mean_squared_log_error( Y_valid_1, x_pred['Predictions1'] )) +\n      np.sqrt(mean_squared_log_error( Y_valid_2, x_pred['Predictions2'] )))/2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Error of 0.0 on validation set due to overlap in training and validation data. Targeting Private Leaderboard**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# xpred = x_pred[['Date','Country/Region','ConfirmedCases','Fatalities','Predictions1','Predictions2']]\n# display(xpred[xpred['Country/Region']==74].tail(5))\n# display(xpred[xpred['Country/Region']==68].tail(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2nd Model trained till 24th March"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train,X_train, Y_train_1, Y_train_2, valid, \\\nX_valid, Y_valid_1, Y_valid_2, new_test,X_test, valid_index, test_index \\\n= split_data_24(data.drop(columns=economy_columns ))\n                     #+lag_columns+trend_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1=RandomForestRegressor(n_estimators=150,random_state = 42)\n\ntest1.fit(X_train, Y_train_1)\n\ntest2=RandomForestRegressor(n_estimators=150,random_state = 42)\n\ntest2.fit(X_train, Y_train_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train,X_train, Y_train_1, Y_train_2, valid,X_valid, Y_valid_1, Y_valid_2, test,X_test, valid_index, test_index = split_data(data.drop(columns=economy_columns))\ntemp_data=data.copy()\ntemp_x_sub= X_test.copy()\nx_pred2=new_test.copy() # Final predictions\nx_pred2['Predictions1']=0\nx_pred2['Predictions2']=0\n\nfor date in sorted(dates):  \n    if date>min(dates):\n        temp_data.loc[temp_data['ForecastId'].isin(forecastids),'ConfirmedCases']=predictions1\n        temp_data.loc[temp_data['ForecastId'].isin(forecastids),'Fatalities']=predictions2\n        temp_data = recalculate_lags(temp_data)\n        temp_x_sub = recreate_submission_split(temp_data.drop(columns=economy_columns))\n    forecastids=temp_data[temp_data['Date']==date]['ForecastId']\n    sub_dataset=temp_x_sub[temp_x_sub['Date']==date].drop(columns=['Date'])\n    predictions1=test1.predict(sub_dataset)\n    predictions1[predictions1 < 0] = 0\n    predictions2=test2.predict(sub_dataset)\n    predictions2[predictions2 < 0] = 0\n    x_pred2.loc[x_pred2['ForecastId'].isin(forecastids),'Predictions1']=predictions1\n    x_pred2.loc[x_pred2['ForecastId'].isin(forecastids),'Predictions2']=predictions2\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Combine validation and test period results"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_pred2.loc[x_pred2['ForecastId'].isin(x_pred['ForecastId']),['Predictions1','Predictions2']]=x_pred[['Predictions1','Predictions2']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(np.sqrt(mean_squared_log_error( Y_valid_1, np.floor(x_pred2[x_pred2.Date.isin(dates_overlap)]['Predictions1']) )) +\n      np.sqrt(mean_squared_log_error( Y_valid_2, np.floor(x_pred2[x_pred2.Date.isin(dates_overlap)]['Predictions2']) )))/2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submissions 12-25th march from 1st model and 26th+ from 2nd model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_submission(x_pred):\n    # Submit predictions\n#     submission = pd.DataFrame({\n#         \"ForecastId\": index, \n#         \"ConfirmedCases\": prediction1,\n#         \"Fatalities\": predictions2\n#     })\n    submission=x_pred[['ForecastId','Predictions1','Predictions2']].copy()\n    submission.columns=['ForecastId','ConfirmedCases','Fatalities']\n    submission.loc[:,'ForecastId']=submission['ForecastId'].astype(int)\n    submission.loc[:,'ConfirmedCases']=submission['ConfirmedCases']\n    submission.loc[:,'Fatalities']=submission['Fatalities']\n    \n#     submission['ConfirmedCases']=np.ceil(submission['ConfirmedCases'])\n    submission.to_csv('submission.csv', index=False)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_submission(x_pred2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}