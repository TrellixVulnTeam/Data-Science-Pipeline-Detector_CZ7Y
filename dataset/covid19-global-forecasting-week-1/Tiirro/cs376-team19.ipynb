{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/covid19-global-forecasting-week-1/train.csv')\ntest_data = pd.read_csv('../input/covid19-global-forecasting-week-1/test.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\n\ntrain_data['Date'] = pd.to_datetime(train_data['Date'], format='%Y-%m-%d')\ntest_data['Date'] = pd.to_datetime(test_data['Date'], format='%Y-%m-%d')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.groupby('Fatalities').mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data pre-processing\ntrain_sl = 63 # time sequence length of training data\ntest_sl =  43 # time sequence length of test data\nnum_region = 284 # number of regions \ntrain_size = train_data.shape[0]\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\n#######################################################################################\n# Combining region features into one.\ntrain_data = train_data.fillna(value='Only')\ntest_data = test_data.fillna(value='Only')\nprint(train_data.head())\n\ntrain_data['region'] = train_data['Country/Region'] + '-' + train_data['Province/State']\ntest_data['region'] = test_data['Country/Region'] + '-' + test_data['Province/State']\n\ntrain_data = train_data.drop(columns=['Province/State', 'Country/Region'])\ntest_data = test_data.drop(columns=['Province/State', 'Country/Region'])\n\nprint(train_data.head())\n#######################################################################################\n\n#######################################################################################\n# numerical data scaling use sklearn.preprocessing.StandardScaler()\n\n#######################################################################################\n\n#######################################################################################\n# in Curve Fitting, We do not use 'Lat', 'Long' features\n\ntrain_data = train_data.drop(columns=['Lat', 'Long'])\ntest_data = test_data.drop(columns=['Lat', 'Long'])\n\n#######################################################################################\n\n#######################################################################################\n# target scaling use sklearn.preprocessing.StandardScaler()\n\nConf_fata = ['ConfirmedCases', 'Fatalities']\n\ntarget_num = train_data.loc[:, Conf_fata] # to do scaling\n\nConf_scaling = StandardScaler()\nFata_scaling = StandardScaler()\n\nConf_processed = Conf_scaling.fit_transform(target_num.loc[:,'ConfirmedCases'].values.reshape(-1, 1))\nFata_processed = Fata_scaling.fit_transform(target_num.loc[:,'Fatalities'].values.reshape(-1, 1))\n\ntrain_data['ConfirmedCases'] = Conf_processed\ntrain_data['Fatalities'] = Fata_processed\n\nprint(train_data.head())\n\n\n\n#######################################################################################\n\n#######################################################################################\n# transform Date feature to more smaller value; days after 2020-01-22\nstart = pd.to_datetime('2020-01-22')\ntrain_data['Days after Jan 22'] = train_data['Date'] - start\ntest_data['Days after Jan 22'] = test_data['Date'] - start\n\n# deviding (864*1E+11); to make its unit as a 'day', dividing 10 to prevent overflow at exponential function\ntrain_data['Days after Jan 22'] = train_data['Days after Jan 22'].astype(np.int64)/((864*1E+11)*4)\ntest_data['Days after Jan 22'] = test_data['Days after Jan 22'].astype(np.int64)/((864*1E+11)*4)\n\ntrain_data = train_data.drop(columns=['Date'])\ntest_data = test_data.drop(columns=['Date'])\n\nprint(train_data.head(63))\n#######################################################################################\n\n#######################################################################################\n# Target value scaling by using logarithm\n\n# train_data['ConfirmedCases'] = train_data['ConfirmedCases'].apply(lambda x : np.log(1+x))\n# train_data['Fatalities'] = train_data['Fatalities'].apply(lambda x : np.log(1+x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data grouping - Cause we want to predict country by country\n\nRegions = np.unique(train_data['region'].values).tolist()\n\ntrain_data_by_regions = {}\ntest_data_by_regions ={}\n\nfor i in range(num_region):\n    region = Regions[i]\n    train_data_by_regions[region] = train_data.iloc[i*train_sl:(i+1)*train_sl, :]\n    test_data_by_regions[region] = test_data.iloc[i*test_sl:(i+1)*test_sl, :]\n    \nprint(train_data_by_regions['Korea, South-Only'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###########################\n# models - Logistic fitting without Lat/Long data\n# decided to discard lat/long data > since we gonna fit this model country by country; so they are meaningless now.\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n# our model; Logistic curve\ndef logi_curve(x, k, x_0, N, b, c):\n    return N / (1+np.exp(k*k*(x_0-x))) + b*x + c # original: N / (1+np.exp(-k*(x-x_0))) # adding bias term. # adding bias term as a linear function to catch more increases\n\n# alternative model; Exponential curve\ndef exp_curve(x, k, x_0, a, b):\n    return a*np.exp(k*(x-x_0))+b\n\ndef curve_fitting(Country, Conf_or_Fata):\n    X = train_data_by_regions[Country].loc[:, 'Days after Jan 22'].to_numpy()\n    \n    if Conf_or_Fata:\n        y = train_data_by_regions[Country].loc[:, 'ConfirmedCases'].to_numpy()\n    else:\n        y = train_data_by_regions[Country].loc[:, 'Fatalities'].to_numpy()\n    \n    try:\n        popt, pcov = curve_fit(logi_curve, X, y,method='lm', maxfev = 100000)\n        logi_or_exp = True\n    except RuntimeError as e:\n        print(e)\n        try:\n            popt, pcov = curve_fit(exp_curve, X, y, maxfev = 5000)\n            logi_or_exp = False\n        except:\n            popt = (0, 0, 0, np.amin(y))\n            logi_or_exp = False\n            \n    return popt, logi_or_exp\n\ndef prediction_by_countries(Country, popt, logi_or_exp):\n    X = test_data_by_regions[Country].loc[:, 'Days after Jan 22'].to_numpy()\n\n    predictions = []\n    if logi_or_exp:\n        for i in range(X.shape[0]):\n            y = logi_curve(X[i], *popt)\n            predictions.append(y)\n    else:\n         for i in range(X.shape[0]):\n            y = exp_curve(X[i], *popt)\n            predictions.append(y)\n\n    return np.array(predictions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training\n\nConf_params_by_countries ={}\nFata_params_by_countries ={}\n\nfor region in Regions:\n    Conf_param, conf_loe = curve_fitting(region, True)\n    Fata_param, fata_loe = curve_fitting(region, False)\n    \n    Conf_params_by_countries[region]=(Conf_param, conf_loe)\n    Fata_params_by_countries[region]=(Fata_param, fata_loe)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting\n\npreds_by_countries = {}\nConf_predictions = np.array([])\nFata_predictions = np.array([])\n\nfor region in Regions:\n    conf_param, conf_loe = Conf_params_by_countries[region]\n    fata_param, fata_loe = Fata_params_by_countries[region]\n    \n    conf_pred = prediction_by_countries(region, conf_param, conf_loe)\n    fata_pred = prediction_by_countries(region, fata_param, fata_loe)\n    \n    preds_by_countries[region]= (conf_pred, fata_pred)\n    \n    Conf_predictions = np.append(Conf_predictions, conf_pred)\n    Fata_predictions = np.append(Fata_predictions, fata_pred)\n\n    \n# inverse transform\nConf_predictions = Conf_scaling.inverse_transform(Conf_predictions)\nFata_predictions = Fata_scaling.inverse_transform(Fata_predictions)\n# inverse transform;\n# Conf_predictions = np.exp(Conf_predictions)-1\n# Fata_predictions = np.exp(Fata_predictions)-1\n\nConf_predictions[Conf_predictions<0]=0\nFata_predictions[Fata_predictions<0]=0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to show our fitted model of korea, south.\nconf_param, conf_loe = Conf_params_by_countries['Korea, South-Only']\nfata_param, fata_loe = Fata_params_by_countries['Korea, South-Only']\nX = train_data_by_regions['Korea, South-Only'].loc[:, 'Days after Jan 22'].to_numpy()\n\nconf_model = []\nfata_model = []\nfor i in range(X.shape[0]):\n    if conf_loe:\n        conf = logi_curve(X[i], *conf_param)\n    else:\n        conf = exp_curve(X[i], *conf_param)\n    if fata_loe:\n        fata = logi_curve(X[i], *fata_param)\n    else:\n        fata = exp_curve(X[i], *fata_param)\n    conf_model.append(conf)\n    fata_model.append(fata)\n\nconf_model = np.array(conf_model)\nfata_model = np.array(fata_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot our results.\n%matplotlib inline\n\ndays_train = train_data_by_regions['Korea, South-Only'].loc[:, 'Days after Jan 22'].to_numpy() *4\ndays_test = test_data_by_regions['Korea, South-Only'].loc[:, 'Days after Jan 22'].to_numpy() *4\n\nConfirmed_cases_train = train_data_by_regions['Korea, South-Only'].loc[:, 'ConfirmedCases'].to_numpy() \nConfirmed_cases_trainedmodel = conf_model\nConfirmed_cases_test = preds_by_countries['Korea, South-Only'][0] #prediction\n\nFatalities_train = train_data_by_regions['Korea, South-Only'].loc[:, 'Fatalities'].to_numpy()\nFatalities_trainedmodel = fata_model\nFatalities_test = preds_by_countries['Korea, South-Only'][1] # prediction\n\nplt.figure(figsize=(25,10))\nplt.plot(days_train, Confirmed_cases_train, 'b-', label='Confirmed_cases_train')\nplt.plot(days_train, Confirmed_cases_trainedmodel, 'r--', label='Confirmed_cases_trained_model')\nplt.plot(days_test, Confirmed_cases_test, 'ro', lw=1, label='Confirmed_cases_test')\nplt.xlabel('Days after Jan 22, 2020')\nplt.ylabel('ConfirmedCases')\nplt.legend(loc='upper left')\nplt.show()\n\nplt.figure(figsize=(25,10))\nplt.plot(days_train, Fatalities_train, 'g-', label='Fatalities_train')\nplt.plot(days_train, Fatalities_trainedmodel, 'r--', label='Fatalities_trained_model')\nplt.plot(days_test, Fatalities_test, 'ro', lw=1, label='Fatalities_test')\nplt.xlabel('Days after Jan 22, 2020')\nplt.ylabel('Fatalities')\nplt.legend(loc='upper left')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/covid19-global-forecasting-week-1/submission.csv')\noutput = pd.DataFrame({\n    'ForecastId': submission['ForecastId'],\n    'ConfirmedCases': Conf_predictions,\n    'Fatalities': Fata_predictions\n})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.head()\noutput","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}