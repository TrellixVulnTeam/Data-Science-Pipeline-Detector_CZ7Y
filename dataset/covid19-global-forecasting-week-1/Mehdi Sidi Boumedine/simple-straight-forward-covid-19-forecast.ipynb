{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            print(\"******************************\")\n            print(\"Column: \",col)\n            if props[col].dtype=='datetime64[ns]':\n                continue \n            print(\"dtype before: \",props[col].dtype)\n            \n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n            print(\"dtype after: \",props[col].dtype)\n            print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return props, NAlist","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n#submission = pd.read_csv(\"../input/covid19-global-forecasting-week-1/submission.csv\")\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-1/test.csv\", parse_dates=[\"Date\"])\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-1/train.csv\", parse_dates=[\"Date\"])\nglobal_data = pd.read_csv(\"../input/externalcountrydata/Global_Data_by_Country_2019.csv\")\n\nreduce_mem_usage(test)\nreduce_mem_usage(train)\nreduce_mem_usage(global_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"global_data.head()\nglobal_data.groupby(\"CountryName\")[\"ExtraColumn\"].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train[\"Province/State\"].isnull(), \"Province/State\"]=train.loc[train[\"Province/State\"].isnull(), \"Country/Region\"]\n\ntrain.rename(columns = {'Country/Region':'Country', 'Province/State':'Province'}, inplace = True) \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain=train.merge(global_data, how='left', left_on=['Country', 'Province'], right_on=['CountryName', 'Province'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.rename(columns = {'Country/Region':'Country', 'Province/State':'Province'}, inplace = True) \n\ntest.loc[test[\"Province\"].isnull(), \"Province\"]=test.loc[test[\"Province\"].isnull(), \"Country\"]\n\nX_test=test.merge(global_data, how='left', left_on=['Country', 'Province'], right_on=['CountryName', 'Province'])\nX_test=X_test.drop(\"Population\", axis=1)\nX_test=X_test.drop(\"CountryName\", axis=1)\nX_test=X_test.rename(columns={\"ExtraColumn\": \"Population\"})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del global_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop(\"Population\", axis=1)\ntrain=train.drop(\"CountryName\", axis=1)\n\ntrain=train.rename(columns={\"ExtraColumn\": \"Population\"})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mindates = train[train[\"ConfirmedCases\"]>0].groupby(['Province'])[\"Date\"].min()\nmindates.reset_index()\nmindatesDF = mindates.to_frame()\nmindatesDF.rename(columns={\"Date\":\"MinDate\"}, inplace=True)\ntrain=train.merge(mindatesDF, how='left', left_on=\"Province\", right_on=\"Province\")\ntrain[\"DaysFrom1stCase\"]=(train[\"Date\"]-train[\"MinDate\"]).dt.days\ntrain.loc [train[\"DaysFrom1stCase\"]<0 , \"DaysFrom1stCase\"] =0\n\n## after version 2 ###\n#train=pd.get_dummies(train, prefix='prov', prefix_sep='_', dummy_na=True, columns=\"Province\", sparse=False, drop_first=False, dtype=None)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder \n\nlabelencoder = LabelEncoder()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"province=labelencoder.fit_transform(train[\"Province\"])\ntrain=pd.concat([train, pd.DataFrame(province)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain=train.drop([\"Country\",\"MinDate\", \"Province\"], axis=1)\n#train=train.drop([\"Country\",\"MinDate\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.ConfirmedCases>0].head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\n\n\ny_train_CC=train.loc[:,\"ConfirmedCases\"]\ny_train_F=train.loc[:, \"Fatalities\"]\nX_train=train.drop([\"ConfirmedCases\", \"Fatalities\", \"Id\", \"Date\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in X_train.columns:\n    X_train[col]=X_train[col].fillna(X_train[col].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\n#pca = PCA(n_components=5)\n#pca.fit(X_train)\n\n#X_train= pd.DataFrame(pca.transform(X_train))\n\n\nX_train_real, X_test_val, y_train_real_CC, y_test_val_CC = train_test_split(\n        X_train, y_train_CC, test_size=0.3, random_state=0)\n\n\n\nlgb_train_CC = lgb.Dataset(X_train_real, y_train_real_CC)\nlgb_eval_CC = lgb.Dataset(X_test_val, y_test_val_CC, reference=lgb_train_CC)\n\n\nX_train_real, X_test_val, y_train_real_F, y_test_val_F = train_test_split(\n        X_train, y_train_F, test_size=0.3, random_state=0)\n\nlgb_train_F = lgb.Dataset(X_train_real, y_train_real_F)\nlgb_eval_F = lgb.Dataset(X_test_val, y_test_val_F, reference=lgb_train_F)\n\n\n#X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_real.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': {'rmse'},\n        'learning_rate': 0.3,\n        'num_leaves': 30,\n        'min_data_in_leaf': 1,\n        'num_iteration': 100,\n        'verbose': 20\n}\n\ngbm_CC = lgb.train(params,\n            lgb_train_CC,\n            num_boost_round=100,\n            valid_sets=lgb_eval_CC,\n            early_stopping_rounds=10)\n\ngbm_F = lgb.train(params,\n            lgb_train_F,\n            num_boost_round=100,\n            valid_sets=lgb_eval_F,\n            early_stopping_rounds=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Transform the Test Dataset before prediction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n## add days since first case column ##\nX_test=X_test.merge(mindatesDF, how='left', left_on=\"Province\", right_on=\"Province\")\nX_test[\"DaysFrom1stCase\"]=(X_test[\"Date\"]-X_test[\"MinDate\"]).dt.days\nX_test.loc [X_test[\"DaysFrom1stCase\"]<0 , \"DaysFrom1stCase\"] =0\n\n#X_test=pd.get_dummies(X_test, prefix='prov', prefix_sep='_', dummy_na=True, columns=\"Province\", sparse=False, drop_first=False, dtype=None)\n\nX_test=X_test.drop([\"Country\", \"MinDate\"], axis=1)\n\nX_test=X_test.drop([\"ForecastId\", \"Date\"], axis=1)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"province=labelencoder.transform(X_test[\"Province\"])\nX_test=pd.concat([X_test,pd.DataFrame(province) ], axis=1)\nX_test=X_test.drop([\"Province\"], axis=1)\nfor col in X_test.columns:\n    X_test[col]=X_test[col].fillna(X_test[col].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_test= pca.transform(X_test)\n\n#y_pred=regressor.predict(X_test)\ny_pred_CC = gbm_CC.predict(X_test, num_iteration=gbm_CC.best_iteration)\ny_pred_F = gbm_F.predict(X_test, num_iteration=gbm_F.best_iteration)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecastId=test.ForecastId.to_numpy()\nsubmission_CC=pd.DataFrame(y_pred_CC)\nsubmission_F=pd.DataFrame(y_pred_F)\nforecastIdDF=pd.DataFrame(forecastId)\nforecastIdDF=forecastIdDF.rename(columns={0:\"ForecastId\"})\nsubmission=pd.concat([forecastIdDF, submission_CC, submission_F ], axis=1)\nsubmission=submission.rename(columns={0:\"ConfirmedCases\", 1:\"Fatalities\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.concat([test, submission ], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test.Province==\"Italy\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.ConfirmedCases.sum()\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}