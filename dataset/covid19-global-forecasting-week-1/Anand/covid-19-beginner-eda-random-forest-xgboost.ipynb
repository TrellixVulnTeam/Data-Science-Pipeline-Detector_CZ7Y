{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Beginner Attempt:\n<p>\nThis is an attempt at performing some EDA and predicting ConfirmedCases and Fatalities using a RandomForest, XGBoost Regressor model. <br>\n    There is overlapping Data in the Test and Train sets, predicting without the overlap predictably results in a lower score. <br> \n    Will appreciate any advice.\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nnp.set_printoptions(threshold=sys.maxsize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_train = 'covid19-global-forecasting-week-1/train-2.csv'\npath_test = 'covid19-global-forecasting-week-1/test.csv'\npath_sbumit = 'covid19-global-forecasting-week-1/submission.csv'\n\ntrain_kaggle = '/kaggle/input/covid19-global-forecasting-week-1/train.csv'\ntest_kaggle = '/kaggle/input/covid19-global-forecasting-week-1/test.csv'\nsubmit_kaggle = '/kaggle/input/covid19-global-forecasting-week-1/submission.csv'\n\ndf_train = pd.read_csv(train_kaggle)\ndf_test = pd.read_csv(test_kaggle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{},"cell_type":"markdown","source":"## About the Data \n1. Contains Daily Reports of Number of Cases and Fatalities for countries.\n2. [Missing Data]Contains some entries with Province/State Information Missing - Dropped.\n3. Contains latitude and longitude for entries, Can Plot on map.\n4. Date - 22nd Feb to 23nd March. (Getting Updated Continuosly)\n5. Country/Region - 163"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset Dimesnions\nprint('Train shape', df_train.shape)\nprint('Test shape', df_test.shape)\n# Missing/Null Values\nprint('\\nTrain Missing\\n', df_train.isnull().sum())\nprint('\\nTest Missing\\n', df_test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Unique countries in the dataset "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"lst = df_train['Country/Region'].unique()\nprint('Total_Countries\\n:', len(lst))\nfor i in lst:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Date Range for the Dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train['Date'].min(), ' - ', df_train['Date'].max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Daily Worldwide Confirmed Cases and Fatalities "},{"metadata":{"trusted":true},"cell_type":"code","source":"# GroupBy syntax (columns to group by in list)[Columns to aggregate, apply function to] . aggregation functions on it \ntrain_cases_conf = df_train.groupby(['Date'])['ConfirmedCases'].sum()\ntrain_cases_conf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cases_conf.plot(figsize = (10,8), title = 'Worldwide Confirmed Cases')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fatal = df_train.groupby(['Date'])['Fatalities'].sum()\ntrain_fatal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fatal.plot(figsize = (10,8), title = 'Worldwide Fatalaties')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check Confirmed cases and fatalities for a country \nscale = \"linear\", \"log\""},{"metadata":{"trusted":true},"cell_type":"code","source":"def country_stats(country, df):\n    country_filt = (df['Country/Region'] == country)\n    df_cases = df.loc[country_filt].groupby(['Date'])['ConfirmedCases'].sum()\n    df_fatal = df.loc[country_filt].groupby(['Date'])['Fatalities'].sum()\n    fig, axes = plt.subplots(nrows = 2, ncols= 1, figsize=(15,15))\n    df_cases.plot(ax = axes[0])\n    df_fatal.plot(ax = axes[1])\n    \ncountry_stats('US', df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fatalities and Confirmed Cases by Country (Log Scale)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# grouping using same Country filter to get fatalities on each date (grouped by date)\n# groupby([list of columns to groupby]) [which columns to apply aggregate fx to ]. (aggregate function)\n# To Do - Fix Ticks \n\ndef country_stats_log(country, df):\n    count_filt =(df_train['Country/Region'] == country)\n    df_count_case = df_train.loc[count_filt].groupby(['Date'])['ConfirmedCases'].sum()\n    df_count_fatal = df_train.loc[count_filt].groupby(['Date'])['Fatalities'].sum()\n    plt.figure(figsize=(15,10))\n    plt.axes(yscale = 'log')\n    plt.plot(df_count_case.index, df_count_case.tolist(), 'b', label = country +' Total Confirmed Cases')\n    plt.plot(df_count_fatal.index, df_count_fatal.tolist(), 'r', label = country +' Total Fatalities')\n    plt.title(country +' COVID Cases and Fatalities (Log Scale)')\n    plt.legend()\n    \n\ncountry_stats_log('US', df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Most Affected Countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# as_index = False to not make the grouping column the index, creates a df here instead of series, preserves\n# Confirmedcases column\n\ntrain_case_country = df_train.groupby(['Country/Region'], as_index=False)['ConfirmedCases'].max()\n\n# Sorting by number of cases\ntrain_case_country.sort_values('ConfirmedCases', ascending=False, inplace = True)\ntrain_case_country","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.bar(train_case_country['Country/Region'][:5], train_case_country['ConfirmedCases'][:5], color = ['red', 'yellow','black','blue','green'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### No. of Cases on a Particular Day, (Not Increase, Cumulative)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confirmed Cases till a particular day by country\n\ndef case_day_country (Date, df):\n    df = df.groupby(['Country/Region', 'Date'], as_index = False)['ConfirmedCases'].sum()\n    date_filter = (df['Date'] == Date)\n    df = df.loc[date_filter]\n    df.sort_values('ConfirmedCases', ascending = False, inplace = True)\n    sns.catplot(x = 'Country/Region', y = 'ConfirmedCases' , data = df.head(10), height=5,aspect=3, kind = 'bar')\n    \n    \ncase_day_country('2020-03-23', df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Wrangling/ Pre-processing/ Cleaning \n1. Identifying and Handling missing values.\n2. Data Formating.\n3. Data Normalization(centering and scaling).\n4. Data bining.\n5. Turning categorical values into numerical values."},{"metadata":{},"cell_type":"markdown","source":"### Need to Exclude Leaky Data, the same Dates are in both train and test set.\n1. First convert object to python datetime type <br>\nUsing pd.to_datetime() <br>\nCheck Getting converted to float, because haven't converted to date before comparison, still object."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Date = pd.to_datetime(df_train['Date'])\nprint(df_train['Date'].max())\nprint(df_test['Date'].min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Truncate df_train by date < df_test['Date'].min()"},{"metadata":{"trusted":true},"cell_type":"code","source":"date_filter = df_train['Date'] < df_test['Date'].min()\ndf_train = df_train.loc[date_filter]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping ID and getting rid of Province/State with NULL values \ndf_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets get Cumulative sum of ConfirmedCases and Fatalities for each country on each data (same as original data)\n# Doing to create copy without ID and \n\ntrain_country_date = df_train.groupby(['Country/Region', 'Date', 'Lat', 'Long'], as_index=False)['ConfirmedCases', 'Fatalities'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_country_date.info())\nprint(train_country_date.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering\nSplitting Date into day, month, day of week. <br>\nCheck if Date is in python datetime format. Else, convert object to python datetime type <br>\nUsing pd.to_datetime()"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_country_date.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using Pandas Series.dt.month\nThe month as January=1, December=12."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding day, month, day of week columns \n\ntrain_country_date['Month'] = train_country_date['Date'].dt.month\ntrain_country_date['Day'] = train_country_date['Date'].dt.day\ntrain_country_date['Day_Week'] = train_country_date['Date'].dt.dayofweek\ntrain_country_date['quarter'] = train_country_date['Date'].dt.quarter\ntrain_country_date['dayofyear'] = train_country_date['Date'].dt.dayofyear\ntrain_country_date['weekofyear'] = train_country_date['Date'].dt.weekofyear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_country_date.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_country_date.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Same Feature Engineering for Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First drop Province/State\ndf_test.drop('Province/State', axis = 1, inplace = True)\n\n# Converting Date Object to Datetime type\n\ndf_test.Date = pd.to_datetime(df_test['Date'])\ndf_test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding Month, DAy, Day_week columns Using Pandas Series.dt.month\n\ndf_test['Month'] = df_test['Date'].dt.month\ndf_test['Day'] = df_test['Date'].dt.day\ndf_test['Day_Week'] = df_test['Date'].dt.dayofweek\ndf_test['quarter'] = df_test['Date'].dt.quarter\ndf_test['dayofyear'] = df_test['Date'].dt.dayofyear\ndf_test['weekofyear'] = df_test['Date'].dt.weekofyear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Councatenating Train-Test to Label encode Country/Region Categorical Variable.\n1. Make copy of train data without Confirmed Cases and Fatalities. Index - 0 to 17608\n2. Concatenate train, test.\n3. Label Encode Countries.\n4. Add back Cofirmed Cases, Fatalities columns to clean_train_data.\n5. Modelling\n6. Saving Predicted Values with ForecastID"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_country_date\n# df_test\n# Lets select the Common Labels and concatenate.\n\nlabels = ['Country/Region', 'Lat', 'Long', 'Date', 'Month', 'Day', 'Day_Week','quarter', 'dayofyear', 'weekofyear']\n\ndf_train_clean = train_country_date[labels]\ndf_test_clean = df_test[labels]\n\ndata_clean = pd.concat([df_train_clean, df_test_clean], axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing Data For Models - LabelEncode Country"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoder for Countries \n\nenc = LabelEncoder()\ndata_clean['Country'] = enc.fit_transform(data_clean['Country/Region'])\ndata_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping Country/Region and Date\n\ndata_clean.drop(['Country/Region', 'Date'], axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting Back into Train and Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"index_split = df_train.shape[0]\ndata_train_clean = data_clean[:index_split]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test_clean = data_clean[index_split:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding Back Confirmed Cases and Fatalities\nUsing original df_train, check shape is same, head, tail have same values. ORDER NEEDS TO BE SAME."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_clean.tail(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Features and Two Labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data_train_clean[['Lat', 'Long', 'Month', 'Day', 'Day_Week','quarter', 'dayofyear', 'weekofyear', 'Country']]\ny_case = df_train['ConfirmedCases']\ny_fatal = df_train['Fatalities']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train-Test Split - Confirmed Cases"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y_case, test_size = 0.3, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train-Test Split - Fatalities"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train_fatal, x_test_fatal, y_train_fatal, y_test_fatal = train_test_split(x, y_fatal, test_size = 0.3, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling - Regression Problem \nUsing features Country/Region, Lat, Long, Month, Day, Day_week, quarter, dayofyear, weekofyear.<br>\nTo predict ConfirmedCases, Fatalities.\n### To predict 2 Different Target Variables, Train two classifiers, one for each."},{"metadata":{},"cell_type":"markdown","source":"# Modelling\n1. Linear Regression - Worse than baseline model. \n2. Logistic Regression (Will need GridSearchCV for Max_iter) - Absolute Trash.\n3. Polynomial Regression - Not Tried\n4. SVM Regressor - Very bad performance with a poly kernel and some variation of c and eta. (read up more)\n4. RandomForest Regressor - Gives 1.7 RMSE, With data leak removed gives - 2.18417 RMSE.\n5. GradientBoost Regressor - Gives slightly lower performance than RF"},{"metadata":{},"cell_type":"markdown","source":"## 3. RandomForest Regressor\n<p> With Leaky Data - Train MSE 284698.84113318456 <br>\nSubmission RMSLE - 1.70407 </p>\n<p> Without Leaky Data - Test MSE 291078.15156607644 <br>\n    Submission RMSLE - 2.18417 </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### For ConfirmedCases"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators =100)\nrf.fit(x_train, y_train.values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.score(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicted Values and MSE\ny_pred_train = rf.predict(x_train)\nprint(mean_squared_error(y_train, y_pred_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training on entire set and predict values.\n\nrf.fit(x, y_case.values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicted ConfirmedCases\nrf_pred_case = rf.predict(data_test_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.plot(rf_pred_case)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### For Fatalities"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(x, y_fatal.values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pred_fatal = rf.predict(data_test_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(rf_pred_fatal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving to Submission.csv\n\n#submission = pd.read_csv(path_sbumit)\n#submission['ConfirmedCases'] = rf_pred_case\n#submission['Fatalities'] = rf_pred_fatal\n\n#submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. XGBoost Regressor\n<p> With Leaky Data - Train MSE <br>\nSubmission RMSLE -  </p>\n<p> Without Leaky Data - Train MSE 10064.67200159855, 4.047602533022124 <br>\n    Submission RMSLE - 2.27873 </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = xgb.XGBRegressor(n_estimators=1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### For ConfirmedCases"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_y_pred = reg.predict(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_train, reg_y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Slightly Better than Random Forest \nreg_y_test_pred = reg.predict(x_test)\nmean_squared_error(y_test, reg_y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualising predictions error on entire train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.fit(x, y_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = reg.predict(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_case, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_pred_case = reg.predict(data_test_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(xgb_pred_case)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### For Fatalities"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.fit(x, y_fatal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking MSE for Fatalities\n\nprint(mean_squared_error(y_fatal, reg.predict(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(reg.predict(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y_fatal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Predict on Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_pred_fatal = reg.predict(data_test_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(xgb_pred_fatal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving to Submission.csv\n\nsubmission = pd.read_csv(submit_kaggle)\nsubmission['ConfirmedCases'] = xgb_pred_case\nsubmission['Fatalities'] = xgb_pred_fatal\n\nsubmission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}