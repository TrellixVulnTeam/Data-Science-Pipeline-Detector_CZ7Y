{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport matplotlib.pyplot as plt \nimport matplotlib.colors as mcolors\nimport pandas as pd \nimport random\nimport math\nimport time\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport datetime\nplt.style.use('seaborn')\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-1/train.csv\", header=0, infer_datetime_format=True, parse_dates=['Date'])\ntest_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-1/test.csv\", header=0, infer_datetime_format=True, parse_dates=['Date'])\nsubmission_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-1/submission.csv\")\ntrain_df.set_index(keys=['Date'], drop=False,inplace=True)\ntest_df.set_index(keys=['Date'], drop=False,inplace=True)\n# print(train_df.head())\nprint(train_df.tail())\nprint(test_df.tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.rename(columns = {\"Country/Region\":\"Country\"},inplace=True)\ntest_df.rename(columns = {\"Country/Region\":\"Country\"},inplace=True)\nlen(train_df.Country.unique())\ntype(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\n\ndef to_integer(dt_time):\n    date_string = dt.datetime.strptime(str(dt_time), '%Y-%m-%d %H:%M:%S')\n    d = dt.datetime.strftime(date_string, '%Y%m%d')\n    return int(d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"Date1\"] = train_df[\"Date\"].apply(lambda x: to_integer(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df['Country/Region'].astype(str)\n# clean_train_df = train_df.drop(\"Province/State\", axis=1,inplace= True)\n# clean_test_df = test_df.drop(\"Province/State\", axis=1,inplace= True)\nprint(train_df)\nprint(test_df.dtypes)\n# print(clean_test_df.dtypes)\n# print(submission_df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #since the dataset contain null values also \n# #count total rows in each column which contain null values\n# print(clean_train_df.isna().sum())\n# print(clean_test_df.isna().sum())\n# print(submission_df.isna().sum())\ntrain = train_df[1:-328]\ntest = train_df[-328:]\nprint(train.shape,test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # sort the dataframe\n# # train_df.sort_values(by='Country', axis=1, inplace=True)\n# # set the index to be this and don't drop\n# # train_df.set_index(keys=['Country'], drop=False,inplace=True)\n# # get a list of names\n# # names=train_df['Country'].unique().tolist()\n# # now we can perform a lookup on a 'view' of the dataframe\n# # k=0\n# # for i in names:\n# #     j=i\n# #     j=train_df.loc[train_df.Country == i]\n# #     print(i)\n# # #     j.drop(\"Country\",axis = 1,inplace=True)\n# #     print(j.shape)\ndef dataPrep(train_df):\n#     col = [\"Date\", \"Lat\", \"Long\", \"ConfirmedCases\", \"Fatalities\"]\n#     inputs = pd.DataFrame(columns = col)\n#     cols = [\"ConfirmedCases\", \"Fatalities\"]\n#     targets = pd.DataFrame(columns = cols)\n#     for u in range(train_df.shape[0]):\n#         inputs.append(train_df[[\"Date\", \"Lat\", \"Long\", \"ConfirmedCases\", \"Fatalities\"]])\n#         targets.append(train_df[[\"ConfirmedCases\", \"Fatalities\"]])\n    from numpy import array\n    inputs = []\n    targets = []\n    for u in range(train_df.shape[0]-8):\n        if(train_df.iloc[u][\"Country\"]==train_df.iloc[u+7][\"Country\"]):\n            inputs.append(np.array(train_df.iloc[u:u+7][[\"Date1\", \"Lat\", \"Long\", \"ConfirmedCases\", \"Fatalities\"]]).tolist())\n            targets.append(np.array(train_df.iloc[u+7][[\"ConfirmedCases\", \"Fatalities\"]]).tolist())\n    return inputs,targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from numpy import array\n# x_input = array([[60, 65, 125], [70, 75, 145], [80, 85, 165]])\n# np.vstack((x_input,[23, 32, 22]))\n# x_input = x_input.reshape((1, 3, 3))\n# x_input.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_inputs,train_targets = dataPrep(train)\ntest_inputs,test_targets = dataPrep(test)\n# train_inputs.reshape(1,3,3)\n# train_inputs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_inputs[0:10],test_targets[0])\nprint(type(train_inputs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nfrom keras import optimizers\n\n\nhidden_layers = 32\noutput_nodes = 2\ninput_nodes = 7\nbatch_size = 32\nepochs = 70\nverbose = 0\nlr = 0.001\nn_features = 5\nprint(n_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = Sequential()\n# model.add(LSTM(hidden_layers,activation='tanh', batch_input_shape=[None, 7, input_nodes], return_sequences=True))\n# model.add(LSTM(hidden_layers))\n# model.add(Dense(output_nodes, activation=\"relu\"))\n# model.add(Dense(output_nodes))\n# model.compile(optimizer='adam', loss='mse')\n\n# #Naive LSTM\n# import tensorflow as tf\n# model = tf.keras.Sequential()\n# model.add(tf.keras.layers.LSTM(hidden_layers, batch_input_shape=[None, 7, input_nodes], return_sequences=True))\n# model.add(tf.keras.layers.LSTM(hidden_layers))\n# model.add(tf.keras.layers.Dense(output_nodes, activation=\"sigmoid\"))\n\n# optimizer = tf.keras.optimizers.Adam(lr=lr)\n# model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n\n#Encode-Decoder\nimport tensorflow as tf\n# model = tf.keras.Sequential()\n# model.add(tf.keras.layers.LSTM(hidden_layers, batch_input_shape=[None, 7, input_nodes], return_sequences=True))\n# model.add(tf.keras.layers.LSTM(hidden_layers))\n# model.add(tf.keras.layers.Dense(output_nodes, activation=\"sigmoid\"))\n# optimizer = tf.keras.optimizers.Adam(lr=lr)\n# model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.LSTM(100, activation='relu', return_sequences=True, input_shape=(input_nodes, n_features)))\n# model.add(tf.keras.layers.RepeatVector(2))\nmodel.add(tf.keras.layers.LSTM(100, activation='sigmoid'))\nmodel.add(tf.keras.layers.Dense(output_nodes))\nmodel.compile(optimizer='adam', loss='mse')\n\n# model.add(tf.keras.layers.LSTM(200, activation='relu', input_shape=(input_nodes, n_features)))\n# model.add(tf.keras.layers.RepeatVector(output_nodes))\n# model.add(tf.keras.layers.LSTM(200, activation='relu', return_sequences=True))\n# model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features)))\n# model.compile(optimizer='adam', loss='mse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', mode='auto', patience=5)\nmodel1 = model.fit(train_inputs, train_targets,\n                    batch_size=batch_size,\n                      epochs=epochs,\n                   validation_data=(test_inputs, test_targets),\n                      callbacks = [early_stopping]\n                      )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"Date1\"] = test_df[\"Date\"].apply(lambda x: to_integer(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\ninput_nodes = 5\nfor i in range(30):\n    X = np.array(train_df[[\"Date1\", \"Lat\", \"Long\", \"ConfirmedCases\", \"Fatalities\"]])[-7:]\n    X = X.reshape(7, input_nodes)\n    print(X)\n    if test_df[\"Date\"].min().timestamp() in train_df[\"Date\"].values.tolist():\n        result = np.array(train_df[train_df[\"Date\"] == test_df[\"Date\"].min()][[\"Date\", \"Lat\", \"Long\", \"ConfirmedCases\", \"Fatalities\"]])[0, 3:]\n    else:\n        result = model.predict(np.array(X).reshape(1, 7, input_nodes)).reshape(-1)\n        X = np.concatenate((X[1:], np.append(X[-1, :3], result).reshape(1, input_nodes)), axis=0)\n#     print(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df = train_df\nmaxlen = 7\ninput_number = input_nodes\ncases_max = train_df[\"ConfirmedCases\"].max()\nfatal_max = train_df[\"Fatalities\"].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_df.shape)\nprint(submission_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nfor idx in test_df.groupby([\"Province/State\", \"Country\"]).count().index:\n    test_df_on_idx = test_df[(test_df[\"Province/State\"] == idx[0]) &\n                             (test_df[\"Country\"] == idx[1])]\n#     print(test_df_on_idx)\n    train_df_on_idx = train_df[(all_df[\"Country\"] == idx[1]) &\n                               (all_df[\"Province/State\"] == idx[0])]\n    inputs = np.array(train_df_on_idx[[\"Date1\", \"Lat\", \"Long\", \"ConfirmedCases\", \"Fatalities\"]])[-maxlen:]\n#     print(inputs)\n    inputs = inputs.reshape(maxlen, input_number)\n    for day in range(94):\n        if int(1000000000*(datetime.timedelta(days=day) + test_df_on_idx[\"Date\"].min()).timestamp()) in train_df_on_idx[\"Date\"].values.tolist():\n            result = np.array(train_df_on_idx[train_df_on_idx[\"Date\"] == (datetime.timedelta(days=day) + test_df_on_idx[\"Date\"].min())][[\"Date\", \"Lat\", \"Long\", \"ConfirmedCases\", \"Fatalities\"]])[0, 3:]\n        else:\n            result = model.predict(np.array(inputs).reshape(1, maxlen, input_number)).reshape(-1)\n            inputs = np.concatenate((inputs[1:], np.append(inputs[-1, :3], result).reshape(1, input_number)), axis=0)\n        results.append([(result[0]), result[1]])\n#         print(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(train_df.iloc[0]['Lat'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cases = []\nfatals = []\nfor i in range(len(results)):\n    n = results[i][0] \n    f = results[i][1]\n    try:\n        cases.append(int(n))\n    except:\n        cases.append(0)\n    \n    try:\n        fatals.append(int(f))\n    except:\n        fatals.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df[\"ConfirmedCases\"] = cases[:-8]\nsubmission_df[\"Fatalities\"] = fatals[:-8]\nsubmission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}