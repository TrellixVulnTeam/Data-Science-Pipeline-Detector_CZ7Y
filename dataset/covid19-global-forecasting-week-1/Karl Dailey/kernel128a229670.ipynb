{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"###############################################################################\n## Imports\n###############################################################################\n\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom google.cloud import bigquery\nfrom sklearn.model_selection import KFold\nfrom scipy.spatial.distance import cdist\nfrom sklearn.metrics import mean_absolute_error\n\n###############################################################################\n## Functions\n###############################################################################\ndef create_time_features(df):\n    \"\"\"\n    Creates time series features from datetime index\n    \"\"\"\n    df['date'] = df.index\n    df['hour'] = df['date'].dt.hour\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    df['weekofyear'] = df['date'].dt.weekofyear\n    \n    X = df[['hour','dayofweek','quarter','month','year',\n           'dayofyear','dayofmonth','weekofyear']]\n    return X\n\ndef get_wg( country = 'US', state   = 'New York' ):\n    booll   = (train['Country/Region']==country) & (train['Province/State']==state) & (train['ConfirmedCases']>0)\n    narf = train[ booll ].copy().reset_index()\n    narf['ts'] = narf.index+1\n    \n    if narf.shape[0] == 0:\n        return\n    \n    r  = 8.171\n    a  = 4449661.60968839\n    c  = 4.83006577124696\n    hc = .998\n    \n    r,a,c,hc = get_params(booll\n                          , [r*.75, 8.171, r*1.25]\n                          , [a*.75, 4449661.60968839, a*1.25]\n                          , [c*.75, 4.83006577124696, c*1.25]\n                          , [hc*.75, .998, .999])\n    \n    narf['pt']  = narf.ts.apply(lambda x: (1-(a/(a+x**c))**r)*(1-hc))\n    \n    temp = narf.copy()\n    temp['ts'] = temp.index\n    temp = temp[['ts','ConfirmedCases','pt']]\n    temp.columns = ['ts','incr', 'ptr']\n    \n    if temp.shape[0] == 0:\n        return\n    \n    narf = pd.merge(narf, temp, on='ts', how='left')\n    narf['incr'] = narf['incr'] - narf['ConfirmedCases']\n    narf['ptr']  = narf['ptr'] - narf['pt']\n    \n    index = narf.shape[0] - 1\n    pop   = narf.iloc[index]['pop']\n    \n    if np.isnan(pop):\n        return\n    \n    cc    = narf.iloc[index]['ConfirmedCases']\n    dfj   = narf.iloc[0]['day_from_jan_first']\n    pt    = (pop-cc) * np.log(1-narf.iloc[index]['pt'])\n    \n    narf['LL'] = narf.apply(lambda x: np.log(x.ptr) * x.incr, axis=1)\n    \n    narf.loc[index,'LL'] = pt\n    \n    LL = np.sum( narf.LL ) \n    \n    bool2 = (test['Country/Region']==country) & (test['Province/State']==state)\n    narft = test[bool2].copy().reset_index()\n    narft['ts'] = narft.day_from_jan_first - dfj\n    \n    narft['pt'] = narft.ts.apply(lambda x: (1-(a/(a+x**c))**r)*(1-hc))\n    narft['wg'] = narft.pt * pop\n    \n    test.loc[bool2, 'wg'] = narft.wg.values\n\nprint('done')\n\n\ndef get_params(booll, rl = [1], al = [1], cl = [1], hcl = [.5]):\n    r_s,a_s,c_s,hc_s = rl[0],al[0],cl[0], hcl[0]\n    LL       = float('-inf')\n    for r in rl:\n        for a in al:\n            for c in cl:\n                for hc in hcl:\n                    narf = train[ booll ].copy().reset_index()\n                    narf['ts'] = narf.index+1\n                    \n                    if narf.shape[0] == 0:\n                        return r_s, a_s, c_s, hc_s\n                    \n                    narf['pt']  = narf.ts.apply(lambda x: (1-(a/(a+x**c))**r)*(1-hc))\n                    \n                    temp = narf.copy()\n                    temp['ts'] = temp.index\n                    temp = temp[['ts','ConfirmedCases','pt']]\n                    temp.columns = ['ts','incr', 'ptr']\n                    \n                    if temp.shape[0] == 0:\n                        return r_s, a_s, c_s, hc_s\n                    \n                    narf = pd.merge(narf, temp, on='ts', how='left')\n                    narf['incr'] = narf['incr'] - narf['ConfirmedCases']\n                    narf['ptr']  = narf['ptr'] - narf['pt']\n                    \n                    index = narf.shape[0] - 1\n                    pop   = narf.iloc[index]['pop']\n                    \n                    if np.isnan(pop):\n                        return r_s, a_s, c_s, hc_s\n                    \n                    cc    = narf.iloc[index]['ConfirmedCases']\n                    dfj   = narf.iloc[0]['day_from_jan_first']\n                    pt    = (pop-cc) * np.log(1-narf.iloc[index]['pt'])\n                    \n                    narf['LL'] = narf.apply(lambda x: np.log(x.ptr) * x.incr, axis=1)\n                    \n                    narf.loc[index,'LL'] = pt\n                    \n                    ll = np.sum( narf.LL )\n                    \n                    if ll > LL:\n                        r_s, a_s, c_s, hc_s = r, a, c, hc\n                        LL = ll\n        return r_s, a_s, c_s, hc_s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n## Read Data\n###############################################################################\n\nPATH = '/kaggle/input/covid19-global-forecasting-week-1/'\ntrain  = pd.read_csv(PATH + 'train.csv')\ntest  = pd.read_csv(PATH + 'test.csv')\n\ndfp  = pd.read_csv('/kaggle/input/population/' + 'population.csv')\n\nprint(train.shape, dfp.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['Province/State'].isnull(), 'Province/State'] = 'NARF'\ntest.loc[test['Province/State'].isnull(), 'Province/State']   = 'NARF'\n\nn = train.shape[0]\ntrain = pd.merge(train, dfp, on=['Country/Region','Province/State'], how='left')\nassert train.shape[0] == n\n\nn = test.shape[0]\ntest = pd.merge(test, dfp, on=['Country/Region','Province/State'], how='left')\nassert test.shape[0] == n\n\ntrain.loc[train['pop'].isnull(),'pop'] = 0\ntest.loc[test['pop'].isnull(),'pop'] = 0\n\nmo = train['Date'].apply(lambda x: x[5:7])\nda = train['Date'].apply(lambda x: x[8:10])\ntrain['day_from_jan_first'] = (da.apply(int)\n                               + 31*(mo=='02') \n                               + 60*(mo=='03')\n                               + 91*(mo=='04')  \n                              )\n\nmo = test['Date'].apply(lambda x: x[5:7])\nda = test['Date'].apply(lambda x: x[8:10])\ntest['day_from_jan_first'] = (da.apply(int)\n                               + 31*(mo=='02') \n                               + 60*(mo=='03')\n                               + 91*(mo=='04')  \n                              )\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['wg'] = np.NaN\n\nfor country in test['Country/Region'].unique():\n    booll = test['Country/Region']==country\n    for state in test[booll]['Province/State'].unique():\n        print( country, state)\n        get_wg(country, state)\n\nprint('done', train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"###############################################################################\n## Weather Data\n###############################################################################\n\nclient = bigquery.Client()\ndataset_ref = client.dataset(\"noaa_gsod\", project=\"bigquery-public-data\")\ndataset = client.get_dataset(dataset_ref)\n\ntables = list(client.list_tables(dataset))\n\ntable_ref = dataset_ref.table(\"stations\")\ntable = client.get_table(table_ref)\nstations_df = client.list_rows(table).to_dataframe()\n\ntable_ref = dataset_ref.table(\"gsod2020\")\ntable = client.get_table(table_ref)\ntwenty_twenty_df = client.list_rows(table).to_dataframe()\n\nstations_df['STN'] = stations_df['usaf'] + '-' + stations_df['wban']\ntwenty_twenty_df['STN'] = twenty_twenty_df['stn'] + '-' + twenty_twenty_df['wban']\n\ncols_1 = ['STN', 'mo', 'da', 'temp', 'min', 'max', 'stp', 'wdsp', 'prcp', 'fog']\ncols_2 = ['STN', 'country', 'state', 'call', 'lat', 'lon', 'elev']\nweather_df = twenty_twenty_df[cols_1].join(stations_df[cols_2].set_index('STN'), on='STN')\n\nweather_df.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n## Join df w/ Weather Data\n###############################################################################\nweather_df['day_from_jan_first'] = (weather_df['da'].apply(int)\n                                   + 31*(weather_df['mo']=='02') \n                                   + 60*(weather_df['mo']=='03')\n                                   + 91*(weather_df['mo']=='04')  \n                                   )\n\nC = []\nfor j in train.index:\n    df = train.iloc[j:(j+1)]\n    mat = cdist(df[['Lat','Long', 'day_from_jan_first']],\n                weather_df[['lat','lon', 'day_from_jan_first']], \n                metric='euclidean')\n    new_df = pd.DataFrame(mat, index=df.Id, columns=weather_df.index)\n    arr = new_df.values\n    new_close = np.where(arr == np.nanmin(arr, axis=1)[:,None],new_df.columns,False)\n    L = [i[i.astype(bool)].tolist()[0] for i in new_close]\n    C.append(L[0])\n    \ntrain['closest_station'] = C\n\ntrain = train.set_index('closest_station').join(weather_df[['temp', 'min', 'max', 'stp', 'wdsp', 'prcp', 'fog']], ).reset_index().drop(['index'], axis=1)\ntrain.sort_values(by=['Id'], inplace=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n## Join dft w/ Weather Data\n###############################################################################\n\nweather_df['day_from_jan_first'] = (weather_df['da'].apply(int)\n                                   + 31*(weather_df['mo']=='02') \n                                   + 60*(weather_df['mo']=='03')\n                                   + 91*(weather_df['mo']=='04')  \n                                   )\n\nC = []\nfor j in test.index:\n    df = test.iloc[j:(j+1)]\n    mat = cdist(df[['Lat','Long', 'day_from_jan_first']],\n                weather_df[['lat','lon', 'day_from_jan_first']], \n                metric='euclidean')\n    new_df = pd.DataFrame(mat, index=df.ForecastId, columns=weather_df.index)\n    arr = new_df.values\n    new_close = np.where(arr == np.nanmin(arr, axis=1)[:,None],new_df.columns,False)\n    L = [i[i.astype(bool)].tolist()[0] for i in new_close]\n    C.append(L[0])\n    \ntest['closest_station'] = C\n\ntest = test.set_index('closest_station').join(weather_df[['temp', 'min', 'max', 'stp', 'wdsp', 'prcp', 'fog']], ).reset_index().drop(['index'], axis=1)\ntest.sort_values(by=['ForecastId'], inplace=True)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n## Cleaning\n###############################################################################\ntrain[\"wdsp\"] = pd.to_numeric(train[\"wdsp\"])\ntest[\"wdsp\"] = pd.to_numeric(test[\"wdsp\"])\ntrain[\"fog\"] = pd.to_numeric(train[\"fog\"])\ntest[\"fog\"] = pd.to_numeric(test[\"fog\"])\n\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['Province/State']=='NARF', 'Province/State'] = np.NaN\ntest.loc[  test['Province/State']=='NARF', 'Province/State'] = np.NaN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop([\"ConfirmedCases\"], axis=1)\ncountries = X_train[\"Country/Region\"]\n\nX_train = X_train.drop([\"Id\"], axis=1)\nX_test = test.drop([\"ForecastId\"], axis=1)\n\n# Change the Date column to be a datetime\nX_train['Date']= pd.to_datetime(X_train['Date']) \nX_test['Date']= pd.to_datetime(X_test['Date']) \n\n#Set the index to the date\nX_train = X_train.set_index(['Date'])\nX_test = X_test.set_index(['Date'])\n\n#Create time features\ncreate_time_features(X_train)\ncreate_time_features(X_test)\n\nX_train.drop(\"date\", axis=1, inplace=True)\nX_test.drop(\"date\", axis=1, inplace=True)\nprint(X_train.shape, X_test.shape)\n\n\nX_train.drop([\"Fatalities\"], axis=1, inplace=True)\n\nprint('done', X_train.shape)\n\"\"\"\ndfw = X_train[[ 'Country/Region', 'Lat', 'Long','dayofyear','Fatalities']].copy()\ndfw.columns = ['Country/Region', 'Lat', 'Long','first_death','Fatalities']\ndfw = dfw[dfw.Fatalities>0]\ndfw.drop('Fatalities', inplace=True, axis=1)\ndfw.reset_index(drop=True, inplace=True)\ndfw.drop_duplicates(subset=['Country/Region', 'Lat', 'Long'], inplace=True)\n\n\nX_train = pd.merge( X_train, dfw, on=['Country/Region', 'Lat', 'Long'], how='left')\n\nX_train['first_death'] = X_train.dayofyear - X_train.first_death\n\nX_train.loc[X_train.dayofyear<X_train.first_death, 'first_death'] = np.NaN\nX_test  = pd.merge( X_test,  dfw, on=['Country/Region', 'Lat', 'Long'], how='left')\n\"\"\"\nprint('done', X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One hot encode the Provice/State and the Country/Region columns\nX_train = pd.concat([X_train,pd.get_dummies(X_train['Province/State'], prefix='ps')],axis=1)\nX_train.drop(['Province/State'],axis=1, inplace=True)\nX_test = pd.concat([X_test,pd.get_dummies(X_test['Province/State'], prefix='ps')],axis=1)\nX_test.drop(['Province/State'],axis=1, inplace=True)\n\nX_train = pd.concat([X_train,pd.get_dummies(X_train['Country/Region'], prefix='cr')],axis=1)\nX_train.drop(['Country/Region'],axis=1, inplace=True)\nX_test = pd.concat([X_test,pd.get_dummies(X_test['Country/Region'], prefix='cr')],axis=1)\nX_test.drop(['Country/Region'],axis=1, inplace=True)\n\nprint(X_train.shape, X_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n## Modeling\n###############################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train    = train[\"Fatalities\"]\ny_train_cc = train[\"ConfirmedCases\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_TRAIN = X_train.values\n\nparams_xgb = {}\nparams_xgb['n_estimators']       = 1100\nparams_xgb['max_depth']          = 9\nparams_xgb['seed']               = 2020\nparams_xgb['colsample_bylevel']  = 1\nparams_xgb['colsample_bytree']   = 1\nparams_xgb['learning_rate']      = 0.300000012\nparams_xgb['reg_alpha']          = 0\nparams_xgb['reg_lambda']         = 1\nparams_xgb['subsample']          = 1\n\nisTraining = False\n\nif isTraining:\n    kf      = KFold(n_splits = 5, shuffle = True, random_state=2020)\n    acc     = []\n\n    for tr_idx, val_idx in kf.split(X_TRAIN, y_train_cc):\n        ## Set up XY train/validation\n        X_tr, X_vl = X_TRAIN[tr_idx], X_TRAIN[val_idx, :]\n        y_tr, y_vl = y_train_cc[tr_idx], y_train_cc[val_idx]\n        print(X_tr.shape)\n\n        model_xgb_cc = xgb.XGBRegressor(**params_xgb)\n        model_xgb_cc.fit(X_tr, y_tr, verbose=True)\n        y_hat = model_xgb_cc.predict(X_vl)\n\n        print('xgb mae :', mean_absolute_error(  y_vl, y_hat) )\n        acc.append(mean_absolute_error( y_vl, y_hat) )\n\n\n    print('done', np.mean(acc))# Best run: 168.26412715647604 #30.2019242771957\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" print('done', np.mean(acc))# Best run: 168.26412715647604 #30.2019242771957","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fit fatalities\nparams_xgb = {}\nparams_xgb['n_estimators']       = 1100\nparams_xgb['max_depth']          = 9\nparams_xgb['seed']               = 2020\nparams_xgb['colsample_bylevel']  = 1\nparams_xgb['colsample_bytree']   = 1\nparams_xgb['learning_rate']      = 0.300000012\nparams_xgb['reg_alpha']          = 0\nparams_xgb['reg_lambda']         = 1\nparams_xgb['subsample']          = 1\n\nmodel_xgb_f = xgb.XGBRegressor(**params_xgb)\nmodel_xgb_f.fit(X_train, y_train, verbose=True)\n\ny_hat_xgb_f = model_xgb_f.predict(X_test.drop('wg',axis=1))\nprint(np.mean(y_hat_xgb_f))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fit confirmed cases\nparams_xgb = {}\nparams_xgb['n_estimators']       = 1100\nparams_xgb['max_depth']          = 9\nparams_xgb['seed']               = 2020\nparams_xgb['colsample_bylevel']  = 1\nparams_xgb['colsample_bytree']   = 1\nparams_xgb['learning_rate']      = 0.300000012\nparams_xgb['reg_alpha']          = 0\nparams_xgb['reg_lambda']         = 1\nparams_xgb['subsample']          = 1\n\nmodel_xgb_cc = xgb.XGBRegressor(**params_xgb)\nmodel_xgb_cc.fit(X_train, y_train_cc, verbose=True)\n\ny_hat_xgb_cc = model_xgb_cc.predict(X_test.drop('wg',axis=1))\n\nprint(np.mean(y_hat_xgb_cc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n## Feature Importantce\n###############################################################################\n\nplot = plot_importance(model_xgb_cc, height=0.9, max_num_features=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test.wg.isnull()].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['y_hat_cc']                = y_hat_xgb_cc\ntest.loc[test.wg.isnull(),'wg'] = test[test.wg.isnull()].y_hat_cc\ntest['y_hat_ens']   = .75 * test.y_hat_cc + .25 * test.wg\n\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['y_hat_ens'] = test.y_hat_ens.astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test[test.wg.isnull()].shape, np.mean(test.wg), np.mean(y_hat_xgb_cc), np.mean(test.y_hat_ens))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n## Submision\n###############################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissionOrig = pd.read_csv(\"../input/covid19-global-forecasting-week-1/submission.csv\")\nsubmissionOrig[\"ConfirmedCases\"]= pd.Series( test.y_hat_ens)#pd.Series(y_hat_xgb_cc)\nsubmissionOrig[\"Fatalities\"]    = pd.Series(y_hat_xgb_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissionOrig.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}