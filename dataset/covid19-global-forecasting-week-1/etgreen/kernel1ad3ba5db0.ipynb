{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '/kaggle/input/covid19-global-forecasting-week-1'\n\npd.read_csv(f'{data_dir}/train.csv', parse_dates=['Date']).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"COLUMN_NAME_MAP = {\n    'Id': 'id',\n    'ForecastId': 'id',\n    'Province/State': 'province',\n    'Country/Region': 'country',\n    'Lat': 'lat',\n    'Long': 'long',\n    'Date': 'date',\n    'ConfirmedCases': 'cases',\n    'Fatalities': 'deaths'\n}\n\ndef get_data(fn):\n    \"\"\"\n    Consistent way to load and format both CSVs.\n    \"\"\"     \n    df = (\n        pd\n        .read_csv(f'{data_dir}/{fn}.csv', parse_dates=['Date'])\n        .rename(columns=COLUMN_NAME_MAP)\n        .set_index('id')\n        .replace(pd.np.nan, '')  # otherwise groupby ignores province = NaN\n    )\n    \n    df['day'] = df.date.dt.dayofyear\n    df['day'] -= 22 # first day in training set\n    \n    # convert these columns if training data\n    if 'deaths' in df:\n        df = df.astype({'deaths': int, 'cases': int})\n    \n    return df\n    \n\ntrain_df = get_data('train')\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cumulative_field_diff = (\n    train_df\n    .groupby(['country', 'province'])[['cases', 'deaths']]\n    .diff()\n    .fillna(0)\n)\n\nbad_rows = (cumulative_field_diff < 0).any(axis=1)\nf'There are {bad_rows.sum()} bad rows where the cumulative number of cases or deaths decreases.'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.loc[~bad_rows]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aggregate across province to get a fairer look at each country"},{"metadata":{"trusted":true},"cell_type":"code","source":"countries_train_df = train_df.groupby(['country', 'date', 'day'])['cases', 'deaths'].sum().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_cases_df = countries_train_df.groupby('country')['cases', 'deaths'].max().sort_values('cases', ascending=False)\n\nmost_cases_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"n = 10\nn_most_cases = most_cases_df.index.to_list()[:n]\ng = sns.lineplot(x='date', y='cases', hue='country', data=countries_train_df.query('country in @n_most_cases'), estimator=None)\ng.xaxis.set_major_locator(plt.MaxNLocator(10))\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.lineplot(x='date', y='deaths', hue='country', data=countries_train_df.query('country in @n_most_cases'), estimator=None)\ng.xaxis.set_major_locator(plt.MaxNLocator(10))\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions"},{"metadata":{},"cell_type":"markdown","source":"From the look of the China deaths/cases, a logistic regression model might be a reasonable choice.\nLet's start by using this."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = get_data('test')\nx_predict = np.sort(test_df.day.unique())\nx_all = np.arange(x_predict.max() + 1)\nprint(f'There are {len(x_predict)} to predict.')\nprint(f'Overlap of {train_df.day.max() - test_df.day.min()} days.')\n\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConstantModel:\n    \"\"\"\n    A very simple model for when there is only one number of cases/deaths.\n    \n    Logisitic regression doesn't raises an exception in this case so use \n    this class instead.\n    \"\"\"\n    def __init__(self):\n        pass\n        \n    def fit(self, X, y, sample_weight=None):\n        self.value = y[0]\n        \n    def predict(self, X):\n        return np.array([self.value] * X.shape[0])\n    \n    def __repr__(self):\n        return f'ConstanModel(value={self.value})'\n\n\ndef run_models(grp_df, feature, target, x_to_predict, model, **model_kwargs):\n            \n    region_data = grp_df[[feature, target]].to_numpy()\n    x = region_data[:, 0].reshape(-1, 1)\n    y = region_data[:, 1]\n    \n    n_target_classes = np.unique(y).shape[0]\n    model = model(**model_kwargs) if n_target_classes > 1 else ConstantModel()\n    model.fit(x, y)\n    \n    result = {\n        feature: x_predict,\n        target: model.predict(x_to_predict.reshape(-1, 1)),\n        f'{target}_model': model\n    }\n        \n    return pd.DataFrame(result).set_index(feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression\nmodel_kwargs = {\n    'max_iter': 10_000\n}\n\ntest_predictions_df = run_models(\n    train_df.query('country == \"Spain\"'),\n    feature='day',\n    target='cases',\n    x_to_predict=x_predict,\n    model=LogisticRegression,\n    **model_kwargs\n)\n\ndisplay(test_predictions_df.head(10))\nsns.lineplot(x='day', y='cases', data=test_predictions_df.reset_index());","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"region_columns = ['country', 'province']\n\ndeath_predictions_df = (\n    train_df\n    .groupby(region_columns)\n    .apply(run_models, feature='day', target='deaths', x_to_predict=x_predict, model=model, **model_kwargs)\n    .reset_index()\n)\n\ncases_predictions_df = (\n    train_df\n    .groupby(region_columns)\n    .apply(run_models, feature='day', target='cases', x_to_predict=x_predict, model=model, **model_kwargs)\n    .reset_index()\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merge_columns = ['country', 'province', 'day']\nresult_df = (\n    test_df\n    .reset_index()\n    .merge(death_predictions_df, on=merge_columns)\n    .merge(cases_predictions_df, on=merge_columns)\n)\n\nresult_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check results"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_df = train_df.merge(\n    result_df, \n    on=merge_columns, \n    how='inner', \n    suffixes=('', '_predicted')\n)[merge_columns + ['cases', 'cases_predicted', 'deaths', 'deaths_predicted']]\n\n\nfor model_type in ('cases', 'deaths',):\n    formula = f'log({model_type}_predicted + 1) - log({model_type} + 1)'\n    formula = f'({formula})**2'\n    evaluation_df = evaluation_df.eval(f'{model_type}_rmsle = {formula}')\n    \n    \nscore_columns = ['cases_rmsle', 'deaths_rmsle']\n(evaluation_df[score_columns].mean() ** 0.5).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The worst results are shown here."},{"metadata":{"trusted":true},"cell_type":"code","source":"(\n    evaluation_df\n    .groupby(region_columns)[['cases_rmsle', 'deaths_rmsle']]\n    .sum()\n    .sort_values('cases_rmsle', ascending=False)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_province = ('Estonia', '')\nexpr = 'country == @country_province[0] and province == @country_province[1]'\nselect_train_df = train_df.query(expr)\nselect_result_df = result_df.query(expr)\n\nmodel_type = 'cases'\nsns.scatterplot(x='day', y=model_type, data=select_train_df, label='actual');\n\npredictions_all = (\n    select_result_df[f'{model_type}_model']\n    .iloc[0]\n    .predict(x_all.reshape(-1, 1))\n)\nsns.scatterplot(x_all, predictions_all, label='prediction');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not too bad. The LR curve is a bit jumpy though and not very smooth."},{"metadata":{},"cell_type":"markdown","source":"### Submit predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = (\n    result_df\n    .copy()\n    .astype({'deaths': int, 'cases': int})\n    .rename(columns={v: k for k, v in COLUMN_NAME_MAP.items()})\n    .filter(['ForecastId', 'ConfirmedCases', 'Fatalities'])\n)\n\n# check that submission is in the correct format\nexpected_submission_df = pd.read_csv(f'{data_dir}/submission.csv')\nfor col in ('ConfirmedCases', 'Fatalities',):\n    expected_submission_df[col] = submission_df[col]  \npd.testing.assert_frame_equal(submission_df, expected_submission_df)\n\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Better model\n\nThis logisitic regression model assumes that the highest point in the training data is the highest it will get to.\nThere is no way of knowing when this will be though.\nPerhaps looking at the daily rate as this must decrease slowly, not instantly."},{"metadata":{"trusted":true},"cell_type":"code","source":"for model_type in ('cases', 'deaths',):\n    train_df[f'daily_{model_type}'] = train_df.groupby(region_columns)[model_type].diff().fillna(0).astype(int)\n    train_df.query('country == \"United Kingdom\" and province == \"United Kingdom\"').plot(x='day', y=f'daily_{model_type}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These need smoothing."},{"metadata":{"trusted":true},"cell_type":"code","source":"for window in (3, 5, 10):\n    train_df[[f'daily_cases_win_{window}', f'daily_deaths_win_{window}']] = (\n        train_df[['daily_cases', 'daily_deaths']].rolling(window, min_periods=1).mean()\n    )\n    \ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model_type in ('cases', 'deaths',):\n    train_df.query('country == \"United Kingdom\" and province == \"United Kingdom\"').plot(x='day', y=f'daily_{model_type}_win_10')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These shows a nice smooth increase that is probably more realistic than the daily increase."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nmpl.rcParams['figure.figsize'] = (8, 6)\nmpl.rcParams['axes.grid'] = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def univariate_data(df, start_index, end_index, history_size, target_size):\n    \"\"\"\n    abcdef -> [abc, bcd, cde], [d, e, f]\n    for history_size = 3, target_size = 1\n    \"\"\"\n    data = []\n    labels = []\n    \n    dataset = df.to_numpy()\n    \n    start_index = start_index + history_size\n    if end_index is None:\n        end_index = len(dataset) - target_size\n    \n    if end_index <= start_index:\n        raise ValueError(f'End index {end_index} not greater than start index {start_index}.')\n\n    for i in range(start_index, end_index):\n        indices = range(i-history_size, i)\n        # Reshape data from (history_size,) to (history_size, 1)\n        data.append(dataset[indices].reshape(history_size, 1))\n        labels.append(dataset[i+target_size])\n\n    return data, labels\n\n\ndef multivariate_data(dataset, target, start_index, end_index, history_size, target_size, step=1):\n    data = []\n    labels = []\n    \n    dataset = dataset.to_numpy()\n    target = target.to_numpy()\n\n    start_index = start_index + history_size\n    if end_index is None:\n        end_index = len(dataset) - target_size\n        \n    if end_index <= start_index:\n        raise ValueError(f'End index {end_index} not greater than start index {start_index}.')\n\n    for i in range(start_index, end_index):\n        indices = range(i-history_size, i, step)\n        data.append(dataset[indices].reshape(history_size, -1))\n\n        if step > 1:\n            labels.append(target[i:i+target_size])\n        else:\n            labels.append(target[i+target_size])\n\n    return data, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = 5\nforecast_jump = 3\nfeatures = 'day'\nscale = True\n\ntrain_sample_size = test_df['day'].min()\nscaled_df = train_df.copy()\n\nif scale:\n    scaled_df[features] = (scaled_df[features] - scaled_df[features].mean()) / scaled_df[features].std()\n\nxs = []\nys = []\nregions = []\n\nfor grp_name, grp_df in scaled_df.groupby(['country', 'province']):\n    grp_x, grp_y = univariate_data(grp_df[features], 0, train_sample_size, history, forecast_jump)\n#     grp_x, grp_y = multivariate_data(\n#         grp_df[features], grp_df[features], 0, train_sample_size, history, forecast_jump\n#     )\n    xs += grp_x\n    ys += grp_y\n    regions += [grp_name] * len(grp_y)\n    break\n    \nx_train_uni = np.array(xs)\ny_train_uni = np.array(ys)\nregions_train = np.array(regions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_uni.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_uni.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Single window of past history')\nprint (x_train_uni[0])\nprint ('\\n Target temperature to predict')\nprint (y_train_uni[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_df = train_df.copy()\n\nif scale:\n    scaled_df[features] = (scaled_df[features] - scaled_df[features].mean()) / scaled_df[features].std()\n\nxs = []\nys = []\nregions = []\n\nfor grp_name, grp_df in scaled_df.groupby(['country', 'province']):\n    # swapped the 2nd and 3rd args below\n    #grp_x, grp_y = univariate_data(grp_df[features], train_sample_size, None, history, forecast_jump)\n    grp_x, grp_y = multivariate_data(\n        grp_df[features], grp_df[features], train_sample_size, None, history, forecast_jump\n    )\n    xs += grp_x\n    ys += grp_y\n    regions += [grp_name] * len(grp_y)\n    \nx_val_uni = np.array(xs)\ny_val_uni = np.array(ys)\nregions_val = np.array(regions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nprint ('Single window of past history')\nprint (x_val_uni[i])\nprint ('\\n Target temperature to predict')\nprint (y_val_uni[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_time_steps(length):\n    return list(range(-length, 0))\n\n\ndef baseline(history):\n    return np.mean(history)\n\n\ndef show_plot(plot_data, delta, title):\n    labels = ['History', 'True Future', 'Model Prediction']\n    marker = ['.-', 'rx', 'go']\n    time_steps = create_time_steps(plot_data[0].shape[0])\n    future = delta if delta else 0\n\n    plt.title(title)\n    for i, x in enumerate(plot_data):\n        if i:\n            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n        else:\n            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n    plt.legend()\n    plt.xlim([time_steps[0], (future+5)*2])\n    plt.xlabel('Time-Step')\n    \n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plot([x_train_uni[0], y_train_uni[0]], 0, 'Sample Example');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plot([x_train_uni[0], y_train_uni[0], baseline(x_train_uni[0])], 0, 'Baseline Prediction Example');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 256\nBUFFER_SIZE = 10000\n\ntrain_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\ntrain_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n\nval_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\nval_univariate = val_univariate.batch(BATCH_SIZE).repeat()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_lstm_model = tf.keras.models.Sequential([\n    tf.keras.layers.LSTM(8, input_shape=x_train_uni.shape[-2:]),\n    tf.keras.layers.Dense(1)\n])\n\nsimple_lstm_model.compile(optimizer='adam', loss='mae')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EVALUATION_INTERVAL = 200\nEPOCHS = 10\n\nsingle_step_history = simple_lstm_model.fit(train_univariate, epochs=EPOCHS,\n                      steps_per_epoch=EVALUATION_INTERVAL,\n                      validation_data=val_univariate, validation_steps=50);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_train_history(history, title):\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(len(loss))\n\n    plt.figure()\n\n    plt.plot(epochs, loss, 'b', label='Training loss')\n    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n    plt.title(title)\n    plt.legend()\n\n    plt.show()\n    \n\nplot_train_history(single_step_history, 'Single Step Training and validation loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x, y in val_univariate.take(3):\n    plot = show_plot([x[0].numpy(), y[0].numpy(),\n                    simple_lstm_model.predict(x)[0]], 0, 'Simple LSTM model')\n    plot.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}