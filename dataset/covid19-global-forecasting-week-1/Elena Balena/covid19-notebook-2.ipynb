{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\ntest_data = pd.read_csv('../input/covid19-global-forecasting-week-1/test.csv',index_col=0)\ntrain_data = pd.read_csv('../input/covid19-global-forecasting-week-1/train.csv',index_col=0)\nsub_sample = pd.read_csv('../input/covid19-global-forecasting-week-1/submission.csv')\n\n#test_data.head()\n#train_data.head()\n#sub_sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data=test_data.rename(columns={'Country/Region':'Country','Province/State':'Province'})\ntrain_data=train_data.rename(columns={'Country/Region':'Country','Province/State':'Province'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data.isnull().sum()\n#test_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#some countries don't have provinces\n#we'll create a new column that sums the country and province name and remove the province column\ntrain_data.fillna(1)\ntrain_data['Province_new'] = train_data['Country']+'_'+train_data['Province'].astype(str)\n#train_data.head()\ntest_data['Province_new'] = test_data['Country']+'_'+test_data['Province'].astype(str)\n#test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_data.groupby('Date').Date.count()\n#test_data.groupby('Country').Country.count()\n#test_data.Province.value_counts()\n#test_data.Province.describe()\n#test_data.Country.value_counts()\n#train_data.groupby('Date').Date.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_data.Date.agg([min,max])\n#train_data.Date.agg([min,max])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert categorical data in Date to actual time dates\nfrom datetime import date\ntest_data['Date'] = pd.to_datetime(test_data['Date'])\ntrain_data['Date'] = pd.to_datetime(train_data['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create day,month,year columns\ntrain_data['Day'] = train_data['Date'].dt.day\ntrain_data['Month'] = train_data['Date'].dt.month\ntrain_data['Year'] = train_data['Date'].dt.year\n\ntest_data['Day'] = test_data['Date'].dt.day\ntest_data['Month'] = test_data['Date'].dt.month\ntest_data['Year'] = test_data['Date'].dt.year\n\n#create column with how many days from the start of the measuring period\nfirst_day = train_data.Date.min()\ntrain_data['Days_since_start'] = (train_data['Date']-first_day).dt.days\ntest_data['Days_since_start'] = (test_data['Date']-first_day).dt.days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#select only the training data before 11/03/2020 so the train and test data don't intersect\ntrain_data_reduced = train_data.drop(train_data[train_data.Date>'2020-3-11'].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We're going to remove the original 'Province' column \n#and the Date column since it's not numerical and we have extracted the info in other numerical columns\ntrain_data_reduced = train_data_reduced.drop(['Province','Date'],axis=1)\ntest_data = test_data.drop(['Province','Date'],axis=1)\n\ntrain_data_reduced['Lat'] = train_data_reduced['Lat'].astype(int)\ntrain_data_reduced['Long'] = train_data_reduced['Long'].astype(int)\ntest_data['Lat'] = test_data['Lat'].astype(int)\ntest_data['Long'] = test_data['Long'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# deal with the cathegorical data\n#from sklearn.preprocessing import LabelEncoder\n#cat_features = ['Country','Province_new']\n#encoder = LabelEncoder()\n#train_data_reduced[cat_features] = train_data_reduced[cat_features].apply(encoder.fit_transform)\n#test_data[cat_features] = test_data[cat_features].apply(encoder.transform)\n#data_cols_train = train_data.drop(cat_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#list(encoder.classes_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(train_data_reduced.Country.nunique())\n#print(train_data_reduced.Province_new.nunique())\n#print(test_data.Country.nunique())\n#print(test_data.Province_new.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(train_data_reduced.Country.unique())\n#print(test_data.Country.unique())\n#they have the same values for Country and Province_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dictionary for countries and provinces\ncountry_labels = train_data_reduced['Country'].astype('category').cat.categories.tolist()\ncountry_dict = {'Country':{k:v for k,v in zip(country_labels,list(range(1,len(country_labels)+1)))}}\ntrain_data_reduced.replace(country_dict, inplace=True)\n\nprovince_labels = train_data_reduced['Province_new'].astype('category').cat.categories.tolist()\nprovince_dict = {'Province_new':{k:v for k,v in zip(province_labels,list(range(1,len(province_labels)+1)))}}\ntrain_data_reduced.replace(province_dict, inplace=True)\n\ntest_data.replace(country_dict,inplace=True)\ntest_data.replace(province_dict, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#separate target from data\n#create validation set from train data\n#from sklearn.model_selection import train_test_split\nX_ = train_data_reduced.drop(['ConfirmedCases','Fatalities'],axis=1)\ny_cases = train_data_reduced['ConfirmedCases']\ny_deaths = train_data_reduced['Fatalities']\n\n#X_train, X_valid, y_cases_train, y_cases_valid,y_deaths_train, y_deaths_valid = train_test_split(\n#X_,y_cases,y_deaths,test_size=0.2, random_state=1)  \n\n#apply a model\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nxgbreg = xgb.XGBRegressor()\n\nparameters = {'objective':['reg:linear'],\n              'learning_rate': [.05, 0.15, .3], \n              'max_depth': [5, 6, 7],\n              'min_child_weight': [1],\n              'silent': [1],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [100, 250, 500]}\n\nmodel_cases = GridSearchCV(xgbreg,\n                        parameters,\n                        cv = 2,\n                        n_jobs = 5,\n                        verbose=True)\n\nmodel_deaths = GridSearchCV(xgbreg,\n                        parameters,\n                        cv = 2,\n                        n_jobs = 5,\n                        verbose=True)\n#model_cases.fit(X_train,y_cases_train)\n#model_deaths.fit(X_train,y_deaths_train)\n\n#y_cases_pred = model_cases.predict(X_valid)\n#y_deaths_pred = model_deaths.predict(X_valid)\n\nmodel_cases.fit(X_,y_cases)\nmodel_deaths.fit(X_,y_deaths)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.metrics import accuracy_score\naccuracy_cases = accuracy_score(y_cases_valid,y_cases_pred)\naccuracy_deaths = accuracy_score(y_deaths_valid,y_deaths_pred)\nprint(accuracy_cases,accuracy_deaths)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#make predictions on test data\ncases =model_cases.predict(test_data).round()\ndeaths = model_deaths.predict(test_data).round()\noutput = pd.DataFrame({'ForecastId': test_data.index,\n                       'ConfirmedCases': cases,\n                       'Fatalities':deaths})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}