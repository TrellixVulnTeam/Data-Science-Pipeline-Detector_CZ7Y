{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-1/train.csv')\ndf_test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-1/test.csv')\nprint(df_train.shape)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"import pandas_profiling\npandas_profiling.ProfileReport(df_train)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Province/State'] = df_train.apply(\n    lambda row: row['Country/Region'] if pd.isnull(row['Province/State']) else row['Province/State'],\n    axis=1\n)\ndf_test['Province/State'] = df_test.apply(\n    lambda row: row['Country/Region'] if pd.isnull(row['Province/State']) else row['Province/State'],\n    axis=1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Date'] = df_train.apply(\n    lambda row: pd.Timestamp(row['Date']).value//10**9,\n    axis=1\n)\ndf_test['Date'] = df_test.apply(\n    lambda row: pd.Timestamp(row['Date']).value//10**9,\n    axis=1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndf = df_train.sort_values('Date')\nplt.plot(df['Date'],np.log2(df['ConfirmedCases'])/np.log2(1.5))\nplt.show()\nplt.plot(df['Date'],np.log2(df['Fatalities'])/np.log2(1.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import Normalizer\nvectorizer = CountVectorizer(binary=True)\nvectorizer.fit(df_train['Province/State'])\nstate_train = vectorizer.transform(df_train['Province/State'])\nstate_test = vectorizer.transform(df_test['Province/State'])\n\nvectorizer = CountVectorizer(binary=True)\nvectorizer.fit(df_train['Country/Region'])\ncountry_train = vectorizer.transform(df_train['Country/Region'])\ncountry_test = vectorizer.transform(df_test['Country/Region'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalizer = Normalizer()\nnormalizer.fit(df_train['Lat'].values.reshape(1, -1))\nlat_train = normalizer.transform(df_train['Lat'].values.reshape(1, -1))\nlat_test = normalizer.transform(df_test['Lat'].values.reshape(1, -1))\n\nnormalizer = Normalizer()\nnormalizer.fit(df_train['Long'].values.reshape(1, -1))\nlong_train = normalizer.transform(df_train['Long'].values.reshape(1, -1))\nlong_test = normalizer.transform(df_test['Long'].values.reshape(1, -1))\n\nnormalizer = Normalizer()\nnormalizer.fit(df_train['Date'].values.reshape(1, -1))\ndate_train = normalizer.transform(df_train['Date'].values.reshape(1, -1))\ndate_test = normalizer.transform(df_test['Date'].values.reshape(1, -1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(state_train.shape)\nprint(country_train.shape)\nprint(lat_train.reshape(-1, 1).shape)\nprint(long_train.shape)\nprint(date_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\ndata_train = hstack([state_train, country_train, lat_train.reshape(-1, 1), long_train.reshape(-1, 1), date_train.reshape(-1, 1)])\ndata_test = hstack([state_test, country_test, lat_test.reshape(-1, 1), long_test.reshape(-1, 1), date_test.reshape(-1, 1)])\n#data_train=data_train.todense()\n#data_test=data_test.todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(data_test[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn import linear_model\ndata_country={}\nlabel_country={}\nfor index, row in df_train.iterrows():\n    if row['Country/Region'] not in data_country:\n        data_country[row['Country/Region']]=[]\n        label_country[row['Country/Region']]=[]\n    \n    data_country[row['Country/Region']].append(data_train[index])\n    label_country[row['Country/Region']].append(row['ConfirmedCases'])\nmodel_country={}\nfor country in data_country.keys():\n    model_country[country]=linear_model.SGDRegressor(max_iter=1000, tol=1e-3)\n    #print(np.array(data_country[country]).reshape(len(label_country[country]),-1).shape)\n    #print(len(label_country[country]))\n    model_country[country].fit(np.array(data_country[country]).reshape(len(label_country[country]),-1),label_country[country])"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"conf_cased_pred=[]\nfor index, row in df_test.iterrows():\n    conf_cased_pred.append(model_country[row['Country/Region']].predict(data_test[index]))\nnormalizer = Normalizer()\nnormalizer.fit(df_train['ConfirmedCases'].values.reshape(1, -1))\nconf_cased_pred_train = normalizer.transform(df_train['ConfirmedCases'].values.reshape(1, -1))\nconf_cased_pred_test = normalizer.transform(conf_cased_pred)\ndata_train_with_conf = hstack([state_train, country_train, lat_train.reshape(-1, 1), long_train.reshape(-1, 1), date_train.reshape(-1, 1), conf_cased_pred_train.reshape(-1,1)])\ndata_test_with_conf = hstack([state_test, country_test, lat_test.reshape(-1, 1), long_test.reshape(-1, 1), date_test.reshape(-1, 1),conf_cased_pred_test.reshape(-1, 1)])\ndata_train_with_conf = data_train_with_conf.todense()\ndata_test_with_conf = data_test_with_conf.todense()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"data_country={}\nlabel_country={}\nfor index, row in df_train.iterrows():\n    if row['Country/Region'] not in data_country:\n        data_country[row['Country/Region']]=[]\n        label_country[row['Country/Region']]=[]\n    \n    data_country[row['Country/Region']].append(data_train_with_conf[index])\n    label_country[row['Country/Region']].append(row['Fatalities'])\nmodel_country={}\nfor country in data_country.keys():\n    model_country[country]=linear_model.SGDRegressor(max_iter=1000, tol=1e-3)\n    #print(np.array(data_country[country]).reshape(len(label_country[country]),-1).shape)\n    #print(len(label_country[country]))\n    model_country[country].fit(np.array(data_country[country]).reshape(len(label_country[country]),-1),label_country[country])"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"fatalities_pred=[]\nfor index, row in df_test.iterrows():\n    fatalities_pred.append(model_country[row['Country/Region']].predict(data_test_with_conf[index]))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nparam_grid = dict(n_estimators=np.array([50,100,200,300,400]), subsample=np.array([0.5,0.6,0.7,0.8,0.9,1]), max_depth=np.array([3,5,7,9]))\nmodel = GradientBoostingRegressor(random_state=21)\nkfold = KFold(n_splits=10, random_state=21)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=kfold)\ngrid_result = grid.fit(data_train.todense(), df_train['ConfirmedCases'])\nprint(grid_result.best_estimator_)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nclf = XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n             min_child_weight=1, monotone_constraints=None,\n             n_estimators=1000, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n             validate_parameters=False, verbosity=None)\nclf.fit(data_train.todense(), df_train['ConfirmedCases'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#conf_cased_pred_tr = clf.predict(data_test.todense())\nconf_cased_pred = clf.predict(data_test.todense())\nnormalizer = Normalizer()\nnormalizer.fit(df_train['ConfirmedCases'].values.reshape(1, -1))\nconf_cased_pred_train = normalizer.transform(df_train['ConfirmedCases'].values.reshape(1, -1))\nconf_cased_pred_test = normalizer.transform(conf_cased_pred.reshape(1, -1))\ndata_train_with_conf = hstack([state_train, country_train, lat_train.reshape(-1, 1), long_train.reshape(-1, 1), date_train.reshape(-1, 1), conf_cased_pred_train.reshape(-1,1)])\ndata_test_with_conf = hstack([state_test, country_test, lat_test.reshape(-1, 1), long_test.reshape(-1, 1), date_test.reshape(-1, 1),conf_cased_pred_test.reshape(-1, 1)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(conf_cased_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"param_grid = dict(n_estimators=np.array([50,100,200,300,400]))\nmodel = GradientBoostingRegressor(random_state=21)\nkfold = KFold(n_splits=10, random_state=21)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=kfold)\ngrid_result = grid.fit(data_train_with_conf.todense(), df_train['Fatalities'])\nprint(grid_result.best_estimator_)"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n             min_child_weight=1, monotone_constraints=None,\n             n_estimators=1000, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n             validate_parameters=False, verbosity=None)\nclf.fit(data_train_with_conf.todense(), np.log(df_train['Fatalities']+0.000000001))\nfatalities_pred = clf.predict(data_test_with_conf.todense())\n#fatalities_pred = np.exp(fatalities_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(fatalities_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"conf_cased_pred=np.concatenate( conf_cased_pred, axis=0 )\nfatalities_pred= np.concatenate( fatalities_pred, axis=0 )\nprint(conf_cased_pred[:5])\nprint(fatalities_pred[:5])"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_submission(conf, fat, sub_name):\n    my_submission = pd.DataFrame({'ForecastId':pd.read_csv('/kaggle/input/covid19-global-forecasting-week-1/test.csv').ForecastId,'ConfirmedCases':conf, 'Fatalities':fat})\n    my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n    print('A submission file has been made')\nmake_submission(conf_cased_pred,fatalities_pred,'submission')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nNN_model = Sequential()\n\n# The Input Layer :\nNN_model.add(Dense(256, kernel_initializer='normal',input_dim = 494, activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(128, kernel_initializer='normal',activation='relu'))\n#NN_model.add(Flatten())\nNN_model.add(Dropout(0.4))\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()\nNN_model.fit(data_train.todense(), df_train['ConfirmedCases'], epochs=100,batch_size=64, validation_split = 0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"conf_cased_pred = NN_model.predict(data_test.todense())\nnormalizer = Normalizer()\nnormalizer.fit(df_train['ConfirmedCases'].values.reshape(1, -1))\nconf_cased_pred_train = normalizer.transform(df_train['ConfirmedCases'].values.reshape(1, -1))\nconf_cased_pred_test = normalizer.transform(conf_cased_pred.reshape(1, -1))\ndata_train_with_conf = hstack([state_train, country_train, lat_train.reshape(-1, 1), long_train.reshape(-1, 1), date_train.reshape(-1, 1), conf_cased_pred_train.reshape(-1,1)])\ndata_test_with_conf = hstack([state_test, country_test, lat_test.reshape(-1, 1), long_test.reshape(-1, 1), date_test.reshape(-1, 1),conf_cased_pred_test.reshape(-1, 1)])"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"NN_model = Sequential()\n# The Input Layer :\nNN_model.add(Dense(256, kernel_initializer='normal',input_dim = 495, activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(128, kernel_initializer='normal',activation='relu'))\n#NN_model.add(Flatten())\nNN_model.add(Dropout(0.4))\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()\nNN_model.fit(data_train_with_conf.todense(), df_train['Fatalities'], epochs=100,batch_size=64, validation_split = 0.1)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"fatalities_pred = NN_model.predict(data_test_with_conf.todense())"},{"metadata":{},"cell_type":"markdown","source":"print(len(np.ndarray.flatten(conf_cased_pred)))\nprint(fatalities_pred.shape)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def make_submission(conf, fat, sub_name):\n    my_submission = pd.DataFrame({'ForecastId':pd.read_csv('/kaggle/input/covid19-global-forecasting-week-1/test.csv').ForecastId,'ConfirmedCases':conf, 'Fatalities':fat})\n    my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n    print('A submission file has been made')\nmake_submission(np.ndarray.flatten(conf_cased_pred),np.ndarray.flatten(fatalities_pred),'submission')"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}