{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option(\"display.max_columns\",100)\ntrain_path=r\"../input/covid19-global-forecasting-week-1/train.csv\"\ntest_path=r\"../input/covid19-global-forecasting-week-1/test.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.read_csv(train_path)\ndf_test=pd.read_csv(test_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.shape)\nprint(df_test.shape) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest=pd.concat([df_train,df_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def func(x):\n    try:\n        x_new=x[\"Country/Region\"]+\"/\"+x[\"Province/State\"]\n    except:\n        x_new=x[\"Country/Region\"] \n    return str(x_new)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest[\"place_id\"]=df_traintest.apply(lambda x:func(x),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest[888:890]     ################成功生成了国家和城市的id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp=np.sort(df_traintest[\"place_id\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"一共有{}个地区的调查数据\".format(len(tmp)))  \nprint(tmp[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest[\"Date\"]=pd.to_datetime(df_traintest[\"Date\"])          ###############将天的数据转换","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest.head()                  ################################训练测试集","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest[\"day\"]=df_traintest[\"Date\"].apply(lambda x: x.dayofyear).astype(np.int16)\ndf_traintest.head()                     ##########################将日期排序","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest[:-10] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest[df_traintest[\"place_id\"]==\"Afghanistan\"]  ######################各地区一直到4.23的数据","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy  #######新的模块,deepcopy是完全独立的复制\nplaces=df_traintest[\"place_id\"].unique()########对每一个地区进行新生追踪","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest2=copy.deepcopy(df_traintest)\ndf_traintest2[\"cases/day\"]=0\ndf_traintest2[\"fatal/day\"]=0\nfor place in places:\n    tmp=df_traintest2[\"ConfirmedCases\"][df_traintest2[\"place_id\"]==place].values\n    tmp[1:]=tmp[1:]-tmp[:-1]   #####################每天的新增确诊数目是隔日之差\n    df_traintest2[\"cases/day\"][df_traintest2[\"place_id\"]==place]=tmp\n    tmp=df_traintest2[\"Fatalities\"][df_traintest2[\"place_id\"]==place].values\n    tmp[1:]=tmp[1:]-tmp[:-1]\n    df_traintest2[\"fatal/day\"][df_traintest2[\"place_id\"]==place]=tmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest2[df_traintest2[\"place_id\"]==\"China/Hubei\"].head()   ##########得到了湖北的感染以及致命人数变化","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_aggregation(df,col,mean_range):\n    df_new=copy.deepcopy(df)  \n    col_new='{}-({}-{})'.format(col,mean_range[0],mean_range[1])###############\n    df_new[col_new]=0\n    tmp=df_new[col].rolling(mean_range[1]-mean_range[0]+1).mean() #################都是每7天滚动求一次均值\n    df_new[col_new][mean_range[0]:]=tmp[:-(mean_range[0])]    ##################手动延后时间序列\n    df_new[col_new][pd.isna(df_new[col_new])]=0       \n    return df_new[[col_new]].reset_index(drop=True)  #####################完全把原来的索引丢弃掉，设立新的数字索引","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def do_aggregations(df):\n    df=(pd.concat([df,df_aggregation(df,'cases/day',[1,1])],axis=1)).reset_index(drop=True)\n    df=(pd.concat([df,df_aggregation(df,'cases/day',[1,7])],axis=1)).reset_index(drop=True)\n    df=(pd.concat([df,df_aggregation(df,'cases/day',[8,14])],axis=1)).reset_index(drop=True)\n    df=(pd.concat([df,df_aggregation(df,'cases/day',[15,21])],axis=1)).reset_index(drop=True)\n    df=(pd.concat([df,df_aggregation(df,'fatal/day',[1,1])],axis=1)).reset_index(drop=True)\n    df=(pd.concat([df,df_aggregation(df,'fatal/day',[1,7])],axis=1)).reset_index(drop=True)\n    df=(pd.concat([df,df_aggregation(df,'fatal/day',[8,14])],axis=1)).reset_index(drop=True)\n    df=(pd.concat([df,df_aggregation(df,'fatal/day',[15,21])],axis=1)).reset_index(drop=True)\n    for threshold in[1,10,100]: ################设立不同的阈值求和\n        days_under_threshold=(df['ConfirmedCases']<threshold).sum()\n        tmp=df[\"day\"]-22-days_under_threshold\n        tmp[tmp<0]=0                            ###########照顾到22号之前已经爆发的地区，比如中国湖北\n        df[\"days_since_{}cases\".format(threshold)]=tmp\n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['Fatalities']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}fatal'.format(threshold)] = tmp\n    if df['place_id'][0]=='China/Hubei':             #################湖北爆发时间比其他地区早，需要特别调整\n        df['days_since_1cases'] += 35 # 2019/12/8\n        df['days_since_10cases'] += 35-13 # 2019/12/8-2020/1/2 assume 2019/12/8+13\n        df['days_since_100cases'] += 4 # 2020/1/18\n        df['days_since_1fatal'] += 13 # 2020/1/9\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest3=[]\nfor place in places:\n    df_tmp=df_traintest2[df_traintest2[\"place_id\"]==place].reset_index(drop=True)\n    df_tmp=do_aggregations(df_tmp)\n    df_traintest3.append(df_tmp)\ndf_traintest3=pd.concat(df_traintest3).reset_index(drop=True)\ndf_traintest3[df_traintest3[\"place_id\"]==\"China/Hubei\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_path=r\"../input/smokingstats/share-of-adults-who-smoke.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_smoking=pd.read_csv(smoke_path)\ndf_smoking.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_smoking_recent=df_smoking.sort_values(\"Year\",ascending=False).reset_index(drop=True) \n####################同个地区多个年份的数据重复\ndf_smoking_recent=df_smoking_recent[df_smoking_recent[\"Entity\"].duplicated()==False]\n###########################改了两列的名字,没什么大变动,方便之后的连接\ndf_smoking_recent['Country/Region'] = df_smoking_recent['Entity']\ndf_smoking_recent['SmokingRate'] = df_smoking_recent['Smoking prevalence, total (ages 15+) (% of adults)']\ndf_smoking_recent.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest4 = pd.merge(df_traintest3,df_smoking_recent[[\"Country/Region\",\"SmokingRate\"]],on=\"Country/Region\",how=\"left\")\nprint(df_traintest4.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest4[df_traintest4[\"place_id\"]==\"China/Hubei\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SmokingRate=df_smoking_recent[\"SmokingRate\"][df_smoking_recent[\"Entity\"]==\"World\"].values[0]\ndf_traintest4[\"SmokingRate\"][pd.isna(df_traintest4[\"SmokingRate\"])]=SmokingRate\ndf_traintest4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_path=r\"../input/smokingstats/WEO.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weo=pd.read_csv(smoke_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_weo['Subject Descriptor'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subs=df_weo[\"Subject Descriptor\"].unique()[:-1]  ###########去掉最后一个空缺值\ndf_weo_agg=df_weo[[\"Country\"]][df_weo[\"Country\"].duplicated()==False].reset_index(drop=True)\nfor sub in subs[:]:\n    df_tmp=df_weo[[\"Country\",\"2019\"]][df_weo[\"Subject Descriptor\"]==sub].reset_index(drop=True)\n    df_tmp=df_tmp[df_tmp[\"Country\"].duplicated()==False].reset_index(drop=True)\n    df_tmp.columns=[\"Country\",sub]          ##############把表头的2019改了\n    df_weo_agg=df_weo_agg.merge(df_tmp,on=\"Country\",how=\"left\")\ndf_weo_agg.columns=[\"\".join(c if c.isalnum() else \"_\" for c in str(x))for x in df_weo_agg.columns]\ndf_weo_agg.columns\ndf_weo_agg['Country/Region'] = df_weo_agg['Country']\ndf_weo_agg.head()        #####################各个经济指标的数据","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest5 = pd.merge(df_traintest4, df_weo_agg, on='Country/Region', how='left')\nprint(df_traintest5.shape)\ndf_traintest5.head()      #####################有空缺值很正常，很粗的数据","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"life_path=r\"../input/smokingstats/Life expectancy at birth.csv\"\ndf_life=pd.read_csv(life_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_life.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp=df_life.iloc[:,1].values.tolist()\ndf_life=df_life[[\"Country\",\"2018\"]]\ndef func(x):\n    try:\n        x_new=float(x.replace(\",\",\"\"))\n    except:\n        print(x)\n        x_new=np.nan \n    return x_new\ndf_life[\"2018\"]=df_life[\"2018\"].apply(lambda x:func(x))\ndf_life.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_life = df_life[['Country', '2018']]\ndf_life.columns = ['Country/Region', 'LifeExpectancy']  #############表头转换","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest6 = pd.merge(df_traintest5, df_life, on='Country/Region', how='left')\nprint(len(df_traintest6))\ndf_traintest6.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_path=r\"../input/countryinfo/covid19countryinfo.csv\"\ndf_country=pd.read_csv(country_path)[[\"country\",\"pop\",\"tests\",\"testpop\",\"density\",\"medianage\",\"urbanpop\",\"quarantine\",\"schools\",\"hospibed\",\"smokers\",\"sex0\",\"sex14\",\"sex25\",\"sex54\",\"sex64\",\"sex65plus\",\"sexratio\",\"lung\",\"femalelung\",\"malelung\"]]\ndf_country.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_country[\"femalelung\"][df_country[\"country\"]==\"China\"] ####################中国，中国香港，中国澳门，湖北特别疫情地区","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_country[\"Country/Region\"]=df_country[\"country\"]\ndf_country=df_country[df_country[\"country\"].duplicated()==False]    ############把不同的称呼的同一地区合并","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_country[df_country['country'].duplicated()].shape)    ############确认无重复","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_country[\"femalelung\"][df_country[\"Country/Region\"]==\"China\"]       #####中国地区数据变成了香港的NaN\n####手动删除香港  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_country[\"femalelung\"][df_country[\"Country/Region\"]==\"China\"]    ###########正常数据","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest7=pd.merge(df_traintest6,df_country.drop([\"country\",\"testpop\",\"tests\"],axis=1),on=[\"Country/Region\"],how=\"left\")\nprint(df_traintest7.shape)\ndf_traintest7.head()\ndf_traintest7[df_traintest7[\"Country/Region\"]==\"China\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest7[df_traintest7[\"place_id\"]==\"China/Hubei\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_label(df, col, freq_limit=0):\n    df[col][pd.isna(df[col])] = 'nan'\n    tmp = df[col].value_counts()\n    cols = tmp.index.values               ######################cols是索引值\n    freq = tmp.values                   ######################freq是对应的值\n    num_cols = (freq>=freq_limit).sum()\n    print(\"col: {}, num_cat: {}, num_reduced: {}\".format(col, len(cols), num_cols))\n    col_new = '{}_le'.format(col)\n    df_new = pd.DataFrame(np.ones(len(df), np.int16)*(num_cols-1), columns=[col_new])\n    for i, item in enumerate(cols[:num_cols]):\n        df_new[col_new][df[col]==item] = i\n    return df_new\n\ndef get_df_le(df, col_index, col_cat):\n    df_new = df[[col_index]]\n    for col in col_cat:\n        df_tmp = encode_label(df, col)\n        df_new = pd.concat([df_new, df_tmp], axis=1)\n    return df_new\n\ndf_traintest7['id'] = np.arange(len(df_traintest7))\ndf_le = get_df_le(df_traintest7, 'id', ['Country/Region', 'Province/State'])\ndf_traintest8 = pd.merge(df_traintest7, df_le, on='id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8['cases/day'] = df_traintest8['cases/day'].astype(np.float)\ndf_traintest8['fatal/day'] = df_traintest8['fatal/day'].astype(np.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# covert object type to float\ndef func(x):\n    x_new = 0\n    try:\n        x_new = float(x.replace(\",\", \"\"))\n    except:\n        x_new = np.nan\n    return x_new\ncols = [\n    'Gross_domestic_product__constant_prices', \n    'Gross_domestic_product__current_prices', \n    'Gross_domestic_product__deflator', \n    'Gross_domestic_product_per_capita__constant_prices', \n    'Gross_domestic_product_per_capita__current_prices', \n    'Output_gap_in_percent_of_potential_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total', \n    'Implied_PPP_conversion_rate', 'Total_investment', \n    'Gross_national_savings', 'Inflation__average_consumer_prices', \n    'Inflation__end_of_period_consumer_prices', \n    'Six_month_London_interbank_offered_rate__LIBOR_', \n    'Volume_of_imports_of_goods_and_services', \n    'Volume_of_Imports_of_goods', \n    'Volume_of_exports_of_goods_and_services', \n    'Volume_of_exports_of_goods', 'Unemployment_rate', 'Employment', 'Population', \n    'General_government_revenue', 'General_government_total_expenditure', \n    'General_government_net_lending_borrowing', 'General_government_structural_balance', \n    'General_government_primary_net_lending_borrowing', 'General_government_net_debt', \n    'General_government_gross_debt', 'Gross_domestic_product_corresponding_to_fiscal_year__current_prices', \n    'Current_account_balance', 'pop'\n]\nfor col in cols:\n    df_traintest8[col] = df_traintest8[col].apply(lambda x: func(x))  \nprint(df_traintest8['pop'].dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8[df_traintest8['place_id']=='China/Hubei'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8.to_csv(\"final_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#######################到这一步数据处理初步完成\n################首先得到一个模型的指标\n################metrics.mean_squared_error文档：https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error\n###############仔细讲解下面的函数：metric是sk中的一个指标类,mean_squared_error平均回归偏误\n###############numpy.clip函数控制预测的数据范围，可能模型抛出一个无穷大的值或负值，要截取预测值的上下限，\nfrom sklearn import metrics\ndef calc_score(y_true, y_pred):\n    y_true[y_true<0] = 0\n    score = metrics.mean_squared_error(np.log(y_true.clip(0, 1e10)+1), np.log(y_pred[:]+1))**0.5\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nparams = {'num_leaves': 8, ###########叶节点数目\n          'min_data_in_leaf': 5,  # 42, ################每个叶子最少的样本数：减少过拟合\n          'objective': 'regression',############目标：回归拟合\n          'max_depth': 8,  ###############深度限制防止过拟合\n          'learning_rate': 0.02,    ############学习速率\n          'boosting': 'gbdt',         #############梯度下降算法\n          'bagging_freq': 5,  # 5           ##########每迭代5次做一次重新取样\n          'bagging_fraction': 0.8,  # 0.5,  ############每次迭代重新取样的比例\n          'feature_fraction': 0.8201,      #########每次迭代选择键树的参数数目\n          'bagging_seed': SEED,       ###########随机种子数\n          'reg_alpha': 1,  # 1.728910519108444,       ############正则化项参数\n          'reg_lambda': 4.9847051755586085,     #############正则化部分参数\n          'random_state': SEED,     ##############我的理解是随机种子数\n          'metric': 'mse',      #############模型自我评价的指标:mse: mean squared error\n          'verbosity': 100,          ########实在不懂,显示训练资讯详细程度(0~3)，默认：1,就是最后训练结果的报告信息\n          'min_gain_to_split': 0.02,  # 0.01077313523861969,      ##############分裂的最小信息增益？？？应该也是防止过拟合\n          'min_child_weight': 5,  # 19.428902804238373,   ################叶节点样本权重小于此值，停止分列，防止过拟合\n          'num_threads': 6,         ############线程数目，与训练速度有关\n          }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict fatalities/day\ncol_target = 'fatal/day'\ncol_var = [\n    'Lat', 'Long',                  #####保留经纬数据？\n    'days_since_1cases', \n    'days_since_10cases', \n    'days_since_100cases',\n    'days_since_1fatal', \n    'days_since_10fatal', 'days_since_100fatal',\n ################################下面是一组十分重要的变量，我认为他在拟合不同的传染阶段，人为调整爆发期\n    'cases/day-(1-1)',  ####################第一天的确诊人数\n    'cases/day-(1-7)',  ################从第七天开始统计的滚动确诊每日平均数目\n    'cases/day-(8-14)',   #################从第八天开始统计的滚动确诊每日平均数目\n    'cases/day-(15-21)',  ##################同理\n    'fatal/day-(1-1)', \n    'fatal/day-(1-7)',         #########同理\n    'fatal/day-(8-14)', \n    'fatal/day-(15-21)', \n    'SmokingRate',\n    'Gross_domestic_product__constant_prices',\n    'Gross_domestic_product__current_prices',\n    'Gross_domestic_product__deflator',\n    'Gross_domestic_product_per_capita__constant_prices',\n    'Gross_domestic_product_per_capita__current_prices',\n    'Output_gap_in_percent_of_potential_GDP',\n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n    'Implied_PPP_conversion_rate', 'Total_investment',\n    'Gross_national_savings', 'Inflation__average_consumer_prices',\n    'Inflation__end_of_period_consumer_prices',\n    'Six_month_London_interbank_offered_rate__LIBOR_',\n    'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n    'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n    'Unemployment_rate',                            ###############失业率\n    'Employment', 'Population',\n    'General_government_revenue', 'General_government_total_expenditure',\n    'General_government_net_lending_borrowing',\n    'General_government_structural_balance',\n    'General_government_primary_net_lending_borrowing',\n    'General_government_net_debt', 'General_government_gross_debt',\n    'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n    'Current_account_balance', \n    'LifeExpectancy',\n    'pop',\n    'density', \n    'medianage', \n    'urbanpop', \n    'hospibed', 'smokers',\n]\ncol_cat = []\n#####################当然是划分训练集和验证集\n#####################日期序号为61天前的数据作为训练集合\n#####################日期学号为61-72天的数据作为验证集合\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<61)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61) & (df_traintest8['day']<72)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\n#######################为什么要取对数：取对数可以缩小特别大特别小的数据之间的差别，比如1和e10,相当于排除了特殊点？\n#######################最后得到的最佳参数肯定是不变的\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n#######################lgb封装自身的训练集\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100, ###########每100次输出一次评测结果\n                  early_stopping_rounds=150,)      #############如果连续150轮都无法优化，那么就提前停下\n\nbest_itr = model.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['fatal/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))       ###################越大越差","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = pd.DataFrame()\ntmp[\"feature\"] = col_var\ntmp[\"importance\"] = model.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\ntmp[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_var = [\n#     'Lat', 'Long',                  #####保留经纬数据？\n    'days_since_1cases', \n    'days_since_10cases', \n    'days_since_100cases',\n    'days_since_1fatal', \n    'days_since_10fatal', \n#     'days_since_100fatal',\n#  ################################下面是一组十分重要的变量，拟合不同的传染阶段，人为调整爆发期\n    'cases/day-(1-1)',  ####################第一天的确诊人数\n    'cases/day-(1-7)',  ################从第七天开始统计的滚动确诊每日平均数目\n    'cases/day-(8-14)',   #################从第八天开始统计的滚动确诊每日平均数目\n#     'cases/day-(15-21)',  ##################同理\n    'fatal/day-(1-1)', \n    'fatal/day-(1-7)',         #########同理\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_cat = []\n#####################当然是划分训练集和验证集\n#####################日期序号为61天前的数据作为训练集合\n#####################日期学号为61-72天的数据作为验证集合\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<61)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61) & (df_traintest8['day']<72)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\n#######################为什么要取对数：取对数可以缩小特别大特别小的数据之间的差别，比如1和e10,相当于排除了特殊点？\n#######################最后得到的最佳参数肯定是不变的\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n#######################lgb封装自身的训练集\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, \n                  valid_sets=[train_data, valid_data],\n                  verbose_eval=100, ###########每100次输出一次评测结果\n                  early_stopping_rounds=150,)      #############如果连续150轮都无法优化，那么就提前停下\n\nbest_itr = model.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = pd.DataFrame()\ntmp[\"feature\"] = col_var\ntmp[\"importance\"] = model.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\ntmp[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['fatal/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score)) ####################只是好了一点点，这代表前面几个变量是解释的因变量中主力军","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_var = [\n#     'Lat', 'Long',                  #####保留经纬数据？\n#     'days_since_1cases', \n#     'days_since_10cases', \n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal', \n#     'days_since_100fatal',\n#  ################################下面是一组十分重要的变量，我认为他在拟合不同的传染阶段，人为调整爆发期\n    'cases/day-(1-1)',  ####################第一天的确诊人数\n    'cases/day-(1-7)',  ################从第七天开始统计的滚动确诊每日平均数目\n    'cases/day-(8-14)',   #################从第八天开始统计的滚动确诊每日平均数目\n    'cases/day-(15-21)',  ##################同理\n    'fatal/day-(1-1)', \n    'fatal/day-(1-7)',         #########同理保留了所有的趋势数据\n    'fatal/day-(8-14)', \n    'fatal/day-(15-21)', \n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_cat = []\n#####################当然是划分训练集和验证集\n#####################日期序号为61天前的数据作为训练集合\n#####################日期学号为61-72天的数据作为验证集合\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<61)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61) & (df_traintest8['day']<72)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\n#######################为什么要取对数：取对数可以缩小特别大特别小的数据之间的差别，比如1和e10,相当于排除了特殊点？\n#######################最后得到的最佳参数肯定是不变的\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n#######################lgb封装自身的训练集\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100, ###########每100次输出一次评测结果\n                  early_stopping_rounds=150,)      #############如果连续150轮都无法优化，那么就提前停下\n\nbest_itr = model.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['fatal/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score)) ####################变得好很多,继续引入更加多的参数","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_var = [\n#     'Lat', 'Long',                  #####保留经纬数据？\n    'days_since_1cases', \n    'days_since_10cases', \n    'days_since_100cases',\n    'days_since_1fatal', \n    'days_since_10fatal', \n    'days_since_100fatal',\n#  ################################下面是一组十分重要的变量，我认为他在拟合不同的传染阶段，人为调整爆发期\n#     'cases/day-(1-1)',  ####################第一天的确诊人数\n#     'cases/day-(1-7)',  ################从第七天开始统计的滚动确诊每日平均数目\n#     'cases/day-(8-14)',   #################从第八天开始统计的滚动确诊每日平均数目\n#     'cases/day-(15-21)',  ##################同理\n#     'fatal/day-(1-1)', \n#     'fatal/day-(1-7)',         #########同理保留了所有的趋势数据\n#     'fatal/day-(8-14)', \n#     'fatal/day-(15-21)', \n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_cat = []\n#####################当然是划分训练集和验证集\n#####################日期序号为61天前的数据作为训练集合\n#####################日期学号为61-72天的数据作为验证集合\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<61)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61) & (df_traintest8['day']<72)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\n#######################为什么要取对数：取对数可以缩小特别大特别小的数据之间的差别，比如1和e10,相当于排除了特殊点？\n#######################最后得到的最佳参数肯定是不变的\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n#######################lgb封装自身的训练集\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100, ###########每100次输出一次评测结果\n                  early_stopping_rounds=150,)      #############如果连续150轮都无法优化，那么就提前停下\n\nbest_itr = model.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['fatal/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score)) ####################变得好很多,继续引入更加多的参数","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_var = [\n#     'Lat', 'Long',                  #####需要保留经纬数据吗，经度纬度可能影响气候？\n#  ################################下面是一组十分重要的变量，我认为他在拟合不同的传染阶段，人为调整爆发期\n    'cases/day-(1-1)',  ####################第一天的确诊人数\n    'cases/day-(1-7)',  ################从第七天开始统计的滚动确诊每日平均数目\n    'cases/day-(8-14)',   #################从第八天开始统计的滚动确诊每日平均数目\n    'cases/day-(15-21)',  ##################同理\n    'fatal/day-(1-1)', \n    'fatal/day-(1-7)',         #########同理\n     'SmokingRate',\n    'Gross_domestic_product__constant_prices',\n    'LifeExpectancy',\n    'pop',\n    'density', \n    'medianage', \n    'urbanpop', \n    'hospibed', 'smokers'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_cat = []\n#####################当然是划分训练集和验证集\n#####################日期序号为61天前的数据作为训练集合\n#####################日期学号为61-72天的数据作为验证集合\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<61)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61) & (df_traintest8['day']<72)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\n#######################为什么要取对数：取对数可以缩小特别大特别小的数据之间的差别，比如1和e10,相当于排除了特殊点？\n#######################最后得到的最佳参数肯定是不变的\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n#######################lgb封装自身的训练集\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100, ###########每100次输出一次评测结果\n                  early_stopping_rounds=150,)      #############如果连续150轮都无法优化，那么就提前停下\n\nbest_itr = model.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['fatal/day'].values\n##################记住之前加了一个1\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score)) ####################难受，更差了","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = pd.DataFrame()\ntmp[\"feature\"] = col_var\ntmp[\"importance\"] = model.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\ntmp[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_var = [\n    'Lat', 'Long',                  #####需要保留经纬数据吗，经度纬度可能影响气候？试一下\n#  ################################下面是一组十分重要的变量，我认为他在拟合不同的传染阶段，人为调整爆发期\n    'cases/day-(1-1)',  ####################第一天的确诊人数\n    'cases/day-(1-7)',  ################从第七天开始统计的滚动确诊每日平均数目\n#     'cases/day-(8-14)',   #################从第八天开始统计的滚动确诊每日平均数目\n#     'cases/day-(15-21)',##################去掉这个最差劲的趋势数据，试一下结果会不会好一点，太长的周期可能不会起作用，换为致命的15-21滚动\n   'fatal/day-(1-7)',\n    'fatal/day-(8-14)', \n    'fatal/day-(15-21)',         #########同理\n     'SmokingRate',\n     'density', \n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_cat = []\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<61)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61) & (df_traintest8['day']<72)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nbest_itr = model.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['fatal/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = pd.DataFrame()\ntmp[\"feature\"] = col_var\ntmp[\"importance\"] = model.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\ntmp[:10]          ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_cat = []\n#####################当然是划分训练集和验证集\n#####################日期序号为61天前的数据作为训练集合\n#####################日期学号为61-72天的数据作为验证集合\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<61)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61) & (df_traintest8['day']<72)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var] \nX_valid = df_valid[col_var]\n#######################为什么要取对数：取对数可以缩小特别大特别小的数据之间的差别，比如1和e10,相当于排除了特殊点？\n#######################最后得到的最佳参数肯定是不变的\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n#######################lgb封装自身的训练集\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100, ###########每100次输出一次评测结果\n                  early_stopping_rounds=150,)      #############如果连续150轮都无法优化，那么就提前停下\n\nbest_itr = model.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['fatal/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<72)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<72)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_target2 = 'cases/day'\ncol_var2 = [\n    'Lat', 'Long',\n    'cases/day-(1-1)', \n    'cases/day-(1-7)', \n    'cases/day-(8-14)',  \n    'cases/day-(15-21)', \n    'days_since_10cases'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_cat = []\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<61)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61) & (df_traintest8['day']<72)]\n# df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr = model2.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['cases/day'].values\ny_pred = np.exp(model2.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))                ##################还可以","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<72)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<72)]\n# df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['cases/day'].values\ny_pred = np.exp(model2.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<72)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=72)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel_pri = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr = model_pri.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\n# df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel_pri = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict fatalities/day\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<72)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=72)]\n# df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2_pri = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr = model2_pri.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\n# df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2_pri = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr = model2_pri.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp = df_traintest8[(df_traintest8['day']<72) | (pd.isna(df_traintest8['ForecastId'])==False)].reset_index(drop=True)\n################################删除之前聚合的数据项\ndf_tmp = df_tmp.drop([\n    'cases/day-(1-1)', 'cases/day-(1-7)', 'cases/day-(8-14)', 'cases/day-(15-21)', \n    'fatal/day-(1-1)', 'fatal/day-(1-7)', 'fatal/day-(8-14)', 'fatal/day-(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest9 = []\n######重新聚合\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_traintest9.append(df_tmp2)\ndf_traintest9 = pd.concat(df_traintest9).reset_index(drop=True)\ndf_traintest9[df_traintest9['day']>68].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################怎么说呢，训练集很准，到真正测试集合上就不行了\nimport seaborn as sns#################这是我们以前python学过的包","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############注意啊，这是什么模型的曲线?\n#############这当然是确诊数目的曲线，但是确诊我训练了两个模型\n########这是哪个模型？\n#########这是我们拟合72天内的模型啊\n##########可以看到72天内效果及其优秀，72天后呵呵\nplace = 'Iran'\ndf_interest_base = df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True)\ndf_interest = copy.deepcopy(df_interest_base)\ndf_interest['ConfirmedCases'] = df_interest['ConfirmedCases'].astype(np.float)\ndf_interest['cases/day'] = df_interest['cases/day'].astype(np.float)\ndf_interest['fatal/day'] = df_interest['fatal/day'].astype(np.float)\ndf_interest['Fatalities'] = df_interest['Fatalities'].astype(np.float)\ndf_interest['cases/day'][df_interest['day']>=72] = -1\ndf_interest['fatal/day'][df_interest['day']>=72] = -1\nlen_known = (df_interest['cases/day']!=-1).sum()\nlen_unknown = (df_interest['cases/day']==-1).sum()\nprint(\"len train: {}, len prediction: {}\".format(len_known, len_unknown))\nX_valid = df_interest[col_var][df_interest['day']>=72]\nX_valid2 = df_interest[col_var2][df_interest['day']>=72]\npred_f =  np.exp(model.predict(X_valid))-1\npred_c = np.exp(model2.predict(X_valid2))-1\ndf_interest['fatal/day'][df_interest['day']>=72] = pred_f.clip(0, 1e10)\ndf_interest['cases/day'][df_interest['day']>=72] = pred_c.clip(0, 1e10)\ndf_interest['Fatalities'] = np.cumsum(df_interest['fatal/day'].values)\ndf_interest['ConfirmedCases'] = np.cumsum(df_interest['cases/day'].values)\nfor j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n    X_valid = df_interest[col_var].iloc[j+len_known]\n    X_valid2 = df_interest[col_var2].iloc[j+len_known]\n    pred_f = model.predict(X_valid)\n    pred_c = model2.predict(X_valid2)\n    pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n    pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n    df_interest['fatal/day'][j+len_known] = pred_f\n    df_interest['cases/day'][j+len_known] = pred_c\n    df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n    df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n    df_interest = df_interest.drop([\n        'cases/day-(1-1)', 'cases/day-(1-7)', 'cases/day-(8-14)', 'cases/day-(15-21)', \n        'fatal/day-(1-1)', 'fatal/day-(1-7)', 'fatal/day-(8-14)', 'fatal/day-(15-21)', \n        'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n        'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',], axis=1)\n    df_interest = do_aggregations(df_interest.reset_index(drop=True))\n# visualize\ntmp = df_interest['cases/day'].values\ntmp = np.cumsum(tmp)\nsns.lineplot(x=df_interest_base['day'], y=tmp, label='pred')\ntmp = df_traintest8['ConfirmedCases'][(df_traintest8['place_id']==place)& (pd.isna(df_traintest8['ForecastId']))].values\nprint(len(tmp), tmp)\nsns.lineplot(x=df_traintest8['day'][(df_traintest8['place_id']==place)& (pd.isna(df_traintest8['ForecastId']))].values,\n             y=tmp, label='true')\nprint(place)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_day_train = df_traintest8['day'][pd.isna(df_traintest8['ForecastId'])].max()\nprint(last_day_train)\ndf_tmp = df_traintest8[\n    (pd.isna(df_traintest8['ForecastId'])) |\n    ((df_traintest8['day']>last_day_train) & (pd.isna(df_traintest8['ForecastId'])==False))].reset_index(drop=True)\ndf_tmp = df_tmp.drop([\n    'cases/day-(1-1)', 'cases/day-(1-7)', 'cases/day-(8-14)', 'cases/day-(15-21)', \n    'fatal/day-(1-1)', 'fatal/day-(1-7)', 'fatal/day-(8-14)', 'fatal/day-(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest10 = []\n############################再次聚合，记不记得，我们又动了数据了\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_traintest10.append(df_tmp2)\ndf_traintest10 = pd.concat(df_traintest10).reset_index(drop=True)\ndf_traintest10[df_traintest10['day']>last_day_train-5].head(10) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"place = places[np.random.randint(len(places))]\nplace = \"Iran\"\ndf_interest_base = df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True)\ndf_interest = copy.deepcopy(df_interest_base)\ndf_interest['ConfirmedCases'] = df_interest['ConfirmedCases'].astype(np.float)\ndf_interest['cases/day'] = df_interest['cases/day'].astype(np.float)\ndf_interest['fatal/day'] = df_interest['fatal/day'].astype(np.float)\ndf_interest['Fatalities'] = df_interest['Fatalities'].astype(np.float)\ndf_interest['cases/day'][df_interest['day']>=72] = -1\ndf_interest['fatal/day'][df_interest['day']>=72] = -1\nlen_known = (df_interest['cases/day']!=-1).sum()\nlen_unknown = (df_interest['cases/day']==-1).sum()\nprint(\"len train: {}, len prediction: {}\".format(len_known, len_unknown))\nX_valid = df_interest[col_var][df_interest['day']>=72]\nX_valid2 = df_interest[col_var2][df_interest['day']>=72]\npred_f =  np.exp(model.predict(X_valid))-1\npred_c = np.exp(model2.predict(X_valid2))-1\ndf_interest['fatal/day'][df_interest['day']>=72] = pred_f.clip(0, 1e10)\ndf_interest['cases/day'][df_interest['day']>=72] = pred_c.clip(0, 1e10)\ndf_interest['Fatalities'] = np.cumsum(df_interest['fatal/day'].values)\ndf_interest['ConfirmedCases'] = np.cumsum(df_interest['cases/day'].values)\nfor j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n    X_valid = df_interest[col_var].iloc[j+len_known]\n    X_valid2 = df_interest[col_var2].iloc[j+len_known]\n    pred_f = model.predict(X_valid)\n    pred_c = model2.predict(X_valid2)\n    pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n    pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n    df_interest['fatal/day'][j+len_known] = pred_f\n    df_interest['cases/day'][j+len_known] = pred_c\n    df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n    df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n    df_interest = df_interest.drop([\n    'cases/day-(1-1)', 'cases/day-(1-7)', 'cases/day-(8-14)', 'cases/day-(15-21)', \n    'fatal/day-(1-1)', 'fatal/day-(1-7)', 'fatal/day-(8-14)', 'fatal/day-(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                                   \n                                   ],  axis=1)\n    df_interest = do_aggregations(df_interest.reset_index(drop=True))\n\n# visualize\ntmp = df_interest['fatal/day'].values\ntmp = np.cumsum(tmp)\nsns.lineplot(x=df_interest_base['day'], y=tmp, label='pred')\ntmp = df_traintest8['Fatalities'][(df_traintest8['place_id']==place)& (pd.isna(df_traintest8['ForecastId']))].values\nprint(len(tmp), tmp)\nsns.lineplot(x=df_traintest8['day'][(df_traintest8['place_id']==place)& (pd.isna(df_traintest8['ForecastId']))].values,\n             y=tmp, label='true')\nprint(place)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"place = 'Iran'\ndf_interest_base = df_traintest10[df_traintest10['place_id']==place].reset_index(drop=True)\ndf_interest = copy.deepcopy(df_interest_base)\ndf_interest['ConfirmedCases'] = df_interest['ConfirmedCases'].astype(np.float)\ndf_interest['cases/day'] = df_interest['cases/day'].astype(np.float)\ndf_interest['fatal/day'] = df_interest['fatal/day'].astype(np.float)\ndf_interest['Fatalities'] = df_interest['Fatalities'].astype(np.float)\ndf_interest['cases/day'][df_interest['day']>last_day_train] = -1\ndf_interest['fatal/day'][df_interest['day']>last_day_train] = -1\nlen_known = (df_interest['cases/day']!=-1).sum()\nlen_unknown = (df_interest['cases/day']==-1).sum()\nprint(\"len train: {}, len prediction: {}\".format(len_known, len_unknown))\nX_valid = df_interest[col_var][df_interest['day']>84]\nX_valid2 = df_interest[col_var2][df_interest['day']>84]\npred_f =  np.exp(model.predict(X_valid))-1\npred_c = np.exp(model2.predict(X_valid2))-1\ndf_interest['fatal/day'][df_interest['day']>last_day_train] = pred_f.clip(0, 1e10)\ndf_interest['cases/day'][df_interest['day']>last_day_train] = pred_c.clip(0, 1e10)\ndf_interest['Fatalities'] = np.cumsum(df_interest['fatal/day'].values)\ndf_interest['ConfirmedCases'] = np.cumsum(df_interest['cases/day'].values)\nfor j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n    X_valid = df_interest[col_var].iloc[j+len_known]\n    X_valid2 = df_interest[col_var2].iloc[j+len_known]\n    pred_f = model_pri.predict(X_valid)\n    pred_c = model2_pri.predict(X_valid2)\n    pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n    pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n    df_interest['fatal/day'][j+len_known] = pred_f\n    df_interest['cases/day'][j+len_known] = pred_c\n    df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n    df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n    df_interest = df_interest.drop([\n         'cases/day-(1-1)', 'cases/day-(1-7)', 'cases/day-(8-14)', 'cases/day-(15-21)', \n    'fatal/day-(1-1)', 'fatal/day-(1-7)', 'fatal/day-(8-14)', 'fatal/day-(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',],  axis=1)\n    df_interest = do_aggregations(df_interest.reset_index(drop=True))\n\n# visualize\ntmp = df_interest['cases/day'].values\ntmp = np.cumsum(tmp)\nsns.lineplot(x=df_interest_base['day'], y=tmp, label='pred')\ntmp = df_traintest10['ConfirmedCases'][(df_traintest10['place_id']==place)& (pd.isna(df_traintest10['ForecastId']))].values\nprint(len(tmp), tmp)\nsns.lineplot(x=df_traintest10['day'][(df_traintest10['place_id']==place)& (pd.isna(df_traintest10['ForecastId']))].values,\n             y=tmp, label='true')\nprint(place)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"place = 'Bhutan'\nplace = places[np.random.randint(len(places))]\n# place = 'Iran'\ndf_interest_base = df_traintest10[df_traintest10['place_id']==place].reset_index(drop=True)\ndf_interest = copy.deepcopy(df_interest_base)\ndf_interest['ConfirmedCases'] = df_interest['ConfirmedCases'].astype(np.float)\ndf_interest['cases/day'] = df_interest['cases/day'].astype(np.float)\ndf_interest['fatal/day'] = df_interest['fatal/day'].astype(np.float)\ndf_interest['Fatalities'] = df_interest['Fatalities'].astype(np.float)\ndf_interest['cases/day'][df_interest['day']>last_day_train] = -1\ndf_interest['fatal/day'][df_interest['day']>last_day_train] = -1\n###################标记训练与预测数据\nlen_known = (df_interest['cases/day']!=-1).sum()\nlen_unknown = (df_interest['cases/day']==-1).sum()\nprint(\"len train: {}, len prediction: {}\".format(len_known, len_unknown))\nX_valid = df_interest[col_var][df_interest['day']>84]\nX_valid2 = df_interest[col_var2][df_interest['day']>84]\npred_f =  np.exp(model.predict(X_valid))-1\npred_c = np.exp(model2.predict(X_valid2))-1\ndf_interest['fatal/day'][df_interest['day']>last_day_train] = pred_f.clip(0, 1e10)\ndf_interest['cases/day'][df_interest['day']>last_day_train] = pred_c.clip(0, 1e10)\n########累计确诊是每日确诊相加\ndf_interest['Fatalities'] = np.cumsum(df_interest['fatal/day'].values)\ndf_interest['ConfirmedCases'] = np.cumsum(df_interest['cases/day'].values)\nfor j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n    X_valid = df_interest[col_var].iloc[j+len_known]\n    X_valid2 = df_interest[col_var2].iloc[j+len_known]\n    pred_f = model_pri.predict(X_valid)\n    pred_c = model2_pri.predict(X_valid2)\n    pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n    pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n    df_interest['fatal/day'][j+len_known] = pred_f\n    df_interest['cases/day'][j+len_known] = pred_c\n    df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n    df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n    df_interest = df_interest.drop([\n        'cases/day-(1-1)', 'cases/day-(1-7)', 'cases/day-(8-14)', 'cases/day-(15-21)', \n        'fatal/day-(1-1)', 'fatal/day-(1-7)', 'fatal/day-(8-14)', 'fatal/day-(15-21)', \n        'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n        'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                                   \n                                   ],  axis=1)\n    df_interest = do_aggregations(df_interest.reset_index(drop=True))\n\n# visualize\ntmp = df_interest['fatal/day'].values\ntmp = np.cumsum(tmp)\nsns.lineplot(x=df_interest_base['day'], y=tmp, label='pred')\ntmp = df_traintest10['Fatalities'][(df_traintest10['place_id']==place)& (pd.isna(df_traintest10['ForecastId']))].values\nprint(len(tmp), tmp)\nsns.lineplot(x=df_traintest10['day'][(df_traintest10['place_id']==place)& (pd.isna(df_traintest10['ForecastId']))].values,\n             y=tmp, label='true')\nprint(place)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_interest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict test data in public\nday_before_public = 71\ndf_preds = []\nfor i, place in enumerate(places[:]):\n#     if place!='Japan' and place!='Afghanistan' :continue\n    df_interest = copy.deepcopy(df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True))\n    df_interest['cases/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_public).sum()\n    len_unknown = (day_before_public<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model.predict(X_valid)\n        pred_c = model2.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal/day'][j+len_known] = pred_f\n        df_interest['cases/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n        df_interest = df_interest.drop([\n            'cases/day-(1-1)', 'cases/day-(1-7)', 'cases/day-(8-14)', 'cases/day-(15-21)', \n            'fatal/day-(1-1)', 'fatal/day-(1-7)', 'fatal/day-(8-14)', 'fatal/day-(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases/day'].values)\n    df_preds.append(df_interest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat prediction\ndf_preds= pd.concat(df_preds)\ndf_preds = df_preds.sort_values('day')\ncol_tmp = ['place_id', 'ForecastId', 'day', 'cases/day', 'cases_pred', 'fatal/day', 'fatal_pred',]\ndf_preds[col_tmp][(df_preds['place_id']=='Afghanistan') & (df_preds['day']>75)].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict test data in public\nday_before_private = 84\ndf_preds_pri = []\nfor i, place in enumerate(places[:]):\n#     if place!='Japan' and place!='Afghanistan' :continue\n    df_interest = copy.deepcopy(df_traintest10[df_traintest10['place_id']==place].reset_index(drop=True))\n    df_interest['cases/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_private).sum()\n    len_unknown = (day_before_private<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model_pri.predict(X_valid)\n        pred_c = model2_pri.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal/day'][j+len_known] = pred_f\n        df_interest['cases/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n        df_interest = df_interest.drop([\n            'cases/day-(1-1)', 'cases/day-(1-7)', 'cases/day-(8-14)', 'cases/day-(15-21)', \n            'fatal/day-(1-1)', 'fatal/day-(1-7)', 'fatal/day-(8-14)', 'fatal/day-(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases/day'].values)\n    df_preds_pri.append(df_interest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat prediction\ndf_preds_pri= pd.concat(df_preds_pri)\ndf_preds_pri = df_preds_pri.sort_values('day')\ncol_tmp = ['place_id', 'ForecastId', 'Date', 'day', 'cases/day', 'cases_pred', 'fatal/day', 'fatal_pred',]\ndf_preds_pri[col_tmp][(df_preds_pri['place_id']=='Japan') & (df_preds_pri['day']>79)].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge 2 preds\ndf_preds[df_preds['day']>last_day_train] = df_preds_pri[df_preds['day']>last_day_train]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preds.to_csv(\"df_preds.csv\", index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load sample submission\ndf_sub = pd.read_csv(\"../input/covid19-global-forecasting-week-1/submission.csv\")\nprint(len(df_sub))\ndf_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge prediction with sub\ndf_sub = pd.merge(df_sub, df_traintest3[['ForecastId', 'place_id', 'day']])\ndf_sub = pd.merge(df_sub, df_preds[['place_id', 'day', 'cases_pred', 'fatal_pred']], on=['place_id', 'day',], how='left')\ndf_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub['ConfirmedCases'] = df_sub['cases_pred']\ndf_sub['Fatalities'] = df_sub['fatal_pred']\ndf_sub = df_sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\ndf_sub.to_csv(\"submission.csv\", index=None)\ndf_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}