{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **INTRODUCTION:**\n\nThis is a very simple implementation of XGBoost for this data. One of the great features of XGBoost is that it has built in functionality to easily see the top features F score. This means we can add a lot of features to the model and then easily see what really makes a difference in prediction. This is extremely important for this challenge since it is not really about the leaderboard results, it is more about determining useful features for models. \n\nI am working on collecting publicly available datasets to continue to add in new variables (features) to see what might be useful."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom google.cloud import bigquery","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/covid19-global-forecasting-week-1/train.csv\")\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-1/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add Weather Data"},{"metadata":{},"cell_type":"markdown","source":"We will be doing this using the technique outlined in the great notebook https://www.kaggle.com/davidbnn92/weather-data?scriptVersionId=30695168"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclient = bigquery.Client()\ndataset_ref = client.dataset(\"noaa_gsod\", project=\"bigquery-public-data\")\ndataset = client.get_dataset(dataset_ref)\n\ntables = list(client.list_tables(dataset))\n\ntable_ref = dataset_ref.table(\"stations\")\ntable = client.get_table(table_ref)\nstations_df = client.list_rows(table).to_dataframe()\n\ntable_ref = dataset_ref.table(\"gsod2020\")\ntable = client.get_table(table_ref)\ntwenty_twenty_df = client.list_rows(table).to_dataframe()\n\nstations_df['STN'] = stations_df['usaf'] + '-' + stations_df['wban']\ntwenty_twenty_df['STN'] = twenty_twenty_df['stn'] + '-' + twenty_twenty_df['wban']\n\ncols_1 = ['STN', 'mo', 'da', 'temp', 'min', 'max', 'stp', 'wdsp', 'prcp', 'fog', 'slp']\ncols_2 = ['STN', 'country', 'state', 'call', 'lat', 'lon', 'elev']\nweather_df = twenty_twenty_df[cols_1].join(stations_df[cols_2].set_index('STN'), on='STN')\n\nweather_df.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.spatial.distance import cdist\n\nweather_df['day_from_jan_first'] = (weather_df['da'].apply(int)\n                                   + 31*(weather_df['mo']=='02') \n                                   + 60*(weather_df['mo']=='03')\n                                   + 91*(weather_df['mo']=='04')  \n                                   )\n\nmo = train['Date'].apply(lambda x: x[5:7])\nda = train['Date'].apply(lambda x: x[8:10])\ntrain['day_from_jan_first'] = (da.apply(int)\n                               + 31*(mo=='02') \n                               + 60*(mo=='03')\n                               + 91*(mo=='04')  \n                              )\n\nC = []\nfor j in train.index:\n    df = train.iloc[j:(j+1)]\n    mat = cdist(df[['Lat','Long', 'day_from_jan_first']],\n                weather_df[['lat','lon', 'day_from_jan_first']], \n                metric='euclidean')\n    new_df = pd.DataFrame(mat, index=df.Id, columns=weather_df.index)\n    arr = new_df.values\n    new_close = np.where(arr == np.nanmin(arr, axis=1)[:,None],new_df.columns,False)\n    L = [i[i.astype(bool)].tolist()[0] for i in new_close]\n    C.append(L[0])\n    \ntrain['closest_station'] = C\n\ntrain = train.set_index('closest_station').join(weather_df[['temp', 'min', 'max', 'stp', 'wdsp', 'prcp', 'fog', 'slp']], ).reset_index().drop(['index'], axis=1)\ntrain.sort_values(by=['Id'], inplace=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.spatial.distance import cdist\n\nweather_df['day_from_jan_first'] = (weather_df['da'].apply(int)\n                                   + 31*(weather_df['mo']=='02') \n                                   + 60*(weather_df['mo']=='03')\n                                   + 91*(weather_df['mo']=='04')  \n                                   )\n\nmo = test['Date'].apply(lambda x: x[5:7])\nda = test['Date'].apply(lambda x: x[8:10])\ntest['day_from_jan_first'] = (da.apply(int)\n                               + 31*(mo=='02') \n                               + 60*(mo=='03')\n                               + 91*(mo=='04')  \n                              )\n\nC = []\nfor j in test.index:\n    df = test.iloc[j:(j+1)]\n    mat = cdist(df[['Lat','Long', 'day_from_jan_first']],\n                weather_df[['lat','lon', 'day_from_jan_first']], \n                metric='euclidean')\n    new_df = pd.DataFrame(mat, index=df.ForecastId, columns=weather_df.index)\n    arr = new_df.values\n    new_close = np.where(arr == np.nanmin(arr, axis=1)[:,None],new_df.columns,False)\n    L = [i[i.astype(bool)].tolist()[0] for i in new_close]\n    C.append(L[0])\n    \ntest['closest_station'] = C\n\ntest = test.set_index('closest_station').join(weather_df[['temp', 'min', 'max', 'stp', 'wdsp', 'prcp', 'fog','slp']], ).reset_index().drop(['index'], axis=1)\ntest.sort_values(by=['ForecastId'], inplace=True)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Issue with wdsp and fog column being objects and not numeric, so change this"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"wdsp\"] = pd.to_numeric(train[\"wdsp\"])\ntest[\"wdsp\"] = pd.to_numeric(test[\"wdsp\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"fog\"] = pd.to_numeric(train[\"fog\"])\ntest[\"fog\"] = pd.to_numeric(test[\"fog\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop the two \"y\" columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop([\"Fatalities\", \"ConfirmedCases\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countries = X_train[\"Country/Region\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countries.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop the Id column"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.drop([\"Id\"], axis=1)\nX_test = test.drop([\"ForecastId\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the datatypes, they need to all be int, float, or bool for XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Change the Date column to be a datetime"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['Date']= pd.to_datetime(X_train['Date']) \nX_test['Date']= pd.to_datetime(X_test['Date']) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set the index to the date"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.set_index(['Date'])\nX_test = X_test.set_index(['Date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create time features based on the new Date index"},{"metadata":{"trusted":true},"cell_type":"code","source":"#def create_time_features(df):\n#    \"\"\"\n#    Creates time series features from datetime index\n#    \"\"\"\n#    df['date'] = df.index\n#    df['hour'] = df['date'].dt.hour\n#    df['dayofweek'] = df['date'].dt.dayofweek\n#    df['quarter'] = df['date'].dt.quarter\n#    df['month'] = df['date'].dt.month\n#    df['year'] = df['date'].dt.year\n#    df['dayofyear'] = df['date'].dt.dayofyear\n#    df['dayofmonth'] = df['date'].dt.day\n#    df['weekofyear'] = df['date'].dt.weekofyear\n    \n#    X = df[['hour','dayofweek','quarter','month','year', 'dayofyear','dayofmonth','weekofyear']]\n#    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create_time_features(X_train)\n#create_time_features(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train.drop(\"date\", axis=1, inplace=True)\n#X_test.drop(\"date\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding more variables to the mix"},{"metadata":{},"cell_type":"markdown","source":"I think now would be a good time to add a couple more variables from outside datasets to this mix to see if any of their data could provide further insight in our predictions. I have collected a couple from the World Bank as well as the UN. These datasets are nice since they have data listed for almost all countries in the world.\n\nLets start with the World Happiness Index dataset from the UN. It has some information related to GINI Coefficients, \"social support\", \"Healthy Life Expectancy at Birth\", Generosity, and Perceptions of Corruption. These indicators could capture some ideas around the healthcare setups in each country and also broad societal differences. They are very generic and broad so I wouldnt expect them to be extremely useful, and even if they show up as very informative on our predictions it would be tough to really break out true actionable insights from them but it is somewhere to start.\n\nWe will just grab the most recent value for each country (most this is 2018) to begin with. If you wanted to get a little more in depth you could probably take an average of the last 5 years or something like that but for now we will stay simple."},{"metadata":{"trusted":true},"cell_type":"code","source":"world_happiness_index = pd.read_csv(\"../input/world-bank-datasets/World_Happiness_Index.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_happiness_grouped = world_happiness_index.groupby('Country name').nth(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_happiness_grouped.drop(\"Year\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(left=X_train, right=world_happiness_grouped, how='left', left_on='Country/Region', right_on='Country name')\nX_test = pd.merge(left=X_test, right=world_happiness_grouped, how='left', left_on='Country/Region', right_on='Country name')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"malaria_world_health = pd.read_csv(\"../input/world-bank-datasets/Malaria_World_Health_Organization.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(left=X_train, right=malaria_world_health, how='left', left_on='Country/Region', right_on='Country')\nX_test = pd.merge(left=X_test, right=malaria_world_health, how='left', left_on='Country/Region', right_on='Country')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(\"Country\", axis=1, inplace=True)\nX_test.drop(\"Country\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"human_development_index = pd.read_csv(\"../input/world-bank-datasets/Human_Development_Index.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(left=X_train, right=human_development_index, how='left', left_on='Country/Region', right_on='Country')\nX_test = pd.merge(left=X_test, right=human_development_index, how='left', left_on='Country/Region', right_on='Country')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop([\"Country\", \"Gross national income (GNI) per capita 2018\"], axis=1, inplace=True)\nX_test.drop([\"Country\", \"Gross national income (GNI) per capita 2018\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"night_ranger_predictors = pd.read_csv(\"../input/covid19-demographic-predictors/covid19_by_country.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#There is a duplicate for Georgia in this dataset from Night Ranger, causing merge issues so we will just drop the Georgia rows\nnight_ranger_predictors = night_ranger_predictors[night_ranger_predictors.Country != \"Georgia\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(left=X_train, right=night_ranger_predictors, how='left', left_on='Country/Region', right_on='Country')\nX_test = pd.merge(left=X_test, right=night_ranger_predictors, how='left', left_on='Country/Region', right_on='Country')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Now transform some data columns to boolean\n"},{"metadata":{},"cell_type":"markdown","source":"\n drop some of the columns including the \"Total Infected\", \"Total Deaths\" columns as these are what we are trying to get our model to predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop([\"Country\", \"Restrictions\", \"Quarantine\", \"Schools\",\"Total Infected\", \"Total Deaths\"], axis=1, inplace=True)\nX_test.drop([\"Country\", \"Restrictions\", \"Quarantine\", \"Schools\",\"Total Infected\", \"Total Deaths\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One hot encode the Provice/State and the Country/Region columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.concat([X_train,pd.get_dummies(X_train['Province/State'], prefix='ps')],axis=1)\nX_train.drop(['Province/State'],axis=1, inplace=True)\nX_test = pd.concat([X_test,pd.get_dummies(X_test['Province/State'], prefix='ps')],axis=1)\nX_test.drop(['Province/State'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.concat([X_train,pd.get_dummies(X_train['Country/Region'], prefix='cr')],axis=1)\nX_train.drop(['Country/Region'],axis=1, inplace=True)\nX_test = pd.concat([X_test,pd.get_dummies(X_test['Country/Region'], prefix='cr')],axis=1)\nX_test.drop(['Country/Region'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Grab the \"y\" variable we want to predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train[\"Fatalities\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = xgb.XGBRegressor(n_estimators=1000, subsample=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.fit(X_train, y_train, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = plot_importance(reg, height=0.9, max_num_features=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use percentage change in the y variable instead of raw numbers\n\nI think another interesting way to look at this might be through percentage change of the y variable. We really care about the percentage change in fatalities from day to day not the total number. This is because we know areas where the infection has been for a longer period of time would automatically have a higher total number, while areas with relatively new infection would have a lower total number of deaths but quite possible a higher percentage change since the virus is spreading more rapidly."},{"metadata":{},"cell_type":"markdown","source":"Get the percentage change for each country, using one day lag. Would be interesting to play around with the lag time (periods) to see if this changes the analysis, my first thought would be to change this to 7 day (one week)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_train = train.groupby([\"Country/Region\"]).Fatalities.pct_change(periods=1)\ny_train=train.Fatalities.applymap(lambda x: np.log(x+1))\n\ny_train.Fatalities[y_train.Fatalities.isnull()] = -1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are issues with pct_change function returning NaN when doing percentage change from 0 to 0, so just change these to 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train.replace(np.nan, -1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are also issues with pct_change function sometimes returning \"inf\" when going from 0 to 0, so just change these to 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train.replace(np.inf, -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = xgb.XGBRegressor(n_estimators=1000, subsample=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.fit(X_train, y_train, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = plot_importance(reg, height=0.9, max_num_features=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Change y variable to Confirmed Cases\n\nWe will use the same train data as above but lets change the y variable to be Confirmed Cases and see if anything changes."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train[\"ConfirmedCases\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = xgb.XGBRegressor(n_estimators=1000, subsample=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.fit(X_train, y_train, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = plot_importance(reg, height=0.9, max_num_features=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_train = train.groupby([\"Country/Region\"]).ConfirmedCases.pct_change(periods=1)\ny_train=train.ConfirmedCases.applymap(lambda x: np.log(x+1))\n\ny_train.ConfirmedCases[y_train.ConfirmedCases.isnull()] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train.replace(np.nan, -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train.replace(np.inf, -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = xgb.XGBRegressor(n_estimators=1000, subsample=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.fit(X_train, y_train, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = plot_importance(reg, height=0.9, max_num_features=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Try running on test data and submitting results"},{"metadata":{},"cell_type":"markdown","source":"One thing to note is that fatalities will always go up, so we will want to adjust any prediction that is less than the previous days prediction to be equal to the previous day. Also the same is true with confirmed cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train[\"ConfirmedCases\"]\nconfirmed_reg = xgb.XGBRegressor(n_estimators=1000, subsample=0.5)\nconfirmed_reg.fit(X_train, y_train, verbose=True)\npreds = confirmed_reg.predict(X_test)\npreds = preds.applymap(lambda x: np.exp(x)-1) #undo log transformation \npreds = np.array(preds)\npreds[preds < 0] = 0\npreds = np.round(preds, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.array(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissionOrig = pd.read_csv(\"../input/covid19-global-forecasting-week-1/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissionOrig[\"ConfirmedCases\"]=pd.Series(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in submissionOrig.iterrows():\n    if index >= 1:\n        if submissionOrig.iloc[index, 'ConfirmedCases'] < submissionOrig.iloc[index - 1, 'ConfirmedCases']:\n            submissionOrig.at[index, 'ConfirmedCases'] = submissionOrig.iloc[index - 1,'ConfirmedCases']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissionOrig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train[\"Fatalities\"]\nconfirmed_reg = xgb.XGBRegressor(n_estimators=1000, subsample=0.5)\nconfirmed_reg.fit(X_train, y_train, verbose=True)\npreds = confirmed_reg.predict(X_test)\npreds = preds.applymap(lambda x: np.exp(x)-1) #preds = np.exp(preds)-1 #undo log transformation\npreds = np.array(preds)\npreds[preds < 0] = 0\npreds = np.round(preds, 0)\nsubmissionOrig[\"Fatalities\"]=pd.Series(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissionOrig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in submissionOrig.iterrows():\n    if index >= 1:\n        if submissionOrig.iloc[index, 'Fatalities'] < submissionOrig.iloc[index - 1, 'Fatalities']:\n            submissionOrig.at[index, 'Fatalities'] = submissionOrig.iloc[index - 1,'Fatalities']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissionOrig.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}