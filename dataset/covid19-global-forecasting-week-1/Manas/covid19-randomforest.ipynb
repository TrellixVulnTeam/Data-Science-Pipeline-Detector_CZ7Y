{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from datetime import datetime\n\nimport pandas as pd\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import tokenize\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn import metrics\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\nimport os\nimport sys\nimport numpy as np\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D\nfrom keras.layers import MaxPool1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adam\n\ntrain = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-1/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-1/test.csv\")\n\n\n\nprint(train.columns)\nprint(test.columns)\n\nprint(len(train['Date'].unique()))\nprint(len(test['Date'].unique()))\n\nprint(train.info())\n#print(test.summary)\n\ntotal = train.isnull().sum()\nprint(total)\n\ntrain['Long'].describe()\n\nprint(train['Lat'].isnull().sum())\n\ntrain['Lat']=train['Lat'].ffill(axis=0)\n\ntrain['Long']= train['Long'].ffill(axis=0)\ntest['Lat']=test['Lat'].ffill(axis=0)\ntest['Long']=test['Long'].ffill(axis=0)\n\ntrain['Province/State'] = train['Province/State'].fillna(train['Country/Region'])\ntest['Province/State'] = test['Province/State'].fillna(test['Country/Region'])\n\ntotal = train.isnull().sum()\nprint(total)\ntotal = test.isnull().sum()\nprint(total)\n\ndata=[train,test]\nll=[]\ny1 = train['Date'].unique()\ny2 = test['Date'].unique()\n\nprint(len(y1))\n\nin_first = set(y1)\nin_second = set(y2)\n\nin_second_but_not_in_first = in_second - in_first\n\nresult = y1\nprint(result)\n\nl1=[]\nfor i in y1:\n  l1.append(i)\n\nfor i in y2:\n  l1.append(i)\n  \nprint(len(l1))\n\ntt = set(l1)\nprint(len(tt))\npp=[]\nfor i in tt:\n  pp.append(i)\npp.sort(key=lambda date: datetime.strptime(date, \"%Y-%m-%d\"))\nprint(pp)\n#print(tt)\ndates={}\nk=0\nfor i in pp:\n  dates[i]=k\n  k=k+1\n\nprint(dates)\n\ndata = [train,test]\nfor dataset in data:\n    dataset['Date1'] = dataset['Date'].map(dates)\n\nprint(train['Date1'])\n\nimport nltk\nnltk.download('wordnet')\n\nl1=[]\nfor i in train['Country/Region']:\n  l1.append(i)\n\nfor i in train['Province/State']:\n  l1.append(i)\n\ntokenizer  = Tokenizer()\ntokenizer.fit_on_texts(l1)\nsequences =  tokenizer.texts_to_sequences(l1)\n\n\n\ntrain['CP']=\"\"\ntrain['S']=\"\"\nfor i in range(0,len(train)):\n  train['CP'][i] = sequences[i]\n\nprint(i)\nfor j in range(0,len(train)):\n  i = i+1\n  train['S'][j] = sequences[i]\n\nl2=[]\nfor i in test['Country/Region']:\n  l2.append(i)\n\nfor i in test['Province/State']:\n  l2.append(i)\n\n\ntokenizer.fit_on_texts(l2)\nsequences1=  tokenizer.texts_to_sequences(l2)\n\ntest['CP']=\"\"\ntest['S']=\"\"\nfor i in range(0,len(test)):\n  test['CP'][i] = sequences1[i]\n\nprint(i)\nfor j in range(0,len(test)):\n  i = i+1\n  test['S'][j] = sequences1[i]\n\nt1 = set(l1)\ncountries={}\nk=0\nfor i in t1:\n  countries[i]=k\n  k=k+1\n\ndata = [train,test]\nfor dataset in data:\n    dataset['CP2'] = dataset['Country/Region'].map(countries)\n    dataset['S2'] = dataset['Province/State'].map(countries)\n\n\ntrain_df = train\ntest_df = test\n\nprint(train_df.columns)\n\nX_train=train_df.drop(\"Id\",axis=1).copy()\nX_train=X_train.drop(\"Province/State\",axis=1)\nX_train=X_train.drop(\"Country/Region\",axis=1)\nX_train=X_train.drop(\"Date\",axis=1)\nX_train=X_train.drop(\"CP\",axis=1)\nX_train=X_train.drop(\"S\",axis=1)\n\nX_test=test_df.drop(\"ForecastId\",axis=1).copy()\nX_test=X_test.drop(\"Province/State\",axis=1)\nX_test=X_test.drop(\"Country/Region\",axis=1)\nX_test=X_test.drop(\"Date\",axis=1)\nX_test=X_test.drop(\"CP\",axis=1)\nX_test=X_test.drop(\"S\",axis=1)\n\nprint(X_train.columns)\nprint(X_test.columns)\n\n\n\nY_train = X_train['Fatalities']\nX_train = X_train.drop(\"Fatalities\",axis=1)\nY_train1 = X_train['ConfirmedCases']\nX_train = X_train.drop(\"ConfirmedCases\",axis=1)\n\nprint(X_train.columns)\n\n\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train1)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train1)\nacc_random_forest = round(random_forest.score(X_train, Y_train1) * 100, 2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.columns)\nprint(X_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['CC']=\"\"\nfor i in range(0,len(Y_prediction)):\n  X_test['CC'][i] = Y_prediction[i]\n\nX_train['CC']=\"\"\nfor i in range(0,len(Y_train1)):\n  X_train['CC'][i] = Y_train1[i]\n\nprint(X_train.columns)\nprint(X_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XX = X_train[['Lat','Long','Date1','CC','CP2','S2']]\nprint(XX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XX1 = X_test[['Lat','Long','Date1','CC','CP2','S2']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk import tokenize\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn import metrics\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.decomposition import PCA\n'''\npca = PCA(n_components=2)\nX_train_count =pca.fit_transform(X_train) \nX_test_counts = pca.transform(X_test)\n'''\n\nsvd = TruncatedSVD(n_components=2)\nnormalizer = Normalizer(copy=False)\nlsa = make_pipeline(svd, normalizer)\n\nX_train_count = lsa.fit_transform(XX)\nX_test_counts = lsa.transform(XX1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = set(train['Fatalities'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=190)\nrandom_forest.fit(X_train_count, Y_train)\n\nY_prediction = random_forest.predict(X_test_counts)\n\nrandom_forest.score(X_train_count, Y_train)\nacc_random_forest = round(random_forest.score(X_train_count, Y_train) * 100, 2)\n\nprint(acc_random_forest)\n\nimport csv\nfields = ['ForecastId','ConfirmedCases','Fatalities'] \nwith open(\"submission.csv\", 'w') as csvfile: \n    # creating a csv writer object \n    csvwriter = csv.writer(csvfile) \n      \n    # writing the fields \n    csvwriter.writerow(fields) \n      \n    # writing the data rows \n    for i in range(0,len(Y_prediction)):\n      csvwriter.writerow([test['ForecastId'][i],X_test['CC'][i],Y_prediction[i]])\n      ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}