{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing training dataset \n* Understanding how convid19 virus spreads globally \n* Observe features might be helpful doing prediction "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-1/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_df = train_df.groupby(['Country/Region', 'Date']).sum().reset_index()\ngrouped_df.head()\nlast_date = train_df.Date.max()\nlatest_grouped = grouped_df[grouped_df['Date']== last_date]\nlatest_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Overlay the global map with the number of comfirmed cases\n* We can observe the most inffected countries "},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\nfig = px.choropleth(latest_grouped, locations=\"Country/Region\", \n                    locationmode='country names', color=\"ConfirmedCases\", \n                    hover_name=\"Country/Region\", range_color=[1,5000], \n                    color_continuous_scale=\"portland\", \n                    title='Countries with Confirmed Cases')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Observe how covid-19 virus spreads over time in 10 countries having most confirmed cases. \n* We can observe the accumulated number of comfirmed cases and fatalities are highly related to \n* Geographic location -\nFor some areas - europe and north asia, there are several neighboring countries get severly infected sinultaneously \n\n* The country - \ntake Iran as an example, no neighboring countries are severly infected. The situation gets serious due to how Iran government handle it. [ref](https://www.mirrormedia.mg/story/20200307intirancoronavirusone/)\n\n* The recent number and the growth velocity of comfirmed cases/fatalities are highly related to forthcoming numbers "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom matplotlib.colors import ListedColormap, LinearSegmentedColormap\nfrom collections import OrderedDict\ndef get_color_map(count): \n    viridis = cm.get_cmap('coolwarm', count)\n    return viridis(np.linspace(0, 1, count))\n\ndef plot_by_country_and_date(grouped_df, region='Country/Region', target='ConfirmedCases'):\n    cdate = grouped_df['Date'].unique()\n    countries = grouped_df[region].unique()\n\n    colors = get_color_map(len(countries)) \n    plt.figure(figsize=(15,10),dpi=100,linewidth = 2)\n    grouped_df = grouped_df.sort_values(by=['Date'])\n    \n    for i, c in enumerate(countries): \n        cur_Y = grouped_df[grouped_df[region] == c][[target]]\n        plt.plot(cdate,cur_Y,'o-',color = colors[i], label=c)\n    date_tick = [] \n    for i, d in enumerate(cdate):\n        if i % 7 == 0:\n            date_tick.append(d)\n        \n    plt.xticks(date_tick,fontsize=10)\n    plt.yticks(fontsize=20)\n    plt.xlabel(\"date\", fontsize=20)\n    plt.ylabel(target, fontsize=20)\n    plt.yscale('log')\n    plt.legend(loc = \"best\", fontsize=10)\n    plt.show() \n    \n    \nlatest_grouped= latest_grouped.sort_values(by='ConfirmedCases') \ntop_10 = latest_grouped.iloc[-10:,:]['Country/Region'].unique()\nbotton_10 = latest_grouped.iloc[:10,:]['Country/Region'].unique()\ngrouped_df['top10'] = grouped_df['Country/Region'].apply(lambda x: x in top_10)\ngrouped_df['botton10'] = grouped_df['Country/Region'].apply(lambda x: x in botton_10)\n\ndata_top_10 = grouped_df[grouped_df['top10']]\nplot_by_country_and_date(data_top_10, target='ConfirmedCases')\nplot_by_country_and_date(data_top_10, target='Fatalities')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Observe how covid-19 spreads over time for each province in China\n* Can see that provinces neighboring Wubei are severly infected. \n* Coastal provinces cannot get rid of being infected, either. Because there are more job opportunities in coastal cities; therefore there are more migrants. \n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import geopandas as gpd\nimport math\nprovince_shp = gpd.read_file('/kaggle/input/chinaprovince/gadm36_CHN_1.shp')\nchina_df = train_df[train_df['Country/Region']=='China']\ng_p_china_df = china_df[china_df['Date']==last_date]\ng_p_china_df  = china_df.groupby(['Province/State']).sum().reset_index()\ng_p_china_df['ConfirmedCasesLog'] = g_p_china_df.ConfirmedCases.apply(lambda x: math.log(x+1))\nall_df = province_shp.merge(g_p_china_df, left_on=('NAME_1'), right_on=('Province/State'))\nall_df.plot(column='ConfirmedCasesLog', cmap='OrRd')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ng_china_df = china_df.groupby(['Province/State','Date']).sum().reset_index()\n\nplot_by_country_and_date(g_china_df, region='Province/State', target='ConfirmedCases')\nplot_by_country_and_date(g_china_df, region='Province/State', target='Fatalities')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We use xgboost to observe important features \n* features - \n  time series consists of the number of confirmed cases and day over day diff of confirmed cases \n  \n  time series consists of the number of fatalities and day over day diff of the number of fatalities \n  \n  Lat, Long\n  \n  Location - Province/State + Country/Region\n  \n  Days since 2020/01/22\n  \n  Target - ConfirmedCases "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_time_feature_config = {\n    \"ConfirmedCases\": 21,\n    \"Fatalities\": 21,\n}\nfrom datetime import datetime\ndate_format = \"%Y-%m-%d\"\nmin_date = datetime.strptime(train_df['Date'].min(), date_format)\nprint(min_date) \n\nm_train_df = train_df[['Date', 'Province/State', 'Country/Region']]\nm_train_df = m_train_df.fillna('')\nm_train_df['Loc']=m_train_df['Province/State']+m_train_df['Country/Region']\nm_train_df = pd.get_dummies(m_train_df, columns=['Loc'], prefix = ['loc'])\nm_train_df['days'] = m_train_df.Date.apply(lambda x: (datetime.strptime(x, date_format) - min_date).days)\nm_train_df = pd.concat([m_train_df, train_df[['Lat','Long']]], axis=1)\n\n\n\nm_train_df = m_train_df.drop([ 'Province/State', 'Country/Region'], axis=1)\n\ncolumns = list(m_train_df.columns)\nfor k, v in train_time_feature_config.items():\n    print('key:'+k)\n    lag = v\n    target = train_df[[k]]\n    np_target = target.to_numpy()\n    columns.append(k)\n    for j in range(1, v):\n        columns.append(k+\":\"+str(j))\n        columns.append(k+\"diff:\"+str(j))\n        next_target = target.shift(j)\n        next_target = next_target.to_numpy()\n        diff_target = target - next_target\n        np_target = np.concatenate((np_target, next_target, diff_target), axis=1)\n    m_train_df = np.concatenate((m_train_df, np_target), axis=1)\n\ntarget = train_df['ConfirmedCases'].shift(-1).to_numpy()\nm_train_df = np.concatenate((m_train_df, target.reshape(target.shape[0],1)), axis=1)\ncolumns.append('target1d')\nm_train_df = pd.DataFrame(data=m_train_df, columns=columns)    \nm_train_df = m_train_df.dropna()\nm_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport xgboost\n\nxgb = xgboost.XGBRegressor(max_depth=7,eval_metric='rmse', reg_lambda=1, num_round=1000)\n\nfeatures = m_train_df.iloc[:,2:-1]\ndata = np.array(features.iloc[0,:].values, copy=False, dtype=np.float32)\n \nX_train, X_test, y_train, y_test = train_test_split(features.values, m_train_df.target1d.values,\n                                                    train_size=0.75, test_size=0.25, random_state=42)\nxgb.fit(X_train,y_train)\nrpredictions = xgb.predict(X_test)\npredictions = [] \nfor p in rpredictions: \n    if p<0: \n        p=0\n    predictions.append(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math \nimport matplotlib.pyplot as plt\nimport numpy as np \n\npredictions = np.array(predictions)\n\nlog_test = np.array(np.log((y_test+1).tolist()))\nlog_predictions = np.array(np.log((predictions+1).tolist()))\n\n\n\nprint( 'rmsle')\nprint( math.sqrt(((log_test - log_predictions)**2).mean()))\nimportances = [] \nfor i, im in enumerate(xgb.feature_importances_):\n    importances.append((columns[i],im))\n    \nimportances = list(reversed(sorted(importances, key=lambda x: x[1]))) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = importances[:10]\nweight = []\nname = []\nfor i, im in enumerate(importances): \n    weight.append(im[1])\n    name.append(im[0])\n\ny_pos = np.arange(len(weight))\nplt.figure(figsize=(30,10))\nplt.bar(y_pos, weight, align='center', alpha=0.5)\nplt.xticks(y_pos, name)\nplt.ylabel('feature importance')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above experiment, important features are \n1. Day over day increase of fatalities and confirmed cases \n2. Number of fatalities and confirmed cases \n2. Lat - geolocation \n3. loc_zambia ?? need to explore more why this feature is important. Perhaps the distribution pattern in zambia is quite different from other areas. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}