{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('/kaggle/input/covidwk1/ca_train1.csv')\ntest = pd.read_csv('/kaggle/input/covidwk1/ca_test1.csv')\ntrain.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.rename(columns={'Id': 'ForecastId'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing of Train and test Data together\n# Concatenate Train and Test Data with a new column to identify Train (Type=0) VS Test Data(Type=1)  \ntrain['Type']=pd.DataFrame(np.zeros(len(train)).astype(int))\ntest['Type']=pd.DataFrame(np.ones(len(test)).astype(int))\n\nprint('Original Train Data shape:{} and Test Data shape:{}'.format(train.shape,test.shape))\n\nfeatures_Data=pd.concat([train.drop(columns=['ConfirmedCases','Fatalities']),test])\nfeatures_Data.drop(columns=['ForecastId','Province/State','Country/Region','Lat','Long'],inplace=True)\nprint('features_Data shape after dropping unnecesary columns ',features_Data.shape)\n\n# Lets deal with Date columns\n# First convert the  column to Date time data type\nfeatures_Data['Date']=pd.to_datetime(features_Data['Date'])\n\n# We can also extract week, day, dayofweek,dayofyear and create separate columns\nfeatures_Data.insert(1,'Week',features_Data['Date'].dt.week)\nfeatures_Data.insert(2,'Day',features_Data['Date'].dt.day)\nfeatures_Data.insert(3,'DayofWeek',features_Data['Date'].dt.dayofweek)\nfeatures_Data.insert(4,'DayofYear',features_Data['Date'].dt.dayofyear)\n\n# Check for Null Values\nprint('Null Value Check:\\n',features_Data.isnull().sum())\n\ntrain_Data=features_Data[features_Data.Type==0].drop(columns=['Date','Type'])\ntest_Data=features_Data[features_Data.Type==1].drop(columns=['Date','Type'])\ndel features_Data\nprint('Final Train Data shape:{} and Test Data shape:{}'.format(train_Data.shape,test_Data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_Data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract features and label\nfeatures=train_Data.values\nlabel1=train.ConfirmedCases.values\nlabel2=train.Fatalities.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets apply Stratified K-Fold Cross Validation\nfrom sklearn.model_selection import StratifiedKFold\n#from sklearn.metrics import f1_score,classification_report,confusion_matrix\nfrom sklearn.metrics import mean_squared_error,median_absolute_error,r2_score\n\ndef stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y):\n    global df_model_selection\n    \n    skf = StratifiedKFold(n_splits, random_state=12,shuffle=True)\n    \n    weighted_r2_score = []\n    \n    for train_index, test_index in skf.split(X,y):\n        X_train, X_test = X[train_index], X[test_index] \n        y_train, y_test = y[train_index], y[test_index]\n        \n        \n        model_obj.fit(X_train, y_train)\n        test_ds_predicted = model_obj.predict( X_test )  \n           \n        weighted_r2_score.append(round(r2_score(y_true=y_test, y_pred=test_ds_predicted),2))\n        \n    sd_weighted_r2_score = np.std(weighted_r2_score, ddof=1)\n    range_of_r2_scores = \"{}-{}\".format(min(weighted_r2_score),max(weighted_r2_score))    \n    df_model_selection = pd.concat([df_model_selection,pd.DataFrame([[process,model_name,sorted(weighted_r2_score),range_of_r2_scores,sd_weighted_r2_score]], columns =COLUMN_NAMES) ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBRFRegressor\nfrom sklearn.neighbors import KNeighborsRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling for Predicting ConfirmedCases ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"COLUMN_NAMES = [\"Process\",\"Model Name\", \"r2 Scores\",\"Range of r2 Scores\",\"Std Deviation of r2 Scores\"]\ndf_model_selection = pd.DataFrame(columns=COLUMN_NAMES)\n\nprocess='ConfirmedCases Prediction'\nn_splits = 5\nX=features\ny=label1\n\n\n# 1.LinearRegression\nmodel_LR=LinearRegression()\nmodel_obj=model_LR\nmodel_name='LinearRegression'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # 2.BayesianRidge\nmodel_BR=BayesianRidge()\nmodel_obj=model_BR\nmodel_name='BayesianRidgeRegressor'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # 3.DecisionTreeRegressor\nmodel_DTR=DecisionTreeRegressor()\nmodel_obj=model_DTR\nmodel_name='DecisionTreeRegressor'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # 4.RandomForestRegressor\nmodel_RFR=RandomForestRegressor()\nmodel_obj=model_RFR\nmodel_name='RandomForestRegressor'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # 5.GradientBoostingRegressor\nmodel_GBR=GradientBoostingRegressor()\nmodel_obj=model_GBR\nmodel_name='GradientBoostingRegressor'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # # 6.XGBRegressor\nmodel_XGBR=XGBRegressor()\nmodel_obj=model_XGBR\nmodel_name='XGBRegressor'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # # 7.XGBRFRegressor\nmodel_XGBRFR=XGBRFRegressor()\nmodel_obj=model_XGBRFR\nmodel_name='XGBRFRegressor'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# 8.KNeighborsRegressor\nmodel_KNNR=KNeighborsRegressor()\nmodel_obj=model_KNNR\nmodel_name='KNeighborsRegressor'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n\ndf_model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## From Above Results GradientBoostingRegressor,XGBRFRegressor,RandomForestRegressor seems to be predicting better \n\n# Now lets try to get the Scores using StratifiedKFold Cross Validation\n\n#Initialize the algo\nmodel=GradientBoostingRegressor()\n\n#Initialize StratifiedKFold Method\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=5, \n              random_state=1,\n              shuffle=True)\n\n#Initialize For Loop \n\ni=0\nfor train,test in kfold.split(features,label1):\n    i = i+1\n    X_train,X_test = features[train],features[test]\n    y_train,y_test = label1[train],label1[test]\n    \n    model.fit(X_train,y_train)\n    test_ds_predicted=model.predict(X_test)\n    train_ds_predicted=model.predict(X_train)\n    \n    test_r2_score=round(r2_score(y_true=y_test, y_pred=test_ds_predicted ),2)\n    train_r2_score=round(r2_score(y_true=y_train, y_pred=train_ds_predicted ),2)\n    \n    print(\"Train r2-Score: {}, Test r2-score: {}, for Sample Split: {}\".format(train_r2_score,test_r2_score,i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets extract the Train and Test sample for split 1\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=5, \n              random_state=1,\n              shuffle=True)\ni=0\nfor train,test in kfold.split(features,label1):\n    i = i+1\n    if i == 1:\n        X_train,X_test,y_train,y_test = features[train],features[test],label1[train],label1[test]\n\n\n########################################################################################################\n#Final Model\nfinalModel=GradientBoostingRegressor()\nfinalModel.fit(X_train,y_train)\n\ntest_ds_predicted=finalModel.predict(X_test)\ntrain_ds_predicted=finalModel.predict(X_train)\n\n\ntest_r2_score=round(r2_score(y_true=y_test, y_pred=test_ds_predicted ),2)\ntrain_r2_score=round(r2_score(y_true=y_train, y_pred=train_ds_predicted ),2)\n\nprint(\"Train r2-Score: {}, Test r2-score: {}\".format(train_r2_score,test_r2_score))\n\n\ntrain_score=np.round(finalModel.score(X_train,y_train),2)\ntest_score=np.round(finalModel.score(X_test,y_test),2)\nprint('Train Accuracy Score is:{} and  Test Accuracy Score:{}'.format(train_score,test_score))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction of ConfirmedCases","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-1/test.csv')\ndf_submission=pd.DataFrame(finalModel.predict(test_Data),columns=['ConfirmedCases'])\ndf_submission.insert(0,'ForecastId',test.ForecastId)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling for Predicting Fatalities ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"COLUMN_NAMES = [\"Process\",\"Model Name\", \"r2 Scores\",\"Range of r2 Scores\",\"Std Deviation of r2 Scores\"]\ndf_model_selection = pd.DataFrame(columns=COLUMN_NAMES)\n\nprocess='Fatalities Prediction'\nn_splits = 5\nX=features\ny=label2\n\n\n# 1.LinearRegression\nmodel_LR=LinearRegression()\nmodel_obj=model_LR\nmodel_name='LinearRegression'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # 2.BayesianRidge\nmodel_BR=BayesianRidge()\nmodel_obj=model_BR\nmodel_name='BayesianRidgeRegressor'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # 3.DecisionTreeRegressor\nmodel_DTR=DecisionTreeRegressor()\nmodel_obj=model_DTR\nmodel_name='DecisionTreeRegressor'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # 4.RandomForestRegressor\nmodel_RFR=RandomForestRegressor()\nmodel_obj=model_RFR\nmodel_name='RandomForestRegressor'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # 5.GradientBoostingRegressor\nmodel_GBR=GradientBoostingRegressor()\nmodel_obj=model_GBR\nmodel_name='GradientBoostingRegressor'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # # 6.XGBRegressor\nmodel_XGBR=XGBRegressor()\nmodel_obj=model_XGBR\nmodel_name='XGBRegressor'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # # 7.XGBRFRegressor\nmodel_XGBRFR=XGBRFRegressor()\nmodel_obj=model_XGBRFR\nmodel_name='XGBRFRegressor'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# 8.KNeighborsRegressor\nmodel_KNNR=KNeighborsRegressor()\nmodel_obj=model_KNNR\nmodel_name='KNeighborsRegressor'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n\ndf_model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## From Above Results GradientBoostingRegressor seems to be predicting better \n# Now lets try to get the Scores using StratifiedKFold Cross Validation\n\n#Initialize the algo\nmodel=GradientBoostingRegressor()\n\n#Initialize StratifiedKFold Method\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=5, \n              random_state=1,\n              shuffle=True)\n\n#Initialize For Loop \n\ni=0\nfor train,test in kfold.split(features,label2):\n    i = i+1\n    X_train,X_test = features[train],features[test]\n    y_train,y_test = label2[train],label2[test]\n    \n    model.fit(X_train,y_train)\n    test_ds_predicted=model.predict(X_test)\n    train_ds_predicted=model.predict(X_train)\n    \n    test_r2_score=round(r2_score(y_true=y_test, y_pred=test_ds_predicted ),2)\n    train_r2_score=round(r2_score(y_true=y_train, y_pred=train_ds_predicted ),2)\n    \n    print(\"Train r2-Score: {}, Test r2-score: {}, for Sample Split: {}\".format(train_r2_score,test_r2_score,i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets extract the Train and Test sample for split 5\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=5, #n_splits should be equal to no of cv value in cross_val_score\n              random_state=1,\n              shuffle=True)\ni=0\nfor train,test in kfold.split(features,label2):\n    i = i+1\n    if i == 5:\n        X_train,X_test,y_train,y_test = features[train],features[test],label2[train],label2[test]\n\n\n########################################################################################################\n#Final Model\nfinalModel=GradientBoostingRegressor()\nfinalModel.fit(X_train,y_train)\n\ntest_ds_predicted=finalModel.predict(X_test)\ntrain_ds_predicted=finalModel.predict(X_train)\n\n\ntest_r2_score=round(r2_score(y_true=y_test, y_pred=test_ds_predicted ),2)\ntrain_r2_score=round(r2_score(y_true=y_train, y_pred=train_ds_predicted ),2)\n\nprint(\"Train r2-Score: {}, Test r2-score: {}\".format(train_r2_score,test_r2_score))\n\n\ntrain_score=np.round(finalModel.score(X_train,y_train),2)\ntest_score=np.round(finalModel.score(X_test,y_test),2)\nprint('Train Accuracy Score is:{} and  Test Accuracy Score:{}'.format(train_score,test_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction of Fatalities","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.insert(2,'Fatalities',pd.DataFrame(finalModel.predict(test_Data),columns=['Fatalities']))\ndf_submission.astype('int').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.astype('int').to_csv('submission.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}