{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport json\nimport geopandas as gpd\nimport seaborn as sns\nfrom matplotlib import rcParams, pyplot as plt, style as style\nfrom bokeh.io import output_notebook, show, output_file\nfrom bokeh.plotting import figure\nfrom bokeh.models import GeoJSONDataSource, LinearColorMapper, ColorBar\nfrom bokeh.palettes import viridis\n\ntrain_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-1/train.csv\", index_col = 'Id')\ntest_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-1/test.csv\", index_col = 'ForecastId')\npd.set_option('display.max_columns', 150)\npd.set_option('display.max_rows', 150)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Data cleaning: removing typos\n\ntrain_df.loc[(train_df['Country/Region'] =='Bahrain') & (train_df['Date'] == '2020-03-13'),'ConfirmedCases'] = 198\ntrain_df.loc[(train_df['Country/Region'] =='Japan')& (train_df['Date'] == '2020-01-23'),'ConfirmedCases'] = 2\ntrain_df.loc[(train_df['Country/Region'] =='Japan')& (train_df['Date'] == '2020-02-06'),'ConfirmedCases'] = 25\ntrain_df.loc[(train_df['Country/Region'] =='Japan')& (train_df['Date'] == '2020-03-16'),'ConfirmedCases'] = 855\ntrain_df.loc[(train_df['Country/Region'] =='Lebanon')& (train_df['Date'] == '2020-03-16'),'ConfirmedCases'] = 119\ntrain_df.loc[(train_df['Country/Region'] =='Montenegro')& (train_df['Date'] == '2020-03-18'),'ConfirmedCases'] = 3\ntrain_df.loc[(train_df['Country/Region'] =='Azerbaijan') & (train_df['Date'] == '2020-03-16'),'ConfirmedCases'] = 25\ntrain_df.loc[(train_df['Country/Region'] =='Cruise Ship') & (train_df['ConfirmedCases'] == 696),'ConfirmedCases'] = 706","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic exploratory data analysis with visualizations using choropleth maps.\n\n This is an ongoing notebook that gets updated. \n\n### Please upvote if you like this notebook\n\nAt first, we use `info()` function to look at the datatypes and to get an idea how many non-null values we have for each feature.\n8788 Provinces are missing, since the majority of reports are broken down not by States or Provinces, but rather by country. Let's substitute them with 'N/A' so they don't get excluded in a `groupby` clause. Then we look at the most impacted countries and the total number of confirmed cases and deaths with Provinces/States breakdown."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_province = train_df.fillna('N/A').groupby(['Country/Region','Province/State'])['ConfirmedCases', 'Fatalities'].max().sort_values(by='ConfirmedCases', ascending=False)\ncountry_province","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 10 countries with the highest number of cases with `seaborn.barblot` visualization. \n\nLet's look at the numbers of Confirmed cases and Fatalities for each country to compare. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregate records by countries\ncountries = country_province.groupby('Country/Region')['ConfirmedCases','Fatalities'].sum().sort_values(by= 'ConfirmedCases',ascending=False)\n\ncountries['country'] = countries.index\n\n# Unpivot the dataframe from wide to long format\ndf_long = pd.melt(countries, id_vars=['country'] , value_vars=['ConfirmedCases','Fatalities'])\n\n#Top countries by confirmed cases\ntop_countries = countries.index[:10]\n\ndf_top_countries = df_long[df_long['country'].isin(top_countries)]\n\nstyle.use('ggplot')\nrcParams['figure.figsize'] = 15,10\nax = sns.barplot(x = 'country', hue=\"variable\", y=\"value\", data=df_top_countries)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To create a static map we will utilize geospatial small scale data at 1:110m resolution. To render a world map we need a shapefile with world coordinates from Natural Earth domain:"},{"metadata":{"trusted":true},"cell_type":"code","source":"shapefile = '/kaggle/input/110m-cultural/ne_110m_admin_0_countries.shp'\n\n#Read shapefile using Geopandas\ngdf = gpd.read_file(shapefile)[['ADMIN', 'ADM0_A3', 'geometry']]\n\n#Rename columns.\ngdf.columns = ['country', 'code', 'geometry']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data preprocessing and cleaning"},{"metadata":{},"cell_type":"markdown","source":"Since the map data was gathered in 2016, some countries have been either renamed or deleted from the list. And moreover, we need to adjust namings, e.g. change 'United States of America' to 'US', etc. and delete the row corresponding to 'Antarctica'."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Drop row corresponding to 'Antarctica'\ngdf = gdf.drop(gdf.index[159])\n\n#common = gdf.merge(df,left_on = 'country', right_on = 'Country/Region')\n#train_df[(~train_df['Country/Region'].isin(common.country))]['Country/Region'].unique()\n#gdf[(~gdf['country'].isin(common.country))]['country'].unique()\n#gdf['country'].unique()\n\ngdf.loc[gdf['country'] == 'Taiwan',['country']] = 'Taiwan*'\ngdf.loc[gdf['country'] == 'Democratic Republic of the Congo',['country']] = 'Congo (Kinshasa)'\n#gdf.loc[gdf['country'] == 'Republic of the Congo',['country']] = 'Congo (Brazzaville)'\ngdf.loc[gdf['country'] == 'Ivory Coast',['country']] = \"Cote d'Ivoire\"\ngdf.loc[gdf['country'] == 'eSwatini',['country']] = 'Eswatini'\ngdf.loc[gdf['country'] == 'Gambia',['country']] = 'The Gambia'\ngdf.loc[gdf['country'] == 'United Republic of Tanzania',['country']] = 'Tanzania'\ngdf.loc[gdf['country'] == 'United States of America',['country']] = 'US' \ngdf.loc[gdf['country'] == 'Republic of Serbia',['country']] = 'Serbia'\ngdf.loc[gdf['country'] == 'South Korea',['country']] = 'Korea, South'\ngdf.loc[gdf['country'] == 'Macedonia',['country']] = 'North Macedonia'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"#merge the dataset with the shapefile by country name\nmerged = gdf.merge(countries, left_on = 'country', right_on = 'country', how = 'left').fillna(0)\n#Read data to json\nmerged_json = json.loads(merged.to_json())\n#Convert to String like object.\njson_data = json.dumps(merged_json)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from bokeh.io import curdoc, output_notebook\nfrom bokeh.models import Slider, HoverTool\nfrom bokeh.layouts import widgetbox, row, column\n\n#Input GeoJSON source that contains features for plotting.\ngeosource = GeoJSONDataSource(geojson = json_data)\n\n#Define a scale color palette with 250 colors.\npalette = viridis(250)\n\n#Reverse color order so that dark blue is the highest number.\npalette = palette[::-1]\n#Instantiate LinearColorMapper that linearly maps numbers in a range, into a sequence of colors.\ncolor_mapper = LinearColorMapper(palette = palette, low = 0, high = countries.ConfirmedCases.max())\n\n#Create color bar\ncolor_bar = ColorBar(color_mapper=color_mapper, label_standoff=12, border_line_color=None, width = 1300, height = 10, location = (0,0), orientation = 'horizontal')\n#Add hover tool\nhover = HoverTool(tooltips = [ ('Country/Region','@country'),('Confirmed Cases', '@ConfirmedCases')])\n\n#Create figure object.\np = figure(title = 'Confirmed Cases of Coronavirus COVID-19', plot_height = 600 , plot_width = 950, toolbar_location = None, tools = [hover])\np.xgrid.grid_line_color = None\np.ygrid.grid_line_color = None\n\n#Add patch renderer to figure. \np.patches('xs','ys', source = geosource,fill_color = {'field' :'ConfirmedCases', 'transform' : color_mapper},\n          line_color = 'black', line_width = 0.25, fill_alpha = 1)\n#Specify layout\np.add_layout(color_bar, 'below')\n \n# Make a column layout of widgetbox() and plot, and add it to the current document\nlayout = column(p,widgetbox())\ncurdoc().add_root(layout)\n#Display plot inline in Jupyter notebook\noutput_notebook()\n#Display plot\nshow(layout)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Input GeoJSON source that contains features for plotting.\ngeosource = GeoJSONDataSource(geojson = json_data)\n#Define a sequential multi-hue color palette.\npalette = viridis(250)\n#Reverse color order so that dark blue is highest confirmed cases.\npalette = palette[::-1]\n#Instantiate LinearColorMapper that linearly maps numbers in a range, into a sequence of colors.\ncolor_mapper = LinearColorMapper(palette = palette, low = 0, high = countries.Fatalities.max())\n\n#Create color bar\ncolor_bar = ColorBar(color_mapper=color_mapper, label_standoff=12, border_line_color=None, width = 1300, height = 10, location = (0,0), orientation = 'horizontal')\n#Add hover tool\nhover = HoverTool(tooltips = [ ('Country/Region','@country'),('Fatalities', '@Fatalities')])\n\n\n#Create figure object.\np = figure(title = 'Fatalities of Coronavirus COVID-19', plot_height = 600 , plot_width = 950, toolbar_location = None, tools = [hover])\np.xgrid.grid_line_color = None\np.ygrid.grid_line_color = None\n#Add patch renderer to figure. \np.patches('xs','ys', source = geosource,fill_color = {'field' :'Fatalities', 'transform' : color_mapper},\n          line_color = 'black', line_width = 0.25, fill_alpha = 1)\n#Specify figure layout.\np.add_layout(color_bar, 'below')\n\n# Make a column layout of widgetbox() and plot, and add it to the current document\nlayout = column(p,widgetbox())\ncurdoc().add_root(layout)\n\n#Display figure inline in Jupyter Notebook.\noutput_notebook()\n#Display figure.\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Time series analysis \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.Date.min(), train_df.Date.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Remove columns we do not need\ncols = ['Lat', 'Long', 'Fatalities']\ntimes_series_cntr = train_df.drop(cols, axis=1).fillna('N/A')\n\n# Aggregate cases by date and country\ntimes_series_cntr = times_series_cntr.groupby(['Date','Province/State','Country/Region'])['ConfirmedCases'].max()\\\n                    .groupby(['Date','Country/Region']).sum()\\\n                    .reset_index()\n\n# Indexing with Time Series Data\ntimes_series_cntr = times_series_cntr.set_index('Date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"times_series_df = times_series_cntr.groupby('Date')['ConfirmedCases'].sum().reset_index()\ntimes_series_df = times_series_df.set_index('Date')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualization of time series data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cumulative total of Confirmed cases\n\ntimes_series_df.plot(figsize=(20, 10), title=\"The Cumulative total of Confirmed cases\")\nplt.legend(loc=2, prop={'size': 20})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# New Confirmed cases throughout the time\n\ntimes_series_df.diff().fillna(0).plot(figsize=(20, 10), title=\"New Confirmed cases throughout the time\")\nplt.legend(loc=2, prop={'size': 20})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What is the highest amount of cases reported on one day and when it happened?\n\ntimes_series_df.diff().loc[times_series_df.diff()['ConfirmedCases']  == times_series_df.diff().fillna(0).max()['ConfirmedCases']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"top_countries_tm = times_series_cntr[times_series_cntr['Country/Region'].isin(top_countries)]\nplt.xticks(rotation=45)\n\nax = sns.lineplot(x=top_countries_tm.index, y=\"ConfirmedCases\", hue=\"Country/Region\", data=top_countries_tm).set_title('Cumulative line')\nplt.legend(loc=2, prop={'size': 12});","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Remove columns we do not need\ncols = ['Province/State', 'Lat', 'Long', 'Fatalities']\n\ntimes_series_cntr = times_series_cntr.sort_values(['Country/Region','Date'])\ntimes_series_cntr['ConfirmedCases'] = times_series_cntr.ConfirmedCases.diff().fillna(0)\ntimes_series_cntr.loc[times_series_cntr['ConfirmedCases'] < 0,['ConfirmedCases']] = 0 \n\ntop_countries_tm = times_series_cntr[times_series_cntr['Country/Region'].isin(top_countries)]\nplt.xticks(rotation=45)\nax = sns.lineplot(x=top_countries_tm.index, y=\"ConfirmedCases\", hue=\"Country/Region\", data=top_countries_tm).set_title('Confirmed cases per day')\nplt.legend(loc=2, prop={'size': 12});","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it can be seen, the outlier dated Feb 13th and located in China and, actually, the day before no cases have been identified in China. Therefore we might assume that this large number could be the sum from two previous days. As well as for Mar 11th, most countries did not report any cases but the day after the number of cases rose sharply."},{"metadata":{},"cell_type":"markdown","source":"We can also visualize our data using a time-series decomposition that allows us to decompose our time series into three distinct components: trend, seasonality, and noise."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from pylab import rcParams\nimport statsmodels.api as sm\n\ntimes_series_df.index = pd.to_datetime(times_series_df.index, format='%Y-%m-%d')\n\nrcParams['figure.figsize'] = 18, 8\ndecomposition = sm.tsa.seasonal_decompose(times_series_df.diff().fillna(0), model='additive')\nfig = decomposition.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying Double exponential smoothing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove columns we do not need\ncols = ['Lat', 'Long','Date','Fatalities']\ntimes_series_cntr_pr = train_df.drop(cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"provinces = train_df['Province/State'].unique()\ncountries = train_df['Country/Region'].unique()\n\ndef double_exponential_smoothing(series, alpha, beta):\n    \"\"\"\n        series - dataset with timeseries\n        alpha - float [0.0, 1.0], smoothing parameter for level\n        beta - float [0.0, 1.0], smoothing parameter for trend\n    \"\"\"\n    # first value is same as series\n    series = list(series)\n    result = [series[0]]\n    for n in range(1, len(series)+14):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= (len(series)): # forecasting\n            value = result[-1]\n        else:\n            value = series[n]\n        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n        trend = beta*(level-last_level) + (1-beta)*trend\n        result.append(int(level+trend))\n    return result\n\ndef plotDoubleExponentialSmoothing(series, alphas, betas):\n    \"\"\"\n        Plots double exponential smoothing with different alphas and betas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters for level\n        betas - list of floats, smoothing parameters for trend\n    \"\"\"\n    series = series.loc[series['Country/Region'] == 'Canada']\n    series = series.loc[series['Province/State'] == 'Alberta']\n    series = series.ConfirmedCases\n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(20, 8))\n        for alpha in alphas:\n            for beta in betas:\n                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n        plt.plot(series.values, label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Double Exponential Smoothing\")\n        plt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotDoubleExponentialSmoothing(times_series_cntr_pr, alphas=[0.15, 0.02], betas=[0.9, 0.09])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Remove columns we do not need\ncols = ['Lat', 'Long','Date','Fatalities']\ntimes_series_cntr_pr = train_df.drop(cols, axis=1)\n\n#Double exponential smoothing for Confirmed cases\n\ncountries = train_df['Country/Region'].unique()\n\ndef double_exponential_smoothing(df, alpha, beta):\n    \"\"\"\n        series - dataset with timeseries\n        alpha - float [0.0, 1.0], smoothing parameter for level\n        beta - float [0.0, 1.0], smoothing parameter for trend\n    \"\"\"\n    result =[]\n    cntr = []\n    prov=[]\n    for c in countries:\n        for p in df.loc[df['Country/Region'] == c]['Province/State'].unique():\n            if p is not np.nan :\n                series = df.loc[(df['Province/State'] == p) & (df['Country/Region'] == c)].ConfirmedCases\n                series = list(series)\n                #result.append(series[0])\n                for n in range(1, len(series)+31):\n                    if n == 1:\n                        level, trend = series[0], series[1] - series[0]\n                    if n >= len(series): # forecasting\n                        value = result[-1]\n                    else:\n                        value = series[n]\n                    last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n                    trend = beta*(level-last_level) + (1-beta)*trend\n                    result.append(int(level+trend))\n                    prov.append(p)\n                    cntr.append(c)\n            \n            elif p is np.nan :\n                series = df.loc[df['Country/Region'] == c].ConfirmedCases\n                series = list(series)\n                #result.append(series[0])\n                for n in range(1, len(series)+31):\n                    if n == 1:\n                        level, trend = series[0], series[1] - series[0]\n                    if n >= len(series): # forecasting\n                        value = result[-1]\n                    else:\n                        value = series[n]\n                    last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n                    trend = beta*(level-last_level) + (1-beta)*trend\n                    result.append(int(level+trend))\n                    prov.append(p)\n                    cntr.append(c)\n\n    return result, cntr, prov","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"t = double_exponential_smoothing(times_series_cntr_pr,0.15, 0.9)\nfull_cc = pd.DataFrame([t[0],t[1],t[2]], index = ['ConfirmedCases','Country/Region','Province/State'], columns= np.arange(1, len(t[0]) + 1)).T\nfull_cc.loc[(full_cc['ConfirmedCases'] < 0) ,'ConfirmedCases'] = 0\nfull_cc = full_cc.sort_values(['Country/Region','ConfirmedCases','Province/State'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Remove training data\n\ntotal_days = len([x for x in train_df.Date.unique() if x not in test_df.Date.unique()]) + test_df.Date.nunique() #93\nindeces = []\nfor j in range(0,284):\n    for i in range(1,51):\n        indeces.append((i+j*total_days))\n\npred_cc = full_cc.drop(indeces).reset_index().ConfirmedCases","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Remove columns we do not need\ncols = ['Lat', 'Long','Date','ConfirmedCases']\ntimes_series_cntr_f = train_df.drop(cols, axis=1)\n\n# Double exponential smoothing for Confirmed cases\n\ncountries = train_df['Country/Region'].unique()\n\ndef double_exponential_smoothing_f(df, alpha, beta):\n    \"\"\"\n        series - dataset with timeseries\n        alpha - float [0.0, 1.0], smoothing parameter for level\n        beta - float [0.0, 1.0], smoothing parameter for trend\n    \"\"\"\n    result =[]\n    cntr = []\n    prov=[]\n    for c in countries:\n        for p in df.loc[df['Country/Region'] == c]['Province/State'].unique():\n            if p is not np.nan :\n                series = df.loc[(df['Province/State'] == p) & (df['Country/Region'] == c)].Fatalities\n                series = list(series)\n                #result.append(series[0])\n                for n in range(1, len(series)+31):\n                    if n == 1:\n                        level, trend = series[0], series[1] - series[0]\n                    if n >= len(series): # forecasting\n                        value = result[-1]\n                    else:\n                        value = series[n]\n                    last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n                    trend = beta*(level-last_level) + (1-beta)*trend\n                    result.append(int(level+trend))\n                    prov.append(p)\n                    cntr.append(c)\n            \n            elif p is np.nan :\n                series = df.loc[df['Country/Region'] == c].Fatalities\n                series = list(series)\n                #result.append(series[0])\n                for n in range(1, len(series)+31):\n                    if n == 1:\n                        level, trend = series[0], series[1] - series[0]\n                    if n >= len(series): # forecasting\n                        value = result[-1]\n                    else:\n                        value = series[n]\n                    last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n                    trend = beta*(level-last_level) + (1-beta)*trend\n                    result.append(int(level+trend))\n                    prov.append(p)\n                    cntr.append(c)\n\n    return result, cntr, prov","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f = double_exponential_smoothing_f(times_series_cntr_f,0.15, 0.9)\nfull_f = pd.DataFrame([f[0],f[1],f[2]], index = ['Fatalities','Country/Region','Province/State'], columns= np.arange(1, len(f[0]) + 1)).T\nfull_f.loc[(full_f['Fatalities'] < 0) ,'Fatalities'] = 0\n\nfull_f = full_f.sort_values(['Country/Region','Fatalities','Province/State'])\npred_f = full_f.drop(indeces).reset_index().Fatalities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df = pd.DataFrame([pred_cc, pred_f], index = ['ConfirmedCases','Fatalities']).T\npredicted_df.index += 1 \npredicted_df.to_csv('submission.csv', index_label = \"ForecastId\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random forest"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.metrics import make_scorer\n\n\ntime_split = TimeSeriesSplit(n_splits=10)\n\n# new cases (non cumulative)\n#y_train_cc = np.array(train_df['ConfirmedCases'].diff().fillna(0).astype(int))\n#y_train_ft = np.array(train_df['Fatalities'].diff().fillna(0).astype(int))\n\ny_train_cc = np.array(train_df['ConfirmedCases'].astype(int))\ny_train_ft = np.array(train_df['Fatalities'].astype(int))\ncols = ['Lat', 'Long', 'ConfirmedCases', 'Fatalities']\n\nfull_df = pd.concat([train_df.drop(cols, axis=1), test_df.drop(['Lat', 'Long'],axis=1)])\nindex_split = train_df.shape[0]\nfull_df = pd.get_dummies(full_df, columns=full_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error\ndef RMSLError(y_test, predictions):\n    return np.sqrt(mean_squared_log_error(y_test, predictions))\n    \nrmsle_score = make_scorer(RMSLError, greater_is_better=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = full_df[:index_split]\nx_test= full_df[index_split:]\nx_train.shape, x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=100, n_jobs= -1, min_samples_leaf=3, random_state=17)\n\nrf.fit(x_train,y_train_cc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#rf_scores = cross_val_score(rf, x_train, y_train, cv=3, scoring='neg_mean_squared_log_error')\ny_pred_cc = rf.predict(x_test)\ny_pred_cc = y_pred_cc.astype(int)\ny_pred_cc[y_pred_cc <0]=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=100, n_jobs= -1, min_samples_leaf=3, random_state=17)\n\nrf.fit(x_train,y_train_ft)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#rf_scores = cross_val_score(rf, x_train, y_train, cv=3, scoring='neg_mean_squared_log_error')\ny_pred_ft = rf.predict(x_test)\ny_pred_ft = y_pred_ft.astype(int)\ny_pred_ft[y_pred_ft <0]=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"predicted_df_rf = pd.DataFrame([y_pred_cc, y_pred_ft], index = ['ConfirmedCases','Fatalities'], columns= np.arange(1, y_pred_cc.shape[0] + 1)).T\npredicted_df_rf.to_csv('submission_rf.csv', index_label = \"ForecastId\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}