{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data\n\nIn this challenge, we will be predict the cumulative number of confirmed COVID19 cases in various locations across the world, as well as the number of resulting fatalities, for future dates. \n\n\nWe have three data files: \n\n* **train.csv** : the training data up to Mar 18, 2020.\n* **test csv** : the dates to predict; there is a week of overlap with the training data for the initial Public leaderboard. Once submissions are paused, the Public leaderboard will update based on last 28 days of predicted data.\n* **submission.csv** : a sample submission in the correct format; again, predictions should be cumulative"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-1/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-1/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of instances in the feature of `Province/State` has `NaN` values. We can replace `NaN` values with country names in the `Country/Region` column for these instances. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#change the NaN values with the country names in the train data \nfor row in range(len(train_data['Province/State'])):\n    if str(train_data['Province/State'][row]) == 'nan':\n        train_data['Province/State'][row] = train_data['Country/Region'][row]\n        \n#change the NaN values with the country names in the test data \nfor row in range(len(test_data['Province/State'])):\n    if str(test_data['Province/State'][row]) == 'nan':\n        test_data['Province/State'][row] = test_data['Country/Region'][row]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let us view Turkey for an example: "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data['Country/Region'] == 'Turkey']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to day number for each country. "},{"metadata":{"trusted":true},"cell_type":"code","source":"day_number = train_data[train_data['Province/State'] == 'Turkey'].shape[0]\nday_number","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `Long` and `Lat` attributes represent the coordinates of each country and these coordinates determine only provinces or states, not cities in the states. So, the information of provinces or states are sufficient for us. Therfore, we can drop the columns of `Long` and `Lat`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.drop(['Long', 'Lat'], axis = 1)\ntest_data = test_data.drop(['Long', 'Lat'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to `encode` the Province/State column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"state_code = 0\ntrain_data['Province/State'][0] = 0\n\n#there are 284 countries \nfor instance in range(0, 284):\n    #encode each province for each day\n    for state_index in range(0, day_number):\n        #the code must be unique for each province\n        train_data['Province/State'][state_index + (day_number * instance)] = state_code\n    state_code += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, let us view Turkey for instance. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data['Country/Region'] == 'Turkey']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can drop the column of `Country/Region` because we will interest with the column `Provinces/State` only. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.drop('Country/Region', axis = 1)\ntest_data = test_data.drop('Country/Region', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pivot the data for each province\n\nWe will applying `pivoting` for each country. The data values of `Confirmed cases` and `Fatalities` will be new columns data for each state. The ascending order based on time will not be broken.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#create new data sets \nCC_dataset = pd.DataFrame()\nFatal_dataset = pd.DataFrame()\ntarget = []\n\n\nfor forecast_id in range(0, 284):\n    #take sample data of Confirmed Cases and Fatalities for one country\n    sample_CC_data = train_data.loc[train_data['Province/State'] == forecast_id]\n    sample_Fatal_data = train_data.loc[train_data['Province/State'] == forecast_id]\n\n    #keep data in the features of Confirmed Cases and Fatalities\n    CC_column = sample_CC_data['ConfirmedCases']\n    sample_CC_data = sample_CC_data.drop(['ConfirmedCases', 'Date', 'Fatalities'], axis = 1)\n\n    Fatal_column = sample_Fatal_data['Fatalities']\n    sample_Fatal_data = sample_Fatal_data.drop(['Fatalities', 'Date', 'ConfirmedCases'], axis = 1)\n\n    #anymore, we can take first row because others are same\n    sample_CC_data = sample_CC_data.head(1)\n    sample_Fatal_data = sample_Fatal_data.head(1)\n\n    #create list\n    CC_list = list(CC_column)\n    Fatal_list = list(Fatal_column)\n\n    #create columns in new CC data frame: CC1,CC2,...\n    for CC_index in range(0,len(CC_list)):\n        sample_CC_data['CC' + str(CC_index+1)] = CC_list[CC_index]\n        \n    #create columns in new Fatality data frame: F1,F2,...\n    for Fatal_index in range(0,len(Fatal_list)):\n        sample_Fatal_data['F' + str(Fatal_index+1)] = Fatal_list[Fatal_index]\n\n    #target list is ready \n    #CC_target = CC_list[-25:]\n    #target.append(CC_target)\n\n    #adding sample data for one country to new data frames \n    CC_dataset = CC_dataset.append(sample_CC_data)\n    Fatal_dataset = Fatal_dataset.append(sample_Fatal_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have two data sets of `Confirmed Cases` and `Fatalities`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"CC_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Fatal_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculate daily cases \n\nNow, we will calculate daily numbers of the confirmed casualities. Because the data set `CC_dataset` consist of cumulative number of `Sars-COV-2` diseases.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#create empty data sets for calculating daily cases and fatalities\nCC_daily_dataset = pd.DataFrame()\nFatality_daily_dataset = pd.DataFrame()\n\ncountry_number = CC_dataset.shape[0]\n\nfor state_id in range(0, country_number):\n    sample_case = CC_dataset.loc[CC_dataset['Province/State'] == state_id]\n    #we must keep the data of case for first day\n    sample_case['Day1CC'] =  sample_case['CC1']\n    \n    sample_fatality = Fatal_dataset.loc[Fatal_dataset['Province/State'] == state_id]\n    #we must keep the data of fatality for first day\n    sample_fatality['Day1F'] = sample_fatality['F1']\n    \n    for case_index in range(1, day_number):\n        #extracting the CC data value of the day from the data value of previous day\n        sample_case['Day' + str(case_index + 1) + 'CC'] = sample_case['CC' + str(case_index + 1)] - sample_case['CC' + str(case_index)]\n        #we do not need the CC data value of the day, anymore\n        sample_case = sample_case.drop(['CC' + str(case_index)], axis = 1)\n        \n        #extracting the fatality data value of the day from the data value of previous day\n        sample_fatality['Day' + str(case_index + 1) + 'F'] = sample_fatality['F' + str(case_index + 1)] - sample_fatality['F' + str(case_index)]\n        #we do not need the fatality data value of the day, anymore\n        sample_fatality = sample_fatality.drop(['F' + str(case_index)], axis = 1)\n    \n    #drop the data of last day\n    sample_case = sample_case.drop(['CC' + str(day_number)], axis = 1)\n    sample_fatality = sample_fatality.drop(['F' + str(day_number)], axis = 1)\n    \n    #assign avaliable daily data to new daily data sets \n    CC_daily_dataset = CC_daily_dataset.append(sample_case)\n    Fatality_daily_dataset = Fatality_daily_dataset.append(sample_fatality)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CC_daily_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Fatality_daily_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_day_CC = CC_dataset.iloc[:,-1].values\nlast_day_F = Fatal_dataset.iloc[:,-1].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't need the columns of `Id` and `Province/State`, anymore. "},{"metadata":{"trusted":true},"cell_type":"code","source":"CC_daily_dataset = CC_daily_dataset.drop(['Id', 'Province/State'], axis = 1)\nFatality_daily_dataset = Fatality_daily_dataset.drop(['Id', 'Province/State'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We shall split `train` and `test` data sets. We will use the data in last ten day cases and fatalities for test. "},{"metadata":{"trusted":true},"cell_type":"code","source":"features_CC = CC_dataset\ntrain_features_CC = CC_dataset.iloc[:,:-10]\ntarget_CC = CC_dataset.iloc[:,-10:]\n\nfeatures_F = Fatal_dataset\ntrain_features_F = Fatal_dataset.iloc[:,:-10]\ntarget_F = Fatal_dataset.iloc[:,-10:]\n\n\n#28 days for predict, 35 days for train and test\ntrain_test_CC_daily_dataset = CC_daily_dataset.iloc[:,:-28]\ntarget_CC_daily_dataset = CC_daily_dataset.iloc[:,-28:]\n\ntrain_test_fatal_daily_dataset = Fatality_daily_dataset.iloc[:,:-28]\ntarget_fatal_daily_dataset = Fatality_daily_dataset.iloc[:,-28:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_CC_daily_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_CC_daily_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_CC = np.array(target_CC)\ntarget_CC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_F = np.array(target_F)\ntarget_F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_CC = np.array(features_CC)\nfeatures_F = np.array(features_F)\n\ntrain_features_CC = np.array(train_features_CC)\ntrain_features_F = np.array(train_features_F)\n\ntrain_test_CC_daily = np.array(train_test_CC_daily_dataset)\ntrain_test_F_daily = np.array(train_test_fatal_daily_dataset)\n\ntarget_CC_daily = np.array(target_CC_daily_dataset)\ntarget_F_daily = np.array(target_fatal_daily_dataset)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train and test split based on instance countries. the test size is 0.2\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_CC_daily, test_CC_daily, train_target_CC_daily, test_target_CC_daily = train_test_split(train_test_CC_daily, \n                                                                                              target_CC_daily, \n                                                                                              test_size = 0.2)\n\ntrain_F_daily, test_F_daily, train_target_F_daily, test_target_F_daily = train_test_split(train_test_F_daily, \n                                                                                          target_F_daily, \n                                                                                          test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n'''\ntrain_CC_daily_dataset = np.expand_dims(train_CC_daily_dataset, axis = 2)\ntrain_fatal_daily_dataset = np.expand_dims(train_fatal_daily_dataset, axis = 2)\n\ntest_CC_daily_dataset = np.expand_dims(test_CC_daily_dataset, axis = 2)\ntest_fatal_daily_dataset = np.expand_dims(test_fatal_daily_dataset, axis = 2)\n\n#train_target_CC_daily_dataset = np.expand_dims(train_target_CC_daily_dataset, axis = 2)\n#train_target_fatal_daily_dataset = np.expand_dims(train_target_fatal_daily_dataset, axis = 2)\n\n#test_target_CC_daily_dataset = np.expand_dims(test_target_CC_daily_dataset, axis = 2)\n#test_target_fatal_daily_dataset = np.expand_dims(test_target_fatal_daily_dataset, axis = 2)\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import normalize\n#scaler = MinMaxScaler(feature_range = (0, 1), copy = False)\n\n#features_CC = scaler.fit_transform(features_CC)\n#target_CC = scaler.fit_transform(target_CC)\n\n#features_F = scaler.fit_transform(features_F)\n#target_F = scaler.fit_transform(target_F)\n\n#norm_features_CC = normalize(features_CC)\n#norm_target_CC = normalize(target_CC)\n\n#norm_features_F = normalize(features_F)\n#norm_target_F = normalize(target_F)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_CC = np.expand_dims(features_CC, axis = 2)\ntrain_features_CC = np.expand_dims(train_features_CC, axis = 2)\n\ntrain_features_CC = np.expand_dims(train_features_CC, axis = 2)\ntrain_features_F = np.expand_dims(train_features_F, axis = 2)\n\ntrain_CC_daily = np.expand_dims(train_CC_daily, axis = 2)\ntrain_F_daily = np.expand_dims(train_F_daily, axis = 2)\n\ntest_CC_daily = np.expand_dims(test_CC_daily, axis = 2)\ntest_F_daily = np.expand_dims(test_F_daily, axis = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import LSTM, Dense, Dropout, Flatten, BatchNormalization, Conv1D, MaxPooling1D\nfrom keras.models import Sequential\nfrom keras.regularizers import l1, l2, l1_l2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(LSTM(512, input_shape = (train_CC_daily.shape[1], 1), \n               return_sequences=True, activation='tanh'))\nmodel.add(LSTM(256, return_sequences=True, activation='relu'))\nmodel.add(LSTM(512, return_sequences=False, activation='tanh'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dense(28, activation = 'linear'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = 'Adam', loss = 'mean_squared_logarithmic_error', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnn = model.fit(train_CC_daily, train_target_CC_daily, batch_size=64, epochs=20, \n                validation_data=(test_CC_daily, test_target_CC_daily))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_CC.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_CC = model.predict(train_CC_daily)\nprediction_F = model.predict(train_F_daily)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_CC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_case = train_features_CC[0][-1] + abs(round(prediction_CC[0][0]))\nnp.append(train_features_CC[0], last_case)\ntrain_features_CC[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_CC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"minimum_CC = []\nmaximum_CC = []\n\nminimum_F = []\nmaximum_F = []\n\nfor index in range(features_CC.shape[0]):\n    minimum_CC.append(min(features_CC[index][-43:]))\n    maximum_CC.append(max(features_CC[index][-43:]))\n    \n    minimum_F.append(min(features_F[index][-43:]))\n    maximum_F.append(max(features_F[index][-43:]))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"for index in range(prediction_CC.shape[0]):\n    for inner_index in range(len(prediction_CC[index])):\n        prediction_CC[index][inner_index] = (prediction_CC[index][inner_index] * (maximum_CC[index] - minimum_CC[index])) + minimum_CC[index]"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"for index in range(prediction_F.shape[0]):\n    for inner_index in range(len(prediction_F[index])):\n        prediction_F[index][inner_index] = (prediction_F[index][inner_index] * (maximum_F[index] - minimum_F[index])) + minimum_F[index]"},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction_CC = scaler.inverse_transform(prediction_CC)\n#prediction_F = scaler.inverse_transform(prediction_F)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction_F = scaler.inverse_transform(prediction_F)\n#prediction_F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index in range(len(prediction_CC)):\n    for inner_index in range(len(prediction_CC[index])):\n        if prediction_CC[index][inner_index] >= 0:\n            prediction_CC[index][inner_index] = round(prediction_CC[index][inner_index])\n        elif prediction_CC[index][inner_index] < 0:\n            prediction_CC[index][inner_index] = round(abs(prediction_CC[index][inner_index]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index in range(len(prediction_F)):\n    for inner_index in range(len(prediction_F[index])):\n        if prediction_F[index][inner_index] >= 0:\n            prediction_F[index][inner_index] = round(prediction_F[index][inner_index])\n        elif prediction_F[index][inner_index] < 0:\n            prediction_F[index][inner_index] = round(abs(prediction_F[index][inner_index]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_CC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index in range(len(prediction_CC)):\n    prediction_CC[index][0] = last_day_CC[index]\n    for inner_index in range(1, len(prediction_CC[index])):\n        prediction_CC[index][inner_index] = prediction_CC[index][inner_index] + prediction_CC[index][inner_index-1]\nprediction_CC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index in range(len(prediction_F)):\n    prediction_F[index][0] = last_day_F[index]\n    for inner_index in range(1, len(prediction_F[index])):\n        prediction_F[index][inner_index] = prediction_F[index][inner_index] + prediction_F[index][inner_index-1]\nprediction_F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_CC.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_CC = CC_dataset.iloc[:,-12:]\ntarget_CC = np.array(target_CC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_CC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_F = Fatal_dataset.iloc[:,-12:]\ntarget_F = np.array(target_F)\ntarget_F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed_cases = np.concatenate((target_CC, prediction_CC), axis = 1)\nfatalities = np.concatenate((target_F, prediction_F), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed_cases","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fatalities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submitted_data = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-1/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submitted_data['ConfirmedCases'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed_cases = confirmed_cases.reshape(12212,)\nfatalities = fatalities.reshape(12212,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submitted_data['Fatalities'] = fatalities\nsubmitted_data['ConfirmedCases'] = confirmed_cases","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submitted_data.to_csv('submission.csv', index = False, encoding = 'utf-8-sig')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}