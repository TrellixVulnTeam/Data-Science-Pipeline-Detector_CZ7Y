{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport sys  \nimport pandas as pd\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.ar_model import ARResults\nfrom scipy.interpolate import Rbf\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nimport numpy as np\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-1/train.csv')\npopulation = pd.read_csv('/kaggle/input/world-bank-country-demographics/World_Bank_Pop_by_Country.csv')\np_density = pd.read_csv('/kaggle/input/world-bank-country-demographics/World_Bank_Pop_Density.csv')\npopulation_density = pd.merge(population[['Country Code','Country Name','2018']], p_density[['Country Code','2018']], on = 'Country Code')\npopulation_density.columns = ['Country Code', 'Country Name', 'Population', 'Density']\npopulation_density= population_density.set_value(249,'Country Name', 'US')\npopulation_density= population_density.set_value(29,'Country Name', 'Brunei')\npopulation_density= population_density.set_value(42,'Country Name', 'Congo (Brazzaville)')\npopulation_density= population_density.set_value(41,'Country Name', 'Congo (Kinshasa)')\npopulation_density= population_density.set_value(52,'Country Name', 'Czechia')\npopulation_density= population_density.set_value(65,'Country Name', 'Egypt')\npopulation_density= population_density.set_value(92,'Country Name', 'French Guiana')\npopulation_density= population_density.set_value(124,'Country Name', 'Korea, South')\npopulation_density= population_density.set_value(252,'Country Name', 'Venezuela')\npopulation_density= population_density.set_value(84,'Country Name', 'The Gambia')\npopulation_density= population_density.set_value(21,'Country Name', 'The Bahamas')\npopulation_density= population_density.set_value(219,'Country Name', 'Slovakia')\npopulation_density= population_density.set_value(251,'Country Name', 'Saint Vincent and the Grenadines')\npopulation_density= population_density.set_value(131,'Country Name', 'Saint Lucia')\npopulation_density= population_density.set_value(200,'Country Name', 'Russia')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"population_density.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Country = 'United Kingdom'\ndf_c = train_data[train_data['Country/Region'] == Country][['Date','Province/State','ConfirmedCases','Fatalities']]\ndf_c.set_index('Date', inplace=True)\ndf_acumed_daycases = pd.DataFrame(df_c.groupby('Date')['ConfirmedCases','Fatalities'].sum())\ndf_diff_daycases = pd.DataFrame(df_c.groupby(['Date'])['ConfirmedCases','Fatalities'].sum().diff())\ndf_acumed_daycases.plot(grid=True),df_diff_daycases.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_n_steps_AR(data, n_steps, use_new_model = True,maxlag=3):\n    data_history = data\n    if n_steps < 5:\n        for i in range(n_steps):\n            model = AR(data_history)\n            model_fit = model.fit(maxlag=maxlag, disp=True)\n            yhat = model_fit.predict(len(data_history), len(data_history))\n            data_history = np.concatenate((data_history, yhat))\n    else:\n        total = sum(data_history)\n        if total < 1000:\n            for i in range(min(14,n_steps)):\n                model = AR(data_history)\n                model_fit = model.fit(maxlag=1, disp=True)\n                yhat = model_fit.predict(len(data_history), len(data_history))\n                data_history = np.concatenate((data_history, yhat))\n            if sum(data_history) >= 15000:\n                for i in range(14,min(20,n_steps)):\n                    model = AR(data_history)\n                    model_fit = model.fit(maxlag=3, disp=True)\n                    yhat = model_fit.predict(len(data_history), len(data_history))\n                    data_history = np.concatenate((data_history, yhat))\n                for i in range(20,n_steps):\n                    t,d,s,p,b,r = ['add', True, None, None, False, True]\n                    model = ExponentialSmoothing(data_history, trend=t, damped=d, seasonal=s, seasonal_periods=p)\n                    model_fit = model.fit(optimized=True, use_boxcox=b, remove_bias=r)\n                    yhat = model_fit.predict(len(data_history), len(data_history))\n                    data_history = np.concatenate((data_history, yhat))\n            else:\n                for i in range(14,n_steps):\n                    model = AR(data_history)\n                    model_fit = model.fit(maxlag=1, disp=True)\n                    yhat = model_fit.predict(len(data_history), len(data_history))\n                    data_history = np.concatenate((data_history, yhat))\n        else:\n            for i in range(n_steps):\n                model = AR(data_history)\n                model_fit = model.fit(maxlag=maxlag, disp=True)\n                yhat = model_fit.predict(len(data_history), len(data_history))\n                data_history = np.concatenate((data_history, yhat))\n    return data_history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_list=[]\nfor country in train_data['Country/Region'].unique():\n    df_c = train_data[train_data['Country/Region'] == country][['Country/Region','Province/State','Date','ConfirmedCases','Fatalities']]\n    if df_c['Province/State'].nunique()==0:\n        _list.append([df_c[df_c['ConfirmedCases'].diff()<0].index,'ConfirmedCases', df_c.loc[df_c[df_c['ConfirmedCases'].diff()<0].index-1, 'ConfirmedCases'].values])\n        train_data.set_value(df_c[df_c['ConfirmedCases'].diff()<0].index,'ConfirmedCases', df_c.loc[df_c[df_c['ConfirmedCases'].diff()<0].index-1, 'ConfirmedCases'].values)\n    else:\n        for province in df_c['Province/State'].unique():\n            df_d = df_c[df_c['Province/State'] == province][['Country/Region','Province/State','ConfirmedCases','Fatalities']]\n            _list.append([df_d[df_d['ConfirmedCases'].diff()<0].index,'ConfirmedCases', df_d.loc[df_d[df_d['ConfirmedCases'].diff()<0].index-1, 'ConfirmedCases'].values])\n            train_data.set_value(df_d[df_d['ConfirmedCases'].diff()<0].index,'ConfirmedCases', df_d.loc[df_d[df_d['ConfirmedCases'].diff()<0].index-1, 'ConfirmedCases'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as ro\nimport warnings\nwarnings.filterwarnings('ignore', 'statsmodels.tsa.ar_model.AR', FutureWarning)\narr_cc = np.empty(0, int)\narr_fa = np.empty(0, int)\narr_fa_rate = np.empty(0, int)\n_list = []\nX = 30\nY = 50\nfor country in train_data['Country/Region'].unique():\n    df_c = train_data[train_data['Country/Region'] == country][['Country/Region','Province/State','Date','ConfirmedCases','Fatalities']]\n    df_c.set_index('Date', inplace=True)    \n    if df_c['Province/State'].nunique()==0:\n        \n        df_acumed_daycases = pd.DataFrame(df_c.groupby('Date')['ConfirmedCases','Fatalities'].sum())\n        df_diff_daycases = pd.DataFrame(df_c.groupby(['Date'])['ConfirmedCases','Fatalities'].sum().diff())\n        data_cc = df_acumed_daycases['ConfirmedCases'].astype('float')\n        result_cc = np.absolute(predict_n_steps_AR(data_cc, X, use_new_model = True))\n        \n        try:\n            population_ds = population_density[population_density['Country Name']==country][['Population', 'Density']]\n            max_cc = population_ds['Population']*0.7\n            indices = np.nonzero(result_cc > max_cc.values[0])\n            replacement = max_cc\n            for index in indices:\n                result_cc[index] = replacement\n        except:\n            result_cc = np.absolute(predict_n_steps_AR(data_cc, X, use_new_model = True))\n        \n        arr_cc = np.append(arr_cc, result_cc[Y:])\n        data_fa = df_acumed_daycases['Fatalities'].astype('float')\n        \n        \n        result_fa = np.absolute(predict_n_steps_AR(data_fa, X, use_new_model = True))\n        \n        indices_cc = np.nonzero(result_cc > 200)\n        if sum(result_fa) == 0 and len(indices_cc) > 0:\n            replacement = result_cc*0.05\n            for index in indices_cc:\n                result_fa[index] = replacement[index]\n\n        rate = np.divide(result_fa,result_cc)\n        indices = np.nonzero(rate > 0.09)\n        replacement = result_cc*0.09\n        for index in indices:\n            result_fa[index] = replacement[index]\n\n        arr_fa = np.append(arr_fa, result_fa[Y:])\n\n        arr_fa_rate = np.append(arr_fa_rate, np.divide(result_fa[Y:],result_cc[Y:]))\n\n    else:\n        df_acumed_daycases = pd.DataFrame(df_c.groupby('Date')['ConfirmedCases','Fatalities'].sum())\n        df_diff_daycases = pd.DataFrame(df_c.groupby(['Date'])['ConfirmedCases','Fatalities'].sum().diff())\n        data_cc = df_acumed_daycases['ConfirmedCases'].astype('float')\n        result_cc = np.absolute(predict_n_steps_AR(data_cc, X, use_new_model = True))\n        population_ds = population_density[population_density['Country Name']==country][['Population', 'Density']]\n        max_cc = population_ds['Population']*0.7\n        \n        try:            \n            indices = np.nonzero(result_cc > max_cc.values[0])\n            _list.append([country,max_cc,result_cc, np.nonzero(result_cc > max_cc.values[0])])\n            for province in df_c['Province/State'].unique():\n                df_d = df_c[df_c['Province/State'] == province][['Country/Region','Province/State','ConfirmedCases','Fatalities']]            \n                df_acumed_daycases = pd.DataFrame(df_d.groupby('Date')['ConfirmedCases','Fatalities'].sum())\n                df_diff_daycases = pd.DataFrame(df_d.groupby(['Date'])['ConfirmedCases','Fatalities'].sum().diff())\n                data_cc = df_acumed_daycases['ConfirmedCases'].astype('float')\n                result_cc = np.absolute(predict_n_steps_AR(data_cc, X, use_new_model = True))            \n\n                max_cc = result_cc[indices.values[0]-1]\n                \n                for index in indices:\n                    result_cc[index] = max_cc\n\n                arr_cc = np.append(arr_cc, result_cc[Y:])\n                \n                data_fa = df_acumed_daycases['Fatalities'].astype('float')\n                result_fa = np.absolute(predict_n_steps_AR(data_fa, X, use_new_model = True))\n\n                indices_cc = np.nonzero(result_cc > 200)\n                if sum(result_fa) == 0 and len(indices_cc) > 0:\n                    replacement = result_cc*0.05\n                    for index in indices_cc:\n                        result_fa[index] = replacement[index]\n\n\n                rate = np.divide(result_fa,result_cc)\n                indices = np.nonzero(rate > 0.09)\n                replacement = result_cc*0.09\n                for index in indices:\n                    result_fa[index] = replacement[index]\n\n                arr_fa = np.append(arr_fa, result_fa[Y:])\n                arr_fa_rate = np.append(arr_fa_rate, np.divide(result_fa[Y:],result_cc[Y:]))\n        except:\n            arr_list = []\n            for province in df_c['Province/State'].unique():\n                df_d = df_c[df_c['Province/State'] == province][['Country/Region','Province/State','ConfirmedCases','Fatalities']] \n                df_acumed_daycases = pd.DataFrame(df_d.groupby('Date')['ConfirmedCases','Fatalities'].sum())\n                df_diff_daycases = pd.DataFrame(df_d.groupby(['Date'])['ConfirmedCases','Fatalities'].sum().diff())\n                data_cc = df_acumed_daycases['ConfirmedCases'].astype('float')\n                result_cc = predict_n_steps_AR(data_cc.astype(int), 36, use_new_model = True)\n                arr_list.append(result_cc)\n            try:\n                indices_whole = np.nonzero([sum(x) for x in zip(*arr_list)] > max_cc.values[0])\n            except:pass\n            for province in df_c['Province/State'].unique():\n                df_d = df_c[df_c['Province/State'] == province][['Country/Region','Province/State','ConfirmedCases','Fatalities']]            \n                df_acumed_daycases = pd.DataFrame(df_d.groupby('Date')['ConfirmedCases','Fatalities'].sum())\n                df_diff_daycases = pd.DataFrame(df_d.groupby(['Date'])['ConfirmedCases','Fatalities'].sum().diff())\n                data_cc = df_acumed_daycases['ConfirmedCases'].astype('float')\n                result_cc = np.absolute(predict_n_steps_AR(data_cc, X, use_new_model = True))            \n\n                try:\n                    replacement = result_cc[indices_whole[0][0]-1]\n                    for index in indices_whole[0]:\n                        result_cc[index] = replacement\n                except:\n                    pass\n\n                arr_cc = np.append(arr_cc, result_cc[Y:])\n                \n                data_fa = df_acumed_daycases['Fatalities'].astype('float')\n                result_fa = np.absolute(predict_n_steps_AR(data_fa, X, use_new_model = True))\n\n                indices_cc = np.nonzero(result_cc > 200)\n                if sum(result_fa) == 0 and len(indices_cc) > 0:\n                    replacement = result_cc*0.05\n                    for index in indices_cc:\n                        result_fa[index] = replacement[index]\n\n                rate = np.divide(result_fa,result_cc)\n                indices = np.nonzero(rate > 0.09)\n                replacement = result_cc*0.09\n                for index in indices:\n                    result_fa[index] = replacement[index]\n\n                arr_fa = np.append(arr_fa, result_fa[Y:])\n                arr_fa_rate = np.append(arr_fa_rate, np.divide(result_fa[Y:],result_cc[Y:]))\nresults = pd.DataFrame()  \nresults['ConfirmedCases'] = ro.round(arr_cc,0)\nresults['Fatalities'] = ro.round(arr_fa,0)\n#results['Fatalities_Rate'] = arr_fa_rate\n\nresults['ForecastId'] = np.arange(1,len(results)+1)\nresults = results[['ForecastId','ConfirmedCases','Fatalities']]\nresults.to_csv('/kaggle/working/submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}