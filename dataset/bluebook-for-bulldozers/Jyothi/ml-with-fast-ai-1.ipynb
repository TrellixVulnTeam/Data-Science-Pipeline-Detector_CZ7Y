{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai.imports import *\n\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.ensemble import forest\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype\nfrom IPython.display import display\n\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92f2ec404c9d9fdaaef093bd12548303185589c5"},"cell_type":"markdown","source":"# functions from fast.ai library"},{"metadata":{"trusted":true,"_uuid":"f43e6a6d21587fd573974d761ba4bd688d8572e3"},"cell_type":"code","source":"def train_cats(df):\n    \"\"\"Change any columns of strings in a panda's dataframe to a column of\n    categorical values. This applies the changes inplace.\n    Parameters:\n    -----------\n    df: A pandas dataframe. Any columns of strings will be changed to\n        categorical values.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    note the type of col2 is string\n    >>> train_cats(df)\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    now the type of col2 is category\n    \"\"\"\n    for n,c in df.items():\n        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da0aac86cc3d6bb49b9a2d690bbe0a115455fed8"},"cell_type":"code","source":"def add_datepart(df, fldname, drop=True):\n    fld = df[fldname]\n    if not np.issubdtype(fld.dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld,infer_datetime_format=True)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    for n in ('Year', 'Month', 'Week', 'Day', 'Dayofweek', \n            'Dayofyear', 'Is_month_end', 'Is_month_start', \n            'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', \n            'Is_year_start'):\n        df[targ_pre+n] = getattr(fld.dt,n.lower())\n\n    df[targ_pre+'Elapsed'] = fld.astype(np.int64) // 10**9\n    if drop: df.drop(fldname, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6eb23f35c4c79ec075693ad4c3a7233c0f280437"},"cell_type":"code","source":"def numericalize(df, col, name, max_n_cat):\n    \"\"\" Changes the column col from a categorical type to it's integer codes.\n    Parameters:\n    -----------\n    df: A pandas dataframe. df[name] will be filled with the integer codes from\n        col.\n    col: The column you wish to change into the categories.\n    name: The column name you wish to insert into df. This column will hold the\n        integer codes.\n    max_n_cat: If col has more categories than max_n_cat it will not change the\n        it to its integer codes. If max_n_cat is None, then col will always be\n        converted.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    note the type of col2 is string\n    >>> train_cats(df)\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    now the type of col2 is category { a : 1, b : 2}\n    >>> numericalize(df, df['col2'], 'col3', None)\n       col1 col2 col3\n    0     1    a    1\n    1     2    b    2\n    2     3    a    1\n    \"\"\"\n    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n        df[name] = col.cat.codes+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"765adf14faca7818fcb5fe0d275d30cc9226b47e"},"cell_type":"code","source":"def fix_missing(df, col, name, na_dict):\n    \"\"\" Fill missing data in a column of df with the median, and add a {name}_na column\n    which specifies if the data was missing.\n    Parameters:\n    -----------\n    df: The data frame that will be changed.\n    col: The column of data to fix by filling in missing data.\n    name: The name of the new filled column in df.\n    na_dict: A dictionary of values to create na's of and the value to insert. If\n        name is not a key of na_dict the median will fill any missing data. Also\n        if name is not a key of na_dict and there is no missing data in col, then\n        no {name}_na column is not created.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> fix_missing(df, df['col1'], 'col1', {})\n    >>> df\n       col1 col2 col1_na\n    0     1    5   False\n    1     2    2    True\n    2     3    2   False\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> fix_missing(df, df['col2'], 'col2', {})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> fix_missing(df, df['col1'], 'col1', {'col1' : 500})\n    >>> df\n       col1 col2 col1_na\n    0     1    5   False\n    1   500    2    True\n    2     3    2   False\n    \"\"\"\n    if is_numeric_dtype(col):\n        if pd.isnull(col).sum() or (name in na_dict):\n            df[name+'_na'] = pd.isnull(col)\n            filler = na_dict[name] if name in na_dict else col.median()\n            df[name] = col.fillna(filler)\n            na_dict[name] = filler\n    return na_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"884bb78d80435fde0bdeaadcc6032818e5bba16d"},"cell_type":"code","source":"def proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n    \"\"\" proc_df takes a data frame df and splits off the response variable, and\n    changes the df into an entirely numeric dataframe. For each column of df \n    which is not in skip_flds nor in ignore_flds, na values are replaced by the\n    median value of the column.\n    Parameters:\n    -----------\n    df: The data frame you wish to process.\n    y_fld: The name of the response variable\n    skip_flds: A list of fields that dropped from df.\n    ignore_flds: A list of fields that are ignored during processing.\n    do_scale: Standardizes each column in df. Takes Boolean Values(True,False)\n    na_dict: a dictionary of na columns to add. Na columns are also added if there\n        are any missing values.\n    preproc_fn: A function that gets applied to df.\n    max_n_cat: The maximum number of categories to break into dummy values, instead\n        of integer codes.\n    subset: Takes a random subset of size subset from df.\n    mapper: If do_scale is set as True, the mapper variable\n        calculates the values used for scaling of variables during training time (mean and standard deviation).\n    Returns:\n    --------\n    [x, y, nas, mapper(optional)]:\n        x: x is the transformed version of df. x will not have the response variable\n            and is entirely numeric.\n        y: y is the response variable\n        nas: returns a dictionary of which nas it created, and the associated median.\n        mapper: A DataFrameMapper which stores the mean and standard deviation of the corresponding continuous\n        variables which is then used for scaling of during test-time.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    note the type of col2 is string\n    >>> train_cats(df)\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    now the type of col2 is category { a : 1, b : 2}\n    >>> x, y, nas = proc_df(df, 'col1')\n    >>> x\n       col2\n    0     1\n    1     2\n    2     1\n    >>> data = DataFrame(pet=[\"cat\", \"dog\", \"dog\", \"fish\", \"cat\", \"dog\", \"cat\", \"fish\"],\n                 children=[4., 6, 3, 3, 2, 3, 5, 4],\n                 salary=[90, 24, 44, 27, 32, 59, 36, 27])\n    >>> mapper = DataFrameMapper([(:pet, LabelBinarizer()),\n                          ([:children], StandardScaler())])\n    >>>round(fit_transform!(mapper, copy(data)), 2)\n    8x4 Array{Float64,2}:\n    1.0  0.0  0.0   0.21\n    0.0  1.0  0.0   1.88\n    0.0  1.0  0.0  -0.63\n    0.0  0.0  1.0  -0.63\n    1.0  0.0  0.0  -1.46\n    0.0  1.0  0.0  -0.63\n    1.0  0.0  0.0   1.04\n    0.0  0.0  1.0   0.21\n    \"\"\"\n    if not ignore_flds: ignore_flds=[]\n    if not skip_flds: skip_flds=[]\n    if subset: df = get_sample(df,subset)\n    else: df = df.copy()\n    ignored_flds = df.loc[:, ignore_flds]\n    df.drop(ignore_flds, axis=1, inplace=True)\n    if preproc_fn: preproc_fn(df)\n    if y_fld is None: y = None\n    else:\n        if not is_numeric_dtype(df[y_fld]): df[y_fld] = df[y_fld].cat.codes\n        y = df[y_fld].values\n        skip_flds += [y_fld]\n    df.drop(skip_flds, axis=1, inplace=True)\n\n    if na_dict is None: na_dict = {}\n    else: na_dict = na_dict.copy()\n    na_dict_initial = na_dict.copy()\n    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if len(na_dict_initial.keys()) > 0:\n        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n    df = pd.get_dummies(df, dummy_na=True)\n    df = pd.concat([ignored_flds, df], axis=1)\n    res = [df, y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65c8d3c6e026783f702eb9a095a79cbe4bf220a6"},"cell_type":"code","source":"def set_rf_samples(n):\n    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n    n random rows.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n))\n\ndef reset_rf_samples():\n    \"\"\" Undoes the changes produced by set_rf_samples.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\nforest.check_random_state(rs).randint(0, n_samples, n_samples))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79229a79bfc874a47ae70530b3e99e69439f2827"},"cell_type":"code","source":"def print_score(m):\n    def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n    res = []\n    RMSE_train = rmse(m.predict(x_train),y_train)\n    RMSE_valid = rmse(m.predict(x_valid),y_valid)\n    R2_train = m.score(x_train, y_train)\n    R2_valid = m.score(x_valid, y_valid)\n    return (f'RMSE_T : {round(RMSE_train,5)}; RMSE_V : {round(RMSE_valid,5)}; R2_T : {round(R2_train,5)}; R2_V : {round(R2_valid,5)}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"351b586325b316124e07e143ba057c96c0801707"},"cell_type":"markdown","source":"# File path"},{"metadata":{"trusted":true,"_uuid":"1a3158d2a7faacdba65ff07ab26f473759342758"},"cell_type":"code","source":"train_path = \"../input/TrainAndValid.csv\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6288198ecca87ac1426915af9a8c34aab7c95d0b"},"cell_type":"markdown","source":"# Data Pre-Processing"},{"metadata":{"trusted":true,"_uuid":"c9d6edc301a098c88b7feea08da429e02b5c3aa4"},"cell_type":"code","source":"# read csv\ndf_raw = pd.read_csv(train_path,parse_dates=[\"saledate\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca5c32d25a29226b484cb9e83e2005a40bee8fb9"},"cell_type":"code","source":"# change the dependent variable into log value, As the evaluation metric RMSLE\ndf_raw.SalePrice = np.log(df_raw.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afd197fc2e39a5511c6bfb062ced21e171c30baf"},"cell_type":"code","source":"# extract details from date\nadd_datepart(df_raw,'saledate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcdeb157690e4c8ba1408b887d066ddc54b9ce6d"},"cell_type":"code","source":"# convert objects into category data type\ntrain_cats(df_raw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d3a1574fd7a7352af8eda48ccff05b3d00f9d4e"},"cell_type":"code","source":"# set the small sample size for building model\nset_rf_samples(50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ae9f3310e47fda8b9b6ba083e667b732d274230"},"cell_type":"code","source":"df_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'],ordered=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28ba597896b15873c0500b47098e738a8fe4026d"},"cell_type":"code","source":"max_cat_code = 5 # for more than 5 cat codes, categorical column replaced with eqivalent car codes\ndf, y, nas = proc_df(df_raw, \"SalePrice\",max_n_cat=max_cat_code)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"138abf421e900003d75e81a8ab5e8792fed1a459"},"cell_type":"markdown","source":"# First model"},{"metadata":{"trusted":true,"_uuid":"bfc8fa6a9f6f934a2887defc8c4a14925372d344"},"cell_type":"code","source":"# split train & valid\nn_split = 20000\nn_train = len(y) - n_split\nx_train, x_valid = df[:n_train], df[n_train:]\ny_train, y_valid = y[:n_train], y[n_train:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0f0eed58dad17910d475616625acfaa793e27ab"},"cell_type":"code","source":"# plain model\nm = RandomForestRegressor(n_jobs=-1)\nm.fit(x_train,y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b044399006b271305e3b04832b49e5ecb70b5e0"},"cell_type":"code","source":"# model with reduced correlation between trees\nm = RandomForestRegressor(n_jobs=-1,max_features=0.5,min_samples_leaf=3)\nm.fit(x_train,y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"987c5d1c81179897c818eb047f0190ae1278857d"},"cell_type":"code","source":"# model with reduced correlation between trees\nm = RandomForestRegressor(n_jobs=-1,max_features=0.5,min_samples_leaf=3,min_samples_split=4)\nm.fit(x_train,y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f1f150c3a3ed9e5d7983ce7b5e75a68d79bc423"},"cell_type":"code","source":"# increse number of estimater to 40\nm = RandomForestRegressor(n_jobs=-1,n_estimators=40,max_features=0.5,min_samples_leaf=3,min_samples_split=4)\nm.fit(x_train,y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a220c6bf974fa26fd0c23cb050843b3d2af57611"},"cell_type":"code","source":"# increase minimum sample leaf size to 5\nm = RandomForestRegressor(n_jobs=-1,n_estimators=40,max_features=0.5,min_samples_leaf=5,min_samples_split=10)\nm.fit(x_train,y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b78acaf99bff6b23d57ef7f85ae258c1ad7a3f1b"},"cell_type":"code","source":"# reset sample size & fit the best model\nreset_rf_samples()\n\nm = RandomForestRegressor(n_jobs=-1,n_estimators=40,max_features=0.5,min_samples_leaf=3,min_samples_split=4)\nm.fit(x_train,y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2079cf6325a3f19efe0d0a9642bd69591075fd7d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}