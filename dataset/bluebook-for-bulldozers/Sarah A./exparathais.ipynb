{"cells":[{"metadata":{"papermill":{"duration":0.036683,"end_time":"2020-08-30T20:12:40.116891","exception":false,"start_time":"2020-08-30T20:12:40.080208","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# **Este notebook trata de técnicas de regressão utilizando modelos de florestas aleatórias.**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-08-30T20:12:40.190182Z","iopub.status.busy":"2020-08-30T20:12:40.189323Z","iopub.status.idle":"2020-08-30T20:12:40.233231Z","shell.execute_reply":"2020-08-30T20:12:40.232428Z"},"papermill":{"duration":0.084995,"end_time":"2020-08-30T20:12:40.233382","exception":false,"start_time":"2020-08-30T20:12:40.148387","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# comandos que não se comunicam com a linguagem Python e sim diretamente com o kernel do Jupyter\n# começam com %\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:12:40.300731Z","iopub.status.busy":"2020-08-30T20:12:40.299722Z","iopub.status.idle":"2020-08-30T20:12:42.113089Z","shell.execute_reply":"2020-08-30T20:12:42.111487Z"},"papermill":{"duration":1.849518,"end_time":"2020-08-30T20:12:42.113268","exception":false,"start_time":"2020-08-30T20:12:40.26375","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# importando os módulos e bibliotecas usados pela linguagem Python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.03129,"end_time":"2020-08-30T20:12:42.188422","exception":false,"start_time":"2020-08-30T20:12:42.157132","status":"completed"},"tags":[]},"cell_type":"markdown","source":"O objeto de estudo deste notebook é o conjunto de dados da competição [Bluebook for Bulldozers](https://www.kaggle.com/c/bluebook-for-bulldozers). O conjunto de dados analisado nesta aula, retirado da competição  [Bluebook for Bulldozers](https://www.kaggle.com/c/bluebook-for-bulldozers), consiste de dados reais relativos a informações variadas acerca de tratores de esteira industriais. Nosso objetivo é criar um modelo que, dadas informações relativas a um trator em particular, seja capaz de predizer seu *preço de venda em leilões*. A abordagem apresentada é baseada na solução proposta por Jeremy Howard em seu curso [Introduction to Machine Learning for Coders](http://course18.fast.ai/ml).\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1e/CAT-D10N-pic001.jpg\" width=\"300\" height=\"300\"/> "},{"metadata":{"papermill":{"duration":0.032332,"end_time":"2020-08-30T20:12:45.37971","exception":false,"start_time":"2020-08-30T20:12:45.347378","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Importando dados\n\nO primeiro passo é importar os dados de treino. Como os arquivos estão armazenados no formato [CSV (comma-separated values)](https://pt.wikipedia.org/wiki/Comma-separated_values), utilizamos a função [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html), da biblioteca [pandas](https://pandas.pydata.org/), para efetuar a leitura:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:12:45.451447Z","iopub.status.busy":"2020-08-30T20:12:45.450394Z","iopub.status.idle":"2020-08-30T20:12:51.491826Z","shell.execute_reply":"2020-08-30T20:12:51.491124Z"},"papermill":{"duration":6.079746,"end_time":"2020-08-30T20:12:51.492009","exception":false,"start_time":"2020-08-30T20:12:45.412263","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"PATH = \"../input/bluebook-for-bulldozers/\" #Arqvuivo csv baixado de https://www.kaggle.com/c/bluebook-for-bulldozers/data?select=Train.zip.\n\ndf_raw = pd.read_csv(f'{PATH}Train.zip',\n                     compression='zip', \n                     low_memory=False, \n                     parse_dates=[\"saledate\"])\n#o dataframe nomeado como df_raw amazenará os dados lidos. \n#\"pd.\" indica que quem está lendo é a biblioteca Pandas.\n#\"read_csv\" é o comando do Pandas para informar que o dataframe está em formado csv. \n#O Pandas tmb tem a função \"read_excel\" quando o arquivo é do excel. é aqui que incluimos a planilha de dados de trabalho.\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.031218,"end_time":"2020-08-30T20:12:51.555745","exception":false,"start_time":"2020-08-30T20:12:51.524527","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Alguns parâmetros foram fornecidos à função: \n\n* **compression** especificou que os arquivos estão comprimidos no formato *zip*;\n* **low_memory** especificou que a função deve ler o arquivo como um todo e não em pequenos pedaços (isso é recomendável quando não sabemos bem que tipos de variáveis há no arquivo, por conta de uma questão técnica relacionada à forma como o [pandas](https://pandas.pydata.org/) infere os tipos de cada variável); \n* **parse_dates** especifica qual variável deve ser processada como data (no caso, no formato ano-mês-dia).\n\nConferindo o tamanho do conjunto:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:12:51.623826Z","iopub.status.busy":"2020-08-30T20:12:51.623064Z","iopub.status.idle":"2020-08-30T20:12:51.65729Z","shell.execute_reply":"2020-08-30T20:12:51.656658Z"},"papermill":{"duration":0.070306,"end_time":"2020-08-30T20:12:51.65745","exception":false,"start_time":"2020-08-30T20:12:51.587144","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"df_raw.shape \n#df_raw é o nome da planilha referente ao dataframe. \".shape\" nos informa o número de linhas e colunas da planilha (dataframe), respectivamente","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.031012,"end_time":"2020-08-30T20:12:51.720111","exception":false,"start_time":"2020-08-30T20:12:51.689099","status":"completed"},"tags":[]},"cell_type":"markdown","source":"São 53 variáveis e mais de 400 mil observações!\n\nVisualizando algumas linhas do conjunto e utilizando o método [describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) para calcular algumas medidas estatísticas:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:12:51.7895Z","iopub.status.busy":"2020-08-30T20:12:51.788316Z","iopub.status.idle":"2020-08-30T20:12:56.015627Z","shell.execute_reply":"2020-08-30T20:12:56.015042Z"},"papermill":{"duration":4.264223,"end_time":"2020-08-30T20:12:56.015752","exception":false,"start_time":"2020-08-30T20:12:51.751529","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"with pd.option_context(\"display.max_columns\", 100): \n    display(df_raw)\n    display(df_raw.describe(include='all'))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.033828,"end_time":"2020-08-30T20:12:56.084341","exception":false,"start_time":"2020-08-30T20:12:56.050513","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Parece que há [variáveis categóricas](https://en.wikipedia.org/wiki/Categorical_variable) e [valores faltantes](https://en.wikipedia.org/wiki/Missing_data) no conjunto de dados. Nan significa \"Not a number\": https://datacadamia.com/data/type/number/nan\n\nPara analisar quais variáveis são numéricas e quais são categóricas, daremos uma olhada em seus tipos:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:12:56.158638Z","iopub.status.busy":"2020-08-30T20:12:56.157844Z","iopub.status.idle":"2020-08-30T20:12:56.1942Z","shell.execute_reply":"2020-08-30T20:12:56.194738Z"},"papermill":{"duration":0.076241,"end_time":"2020-08-30T20:12:56.194906","exception":false,"start_time":"2020-08-30T20:12:56.118665","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"df_raw.dtypes","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.03405,"end_time":"2020-08-30T20:12:56.263518","exception":false,"start_time":"2020-08-30T20:12:56.229468","status":"completed"},"tags":[]},"cell_type":"markdown","source":"As váriaveis não-numéricas são interpretadas pelo [pandas](https://pandas.pydata.org/) como sendo do tipo genérico **object**. Nota-se acima que são a grande maioria!\n\nVamos analisar quantas e quais são as categorias em cada variável categórica. No output abaixo, cada linha contém o nome de uma variável, o número de categorias entre parênteses e as categorias em si entre colchetes."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:12:56.338354Z","iopub.status.busy":"2020-08-30T20:12:56.337645Z","iopub.status.idle":"2020-08-30T20:13:00.639001Z","shell.execute_reply":"2020-08-30T20:13:00.639585Z"},"papermill":{"duration":4.341785,"end_time":"2020-08-30T20:13:00.639746","exception":false,"start_time":"2020-08-30T20:12:56.297961","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"for n, c in df_raw.items():\n    if not pd.api.types.is_numeric_dtype(c) and not pd.api.types.is_datetime64_any_dtype(c):\n        print(f'{n} ({len(c.unique())}): {c.unique()}')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.035647,"end_time":"2020-08-30T20:13:00.711238","exception":false,"start_time":"2020-08-30T20:13:00.675591","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Há variáveis de alta cardinalidade: por exemplo, **fiModelDesc** tem 4999 valores possíveis!"},{"metadata":{"papermill":{"duration":0.035408,"end_time":"2020-08-30T20:13:00.782326","exception":false,"start_time":"2020-08-30T20:13:00.746918","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Usando a biblioteca [missingno](https://github.com/ResidentMario/missingno) para dar uma olhada nos valores faltantes:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:13:00.860636Z","iopub.status.busy":"2020-08-30T20:13:00.859868Z","iopub.status.idle":"2020-08-30T20:13:09.932039Z","shell.execute_reply":"2020-08-30T20:13:09.931197Z"},"papermill":{"duration":9.113995,"end_time":"2020-08-30T20:13:09.932175","exception":false,"start_time":"2020-08-30T20:13:00.81818","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# para poder importar módulos que não estejam nos kernels do kaggle, \n# devemos instalá-los com o pip\n\n\n!pip install missingno","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:13:10.01181Z","iopub.status.busy":"2020-08-30T20:13:10.011026Z","iopub.status.idle":"2020-08-30T20:13:20.12673Z","shell.execute_reply":"2020-08-30T20:13:20.125907Z"},"papermill":{"duration":10.158225,"end_time":"2020-08-30T20:13:20.126917","exception":false,"start_time":"2020-08-30T20:13:09.968692","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import missingno\n\n\n#abaixo estão alguns exemplo de visualização de dados faltantes:","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missingno.bar(df_raw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missingno.matrix(df_raw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missingno.heatmap(df_raw)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.036758,"end_time":"2020-08-30T20:13:20.202609","exception":false,"start_time":"2020-08-30T20:13:20.165851","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Há uma grande proporção de valores faltantes.\n\nO fato de haver variáveis categóricas e valores faltantes é crítico, já que algoritmos de regressão são projetados para lidar com variáveis numéricas. Portanto, é preciso pré-processar os dados e transformá-los em uma matriz de números antes de efetuar treinos de modelos de aprendizado."},{"metadata":{"papermill":{"duration":0.0407,"end_time":"2020-08-30T20:13:20.28004","exception":false,"start_time":"2020-08-30T20:13:20.23934","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Pré-processamento dos dados\n\nA primeira etapa de pré-processamento será aplicar a função logaritmo à variável predita:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:13:20.359866Z","iopub.status.busy":"2020-08-30T20:13:20.35896Z","iopub.status.idle":"2020-08-30T20:13:20.409652Z","shell.execute_reply":"2020-08-30T20:13:20.409006Z"},"papermill":{"duration":0.093022,"end_time":"2020-08-30T20:13:20.409798","exception":false,"start_time":"2020-08-30T20:13:20.316776","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"df_raw.SalePrice = np.log(df_raw.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.040371,"end_time":"2020-08-30T20:13:20.48787","exception":false,"start_time":"2020-08-30T20:13:20.447499","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Esse procedimento não é necessário, apenas conveniente. Ele é muito comum na predição de preços, já que nesses casos nos importamos mais com proporções do que com diferenças absolutas. Por exemplo, muitas vezes é mais significativo predizer um aumento ou redução de 10% (proporção) do que de 10 reais (diferença absoluta). Ao aplicar o logaritmo, os dados são transpostos para a escala logarítmica, em que as proporções se transformam em diferenças absolutas."},{"metadata":{"papermill":{"duration":0.036864,"end_time":"2020-08-30T20:13:20.565699","exception":false,"start_time":"2020-08-30T20:13:20.528835","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Vamos encapsular o restante de nosso procedimento de pré-processamento em uma função:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:13:20.649065Z","iopub.status.busy":"2020-08-30T20:13:20.648253Z","iopub.status.idle":"2020-08-30T20:13:20.682875Z","shell.execute_reply":"2020-08-30T20:13:20.682081Z"},"papermill":{"duration":0.079705,"end_time":"2020-08-30T20:13:20.683055","exception":false,"start_time":"2020-08-30T20:13:20.60335","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def pre_process (df):\n    \n    new_df = pd.DataFrame()\n    \n    for n,c in df.items():\n                \n        if pd.api.types.is_numeric_dtype(c):\n            # substituindo NaN numericos pelas medianas de cada coluna\n            new_df[n] = c.fillna(value=c.median())\n        else:\n            # interpretando o que nao for numerico como variaveis categoricas \n            # e transformando cada categoria em um numero\n            new_df[n] = pd.Categorical(c.astype('category').cat.as_ordered()).codes\n    \n    return new_df     ","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.036717,"end_time":"2020-08-30T20:13:20.756985","exception":false,"start_time":"2020-08-30T20:13:20.720268","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Na função acima, iteramos ao longo de todas as colunas do conjunto de dados. Se a coluna em questão for numérica, os valores faltantes são substituídos pela mediana. Se não for numérica, são transformadas em categorias e cada categoria, por sua vez, é associada a um número.\n\nValores faltantes nas váriaveis categóricas não são tão graves porque o próprio fato de um valor estar faltando pode ser interpretado como uma categoria. Já nas variáveis numéricas, eles são críticos, por isso foi preciso substitui-los por algum número.\n\nAplicando a função acima e gerando o [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) processado:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:13:20.848265Z","iopub.status.busy":"2020-08-30T20:13:20.847207Z","iopub.status.idle":"2020-08-30T20:13:23.129957Z","shell.execute_reply":"2020-08-30T20:13:23.129319Z"},"papermill":{"duration":2.335893,"end_time":"2020-08-30T20:13:23.13009","exception":false,"start_time":"2020-08-30T20:13:20.794197","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"df_proc = pre_process(df_raw)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.036774,"end_time":"2020-08-30T20:13:23.204406","exception":false,"start_time":"2020-08-30T20:13:23.167632","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Separando o conjunto em X (variáveis preditoras) e y (variável predita):"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = df_proc.drop('SalePrice', axis=1), df_proc['SalePrice'] \n#a função proc após o nome do datagrame diz que é para \"pular\" a coluna SalePrice. \n#axis=1 informa que estamos pulando uma coluna. É possível tmb pular linhas.\n#assim, todas as outras colunas representarão X e a coluna do preço de venda será y.","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:13:23.284099Z","iopub.status.busy":"2020-08-30T20:13:23.283288Z","iopub.status.idle":"2020-08-30T20:13:23.372828Z","shell.execute_reply":"2020-08-30T20:13:23.372171Z"},"papermill":{"duration":0.131471,"end_time":"2020-08-30T20:13:23.372958","exception":false,"start_time":"2020-08-30T20:13:23.241487","status":"completed"},"tags":[],"trusted":false},"cell_type":"markdown","source":"\n"},{"metadata":{},"cell_type":"markdown","source":"### Separando conjuntos de treino e de teste (validação)\n\nNo Aprendizado de Máquina, não é boa prática utilizar na etapa de treino todos os dados disponíveis. Sempre deve-se reservar uma parcela dos dados para efetuar um teste, de modo a verificar-se a capacidade preditiva do modelo.\n\nPara efetuar a separação dos dados *dados de treino* e *dados de teste*, usaremos a função [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html):"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:13:23.53029Z","iopub.status.busy":"2020-08-30T20:13:23.529583Z","iopub.status.idle":"2020-08-30T20:13:23.583057Z","shell.execute_reply":"2020-08-30T20:13:23.583629Z"},"papermill":{"duration":0.099007,"end_time":"2020-08-30T20:13:23.583798","exception":false,"start_time":"2020-08-30T20:13:23.484791","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"n_valid = 12000\nn_trn = len(df_proc)-n_valid\n\nX_treino, X_validacao = X[:n_trn].copy(), X[n_trn:].copy()\ny_treino, y_validacao = y[:n_trn].copy(), y[n_trn:].copy()\n\ny_treino.shape, y_validacao.shape","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.038065,"end_time":"2020-08-30T20:13:23.660627","exception":false,"start_time":"2020-08-30T20:13:23.622562","status":"completed"},"tags":[]},"cell_type":"markdown","source":"O conjunto de validação é um conjunto usado para testes intermediários durante o processo de modelagem. Além da validação, é recomendável que haja um conjunto de teste em separado para ser usado *após* a finalização da modelagem. Todas as competições do Kaggle possuem esse conjunto, que é usado para criar as pontuações e leaderboards.\n\nPerceba que, como há *evolução temporal*, não separamos o treino e o teste de maneira aleatória (existe comando específico para isto). Agora retiramos as 12000 últimas observações para teste, o que faz sentido, pois na vida real um modelo é aplicado para prever valores em instantes de tempo posteriores aos usados no treino."},{"metadata":{"papermill":{"duration":0.03701,"end_time":"2020-08-30T20:13:23.735771","exception":false,"start_time":"2020-08-30T20:13:23.698761","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Treinando a primeira floresta\n\nPara poupar tempo, vamos definir uma função, chamada **display_score**, que aceita um modelo treinado e imprime na tela as métricas $R^2$ e [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) relativas ao treino e à validação. Essas são apenas algumas das métricas estatísticas; existem várias outras; a escolha de qual usar deve ser ditada pelo problema que se está resolvendo."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:13:23.819506Z","iopub.status.busy":"2020-08-30T20:13:23.818758Z","iopub.status.idle":"2020-08-30T20:13:23.854792Z","shell.execute_reply":"2020-08-30T20:13:23.854164Z"},"papermill":{"duration":0.081543,"end_time":"2020-08-30T20:13:23.854925","exception":false,"start_time":"2020-08-30T20:13:23.773382","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def rmse(x,y): \n    \n    return np.sqrt(sklearn.metrics.mean_squared_error(x,y))\n\ndef display_score(m):\n    \n    res = [[rmse(m.predict(X_treino), y_treino), m.score(X_treino, y_treino)],\n          [rmse(m.predict(X_validacao), y_validacao), m.score(X_validacao, y_validacao)]]\n    \n    score = pd.DataFrame(res, columns=['RMSE','R2'], index = ['Treino','Validação'])\n    #nesta linha, estamos criando um dataframe chamado score no qual os dados da matriz res serão armazenados.\n    if hasattr(m, 'oob_score_'): \n        score.loc['OOB'] = [rmse(y_treino, m.oob_prediction_), m.oob_score_]\n        #Out-of-bag score=OOB, mais abaixo comentaremos sobre ele\n    display(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nNosso modelo de regressão será o [modelo de florestas aleatórias](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). Vamos importá-lo do [scikit-learn](https://scikit-learn.org/), armazenando-o em um objeto chamado **m** (poderia ser qualquer outro nome):"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.037501,"end_time":"2020-08-30T20:13:23.929999","exception":false,"start_time":"2020-08-30T20:13:23.892498","status":"completed"},"tags":[]},"cell_type":"markdown","source":"A célula abaixo treina um modelo de florestas aleatórias que especificaremos como o *modelo base*:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:13:24.012544Z","iopub.status.busy":"2020-08-30T20:13:24.011798Z","iopub.status.idle":"2020-08-30T20:16:57.389877Z","shell.execute_reply":"2020-08-30T20:16:57.389173Z"},"papermill":{"duration":213.422058,"end_time":"2020-08-30T20:16:57.390004","exception":false,"start_time":"2020-08-30T20:13:23.967946","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"m_base = sklearn.ensemble.RandomForestRegressor(n_jobs=-1, oob_score = True, random_state = 0)\n%time m_base.fit(X_treino, y_treino)\ndisplay_score(m_base)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explicando as três linhas acima:\n\n* na primeira linha, definimos o modelo. O hiperparâmetro **n_jobs = -1** especifica que, caso haja múltiplos processadores no computador, todos devem ser usados em paralelo. O hiperparâmetro **oob_score** será explicado mais adiante.\n\n* na segunda linha, treinamos o modelo. Repare no uso do comando mágico %time, que mede o tempo necessário para essa tarefa.\n\n* na terceira linha, usamos a função **display_score**, definida anteriormente, para imprimir as métricas."},{"metadata":{"papermill":{"duration":0.037569,"end_time":"2020-08-30T20:16:57.465536","exception":false,"start_time":"2020-08-30T20:16:57.427967","status":"completed"},"tags":[]},"cell_type":"markdown","source":"\n\nOs resultados são bem satisfatórios! O $R^2$ é próximo de 1! De fato, o modelo de florestas aleatórias é excelente para resolver esse tipo de problema: com alto grau de não-linearidade, dados sem estrutura clara e grande número de variáveis categóricas.\n\nA partir de agora, concentraremos nossos esforços em:\n\n* entender como funciona o modelo;\n* utilizar algumas técnicas para melhorar os resultados."},{"metadata":{"papermill":{"duration":0.038499,"end_time":"2020-08-30T20:16:57.542695","exception":false,"start_time":"2020-08-30T20:16:57.504196","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# O que é uma floresta aleatória, afinal?\n\nFlorestas, obviamente, são feitas de árvores! Em particular, os modelos de florestas aleatórias são compostos por vários modelos mais simples conhecidos como *árvores de decisão*. Portanto, antes de entender a floresta, é preciso entender a árvore."},{"metadata":{"papermill":{"duration":0.037559,"end_time":"2020-08-30T20:16:57.61821","exception":false,"start_time":"2020-08-30T20:16:57.580651","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Treinando e visualizando uma árvore de decisão\n\nNa nomenclatura do [scikit-learn](https://scikit-learn.org/stable/), cada árvore é chamada de *estimador*. Para treinar apenas 1 árvore, portanto, podemos fornecer o hiperparâmetro **n_estimators=1** para o modelo:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:16:57.702097Z","iopub.status.busy":"2020-08-30T20:16:57.701004Z","iopub.status.idle":"2020-08-30T20:16:59.576066Z","shell.execute_reply":"2020-08-30T20:16:59.575486Z"},"papermill":{"duration":1.920092,"end_time":"2020-08-30T20:16:59.5762","exception":false,"start_time":"2020-08-30T20:16:57.656108","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.037553,"end_time":"2020-08-30T20:16:59.651705","exception":false,"start_time":"2020-08-30T20:16:59.614152","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Os resultados são bem piores do que antes! Usar a floresta ao invés de 1 só árvore parece fazer toda a diferença. Outro detalhe: o treinamento ocorre de forma bem mais rápida.\n\nA seguir, define-se uma função para visualizar uma árvore:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:16:59.736178Z","iopub.status.busy":"2020-08-30T20:16:59.735407Z","iopub.status.idle":"2020-08-30T20:16:59.769766Z","shell.execute_reply":"2020-08-30T20:16:59.769103Z"},"papermill":{"duration":0.08022,"end_time":"2020-08-30T20:16:59.769893","exception":false,"start_time":"2020-08-30T20:16:59.689673","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def draw_tree(t, df, size=10, ratio=1, precision=0):\n   \n    import re\n    import graphviz\n    import sklearn.tree\n    import IPython.display\n    \n    s=sklearn.tree.export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n                                   special_characters=True, rotate=True, precision=precision)\n    IPython.display.display(graphviz.Source(re.sub('Tree {',\n       f'Tree {{ size={size}; ratio={ratio}', s)))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.037546,"end_time":"2020-08-30T20:16:59.846184","exception":false,"start_time":"2020-08-30T20:16:59.808638","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Utilizando-a para visualizar a árvore recém-treinada:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:16:59.928016Z","iopub.status.busy":"2020-08-30T20:16:59.927241Z","iopub.status.idle":"2020-08-30T20:17:00.532125Z","shell.execute_reply":"2020-08-30T20:17:00.531409Z"},"papermill":{"duration":0.647891,"end_time":"2020-08-30T20:17:00.532241","exception":false,"start_time":"2020-08-30T20:16:59.88435","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"draw_tree(m.estimators_[0], X_treino, precision=3)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.038988,"end_time":"2020-08-30T20:17:00.61052","exception":false,"start_time":"2020-08-30T20:17:00.571532","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Uma árvore é uma sequência de decisões binárias. Cada quadradinho acima é um *nó* e representa uma porção dos dados. O primeiro nó é chamado de *raiz* e contém a totalidade dos dados. Os últimos nós são chamados de *folhas*. Cada nó é gerado a partir de um nó da camada anterior por meio de uma decisão correspondente a alguma variável. Os dois nós da segunda camada, por exemplo, são gerados a partir do primeiro nó por meio de uma decisão relativa à variável **Coupler_System**.\n\nEm cada nó são exibidos:\n\n* uma métrica de predição (no caso, **mse**);\n* o número de amostras (**samples**);\n* a predição em si (**value**), que corresponde simplesmente à média da variável predita no nó.\n\nO *split*, ou seja, a decisão a ser tomada em cada nó, é especificada de modo a minimizar os erros dos dois nós resultantes. Há vários algoritmos capazes de efetuar essa minimização, como o [CART](https://medium.com/@arifromadhan19/regrssion-in-decision-tree-a-step-by-step-cart-classification-and-regression-tree-196c6ac9711e), por exemplo.\n\nA árvore acima foi treinada com profundidade 3. O que acontece se treinarmos uma árvore maior? Na célula abaixo, usamos o [default do modelo](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), que cresce a árvore até que todas as folhas estejam *puras*. Uma folha pura contém apenas 1 valor da variável predita. Em outras palavras, cada valor assumido pela variável predita corresponde a 1 folha na árvore. Como nosso conjunto tem 389125 linhas, a árvore gerada será bem grande!!"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:17:00.695683Z","iopub.status.busy":"2020-08-30T20:17:00.694918Z","iopub.status.idle":"2020-08-30T20:17:09.266289Z","shell.execute_reply":"2020-08-30T20:17:09.266901Z"},"papermill":{"duration":8.617461,"end_time":"2020-08-30T20:17:09.267055","exception":false,"start_time":"2020-08-30T20:17:00.649594","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.038821,"end_time":"2020-08-30T20:17:09.345126","exception":false,"start_time":"2020-08-30T20:17:09.306305","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Os resultados melhoram, mas o sobreajuste é enorme! Em outras palavras, temos um ajuste perfeito para o conjunto de treino, mas não temos o mesmo resultado para o teste pq o modelo tem parâmetros demais, muitos graus de liberdade! Esse é um problema dos modelos de (uma) árvore: como eles são muito flexíveis e conseguem modelar todo o espaço dos dados por meio de várias partições, grandes são as chances de se aprender um número excessivo de comportamentos, não correspondentes a padrões generalizáveis.\n\nPara melhorar a generalização, não é suficiente usar árvores maiores. É preciso combinar os resultados de múltiplas árvores."},{"metadata":{"papermill":{"duration":0.039093,"end_time":"2020-08-30T20:17:09.423505","exception":false,"start_time":"2020-08-30T20:17:09.384412","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Juntando várias árvores\n\nPara entendermos como as várias árvores formam a floresta, voltemos ao nosso modelo base:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:17:09.508478Z","iopub.status.busy":"2020-08-30T20:17:09.507454Z","iopub.status.idle":"2020-08-30T20:17:17.791561Z","shell.execute_reply":"2020-08-30T20:17:17.790826Z"},"papermill":{"duration":8.328801,"end_time":"2020-08-30T20:17:17.791685","exception":false,"start_time":"2020-08-30T20:17:09.462884","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"display_score(m_base)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.038912,"end_time":"2020-08-30T20:17:17.870045","exception":false,"start_time":"2020-08-30T20:17:17.831133","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Na célula abaixo, criamos e exibimos um [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) em que cada linha corresponde a uma observação do conjunto de validação e cada coluna corresponde à predição de uma das árvores da floresta. No final do [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) adicionamos três colunas, respectivamente, com: \n\n* as médias das predições de todas as árvores;\n* os desvios-padrão das predições de todas as árvores;\n* o valor verdadeiro da variável predita."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:17:17.955874Z","iopub.status.busy":"2020-08-30T20:17:17.955112Z","iopub.status.idle":"2020-08-30T20:17:18.859814Z","shell.execute_reply":"2020-08-30T20:17:18.859019Z"},"papermill":{"duration":0.950331,"end_time":"2020-08-30T20:17:18.859939","exception":false,"start_time":"2020-08-30T20:17:17.909608","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"preds = np.stack([t.predict(X_validacao) for t in m_base.estimators_]).T\npreds_df = pd.DataFrame(preds) #criando o dataframe preds_df com os dados obtidos na linha de cima\n\npreds_df['medias'] = preds_df.mean(axis=1) #uma das colunas de preds_df conterá todas as médias\npreds_df['stds'] = preds_df.std(axis=1) #uma das colunas de preds_df conterá todas os desvios padrões\npreds_df['valor real'] = y_validacao.values \npreds_df","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.040141,"end_time":"2020-08-30T20:17:18.940299","exception":false,"start_time":"2020-08-30T20:17:18.900158","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Olha que interessante: o erro de predição de cada árvore individual é alto (já que as árvores são sobreajustadas); mas quando *tiramos a média de todas as 100 árvores* (que formam a floresta!), o erro é baixo. Parece mágica! Tiramos a média de várias predições meia-boca e o resultado... é uma excelente predição!!! O que está acontecendo?\n\nO truque é *fazer com que as árvores apresentem o mínimo possível de correlação entre elas*. Sendo assim, cada árvore aprende de maneira sobreajustada uma porção isolada dos padrões que queremos capturar. Ao tirarmos a média, juntamos todos os pedacinhos que cada árvore aprendeu individualmente... e criamos um modelo completo e robusto!\n\nA principal estratégia para plantar uma floresta de árvores descorrelacionadas é fazer com que cada árvore utilize uma parcela aleatória dos dados. Dessa maneira, cada árvore sobreajusta de diferentes maneiras em diferentes fenômenos; ou seja, todas elas têm grandes erros, mas os erros são aleatórios. \n\n*E, de acordo com a Estatística, qual é a média de um monte de erros aleatórios?* \n\nZero!!\n\nNo algoritmo de florestas aleatórias, cada árvore efetua amostras *com reposição* (esse procedimento é conhecido como [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating)). Dessa maneira, nem todo o conjunto de dados é utilizado por cada árvore, já que no procedimento de amostragem várias observações podem se repetir. Em média, aproximadamente apenas 63,2% dos dados são utilizados por cada árvore. Isso ajuda bastante a diminuir a correlação entre elas.\n\nUma vantagem da técnica é que podemos utilizar os desvios-padrões das árvores para estabelecer um grau de confiança das predições: quanto menor o desvio-padrão de uma predição, mais as àrvores concordam quanto ao seu valor, então maior pode ser nossa confiança em relação à acurácia.\n\nAbaixo, para termos uma noção visual do resultado, plotamos as predições contra os valores verdadeiros e comparamos com a reta $x=y$:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:17:19.027989Z","iopub.status.busy":"2020-08-30T20:17:19.027185Z","iopub.status.idle":"2020-08-30T20:17:19.268376Z","shell.execute_reply":"2020-08-30T20:17:19.268921Z"},"papermill":{"duration":0.288668,"end_time":"2020-08-30T20:17:19.269083","exception":false,"start_time":"2020-08-30T20:17:18.980415","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"plt.plot(y_validacao.values, preds_df.mean(axis=1), '.')\n\nplt.plot(plt.gca().get_ylim(), plt.gca().get_ylim());\n\nplt.xlabel('y_valid')\nplt.ylabel('y_valid_pred');","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.040729,"end_time":"2020-08-30T20:17:19.437621","exception":false,"start_time":"2020-08-30T20:17:19.396892","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Out-of-bag score\n\nAcabamos de aprender que, devido à amostragem com reposição, cada árvore ignora uma parcela de observações.\n\nA métrica [OOB (out-of-bag)](https://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html) se vale desse fato para medir a capacidade preditiva do modelo *sem a necessidade de um conjunto de teste em separado*. Para efetuar as predições e calcular o OOB, cada árvore utiliza os dados de treino que foram por ela ignorados. Como a árvore não treinou o modelo com esses dados, eles efetivamente funcionam como um bom conjunto de teste!\n\nNo [scikit-learn](https://scikit-learn.org/), é preciso fornecer o parâmetro **oob_score = True** para que o OOB seja calculado durante o treino."},{"metadata":{"papermill":{"duration":0.040558,"end_time":"2020-08-30T20:17:19.519243","exception":false,"start_time":"2020-08-30T20:17:19.478685","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Sintonizando hiperparâmetros\n\nNesta seção daremos uma olhada em como podemos mexer em alguns hiperparâmetros do modelo de modo a melhorar os desempenhos preditivo e computacional. Depois do pre-processamento dos dados, o modelo final da árvores dependerá desta sintonia. Esse tipo de resultados pode vir no TCC para ser discutido."},{"metadata":{"papermill":{"duration":0.040798,"end_time":"2020-08-30T20:17:19.601397","exception":false,"start_time":"2020-08-30T20:17:19.560599","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## n_estimators\n\n**n_estimators** é o número de árvores na floresta. A regra para escolher esse valor é simples: quanto mais árvores, melhor a capacidade preditiva do modelo, mas maior o custo computacional.\n\nVamos dar uma olhada na relação entre a métrica $R^2$ e a quantidade de árvores em nossa floresta:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:17:19.690944Z","iopub.status.busy":"2020-08-30T20:17:19.690155Z","iopub.status.idle":"2020-08-30T20:17:19.96073Z","shell.execute_reply":"2020-08-30T20:17:19.961293Z"},"papermill":{"duration":0.31904,"end_time":"2020-08-30T20:17:19.961481","exception":false,"start_time":"2020-08-30T20:17:19.642441","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"plt.plot([sklearn.metrics.r2_score(y_validacao, np.mean(preds[:,:i+1], axis=1)) for i in range(100)]);","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.040984,"end_time":"2020-08-30T20:17:20.044402","exception":false,"start_time":"2020-08-30T20:17:20.003418","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Há um platô a partir do qual adicionar mais árvores não faz muita diferença. Esse é o comportamento esperado: a partir de um certo número de árvores, os ganhos de desempenho passam a ser muito pequenos.\n\nO default do [scikit-learn](https://scikit-learn.org/) é usar 100 árvores. Mas da figura acima nota-se que muito antes disso o modelo atinge o platô de desempenho. Isso sugere a diminuição do número de árvores, de modo a economizar custo computacional."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:17:20.13506Z","iopub.status.busy":"2020-08-30T20:17:20.133922Z","iopub.status.idle":"2020-08-30T20:18:25.98319Z","shell.execute_reply":"2020-08-30T20:18:25.982438Z"},"papermill":{"duration":65.897509,"end_time":"2020-08-30T20:18:25.983336","exception":false,"start_time":"2020-08-30T20:17:20.085827","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor(n_estimators = 30, n_jobs=-1, oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.04107,"end_time":"2020-08-30T20:18:26.066229","exception":false,"start_time":"2020-08-30T20:18:26.025159","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Reduzimos bastante o tempo computacional, sem prejudicar muito a acurácia!"},{"metadata":{"papermill":{"duration":0.041198,"end_time":"2020-08-30T20:18:26.148805","exception":false,"start_time":"2020-08-30T20:18:26.107607","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## max_samples\n\nO hiperparâmetro **max_samples** restringe o número de observações que serão amostradas por cada árvore durante o treino. É um bom truque para quando os conjuntos de dados são muito grandes: todos os dados ficam disponíveis para o treino do modelo, mas a amostragem de cada árvore se dá apenas em subconjuntos de tamanho **max_samples**. Isso reduz o custo computacional e pode ajudar a atenuar problemas de sobreajuste."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:18:26.239836Z","iopub.status.busy":"2020-08-30T20:18:26.238741Z","iopub.status.idle":"2020-08-30T20:19:13.434612Z","shell.execute_reply":"2020-08-30T20:19:13.433927Z"},"papermill":{"duration":47.244456,"end_time":"2020-08-30T20:19:13.43474","exception":false,"start_time":"2020-08-30T20:18:26.190284","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor(max_samples = 40000, n_jobs=-1, oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.042094,"end_time":"2020-08-30T20:19:13.519097","exception":false,"start_time":"2020-08-30T20:19:13.477003","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## min_samples_leaf e max_features\n\n* O hiperparâmetro **min_samples_leaf** especifica o número mínimo de amostras contidas em cada folha. Em outras palavras, determina o número necessário de amostras em um nó para interromper o crescimento de seu ramo. Aumentar **min_samples_leaf** faz com que as árvores sejam menos profundas, o que diminui a acurácia de cada árvore individual, mas também potencialmente diminui a correlação entre elas, melhorando a generalização.\n\n* O hiperparâmetro **max_features** especifica um número máximo de variáveis a ser considerado para decidir o split de cada nó. Diminuir esse número máximo diminui o efeito de variáveis muito influentes, diminuindo assim a correlação entre as árvores. A redução das variáveis disponíveis por split foi proposta no [paper original do modelo de florestas aleatórias](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) e muitos consideram que um modelo só poder ser considerado floresta aleatória se efetuar esse procedimento. O [scikit-learn](https://scikit-learn.org/), no entanto, por default não o efetua.\n\nVamos mexer nesses parâmetros e tentar obter um resultado melhor:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:19:13.609995Z","iopub.status.busy":"2020-08-30T20:19:13.609264Z","iopub.status.idle":"2020-08-30T20:20:57.527204Z","shell.execute_reply":"2020-08-30T20:20:57.526431Z"},"papermill":{"duration":103.966247,"end_time":"2020-08-30T20:20:57.527331","exception":false,"start_time":"2020-08-30T20:19:13.561084","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor(min_samples_leaf = 3, max_features = 0.5, \n                                           n_jobs=-1, oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.042595,"end_time":"2020-08-30T20:20:57.614309","exception":false,"start_time":"2020-08-30T20:20:57.571714","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Melhoramos! Diminuindo agora o número de árvores para obter um modelo mais eficiente, que usaremos nas análises que seguirão:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:20:57.707731Z","iopub.status.busy":"2020-08-30T20:20:57.706828Z","iopub.status.idle":"2020-08-30T20:21:50.818673Z","shell.execute_reply":"2020-08-30T20:21:50.817955Z"},"papermill":{"duration":53.161786,"end_time":"2020-08-30T20:21:50.818803","exception":false,"start_time":"2020-08-30T20:20:57.657017","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor(n_estimators = 50, min_samples_leaf = 3, \n                                           max_features = 0.5, n_jobs=-1, \n                                           oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.042351,"end_time":"2020-08-30T20:21:50.904116","exception":false,"start_time":"2020-08-30T20:21:50.861765","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Em suma, ajustando os hiperparâmetros da floresta, conseguimos, em relação ao modelo base:\n\n- melhorar a métrica de desempenho na terceira casa decimal. Na maioria das situações práticas isso não seria importante;\n- reduzir o esforço computacional.\n"},{"metadata":{"papermill":{"duration":0.042221,"end_time":"2020-08-30T20:21:51.163985","exception":false,"start_time":"2020-08-30T20:21:51.121764","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Interpretação do modelo e engenharia de características\n\nMuito se diz por aí que não é possível interpretar as predições feitas por modelos de aprendizado de máquina, por eles serem complicados e totalmente empíricos. \n\nContudo,e em particular, o modelo de florestas aleatórias pode fornecer muitas informações sobre a natureza das predições e as influências exercidas por cada variável nos resultados. Essas informações podem ser valiosas na importante atividade de [engenharia de características](https://en.wikipedia.org/wiki/Feature_engineering) (mais conhecida pela expressão em inglês *feature engineering*), que consiste na manipulação das variáveis (colunas) do conjunto de dados com o objetivo de melhorar o desempenho dos modelos."},{"metadata":{"papermill":{"duration":0.042156,"end_time":"2020-08-30T20:21:51.24912","exception":false,"start_time":"2020-08-30T20:21:51.206964","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Importâncias das variáveis\n\nO modelo de florestas aleatórias calcula internamente um ranking de importância das variáveis. Para uma dada variável, quanto maior a diminuição do erro em splits de decisões tomadas com base nessa variável, mais importante ela será. Esse ranking fica armazenado no atributo **feature_importances_** do modelo. \n\nNa próxima célula, definimos uma função que aceita um modelo e uma lista com os nomes das variáveis, imprime na tela informações relativas ao ranking de importância e retorna um [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) com o ranking em si."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:21:51.34697Z","iopub.status.busy":"2020-08-30T20:21:51.346141Z","iopub.status.idle":"2020-08-30T20:21:51.383026Z","shell.execute_reply":"2020-08-30T20:21:51.383655Z"},"papermill":{"duration":0.091988,"end_time":"2020-08-30T20:21:51.383812","exception":false,"start_time":"2020-08-30T20:21:51.291824","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def plotar_importancias(modelo, tags, n=10):\n    \n    fig, ax = plt.subplots(1,2, figsize = (20,4))\n\n    coefs = []\n    abs_coefs = []\n\n    if hasattr(modelo,'coef_'):\n        imp = modelo.coef_\n    elif hasattr(modelo,'feature_importances_'):\n        imp = modelo.feature_importances_\n    else:\n        print('sorry, nao vai rolar!')\n        return\n\n    coefs = (pd.Series(imp, index = tags))\n    coefs.plot(use_index=False, ax=ax[0]);\n    abs_coefs = (abs(coefs)/(abs(coefs).sum()))\n    abs_coefs.sort_values(ascending=False).plot(use_index=False, ax=ax[1],marker='.')\n\n    ax[0].set_title('Importâncias relativas das variáveis')\n    ax[1].set_title('Importâncias relativas das variáveis - ordem decrescente')\n\n    abs_coefs_df = pd.DataFrame(np.array(abs_coefs).T,\n                                columns = ['Importancias'],\n                                index = tags)\n\n    df = abs_coefs_df['Importancias'].sort_values(ascending=False)\n    \n    print(df.iloc[0:n])\n    plt.figure()\n    df.iloc[0:n].plot(kind='barh', figsize=(15,0.25*n), legend=False)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.042153,"end_time":"2020-08-30T20:21:51.468186","exception":false,"start_time":"2020-08-30T20:21:51.426033","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Utilizando a função para analisar as importâncias do nosso último modelo:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:21:51.55914Z","iopub.status.busy":"2020-08-30T20:21:51.558308Z","iopub.status.idle":"2020-08-30T20:21:52.494948Z","shell.execute_reply":"2020-08-30T20:21:52.494194Z"},"papermill":{"duration":0.984424,"end_time":"2020-08-30T20:21:52.495079","exception":false,"start_time":"2020-08-30T20:21:51.510655","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"imp = plotar_importancias(m, X_validacao.columns,30)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.043648,"end_time":"2020-08-30T20:21:52.582842","exception":false,"start_time":"2020-08-30T20:21:52.539194","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Removendo variáveis pouco importantes\n\nVariáveis sem importância podem ser descartadas, o que talvez melhore a acurácia do modelo e certamente melhorará o desempenho computacional.\n\nSelecionando, por exemplo, apenas as que apresentam mais que 0,5% importância:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:21:52.678312Z","iopub.status.busy":"2020-08-30T20:21:52.677565Z","iopub.status.idle":"2020-08-30T20:21:52.713321Z","shell.execute_reply":"2020-08-30T20:21:52.713907Z"},"papermill":{"duration":0.08733,"end_time":"2020-08-30T20:21:52.714075","exception":false,"start_time":"2020-08-30T20:21:52.626745","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"to_keep = imp[imp>0.005].index\nto_keep.shape","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:21:52.808411Z","iopub.status.busy":"2020-08-30T20:21:52.807613Z","iopub.status.idle":"2020-08-30T20:21:52.854518Z","shell.execute_reply":"2020-08-30T20:21:52.855141Z"},"papermill":{"duration":0.09693,"end_time":"2020-08-30T20:21:52.855319","exception":false,"start_time":"2020-08-30T20:21:52.758389","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"X_treino = X_treino[to_keep]\nX_validacao = X_validacao[to_keep]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.044444,"end_time":"2020-08-30T20:21:52.944282","exception":false,"start_time":"2020-08-30T20:21:52.899838","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Eliminamos por volta de 30 variáveis irrelevantes."},{"metadata":{"papermill":{"duration":0.044148,"end_time":"2020-08-30T20:21:53.033291","exception":false,"start_time":"2020-08-30T20:21:52.989143","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Análise de correlações\n\nUma análise de correlações é útil para entender as relações entre as variáveis.\n\nA correlação mais utilizada para isso é a [correlação de Pearson](https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_Pearson), que mede o grau de *associação linear* entre as variáveis. Duas variáveis são linearmente associadas se mudanças em uma variável implicam em mudanças diretamente proporcionais na outra variável.\n\nAqui usaremos a [correlação de Spearman](https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_postos_de_Spearman), que mede o grau de *associação monotônica* entre as variáveis. Duas variáveis são monotonicamente associadas se mudanças em uma variável implicam em mudanças no mesmo sentido (crescente ou decrescente) na outra variável. É uma concepção de associação mais genérica do que a de Pearson.\n\nA função abaixo aceita um [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) e plota um gráfico do tipo [dendograma](https://en.wikipedia.org/wiki/Dendrogram) mostrando as correlações de Spearman entre as variáveis:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:21:53.130432Z","iopub.status.busy":"2020-08-30T20:21:53.129629Z","iopub.status.idle":"2020-08-30T20:21:53.164492Z","shell.execute_reply":"2020-08-30T20:21:53.165086Z"},"papermill":{"duration":0.087756,"end_time":"2020-08-30T20:21:53.165274","exception":false,"start_time":"2020-08-30T20:21:53.077518","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def dendogram_spearmanr(df, tags):\n\n    import scipy.cluster.hierarchy\n    import scipy.stats\n    \n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = scipy.cluster.hierarchy.distance.squareform(1-corr)\n    z = scipy.cluster.hierarchy.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=(18,8))\n    dendrogram = scipy.cluster.hierarchy.dendrogram(z, labels=tags, orientation='left', leaf_font_size=16)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:21:53.260054Z","iopub.status.busy":"2020-08-30T20:21:53.259247Z","iopub.status.idle":"2020-08-30T20:21:54.235958Z","shell.execute_reply":"2020-08-30T20:21:54.235137Z"},"papermill":{"duration":1.025589,"end_time":"2020-08-30T20:21:54.236089","exception":false,"start_time":"2020-08-30T20:21:53.2105","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"dendogram_spearmanr(X_treino, X_treino.columns)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.045594,"end_time":"2020-08-30T20:21:54.328007","exception":false,"start_time":"2020-08-30T20:21:54.282413","status":"completed"},"tags":[]},"cell_type":"markdown","source":"O gráfico foi gerado por meio de técnicas de [clusterização hierárquica](https://en.wikipedia.org/wiki/Hierarchical_clustering), que separaram as variáveis em grupos de acordo com as correlações entre elas. É evidente que algumas variáveis possuem altíssima correlação, como **GrouserTracks**, **Hydraulics_Flow** e **Coupler_System**, por exemplo. Isso significa que elas possuem a mesma informação e são potencialmente redundantes.\n\nNas próximas células, removeremos algumas variáveis que o gráfico indica como redundantes e verificaremos o efeito no OOB. Caso o efeito seja pequeno, podemos descartar as variáveis."},{"metadata":{"papermill":{"duration":0.045675,"end_time":"2020-08-30T20:21:54.419904","exception":false,"start_time":"2020-08-30T20:21:54.374229","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Removendo variáveis redundantes\n\nA função abaixo é definida para agilizar as análises: ela aceita um conjunto X, efetua um treino e retorna o score OOB."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:21:54.519782Z","iopub.status.busy":"2020-08-30T20:21:54.518698Z","iopub.status.idle":"2020-08-30T20:21:54.562435Z","shell.execute_reply":"2020-08-30T20:21:54.561797Z"},"papermill":{"duration":0.096585,"end_time":"2020-08-30T20:21:54.562577","exception":false,"start_time":"2020-08-30T20:21:54.465992","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def get_oob(X):\n    m = sklearn.ensemble.RandomForestRegressor(n_estimators=30, min_samples_leaf=5, \n                                               max_features=0.6, n_jobs=-1, max_samples = 100000,\n                                               oob_score=True, random_state = 0)\n    m.fit(X, y_treino)\n    return m.oob_score_","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.045944,"end_time":"2020-08-30T20:21:54.654959","exception":false,"start_time":"2020-08-30T20:21:54.609015","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Estabelecendo nossa referência:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:21:54.752391Z","iopub.status.busy":"2020-08-30T20:21:54.751625Z","iopub.status.idle":"2020-08-30T20:22:04.49349Z","shell.execute_reply":"2020-08-30T20:22:04.494057Z"},"papermill":{"duration":9.792916,"end_time":"2020-08-30T20:22:04.494219","exception":false,"start_time":"2020-08-30T20:21:54.701303","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"get_oob(X_treino)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.044434,"end_time":"2020-08-30T20:22:04.583839","exception":false,"start_time":"2020-08-30T20:22:04.539405","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Efetuando análises com remoções de uma variável potencialmente redundante por vez:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:22:04.695547Z","iopub.status.busy":"2020-08-30T20:22:04.694322Z","iopub.status.idle":"2020-08-30T20:23:11.923205Z","shell.execute_reply":"2020-08-30T20:23:11.922489Z"},"papermill":{"duration":67.294502,"end_time":"2020-08-30T20:23:11.923323","exception":false,"start_time":"2020-08-30T20:22:04.628821","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"for c in ('Grouser_Tracks', 'Hydraulics_Flow', 'Coupler_System',\n          'fiModelDesc', 'fiBaseModel', 'ProductGroup'):\n    print(c, get_oob(X_treino.drop(c, axis=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.045776,"end_time":"2020-08-30T20:23:12.016208","exception":false,"start_time":"2020-08-30T20:23:11.970432","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Nenhuma dessas variáveis parece fazer falta, já que o OOB não diminui significativamente!\n\nEfetuando de fato a remoção de 3 delas:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:23:12.113595Z","iopub.status.busy":"2020-08-30T20:23:12.11278Z","iopub.status.idle":"2020-08-30T20:23:21.679818Z","shell.execute_reply":"2020-08-30T20:23:21.679152Z"},"papermill":{"duration":9.618311,"end_time":"2020-08-30T20:23:21.679953","exception":false,"start_time":"2020-08-30T20:23:12.061642","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"to_drop = ['fiModelDesc', 'Grouser_Tracks', 'Hydraulics_Flow']\nget_oob(X_treino.drop(to_drop, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:23:21.779173Z","iopub.status.busy":"2020-08-30T20:23:21.778136Z","iopub.status.idle":"2020-08-30T20:23:21.840182Z","shell.execute_reply":"2020-08-30T20:23:21.841036Z"},"papermill":{"duration":0.114306,"end_time":"2020-08-30T20:23:21.841268","exception":false,"start_time":"2020-08-30T20:23:21.726962","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"X_treino = X_treino.drop(to_drop, axis=1)\nX_treino.shape","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.06538,"end_time":"2020-08-30T20:23:21.974062","exception":false,"start_time":"2020-08-30T20:23:21.908682","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Com esse procedimento, conseguimos diminuir ainda mais o número de variáveis."},{"metadata":{"papermill":{"duration":0.045736,"end_time":"2020-08-30T20:23:22.254326","exception":false,"start_time":"2020-08-30T20:23:22.20859","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Variáveis *dummy* ou *one-hot encoding*\n\nO uso de [variáveis *dummy*](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) ou *one-hot encoding* é uma estratégia diferente para organizar variáveis categóricas. Nessa metodologia, as variáveis são desmembradas em variáveis binárias correspondentes a cada uma de suas categorias. A figura a seguir ilustra bem a situação:\n\n<img src=\"https://i1.wp.com/thierrymoudiki.github.io/images/2020-02-28/2020-02-28-image1.png?w=578&ssl=1\" width=\"500\" height=\"500\"/>\n\nA vantagem dessa representação é que a influência de categorias específicas pode tornar-se mais clara. Em alguns casos, a acurácia do modelo pode aumentar (em modelos de florestas aleatórias, no entanto, isso não é comum). A desvantagem é que a dimensionalidade dos dados (número de colunas) aumenta, diminuindo o desempenho computacional.\n\nA seguir definimos uma nova função para pré-processamento, que utiliza por sua vez a função [get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) do [pandas](https://pandas.pydata.org/) para implementar o *one-hot encoding*. Há a especificação de um número máximo de categorias, expressa pelo argumento **max_cats**, que evita o aumento excessivo da dimensionalidade no caso de variáveis de alta cardinalidade. Caso uma variável tenha mais do que **max_cats** categorias, não são geradas variáveis *dummy* a partir dela."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:23:22.355934Z","iopub.status.busy":"2020-08-30T20:23:22.354899Z","iopub.status.idle":"2020-08-30T20:23:22.390528Z","shell.execute_reply":"2020-08-30T20:23:22.3899Z"},"papermill":{"duration":0.090113,"end_time":"2020-08-30T20:23:22.390658","exception":false,"start_time":"2020-08-30T20:23:22.300545","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def pre_process_OHE (df, max_cats = 10):\n    \n    new_df = pd.DataFrame()\n    \n    for n,c in df.items():\n                \n        if pd.api.types.is_numeric_dtype(c):\n            # substituindo NaN numericos pelas medianas de cada coluna\n            new_df[n] = c.fillna(value=c.median())\n        else:\n            # interpretando o que nao for numerico como variaveis categoricas \n            new_df[n] = pd.Categorical(c.astype('category').cat.as_ordered())\n            # transformando cada categoria em um numero, caso nao va fazer one hot encoding com ela\n            if len(c.astype('category').cat.categories) > max_cats:\n                new_df[n] = pd.Categorical(new_df[n]).codes+1\n    \n    # a função pd.get_dummies faz o one-hot encoding\n    return pd.get_dummies(new_df)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.04575,"end_time":"2020-08-30T20:23:22.482523","exception":false,"start_time":"2020-08-30T20:23:22.436773","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Dando uma conferida no [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) original, para fins de comparação:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:23:22.579479Z","iopub.status.busy":"2020-08-30T20:23:22.57873Z","iopub.status.idle":"2020-08-30T20:23:23.014097Z","shell.execute_reply":"2020-08-30T20:23:23.013489Z"},"papermill":{"duration":0.485225,"end_time":"2020-08-30T20:23:23.014215","exception":false,"start_time":"2020-08-30T20:23:22.52899","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"df_raw","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.048229,"end_time":"2020-08-30T20:23:23.10945","exception":false,"start_time":"2020-08-30T20:23:23.061221","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Pré-processando o [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) com *one-hot encoding*:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:23:23.213182Z","iopub.status.busy":"2020-08-30T20:23:23.212139Z","iopub.status.idle":"2020-08-30T20:23:28.32728Z","shell.execute_reply":"2020-08-30T20:23:28.326661Z"},"papermill":{"duration":5.167471,"end_time":"2020-08-30T20:23:28.327435","exception":false,"start_time":"2020-08-30T20:23:23.159964","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"df_proc_ohe = pre_process_OHE(df_raw)\ndf_proc_ohe","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.04717,"end_time":"2020-08-30T20:23:28.423249","exception":false,"start_time":"2020-08-30T20:23:28.376079","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Quase triplicamos a quantidade de variáveis! Inicialmente tínhamos 53 colunas, agora temos 145!\n\nAgora vamos repetir o processo de treinamento e análise de variáveis com o novo conjunto de dados."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:23:28.526223Z","iopub.status.busy":"2020-08-30T20:23:28.525483Z","iopub.status.idle":"2020-08-30T20:23:28.743036Z","shell.execute_reply":"2020-08-30T20:23:28.742415Z"},"papermill":{"duration":0.272273,"end_time":"2020-08-30T20:23:28.743172","exception":false,"start_time":"2020-08-30T20:23:28.470899","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"X, y = df_proc_ohe.drop('SalePrice', axis=1), df_proc_ohe['SalePrice']\n\nn_valid = 12000\nn_trn = len(df_proc)-n_valid\n\nX_treino, X_validacao = X[:n_trn].copy(), X[n_trn:].copy()\ny_treino, y_validacao = y[:n_trn].copy(), y[n_trn:].copy()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:23:28.847132Z","iopub.status.busy":"2020-08-30T20:23:28.84633Z","iopub.status.idle":"2020-08-30T20:24:47.5524Z","shell.execute_reply":"2020-08-30T20:24:47.551775Z"},"papermill":{"duration":78.761262,"end_time":"2020-08-30T20:24:47.552536","exception":false,"start_time":"2020-08-30T20:23:28.791274","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor(n_estimators = 50, min_samples_leaf = 3, \n                                           max_features = 0.5, n_jobs=-1, \n                                           oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:24:47.654911Z","iopub.status.busy":"2020-08-30T20:24:47.654092Z","iopub.status.idle":"2020-08-30T20:24:48.735124Z","shell.execute_reply":"2020-08-30T20:24:48.734406Z"},"papermill":{"duration":1.134471,"end_time":"2020-08-30T20:24:48.735248","exception":false,"start_time":"2020-08-30T20:24:47.600777","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"plotar_importancias(m, X_validacao.columns,30)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.050101,"end_time":"2020-08-30T20:24:48.835757","exception":false,"start_time":"2020-08-30T20:24:48.785656","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Olha que interessante: a categoria **w AC** de Enclosure_EROPS é a mais relevante para a determinação do preço (o que faz sentido, já que **w AC** significa \"com ar-condicionado\")."},{"metadata":{"papermill":{"duration":0.051008,"end_time":"2020-08-30T20:24:48.936207","exception":false,"start_time":"2020-08-30T20:24:48.885199","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Análise de contribuições\n\nÁrvores e florestas podem ser interpretadas! Em particular, é possível entender o *porquê* de uma predição em específico, analisando as decisões que as árvores tomam para chegar a essa predição.\n\nO módulo [treeinterpreter](https://pypi.org/project/treeinterpreter/) serve justamente para isso:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:24:49.042474Z","iopub.status.busy":"2020-08-30T20:24:49.041668Z","iopub.status.idle":"2020-08-30T20:25:08.248524Z","shell.execute_reply":"2020-08-30T20:25:08.249159Z"},"papermill":{"duration":19.263059,"end_time":"2020-08-30T20:25:08.249317","exception":false,"start_time":"2020-08-30T20:24:48.986258","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"!pip install treeinterpreter\n!pip install waterfallcharts","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:25:08.357848Z","iopub.status.busy":"2020-08-30T20:25:08.356832Z","iopub.status.idle":"2020-08-30T20:25:08.40219Z","shell.execute_reply":"2020-08-30T20:25:08.401588Z"},"papermill":{"duration":0.102259,"end_time":"2020-08-30T20:25:08.40232","exception":false,"start_time":"2020-08-30T20:25:08.300061","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from treeinterpreter import treeinterpreter as ti\nimport waterfall_chart","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.050088,"end_time":"2020-08-30T20:25:08.50286","exception":false,"start_time":"2020-08-30T20:25:08.452772","status":"completed"},"tags":[]},"cell_type":"markdown","source":"[treeinterpreter](https://pypi.org/project/treeinterpreter/) decompõe uma predição na soma *bias+contribuições*. O bias é a média da variável predita. As contribuições refletem o quanto cada variável contribui para afastar uma predição em específico dessa média.\n\nAs contribuições de cada variável são calculadas por meio dos efeitos na predição dos vários splits que a envolvem. Esse [artigo](http://blog.datadive.net/interpreting-random-forests/) detalha bem a ideia.\n\nVamos dar uma olhada nas contribuições da primeira linha do conjunto de validação:"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:25:08.612058Z","iopub.status.busy":"2020-08-30T20:25:08.611201Z","iopub.status.idle":"2020-08-30T20:26:04.193642Z","shell.execute_reply":"2020-08-30T20:26:04.194257Z"},"papermill":{"duration":55.64111,"end_time":"2020-08-30T20:26:04.194444","exception":false,"start_time":"2020-08-30T20:25:08.553334","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"row = X_validacao.values[np.newaxis,0]\n\nprediction, bias, contributions = ti.predict(m, row)\n\nidxs = np.argsort(contributions[0])\n[o for o in zip(X_validacao.columns[idxs], X_validacao.iloc[0][idxs], contributions[0][idxs])]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.051148,"end_time":"2020-08-30T20:26:04.296995","exception":false,"start_time":"2020-08-30T20:26:04.245847","status":"completed"},"tags":[]},"cell_type":"markdown","source":"A lista acima está organizada em ordem crescente de contribuições.\n\nDela podemos inferir que o que mais contribui para diminuir o preço do trator em questão é seu tamanho pequeno (**ProductSize_Mini**) e o que mais contribui para aumentar o preço é o ano de fabricação (1999). O que parece fazer todo sentido.\n\nUma boa maneira de visualizar essa análise é utilizando um [gráfico do tipo cascata](https://en.wikipedia.org/wiki/Waterfall_chart):"},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:26:04.407134Z","iopub.status.busy":"2020-08-30T20:26:04.406265Z","iopub.status.idle":"2020-08-30T20:26:04.811896Z","shell.execute_reply":"2020-08-30T20:26:04.811247Z"},"papermill":{"duration":0.463049,"end_time":"2020-08-30T20:26:04.812031","exception":false,"start_time":"2020-08-30T20:26:04.348982","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"waterfall_chart.plot(X_validacao.columns, contributions[0], threshold=0.08, \n                     rotation_value=90,formatting='{:,.3f}');","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.051765,"end_time":"2020-08-30T20:26:04.915743","exception":false,"start_time":"2020-08-30T20:26:04.863978","status":"completed"},"tags":[]},"cell_type":"markdown","source":"No gráfico acima, cada barra representa a contribuição positiva (barras verdes) ou negativa (barras vermelhas) de cada variável para o desvio em relação à média. A última barra, de cor azul, indica o desvio total da predição em relação à média."},{"metadata":{"papermill":{"duration":0.05263,"end_time":"2020-08-30T20:26:06.803262","exception":false,"start_time":"2020-08-30T20:26:06.750632","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Modelo final\n\nChegou a hora de gerarmos nosso modelo final! Utilizaremos todos os dados para treinar. A métrica de avaliação será o OOB."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-30T20:26:06.919427Z","iopub.status.busy":"2020-08-30T20:26:06.918289Z","iopub.status.idle":"2020-08-30T20:27:18.070275Z","shell.execute_reply":"2020-08-30T20:27:18.069662Z"},"papermill":{"duration":71.213821,"end_time":"2020-08-30T20:27:18.070415","exception":false,"start_time":"2020-08-30T20:26:06.856594","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"df_final = pre_process(df_raw)\n\nX, y = df_final.drop('SalePrice', axis=1)[to_keep].drop(to_drop, axis=1), df_final['SalePrice']\n\nm = sklearn.ensemble.RandomForestRegressor(min_samples_leaf = 3, \n                                           max_features = 0.5, n_jobs=-1, \n                                           oob_score = True, random_state = 0)\n%time m.fit(X, y)\nm.oob_score_","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.052783,"end_time":"2020-08-30T20:27:18.176695","exception":false,"start_time":"2020-08-30T20:27:18.123912","status":"completed"},"tags":[]},"cell_type":"markdown","source":"O próximo passo natural seria aplicar o modelo final a um conjunto de teste em separado. Nosso exemplo, no entanto, veio de uma competição encerrada do Kaggle que não permite mais submeter o modelo e avaliar o desempenho no conjunto de teste da competição.\n\nContudo, em problemas da vida real, devemos separar sempre um conjunto de teste para ser usado apenas na avaliação do modelo final!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}