{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ðŸšœ Predicting the Sale Price of Bulldozers using Machine Learning\n\n## Problem Definition\n\nFor this dataset, the problem we're trying to solve, or better, the question we're trying to answer is,\n\nHow well can we predict the future sale price of a bulldozer, given its characteristics previous examples of how much similar bulldozers have been sold for?\n\n## Data\n\nLooking at the dataset from Kaggle, you can you it's a time series problem. This means there's a time attribute to dataset.\n\nIn this case, it's historical sales data of bulldozers. Including things like, model type, size, sale date and more.\n\nThere are 3 datasets:\n\n    - Train.csv - Historical bulldozer sales examples up to 2011 (close to 400,000 examples with 50+ different attributes, including SalePrice which is the target variable).\n    - Valid.csv - Historical bulldozer sales examples from January 1 2012 to April 30 2012 (close to 12,000 examples with the same attributes as Train.csv).\n    - Test.csv - Historical bulldozer sales examples from May 1 2012 to November 2012 (close to 12,000 examples but missing the SalePrice attribute, as this is what we'll be trying to predict).\n    \n## Evaluation\n\nFor this problem, Kaggle has set the evaluation metric to being root mean squared log error (RMSLE). As with many regression evaluations, the goal will be to get this value as low as possible.\n\nTo see how well our model is doing, we'll calculate the RMSLE and then compare our results to others on the Kaggle leaderboard.\n\n## Features\nFeatures are different parts of the data. During this step, you'll want to start finding out what you can about the data.\n\nOne of the most common ways to do this, is to create a data dictionary.\n\nFor this dataset, Kaggle provide a data dictionary which contains information about what each attribute of the dataset means. You can download this file directly from the Kaggle competition page (account required) or view it on Google Sheets.\n\nWith all of this being known, let's get started!\n\nFirst, we'll import the dataset and start exploring. Since we know the evaluation metric we're trying to minimise, our first goal will be building a baseline model and seeing how it stacks up against the competition.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Importing the data and preparing it for model\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/bluebook-for-bulldozers/TrainAndValid.csv\",\n                 low_memory=False,\n                error_bad_lines=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are total of 52 columns present here where `SalePrice` is the target column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig , ax = plt.subplots(figsize = (20,10))\nax.scatter(df['saledate'][:1000],df['SalePrice'][:1000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.SalePrice.plot.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parsing dates\n\n    In the dataframe `Saledate` column is in object type we have convert it into a date type.\n    \n    We can do this using parse_dates feature in the read_csv function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/bluebook-for-bulldozers/TrainAndValid.csv', \n                 low_memory=False,\n                 parse_dates=['saledate']\n                )\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig , ax = plt.subplots()\nax.scatter(df[\"saledate\"][:1000], df[\"SalePrice\"][:1000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.saledate.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sorting the Dataframe by saledate\n\nSince we are working in a timeseries problem we have make our data as a historical one we can do that by sorting the saledate column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sort_values(by=['saledate'],ascending = True,inplace = True)\ndf.saledate.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making a copy of the original data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp = df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding date time parameters seperately for the saledate column\n\nWhy?\n\nSo we can enrich our dataset with as much information as possible.\n\nBecause we imported the data using read_csv() and we asked pandas to parse the dates using parase_dates=[\"saledate\"], we can now access the different datetime attributes of the saledate column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp['saleyear'] = df_tmp.saledate.dt.year\ndf_tmp['salemonth'] = df_tmp.saledate.dt.month\ndf_tmp['saleday'] = df_tmp.saledate.dt.day\ndf_tmp['saledayofweek'] = df_tmp.saledate.dt.dayofweek\ndf_tmp['saledayofyear'] = df_tmp.saledate.dt.dayofyear\n\n#dropping the original saledate column\ndf_tmp.drop('saledate',axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Converting strings to categories\n\nOne way to help turn all of our data into numbers is to convert the columns with the string datatype into a category datatype.\n\nwhy categorical ?\n\n **Under the hood pandas will handle all the categorical objects as numerical.**\n\nTo do this we can use the pandas api types which allows us to interact and manipulate the types of data.\n\nlink for reference is https://pandas.pydata.org/pandas-docs/stable/reference/general_utility_functions.html#dtype-introspection \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To check whether a column is string we use \n\npd.api.types.is_string_dtype(df_tmp['UsageBand'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Thse columns contains string\n\nfor label , content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### To change all the string columns into categorical\n\nfor label , content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        df_tmp[label] = content.astype('category').cat.as_ordered()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp.state.cat.categories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pandas will convert all the categorical into numbers to view it we have to see using the `.codes` and for null values -1 will be assigned**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp.state.cat.codes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling the missing values\n\nOur data has a plenty of missing values it seems uff..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filling the numerical missing values\n \nWe are going to fill all the numerical missing values with `median`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for label , content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        print(label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the numerical columns present..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for label , content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the columns that have missing numerical values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for label , content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            #Adding a binary column which tells if the data is missing or not\n            df_tmp[label+'_is_missing'] = pd.isnull(content)\n            #Filling the numeric place with the median value\n            df_tmp[label] = content.fillna(content.median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Why add a binary column indicating whether the data was missing or not?\n\nWe can easily fill all of the missing numeric values in our dataset with the median. However, a numeric value may be missing for a reason. In other words, absence of evidence may be evidence of absence. Adding a binary column which indicates whether the value was missing or not helps to retain this information.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Filling and turning categorical to numbers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for label , content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        # Add binary column to inidicate whether sample had missing value\n        df_tmp[label+\"_is_missing\"] = pd.isnull(content)\n        # We add the +1 because pandas encodes missing categories as -1\n        df_tmp[label] = pd.Categorical(content).codes+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**we have filled all the missing values and also changed the categorical variables into numerical using the pandas generated codes for the categories**","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_tmp.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_tmp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now there are 103 columns in the training dataframe including the target column.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Model building\n\n### Training and validation set splitting\n\nThere is a test.csv file which we have to predict, to tune the model hyperparameters also to improve the score we are splitting the data into train and validation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp.saleyear","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are values from the year of 1989 to 2012..., So we are training our model from the 1989 to 2011 and the validation set consists of all the attributes in the 2012 saleyear.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp.saleyear.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_val = df_tmp[df_tmp.saleyear == 2012]\ndf_train = df_tmp[df_tmp.saleyear != 2012]\n\nlen(df_train) , len(df_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train , y_train = df_train.drop('SalePrice',axis = 1),df_train['SalePrice']\nX_val , y_val = df_val.drop('SalePrice',axis = 1),df_val['SalePrice']\n\nX_train.shape , y_train.shape , X_val.shape , y_val.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building evaluation function\n\nWe know that in this the evaluation function is RMSLE - Root Mean Squared Log Error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error , mean_absolute_error,r2_score\n\n\n#Function to return the RMSLE\n\ndef rmsle(y_test , y_preds):\n    \"\"\"\n    Caculates Root mean squared log error for given y_true and y_preds\n    \"\"\"\n    return np.sqrt(mean_squared_log_error(y_test,y_preds))\n\n\n#Function to evaluate model on different metrics\n\ndef show_scores(model):\n    \n    train_preds = model.predict(X_train)\n    val_preds = model.predict(X_val)\n    \n    scores = {'Training MAE':mean_absolute_error(y_train,train_preds),\n              'Validation MAE':mean_absolute_error(y_val,val_preds),\n              'Training RMSLE':rmsle(y_train,train_preds),\n              'Validation RMSLE':rmsle(y_val,val_preds),\n              'Training R2':r2_score(y_train,train_preds),\n              'Validation R2':r2_score(y_val,val_preds)\n             }\n    \n\n    return scores\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are large number of samples present in the training set, hence what we do setting the `max_samples = 10000` so training will be done only on the 10000 samples which will reduce the time for the traninig.\n\n**We use this 10,000 samples to tune the hyperparameter and then using the best params we will train the whole dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmodel = RandomForestRegressor(random_state=42,\n                              max_samples=10000)\n\nmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_scores(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RandomizedSearchCV for Hyperparameter tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrf_grid = {'n_estimators':np.arange(10,100,10),\n           'max_depth':[None,3,5,10],\n           'min_samples_split': np.arange(2,20,2),\n           'min_samples_leaf': np.arange(1,20,2),\n           'max_features': [0.5,1,'sqrt','auto'],\n           'max_samples' : [10000]\n          }\n\nrs_model = RandomizedSearchCV(RandomForestRegressor(random_state=42),\n                             param_distributions=rf_grid,\n                             n_iter=10,\n                             cv = 5,\n                             verbose=True\n                             )\n\nrs_model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_scores(rs_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the model with the best params","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nideal_model = RandomForestRegressor( n_estimators= 60,\n                                     min_samples_split= 10,\n                                     min_samples_leaf= 1,\n                                     max_features= 'auto',\n                                     max_depth= 10,\n                                    random_state = 42)\n\nideal_model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_scores(ideal_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction on test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('../input/bluebook-for-bulldozers/Test.csv',\n                      low_memory=False,\n                      parse_dates=['saledate']\n                     )\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocess the Test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_data(df):\n    \"\"\"\n    perform the transformations on the data and returns it\n    \"\"\"\n    df['saleyear'] = df.saledate.dt.year\n    df['salemonth'] = df.saledate.dt.month\n    df['saleday'] = df.saledate.dt.day\n    df['saledayofweek'] = df.saledate.dt.dayofweek\n    df['saledayofyear'] = df.saledate.dt.dayofyear\n    \n    df.drop('saledate',axis=1,inplace = True)\n    \n    #Filling the numeric rows with median    \n    for label , content in df.items():\n        if pd.api.types.is_numeric_dtype(content):\n            if pd.isnull(content).sum():\n                df[label+'_is_missing'] = pd.isnull(content)\n                \n                df[label] = content.fillna(content.median())\n                \n        #Filling the categorical missing data and turn categories into numeric\n        if not pd.api.types.is_numeric_dtype(content):\n            df[label+'_is_missing'] = pd.isnull(content)\n            \n            df[label] = pd.Categorical(content).codes+1\n            \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = preprocess_data(df_test)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** After the preproceesing of test data there is one column missing in that when compared to the training data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape , df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(X_train.columns) - set(df_test.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that `auctioneer_ID_is_missing` columns is not present in the test dataset so we add that column manually","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['auctioneerID_is_missing'] = False\n\ndf_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making prediction on our ideal_model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = ideal_model.predict(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Format predictions into the same format Kaggle is after\ndf_preds = pd.DataFrame()\ndf_preds[\"SalesID\"] = df_test[\"SalesID\"]\ndf_preds[\"SalesPrice\"] = test_preds\ndf_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating submission file for kaggle","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preds.to_csv('submission1.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importances","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ideal_model.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Funtion for plotting feature importances\ndef plot_features(columns , importances , n=20):\n    \"\"\"\n    To plot the important features that makes the prediction\n    \"\"\"\n    df = (pd.DataFrame({'features':columns,\n                        'feature_importances':importances})\n                     .sort_values('feature_importances',ascending=False)\n                     .reset_index(drop=True))\n    \n    fig,ax = plt.subplots()\n    ax.barh(df['features'][:n] , df['feature_importances'][:20])\n    ax.set_ylabel('Features')\n    ax.set_xlabel('Feature importance')\n    ax.invert_yaxis()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_features(X_train.columns , ideal_model.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**These are the importances of the feature that lead to the prediction of the model**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}