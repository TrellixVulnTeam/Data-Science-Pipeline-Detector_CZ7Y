{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip uninstall fastai -y\n!pip install fastai==0.7.0\n!pip list | grep fast","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from fastai.imports import *\nfrom fastai.structured import *\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom pandas_summary import DataFrameSummary\nfrom IPython.display import display\n\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed6f731c090fd1ac86f9f0c0f8c3f3e9ce8f2eea"},"cell_type":"code","source":"df_raw = pd.read_csv('../input/train/Train.csv', low_memory=False, parse_dates=['saledate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bee9056ee454eebe934a4c8a741677acf4779a2"},"cell_type":"code","source":"def display_all(df):\n    with pd.option_context('display.max_rows', 1000):\n        with pd.option_context('display.max_columns', 1000):\n            display(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c996ac3c4ed9a69594df92c273ce4e807416ab31"},"cell_type":"code","source":"display_all(df_raw.tail().transpose())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c03e4af3254134bed8ca9b2207f03dac9e168200"},"cell_type":"code","source":"df_raw.SalePrice = np.log(df_raw.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7435d7a24ad6ffe03ec44e9212297440a50eb301"},"cell_type":"code","source":"train_cats(df_raw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61887c21e9c9bed5bd579d0a25f0b21ef9f1da76"},"cell_type":"code","source":"df_raw.UsageBand.cat.categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"342201ad399f7b08994a489be142cc31960ced69"},"cell_type":"code","source":"df_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'], ordered=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs('tmp', exist_ok=True)\ndf_raw.to_feather('tmp/raw')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8943bfc82c5d9fd38c1f541884afae333f17e2cb"},"cell_type":"code","source":"add_datepart(df_raw, 'saledate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d68f806fa2d59b4feed9cb294e156375da8da310"},"cell_type":"code","source":"df, y, nas = proc_df(df_raw, 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5037ea45555b2f3e955c9b5447e4ce8f8a0f4064"},"cell_type":"code","source":"m = RandomForestRegressor(n_jobs=-1)\nm.fit(df, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5289a4fabb2d05dda695fd122b414f9ae6bd6b3a"},"cell_type":"code","source":"m.score(df, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_vals(a, n): return a[:n].copy(), a[n:].copy()\n\nn_valid = 12000 # Same as kaggle's test set size\nn_trn = len(df)-n_valid\nraw_train, raw_valid = split_vals(df_raw, n_trn)\nX_train, X_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(x, y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n              m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'):\n        res.append(m.oob_score_)\n    print(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice', subset=30000)\nX_train, _ = split_vals(df_trn, 20000)\ny_train, _ = split_vals(y_trn, 20000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw tree here\nm = RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bagging\n# Training multiple trees with rows chosen at random\n# So that each tree has a different insight on the data\nm = RandomForestRegressor(n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.stack([t.predict(X_valid) for t in m.estimators_])\npreds[:,0], np.mean(preds[:,0]), y_valid[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our forest predicted 9.3, real value was 9.1\npreds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we see adding more and more trees mean increasing the r_sqaured\n# Let's try adding more trees\nm = RandomForestRegressor(n_estimators=20, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that even though we are doubling the number of trees, the r_sqaured on validation set is still 0.7\n\nSo after a point adding more and more trees does not make sense. It will never get worse but it's not getting worse"},{"metadata":{},"cell_type":"markdown","source":"# Out-of-bag (OOB) score"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we pass oob_score as True, the model takes all the rows (which were left out randomly) to create a validation dataset for each tree and then averages them to get the accuracy!"},{"metadata":{},"cell_type":"markdown","source":"# Reducing over fitting\nInstead of creating bags from a subset, why not give each tree access to the complete dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')\nX_train, X_valid = split_vals(df_trn, n_trn)\ny_train, y_valid = split_vals(y_trn, n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_rf_samples(20000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tree building parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"reset_rf_samples()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another way to reduce overfitting is restrict min_samples_leaf.\nWe tell the tree to stop spliting when it has 3 leafs.\n3, 5, 7 are good values to try with but for large datasets it can be hunderds or thousands"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another idea is your trees should not be correlated with each other\nTherefore in addition to taking a samples of rows we also take samples of columns.\nThis way we can reduce correaltion between columns and get a better result\n\nHere we use max_features=0.5 \nWhich mean randomly pick half of the columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confidence based on tree variance"},{"metadata":{},"cell_type":"markdown","source":"Now, we know that we take mean of predictions from different trees and that minimizes our error, but what if we come across a row which is new to the trees and most of the trees give wrong insights.\n\nTo tackle this problem, we take standard deviation of our predictions and see if our std. dev. is high, we know that this is a row that our forest has not seen before!"},{"metadata":{"trusted":true},"cell_type":"code","source":"set_rf_samples(50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time preds = np.stack([t.predict(X_valid) for t in m.estimators_])\n# mean, std. deviation\nnp.mean(preds[:, 0]), np.std(preds[:, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preds(t): return t.predict(X_valid)\n# parallel_trees is a fast ai function that get help you run trees in parallel\n%time preds = np.stack(parallel_trees(m, get_preds))\nnp.mean(preds[:, 0]), np.std(preds[:, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = raw_valid.copy()\nx['pred_std'] = np.std(preds, axis=0)\nx['pred'] = np.mean(preds, axis=0)\nx.Enclosure.value_counts().plot.barh();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flds = ['Enclosure', 'SalePrice', 'pred', 'pred_std']\nenc_summ = x[flds].groupby('Enclosure', as_index=False).mean()\nenc_summ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc_summ = enc_summ[~pd.isnull(enc_summ.SalePrice)]\nenc_summ.plot('Enclosure', 'SalePrice', 'barh', xlim=(0, 11))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc_summ.plot('Enclosure', 'pred', 'barh', xerr='pred_std', alpha=0.6, xlim=(0, 11))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_valid.ProductSize.value_counts().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flds = ['ProductSize', 'SalePrice', 'pred', 'pred_std']\nsumm = x[flds].groupby('ProductSize').mean()\nsumm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(summ.pred/summ.pred_std).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance (Most important)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = rf_feat_importance(m, df_trn)\nfi[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi.plot('cols', 'imp', figsize=(10, 6), legend=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fi(fi[:30])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_keep = fi[fi.imp > 0.005].cols; len(to_keep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_keep = df_trn[to_keep].copy()\nX_train, X_valid = split_vals(df_keep, n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = rf_feat_importance(m, df_keep)\nplot_fi(fi)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice here that importance for Coupler System reduced drastically after removing less important feature. That is because there must be some co relation with some features that spiked by the importance of Coupler System. \nThat is why I trust the new dataset better than the previous dataset."},{"metadata":{},"cell_type":"markdown","source":"# One-hot encoding\n\nmax_n_cats=7 means one-hot enocde every category with number of cats less than 7."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(df_trn2, n_trn)\n\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = rf_feat_importance(m, df_trn2)\nplot_fi(fi[:25])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case the model got worse, but gave us another insight that Enclosure_EROPS is the most importance thing, even more than YearMade!\n\nNow you can know what EROPS mean and why is it so important."},{"metadata":{},"cell_type":"markdown","source":"# Removing redundant variables\nOne thing that makes it harder to interpret the data is if variables with very similar meanings exist in the dataset. So we try to remove redundant variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster import hierarchy as hc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16, 12))\ndendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_oob(df):\n    m = RandomForestRegressor(n_estimators=30, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)\n    x, _ = split_vals(df, n_trn)\n    m.fit(x, y_train)\n    return m.oob_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_oob(df_keep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in ('saleYear', 'saleElapsed', 'fiModelDesc', 'fiBaseModel', 'Grouser_Tracks', 'Coupler_System'):\n    print(c, get_oob(df_keep.drop(c, axis=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop = ['saleYear', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(df_keep.drop(to_drop, axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So this looks good, let's use this dataframe from now on!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_keep.drop(to_drop, axis=1, inplace=True)\nX_train, X_valid = split_vals(df_keep, n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('tmp/keep_cols.npy', np.array(df_keep.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keep_cols = np.load('tmp/keep_cols.npy', allow_pickle=True)\ndf_keep = df_trn[keep_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And let's see how the model performs on full dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"reset_rf_samples()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=40, max_features=0.5, min_samples_leaf=3, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Partial Dependance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pdpbox import pdp\nfrom plotnine import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_rf_samples(50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(df_trn2, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"??plot_fi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fi(rf_feat_importance(m, df_trn2)[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One important piece of information is how old was the Bulldozer which was sold.\nSo we plot YearMade and saleElapsed on a scatter plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.plot('YearMade', 'saleElapsed', 'scatter', alpha=0.1, figsize=(10,8));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we understand, there are data points that have year made in 1000s. We can assume that no Bulldozers were made in 1000\nSo this may be a way to handle empty or unknown values"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_all = get_sample(df_raw[df_raw.YearMade>1960], 300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ggplot(x_all, aes('YearMade', 'SalePrice')) + stat_smooth(se=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = get_sample(X_train[X_train.YearMade>1930], 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"??pdp.pdp_isolate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_pdp(feat, clusters=None, feat_name=None):\n    feat_name = feat_name or feat\n    p = pdp.pdp_isolate(m, x, x.columns, feat)\n    return pdp.pdp_plot(p, feat_name, plot_lines=True,\n                       cluster=clusters is not None, n_cluster_centers=clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pdp('YearMade')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pdp('YearMade', clusters=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = ['saleElapsed', 'YearMade']\np = pdp.pdp_interact(m, x, x.columns, feats)\npdp.pdp_interact_plot(p, feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pdp(['Enclosure_EROPS w AC', 'Enclosure_EROPS', 'Enclosure_OROPS'], 5, 'Enclosure')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.YearMade[df_raw.YearMade<1950] = 1950\ndf_keep['age'] = df_raw['age'] = df_raw.saleYear - df_raw.YearMade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\nm.fit(X_train, y_train)\nplot_fi(rf_feat_importance(m, df_keep))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tree interpreter"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install treeinterpreter\nfrom treeinterpreter import treeinterpreter as ti","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_valid = split_vals(df_raw[df_keep.columns], n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row = X_valid.values[None, 0]; row","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction, bias, contributions = ti.predict(m, row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction[0], bias[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[o for o in zip(df_keep.columns, df_valid.iloc[0], contributions[0])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contributions[0].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}