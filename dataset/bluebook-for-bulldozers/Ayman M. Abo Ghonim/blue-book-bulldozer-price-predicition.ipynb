{"cells":[{"metadata":{"id":"IC-JCsEofVO0"},"cell_type":"markdown","source":"# <font color='red'>Problem Definition & Formulation</font>\n"},{"metadata":{"id":"0LuGZchCfdDA"},"cell_type":"markdown","source":"The problem formulation phase of the ML Pipeline is critical, and it’s where everything begins.It starts by seeing a problem and thinking “what question, if I could answer it, would provide the most value to my business?” \n\nPart of the problem formulation phase includes seeing where there are opportunities to use machine learning and consider the following questions:\n1.\tIs machine learning appropriate for this problem, and why or why not?\n2.\tWhat is the ML problem if there is one, and what would a success metric look like?\n3.\tWhat kind of ML problem is this?\n4.\tIs the data appropriate?\n"},{"metadata":{"id":"y8-ys83PfiLM"},"cell_type":"markdown","source":"# Answers for the four mentioned questions!\nTo answer those question, I must understand the problem well and then explore the data to build and inution about it, then come up with rigoures arguments which lead to a conclusion and prespictives. \n\n1)\tML is appropriate because of the scale and Variety of the Data . There are potentially High diemensional Features and about of half million of the Training examples which  makes the problem to be very difficult for Human to be solved without the use of ML. In addition, ML solution will offer a scalable and reusable solution for the problem.\n\n2)\tThe problem is :How well can we predict the future sale price of a bulldozer, given its characteristics and previous examples of how much similar bulldozers have been sold for?\ni.\tSuccess would be the minimum root mean squared log error (RMSLE) because it is a Kaggle competition and  Kaggle has set this evaluation metric to being used.\n\n3)\tThis is a supervised Regression ML problem because we have a labeled data point and the output is a Numerical value.\n\n4)\tThis data is appropriate because it has variety of historical data of similar products and The characteristics of the Bulldozer,  and there are a lot of examples with  labeled target to train, tune and test the model .\n"},{"metadata":{"id":"xJPFPZg6jjwX"},"cell_type":"markdown","source":"# <font color='blue'>Importing Libraries and Modules </font>"},{"metadata":{"id":"GnwzEF0mfb5p","trusted":true},"cell_type":"code","source":"# Import data analysis tools \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas_profiling import ProfileReport\n\n%matplotlib inline\n\n\n# import model modules\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBRFRegressor\n# import model evaluation modules\nfrom sklearn.metrics import mean_squared_log_error #because kaggle want this metric\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"g1Lsz1VXrWY6"},"cell_type":"markdown","source":"# <font color='blue'>Loading Data </font>\n\nThere are 3 datasets:\n\n* Train.csv - Historical bulldozer sales examples up to 2011 (close to 400,000 examples with 50+ different attributes, including SalePrice which is the target variable).\n* Valid.csv - Historical bulldozer sales examples from January 1 2012 to April 30 2012 (close to 12,000 examples with the same attributes as Train.csv).\n* Test.csv - Historical bulldozer sales examples from May 1 2012 to November 2012 (close to 12,000 examples but missing the SalePrice attribute, as this is what we'll be trying to predict).\n\n## Features\n\n\nFor this dataset, Kaggle provide a data dictionary which contains information about what each attribute of the dataset means. You can download this file directly from the Kaggle competition page  [here](https://www.kaggle.com/c/bluebook-for-bulldozers/download/Bnl6RAHA0enbg0UfAvGA%2Fversions%2FwBG4f35Q8mAbfkzwCeZn%2Ffiles%2FData%20Dictionary.xlsx) (account required) or view it on Google Sheets.\n\nWith all of this being known, let's get started!\n\n"},{"metadata":{"id":"VrXZt8NHrCJi","outputId":"d5c6a728-3985-4330-af9b-248da51f65ec","trusted":true},"cell_type":"code","source":"# Import the training and validation set\ndf = pd.read_csv(\"../input/bluebook-for-bulldozers/TrainAndValid.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"P-j0R0ctvnKb"},"cell_type":"markdown","source":"# <font color='red'>  EDA & Data Preparation </font>\n\n# EDA is crucial step that help us explore and understand our data to build an intution  about it, and outline the required preprocessing steps before the modeling stage. "},{"metadata":{"id":"O4bv-b17fWEd"},"cell_type":"markdown","source":"## * In this new version, I will use Pandas profiling as an automated EDA technique."},{"metadata":{"id":"quLpCvhNqMpj","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"gLkgfVRGf5TA","trusted":false},"cell_type":"code","source":"# Automated_EDA_Report = ProfileReport (df)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"dGz8WZF1qnbz","trusted":false},"cell_type":"code","source":"# Automated_EDA_Report.to_notebook_iframe()","execution_count":null,"outputs":[]},{"metadata":{"id":"WaLVdxY9tPkG","trusted":false},"cell_type":"code","source":"# Lets start with an overview of our data\n# df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"vTC1qMP1wEA1","trusted":false},"cell_type":"code","source":"#Lets identify our features(Columns) names, counts and datatypes\n#df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"g2Da7W5BAQYb"},"cell_type":"markdown","source":"### It is clear that we have combination of numerical and categorical data, and we have missed data because the count of features is not the same for each features."},{"metadata":{"id":"dbDKFK1o-1fn","trusted":false},"cell_type":"code","source":"#Lets check the missing values  \n#df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"Eqq-BDSkA1Jd"},"cell_type":"markdown","source":"### There are some features without missing data, some with accepted small amount of missing data that can be imputed, and others with very large missing data that we can not deal with and we will drop them later."},{"metadata":{"id":"iR9y1Iic_SQR","trusted":false},"cell_type":"code","source":"#Check the distribution of the SalePrice using histogram\n#df.SalePrice.plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"id":"UYpjy-PyDPbh","trusted":false},"cell_type":"code","source":"# Check some statistics about SalePrice\n#df[\"SalePrice\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"k5FlhLyeHPwt"},"cell_type":"markdown","source":"## Parsing dates\nWhen working with time series data, it's a good idea to make sure any date data is the format of a [datetime object](https://docs.python.org/3/library/datetime.html) (a Python data type which encodes specific information about dates).\n"},{"metadata":{"id":"SZqiJAmaERJD","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/bluebook-for-bulldozers/TrainAndValid.csv\",\n                 low_memory=False,\n                 parse_dates=[\"saledate\"])","execution_count":null,"outputs":[]},{"metadata":{"id":"Qz_Pfs7_E2Oe","outputId":"6d79d4bd-cbc5-4e8c-c9ed-1c32085b0500","trusted":true},"cell_type":"code","source":"# check the datatype of saledate after parsing\n#df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"zx6OC_6BJcAN"},"cell_type":"markdown","source":"### As saledate is datetime object type, We can make some feature engineering on it to extract the different attributes of It."},{"metadata":{"id":"oXuPqPIOt_K-","trusted":true},"cell_type":"code","source":"# lets sort the date first\ndf.sort_values(by=['saledate'],\n               inplace=True,\n               ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"dVz7CU0tGN5H","trusted":true},"cell_type":"code","source":"df[\"saleYear\"] = df.saledate.dt.year\ndf[\"saleMonth\"] = df.saledate.dt.month\ndf[\"saleDay\"] = df.saledate.dt.day\ndf[\"saleDayofweek\"] = df.saledate.dt.dayofweek\ndf[\"saleDayofyear\"] = df.saledate.dt.dayofyear\n\n# Drop original saledate as we do not need it\ndf.drop(\"saledate\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"0LVPg9dUKs_0"},"cell_type":"markdown","source":"### Handling missing data\nwe will start by recheck the miising data to drop columns with significant amount of missied data"},{"metadata":{"id":"mYScxnTcsvwZ"},"cell_type":"markdown","source":"## After some experimentation, I found that imputing the missing values with the median has good impact in the performance of model, so I will not drop them in this version."},{"metadata":{"id":"AiJr7CVRGtBE","outputId":"9eae6755-b297-416c-dd28-278b2bce2eeb","trusted":true},"cell_type":"code","source":"#df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"nViBniqPOb27","trusted":true},"cell_type":"code","source":"# lets drop columns with more than 50 % missed data because this columns(Features) do not add any value\n# as well they may mislead the algorithm and to save the memory and processing time \n#limitPer = len(df) * .50\n#df = df.dropna(thresh=limitPer,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"zgxfRGnPOjqf","trusted":true},"cell_type":"code","source":"#df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"luwRcREIpdfR"},"cell_type":"markdown","source":"# Filling the missed value by Median for numerical features and for Categorical non numerical Features after converting them into numerical types"},{"metadata":{"id":"bzi1Jk6mpt4z","trusted":true},"cell_type":"code","source":"# Fill numeric rows with the median\nfor label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            # Fill missing numeric values with median since it's more robust than the mean\n            df[label] = content.fillna(content.median())","execution_count":null,"outputs":[]},{"metadata":{"id":"AHXoBFNaqW-1","trusted":true},"cell_type":"code","source":"# Turn categorical variables into numbers\nfor label, content in df.items():\n    # Check columns which *aren't* numeric\n    if not pd.api.types.is_numeric_dtype(content):\n        # We add the +1 because pandas encodes missing categories as -1\n        df[label] = pd.Categorical(content).codes+1        ","execution_count":null,"outputs":[]},{"metadata":{"id":"u5m440JdrCmt","outputId":"de126c9e-1d48-4528-9e03-e3496876f6ce","trusted":true},"cell_type":"code","source":"# Check for Missing Values\n#df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"u2n5ojdZu-AN"},"cell_type":"markdown","source":"### It turns out that There is no missing values now, all data is numerical form,  So lets Go to Modeling stage to build up a predictive model for that regression problem!"},{"metadata":{"id":"wUQtM2MEviim"},"cell_type":"markdown","source":"# <font color='red'>  Modeling </font>"},{"metadata":{"id":"Tp3ZQPCVttEF","outputId":"5600b45d-fcd8-4eec-ea53-dff4bbf087ff","trusted":true},"cell_type":"code","source":"# Splitting data ito x and y and into train and valid datasets\ndf_val = df[df.saleYear == 2012]\ndf_train = df[df.saleYear != 2012]\nX_train, y_train = df_train.drop(\"SalePrice\", axis=1), df_train.SalePrice\nX_valid, y_valid = df_val.drop(\"SalePrice\", axis=1), df_val.SalePrice\n\nnp.random.seed(42) # random seed for reproduciblity of the result.\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape\n","execution_count":null,"outputs":[]},{"metadata":{"id":"YlVck3OmxH1l"},"cell_type":"markdown","source":"### According to the Kaggle data page, the validation set and test set are split according to dates.\n* Training = all samples up until 2011\n* Valid = all samples form January 1, 2012 - April 30, 2012\n* Test = all samples from May 1, 2012 - November 2012"},{"metadata":{"id":"MTgoWZz-wUFH","trusted":true},"cell_type":"code","source":"#After experimentation, I will use the RandomForesst Regressor which gives me a high performance in this dataset\nRF = RandomForestRegressor(n_estimators=50,  n_jobs=-1, random_state=42,)\nXGB= XGBRFRegressor(n_estimators=50, n_jobs=-1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"jwMuEvb5z7H-","outputId":"e88274bb-cbe9-4b1a-a234-fcdc97fd3547","trusted":true},"cell_type":"code","source":" RF.fit(X_train, y_train)\n XGB.fit(X_train, y_train)   ","execution_count":null,"outputs":[]},{"metadata":{"id":"lLwBcKTjwBTh"},"cell_type":"markdown","source":"## Model Evaluation "},{"metadata":{"id":"5E4oaX-hwARw","outputId":"56988c68-30e3-4ab9-fd4b-f0cd6401d468","trusted":true},"cell_type":"code","source":"#lets score on the validation set to see our performance regarding to the leaderbooard on kaggle\nRF.score(X_valid, y_valid)\nXGB.score(X_valid, y_valid)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"akJ94AZNwmlr"},"cell_type":"markdown","source":"### Score by default use the R2 metric, but the competition choosed RMSLE\n* Root Mean Square Log Error"},{"metadata":{"id":"jsKoo1mxwcge","outputId":"0ddf8e5a-3835-474e-ce2f-12889aa11e4f","trusted":true},"cell_type":"code","source":"# there is no direct method to get RMSLE, so we need to find MSLE first\ny_preds = RF.predict(X_valid)\ny_preds2 = XGB.predict(X_valid)\n\nRMSLE = np.sqrt(mean_squared_log_error(y_valid, y_preds))\nprint (f\"RMSLE of RandomeForest on Valid Dataset is:{RMSLE}\")\nRMSLE2 = np.sqrt(mean_squared_log_error(y_valid, y_preds2))\nprint (f\"RMSLE of XGB Regressor on Valid Dataset is:{RMSLE2}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"c1L1bL6xxkwC"},"cell_type":"markdown","source":"## Now, lets score on the test set to see our final performance\n* we will use the same preprocessing steps that used on the training and validation sets, otherwise, we can not used our trained model!!"},{"metadata":{"id":"L7Ho7vvlxgQO","trusted":true},"cell_type":"code","source":"X_test = pd.read_csv(\"../input/bluebook-for-bulldozers/Test.csv\",\n                 low_memory=False,\n                 parse_dates=[\"saledate\"])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"uHoLNExJzNb9","trusted":true},"cell_type":"code","source":"#sort by saledate\nx_test.sort_values(by=['saledate'],\n               inplace=True,\n               ascending=True)\n\n#feature engineering on Date\nX_test[\"saleYear\"] = X_test.saledate.dt.year\nX_test[\"saleMonth\"] = X_test.saledate.dt.month\nX_test[\"saleDay\"] = X_test.saledate.dt.day\nX_test[\"saleDayofweek\"] = X_test.saledate.dt.dayofweek\nX_test[\"saleDayofyear\"] = X_test.saledate.dt.dayofyear\n\n# Drop original saledate as we do not need it\nX_test.drop(\"saledate\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"8YhopXXFztDK","trusted":true},"cell_type":"code","source":"# Fill numeric rows with the median\nfor label, content in X_test.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            # Fill missing numeric values with median since it's more robust than the mean\n            X_test[label] = content.fillna(content.median())","execution_count":null,"outputs":[]},{"metadata":{"id":"PTaOi63W6D1g","trusted":true},"cell_type":"code","source":"# Turn categorical variables into numbers\nfor label, content in X_test.items():\n    # Check columns which *aren't* numeric\n    if not pd.api.types.is_numeric_dtype(content):\n        # We add the +1 because pandas encodes missing categories as -1\n        X_test[label] = pd.Categorical(content).codes+1        ","execution_count":null,"outputs":[]},{"metadata":{"id":"2Ge8rBAi6Guz","trusted":true},"cell_type":"code","source":"#predicting the SalePrice\ny_preds = RF.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"KFNm9tql6iTj"},"cell_type":"markdown","source":"### Since we do not have the True Price and we can not submit  our solution on Kaggle, we can not calculate the final performance of the model (RMSLE )"},{"metadata":{"id":"c-CcsK0Y6tz4","trusted":true},"cell_type":"code","source":" #rmsle = np.sqrt(mean_squared_log_error(y_test, y_preds))\n #rmsle","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}