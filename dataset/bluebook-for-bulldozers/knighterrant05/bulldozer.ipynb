{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # Predicting the Sale Price of Bulldozers using Machine Learning\n\n### 1. Problem definition\n\n> How can we predict the future sale price of a bulldozer, given its characteristics and previous examples of how much similar bulldozers have been sold for?\n\n### 2. Data\n\n* The data is downloaded from kaggle bluebook for Bulldozers competition:\n\n\n### 3. Evaluation\n\n* The evaluation metric for this competition is the RMSLE (root mean squared log error) betwee the actual and predicted auction prices.\n\n* For more on the evaluation of this project check:\nhttps://www.kaggle.com/c/bluebook-for-bulldozers/overview/evaluation\n\n* **Note:** The goal for most regression evaluation metrics is to minimize the error. For example, our goal for this project will be to build a machine learning model which minimizes RMSLE.\n\n### 4. Features\n\nKaggle provides a data dictionary detailing all of the features of the dataset. You can view this data dictionary on Google Sheets:\nhttps://docs.google.com/spreadshets/d/181y-bLR8sbJLITkWG7ozKm8l33RyieQ2Fpgix-beSYT/edit?usp=sharing","metadata":{}},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nPATH = \"../input/bluebook-for-bulldozers/TrainAndValid.csv\"\ndf = pd.read_csv(f\"{PATH}\", low_memory= False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.saledate[:1000]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.saledate.dtype","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots()\nax.scatter(df['saledate'][:1000],df[\"SalePrice\"][:1000])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.SalePrice.plot.hist()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Parsing dates\n\nWhen we work with time series data, we want to enrich the time & date compoent as much as possible.\n\nWe can do that by telling pandas which of our columns has dates in it using the parse_dates parameter. ","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"# Import data again but this time parse dates\nPATH = \"../input/bluebook-for-bulldozers/TrainAndValid.csv\"\ndf = pd.read_csv(f\"{PATH}\", low_memory= False, parse_dates=[\"saledate\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.saledate.dtype","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.saledate[:1000]","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots()\nax.scatter(df[\"saledate\"][:1000],df[\"SalePrice\"][:1000])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head().T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.saledate.head(20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sort DataFrame by saledate\n\nWhen working with time series data, its a good idea to sort it by date","metadata":{}},{"cell_type":"code","source":"# Sort DataFrame in date order\ndf.sort_values(by=[\"saledate\"],inplace=True,ascending=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.saledate.head(20)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make a copy of the original DataFrame\n\nWe make a copy of the original dataframe so when we manipulate the copy, we've still got our original data.","metadata":{}},{"cell_type":"code","source":"# Make a copy\ndf_tmp = df.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.saledate.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add datetime parameters for saledate column","metadata":{}},{"cell_type":"code","source":"df_tmp[:1].saledate.dt.year","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp[:1].saledate.dt.day","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp[:1].saledate","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp[\"saleYear\"] = df_tmp.saledate.dt.year\ndf_tmp[\"saleMonth\"] = df_tmp.saledate.dt.month\ndf_tmp[\"saleDay\"] = df_tmp.saledate.dt.day\ndf_tmp[\"saleDayOfWeek\"] = df_tmp.saledate.dt.dayofweek\ndf_tmp[\"saleDayOfYear\"] = df_tmp.saledate.dt.dayofyear","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we can remove the saledate column \ndf_tmp.drop(\"saledate\",axis=1,inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the values of different columns\ndf_tmp.state.value_counts()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_tmp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Modelling\n\nWe've done enough EDA (we could always do more) but let's start to do some model-driven EDA","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp[\"UsageBand\"].dtype","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert string to categories\n\nOne way we can turn our data into numbers is by converting them into pandas categories.\n\nWe can check the different datatypes compatible with pandas here:\nhttps://pandas.pydata.org/pandas-docs/stable/reference/general_utility_functions.html#data-types-related-functionality","metadata":{}},{"cell_type":"code","source":"df_tmp.head()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.api.types.is_string_dtype(df_tmp[\"UsageBand\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the columns which contain strings\n# label = columns\n# content = values\nfor label,content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If you are wondering what df.items() does, here's an example\nrandom_dict = {\"key1\":\"hello\",\n               \"key2\":\"world\"}\nfor key,value in random_dict.items():\n    print(f\"this is a key:{key}\",\n          f\"this is a value:{value}\")","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This will turn all of the string value into category values\nfor label,content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        df_tmp[label] = content.astype(\"category\").cat.as_ordered()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.info()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.state.cat.categories","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.state.value_counts()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.state.cat.codes","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks to pandas Categories we now have a way to access all of our data in the form of numbers.\n\nBut we still have a bunch of missing data...","metadata":{}},{"cell_type":"code","source":"# Check misisng data\ndf_tmp.isnull().sum()/len(df_tmp)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save preprocessed data","metadata":{}},{"cell_type":"code","source":"df_tmp.head().T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.isna().sum()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fill missing values\n\n### Fill numerical missing values first","metadata":{}},{"cell_type":"code","source":"for label,content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        print(label)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.ModelID","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for which numeric columns have null values\nfor label,content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill numeric rows with the median\nfor label,content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            # Add a binary column which tells us if the data was missing or not\n            df_tmp[label+\"_is_missing\"] = pd.isnull(content)\n            # Fill missing numeric values with median\n            df_tmp[label] = content.fillna(content.median())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Demonstrate how median is more robust than mean\nhundreds = np.full((1000,),100)\nhundreds_billion = np.append(hundreds, 1000000000)\nnp.mean(hundreds), np.mean(hundreds_billion), np.median(hundreds), np.median(hundreds_billion)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hundreds_billion","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if there's any null numeric values\nfor label,content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check to see how many examples were missing\ndf_tmp.auctioneerID_is_missing.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.isna().sum()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filling and turning categorical variables into numbers","metadata":{}},{"cell_type":"code","source":"# Check for columns which aren't numeric\nfor label,content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        print(label)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Categorical(df_tmp[\"state\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Categorical(df_tmp[\"state\"]).codes + 1","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# You can see that result can also be -1 \n# Thus we added 1 to remove the negative sign\npd.Categorical(df_tmp[\"UsageBand\"]).codes ","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Turn categorical variables into numbers and fill missing","metadata":{}},{"cell_type":"code","source":"for label,content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        # Add binary column to indicate whether sample had missing value\n        df_tmp[label+\"_is_missing\"] = pd.isnull(content)\n        # Turn categories into numbers and add +1\n        df_tmp[label] = pd.Categorical(content).codes + 1 ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Categorical(df_tmp[\"state\"]).codes+1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.head().T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now that all of data is numeric as well as our dataframe has no missing values, we should be able to build a machine learning model.  ","metadata":{}},{"cell_type":"code","source":"df_tmp.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_tmp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n# Instantiate model\nmodel = RandomForestRegressor(n_jobs=-1,\n                              random_state=42,\n                              n_estimators=100)\n\n# Fit the model\nmodel.fit(df_tmp.drop(\"SalePrice\", axis=1), df_tmp[\"SalePrice\"])","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.score(df_tmp.drop(\"SalePrice\",axis=1), df_tmp[\"SalePrice\"])","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Questions:** Why doesn't the above metric hold water? (wh isn't the metric reliable)","metadata":{}},{"cell_type":"markdown","source":"### Splitting data into tain/validation sets ","metadata":{}},{"cell_type":"code","source":"df_tmp.saleYear","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.saleYear.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into training and validation\ndf_val = df_tmp[df_tmp.saleYear == 2012]\ndf_train = df_tmp[df_tmp.saleYear!= 2012]\n\nlen(df_val), len(df_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into x & y\nx_train,y_train = df_train.drop(\"SalePrice\",axis=1),df_train.SalePrice\nx_valid,y_valid = df_val.drop(\"SalePrice\",axis=1),df_val.SalePrice\n\nx_train.shape,y_train.shape,x_valid.shape,y_valid.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building an Evaluation function","metadata":{}},{"cell_type":"code","source":"# Create evaluation function (the competition uses RMSLE)\nfrom sklearn.metrics import mean_squared_log_error, mean_absolute_error,r2_score\n\ndef rmsle(y_test,y_preds):\n    '''\n    Calculate root mean squared log error between predictions and\n    true labels.\n    '''\n    return np.sqrt(mean_squared_log_error(y_test,y_preds))\n# Create function to evaluate model on a few different levels\ndef show_scores(model):\n    train_preds = model.predict(x_train)\n    val_preds   = model.predict(x_valid)\n    scores = {\"Training MAE\": mean_absolute_error(y_train, train_preds),\n              \"Valid MAE\": mean_absolute_error(y_valid, val_preds),\n              \"Training RMSLE\": rmsle(y_train, train_preds),\n              \"Valid RMSLE\": rmsle(y_valid, val_preds),\n              \"Training R^2\": r2_score(y_train, train_preds),\n              \"Valid R^2\": r2_score(y_valid, val_preds)}\n    return scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing our model on a subset (to tune the hyperparameters)","metadata":{}},{"cell_type":"code","source":"# # This takes far too long for experimenting\n# %%time\n# model = RandomForestRegressor(n_jobs=-1,\n#                               random_state=42,\n#                               n_estimators=100)\n\n#model.fit(x_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(x_train),len(y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change max_samples value \n# Version 0.22 has argument max_samples which saves time \nmodel = RandomForestRegressor(n_jobs=-1,\n                              random_state=42,\n                              n_estimators=100,\n                              max_samples=10000)\nmodel.fit(x_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since n_estimators = 100\nx_train.shape[0]*100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_scores(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter tuning with RandomizedSearchCV","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\n# Different RandomForestRegressor hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10,100,10),\n           \"max_depth\": [None,3,5,10],\n           \"min_samples_split\": np.arange(2,20,2),\n           \"min_samples_leaf\": np.arange(1,20,2),\n           \"max_features\": [0.5,1,\"sqrt\",\"auto\"],\n           \"max_samples\": [10000]}\n\n#Instantiate RandomizedSearchCV model\nrs_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1,\n                                                    random_state=42),\n                             param_distributions =  rf_grid,\n                             n_iter = 5,\n                             cv = 5,\n                             verbose = True)\nrs_model.fit(x_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs_model.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_scores(rs_model)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train a model with the bes hyperpaprameters\n\n**Note:** These were found after 100 iterations of RandomizedSearchCV, i.e. n_iter = 100","metadata":{}},{"cell_type":"code","source":"%%time\n\n#Most Ideal Hyperparameters\nideal_model = RandomForestRegressor(n_estimators = 40,\n                                    min_samples_leaf = 1,\n                                    min_samples_split = 14,\n                                    max_features = 0.5,\n                                    n_jobs = -1,\n                                    max_samples = None,\n                                    random_state = 42)\n\n# Fit the ideal model\nideal_model.fit(x_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scores for ideal_model (trained on all the data)\n# Valid RMSLE is better\nshow_scores(ideal_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scores for rs_model (trained on 10000 examples)\nshow_scores(rs_model)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make Predictions on Test data","metadata":{}},{"cell_type":"code","source":"# Import the test data\nPATH = \"../input/bluebook-for-bulldozers/Test.csv\"\ndf_test = pd.read_csv(f\"{PATH}\", low_memory= False, parse_dates=[\"saledate\"])\ndf_test.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preproessing the test data (getting the dataset i teh same format as our training dataset)","metadata":{}},{"cell_type":"code","source":"def preprocess_data(df):\n    '''\n    Performs transformations on df and returns transformed df.\n    '''\n    df[\"saleYear\"] = df.saledate.dt.year\n    df[\"saleMonth\"] = df.saledate.dt.month\n    df[\"saleDay\"] = df.saledate.dt.day\n    df[\"saleDayOfWeek\"] = df.saledate.dt.dayofweek\n    df[\"saleDayofYear\"] = df.saledate.dt.dayofyear\n    \n    df.drop(\"saledate\",axis =1,inplace=True)\n    \n    # Fill the numeric rows with median\n    for label,content in df.items():\n        if pd.api.types.is_numeric_dtype(content):\n            if pd.isnull(content).sum():\n                df[label+\"is_missing\"] = pd.isnull(content)\n                df[label] = content.fillna(content.median())\n                \n    # Fill categorical missing data turn categories into numbers\n        if not pd.api.types.is_numeric_dtype(content):\n            df[label+\"is missing\"] = pd.isnull(content)\n            # We add +1 to the category code\n            df[label] = pd.Categorical(content).codes+1\n            \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = preprocess_data(df_test)\ndf_test.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can find how the columns differ using sets\n# Somethig is wrong with the output here\n# It should have only displayed 'auctioneerID'\nset(x_train.columns) - set(df_test.columns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Manually adjust df_test to have auctioneerID_is_missing column\ndf_test[\"auctioneerID_is_missing\"] = False\ndf_test.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Finally our test dataframe has the same features as our training dataframe, we can make predictions!","metadata":{}},{"cell_type":"code","source":"# Make predictions on the test data\ntest_preds = ideal_model.predict(df_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We've made some preditions but they're not in the same format Kaggle is asking for:\nhttps://www.kaggle.com/c/bluebo-for-bulldozers/overview/evaluation","metadata":{}},{"cell_type":"code","source":"# Format predictions into the same format Kaggle is after\ndf_preds = pd.DataFrame()\ndf_preds[\"SalesID\"] = df_test[\"SalesID\"]\ndf_preds[\"SalesPrice\"] = test_preds\ndf_preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Export prediction data\ndf_preds.to_csv('test_predictions.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance\n\nFeature importane seeks to figure out which different attributes of the data were most important when it comes to predicting the \n**target variable** (SalePrice).","metadata":{}},{"cell_type":"code","source":"# Find feature importance of our best model\nlen(ideal_model.feature_importances_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_features(columns,importances,n=20):\n    df = (pd.DataFrame({\"features\":columns,\n                        \"feature_importances\": importances})\n         .sort_values(\"feature_importances\", ascending = False)\n         .reset_index(drop = True))\n    # Plot the dataframes\n    fig,ax = plt.subplots()\n    ax.barh(df[\"features\"][:n], df[\"feature_importances\"][:20])\n    ax.set_ylabel(\"Features\")\n    ax.set_xlabel(\"Feature Importance\")\n    ax.invert_yaxis()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_features(x_train.columns,ideal_model.feature_importances_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}