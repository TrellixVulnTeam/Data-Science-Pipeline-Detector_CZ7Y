{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Escola Piloto Virtual - PEQ/COPPE/UFRJ\n\n## Data Science e Machine Learning na Prática - Introdução e Aplicações na Indústria de Processos\n\nEste notebook é referente à Aula 1 do curso, que trata de técnicas de regressão utilizando modelos de florestas aleatórias."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# comandos mágicos que não se comunicam com a linguagem Python e sim diretamente com o kernel do Jupyter\n# começam com %\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importando os principais módulos que usaremos ao longo da aula\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble\n\n# você também pode importar apenas uma parte de cada módulo, por exemplo:\n# from sklearn.ensemble import RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Um exemplinho introdutório...\n\nO objeto de estudo deste notebook é o conjunto de dados da competição [Bluebook for Bulldozers](https://www.kaggle.com/c/bluebook-for-bulldozers), que pode ser traduzida livremente como \"*Tabela FIPE para Tratores de Esteira*\". Antes de começarmos a analisar esses dados, no entanto, vamos usar um exemplo mais simples, o conjunto [Boston Housing](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html), para nos familiarizarmos com a funcionalidade básica da regressão com o [scikit-learn](https://scikit-learn.org/stable/)."},{"metadata":{},"cell_type":"markdown","source":"### Importando dados\n\n\nO conjunto de dados em questão é disponibilizado no próprio [scikit-learn](https://scikit-learn.org/stable/) em seu módulo [sklearn.datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes) e pode ser importado com o seguinte comando:"},{"metadata":{"trusted":true},"cell_type":"code","source":"boston = sklearn.datasets.load_boston()\nboston","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O objeto **boston** possui vários elementos: \n\n* data;\n* target;\n* feature_names;\n* DESCR;\n* filename.\n\nImprimindo a descrição (DESCR):"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(boston.DESCR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As variáveis preditoras e predita estão armazenadas no objeto **boston** em seus elementos **data** e **target**, respectivamente. Vamos dar a elas nomes mais simpáticos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = boston.data, boston.target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separando conjuntos de treino e de teste\n\nNo Aprendizado de Máquina, não é boa prática utilizar na etapa de treino todos os dados disponíveis. Sempre deve-se reservar uma parcela dos dados para efetuar um teste, de modo a verificar-se a capacidade preditiva do modelo.\n\nPara efetuar a separação dos dados Boston em *dados de treino* e *dados de teste*, usaremos a função [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html):"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.1, \n                                                                            random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A função [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) separa os dados em *treino* e *teste* de maneira aleatória. No caso acima, uma fração de 10% dos dados é reservada para teste.\n\nA estratégia de separação aleatória é adequada quando os dados são independentes e identicamente distribuídos; em situações em que isso não vale (por exemplo, quando há dependências temporais), devemos usar outras estratégias.\n\nObs: o argumento **random_state** especifica a semente de geração de pseudo-aleatoriedade do algoritmo; isso faz com que em todas as execuções os resultados sejam sempre os mesmos."},{"metadata":{},"cell_type":"markdown","source":"### Treinando o modelo\n\nNosso modelo de regressão será o [modelo de florestas aleatórias](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). Vamos importá-lo do [scikit-learn](https://scikit-learn.org/), armazenando-o em um objeto chamado **m** (poderia ser qualquer outro nome):"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Não se preocupe, você já vai entender como esse modelo funciona! Antes disso, vamos treiná-lo. \n\nDe modo a treinar um modelo no [scikit-learn](https://scikit-learn.org/), devemos fornecer os valores de X e y ao método **fit**, contido no modelo:"},{"metadata":{"trusted":true},"cell_type":"code","source":"m.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pronto! Nosso modelo está treinado! Fácil, né?\n\nVamos calcular as predições do modelo para o conjunto de teste. Para isso, usa-se o método **predict**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = m.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para visualizar o resultado, plotaremos um gráfico com os valores verdadeiros no eixo $x$ e as predições no eixo $y$. Como o objetivo é que as predições estejam o mais perto possível dos valores verdadeiros, quanto mais esses pontos se aproximarem da reta $x=y$, melhores as predições!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotando valores verdadeiros contra predições\nplt.plot(y_test, y_test_pred,'.')\n\n# plotando a reta x=y\nplt.plot(plt.gca().get_ylim(), plt.gca().get_ylim())\n\n# legenda dos eixos\nplt.xlabel('y_test')\nplt.ylabel('y_test_pred');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Atenção: a reta da figura acima *não* é o modelo! É apenas a reta $x=y$. Não poderíamos visualizar o modelo no plano cartesiano, já que ele é multidimensional. Além do mais, como veremos a seguir, o modelo de florestas aleatórias é não-linear, portanto não assumiria a forma de uma reta."},{"metadata":{},"cell_type":"markdown","source":"Calculando algumas métricas de desempenho:"},{"metadata":{"trusted":true},"cell_type":"code","source":"mae = sklearn.metrics.mean_absolute_error(y_test, y_test_pred)\nr2 = sklearn.metrics.r2_score(y_test, y_test_pred)\n\nprint(f'MAE: {mae}')\nprint(f'R2: {r2}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A primeira métrica é o [erro absoluto médio](https://en.wikipedia.org/wiki/Mean_absolute_error) e corresponde à média dos módulos das distâncias entre os valores verdadeiros e os valores preditos. Quanto menor, melhor.\n\nA segunda métrica é o [coeficiente de determinação](https://en.wikipedia.org/wiki/Coefficient_of_determination) $R^2$, uma medida da variância dos dados explicada pelo modelo. Pode variar de $-\\infty$ a 1; quanto mais perto de 1, melhor ($R^2 = 0$ implica que o desempenho do modelo é equivalente a usar a média dos dados; valores negativos implicam que o modelo é pior que isso).\n\nEssas são apenas algumas das métricas; existem várias [outras](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation); a escolha de qual usar deve ser ditada pelo problema que se está resolvendo."},{"metadata":{},"cell_type":"markdown","source":"***Mão na massa 1!***\n\n* Na primeira célula:\n    * Procure na [API](https://scikit-learn.org/stable/modules/classes.html) ou no [guia do usuário](https://scikit-learn.org/stable/user_guide.html#) do [scikit-learn](https://scikit-learn.org/) outro modelo e outras métricas para avaliação de resultados de regressão e as utilize para reproduzir os passos acima. Dependendo da sua escolha, talvez seja necessário importar novos módulos do [scikit-learn](https://scikit-learn.org/), como fiz no começo do notebook.\n* Na segunda célula:\n    * Repita todo o procedimento para o conjunto de dados *diabetes*, também disponível no [scikit-learn](https://scikit-learn.org/) (procure-o [aqui](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets))."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble\nimport sklearn.neural_network\n\nimport pandas as pd\n\nboston = sklearn.datasets.load_boston()\nX, y = boston.data, boston.target\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.1, \n                                                                            random_state = 0)\n\n#X_train = X_train[:,(5,12)]\n#X_test = X_test[:,(5,12)]\n\n# Modelo Random Forest\nm = sklearn.ensemble.RandomForestRegressor(random_state=0)\nm.fit(X_train, y_train)\nimportance = m.feature_importances_\n\ny_test_pred = m.predict(X_test)\n\n# Modelo NN MLP\nm2 = sklearn.neural_network.MLPRegressor(random_state=0, max_iter=10000, hidden_layer_sizes=(20,20,20,20,20))\nm2.fit(X_train, y_train)\ny_test_pred2 = m2.predict(X_test)\n\n# plotando valores verdadeiros contra predições\nplt.figure()\nplt.plot(y_test, y_test_pred,'.', label='RF')\nplt.plot(y_test, y_test_pred2,'.', label='NLP')\nplt.plot(plt.gca().get_ylim(), plt.gca().get_ylim(), label='y=x')\nplt.xlabel('y_test')\nplt.ylabel('y_test_pred');\nplt.legend();\n\n# métricas de avaliação\nmae = sklearn.metrics.mean_absolute_error(y_test, y_test_pred)\nmse = sklearn.metrics.mean_squared_error(y_test, y_test_pred)\nmaxerror = sklearn.metrics.max_error(y_test, y_test_pred)\nr2 = sklearn.metrics.r2_score(y_test, y_test_pred)\n\nmae2 = sklearn.metrics.mean_absolute_error(y_test, y_test_pred2)\nmse2 = sklearn.metrics.mean_squared_error(y_test, y_test_pred2)\nmaxerror2 = sklearn.metrics.max_error(y_test, y_test_pred2)\nr22 = sklearn.metrics.r2_score(y_test, y_test_pred2)\n\nnomes =['MAE','MSE','Erro máximo','R^2']\n\ndados = {'Random Forest': [mae,mse,maxerror,r2],\n         'NLP': [mae2,mse2,maxerror2,r22]}\n\ndf1 = pd.DataFrame(dados, index = nomes)\ndisplay(df1)\n\nplt.figure()\nplt.bar(boston.feature_names, importance)\nplt.xticks(rotation=45)\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble\nimport sklearn.neural_network\n\nimport pandas as pd\n\ndiabetes = sklearn.datasets.load_diabetes()\nX, y = diabetes.data, diabetes.target\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.1, \n                                                                            random_state = 0)\n\n# Modelo Random Forest\nm = sklearn.ensemble.RandomForestRegressor(random_state=0)\nm.fit(X_train, y_train)\nimportance = m.feature_importances_\n\ny_test_pred = m.predict(X_test)\n\n# Modelo NN MLP\nm2 = sklearn.neural_network.MLPRegressor(random_state=0, max_iter=10000, hidden_layer_sizes=(20,20,20,20))\nm2.fit(X_train, y_train)\ny_test_pred2 = m2.predict(X_test)\n\n# plotando valores verdadeiros contra predições\nplt.figure()\nplt.plot(y_test, y_test_pred,'.', label='RF')\nplt.plot(y_test, y_test_pred2,'.', label='NLP')\nplt.plot(plt.gca().get_ylim(), plt.gca().get_ylim(), label='y=x')\nplt.xlabel('y_test')\nplt.ylabel('y_test_pred');\nplt.legend();\n\n# métricas de avaliação\nmae = sklearn.metrics.mean_absolute_error(y_test, y_test_pred)\nmse = sklearn.metrics.mean_squared_error(y_test, y_test_pred)\nmaxerror = sklearn.metrics.max_error(y_test, y_test_pred)\nr2 = sklearn.metrics.r2_score(y_test, y_test_pred)\n\nmae2 = sklearn.metrics.mean_absolute_error(y_test, y_test_pred2)\nmse2 = sklearn.metrics.mean_squared_error(y_test, y_test_pred2)\nmaxerror2 = sklearn.metrics.max_error(y_test, y_test_pred2)\nr22 = sklearn.metrics.r2_score(y_test, y_test_pred2)\n\nnomes =['MAE','MSE','Erro máximo','R^2']\n\ndados = {'Random Forest': [mae,mse,maxerror,r2],\n         'NLP': [mae2,mse2,maxerror2,r22]}\n\ndf1 = pd.DataFrame(dados, index = nomes)\ndisplay(df1)\n\nplt.figure()\nplt.bar(diabetes.feature_names, importance)\nplt.xticks(rotation=45)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nosso exemplo foi simples, mas o suficiente para motivar várias perguntas:\n\n* Como entender melhor a natureza dos dados?\n* O que se deve fazer quando há problemas nos dados (buracos, outliers, etc)?\n* Mesmo que os dados não tenham problemas, é possível mexer neles antes de entregá-los ao modelo de florestas aleatórias, de modo a melhorar os resultados? Como?\n* O que são as tais florestas que compõem o modelo?\n* É possível mexer nas florestas de modo a melhorar os resultados? Como?\n* Se a estratégia de separação treino/teste usada acima não for adequada, que outras estratégias podemos adotar?\n* O que fazer se o conjunto de dados for muito grande, tornando o treino exageradamente demorado?\n* É possível interpretar os resultados?\n\n\nVamos responder a todas elas ao longo desta aula!!"},{"metadata":{},"cell_type":"markdown","source":"# Tabela FIPE para Tratores de Esteira :D"},{"metadata":{},"cell_type":"markdown","source":"O conjunto de dados analisado nesta aula, retirado da competição  [Bluebook for Bulldozers](https://www.kaggle.com/c/bluebook-for-bulldozers), consiste de dados reais relativos a informações variadas acerca de tratores de esteira industriais. Nosso objetivo é criar um modelo que, dadas informações relativas a um trator em particular, seja capaz de predizer seu *preço de venda em leilões*.\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1e/CAT-D10N-pic001.jpg\" width=\"600\" height=\"600\"/>"},{"metadata":{},"cell_type":"markdown","source":"A abordagem aqui apresentada é baseada na solução proposta por Jeremy Howard em seu excelente curso [Introduction to Machine Learning for Coders](http://course18.fast.ai/ml)."},{"metadata":{},"cell_type":"markdown","source":"# Importando dados\n\nO primeiro passo é importar os dados de treino. Como os arquivos estão armazenados no formato [CSV (comma-separated values)](https://pt.wikipedia.org/wiki/Comma-separated_values), utilizamos a função [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html), da biblioteca [pandas](https://pandas.pydata.org/), para efetuar a leitura:"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = \"../input/bluebook-for-bulldozers/\"\n\ndf_raw = pd.read_csv(f'{PATH}Train.zip',\n                     compression='zip', \n                     low_memory=False, \n                     parse_dates=[\"saledate\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alguns parâmetros foram fornecidos à função: \n\n* **compression** especificou que os arquivos estão comprimidos no formato *zip*;\n* **low_memory** especificou que a função deve ler o arquivo como um todo e não em pequenos pedaços (isso é recomendável quando não sabemos bem que tipos de variáveis há no arquivo, por conta de uma questão técnica relacionada à forma como o [pandas](https://pandas.pydata.org/) infere os tipos de cada variável); \n* **parse_dates** especifica qual variável deve ser processada como data (no caso, no formato ano-mês-dia).\n\nConferindo o tamanho do conjunto:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"São 53 variáveis e quase meio milhão de observações!\n\nVisualizando algumas linhas do conjunto e utilizando o método [describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) para calcular algumas medidas estatísticas:"},{"metadata":{"trusted":true},"cell_type":"code","source":"with pd.option_context(\"display.max_columns\", 100): \n    display(df_raw)\n    display(df_raw.describe(include='all'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Parece que há [variáveis categóricas](https://en.wikipedia.org/wiki/Categorical_variable) e [valores faltantes](https://en.wikipedia.org/wiki/Missing_data) no conjunto de dados.\n\nPara analisar quais variáveis são numéricas e quais são categóricas, daremos uma olhada em seus tipos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As váriaveis não-numéricas são interpretadas pelo [pandas](https://pandas.pydata.org/) como sendo do tipo genérico **object**. Nota-se acima que são a grande maioria!\n\nVamos analisar quantas e quais são as categorias em cada variável categórica. No output abaixo, cada linha contém o nome de uma variável, o número de categorias entre parênteses e as categorias em si entre colchetes."},{"metadata":{"trusted":true},"cell_type":"code","source":"for n, c in df_raw.items():\n    if not pd.api.types.is_numeric_dtype(c) and not pd.api.types.is_datetime64_any_dtype(c):\n        print(f'{n} ({len(c.unique())}): {c.unique()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Há variáveis de alta cardinalidade: por exemplo, **fiModelDesc** tem 4999 valores possíveis!"},{"metadata":{},"cell_type":"markdown","source":"Usando a biblioteca [missingno](https://github.com/ResidentMario/missingno) para dar uma olhada nos valores faltantes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# para poder importar módulos que não estejam nos kernels do kaggle, \n# devemos instalá-los com o pip\n\n!pip install missingno","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno\n\nmissingno.bar(df_raw)\nmissingno.matrix(df_raw);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Há uma grande proporção de valores faltantes, a maioria deles nas variáveis categóricas (todas além das oito primeiras).\n\nO fato de haver variáveis categóricas e valores faltantes é crítico, já que algoritmos de regressão são projetados para lidar com variáveis numéricas. Portanto, é preciso pré-processar os dados e transformá-los em uma matriz de números antes de efetuar treinos de modelos de aprendizado."},{"metadata":{},"cell_type":"markdown","source":"# Pré-processamento dos dados\n\nA primeira etapa de pré-processamento será aplicar a função logaritmo à variável predita:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.SalePrice = np.log(df_raw.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Esse procedimento não é necessário, apenas conveniente. Ele é muito comum na predição de preços, já que nesses casos nos importamos mais com proporções do que com diferenças absolutas. Por exemplo, muitas vezes é mais significativo predizer um aumento ou redução de 10% (proporção) do que de 10 reais (diferença absoluta). Ao aplicar o logaritmo, os dados são transpostos para a escala logarítmica, em que as proporções se transformam em diferenças absolutas."},{"metadata":{},"cell_type":"markdown","source":"Vamos encapsular o restante de nosso procedimento de pré-processamento em uma função:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_process (df):\n    \n    new_df = pd.DataFrame()\n    \n    for n,c in df.items():\n                \n        if pd.api.types.is_numeric_dtype(c):\n            # substituindo NaN numericos pelas medianas de cada coluna\n            new_df[n] = c.fillna(value=c.median())\n        else:\n            # interpretando o que nao for numerico como variaveis categoricas \n            # e transformando cada categoria em um numero\n            new_df[n] = pd.Categorical(c.astype('category').cat.as_ordered()).codes+1\n    \n    return new_df     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Na função acima, iteramos ao longo de todas as colunas do conjunto de dados. Se a coluna em questão for numérica, os valores faltantes são substituídos pela mediana. Se não for numérica, são transformadas em categorias e cada categoria, por sua vez, é associada a um número (ao final, o valor 1 é adicionado para que a contagem comece do 1 e não do 0, o que é conveniente para os algoritmos).\n\nValores faltantes nas váriaveis categóricas não são tão graves porque o próprio fato de um valor estar faltando pode ser interpretado como uma categoria. Já nas variáveis numéricas, eles são críticos, por isso foi preciso substitui-los por algum número.\n\nAplicando a função acima e gerando o [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) processado:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_proc = pre_process(df_raw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separando o conjunto em X (variáveis preditoras) e y (variável predita):"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = df_proc.drop('SalePrice', axis=1), df_proc['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separando o conjunto em duas partes, o treino e a validação:"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_valid = 12000\nn_trn = len(df_proc)-n_valid\n\nX_treino, X_validacao = X[:n_trn].copy(), X[n_trn:].copy()\ny_treino, y_validacao = y[:n_trn].copy(), y[n_trn:].copy()\n\ny_treino.shape, y_validacao.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O conjunto de validação é um conjunto usado para testes intermediários durante o processo de modelagem. Além da validação, é recomendável que haja um conjunto de teste em separado para ser usado *após* a finalização da modelagem. Todas as competições do Kaggle possuem esse conjunto, que é usado para criar as pontuações e leaderboards.\n\nPerceba que, como há *evolução temporal*, não separamos o treino e o teste de maneira aleatória, como fizemos no exemplo introdutório. Agora retiramos as 12000 últimas observações para teste, o que faz sentido, pois na vida real um modelo é aplicado para prever valores em instantes de tempo posteriores aos usados no treino."},{"metadata":{},"cell_type":"markdown","source":"# Treinando a primeira floresta\n\nPara poupar tempo, vamos definir uma função, chamada **display_score**, que aceita um modelo treinado e imprime na tela as métricas $R^2$ e [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) relativas ao treino e à validação:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(x,y): \n    \n    return np.sqrt(sklearn.metrics.mean_squared_error(x,y))\n\ndef display_score(m):\n    \n    res = [[rmse(m.predict(X_treino), y_treino), rmse(m.predict(X_validacao), y_validacao)],\n          [m.score(X_treino, y_treino), m.score(X_validacao, y_validacao)]]\n    \n    score = pd.DataFrame(res, index=['RMSE','R2'], columns = ['Treino','Validação'])\n    \n    if hasattr(m, 'oob_score_'): \n        score.loc['OOB R2'] = [m.oob_score_,'-']\n        \n    display(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A célula abaixo treina um modelo de florestas aleatórias que especificaremos como o *modelo base*:"},{"metadata":{"trusted":true},"cell_type":"code","source":"m_base = sklearn.ensemble.RandomForestRegressor(n_jobs=-1, oob_score = True, random_state = 0)\n%time m_base.fit(X_treino, y_treino)\ndisplay_score(m_base)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explicando as três linhas acima:\n\n* na primeira linha, definimos o modelo. O hiperparâmetro **n_jobs = -1** especifica que, caso haja múltiplos processadores no computador, todos devem ser usados em paralelo. O hiperparâmetro **oob_score** será explicado mais adiante.\n\n* na segunda linha, treinamos o modelo. Repare no uso do comando mágico %time, que mede o tempo necessário para essa tarefa.\n\n* na terceira linha, usamos a função **display_score**, definida anteriormente, para imprimir as métricas.\n\nOs resultados são bem satisfatórios! O $R^2$ é próximo de 1 e, analisando o [leaderboard da competição](https://www.kaggle.com/c/bluebook-for-bulldozers/leaderboard), nosso RMSE está no mesmo patamar do top 20! De fato, o modelo de florestas aleatórias é excelente para resolver esse tipo de problema: com alto grau de não-linearidade, dados sem estrutura clara e grande número de variáveis categóricas.\n\nA partir de agora, concentraremos nossos esforços em:\n\n* entender como funciona o modelo;\n* utilizar algumas técnicas para melhorar os resultados."},{"metadata":{},"cell_type":"markdown","source":"# O que é uma floresta aleatória, afinal?\n\nFlorestas, obviamente, são feitas de árvores!\n\nEm particular, os modelos de florestas aleatórias são compostos por vários modelos mais simples conhecidos como *árvores de decisão*.\n\nPortanto, antes de entender a floresta, é preciso entender a árvore."},{"metadata":{},"cell_type":"markdown","source":"## Treinando e visualizando uma árvore de decisão\n\nNa nomenclatura do [scikit-learn](https://scikit-learn.org/stable/), cada árvore é chamada de *estimador*. Para treinar apenas 1 árvore, portanto, podemos fornecer o hiperparâmetro **n_estimators=1** para o modelo:"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Os resultados são bem piores do que antes! Usar a floresta ao invés de 1 só árvore parece fazer toda a diferença. Outro detalhe: o treinamento ocorre de forma bem mais rápida.\n\nA seguir, define-se uma função para visualizar uma árvore:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_tree(t, df, size=10, ratio=1, precision=0):\n   \n    import re\n    import graphviz\n    import sklearn.tree\n    import IPython.display\n    \n    s=sklearn.tree.export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n                                   special_characters=True, rotate=True, precision=precision)\n    IPython.display.display(graphviz.Source(re.sub('Tree {',\n       f'Tree {{ size={size}; ratio={ratio}', s)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Utilizando-a para visualizar a árvore recém-treinada:"},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_tree(m.estimators_[0], X_treino, precision=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Uma árvore é uma sequência de decisões binárias. Cada quadradinho acima é um *nó* e representa uma porção dos dados. O primeiro nó é chamado de *raiz* e contém a totalidade dos dados. Os últimos nós são chamados de *folhas*. Cada nó é gerado a partir de um nó da camada anterior por meio de uma decisão correspondente a alguma variável. Os dois nós da segunda camada, por exemplo, são gerados a partir do primeiro nó por meio de uma decisão relativa à variável **Coupler_System**.\n\nEm cada nó são exibidos:\n\n* uma métrica de predição (no caso, **mse**);\n* o número de amostras (**samples**);\n* a predição em si (**value**), que corresponde simplesmente à média da variável predita no nó.\n\nO *split*, ou seja, a decisão a ser tomada em cada nó, é especificada de modo a minimizar os erros dos dois nós resultantes. Há vários algoritmos capazes de efetuar essa minimização, como o [CART](https://medium.com/@arifromadhan19/regrssion-in-decision-tree-a-step-by-step-cart-classification-and-regression-tree-196c6ac9711e), por exemplo.\n\nA árvore acima foi treinada com profundidade 3. O que acontece se treinarmos uma árvore maior? Na célula abaixo, usamos o [default do modelo](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), que cresce a árvore até que todas as folhas estejam *puras*. Uma folha pura contém apenas 1 valor da variável predita. Em outras palavras, cada valor assumido pela variável predita corresponde a 1 folha na árvore. Como nosso conjunto tem 389125 linhas, a árvore gerada será bem grande!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Os resultados melhoram, mas o sobreajuste é enorme! Esse é um problema dos modelos de (uma) árvore: como eles são muito flexíveis e conseguem modelar todo o espaço dos dados por meio de várias partições, grandes são as chances de se aprender um número excessivo de comportamentos, não correspondentes a padrões generalizáveis.\n\nPara melhorar a generalização, não é suficiente usar árvores maiores. É preciso combinar os resultados de múltiplas árvores."},{"metadata":{},"cell_type":"markdown","source":"## Juntando várias árvores\n\nPara entendermos como as várias árvores formam a floresta, voltemos ao nosso modelo base:"},{"metadata":{"trusted":true},"cell_type":"code","source":"display_score(m_base)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Na célula abaixo, criamos e exibimos um [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) em que cada linha corresponde a uma observação do conjunto de validação e cada coluna corresponde à predição de uma das árvores da floresta. No final do [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) adicionamos três colunas, respectivamente, com: \n\n* as médias das predições de todas as árvores;\n* os desvios-padrão das predições de todas as árvores;\n* o valor verdadeiro da variável predita."},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.stack([t.predict(X_validacao) for t in m_base.estimators_]).T\npreds_df = pd.DataFrame(preds)\n\npreds_df['medias'] = preds_df.mean(axis=1)\npreds_df['stds'] = preds_df.std(axis=1)\npreds_df['valor real'] = y_validacao.values\npreds_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Olha que interessante: o erro de predição de cada árvore individual é alto (já que as árvores são sobreajustadas); mas quando *tiramos a média de todas as 100 árvores* (que formam a floresta!), o erro é baixo. Parece mágica! Tiramos a média de várias predições meia-boca e o resultado... é uma excelente predição!!! O que está acontecendo?\n\nO truque é *fazer com que as árvores apresentem o mínimo possível de correlação entre elas*. Sendo assim, cada árvore aprende de maneira sobreajustada uma porção isolada dos padrões que queremos capturar. Ao tirarmos a média, juntamos todos os pedacinhos que cada árvore aprendeu individualmente... e criamos um modelo completo e robusto!\n\nA principal estratégia para plantar uma floresta de árvores descorrelacionadas é fazer com que cada árvore utilize uma parcela aleatória dos dados. Dessa maneira, cada árvore sobreajusta de diferentes maneiras em diferentes fenômenos; ou seja, todas elas têm grandes erros, mas os erros são aleatórios. \n\n*E, de acordo com a Estatística, qual é a média de um monte de erros aleatórios?* \n\nZero!!\n\nNo algoritmo de florestas aleatórias, cada árvore efetua amostras *com reposição* (esse procedimento é conhecido como [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating)). Dessa maneira, nem todo o conjunto de dados é utilizado por cada árvore, já que no procedimento de amostragem várias observações podem se repetir. Em média, aproximadamente apenas 63,2% dos dados são utilizados por cada árvore. Isso ajuda bastante a diminuir a correlação entre elas.\n\nAbaixo, para termos uma noção visual do resultado, plotamos as predições contra os valores verdadeiros e comparamos com a reta $x=y$:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y_validacao.values, preds_df.mean(axis=1), '.')\n\nplt.plot(plt.gca().get_ylim(), plt.gca().get_ylim());\n\nplt.xlabel('y_valid')\nplt.ylabel('y_valid_pred');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nada mal."},{"metadata":{},"cell_type":"markdown","source":"## Out-of-bag score\n\nAcabamos de aprender que, devido à amostragem com reposição, cada árvore ignora uma parcela de observações.\n\nA métrica [OOB (out-of-bag)](https://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html) se vale desse fato para medir a capacidade preditiva do modelo *sem a necessidade de um conjunto de teste em separado*. Para efetuar as predições e calcular o OOB, cada árvore utiliza os dados de treino que foram por ela ignorados. Como a árvore não treinou o modelo com esses dados, eles efetivamente funcionam como um bom conjunto de teste!\n\nNo [scikit-learn](https://scikit-learn.org/), é preciso fornecer o parâmetro **oob_score = True** para que o OOB seja calculado durante o treino."},{"metadata":{},"cell_type":"markdown","source":"# Sintonizando hiperparâmetros\n\nNesta seção daremos uma olhada em como podemos mexer em alguns hiperparâmetros do modelo de modo a melhorar os desempenhos preditivo e computacional."},{"metadata":{},"cell_type":"markdown","source":"## n_estimators\n\n**n_estimators** é o número de árvores na floresta. A regra para escolher esse valor é simples: quanto mais árvores, melhor a capacidade preditiva do modelo, mas maior o custo computacional.\n\nVamos dar uma olhada na relação entre a métrica $R^2$ e a quantidade de árvores em nossa floresta:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot([sklearn.metrics.r2_score(y_validacao, np.mean(preds[:,:i+1], axis=1)) for i in range(100)]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Há um platô a partir do qual adicionar mais árvores não faz muita diferença. Esse é o comportamento esperado: a partir de um certo número de árvores, os ganhos de desempenho passam a ser muito pequenos.\n\nO default do [scikit-learn](https://scikit-learn.org/) é usar 100 árvores. Mas da figura acima nota-se que muito antes disso o modelo atinge o platô de desempenho. Isso sugere a diminuição do número de árvores, de modo a economizar custo computacional."},{"metadata":{"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor(n_estimators = 10, n_jobs=-1, oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Passamos de 3-4min para 20-30s, sem prejudicar muito a acurácia!"},{"metadata":{},"cell_type":"markdown","source":"## max_samples\n\nO hiperparâmetro **max_samples** restringe o número de observações que serão amostradas por cada árvore durante o treino. É um bom truque para quando os conjuntos de dados são muito grandes: todos os dados ficam disponíveis para o treino do modelo, mas a amostragem de cada árvore se dá apenas em subconjuntos de tamanho **max_samples**. Isso reduz o custo computacional e pode ajudar a atenuar problemas de sobreajuste."},{"metadata":{"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor(max_samples = 40000, n_jobs=-1, oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## min_samples_leaf e max_features\n\n* O hiperparâmetro **min_samples_leaf** especifica o número mínimo de amostras contidas em cada folha. Em outras palavras, determina o número necessário de amostras em um nó para interromper o crescimento de seu ramo. Aumentar **min_samples_leaf** faz com que as árvores sejam menos profundas, o que diminui a acurácia de cada árvore individual, mas também potencialmente diminui a correlação entre elas, melhorando a generalização.\n\n* O hiperparâmetro **max_features** especifica um número máximo de variáveis a ser considerado para decidir o split de cada nó. Diminuir esse número máximo diminui o efeito de variáveis muito influentes, diminuindo assim a correlação entre as árvores. A redução das variáveis disponíveis por split foi proposta no [paper original do modelo de florestas aleatórias](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) e muitos consideram que um modelo só poder ser considerado floresta aleatória se efetuar esse procedimento. O [scikit-learn](https://scikit-learn.org/), no entanto, por default não o efetua.\n\nVamos mexer nesses parâmetros e tentar obter um resultado melhor:"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor(min_samples_leaf = 3, max_features = 0.5, \n                                           n_jobs=-1, oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Melhoramos! Diminuindo agora o número de árvores para obter um modelo mais eficiente, que usaremos nas análises que seguirão:"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor(n_estimators = 50, min_samples_leaf = 3, \n                                           max_features = 0.5, n_jobs=-1, \n                                           oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Em suma, ajustando os hiperparâmetros da floresta, conseguimos, em relação ao modelo base:\n\n- melhorar a métrica de desempenho na terceira casa decimal. Na maioria das situações práticas isso não seria importante, mas pode valer milhares de dólares em uma competição Kaggle;\n- reduzir o esforço computacional para aproximadamente 20% do original.\n\nÉ possível automatizar a busca pelos hiperparâmetros utilizando técnicas numéricas de otimização, mas este tema será tratado em uma próxima aula."},{"metadata":{},"cell_type":"markdown","source":"***Mão na massa 2!***\n\n* Treine o modelo mais algumas vezes, variando os valores dos hiperparâmetros apresentados. Analise os efeitos nos resultados. Leia a [referência do modelo](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) e faça o mesmo com outros hiperparâmetros que não discutimos. Tente superar o desempenho acima :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#n_estimators: diminuir não prejudica significativamente nas métricas de treino e validação e melhora muito o custo computacional (mantive 50 para comparação)\n#min_samples_leaf: identifiquei que 4 fica melhor que 3 ou 5, talvez seja um valor ótimo para esse conjunto de dados e essa configuração\n#max_features: mexer no max_features não melhorou o treinamento\n#max_leaf_nodes: não surtiu efeito positivo nas métricas de treino e validação, na realidade esse hiperparâmetro compete com o min_samples_leaf, que apresentou melhora significativa\n#min_impurity_decrease: o aumento desse hiperparâmetro piorou muito as métricas de treino e validação\n#bootstrap: não consegui rodar com a opção false (aparece um erro)\n#verbose: somente mostra um log mais detalhado do processo de treinamento\n#warm_start: partir da solução anterior não apresentou benefício\n#ccp_alpha: o aumento desse hiperparâmetro piorou muito as métricas de treino e validação\n#max_samples: limitar o número de amostras não melhorou o processo de treinamento\n\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 50, min_samples_leaf = 4, \n                                           max_features = 0.5, n_jobs=-1,\n                                           oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Interpretação do modelo e engenharia de características\n\nMuito se diz por aí que não é possível interpretar as predições feitas por modelos de aprendizado de máquina, por eles serem complicados e totalmente empíricos. Nossa missão nesta seção é mostrar que essa afirmação é falsa.\n\nEm particular, o modelo de florestas aleatórias pode fornecer muitas informações sobre a natureza das predições e as influências exercidas por cada variável nos resultados. Essas informações podem ser valiosas na importante atividade de [engenharia de características](https://en.wikipedia.org/wiki/Feature_engineering) (mais conhecida pela expressão em inglês *feature engineering*), que consiste na manipulação das variáveis (colunas) do conjunto de dados com o objetivo de melhorar o desempenho dos modelos."},{"metadata":{},"cell_type":"markdown","source":"## Importâncias das variáveis\n\nO modelo de florestas aleatórias calcula internamente um ranking de importância das variáveis. Para uma dada variável, quanto maior a diminuição do erro em splits de decisões tomadas com base nessa variável, mais importante ela será. Esse ranking fica armazenado no atributo **feature_importances_** do modelo. \n\nNa próxima célula, definimos uma função que aceita um modelo e uma lista com os nomes das variáveis, imprime na tela informações relativas ao ranking de importância e retorna um [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) com o ranking em si."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotar_importancias(modelo, tags, n=10):\n    \n    fig, ax = plt.subplots(1,2, figsize = (20,4))\n\n    coefs = []\n    abs_coefs = []\n\n    if hasattr(modelo,'coef_'):\n        imp = modelo.coef_\n    elif hasattr(modelo,'feature_importances_'):\n        imp = modelo.feature_importances_\n    else:\n        print('sorry, nao vai rolar!')\n        return\n\n    coefs = (pd.Series(imp, index = tags))\n    coefs.plot(use_index=False, ax=ax[0]);\n    abs_coefs = (abs(coefs)/(abs(coefs).sum()))\n    abs_coefs.sort_values(ascending=False).plot(use_index=False, ax=ax[1],marker='.')\n\n    ax[0].set_title('Importâncias relativas das variáveis')\n    ax[1].set_title('Importâncias relativas das variáveis - ordem decrescente')\n\n    abs_coefs_df = pd.DataFrame(np.array(abs_coefs).T,\n                                columns = ['Importancias'],\n                                index = tags)\n\n    df = abs_coefs_df['Importancias'].sort_values(ascending=False)\n    \n    print(df.iloc[0:n])\n    plt.figure()\n    df.iloc[0:n].plot(kind='barh', figsize=(15,0.25*n), legend=False)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Utilizando a função para analisar as importâncias do nosso último modelo:"},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = plotar_importancias(m, X_validacao.columns,30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removendo variáveis pouco importantes\n\nVariáveis sem importância podem ser descartadas, o que talvez melhore a acurácia do modelo e certamente melhorará o desempenho computacional.\n\nSelecionando, por exemplo, apenas as que apresentam mais que 0,5% importância:"},{"metadata":{"trusted":true},"cell_type":"code","source":"to_keep = imp[imp>0.005].index\nto_keep.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_treino = X_treino[to_keep]\nX_validacao = X_validacao[to_keep]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Eliminamos por volta de 30 variáveis irrelevantes."},{"metadata":{},"cell_type":"markdown","source":"## Análise de correlações\n\nUma análise de correlações é útil para entender as relações entre as variáveis.\n\nA correlação mais utilizada para isso é a [correlação de Pearson](https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_Pearson), que mede o grau de *associação linear* entre as variáveis. Duas variáveis são linearmente associadas se mudanças em uma variável implicam em mudanças diretamente proporcionais na outra variável.\n\nAqui usaremos a [correlação de Spearman](https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_postos_de_Spearman), que mede o grau de *associação monotônica* entre as variáveis. Duas variáveis são monotonicamente associadas se mudanças em uma variável implicam em mudanças no mesmo sentido (crescente ou decrescente) na outra variável. É uma concepção de associação mais genérica do que a de Pearson.\n\nA função abaixo aceita um [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) e plota um gráfico do tipo [dendograma](https://en.wikipedia.org/wiki/Dendrogram) mostrando as correlações de Spearman entre as variáveis:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def dendogram_spearmanr(df, tags):\n\n    import scipy.cluster.hierarchy\n    import scipy.stats\n    \n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = scipy.cluster.hierarchy.distance.squareform(1-corr)\n    z = scipy.cluster.hierarchy.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=(18,8))\n    dendrogram = scipy.cluster.hierarchy.dendrogram(z, labels=tags, orientation='left', leaf_font_size=16)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dendogram_spearmanr(X_treino, X_treino.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O gráfico foi gerado por meio de técnicas de [clusterização hierárquica](https://en.wikipedia.org/wiki/Hierarchical_clustering), que separaram as variáveis em grupos de acordo com as correlações entre elas. É evidente que algumas variáveis possuem altíssima correlação, como **GrouserTracks**, **Hydraulics_Flow** e **Coupler_System**, por exemplo. Isso significa que elas possuem a mesma informação e são potencialmente redundantes.\n\nNas próximas células, removeremos algumas variáveis que o gráfico indica como redundantes e verificaremos o efeito no OOB. Caso o efeito seja pequeno, podemos descartar as variáveis."},{"metadata":{},"cell_type":"markdown","source":"## Removendo variáveis redundantes\n\nA função abaixo é definida para agilizar as análises: ela aceita um conjunto X, efetua um treino e retorna o score OOB."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_oob(X):\n    m = sklearn.ensemble.RandomForestRegressor(n_estimators=30, min_samples_leaf=5, \n                                               max_features=0.6, n_jobs=-1, max_samples = 100000,\n                                               oob_score=True, random_state = 0)\n    m.fit(X, y_treino)\n    return m.oob_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Estabelecendo nossa referência:"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_oob(X_treino)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Efetuando análises com remoções de uma variável potencialmente redundante por vez:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in ('Grouser_Tracks', 'Hydraulics_Flow', 'Coupler_System',\n          'fiModelDesc', 'fiBaseModel','ProductGroupDesc', 'ProductGroup'):\n    print(c, get_oob(X_treino.drop(c, axis=1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nenhuma dessas variáveis parece fazer falta, já que o OOB não diminui significativamente!\n\nEfetuando de fato as remoções:"},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop = ['ProductGroupDesc', 'fiModelDesc', 'Grouser_Tracks', 'Hydraulics_Flow']\nget_oob(X_treino.drop(to_drop, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_treino = X_treino.drop(to_drop, axis=1)\nX_treino.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Com esse procedimento, conseguimos diminuir ainda mais o número de variáveis."},{"metadata":{},"cell_type":"markdown","source":"***Mão na massa 3!***\n\n* Repita os procedimentos de análise de importância e análise de correlação com os conjuntos de dados [Boston Housing](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) e [Diabetes](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html) usados no começo do notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble\nimport sklearn.neural_network\n\nimport pandas as pd\n\nboston = sklearn.datasets.load_boston()\nX, y = boston.data, boston.target\n\nX_treino, X_validacao, y_treino, y_validacao = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.2, \n                                                                            random_state = 0)\n\n# Modelo Random Forest - Caso Base\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 130, min_samples_leaf = 1, \n                                           max_features = 0.99, n_jobs=-1,\n                                           oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)\n\n# Análise de importância das variáveis\nimp = plotar_importancias(m, boston.feature_names,30)\ndisplay(boston.feature_names)\n#removendo variáveis com imp<0.01\nX_treino = X_treino[:,[0,4,5,6,7,9,10,12]]\nX_validacao = X_validacao[:,[0,4,5,6,7,9,10,12]]\n\n# Modelo Random Forest - Removendo Variáveis Irrelevantes\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 130, min_samples_leaf = 1, \n                                           max_features = 0.99, n_jobs=-1,\n                                           oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)\n\n# Análise da correlação entre as variáveis\n\ndendogram_spearmanr(X_treino, boston.feature_names)\n# removendo a variável RM, pois ela tem correlação com AGE\nX_treino, X_validacao, y_treino, y_validacao = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.2, \n                                                                            random_state = 0)\n\nX_treino = X_treino[:,[0,4,6,7,9,10,12]]\nX_validacao = X_validacao[:,[0,4,6,7,9,10,12]]\n\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 130, min_samples_leaf = 1, \n                                           max_features = 0.99, n_jobs=-1,\n                                           oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)\n\n# Neste modelo não vale a pena remover variáveis, nem pelo grau de importância, nem pela correlação. \n# A remoção das variáveis com importancia menor do que 0.01 não impactou de forma sensível no R^2 da\n# validação, porém também não se refletiu em um ganho de tempo computacional.\n# A remoção da variável RM, por ter correlação com AGE acarretou em uma queda forte do R^2, não se\n# refletindo em qualquer benefício para o modelo.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble\nimport sklearn.neural_network\n\nimport pandas as pd\n\ndiabetes = sklearn.datasets.load_diabetes()\nX, y = diabetes.data, diabetes.target\n\nX_treino, X_validacao, y_treino, y_validacao = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.1, \n                                                                            random_state = 0)\n\ndisplay(X_treino.shape)\n\n# Modelo Random Forest - Caso Base\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 100, min_samples_leaf = 1,\n                                           n_jobs=-1, oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)\n\n#preds = np.stack([t.predict(X_validacao) for t in m.estimators_]).T\n#preds_df = pd.DataFrame(preds)\n\n#preds_df['medias'] = preds_df.mean(axis=1)\n#preds_df['stds'] = preds_df.std(axis=1)\n#preds_df['valor real'] = y_validacao\n#plt.plot([sklearn.metrics.r2_score(y_validacao, np.mean(preds[:,:i+1], axis=1)) for i in range(100)]);\n\n\n# Análise de importância das variáveis\nimp = plotar_importancias(m, diabetes.feature_names,30)\ndisplay(diabetes.feature_names)\n#removendo variáveis com imp<0.05\nX_treino = X_treino[:,[0,2,3,5,6,8,9]]\nX_validacao = X_validacao[:,[0,2,3,5,6,8,9]]\n\n# Modelo Random Forest - Removendo Variáveis Irrelevantes\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 100, min_samples_leaf = 1,\n                                           n_jobs=-1, oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)\n\n# Análise da correlação entre as variáveis\ndendogram_spearmanr(X_treino, diabetes.feature_names)\n\n# Neste modelo não vale a pena remover variáveis, nem pelo grau de importância, nem pela correlação. \n# A remoção das variáveis com importancia menor do que 0.05 impactou de forma sensível no R^2 da\n# validação, que já se mostrou muito baixo.\n# Não há variáveis fortemente correlacionadas.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Variáveis *dummy* ou *one-hot encoding*\n\nO uso de [variáveis *dummy*](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) ou *one-hot encoding* é uma estratégia diferente para organizar variáveis categóricas. Nessa metodologia, as variáveis são desmembradas em variáveis binárias correspondentes a cada uma de suas categorias. A figura a seguir ilustra bem a situação:\n\n<img src=\"https://i1.wp.com/thierrymoudiki.github.io/images/2020-02-28/2020-02-28-image1.png?w=578&ssl=1\" width=\"500\" height=\"500\"/>\n\nA vantagem dessa representação é que a influência de categorias específicas pode tornar-se mais clara. Em alguns casos, a acurácia do modelo pode aumentar. A desvantagem é que a dimensionalidade dos dados (número de colunas) aumenta, diminuindo o desempenho computacional.\n\nA seguir definimos uma nova função para pré-processamento, que utiliza por sua vez a função [get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) do [pandas](https://pandas.pydata.org/) para implementar o *one-hot encoding*. Há a especificação de um número máximo de categorias, expressa pelo argumento **max_cats**, que evita o aumento excessivo da dimensionalidade no caso de variáveis de alta cardinalidade. Caso uma variável tenha mais do que **max_cats** categorias, não são geradas variáveis *dummy* a partir dela."},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_process_OHE (df, max_cats = 10):\n    \n    new_df = pd.DataFrame()\n    \n    for n,c in df.items():\n                \n        if pd.api.types.is_numeric_dtype(c):\n            # substituindo NaN numericos pelas medianas de cada coluna\n            new_df[n] = c.fillna(value=c.median())\n        else:\n            # interpretando o que nao for numerico como variaveis categoricas \n            new_df[n] = pd.Categorical(c.astype('category').cat.as_ordered())\n            # transformando cada categoria em um numero, caso nao va fazer one hot encoding com ela\n            if len(c.astype('category').cat.categories) > max_cats:\n                new_df[n] = pd.Categorical(new_df[n]).codes+1\n    \n    # a função pd.get_dummies faz o one-hot encoding\n    return pd.get_dummies(new_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dando uma conferida no [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) original, para fins de comparação:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pré-processando o [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) com *one-hot encoding*:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_proc_ohe = pre_process_OHE(df_raw)\ndf_proc_ohe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quase triplicamos a quantidade de variáveis!\n\nAgora vamos repetir o processo de treinamento e análise de variáveis com o novo conjunto de dados."},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = df_proc_ohe.drop('SalePrice', axis=1), df_proc_ohe['SalePrice']\n\nn_valid = 12000\nn_trn = len(df_proc)-n_valid\n\nX_treino, X_validacao = X[:n_trn].copy(), X[n_trn:].copy()\ny_treino, y_validacao = y[:n_trn].copy(), y[n_trn:].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = sklearn.ensemble.RandomForestRegressor(n_estimators = 50, min_samples_leaf = 3, \n                                           max_features = 0.5, n_jobs=-1, \n                                           oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotar_importancias(m, X_validacao.columns,30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Olha que interessante: a categoria **w AC** de Enclosure_EROPS é a mais relevante para a determinação do preço (o que faz sentido, já que **w AC** significa \"com ar-condicionado\")."},{"metadata":{},"cell_type":"markdown","source":"## Análise de contribuições\n\nÁrvores e florestas podem ser interpretadas! Em particular, é possível entender o *porquê* de uma predição em específico, analisando as decisões que as árvores tomam para chegar a essa predição.\n\nO módulo [treeinterpreter](https://pypi.org/project/treeinterpreter/) serve justamente para isso:"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install treeinterpreter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from treeinterpreter import treeinterpreter as ti","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[treeinterpreter](https://pypi.org/project/treeinterpreter/) decompõe uma predição na soma *bias+contribuições*. O bias é a média da variável predita. As contribuições refletem o quanto cada variável contribui para afastar uma predição em específico dessa média.\n\nAs contribuições de cada variável são calculadas por meio dos efeitos na predição dos vários splits que a envolvem. Esse [artigo](http://blog.datadive.net/interpreting-random-forests/) detalha bem a ideia.\n\nVamos dar uma olhada nas contribuições da primeira linha do conjunto de validação:"},{"metadata":{"trusted":true},"cell_type":"code","source":"row = X_validacao.values[np.newaxis,0]\n\nprediction, bias, contributions = ti.predict(m, row)\n\nidxs = np.argsort(contributions[0])\n[o for o in zip(X_validacao.columns[idxs], X_validacao.iloc[0][idxs], contributions[0][idxs])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lista acima está organizada em ordem crescente de contribuições.\n\nDela podemos inferir que o que mais contribui para diminuir o preço do trator em questão é seu tamanho pequeno (**ProductSize_Mini**) e o que mais contribui para aumentar o preço é o ano de fabricação (1999). O que parece fazer todo sentido.\n\nSe você acreditava na falácia de que os modelos de aprendizado de máquina não possibilitavam interpretação de seus resultados, espero que sua opinião tenha mudado agora!!!"},{"metadata":{},"cell_type":"markdown","source":"***Mão na massa 4!***\n\n* Repita o procedimento de análise de contribuições com os conjuntos de dados [Boston Housing](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) e [Diabetes](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html) usados no começo do notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble\nimport sklearn.neural_network\n\nimport pandas as pd\n\nboston = sklearn.datasets.load_boston()\nX, y = boston.data, boston.target\n\nX_treino, X_validacao, y_treino, y_validacao = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.2, \n                                                                            random_state = 0)\n\n# Modelo Random Forest - Caso Base\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 130, min_samples_leaf = 1, \n                                           max_features = 0.99, n_jobs=-1,\n                                           oob_score = True, random_state = 0)\nm.fit(X_treino, y_treino)\n\nrow = X_validacao[np.newaxis,0]\nprediction, bias, contributions = ti.predict(m, row)\n\nidxs = np.argsort(contributions[0])\n\nrotulos=boston.feature_names[idxs]\n\nfor o in range(12):\n    print(rotulos[o], row[0,o], contributions[0,o])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble\nimport sklearn.neural_network\n\nimport pandas as pd\n\ndiabetes = sklearn.datasets.load_diabetes()\nX, y = diabetes.data, diabetes.target\n\nX_treino, X_validacao, y_treino, y_validacao = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.1, \n                                                                            random_state = 0)\n\n# Modelo Random Forest - Caso Base\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 100, min_samples_leaf = 1,\n                                           n_jobs=-1, oob_score = True, random_state = 0)\nm.fit(X_treino, y_treino)\n\nrow = X_validacao[np.newaxis,0]\nprediction, bias, contributions = ti.predict(m, row)\n\nidxs = np.argsort(contributions[0])\n\nrotulos = np.array(diabetes.feature_names)\nrotulos = rotulos[idxs]\n\nfor o in range(8):\n    print(rotulos[o], row[0,o], contributions[0,o])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aspectos temporais\n\nO modelo de florestas aleatórias é ótimo para capturar não-linearidades e representar dados sem estrutura matemática definida. No entanto, ele não consegue efetuar extrapolações e modelar tendências temporais. Um exemplo simples ajuda a ilustrar:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# criando exemplo simples com tendência linear\nN = 30\nx = np.arange(N)\ny = 2*x\n\n# adicionando ruído\ny = y+ 3*np.random.randn(N)\n\n# separando em treino e teste\n\nn = int(N/2)\n\n# se eu nao criar esse novo eixo em x a seguir, o sklearn reclama, \n# pq pra ele a array de dados preditores tem q ter 2 dimensoes:\nx = x[:,np.newaxis]   \n\nx_treino, y_treino = x[:n], y[:n]\nx_treino, y_treino = x[:n], y[:n]\n\nx_teste, y_teste = x[n:], y[n:]\nx_teste, y_teste = x[n:], y[n:]\n\n# especificando modelos\n\nimport sklearn.linear_model\nimport sklearn.neural_network\nimport sklearn.svm\nimport sklearn.neighbors\n\nmodelos = [sklearn.linear_model.LinearRegression(),\n           sklearn.neural_network.MLPRegressor(),\n           sklearn.ensemble.RandomForestRegressor(),\n           sklearn.neighbors.KNeighborsRegressor(),\n           sklearn.svm.SVR()]\n\n# preparando janela do gráfico\nfig, ax = plt.subplots(1,5,figsize=(20,3))\n\n# calculando e plotando\nfor i in range(len(modelos)):\n    modelos[i].fit(x_treino, y_treino)\n    ax[i].plot(x, y)\n    ax[i].plot(x, modelos[i].predict(x),'.')\n    ax[i].set_title(modelos[i].__class__.__name__)\n    ax[i].axvline(n,ls='--',c='k')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Os dois primeiros modelos, [regressão linear](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) e [rede neural MLP](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html), capturam a tendência linear com bastante clareza. As redes neurais, que serão tema da próxima aula, são bem-sucedidas na extrapolação porque possuem estrutura matemática bem-definida. Essa estrutura possibilita a captura da tendência linear.\n\nO desempenho do modelo de florestas aleatórias é triste. Ocorre sobreajuste no treino e a baixa capacidade de extrapolação é evidente no teste. O motivo é o fato de o modelo não possuir estrutura matemática definida, já que se baseia puramente em partições no conjunto de dados. Isso proporciona flexibilidade para a modelagem, mas o preço a ser pago é justamente essa incapacidade de extrapolação.\n\nO quarto modelo, chamado de [k-vizinhos mais próximos (kNN, k-nearest neighbors)](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html), também não tem estrutura matemática definida, pois [se baseia puramente nas distâncias entre os pontos](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\n\nO quinto modelo, [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html), baseado em [máquinas de vetores-suporte](https://en.wikipedia.org/wiki/Support_vector_machine), tem estrutura matemática bem-definida, mas é inerentemente não-linear e por isso fracassa miseravelmente."},{"metadata":{},"cell_type":"markdown","source":"### Dicas sobre aspectos temporais\n\nFlorestas aleatórias não são designadas para modelar tendências temporais, mas isso não significa que tais aplicações sejam impossíveis. No entanto, procedimentos adicionais são necessários para criar boas soluções. Algumas dicas de engenharia de características a respeito:\n\n* adicionar variáveis atrasadas no tempo para modelar autocorrelação (como [aqui](https://machinelearningmastery.com/feature-selection-time-series-forecasting-python/));\n* adicionar variáveis que representem derivadas (*delta features*) para modelar tendências (como [aqui](https://www.sciencedirect.com/science/article/pii/S002002551931076X));\n* criar variáveis que detalham datas em distintas granularidades (a função [add_datepart](https://docs.fast.ai/tabular.core#add_datepart), da biblioteca [fastai](https://docs.fast.ai/), é ótima para isso).\n"},{"metadata":{},"cell_type":"markdown","source":"# Modelo final\n\nChegou a hora de gerarmos nosso modelo final! Utilizaremos todos os dados para treinar. A métrica de avaliação será o OOB."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final = pre_process(df_raw)\n\nX, y = df_final.drop('SalePrice', axis=1)[to_keep].drop(to_drop, axis=1), df_final['SalePrice']\n\nm = sklearn.ensemble.RandomForestRegressor(min_samples_leaf = 3, \n                                           max_features = 0.5, n_jobs=-1, \n                                           oob_score = True, random_state = 0)\n%time m.fit(X, y)\nm.oob_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O próximo passo natural seria aplicar o modelo final a um conjunto de teste em separado. Nosso exemplo, no entanto, veio de uma competição  já encerrada do Kaggle, então não é mais possível submeter o modelo e avaliar o desempenho no conjunto de teste da competição.\n\nMas lembre-se: em problemas da vida real, separe sempre um conjunto de teste para ser usado apenas na avaliação do modelo final!\n\nÉ isso! Até a próxima aula, galera!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}