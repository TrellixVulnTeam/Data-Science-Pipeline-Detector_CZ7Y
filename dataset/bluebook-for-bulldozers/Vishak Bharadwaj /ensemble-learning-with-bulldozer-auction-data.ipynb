{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bluebook for Bulldozers"},{"metadata":{},"cell_type":"markdown","source":"We will be looking at the Blue Book for Bulldozers Kaggle Competition: \"The goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuration. The data is sourced from auction result postings and includes information on usage and equipment configurations.\""},{"metadata":{},"cell_type":"markdown","source":"This dataset/competition has been chosen because of the closeness of the data to the realtime workplace.\n\nLink here : https://www.kaggle.com/c/bluebook-for-bulldozers"},{"metadata":{"trusted":true},"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom structured import *\nimport warnings\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn import metrics \n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! unzip ../input/bluebook-for-bulldozers/Train.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw = pd.read_csv('./Train.csv',low_memory=False,\n                    parse_dates=[\"saledate\"])\ndf_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.saledate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#since the kaggle competition evaluation metric is the RMSLE(Root mean square log error)\ndf_raw.SalePrice=np.log(df_raw.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initial processing"},{"metadata":{},"cell_type":"markdown","source":"you need to drop the target variable convert the categorical variables to numbers and then fit "},{"metadata":{},"cell_type":"markdown","source":"The dataset has both continous and categorical variables like the datetime thing etc which you can use.\nAnd you need a piece of feature engineering to get info out of this"},{"metadata":{},"cell_type":"markdown","source":"### 1. Dealing with dates"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.saledate #datatype is datetime","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The **add_datepart** method extracts particular date fields from a complete datetime for the purpose of constructing categoricals. You should always consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend/cyclical behavior as a function of time at any of these granularities."},{"metadata":{"trusted":true},"cell_type":"code","source":"add_datepart(df_raw, 'saledate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Convert strings to numbers for pandas"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The categorical variables are currently stored as strings, which is inefficient, and doesn't provide the numeric coding required for a random forest. Therefore we call **train_cats** to convert strings to pandas categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cats(df_raw) #converts most of these objects into categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.info() #how most objects have been turned to category","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note** :Category is a pandas datatype"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.UsageBand","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.UsageBand.cat.categories #gives you the categories for the usage band feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make things easier for the random forest, we rearrange the categories in the UsageBand feature to make more sense to split on"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'], ordered=True, inplace=True)\n#order it so the splitting gets the maximum benifit from it ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.UsageBand.cat.categories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normally, pandas will continue displaying the text categories, while treating them as numerical data internally. Optionally, we can replace the text categories with numbers, which will make this variable non-categorical, like so:."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.UsageBand.cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.head() #usage band still says high or low but behind the scenes they've been made into numbers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.to_feather(('bulldozers-raw'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw = pd.read_feather('bulldozers-raw')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" what proc_df does :\n proc_df takes a data frame df and splits off the response variable, and\n changes the df into an entirely numeric dataframe. For each column of df \n which is not in skip_flds nor in ignore_flds, na values are replaced by the\nmedian value of the column.\n\n 1. fix_missing - Fill missing data in a column of df with the median, and add a {name}_na column\n    which specifies if the data was missing.\n 2. scale_vars(if needed)\n 3. numericalize - Changes the column col from a categorical type to it's integer codes."},{"metadata":{"trusted":true},"cell_type":"code","source":"df, y, nas = proc_df(df_raw, 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_vals(a,n): \n    return a[:n].copy(), a[n:].copy()\n\nn_valid = 12000  # same as Kaggle's test set size\nn_trn = len(df)-n_valid\n# raw_train, raw_valid = split_vals(df_raw, n_trn)\nX_train, X_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n#let's track the metrics we're interested in \ndef rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor() \n%time m.fit(X_train, y_train)\nprint_score(m) #training rmse, valid rmse, training accuracy and validation accuracy respectively","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2 mins is too long and anything more than 10 secs will slow down the iteration process. so use proc_df to reduce the size of the training set. Proc_df has a subset attribute that handles it"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_raw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice', subset=50_000) \nX_train, _ = split_vals(df_trn, 40_000) \ny_train, _ = split_vals(y_trn, 40_000) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor()\n%time m.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use that subset which takes around 8-10 seconds to compute\n\nnote that as you increase the size of the subset, the accuracy of the model increases meaning you can try out the hyperparameters here, and then go back to the bigger dataset after you find the best ones"},{"metadata":{},"cell_type":"markdown","source":"Each tree is stored in estimators_ so run the validation set through each tree \nSo for every row you have 1 prediction per tree, so 12000 pedictions per tree and there are 10(will change to 100 as default) trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.stack([t.predict(X_valid) for t in m.estimators_])\npreds[:,0], np.mean(preds[:,0]), y_valid[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=20, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m) #prev was 81","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=50, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=75, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=100, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=125, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=160, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=170, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Score decreases at 170. So let's revert to 160"},{"metadata":{},"cell_type":"markdown","source":"### Out-of-bag (OOB) score"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=160, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m) #final output is oob error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows that our validation set time difference is making an impact, as is model over-fitting."},{"metadata":{},"cell_type":"markdown","source":"## Reducing over-fitting"},{"metadata":{},"cell_type":"markdown","source":"It turns out that one of the easiest ways to avoid over-fitting is also one of the best ways to speed up analysis: subsampling. Let's return to using our full dataset, so that we can demonstrate the impact of this technique."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')\nX_train, X_valid = split_vals(df_trn, n_trn)\ny_train, y_valid = split_vals(y_trn, n_trn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The basic idea is this: rather than limit the total amount of data that our model can access, let's instead limit it to a different random subset per tree. That way, given enough trees, the model can still see all the data, but for each individual tree it'll be just as fast as if we had cut down our dataset as before."},{"metadata":{},"cell_type":"markdown","source":"This requires using the set_rf_samples method, that changes sklearn source code \nTo see the its implementation check \nhttps://github.com/VishakBharadwaj94/bluebook_for_bulldozers/blob/master/bluebook_for_bulldozers.ipynb"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=150, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the model now at near 90% accuracy, we can dig deep in for further insights\n\nFor model Interpretation. Have a look at :\n\nhttps://github.com/VishakBharadwaj94/bluebook_for_bulldozers/blob/master/rf_interp.ipynb"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}