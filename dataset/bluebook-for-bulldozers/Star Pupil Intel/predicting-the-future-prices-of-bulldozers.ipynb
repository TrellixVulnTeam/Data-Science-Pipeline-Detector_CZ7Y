{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Predicting the Sale Price of Bulldozers\n\n## 1. Problem definition\n    \n    How well can we predict the future sales of a bulldozer given it's characteristics and previous examples of how much similar bulldozers have been sold for?\n    \n    \n## 2. Data\n\n    The data is taken from the Kaggle Bluebook for Bulldozers competition: https://www.kaggle.com/c/bluebook-for-bulldozers/data\n    \n    There are three main datasets:\n    \n    * Train.csv is the training set, which contains data through the end of 2011.\n    * Valid.csv is the validation set, which contains data from January 1, 2012-April 30, 2012\n    * Test.csv is the test set, which contains data from May, 2012-November, 2012.\n    \n## 3. Evaluation\n\n    The evaluation metric for this competition in the RMSLE(root mean squared log error) between the True labels and predicted labels.\n    \n## 4. Features\n\n    Kaggle provides a data dictionary detailing all the features of the dataset. It can be viewed here: https://docs.google.com/spreadsheets/d/1zRkHaM6oMOd-Fdo7hqhNkLEt4JJXJP1c_tugelQ1nXY/edit#gid=1461612573"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import tools for EDA\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the training and validation datasets\ndf = pd.read_csv('../input/bluebook-for-bulldozers/TrainAndValid.csv',\n                 low_memory=False)\ndf.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check info on the data\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for null values\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the SalePrice frequency ditribution\nsns.distplot(df.SalePrice, bins=100);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Parsing dates\n\nWhen we work with time series data, we want to enrich the time and date component as much as possible.\n\nWe can do that by telling Pandas which of our columns contain dates using the `parse-dates` parameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the data agin but this time parse dates\ndf = pd.read_csv('../input/bluebook-for-bulldozers/TrainAndValid.csv',\n                 low_memory=False,\n                 parse_dates=['saledate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.saledate.dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.saledate[:1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sort the DataFrame by date\ndf.sort_values(by=['saledate'],\n               inplace=True,\n               ascending=True)\ndf.saledate.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make a copy of the original DataFrame\n\nWe make a copy of the original DataFrame so when we manipulate the copy we still have our original data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a copy of df to make edits on\ndf_tmp = df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a function to preprocess the data into a format we can train on, evaluate and make predictions\n\ndef preprocess_data(df):\n    '''\n    Performs transformations of the df and returns transformed df.\n    '''\n    # Add datetime parameters to df and drop saledate column\n    df['saleyear'] = df.saledate.dt.year\n    df['salemonth'] = df.saledate.dt.month\n    df['saleday'] = df.saledate.dt.day\n    df['saledayofweek'] = df.saledate.dt.dayofweek\n    df['saledayofyear'] = df.saledate.dt.dayofyear\n    \n    df.drop('saledate',\n            axis=1,\n            inplace=True)\n    # Turn all string values into categorical values\n    for label, content in df.items():\n        if pd.api.types.is_string_dtype(content):\n            df[label] = content.astype('category').cat.as_ordered()\n    \n    # Fill missing numeric values\n    for label, content in df.items():\n        if pd.api.types.is_numeric_dtype(content):\n            if pd.isnull(content).sum():\n                # Add a binary column which declares the data missing or not\n                df[label+'_is_missing'] = pd.isnull(content)\n                # Fill missing numeric values with median\n                df[label] = content.fillna(content.median())\n                \n        # Fill missing categorical data and converted categories to numbers\n        if not pd.api.types.is_numeric_dtype(content):\n            # Add binary column to indicate whether sample had missing values\n            df[label+'_is_missing'] = pd.isnull(content)\n            # Turn categories into numbers\n            df[label] = pd.Categorical(content).codes + 1\n            \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp = preprocess_data(df)\ndf_tmp.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the Data\n\nNow we've preprocessed the data, we can split into training and validation sets\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data into train and validation sets\ndf_train = df_tmp[df_tmp.saleyear != 2012]\ndf_val = df_tmp[df_tmp.saleyear == 2012]\n\nlen(df_train), len(df_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data into X & y\nX_train, y_train = df_train.drop('SalePrice', axis=1), df_train.SalePrice\nX_val, y_val = df_val.drop('SalePrice', axis=1), df_val.SalePrice\n\nX_train.shape, y_train.shape, X_val.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation\n\nThe evaluation metric for this Kaggle comp is RootMeanSquaredLogError.\nThere is no RMSLE function in the Sklearn library so we will create one.\n\nWe will also create a function to print the scores of the metrics we will use.\nIn the interest of being thorough, we will also evaluate the model with the following metrics:\n\n* Mean Absolute Error\n* R^2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import metrics for evaluation\nfrom sklearn.metrics import mean_squared_log_error, mean_absolute_error, r2_score\n\n# Create an evaluation function to calculate the RMSLE\ndef rmsle(y_test, y_preds):\n    '''\n    Calculates root mean squared log error between predictions and true labels.\n    '''\n    return np.sqrt(mean_squared_log_error(y_test, y_preds))\n\n# Create another function to show scores of given metrics\ndef show_scores(model):\n    train_preds = model.predict(X_train)\n    val_preds = model.predict(X_val)\n    scores = {'Training MAE': mean_absolute_error(y_train, train_preds),\n              'Validation MAE': mean_absolute_error(y_val, val_preds),\n              'Training RMSLE': rmsle(y_train, train_preds),\n              'Validation RMSLE': rmsle(y_val, val_preds),\n              'Training R^2': r2_score(y_train, train_preds),\n              'Validation R^2': r2_score(y_val, val_preds)\n             }\n    \n    return scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing the model on a subset(for tuning hyperparams)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Import a RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instantiate a RandomForestRegressor with the max_samples set to 20,000\nmodel = RandomForestRegressor(n_jobs=-1,\n                              random_state=42,\n                              max_samples=20000)\n\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_scores(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter tuning with RandomizedSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Import the RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Create a dict grid of various hyperparams to try\nrf_grid = {'n_estimators': np.arange(10, 100, 10),\n           'max_depth': [None, 3, 5, 10],\n           'min_samples_split': np.arange(2, 20, 2),\n           'min_samples_leaf': np.arange(1, 20, 2),\n           'max_features': [0.5, 1, 'sqrt', 'auto'],\n           'max_samples': [15000]\n          }\n\n# Instantiate the RandomSearchCV model\nrs_model = RandomizedSearchCV(RandomForestRegressor(random_state=42),\n                              param_distributions=rf_grid,\n                              n_iter=5,\n                              cv=5,\n                              verbose=True,\n                              n_jobs=-1)\n\n# Fit the RSCV model\nrs_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the best params\nrs_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the scores\nshow_scores(rs_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Train a model on the whole set using the best_params\nideal_model = RandomForestRegressor(n_estimators=80,\n                                    min_samples_leaf=3,\n                                    min_samples_split=14,\n                                    max_features=0.5,\n                                    n_jobs=-1,\n                                    max_samples=None,\n                                    max_depth=None,\n                                    random_state=42)\nideal_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scores for rs_model, trained on a subset of the data with 15,000 samples\nshow_scores(rs_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scores for ideal_model, trained on all the data\nshow_scores(ideal_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the original competition is over and as this is merely a training note book, we will not further tune the model.\n\n\nLet's make some predictions!"},{"metadata":{},"cell_type":"markdown","source":"# Make predictions on the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the test data\ndf_test = pd.read_csv('../input/bluebook-for-bulldozers/Test.csv',\n                      low_memory=False,\n                      parse_dates=['saledate'])\n\ndf_test.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.sort_values(by=['saledate'], inplace=True, ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.saledate.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocess the test data using the preprocess_data function we created earlier\npreprocess_data(df_test)\ndf_test.head(), df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the difference in number of columns between the training and test data\nset(X_train.columns) - set(df_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Manually add a column for 'auctioneerID_is_missing'\ndf_test['auctioneerID_is_missing'] = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on the test data\ntest_preds =ideal_model.predict(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Format and export prediction data in the format requested by Kaggle\n# df_preds = pd.DataFrame()\n# df_preds['SalesID'] = df_test.SalesID\n# df_preds['SalePrice'] = test_preds\n\n# df_preds.to_csv('../input/bluebook-for-bulldozers/test-predictions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the feture importance of our best model\nideal_model.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualise that a bit better\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a function to plot the feature importance\n\ndef plot_features(columns, importances, n=20):\n    df = (pd.DataFrame({'features': columns,\n                        'feature_importances': importances})\n           .sort_values('feature_importances', ascending=False)\n           .reset_index(drop=True))\n    # Plot the df\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.barh(df['features'][:n], df['feature_importances'][:20])\n    ax.set_ylabel('Features')\n    ax.set_xlabel('Feature Importance')\n    ax.invert_yaxis()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_features(X_train.columns, ideal_model.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}