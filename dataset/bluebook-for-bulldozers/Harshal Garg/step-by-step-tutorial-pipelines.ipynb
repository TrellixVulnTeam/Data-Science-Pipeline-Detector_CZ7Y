{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-10T18:45:17.180025Z","iopub.execute_input":"2021-06-10T18:45:17.181064Z","iopub.status.idle":"2021-06-10T18:45:17.195878Z","shell.execute_reply.started":"2021-06-10T18:45:17.180845Z","shell.execute_reply":"2021-06-10T18:45:17.194285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## For a more detailed notebook. Check version 3","metadata":{}},{"cell_type":"markdown","source":"## 1. Problem Definition\n\nFor this dataset, the problem we're trying to solve, or better, the question we're trying to answer is,\n\n> How well can we predict the future sale price of a bulldozer, given its characteristics previous examples of how much similar bulldozers have been sold for?\n\n## 2. Data\n\nIn this case, it's historical sales data of bulldozers. Including things like, model type, size, sale date and more.\n\nThere are 3 datasets:\n1. **Train.csv** - Historical bulldozer sales examples up to 2011 (close to 400,000 examples with 50+ different attributes, including `SalePrice` which is the **target variable**).\n2. **Valid.csv** - Historical bulldozer sales examples from January 1 2012 to April 30 2012 (close to 12,000 examples but missing the `SalePrice` attribute, as this is what we'll use to tune HyperParameters).\n3. **Test.csv** - Historical bulldozer sales examples from May 1 2012 to November 2012 (close to 12,000 examples but missing the `SalePrice` attribute, as this is what we'll be trying to predict).\n\n## 3. Evaluation\n\nFor this problem, [Kaggle has set the evaluation metric to being root mean squared log error (RMSLE)](https://www.kaggle.com/c/bluebook-for-bulldozers/overview/evaluation). As with many regression evaluations, the goal will be to get this value as low as possible.\n\n[Root Mean Squre vs Root Mean Squared Log Error](https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a)\n\nTo see how well our model is doing, we'll calculate the RMSLE and then compare our results to others on the [Kaggle leaderboard](https://www.kaggle.com/c/bluebook-for-bulldozers/leaderboard).\n\n## 4. Features\n\nFeatures are different parts of the data. During this step, you'll want to start finding out what you can about the data.\n\nOne of the most common ways to do this, is to create a **data dictionary**.\n\nFor this dataset, Kaggle provide a data dictionary which contains information about what each attribute of the dataset means.\n\nWith all of this being known, let's get started! \n\nFirst, we'll import the dataset and start exploring. Since we know the evaluation metric we're trying to minimise, our first goal will be building a baseline model and seeing how it stacks up against the competition.","metadata":{}},{"cell_type":"code","source":"train_and_valid = pd.read_csv(\"../input/bluebook-for-bulldozers/TrainAndValid.csv\",\n                             low_memory = False, #to avoid the error shown above\n                             parse_dates = [\"saledate\"]) #to convert into date time, (see column index - 9)\ntrain_and_valid.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:45:17.232834Z","iopub.execute_input":"2021-06-10T18:45:17.233433Z","iopub.status.idle":"2021-06-10T18:45:24.361838Z","shell.execute_reply.started":"2021-06-10T18:45:17.23338Z","shell.execute_reply":"2021-06-10T18:45:24.360704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_and_valid.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:45:24.363258Z","iopub.execute_input":"2021-06-10T18:45:24.36351Z","iopub.status.idle":"2021-06-10T18:45:24.398585Z","shell.execute_reply.started":"2021-06-10T18:45:24.363482Z","shell.execute_reply":"2021-06-10T18:45:24.397726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_and_valid.head().T","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:45:24.400273Z","iopub.execute_input":"2021-06-10T18:45:24.400547Z","iopub.status.idle":"2021-06-10T18:45:24.422858Z","shell.execute_reply.started":"2021-06-10T18:45:24.400512Z","shell.execute_reply":"2021-06-10T18:45:24.421753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Data Visualization","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.scatter(train_and_valid[\"saledate\"][:1000], train_and_valid[\"SalePrice\"][:1000]);","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:45:24.424849Z","iopub.execute_input":"2021-06-10T18:45:24.425433Z","iopub.status.idle":"2021-06-10T18:45:24.648375Z","shell.execute_reply.started":"2021-06-10T18:45:24.425385Z","shell.execute_reply":"2021-06-10T18:45:24.64705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_and_valid.SalePrice.plot.hist();","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:45:24.649752Z","iopub.execute_input":"2021-06-10T18:45:24.650105Z","iopub.status.idle":"2021-06-10T18:45:24.890636Z","shell.execute_reply.started":"2021-06-10T18:45:24.650068Z","shell.execute_reply":"2021-06-10T18:45:24.889392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Process the data\n\nThis won't work since we've got missing numbers and categories\n```python\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_jobs=-1)\nmodel.fit(df_tmp.drop(\"SalePrice\", axis=1), df_tmp.SalePrice)\n```\nWe still have data types which are not compatible for the model\n\n---\n### Add datetime parameters for saledate column\n\nSo we can enrich our dataset with as much information as possible.\n\nBecause we imported the data using `read_csv()` and we asked pandas to parse the dates using `parase_dates=[\"saledate\"]`, we can now access the [different datetime attributes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html) of the `saledate` column.\n\n### Convert strings to categories\n\nOne way to help turn all of our data into numbers is to convert the columns with the string datatype into a category datatype.\n\nTo do this we can use the [pandas types API](https://pandas.pydata.org/pandas-docs/stable/reference/general_utility_functions.html#data-types-related-functionality) which allows us to interact and manipulate the types of data.\n\n### Filling numerical values\nWe're going to fill any column with missing values with the median of that column.\nAfter filling the numeric values, we'll do the same with the categorical values at the same time as turning them into numbers.","metadata":{}},{"cell_type":"code","source":"# Import Pipeline from sklearn's pipeline module\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\ndef preprocess_data(df):\n    # Add datetime parameters for saledate\n#     global saleYear, saleMonth, saleDay\n    df['saleYear'] = df['saledate'].dt.year\n    df['saleMonth'] = df['saledate'].dt.month\n    df['saleDay'] = df['saledate'].dt.day\n\n    # Drop original saledate\n    df.drop(\"saledate\", axis=1, inplace=True)\n#     df.drop(\"SalesID\", axis=1, inplace=True) # this doesn't follow a pattern\n    \n    \n    # Fill numeric rows with the median\n    for label, content in df.items():\n        temp = label+\"_is_missing\"\n        if pd.api.types.is_numeric_dtype(content):\n#         if pd.isnull(content).sum():\n            df[temp] = pd.isnull(content)\n            df[label] = content.fillna(content.median())\n                \n        # Turn categorical variables into numbers\n        if not pd.api.types.is_numeric_dtype(content):\n            df[label+\"_is_missing\"] = pd.isnull(content)\n            # We add the +1 because pandas encodes missing categories as -1\n            df[label] = pd.Categorical(content).codes+1        \n    \n    return df\n\ndata_process = FunctionTransformer(preprocess_data, validate = False)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:45:24.892468Z","iopub.execute_input":"2021-06-10T18:45:24.892913Z","iopub.status.idle":"2021-06-10T18:45:25.910866Z","shell.execute_reply.started":"2021-06-10T18:45:24.892849Z","shell.execute_reply":"2021-06-10T18:45:25.909678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into train/valid sets\n\nWe need had to preprocess the data so we didn't took \"train.csv\" and \"valid.csv\" separately. We will split our processed data. The validation set and test set are split according to dates.  \nThis makes sense since we're working on a time series problem.    \nE.g. using past events to try and predict future events.  \n\nKnowing this, randomly splitting our data into train and test sets using something like `train_test_split()` wouldn't work.  \nInstead, we split our data into training, validation and test sets using the date each sample occured.  \nIn our case:\n* Training = all samples up until 2011\n* Valid = all samples form January 1, 2012 - April 30, 2012\n* Test = all samples from May 1, 2012 - November 2012\n\nFor more on making good training, validation and test sets, check out the post [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/) by Rachel Thomas.","metadata":{}},{"cell_type":"code","source":"# split the data into training and validation\ndf_valid = train_and_valid[train_and_valid['saledate'].dt.year == 2012] \ndf_train = train_and_valid[train_and_valid['saledate'].dt.year != 2012]\n\ndf_valid.shape, df_train.shape\n\n# split the data in X and y\n\nX_train = df_train.drop(\"SalePrice\", axis = 1)\ny_train = df_train[\"SalePrice\"]\n\nX_valid = df_valid.drop(\"SalePrice\", axis = 1)\ny_valid = df_valid[\"SalePrice\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:45:25.912091Z","iopub.execute_input":"2021-06-10T18:45:25.912384Z","iopub.status.idle":"2021-06-10T18:45:26.439218Z","shell.execute_reply.started":"2021-06-10T18:45:25.912355Z","shell.execute_reply":"2021-06-10T18:45:26.438204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Modelling\n\nWe've explored our dataset a little as well as enriched it with some datetime attributes, now let's try to model.\n\nFollowing the [Scikit-Learn machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html), we find a [RandomForestRegressor()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn-ensemble-randomforestregressor) might be a good candidate.\n\n### Hyperparameter Tunning\n\nBefore training the model on full 400k records, we will train it on 10k records and find the best model by hyperparameter tunning. Once we are able to find the best sets of parameter, we will train the model with full dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrf_grid = {\"n_estimators\" : np.arange(10, 100, 10), \n          \"max_depth\": [None, 3, 5, 10],\n          \"min_samples_split\" : np.arange(2, 20, 2),\n          \"min_samples_leaf\" : np.arange(1, 20, 2),\n          \"max_features\" : [0.5, 1, \"sqrt\", \"auto\"],\n          \"max_samples\" : [10000]}\n\nmodel = RandomizedSearchCV(RandomForestRegressor(),\n                              param_distributions = rf_grid,\n                              n_iter = 20,\n                              n_jobs = -1,\n                              cv = 5, \n                              verbose = True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:45:26.443474Z","iopub.execute_input":"2021-06-10T18:45:26.443829Z","iopub.status.idle":"2021-06-10T18:45:26.787649Z","shell.execute_reply.started":"2021-06-10T18:45:26.443792Z","shell.execute_reply":"2021-06-10T18:45:26.786643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating the pipeline","metadata":{}},{"cell_type":"code","source":"model_pipeline = Pipeline(steps = [\n    (\"preprocess\", data_process),\n    ('model_rfr', model)\n])","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:45:26.789451Z","iopub.execute_input":"2021-06-10T18:45:26.789774Z","iopub.status.idle":"2021-06-10T18:45:26.795272Z","shell.execute_reply.started":"2021-06-10T18:45:26.789742Z","shell.execute_reply":"2021-06-10T18:45:26.794074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fitting the pipeline","metadata":{}},{"cell_type":"code","source":"model_pipeline.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:45:26.796469Z","iopub.execute_input":"2021-06-10T18:45:26.796746Z","iopub.status.idle":"2021-06-10T18:50:20.474478Z","shell.execute_reply.started":"2021-06-10T18:45:26.796717Z","shell.execute_reply":"2021-06-10T18:50:20.473173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds = model_pipeline.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:50:20.476195Z","iopub.execute_input":"2021-06-10T18:50:20.476531Z","iopub.status.idle":"2021-06-10T18:50:20.866392Z","shell.execute_reply.started":"2021-06-10T18:50:20.476499Z","shell.execute_reply":"2021-06-10T18:50:20.865319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building an evaluation function\n\nIt's important to understand the evaluation metric you're going for.  \n**RMSLE** = generally you don't care as much if you're off by $10 as much as you'd care if you were off by 10%, you care more about ratios rather than differences. **MAE** (mean absolute error) is more about exact differences.\n\nSince Scikit-Learn doesn't have a function built-in for RMSLE, we'll create our own. We can do this by taking the square root of Scikit-Learn's [mean_squared_log_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error) (MSLE). MSLE is the same as taking the log of mean squared error (MSE).  \nWe'll also calculate the MAE and R^2 for our evaluation","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_log_error, r2_score\n\ndef rmsle(y_test, y_preds):\n    return np.sqrt(mean_squared_log_error(y_test, y_preds))\n\n#create a function to get other values\ndef show_scores(y_test, y_preds):\n    scores = {\"MAE\" : mean_absolute_error(y_test, y_preds),\n              \"RMSLE\" : rmsle(y_test, y_preds),\n              \"R^2\" : r2_score(y_test, y_preds)} #default parameter \n    return scores","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:50:20.867633Z","iopub.execute_input":"2021-06-10T18:50:20.86796Z","iopub.status.idle":"2021-06-10T18:50:20.874279Z","shell.execute_reply.started":"2021-06-10T18:50:20.86793Z","shell.execute_reply":"2021-06-10T18:50:20.872912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_scores(y_valid, y_preds)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:50:20.876257Z","iopub.execute_input":"2021-06-10T18:50:20.87671Z","iopub.status.idle":"2021-06-10T18:50:20.896349Z","shell.execute_reply.started":"2021-06-10T18:50:20.87667Z","shell.execute_reply":"2021-06-10T18:50:20.894347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pipeline[1]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:50:20.898355Z","iopub.execute_input":"2021-06-10T18:50:20.898773Z","iopub.status.idle":"2021-06-10T18:50:20.912557Z","shell.execute_reply.started":"2021-06-10T18:50:20.898731Z","shell.execute_reply":"2021-06-10T18:50:20.911623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Creating test predictions\n\nAlthough we are creating a submission file. The competiton is closed and we have to consider the valid scores as evaluation parameter only.","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/bluebook-for-bulldozers/Test.csv\",\n                             low_memory = False, #to avoid the error shown above\n                             parse_dates = [\"saledate\"]) #to convert into date time, (see column index - 9)\ntest.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:50:20.913968Z","iopub.execute_input":"2021-06-10T18:50:20.914596Z","iopub.status.idle":"2021-06-10T18:50:21.185445Z","shell.execute_reply.started":"2021-06-10T18:50:20.914554Z","shell.execute_reply":"2021-06-10T18:50:21.184134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds_test = model_pipeline.predict(test)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:50:21.187885Z","iopub.execute_input":"2021-06-10T18:50:21.188292Z","iopub.status.idle":"2021-06-10T18:50:21.618226Z","shell.execute_reply.started":"2021-06-10T18:50:21.188241Z","shell.execute_reply":"2021-06-10T18:50:21.616653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_submission = pd.DataFrame(data = [test[\"SalesID\"].astype(int), \n                                y_preds_test], \n                        index = [\"SalesID\", \"SalesPrice\"])\ntest_submission = test_submission.T\ntest_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:50:21.620481Z","iopub.execute_input":"2021-06-10T18:50:21.620911Z","iopub.status.idle":"2021-06-10T18:50:21.947322Z","shell.execute_reply.started":"2021-06-10T18:50:21.620858Z","shell.execute_reply":"2021-06-10T18:50:21.946198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_submission.to_csv(\"./predictions.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:50:21.948949Z","iopub.execute_input":"2021-06-10T18:50:21.949248Z","iopub.status.idle":"2021-06-10T18:50:21.995672Z","shell.execute_reply.started":"2021-06-10T18:50:21.949218Z","shell.execute_reply":"2021-06-10T18:50:21.994389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance","metadata":{}},{"cell_type":"code","source":"feature_importance= pd.DataFrame(data = [model.best_estimator_.feature_importances_],\n                                 columns = test.columns,\n                                 index = [\"features_importance\"]).T.sort_values(\"features_importance\",\n                                               ascending = False)\n    \nfeature_importance","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:50:21.997124Z","iopub.execute_input":"2021-06-10T18:50:21.99768Z","iopub.status.idle":"2021-06-10T18:50:22.039334Z","shell.execute_reply.started":"2021-06-10T18:50:21.997622Z","shell.execute_reply":"2021-06-10T18:50:22.038212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns \n\ndef plot_features(df, n = 20): # plot top 20 (be default) features\n    \n    sns.barplot(x = \"features_importance\",\n                y = df.index[:n],\n               data = df[:n],\n               orient = 'h')\nplot_features(feature_importance)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:50:22.040666Z","iopub.execute_input":"2021-06-10T18:50:22.041023Z","iopub.status.idle":"2021-06-10T18:50:22.425302Z","shell.execute_reply.started":"2021-06-10T18:50:22.040989Z","shell.execute_reply":"2021-06-10T18:50:22.423883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}