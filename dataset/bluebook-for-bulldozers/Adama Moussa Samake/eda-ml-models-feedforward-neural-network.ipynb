{"cells":[{"metadata":{"id":"WNprQo0hXk_F","outputId":"33034e07-cc83-4936-9121-15a0f087988e","trusted":true},"cell_type":"code","source":"import os\nfrom zipfile import ZipFile\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport missingno as msm\nimport pandas as pd\nfrom scipy import stats\n\nfrom sklearn.feature_selection import SelectKBest, f_regression, SelectFromModel, mutual_info_regression\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nfrom xgboost import XGBRegressor\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# All the remarks and comment are welcome. Thank you....","execution_count":null},{"metadata":{"id":"StHlMfOVbdE_"},"cell_type":"markdown","source":"*`Problem statement`*: Predicting the sale price of bulldozers sold at auctions.\n\n*`Data`*: the data for this competition is split into three parts, which are trainset validationset and testset.\n\n*`The key fields are in train.csv are`*:\n- `SalesID`: the unique identifier of the sale\n- `MachineID`: the unique identifier of a machine.  A machine can be sold multiple times\n- `saleprice`: what the machine sold for at auction (only provided in train.csv)\n- `saledate`: the date of the sale.\nIt is important to underline that the ","execution_count":null},{"metadata":{"id":"6JI76oCagArh"},"cell_type":"markdown","source":"First of all let's create a function to unzip the trainset.","execution_count":null},{"metadata":{"id":"mbYmA3p_i7tV","outputId":"1c7cc3c9-9b6b-49c0-8e15-b809df8ffcb9","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/trainset/Train.csv', low_memory=False, parse_dates=['saledate'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Yr6mgGA5jZaw","outputId":"ba2861ea-af94-4982-a820-a720c748702d","trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"BT2WTOq7jgF2"},"cell_type":"markdown","source":"#EDA","execution_count":null},{"metadata":{"id":"rS34qfF57JDc","trusted":true},"cell_type":"code","source":"df_eda = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"wNh8S_78j3G6"},"cell_type":"markdown","source":"1. Dataset shape (rows and columns): (401125, 53)\n2. Target attribute name: SalePrices\n3. Data types of features:  2 x float64, 6 x int64(6), 45 x object\n4. Understand the problem:  We'll look at each variable and do an analysis about their meaning and importance for this problem.\n5. Analysis of different features\n  - Missing values analysis\n  - Categorical feature\n  - Numerical feature (discret and continue feature)\n  - Univariable study: We'll focus on the dependent variable ('SalePrice') and try to know a little bit more about it.\n  - Multivariate study. We'll try to understand how the dependent variable and independent variables relate.\n8. Data cleaning: We'll clean the dataset and handle the missing data, outliers and categorical variables.","execution_count":null},{"metadata":{"id":"_-9aiUzV0ZCl"},"cell_type":"markdown","source":"After looking at each variable and try to understand their meaning and relevance to this problem ( we have selected based on our understanding about the meaning then we have ploted those feature versus SalePrice). The result has shown Drive_System, Engine_Horsepower can play an important role in this problem. However, none of the above mentioned key fields has strong relationship with salePrice.","execution_count":null},{"metadata":{"id":"zqA5jl6p6P4K","outputId":"0a90595b-aba1-41d4-99ab-7b337618734a","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13, 5))\ni = 0\nfor attribute in ('Engine_Horsepower', 'Drive_System'):\n  i += 1\n  plt.subplot(1, 2, i)\n  sns.boxplot(df_eda[attribute], df_eda['SalePrice'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"vGpGRmViONjw"},"cell_type":"markdown","source":"Engine Horsepower, Drive System seem to be related with SalePrice. The relationship seems to be based on  different category. The box plot shows how sales prices increase with the different category.\nIt is important to understand we just analysed four variables. However, there are many other that we should analyse. ","execution_count":null},{"metadata":{"id":"nbFjtOYbj87Z"},"cell_type":"markdown","source":"Univarible analysis:\n- Independant variable analysis","execution_count":null},{"metadata":{"id":"5La9Sx7YgeBL","outputId":"54d34397-d323-4084-b220-7b80ef687a60","trusted":true},"cell_type":"code","source":"# Missing value\nmsm.bar(df_eda, figsize=(24, 5), fontsize=16, labels=True, log=False )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"KUO36kIdkc5_","outputId":"80d376a6-8922-4c14-d931-f5b6d7875234","trusted":true},"cell_type":"code","source":"(df_eda.isnull().sum()/len(df_eda)).sort_values()","execution_count":null,"outputs":[]},{"metadata":{"id":"LmBS_vD_k-do"},"cell_type":"markdown","source":"The missing value varie between 0.08% to 94% depending on feature. \nBased on that information, it is important to underline that all the features that contain more than 90% can be dropped during data processing (One can argues that they cannot help the ML model to generalize its prediction).However, we will keep them.","execution_count":null},{"metadata":{"id":"Ua6VW6-cnfNl","outputId":"ca26f834-0673-4372-cd1c-094a61891550","trusted":true},"cell_type":"code","source":"# Let's identify the which feature contains more than 90% missing value\nheigh_nan_features = [feature for feature in df_eda.columns  if (df_eda[feature].isna().sum()/len(df_eda)) > 0.9]\nheigh_nan_features","execution_count":null,"outputs":[]},{"metadata":{"id":"NTSHkWYSpwwC","outputId":"c03496ef-f556-49f7-9f9c-731256d7ca72","trusted":true},"cell_type":"code","source":"# Categorical features\ncategorical_feature_list = [feature for feature in df_eda.columns if feature != 'saledate' and df_eda[feature].dtype == 'O']\n\nfor feature in categorical_feature_list:\n  print(f'{feature :-<50} {df_eda[feature].nunique()}')","execution_count":null,"outputs":[]},{"metadata":{"id":"DC-y-yBssJhV"},"cell_type":"markdown","source":"The categories per feature varie between 2 and 4999. Disaggregation of fiModelDesc have the highest number of categories","execution_count":null},{"metadata":{"id":"ac2PV4WP3W19","trusted":true},"cell_type":"code","source":"# Numerical categories analysis\nnumerical_feature_list = [feature for feature in df_eda.columns if feature not in ('SalePrice', 'saledate') and df_eda[feature].dtype != 'O']","execution_count":null,"outputs":[]},{"metadata":{"id":"L1nvPeIYG86L","outputId":"65343b89-58c7-45ea-9a14-3796a9173c82","trusted":true},"cell_type":"code","source":"discret_value_feature = [feature for feature in numerical_feature_list if len(df_eda[feature].unique()) < 25]\nsns.countplot(x='datasource', data=df_eda[['datasource']]);","execution_count":null,"outputs":[]},{"metadata":{"id":"T7CQafWPHAYJ","trusted":true},"cell_type":"code","source":"continuous_value_feature = [feature for feature in numerical_feature_list if df_eda[feature].nunique() > 25]\ndef plot_hist (nrow=1, ncol=1, feature_list=None, figsize=(24, 10)):\n  plt.figure(figsize=figsize)\n  i = 0\n  for feature in feature_list:\n    i += 1\n    plt.subplot(nrow, ncol, i)\n    sns.distplot(df_eda[feature], bins=50)\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"yRYVLq5UHGzJ","outputId":"6badd3a1-8bd8-4273-fd19-97e5483ddfaf","trusted":true},"cell_type":"code","source":"plot_hist(nrow=2, ncol=3, feature_list=continuous_value_feature)","execution_count":null,"outputs":[]},{"metadata":{"id":"ryXkmNZK3rQE"},"cell_type":"markdown","source":"All the independant features that contain continuouse value are skewed. Therefore,one can use log transformation to reshape the data distribution to normal one.","execution_count":null},{"metadata":{"id":"F_FKskDv2SZ8"},"cell_type":"markdown","source":"***Now let's analysis dependant variable (target feature)***","execution_count":null},{"metadata":{"id":"8g2-GBtq_UZP","outputId":"ab60bfa0-d0a2-47cd-9777-63cdf5261d84","trusted":true},"cell_type":"code","source":"#2 SalePrice\ndf_eda['SalePrice'].describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"4kU3sFlL_fZv","outputId":"4d15d7ef-a642-44fc-da45-f6bfc0e4c6a5","trusted":true},"cell_type":"code","source":"sns.distplot(df_eda['SalePrice'], bins=50);","execution_count":null,"outputs":[]},{"metadata":{"id":"uz95BCqOBlqB"},"cell_type":"markdown","source":" The SalePrice deviate from the normal distribution. It is positively skewed. Hence, this would mean that many  Bulldozers were being sold for less than the average value (31099.712848).","execution_count":null},{"metadata":{"id":"e-Na0V5x_ypn","outputId":"a307720f-e47a-412d-e6fe-61559d2359b1","trusted":true},"cell_type":"code","source":"df_eda['SalePrice'].skew(), df_eda['SalePrice'].kurt()","execution_count":null,"outputs":[]},{"metadata":{"id":"JhQO-Mr5AJND","outputId":"76c7862c-2749-4779-f19b-9641c1450bda","trusted":true},"cell_type":"code","source":"sns.boxplot(df_eda['SalePrice']);","execution_count":null,"outputs":[]},{"metadata":{"id":"SYLlkcKF-Ndv"},"cell_type":"markdown","source":"Boxplot has shown that salePrice variable contains some outliers.","execution_count":null},{"metadata":{"id":"0Vu0UIdb-wF0"},"cell_type":"markdown","source":"***Multivariate study*\nWe are going to investigate how target variable (SalePrice) and independent variables are related.***","execution_count":null},{"metadata":{"id":"orxUew7fDBi6","outputId":"df417de7-be9f-41af-b207-1db54a25ad89","trusted":true},"cell_type":"code","source":"# Descret value features\nsns.boxplot(x='datasource', y='SalePrice', data=df_eda);","execution_count":null,"outputs":[]},{"metadata":{"id":"mlOjR_iBASnI","trusted":true},"cell_type":"code","source":"# continuous value feature\ndef scatter_plot (nrow=1, ncol=1, feature_list=None, figsize=(24, 10), target='SalePrice'):\n  plt.figure(figsize=figsize)\n  i = 0\n  for feature in feature_list:\n    i += 1\n    plt.subplot(nrow, ncol, i)\n    sns.scatterplot(df_eda[feature], df_eda[target])\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"TldoqU3qDZsJ","outputId":"87f23c16-ad97-4b97-eae0-d2eebfde43f9","trusted":true},"cell_type":"code","source":"scatter_plot(nrow=2, ncol=3, feature_list=continuous_value_feature)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"uUAHytR7EtIJ"},"cell_type":"markdown","source":"It is quiet hard to quantify the relation between SalePrice and numerical feature values. However, we are going one more approach which allows us to determine the corelation coefficient between SalePrice and different independant feature.","execution_count":null},{"metadata":{"id":"nXj087DHGfXU","outputId":"2084b562-5fc1-401a-8bf7-d453dc3a93ae","trusted":true},"cell_type":"code","source":"corr = df_eda.corr()\nplt.figure(figsize=(7, 7))\nsns.heatmap(corr, cmap='YlGn_r', cbar=False, annot=True);","execution_count":null,"outputs":[]},{"metadata":{"id":"pEnvft1UG4Hh","outputId":"e1af92fc-b7bb-4e3e-9957-58adc0191aca","trusted":true},"cell_type":"code","source":"# Let's plot what we have seen in heatmap using pairplot\nsns.pairplot(df_eda[numerical_feature_list + ['SalePrice']], height = 2.5);","execution_count":null,"outputs":[]},{"metadata":{"id":"2t0K1vqqNJ5W"},"cell_type":"markdown","source":"Although we already know some of the figures, this scatter plot gives us a reasonable idea about relationships between variables.\nThe correlation between dependandt and independant features are very close to zero as one can see on heatmap plot. Also, the above mega scatter plot as alo shwon that.","execution_count":null},{"metadata":{"id":"ztZpL4o5QHk2"},"cell_type":"markdown","source":"***Outliers***\nAs already mentioned the feature SalePrice contains outliers, which can affect our models. But it can be at the same time a valuable source of information about specific behaviours. We will do a quick analysis through the standard deviation.\n\nWe will first establish a threshold that defines an observation as an outlier. To do so, we'll standardize the data. Moreover, data standardization means converting data values to have mean of 0 and a standard deviation of 1.","execution_count":null},{"metadata":{"id":"iNZIdB6iljnR","outputId":"e901f100-a555-4dad-8cf4-1c4f40469483","trusted":true},"cell_type":"code","source":"saleprice_scaled = StandardScaler().fit_transform(df_eda['SalePrice'].to_numpy().reshape(-1, 1));\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","execution_count":null,"outputs":[]},{"metadata":{"id":"FAeIzh32mxgN"},"cell_type":"markdown","source":"Low range values are similar and not too far from 0. However, high range values are far from 0.\nNote that for now, we'll not consider any of these values as an outlier but we should be pay more attention on those high rnge values.","execution_count":null},{"metadata":{"id":"FbYFTwcD6W7M"},"cell_type":"markdown","source":"***Statistical bases for multivariate analysis***\n-  We have already done some data analysis and discovered a lot about dependant variable('SalePrice'). Now it's time to go even deeper and understand how 'SalePrice' complies with the statistical assumptions that enables us to apply multivariate techniques.\n ","execution_count":null},{"metadata":{"id":"G9qS9bVd603c"},"cell_type":"markdown","source":"- Histogram - Kurtosis and skewness.\n- Normal probability plot - Data distribution should closely follow the diagonal that represents the normal distribution.","execution_count":null},{"metadata":{"id":"ugQewJHy71md","trusted":true},"cell_type":"code","source":"def statical_analysis (df, attribute_name, figsize=(15, 5)):\n\n  plt.figure(figsize=figsize)\n  i = 0\n  for item in (1, 2):\n    i +=1\n    plt.subplot(1, 2, i)\n    if i == 1: sns.distplot(df[attribute_name], bins=50, fit=stats.norm)\n    else:  stats.probplot(df[attribute_name], plot=plt)\n    \n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"5IyaMwmdEh6l","outputId":"03deba2a-6274-48a5-ec00-c17f0d626bd5","trusted":true},"cell_type":"code","source":"# For sale price \nstatical_analysis(df_eda, 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"id":"Bg8hjvcU-T54"},"cell_type":"markdown","source":"'SalePrice' is not normally distributed. It depicts 'peakedness', positive skewness and does not follow the diagonal line, see probability plot. Hence, we will need a log transformation to solve the problem. \nIt is import to underline that apart of SalePrice the majority of numerical variable in dataset are just Id. Therefore, will not apply log transformation to those attributes.","execution_count":null},{"metadata":{"id":"E8Rh3QjcFpcN","outputId":"d0c47074-e0f8-4409-d042-00a014254d1e","trusted":true},"cell_type":"code","source":"# let's apply log transform to SalePrice\ndf_log = df_eda.copy()\ndf_log['SalePrice'] = np.log1p(df_log['SalePrice'])\nstatical_analysis(df_log, 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"id":"6YHBHAA9Fwc_","outputId":"c754722f-aa9a-4fa7-f099-6baf4d499341","trusted":true},"cell_type":"code","source":"numerical_feature_list","execution_count":null,"outputs":[]},{"metadata":{"id":"84YGYCDXGRq5","outputId":"4d87ac97-e882-4db7-c0c4-7da112a386d9","trusted":true},"cell_type":"code","source":"statical_analysis(df_eda, 'MachineHoursCurrentMeter')","execution_count":null,"outputs":[]},{"metadata":{"id":"2iqccLNLHdsn","outputId":"7b8b9383-68b9-48cd-c5ee-0d72b5bacf45","trusted":true},"cell_type":"code","source":"#apply log transform\ndf_log['MachineHoursCurrentMeter'] = np.log1p(df_log['MachineHoursCurrentMeter'])\nstatical_analysis(df_log, 'MachineHoursCurrentMeter')","execution_count":null,"outputs":[]},{"metadata":{"id":"RikHN97gH07n"},"cell_type":"markdown","source":"Feature ingineering","execution_count":null},{"metadata":{"id":"d4E2YPACZaGo","trusted":true},"cell_type":"code","source":"df = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"KW-O88hOHiY4","trusted":true},"cell_type":"code","source":"# let's first work on sale date \ndef date_preprocessing (dataFrame, feature='saledate'):\n  dataFrame['saleYear'] = dataFrame[feature].dt.year\n  dataFrame['saleMonth'] = dataFrame[feature].dt.month\n  dataFrame['saleDay'] = dataFrame[feature].dt.day\n  dataFrame['saleDayOfWeek'] = dataFrame[feature].dt.dayofweek\n  dataFrame['saleDayOfYear'] = dataFrame[feature].dt.dayofyear\n  dataFrame.drop(feature, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"SGn9NkGFLOtr","trusted":true},"cell_type":"code","source":"date_preprocessing(df)","execution_count":null,"outputs":[]},{"metadata":{"id":"GBNLmSyFHoLR","trusted":true},"cell_type":"code","source":"def Ordinal_encoder (dataFrame, feature_list):\n  mask = {'Mini': 1, 'Small': 1, 'Medium': 2, 'Large / Medium': 3,  'Large': 3, 'Low': 1, 'High': 3}\n  for label in feature_list:\n    dataFrame[label] = dataFrame[label].map(mask)","execution_count":null,"outputs":[]},{"metadata":{"id":"keejThfnLbU_","trusted":true},"cell_type":"code","source":"Ordinal_encoder(df, ['ProductSize', 'UsageBand'])","execution_count":null,"outputs":[]},{"metadata":{"id":"LRv3voy3b0hR","trusted":true},"cell_type":"code","source":"# Now let's handle Missing value\nclass FillMissing ():\n\n  def fill_categorical (self,  dataFrame):\n    for label, content in  dataFrame.items():\n      if pd.api.types.is_string_dtype(content):\n         dataFrame[label].fillna('missing', inplace=True)\n    # return df\n\n  def fill_numerical (self,  dataFrame):\n    for label, content in  dataFrame.items():\n     if pd.api.types.is_numeric_dtype(content):\n        dataFrame[label] = content.fillna(content.median())\n    # return df","execution_count":null,"outputs":[]},{"metadata":{"id":"D1XOliqgHxHq","trusted":true},"cell_type":"code","source":"fill_missing_value = FillMissing()","execution_count":null,"outputs":[]},{"metadata":{"id":"lte9_bHpLy3J","trusted":true},"cell_type":"code","source":"fill_missing_value.fill_categorical(df)","execution_count":null,"outputs":[]},{"metadata":{"id":"-RB4iIXodNkP","trusted":true},"cell_type":"code","source":"fill_missing_value.fill_numerical(df)","execution_count":null,"outputs":[]},{"metadata":{"id":"t6PKOk97B9n_","trusted":true},"cell_type":"code","source":"def nominal_encoder (dataFrame, label_list):\n\n  for label, content in  dataFrame.items():\n    if pd.api.types.is_string_dtype(content):\n       dataFrame[label] = content.astype('category').cat.as_ordered()\n       dataFrame[label] = pd.Categorical(content).codes + 3","execution_count":null,"outputs":[]},{"metadata":{"id":"wJ16blP0MUWm","trusted":true},"cell_type":"code","source":"label_list = [feature for feature in df.columns if df[feature].dtype == 'O']\nnominal_encoder(df, label_list)","execution_count":null,"outputs":[]},{"metadata":{"id":"cFb89H0PMY_g","trusted":true},"cell_type":"code","source":"#drop IDs \ndf =df.drop(['SalesID', 'MachineID','ModelID','auctioneerID'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"gwm0YQiXzCMg"},"cell_type":"markdown","source":"# Feature selection\n- SelectKBest will be used along decision tree during feature selection process. It is important to underline that we have choose decision because it doesn't require a lot data preparation and it is also quiet fast.\n- Also we will split trainset. Hence, this will allow us to check if our model is not overfitting.","execution_count":null},{"metadata":{"id":"IES1UjNiKJ8B","trusted":true},"cell_type":"code","source":"# We will be using decision to simply findout which value of k that gives small MSE.\nfeature_selection = make_pipeline(SelectKBest(score_func=f_regression, k=52), DecisionTreeRegressor(max_depth=10, random_state=0))","execution_count":null,"outputs":[]},{"metadata":{"id":"4Mvlw8Qd9EI7","outputId":"428f9f5c-5159-4b76-9e5a-3a9d7bf934c2","trusted":true},"cell_type":"code","source":"# For the purpose will split the training set into train and test. It is important to bear in mind that the mentioned splited dataset will be use only for finding best K value\nX_1, y_1 = df.drop('SalePrice', axis=1), df['SalePrice']\nX_1, X_2, y_1, y_2 = train_test_split(X_1, y_1, test_size=.2)\nfeature_selection.fit(X_1, y_1)","execution_count":null,"outputs":[]},{"metadata":{"id":"UVRWW-nBz11g","trusted":true},"cell_type":"code","source":"y_1_pred, y_2_pred= feature_selection.predict(X_1),  feature_selection.predict(X_2)","execution_count":null,"outputs":[]},{"metadata":{"id":"_tBn0dkmz-VT","outputId":"00fd119d-b43a-49e2-87b5-3343488eab11","trusted":true},"cell_type":"code","source":"print(f'mse_1: {np.sqrt(mean_squared_error(y_1, y_1_pred))}, mse_2:{np.sqrt(mean_squared_error(y_2, y_2_pred))}')","execution_count":null,"outputs":[]},{"metadata":{"id":"jSN2_LJXOe7l"},"cell_type":"markdown","source":"We will keep all feature because they have provided the smallest MSE","execution_count":null},{"metadata":{"id":"l5_mXdZLALK3"},"cell_type":"markdown","source":"# Feature scaling","execution_count":null},{"metadata":{"id":"kp4A9FhZAJRe","trusted":true},"cell_type":"code","source":"def scale_feature (X):\n  scaled = StandardScaler().fit_transform(X)\n  return scaled","execution_count":null,"outputs":[]},{"metadata":{"id":"TiQe4gS8Kd4O","trusted":true},"cell_type":"code","source":"X_train, y_train = df.drop('SalePrice', axis=1), df['SalePrice']\nX_train = scale_feature(X_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"P7VkDBNFBAJS"},"cell_type":"markdown","source":"# Let's train our models (model experimentation)","execution_count":null},{"metadata":{"id":"ZE6KDUxJ5pjF","trusted":true},"cell_type":"code","source":"def model_training (models, X_train, y_train, cv=3):\n  for name, model in models.items():\n    print(name)\n    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_mean_squared_error')\n    print({'score': np.sqrt(-scores).mean(), 'std': np.sqrt(-scores).std() })","execution_count":null,"outputs":[]},{"metadata":{"id":"zIUAA4lh5tDU","outputId":"c610a4c6-7918-4a27-f0e2-85612967006d","trusted":true},"cell_type":"code","source":"models = {'KNeighborsRegressor': KNeighborsRegressor(),\n          'RandomForestRegressor': RandomForestRegressor(), \n          'ExtraTreesRegressor': ExtraTreesRegressor(),  \n          'AdaBoostRegressor': AdaBoostRegressor(), \n          'GradientBoostingRegressor':  GradientBoostingRegressor(), \n          'XGBRegressor':  XGBRegressor()\n          }\n\nmodel_training(models, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyper parameters tuning\nWe will proceed hyperparameters tune for ExtraTreesRegressor since it provides the smallest MSE value","execution_count":null},{"metadata":{"id":"Sv6zRvzJO3fA","trusted":true},"cell_type":"code","source":"def hyperparameters_tuning (model, X_train, y_train, param_grid, cv=5):\n  grid = RandomizedSearchCV(model, param_grid, n_iter=10, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42, verbose=2, cv=cv)\n  grid.fit(X_train, y_train)\n  \n  return grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"id":"INerzZjle8jG","outputId":"ff15ee4f-1414-4126-95ec-3bb095f56e62","trusted":true},"cell_type":"code","source":"extra_forest_param_grid = {'n_estimators':[150, 300, 600], \n                          'min_samples_leaf':[2, 3],  \n                          'min_samples_split':[14, 12], \n                          'max_features':[0.7, 1],\n                          'max_samples':[10000], # 10000 has been used due to the limited available memory of notebook (in Kaggle).\n                          'bootstrap':[None, True]\n              }\nextra_forest = hyperparameters_tuning(ExtraTreesRegressor(), X_train, y_train, extra_forest_param_grid)\nextra_forest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation of best estimator (model)","execution_count":null},{"metadata":{"id":"bdWRQ7D-ojDW","trusted":true},"cell_type":"code","source":"# let's prepare validation data \nX_valid = pd.read_csv('../input/bluebook-for-bulldozers/Valid.csv', parse_dates=['saledate'])\ny_valid = pd.read_csv('../input/bluebook-for-bulldozers/ValidSolution.csv')\ny_valid = y_valid['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"id":"GXJX04HkjvMP","trusted":true},"cell_type":"code","source":"#drop IDs \nX_valid =X_valid.drop(['SalesID', 'MachineID','ModelID','auctioneerID'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"HEIeoSIas2UZ","trusted":true},"cell_type":"code","source":"date_preprocessing(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"id":"R4ABPUr0p7N4","trusted":true},"cell_type":"code","source":"Ordinal_encoder(X_valid, ['ProductSize', 'UsageBand'])","execution_count":null,"outputs":[]},{"metadata":{"id":"bIZJUxkhqwey","trusted":true},"cell_type":"code","source":"fill_missing_value.fill_categorical(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"id":"IvHYZK5Mq2Xb","trusted":true},"cell_type":"code","source":"fill_missing_value.fill_numerical(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"id":"2S3yjTE8gHNS","trusted":true},"cell_type":"code","source":"nominal_encoder(X_valid, label_list)","execution_count":null,"outputs":[]},{"metadata":{"id":"CjVHHyPfg1IX","trusted":true},"cell_type":"code","source":"X_valid = scale_feature(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"id":"Us0OlYzL7hx-","trusted":true},"cell_type":"code","source":"def evaluate_best_model (X_train, y_train, X_valid, y_valid):\n\n    estimator = ExtraTreesRegressor(bootstrap=None, ccp_alpha=0.0, criterion='mse',\n                    max_depth=None, max_features=0.7, max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    min_impurity_split=None, min_samples_leaf=2,\n                    min_samples_split=12, min_weight_fraction_leaf=0.0,\n                    n_estimators=300, n_jobs=None, oob_score=False,\n                    random_state=None, verbose=0, warm_start=False)\n    \n    estimator.fit(X_train, y_train)\n    y_train_pred = estimator.predict(X_train)\n    y_valid_pred = estimator.predict(X_valid)\n\n    return {\n          'MSE_train': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n          'MSLE_train':np.sqrt( mean_squared_log_error(y_train, y_train_pred)),\n          'MSE_valid': np.sqrt(mean_squared_error(y_valid, y_valid_pred)),\n          'MSLE_valid': np.sqrt(mean_squared_log_error(y_valid, y_valid_pred))\n      }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_best_model(X_train, y_train, X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best estimator seems to overfit our training data. One can perform more hyper parameters tuning in order to solve the overfitting issue (or intance set max_depth to a value or check more possible value for min_samples_leaf, n_estimators value etc ... ). However, I will leave as it is for this moment.\nNext, I will build feedforward neural netwok and train and evaluate. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# FeedForward Neural Network","execution_count":null},{"metadata":{"id":"4SvY_4AghARh","trusted":true},"cell_type":"code","source":"FNN = keras.models.Sequential([\n                               Flatten(input_shape=[52]),\n                               Dense(350, activation='relu', kernel_initializer='lecun_normal'),\n                               keras.layers.Dropout(.3),\n                               Dense(250, activation='relu', kernel_initializer='lecun_normal'),\n                               keras.layers.Dropout(.3),\n                               Dense(150, activation='relu', kernel_initializer='lecun_normal'),\n                               keras.layers.Dropout(.4),\n                               Dense(100, activation='relu', kernel_initializer='lecun_normal'),\n                               keras.layers.Dropout(.5),\n                               Dense(70, activation='relu', kernel_initializer='lecun_normal'),\n                               keras.layers.Dropout(.5),\n                               Dense(1)\n])","execution_count":null,"outputs":[]},{"metadata":{"id":"Rejr9FpDiJTE","trusted":true},"cell_type":"code","source":"FNN.compile(optimizer='adam', loss='mse', metrics=['mse'])","execution_count":null,"outputs":[]},{"metadata":{"id":"0JVzYM6EOEpl","outputId":"5bddc926-5c7c-4ba3-8bce-59f6ab2b225b","trusted":true},"cell_type":"code","source":"history = FNN.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=100, batch_size=10, callbacks=[keras.callbacks.EarlyStopping(patience=10)])","execution_count":null,"outputs":[]},{"metadata":{"id":"_zg0YZOpu-HZ","outputId":"1b60afe7-d8cc-4ec7-9ef1-c3a27e8949a9","trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'], label='Training')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_feedforwardNN (X_train, y_train, X_valid, y_valid):\n    y_train_pred = FNN.predict(X_train)\n    y_valid_pred = FNN.predict(X_valid)\n\n    \n    return {\n          'MSE_train': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n          'MSLE_train':np.sqrt( mean_squared_log_error(y_train, y_train_pred)),\n          'MSE_valid': np.sqrt(mean_squared_error(y_valid, y_valid_pred)),\n          'MSLE_valid': np.sqrt(mean_squared_log_error(y_valid, y_valid_pred))\n      }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_feedforwardNN (X_train, y_train, X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"id":"TktImH27n3t3"},"cell_type":"markdown","source":"# Predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's prepare the test data for prediction\ntestset =  pd.read_csv('../input/bluebook-for-bulldozers/Test.csv', parse_dates=['saledate'])\n#drop IDs \nX_test =testset.drop(['SalesID', 'MachineID','ModelID','auctioneerID'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_preprocessing(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ordinal_encoder(X_test, ['ProductSize', 'UsageBand'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_missing_value.fill_categorical(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_missing_value.fill_numerical(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nominal_encoder(X_test, label_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = scale_feature(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_predict = FNN.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = pd.DataFrame()\nprediction['SalesID'] = testset['SalesID']\nprediction['SalePrice']= X_test_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}