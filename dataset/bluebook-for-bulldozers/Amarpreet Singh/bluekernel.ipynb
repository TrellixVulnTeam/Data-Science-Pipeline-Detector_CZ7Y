{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This kernel uses two methods to solve the problem, processing with Fastai gives better results, but I would recommend anyone to go through the full code and be able to understand the differences in those methods, which gives us such a drastic variation in score"},{"metadata":{},"cell_type":"markdown","source":"## Importing all the necessary stuff"},{"metadata":{"_uuid":"ba749341-7eea-4593-9312-310faed3977e","_cell_guid":"5c70fc04-433e-4789-877d-4340c8828dd9","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This is the basic code for accessing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting the Training and the Testing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/bluebook-for-bulldozers/trainandvalid/TrainAndValid.csv')\ntest = pd.read_csv('/kaggle/input/bluebook-for-bulldozers/Test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding the type of data in those files"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Since this file has a lot of columns, lets check the number of NaN(s)....."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking at the type of values in each column. Other columns can be checked by replacing the column names"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.fiProductClassDesc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalizing the data"},{"metadata":{},"cell_type":"markdown","source":"## This time we are using the log of the Sale Price"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['SalePrice'] = np.log(train.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking at the description above and considering the number of NaNs in each column, we would be better to consider only the really important columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_consider = ['YearMade', 'datasource', 'state', 'fiBaseModel', 'fiProductClassDesc' , 'fiModelDesc']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This function will take in the training and validation data, and output mean squared error. This function will basically tell us how distributed our data really is."},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_score(model, X_trn, y_trn, X_val, y_val):\n    model.fit(X_trn, y_trn)\n    pred = model.predict(X_val)\n    return np.sqrt(mse(pred, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preparation and preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[features_to_consider]\ny = train.SalePrice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We will be using the Label Encoder as the number of unique values in each column are a lot. If we were to use One-Hot Encoding, the number of columns would increase drastically"},{"metadata":{"trusted":true},"cell_type":"code","source":"LabelEnc = LabelEncoder()\nX['state']=LabelEnc.fit_transform(X.state)\nX['fiBaseModel']= LabelEnc.fit_transform(X.fiBaseModel)\nX['fiProductClassDesc']= LabelEnc.fit_transform(X.fiProductClassDesc)\nX['fiModelDesc']= LabelEnc.fit_transform(X.fiModelDesc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The first model we use is the Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearRegression()\nmodel_score(model, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This is the Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor(max_depth=30, min_samples_split=20, n_estimators=110, n_jobs= -1)\nmodel_score(model, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's use Multi Layered Perceptrons too"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MLPRegressor(hidden_layer_sizes=(100), activation=\"relu\", solver=\"adam\", alpha=0.0001, verbose=True)\nmodel_score(model, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking at the above mse(s), we can safely assume that our data is spread out all over"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# While looking at the different types of methods to solve these type of problems, I came accross a Deep Learning library called fastai."},{"metadata":{},"cell_type":"markdown","source":"## Let's try it out"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This is the basic installation to be able to use fastai.structured"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/fastai/fastai@2e1ccb58121dc648751e2109fc0fbf6925aa8887","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing(again!!!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom pandas_summary import DataFrameSummary\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom time import time\n\n\n## These are the fastai imports\nfrom fastai.imports import *\nfrom fastai.structured import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/bluebook-for-bulldozers/trainandvalid/TrainAndValid.csv', low_memory=False, parse_dates=[\"saledate\"])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ‘train_cats’ method is used for turning ‘string’ type columns into ‘category’ type columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cats(data)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We will use add_datepart helper function to add columns relevant to a date in the salesdate column"},{"metadata":{"trusted":true},"cell_type":"code","source":"add_datepart(data, 'saledate')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's take a look at the data now"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that the salesdate column is now gone, and instead we get different columns with different details of the date like day, days of the week, day of the year, etc. all of which are good determining factors for the Sale Price of the product"},{"metadata":{},"cell_type":"markdown","source":"### Now for some of the other useful columns in this dataset, we can see that UsageBand can influence the Sale Price\n### We can map it into numbers manually, but we will use the astype() function here"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.UsageBand = data.UsageBand.astype('category')\ndata.UsageBand = data.UsageBand.cat.codes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Usage band has been converted into int instead of string "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets normalize the data\n### We will be using log for that"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['SalePrice'] = np.log(data['SalePrice'])\ndata['SalePrice'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This looks much simpler now"},{"metadata":{},"cell_type":"markdown","source":"### Lets take at the empty values in our table, and for that we will be using proc_df function\n1. For continuous variables, it checks whether a column has missing values or not\n2. If the column has missing values, it creates another column called columnname_na, which has 1 for missing and 0 for not missing\n3. Simultaneously, the missing values are replaced with the median of the column\n4. For categorical variables, pandas replaces missing values with -1. So proc_df adds 1 to all the values for categorical variables. Thus, we have 0 for missing while all othervalues are incremented by 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"finalData, Y, nas = proc_df(data, 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finalData.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looks like we got rid of all the empty values in the table"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Y)\nlen(Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We will use Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor(n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the data and getting the score"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(finalData, Y)\nmodel.score(finalData, Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This looks comparatively better, but let's try splitting the data "},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data into training and testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(finalData, Y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train)\nmodel.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finally we have a values that we can agree with, this gives a good score as compared to the previous efforts"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.score(X_test, y_test) * 100)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}