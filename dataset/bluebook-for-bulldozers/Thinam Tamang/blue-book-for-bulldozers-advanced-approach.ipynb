{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Initialization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Downloading all Dependencies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport math\nimport graphviz\nimport scipy\n# import ggplot\n\n# from pandas_summary import DataFrameSummary\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\nfrom sklearn.tree import export_graphviz\nfrom sklearn.ensemble import forest\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom scipy.cluster import hierarchy as hc\nfrom pdpbox import pdp\nfrom plotnine import *\n# from concurrent.futures import ProcessPoolExecutor\n\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ??display # Uncomment for Documentation ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Writing Functions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"These functions are very useful for our notebook. I will try my best to provide the useful insights as much as possible so that it will be helpful for you. You can use implement these functions as well and every functions are written without much dependencies and extensions. I will update this notebook soon with the information of every functions soon.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_datepart(df, fldname, drop = True):\n    fld = df[fldname]\n    if not np.issubdtype(fld.dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format = True)\n    targ_pre = re.sub(\"[Dd]ate$\", '', fldname)\n    for n in ('Year', 'Month', 'Week', 'DayofWeek', 'DayofYear', 'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start'):\n        df[targ_pre + n] = getattr(fld.dt, n.lower())\n    df[targ_pre + 'Elasped'] = fld.astype(np.int64) // 10**9\n    if drop: df.drop(fldname, axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_cats(df, trn):\n    for n, c in df.items():\n        if trn[n].dtype.name == \"category\":\n            df[n] = pd.Categorical(c, categories = trn[n].cat.categories, ordered = True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_cats(df):\n    for n,c in df.items():\n        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fix_missing(df, col, name, na_dict):\n    if is_numeric_dtype(col):\n        if pd.isnull(col).sum() or (name in na_dict): \n            df[name + '_na'] = pd.isnull(col)\n            filler = na_dict[name] if name in na_dict else col.median()\n            df[name] = col.fillna(filler)\n            na_dict[name] = filler\n    return na_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None, prepoc_fn=None, max_n_cat=None,\n           subset=None, mapper=None):\n    if not ignore_flds: ignore_flds=[]\n    if not skip_flds: skip_flds=[]\n    if subset:\n        df = get_sample(df, subset)\n    else:\n        df = df.copy()\n    ignored_flds = df.loc[:, ignore_flds]\n    df.drop(ignore_flds, axis=1, inplace=True)\n    if prepoc_fn: prepoc_fn(df)\n    if y_fld is None: y=None\n    else:\n        if not is_numeric_dtype(df[y_fld]): df[y_fld] = pd.Categorical(df[y_fld]).codes\n        y = df[y_fld].values\n        skip_flds += [y_fld]\n    df.drop(skip_flds, axis=1, inplace=True)\n    \n    if na_dict is None: na_dict = {}\n    else: na_dict = na_dict.copy()\n    na_dict_initial = na_dict.copy()\n    for n, c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if len(na_dict_initial.keys()) > 0:\n        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n, c in df.items(): numericalize(df, c, n, max_n_cat)\n    df = pd.get_dummies(df, dummy_na=True)\n    df = pd.concat([ignored_flds, df], axis=1)\n    res = [df, y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def numericalize(df, col, name, max_n_cat):\n    if not is_numeric_dtype(col) and (max_n_cat is None or col.nunique()>max_n_cat):\n        df[name] = col.cat.codes+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_vals(a, n):\n    return a[:n].copy(), a[n:].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sample(df, n):\n    idxs = sorted(np.random.permutation(len(df))[:n])\n    return df.iloc[idxs].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_rf_samples(n):\n    forest._generate_sample_indices = (lambda rs, n_samples:\n                                      forest.check_random_state(rs).randit(0, n_samples, n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reset_rf_samples():\n    forest._generate_sample_indices = (lambda rs, n_samples:\n                                      forest.check_random_state(rs).randit(0, n_samples, n_samples))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_vars(df, mapper):\n    warnings.filterwarnings(\"ignore\", category = sklearn.exceptions.DataConversionWarning)\n    if mapper is None:\n        map_f = [([n], StandardScaler()) for n in df.columns if is_numeric_dtype(df[n])]\n        mapper = DataFrameMapper(map_f).fit(df)\n    df[mapper.transformed_names_] = mapper.transform(df)\n    return mapper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(x, y):\n    return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train),\n          rmse(m.predict(X_valid), y_valid),\n          m.score(X_train, y_train),\n          m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'):\n        res.append(m.oob_score_)\n    print(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading and Pre-processing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's first read the data with pandas alias.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw = pd.read_csv('/kaggle/input/bluebook-for-bulldozers/Train.zip', low_memory = False, parse_dates = [\"saledate\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to predict the SalePrice so let's take some insights about the Saleprice first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.SalePrice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000):\n        with pd.option_context(\"display.max_columns\", 1000):\n            display(df)\n            \ndisplay_all(df_raw.tail().transpose())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In kaggle the score is predicted using \"rmse\", root mean squared error, so let's change the SalePrice in log number.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.SalePrice = np.log(df_raw.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, the salePrice column is converted into log numbers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.SalePrice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initial Processing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this notebook, we will use RandomForestRegressor.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.saledate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fld = df_raw.saledate\nfld.dt.year","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can scroll up to see what does add_datepart function which will appoint saledate as the index of the DataFrame.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"add_datepart(df_raw, \"saledate\")\ndf_raw.saleYear.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cats(df_raw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"UsageBand is very useful to get the insights of Categorical objects. Some columns of our DataFrame also contains some categorical objects. So let's deal with it first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.UsageBand.cat.categories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's arrange the order of the Categorical variables or objects so that our code won't surprise us later. We have to deal with small information of our datasets so that our result on validation set won't surprise us.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.UsageBand.cat.set_categories([\"High\", \"Medium\", \"Low\"], ordered = True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's observe the percentage of null variables on each column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"display_all(df_raw.isnull().sum().sort_index()/len(df_raw))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Saving ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs('tmp', exist_ok = True)\ndf_raw.to_feather('tmp/raw')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building the RandomForestRegressor Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw = pd.read_feather('tmp/raw')\ndf_trn, y_trn, nas = proc_df(df_raw, \"SalePrice\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_valid = 12000\nn_trn = len(df_trn) - n_valid\nX_train, X_valid = split_vals(df_trn, n_trn)\ny_train, y_valid = split_vals(y_trn, n_trn)\n\nraw_train, raw_valid = split_vals(df_raw, n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_rf_samples(50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=80, n_jobs=-1, min_samples_leaf=3, max_features=0.5, oob_score=True)\n%prun m.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time preds = np.stack([t.predict(X_valid) for t in m.estimators_])\nnp.mean(preds[:, 0]), np.std(preds[:, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = raw_valid.copy()\nx['pred_std'] = np.std(preds, axis=0)\nx['pred'] = np.mean(preds, axis=0)\nx.Enclosure.value_counts().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flds = ['Enclosure', 'SalePrice', 'pred', 'pred_std']\nenc_sum = x[flds].groupby(\"Enclosure\", as_index=False).mean()\nenc_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc_sum = enc_sum[~pd.isnull(enc_sum.SalePrice)]\nenc_sum.plot(\"Enclosure\", \"SalePrice\", \"barh\", xlim=(0, 11))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc_sum.plot(\"Enclosure\", \"pred\", \"barh\", xerr='pred_std', alpha=0.7, xlim=(0, 11))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_valid.ProductSize.value_counts().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flds = ['ProductSize', 'SalePrice', 'pred', 'pred_std']\nsumm = x[flds].groupby(\"ProductSize\").mean()\nsumm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(summ.pred_std / summ.pred).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = rf_feat_importance(m, df_trn)\nfi[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi.plot('cols', 'imp', figsize=(10, 6), legend=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_fi(fi):\n    return fi.plot(\"cols\", 'imp', 'barh', figsize=(12, 7), legend=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fi(fi[:30])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_keep = fi[fi.imp > 0.005].cols\nlen(to_keep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_keep = df_trn[to_keep].copy()\nX_train, X_valid = split_vals(df_keep, n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=80, n_jobs=-1, min_samples_leaf=3, max_features = 0.5, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = rf_feat_importance(m, df_keep)\nplot_fi(fi)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One Hot Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn2, y_trn, nas = proc_df(df_raw, \"SalePrice\", max_n_cat = 7)\nX_train, X_valid = split_vals(df_trn2, n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=80, n_jobs=-1, min_samples_leaf=3, max_features=0.5, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = rf_feat_importance(m, df_trn2)\nplot_fi(fi[:25])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing Redundant Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1 - corr)\nz = hc.linkage(corr_condensed, method=\"average\")\nfig = plt.figure(figsize=(16, 10))\ndendrogram = hc.dendrogram(z, labels = list(df_keep.columns), orientation = \"left\", leaf_font_size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_oob(df):\n    m = RandomForestRegressor(n_estimators=40, n_jobs=-1, min_samples_leaf=5, max_features=0.5, oob_score=True)\n    x, _ = split_vals(df, n_trn)\n    m.fit(x, y_train)\n    return m.oob_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oob score in RandomForest does the same work as validation. It is not necessary to calculate prediction on validation set after calculating oob score. It somehow does the same work.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"get_oob(df_keep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_keep.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in ('saleYear', 'saleElasped', 'fiModelDesc', 'fiBaseModel', 'Grouser_Tracks', 'Coupler_System'):\n    print(c, get_oob(df.drop(c, axis=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df_keep.copy()\nto_drop = ['saleYear', 'fiModelDesc', 'Grouser_Tracks']\nget_oob(df1.drop(to_drop, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_keep.drop(to_drop, axis=1, inplace=True)\nX_train, X_valid = split_vals(df_keep, n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('tmp/keep_cols.npy', np.array(df_keep.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keep_cols = np.load('tmp/keep_cols.npy', allow_pickle=True)\ndf_keep = df_trn[keep_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reset_rf_samples()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=80, n_jobs=-1, min_samples_leaf=3, max_features=0.5, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Partial Dependance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"set_rf_samples(50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(df_trn2, n_trn)\nm = RandomForestRegressor(n_estimators=80, n_jobs=-1, min_samples_leaf=3, max_features=0.6)\nm.fit(X_train, y_train)\n# print_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fi(rf_feat_importance(m, df_trn2)[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.plot(\"YearMade\", \"saleElasped\", \"scatter\", alpha=0.01, figsize=(10,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_all = get_sample(df_raw[df_raw.YearMade > 1930], 500)\n# ggplot(x_all, aes('YearMade', 'SalePrice')) + stat_smooth(se=True, method=\"loess\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = get_sample(X_train[X_train.YearMade > 1930], 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_pdp(feat, clusters=None, feat_name=None):\n    feat_name = feat_name or feat\n    p = pdp.pdp_isolate(m, x, feature=feat, model_features=x.columns)\n    return pdp.pdp_plot(p, feat_name, plot_lines=True,\n                       cluster = clusters is not None,\n                       n_cluster_centers = clusters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pdp is one of the best unknown tools for visualization and it is still unknown to many Data Scientists.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pdp('YearMade')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pdp('YearMade', clusters=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pdp(['Enclosure_EROPS w AC', 'Enclosure_EROPS', 'Enclosure_OROPS'], 5, 'Enclosure')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.YearMade[df_raw.YearMade < 1950] = 1950\ndf_keep[\"age\"] = df_raw[\"age\"] = df_raw.saleYear - df_raw.YearMade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=80, n_jobs=-1, min_samples_leaf=3, max_features=0.6)\nm.fit(X_train, y_train)\nplot_fi(rf_feat_importance(m, df_keep))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tree Interpreter","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install treeinterpreter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from treeinterpreter import treeinterpreter as ti","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_valid = split_vals(df_raw[df_keep.columns], n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row = X_valid.values[None, 0]\nrow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction, bias, contributions = ti.predict(m, row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction[0], bias[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idxs = np.argsort(contributions[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[o for o in zip(df_keep.columns[idxs], df_valid.iloc[0][idxs], contributions[0][idxs])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contributions[0].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extrapolation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ext = df_keep.copy()\ndf_ext['is_valid'] = 1\ndf_ext.is_valid[:n_trn] = 0\nx, y, nas = proc_df(df_ext, 'is_valid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestClassifier(n_estimators=80, n_jobs=-1, min_samples_leaf=3, max_features=0.6, oob_score=True)\nm.fit(x, y)\nm.oob_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = rf_feat_importance(m, x)\nfi[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = ['SalesID', 'saleElasped', 'MachineID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(X_train[feats]/1000).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(X_valid[feats]/1000).describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.drop(feats, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestClassifier(n_estimators=80, n_jobs=-1, min_samples_leaf=3, max_features=0.6, oob_score=True)\nm.fit(x, y)\nm.oob_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = rf_feat_importance(m, x)\nfi[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_rf_samples(50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = ['SalesID', 'saleElasped', 'MachineID', 'age', 'YearMade', 'saleDayofYear']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=40, n_jobs=-1, min_samples_leaf=3, max_features=0.6, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_keep.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in feats:\n    df_subs = df.drop(f, axis=1)\n    X_train, X_valid = split_vals(df_subs, n_trn)\n    m = RandomForestRegressor(n_estimators=40, n_jobs=-1, min_samples_leaf=3, max_features=0.6, oob_score = True)\n    m.fit(X_train, y_train)\n    print(f)\n    print_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_keep.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_subs = df.drop(['SalesID', 'MachineID', 'saleDayofYear'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid = split_vals(df_subs, n_trn)\nm = RandomForestRegressor(n_estimators=80, n_jobs=-1, min_samples_leaf=3, max_features=0.6, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fi(rf_feat_importance(m, X_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=160, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, the rmse is 0.2175.. which is not bad and you can check the rmse score of public leaderboard as well.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As this notebook is so long, I am sorry that i have written much description about various techniques. You can search yourself as it will be more helpful for you and you can learn more.\nThank you.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}