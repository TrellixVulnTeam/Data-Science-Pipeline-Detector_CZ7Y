{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## About\n\nThis code enables to train a 5-fold ResNext-50 in the kaggle kernels. There is unfortunately some timeout issues.\n\nThe 5 fold blend with our [post-processing method](https://www.kaggle.com/theoviel/inference-theo) achieves private LB 0.675 (3rd place)\n\n\nCode is a bit dirty, the clean version is available on [GitHub](https://github.com/TheoViel/kaggle_birdcall_identification)","metadata":{"_uuid":"439932ec-edb0-4d85-a4c2-9b8ea5487441","_cell_guid":"48b81818-7cd8-4b0b-8dbe-28e45ce39347","trusted":true}},{"cell_type":"markdown","source":"## Initialization","metadata":{"_uuid":"7f850844-e8ef-4e15-a851-2813d60657dc","_cell_guid":"708b36e2-dfc3-4003-b1c3-cbdbb95b5cc7","trusted":true}},{"cell_type":"code","source":"!pip3 install audiomentations pysndfx","metadata":{"_uuid":"e8ee1a62-2174-41dc-8496-1a1878d1e151","_cell_guid":"eef16bd1-640a-45dd-a009-60da22b0dd67","collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\n\nsys.path = ['../input/bird-outputs/src/src/',] + sys.path\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.model_selection import *\n\npd.options.display.max_rows = 500\npd.options.display.max_columns = 500","metadata":{"_uuid":"54c76d32-49b1-464a-9b2c-8f5ea8b10849","_cell_guid":"bc732048-f589-4c74-a168-35a8349de515","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{"_uuid":"c16227d1-2f02-4c3c-af5a-d3cc02685a19","_cell_guid":"620e85b5-8e2e-4734-88e1-45e0369c5254","trusted":true}},{"cell_type":"code","source":"ROOT = Path.cwd().parent\nINPUT_ROOT = ROOT / \"input\"\nRAW_DATA = INPUT_ROOT / \"birdsong-recognition\"\nTRAIN_AUDIO_DIR = RAW_DATA / \"train_audio\"\nTEST_AUDIO_DIR = RAW_DATA / \"test_audio\"\n\nTRAIN_RESAMPLED_AUDIO_DIRS = [\n  INPUT_ROOT / \"birdsong-resampled-train-audio-{:0>2}\".format(i)  for i in range(5) # 靠右显示,左侧空白补0\n]\n\n\nresampled_infos = [] # 鸟名+波形文件名+路径名, 路径名采用linux格式, 不出现反斜杠转义\n\nfor audio_d in TRAIN_RESAMPLED_AUDIO_DIRS:\n    if not audio_d.exists():\n        continue\n\n    for ebird_d in audio_d.iterdir():\n        if ebird_d.is_file():\n            continue\n\n        for wav_f in ebird_d.iterdir():\n            resampled_infos.append([ebird_d.name, wav_f.name, wav_f.as_posix()]) \n\n            \ntrain_resampled_infos = pd.DataFrame(resampled_infos, columns=[\"ebird_code\", \"resampled_filename\", \"file_path\"])\ntrain = pd.read_csv(TRAIN_RESAMPLED_AUDIO_DIRS[0] / \"train_mod.csv\")\n\ntrain_all = pd.merge(train, train_resampled_infos, on=[\"ebird_code\", \"resampled_filename\"], how=\"inner\")\ndf_train = train_all.copy()\n\n\n# 另外两个训练集中的采样数据\nEXTRA_RESAMPLED_AUDIO_DIRS = [INPUT_ROOT / f\"xenoexternalwav0/external-xeno-wav-{i}\"  for i in range(3)] + [\n    INPUT_ROOT / f\"xenoexternalwav1/external-xeno-wav-{i}\"  for i in (3,4)\n    ]\n\nresampled_infos = []\nfor audio_d in EXTRA_RESAMPLED_AUDIO_DIRS:\n    if not audio_d.exists():\n        continue\n    for ebird_d in audio_d.iterdir():\n        if ebird_d.is_file():\n            continue\n        for wav_f in ebird_d.iterdir():\n            resampled_infos.append([wav_f.name, wav_f.as_posix()])\n            \nextra_resampled_infos = pd.DataFrame(resampled_infos, columns=[\"ebird_code\", \"file_path\"]).sort_values(\"ebird_code\").reset_index(drop=True)\n\ndf_extra = pd.read_csv(INPUT_ROOT / \"xenoexternalwav0/train_extended.csv\")\ndf_extra_ = pd.merge(df_extra, extra_resampled_infos, on=[\"ebird_code\"], how=\"left\")\n\n\npaths = []\nfor c, file in df_extra_[[\"file_path\", \"filename\"]].values:\n    path = f\"{c}/{file[:-4]}.wav\"\n    paths.append(path)\ndf_extra[\"file_path\"] = paths\n\n\ndf_extra = df_extra[df_extra['duration'] < 200]  # 移除太长的样本","metadata":{"_uuid":"44a45027-4f80-4269-a946-572b88533e5b","_cell_guid":"aedafdbb-41cc-4420-ae5f-8754c125152b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Params","metadata":{"_uuid":"1fc3248b-7c16-46a9-9a8b-ac6133a98343","_cell_guid":"eeecfad6-96ff-4edd-9197-da2ccba75a49","trusted":true}},{"cell_type":"code","source":"import os\nimport torch\nimport warnings\nimport numpy as np\nwarnings.simplefilter(action=\"ignore\", category=UserWarning)\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\nSEED = 5518\nDATA_PATH = \"../input/birdsong-recognition/\"\nAUDIO_PATH = \"../input/birdsong-recognition/train_audio/\"\nBACKGROUND_PATH = \"../input/bird-backgrounds/\" # 背景噪音\n\nMEAN = np.array([0.485, 0.456, 0.406])\nSTD = np.array([0.229, 0.224, 0.225])\n\nNUM_WORKERS = 0\n# NUM_WORKERS = 4\n\nVAL_BS = 32\n\nDEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n\nCLASSES = sorted(os.listdir(AUDIO_PATH))\nNUM_CLASSES = len(CLASSES)\n\nCP_TODAY = \"\"","metadata":{"_uuid":"408a20a4-a21f-4609-bf91-4e14400c1fed","_cell_guid":"09b0c543-1058-4c0c-b787-91e1aa3799e0","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{"_uuid":"2b50bca1-1ea9-49d2-b454-b944d06c87cc","_cell_guid":"8de0b601-5a03-4155-ae95-e4b528d56244","trusted":true}},{"cell_type":"code","source":"import os\nimport torch\nimport random\nimport numpy as np\nimport torch.nn as nn\nfrom sklearn.metrics import f1_score\n\n# 固定随机种子\ndef seed_everything(seed): \n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True  # False\n\n\ndef save_model_weights(model, filename, verbose=1, cp_folder=\"\"):\n    \"\"\"\n    保存PyTorch模型权重参数\n    \n    Arguments:\n        model {torch module} -- PyTorch模型\n        filename {str} -- checkpoint的名称\n    \n    Keyword Arguments:\n        verbose {int} -- 保存的具体log\n        cp_folder {str} -- 目标文件夹\n    \"\"\"\n    if verbose:\n        print(f\"\\n -> Saving weights to {os.path.join(cp_folder, filename)}\\n\")\n    torch.save(model.state_dict(), os.path.join(cp_folder, filename))\n\n\ndef load_model_weights(model, filename, verbose=1, cp_folder=\"\"):\n    \"\"\"\n    加载模型参数,exception处理CPU/GPU相关错误\n    \n    Arguments:\n        model {torch module} -- PyTorch模型\n        filename {str} -- checkpoint名称\n    \n    Keyword Arguments:\n        verbose {int} -- 显示log\n        cp_folder {str} -- 目标文件夹\n    \n    Returns:\n        torch module -- 加载权重后的模型\n    \"\"\"\n    if verbose:\n        print(f\"\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n\")\n    try:\n        model.load_state_dict(os.path.join(cp_folder, filename), strict=strict)\n    except BaseException:\n        model.load_state_dict(\n            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n            strict=True,\n        )\n    return model\n\n\ndef count_parameters(model, all=False):\n    \"\"\"\n    count模型参数\n    \n    Returns:\n        int -- Number of parameters\n    \"\"\"\n    if all:\n        return sum(p.numel() for p in model.parameters())\n    else:\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n# 对类别进行独热编码, 用于F1度量计算\nONE_HOT = np.eye(NUM_CLASSES) # 264种, 每一行对应一种鸟类的编码\n\n\ndef f1(truth, pred, threshold=0.5, avg=\"samples\"):\n\n    if len(truth.shape) == 1:\n        truth = ONE_HOT[truth]\n\n    pred = (pred > threshold).astype(int)\n\n    return f1_score(truth, pred, average=avg)","metadata":{"_uuid":"495ce2c0-158a-4e80-8022-e937ba4de937","_cell_guid":"f194d437-c9a5-4e23-a0c5-715ac02af308","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transforms","metadata":{"_uuid":"32d0e017-d207-4994-a888-d70ef45fac74","_cell_guid":"89fd58b8-83ef-481b-9a9e-e1f0aacee293","trusted":true}},{"cell_type":"code","source":"import cv2\nimport pysndfx # 混响\nimport numpy as np\nfrom audiomentations import *\n\n\ndef mono_to_color(X, eps=1e-6, mean=None, std=None):\n    X = np.stack([X, X, X], axis=-1)\n\n    # 标准化\n    mean = mean or X.mean()\n    std = std or X.std()\n    X = (X - mean) / (std + eps)\n\n    # 标准化到[0, 255]\n    _min, _max = X.min(), X.max()\n\n    if (_max - _min) > eps:\n        V = np.clip(X, _min, _max)\n        V = 255 * (V - _min) / (_max - _min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(X, dtype=np.uint8)\n\n    return V\n\n\ndef resize(image, size=None):\n    if size is not None:\n        h, w, _ = image.shape\n        new_w, new_h = int(w * size / h), size\n        image = cv2.resize(image, (new_w, new_h))\n\n    return image\n\n\ndef normalize(image, mean=None, std=None):\n    image = image / 255.0\n    if mean is not None and std is not None:\n        image = (image - mean) / std\n    return np.moveaxis(image, 2, 0).astype(np.float32)\n\n\ndef crop_or_pad(y, length, sr, train=True, probs=None):\n    if len(y) <= length:\n        y = np.concatenate([y, np.zeros(length - len(y))])\n    else:\n        if not train:\n            start = 0\n        # 如果是训练集\n        elif probs is None:\n            start = np.random.randint(len(y) - length)\n        else:\n            start = (\n                np.random.choice(np.arange(len(probs)), p=probs) + np.random.random()\n            )\n            start = int(sr * (start))\n\n        y = y[start : start + length]\n\n    return y.astype(np.float32)\n\n\n# 对.wav文件进行混响\ndef get_wav_transforms():\n    transforms = Compose(\n        [\n            AddGaussianSNR(max_SNR=0.5, p=0.5),\n            AddBackgroundNoise(\n                sounds_path=BACKGROUND_PATH, min_snr_in_db=0, max_snr_in_db=2, p=0.5\n            ),\n        ]\n    )\n\n    return transforms\n\n\nclass AudioAugmentation:\n    def __init__(self, p_effects=0.5, p_noise=0.5):\n        self.p_effects = p_effects\n\n        self.noise_transfos = Compose(\n            [\n                AddGaussianSNR(max_SNR=0.5, p=p_noise),\n                AddBackgroundNoise(\n                    sounds_path=BACKGROUND_PATH, min_snr_in_db=0, max_snr_in_db=2, p=p_noise\n                ),\n            ]\n        )\n\n    def __call__(self, y, sr):\n        y = self.noise_transfos(y, sr)\n\n        if np.random.uniform() < self.p_effects:\n            effects_chain = (\n                pysndfx.AudioEffectsChain()\n                .reverb(\n                    reverberance=random.randrange(50),\n                    room_scale=random.randrange(50),\n                    stereo_depth=random.randrange(50),\n                )\n                .pitch(shift=random.randrange(-300, 300))\n                .overdrive(gain=random.randrange(2, 20))\n            )\n\n            y = effects_chain(y)\n\n        return y","metadata":{"_uuid":"05a6da7f-9365-44b6-903f-3410597fbdfe","_cell_guid":"78900227-be3a-42f0-85a3-c597ff544c55","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{"_uuid":"2692f329-99d7-4305-971e-4fac09d65a77","_cell_guid":"e6eb1aba-3004-4cff-bc90-d8f57508d858","trusted":true}},{"cell_type":"code","source":"import os\nimport pickle\nimport librosa\nimport soundfile\nimport numpy as np\nfrom torch.utils.data import Dataset\n\n\nONE_HOT = np.eye(len(CLASSES))\nCONF_PATH = \"../input/bird-outputs/preds_oof_2.pkl\"\nassert os.path.isfile(CONF_PATH)\n\n\ndef compute_melspec(y, params): \n    melspec = librosa.feature.melspectrogram(\n        y,\n        sr=params.sr,\n        n_mels=params.n_mels,\n        fmin=params.fmin,\n        fmax=params.fmax,\n    )\n\n    \n    melspec = librosa.power_to_db(melspec).astype(np.float32) # 提取对数梅尔频谱特征\n    return melspec\n\n\nclass BirdDataset(Dataset):\n    def __init__(self, df, params, audio_path=\"\", train=True, use_conf=False):\n        self.train = train\n        self.params = params\n        self.audio_path = audio_path\n\n        self.wav_transfos = get_wav_transforms() if train else None\n\n        self.spec_transfos = None\n\n        self.y = np.array([CLASSES.index(c) for c in df[\"ebird_code\"]])\n        self.paths = df[\"file_path\"].values\n\n        self.sample_len = params.duration * params.sr\n\n        self.use_conf = use_conf\n        \n        # 采用置信度时\n        if use_conf:\n            with open(CONF_PATH, \"rb\") as file:\n                self.confidences = pickle.load(file)\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, idx: int): # 预处理的具体操作\n        y, sr = soundfile.read(self.audio_path + self.paths[idx])\n\n        if self.use_conf:\n            name = \"/\".join(self.paths[idx].split('/')[-2:])\n            \n            confs = self.confidences[name][:, self.y[idx]]\n            if len(confs):\n                confs = confs / np.sum(confs)\n            else:\n                confs = None\n        else:\n            confs = None\n\n        y = crop_or_pad(\n            y, self.sample_len, sr=self.params.sr, train=self.train, probs=confs\n        )\n\n        if self.wav_transfos is not None:\n            y = self.wav_transfos(y, self.params.sr)\n\n        melspec = compute_melspec(y, self.params)\n\n        if self.spec_transfos is not None:\n            melspec = self.spec_transfos(melspec)\n\n        image = mono_to_color(melspec)\n        image = resize(image, self.params.img_size)\n        image = normalize(image, mean=None, std=None)\n\n        return image, ONE_HOT[self.y[idx]]","metadata":{"_uuid":"dfdd4c86-043e-4f4c-9c89-5942820bf89c","_cell_guid":"f364fcae-d71d-44da-b567-1c35e78ccb77","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"_uuid":"6dfa591b-1d68-406e-9bd4-c2b388f4dacf","_cell_guid":"3272870e-2049-4935-a35f-f305d484901f","trusted":true}},{"cell_type":"code","source":"import torch\n\ndef get_model(name, use_msd=False, num_classes=1):\n    model = torch.hub.load('pytorch/vision:v0.6.0', name, pretrained=True)\n    nb_ft = model.fc.in_features\n    del model.fc\n    model.fc = nn.Linear(nb_ft, num_classes)\n\n    return model","metadata":{"_uuid":"05a9b964-7f9c-440e-a042-4380ee9669ea","_cell_guid":"bd7f2d75-9230-485e-9c14-e36d23372610","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"_uuid":"e3015fa2-ff94-4d89-a643-fdfa547f8a77","_cell_guid":"1012326c-b358-4001-ab0b-f0bd253e7476","trusted":true}},{"cell_type":"code","source":"# import gc # 垃圾回收, 防止内存爆炸\nimport time\nimport torch\nimport numpy as np\nimport torch.nn as nn\n\nfrom tqdm import tqdm\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import RandomSampler\nfrom transformers import get_linear_schedule_with_warmup\nfrom torchvision.models.inception import InceptionOutputs\n\nfrom training.mixup import mixup_data\n# from params import NUM_WORKERS, NUM_CLASSES\nfrom training.specaugment import SpecAugmentation\n\n\ndef smooth_label(y , alpha=0.01):\n    y = y * (1 - alpha)\n    y[y == 0] = alpha\n    return y\n\n    \ndef fit(\n    model,\n    train_dataset,\n    val_dataset,\n    epochs=50,\n    batch_size=32,\n    val_bs=32,\n    warmup_prop=0.1,\n    lr=1e-3,\n    alpha=0.4,\n    mixup_proba=0.0,\n    specaugment_proba=0.0,\n    label_smoothing=0.0,\n    verbose=1,\n    verbose_eval=1,\n):\n    \"\"\"\n    常见的fit函数\n    \n    Arguments:\n        model {torch model} -- 模型\n        train_dataset {torch dataset} -- 训练集\n        val_dataset {torch dataset} -- 验证集\n    \n    Keyword Arguments:\n        epochs {int} -- 训练遍数\n        batch_size {int} -- 批大小\n        val_bs {int} -- 验证集批大小\n        warmup_prop {float} -- 预热学习率, ResNet论文中提到的学习率预热方法,先使用一个较小的学习率防止模型振荡\n        lr {float} -- Start(or maximum)的学习率\n        alpha {float} -- mixup数据增强的alpha值\n        mixup_proba {float} -- 使用mixup的概率\n        specaugment_proba {float} -- 使用频谱增强的概率\n        verbose {int} -- epochs中是否显示log\n        verbose_eval {int} -- epochs中是否perform evaluation\n\n    Returns:\n        numpy array -- 最后一epoch的预测值\n    \"\"\"\n\n    avg_val_loss = 0.\n    avg_loss = 0.\n    score = 0.\n\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    loss_fct = nn.BCEWithLogitsLoss(reduction=\"mean\").cuda()\n\n    spec_augmenter = SpecAugmentation(\n        time_drop_width=16, time_stripes_num=2, freq_drop_width=8, freq_stripes_num=2\n    )\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n        pin_memory=True,\n        num_workers=NUM_WORKERS,\n    )\n    val_loader = DataLoader(\n        val_dataset, batch_size=val_bs, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS\n    )\n\n    num_warmup_steps = int(warmup_prop * epochs * len(train_loader))\n    num_training_steps = int(epochs * len(train_loader))\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps, num_training_steps\n    )\n\n    for epoch in range(epochs):\n        model.train()\n        start_time = time.time()\n        optimizer.zero_grad()\n\n        avg_loss = 0\n        for step, (x, y_batch) in enumerate(train_loader):\n            if specaugment_proba:\n                if np.random.rand() < specaugment_proba:\n                    x = spec_augmenter(x)\n\n            if np.random.rand() < mixup_proba:\n                x, y_a, y_b, _ = mixup_data(x.cuda(), y_batch.cuda(), alpha=alpha)\n                y_batch = torch.clamp(y_a + y_b, 0, 1)\n\n                \n            y_pred = model(x.cuda())\n\n            loss = loss_fct(y_pred, y_batch.cuda().float())\n\n            loss.backward()\n            avg_loss += loss.item() / len(train_loader)\n\n            optimizer.step()\n            optimizer.zero_grad()\n            scheduler.step()\n\n        if (epoch + 1) % verbose_eval == 0 or (epoch + 1 == epochs):\n            model.eval()\n\n            avg_val_loss = 0.0\n            with torch.no_grad():\n                preds = np.empty((0, NUM_CLASSES))\n                for x, y_batch in val_loader:\n                    y_pred = model(x.cuda()).detach()\n                    loss = loss_fct(y_pred, y_batch.cuda().float())\n                    avg_val_loss += loss.item() / len(val_loader)\n\n                    preds = np.concatenate([preds, torch.sigmoid(y_pred).cpu().numpy()])\n\n            micro_f1 = f1(val_dataset.y, preds, avg=\"micro\")\n            samples_f1 = f1(val_dataset.y, preds)\n\n        elapsed_time = time.time() - start_time\n        if (epoch + 1) % verbose == 0:\n            elapsed_time = elapsed_time * verbose\n            lr = scheduler.get_lr()[0]\n            print(\n                f\"Epoch {epoch + 1}/{epochs} \\t lr={lr:.1e} \\t t={elapsed_time:.0f}s  \\t loss={avg_loss:.4f} \\t \",\n                end=\"\",\n            )\n            if (epoch + 1) % verbose_eval == 0 or (epoch + 1 == epochs):\n                print(\n                    f\"val_loss={avg_val_loss:.4f} \\t micro_f1={micro_f1:.3f} \\t samples_f1={samples_f1:.3f}\"\n                )\n            else:\n                print(\"\")\n\n    torch.cuda.empty_cache()\n    return preds\n\n\ndef predict(model, dataset, batch_size=64):\n    \"\"\"\n    Usual torch predict function\n\n    Arguments:\n        model {torch model} -- 模型\n        dataset {torch dataset} -- 预测的集\n\n    Keyword Arguments:\n        batch_size {int} -- batch size\n\n    Returns:\n        numpy array -- 预测\n    \"\"\"\n    model.eval()\n    preds = np.empty((0, NUM_CLASSES))\n\n    loader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS\n    )\n    with torch.no_grad():\n        for x, _ in loader:\n            y_pred = model(x.cuda()).detach()\n            preds = np.concatenate([preds, torch.sigmoid(y_pred).cpu().numpy()])\n\n    return preds","metadata":{"_uuid":"33c47b84-a404-47b5-86bf-20ee64917994","_cell_guid":"923b8335-81b1-4c6e-a88f-3c084269f4f0","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{"_uuid":"b2ab2fce-1381-49cb-b8bd-0367159d9695","_cell_guid":"fec89b7f-e093-41a0-9444-ab480c341e0e","trusted":true}},{"cell_type":"code","source":"def train(config, df_train, df_val, fold):\n\n    print(f\"    -> {len(df_train)} training birds\")\n    print(f\"    -> {len(df_val)} validation birds\")\n\n    seed_everything(config.seed)\n\n    model = get_model( # 获取配置中的预训练模型名称, 包括了resnext50等在内\n        config.selected_model, use_msd=config.use_msd, num_classes=NUM_CLASSES\n    ).cuda()\n    model.zero_grad()\n\n    train_dataset = BirdDataset(\n        df_train, AudioParams, audio_path=\"\", use_conf=config.use_conf\n    )\n    val_dataset = BirdDataset(df_val, AudioParams, audio_path=\"\", train=False)\n\n    n_parameters = count_parameters(model)\n    print(f\"    -> 有{n_parameters}个训练参数.\\n\")\n\n    pred_val = fit(\n        model,\n        train_dataset,\n        val_dataset,\n        epochs=config.epochs,\n        batch_size=config.batch_size,\n        val_bs=config.val_bs,\n        lr=config.lr,\n        warmup_prop=config.warmup_prop,\n        alpha=config.alpha,\n        mixup_proba=config.mixup_proba,\n        specaugment_proba=config.specaugment_proba,\n        label_smoothing=config.label_smoothing,\n        verbose_eval=config.verbose_eval,\n    )\n\n    if config.save:\n        save_model_weights(\n            model,\n            f\"{config.selected_model}_{config.name}_{fold}.pt\",\n            cp_folder=CP_TODAY,\n        )\n\n    return pred_val\n\n\ndef k_fold(config, df, df_extra=None):\n\n    skf = StratifiedKFold(n_splits=config.k, random_state=config.random_state) # 分层采样进行K折\n    splits = list(skf.split(X=df, y=df[\"ebird_code\"])) # X: 数据集, y: 标签集\n\n    \n    pred_oof = np.zeros((len(df), NUM_CLASSES))\n\n    for i, (train_idx, val_idx) in enumerate(splits):\n        if i in config.selected_folds:\n            print(f\"\\n-------------   Fold {i + 1} / {config.k}  -------------\\n\")\n\n            df_train = df.iloc[train_idx].copy()\n            df_val = df.iloc[val_idx].copy()\n\n            if df_extra is not None:\n                df_train = pd.concat((df_train, df_extra), 0).reset_index(drop=True)\n\n            pred_val = train(config, df_train, df_val, i) # 训练\n            pred_oof[val_idx] = pred_val\n\n    return pred_oof","metadata":{"_uuid":"eefcf62d-e608-41c2-9327-cefec13043f9","_cell_guid":"d26bcf5c-b259-40c1-b867-278bcb8a1d50","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    # General\n    seed = 5518\n    verbose = 1\n    verbose_eval = 31\n    save = True\n\n    # k-fold\n    k = 5\n    random_state = 42\n    selected_folds = [0] \n\n    selected_model = 'resnext50_32x4d'\n    \n    use_msd = False\n    use_conf = False\n    \n    img_size = None\n    batch_size = 64\n    epochs = 30\n    lr = 1e-3\n    warmup_prop = 0.05\n    val_bs = 64\n\n    label_smoothing = 0.\n    specaugment_proba = 0.\n    mixup_proba = 0.5\n    alpha = 5\n\n    name = \"extra\"\n    \nclass AudioParams:\n    sr = 32000\n    duration = 5\n    img_size = None\n\n    # Melspectrogram\n    n_mels = 128\n    fmin = 20\n    fmax = 16000","metadata":{"_uuid":"bb620657-a0cc-43da-a60f-2f08bf1a1db4","_cell_guid":"77ca8a1b-2c86-40e2-b5fe-ddf625f09b0d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_oof = k_fold(Config, df_train, df_extra)","metadata":{"_uuid":"26c15e8b-2e03-4619-9b98-6d15cb5d92b8","_cell_guid":"b8ea5ec6-67e4-42d8-9582-9a2811529862","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}