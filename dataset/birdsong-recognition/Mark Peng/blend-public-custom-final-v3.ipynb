{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, \"../input/resnest/\")\n\nimport os\nimport gc\nimport time\nimport math\nimport shutil\nimport random\nimport warnings\nimport typing as tp\nfrom pathlib import Path\nfrom contextlib import contextmanager\n\nimport yaml\nfrom joblib import delayed, Parallel\n\nimport cv2\nimport librosa\nimport audioread\nimport soundfile as sf\n\nimport numpy as np\nimport pandas as pd\n\nfrom fastprogress import progress_bar\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport resnest.torch as resnest_torch\n\npd.options.display.max_rows = 500\npd.options.display.max_columns = 500\n\nimport gc\ngc.enable()\n\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\ndevice = torch.device(\"cuda:0\")\n\nrand_seed = 1120\n\ndebug_mode = False\n# debug_mode = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_size = 224\n\nn_classes = 264\n\nuse_fold = 0\n\nmodel_files = [\n    (\"resnest50_fast_2s2x40d\", \"../input/bird-custom-ext-v1-fold0/F1_snapshot_epoch_26.pth\"),\n    (\"resnest50_fast_2s2x40d\", \"../input/bird-custom-ext-v1-fold1/F1_snapshot_epoch_30.pth\"),\n    (\"resnest50_fast_2s2x40d\", \"../input/bird-custom-ext-v1-fold2/F1_snapshot_epoch_29.pth\"),\n    (\"resnest50_fast_2s2x40d\", \"../input/bird-custom-ext-v1-fold3/F1_snapshot_epoch_30.pth\"),\n    (\"resnest50_fast_2s2x40d\", \"../input/bird-custom-ext-v1-fold4/snapshot_epoch_25.pth\"),\n]\n\npublic_model_file =  (\"resnest50_fast_1s1x64d\", \"../input/training-birdsong-baseline-resnest50-fast/best_model.pth\")\nresnest_200_model_files =  [\n    (\"resnest200\", \"../input/resnest200-v2-fold0/resnest200_v2_epoch_44.pth\"),\n    (\"resnest200\", \"../input/resnest200-fold0/resnest200_epoch_50.pth\")\n]\nnocall_model_files = [\n    (\"resnest50_fast_2s2x40d\", \"../input/bird-custom-nocall-v2-fold1/F1_snapshot_epoch_4.pth\"),\n    (\"resnest50_fast_2s2x40d\", \"../input/bird-custom-nocall-v2-fold1/F1_snapshot_epoch_10.pth\")\n]\n\ninfer_batch_size = 1024\n\nprob_threshold = 0.3\n# prob_threshold = 0.5\n# prob_threshold = 0.6\n\nnum_workers = 4\n\nTARGET_SR = 32000\n\nmelspectrogram_parameters = {\"n_mels\": 128, \"fmin\": 20, \"fmax\": 16000}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"### Utilities"},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False  # for reproducibility\n\n\n@contextmanager\ndef timer(name: str) -> None:\n    \"\"\"Timer Util\"\"\"\n    t0 = time.time()\n    print(\"[{}] start\".format(name))\n    yield\n    print(\"[{}] done in {:.0f} s\".format(name, time.time() - t0))\n\n\ndef print_model_info(model):\n    total = 0\n    trainable = 0\n    non_trainable = 0\n    for p in model.parameters():\n        if p.requires_grad:\n            trainable += p.numel()\n        else:\n            non_trainable += p.numel()\n        total += p.numel()\n\n    print(f\"Total number of model parameters: {total:,}\")\n    print(f\"Total number of trainable parameters: {trainable:,}\")\n    print(f\"Total number of non-trainable parameters: {non_trainable:,}\")\n\n\nset_seed(rand_seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read data"},{"metadata":{"trusted":true},"cell_type":"code","source":"ROOT = Path.cwd().parent\nINPUT_ROOT = ROOT / \"input\"\nRAW_DATA = INPUT_ROOT / \"birdsong-recognition\"\nTRAIN_AUDIO_DIR = RAW_DATA / \"train_audio\"\nTEST_AUDIO_DIR = RAW_DATA / \"test_audio\"\n\n# ROOT = Path(\"/workspace/Kaggle/Bird\")\n# TRAIN_AUDIO_DIR = Path(\n#     \"/workspace/Kaggle/Bird/birdsong-recognition/train_audio\")\n# TRAIN_RESAMPLED_AUDIO_DIR = ROOT / \"birdsong-resampled-train-audio\"\n# TEST_AUDIO_DIR = Path(\"/workspace/Kaggle/Bird/birdsong-recognition/test_audio\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(RAW_DATA / \"train.csv\")\nif not TEST_AUDIO_DIR.exists():\n    TEST_AUDIO_DIR = INPUT_ROOT / \"birdcall-check\" / \"test_audio\"\n    test = pd.read_csv(INPUT_ROOT / \"birdcall-check\" / \"test.csv\")\nelse:\n    test = pd.read_csv(RAW_DATA / \"test.csv\")\n\n# train = pd.read_csv(TRAIN_RESAMPLED_AUDIO_DIR / \"train_mod.csv\")\n\n# if not TEST_AUDIO_DIR.exists():\n#     TEST_AUDIO_DIR = ROOT / \"birdcall-check\" / \"test_audio\"\n#     test = pd.read_csv(ROOT / \"birdcall-check\" / \"test.csv\")\n# else:\n#     test = pd.read_csv(ROOT / \"birdsong-recognition\" / \"test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/birdsong-recognition/sample_submission.csv\")\n# sub = pd.read_csv(ROOT / \"birdsong-recognition/sample_submission.csv\")\nprint(sub.shape)\nprint(sub.head())\n# sub.to_csv(\"submission.csv\", index=False)  # this will be overwritten if everything goes well","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Definition"},{"metadata":{},"cell_type":"markdown","source":"### Dataset\n\nFor `site_3`, I decided to use the same procedure as I did for `site_1` and `site_2`, which is, crop 5 seconds out of the clip and provide prediction on that short clip.\nThe only difference is that I crop 5 seconds short clip from start to the end of the `site_3` clip and aggeregate predictions for each short clip after I did prediction for all those short clips."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"BIRD_CODE = {\n    'aldfly': 0, 'ameavo': 1, 'amebit': 2, 'amecro': 3, 'amegfi': 4,\n    'amekes': 5, 'amepip': 6, 'amered': 7, 'amerob': 8, 'amewig': 9,\n    'amewoo': 10, 'amtspa': 11, 'annhum': 12, 'astfly': 13, 'baisan': 14,\n    'baleag': 15, 'balori': 16, 'banswa': 17, 'barswa': 18, 'bawwar': 19,\n    'belkin1': 20, 'belspa2': 21, 'bewwre': 22, 'bkbcuc': 23, 'bkbmag1': 24,\n    'bkbwar': 25, 'bkcchi': 26, 'bkchum': 27, 'bkhgro': 28, 'bkpwar': 29,\n    'bktspa': 30, 'blkpho': 31, 'blugrb1': 32, 'blujay': 33, 'bnhcow': 34,\n    'boboli': 35, 'bongul': 36, 'brdowl': 37, 'brebla': 38, 'brespa': 39,\n    'brncre': 40, 'brnthr': 41, 'brthum': 42, 'brwhaw': 43, 'btbwar': 44,\n    'btnwar': 45, 'btywar': 46, 'buffle': 47, 'buggna': 48, 'buhvir': 49,\n    'bulori': 50, 'bushti': 51, 'buwtea': 52, 'buwwar': 53, 'cacwre': 54,\n    'calgul': 55, 'calqua': 56, 'camwar': 57, 'cangoo': 58, 'canwar': 59,\n    'canwre': 60, 'carwre': 61, 'casfin': 62, 'caster1': 63, 'casvir': 64,\n    'cedwax': 65, 'chispa': 66, 'chiswi': 67, 'chswar': 68, 'chukar': 69,\n    'clanut': 70, 'cliswa': 71, 'comgol': 72, 'comgra': 73, 'comloo': 74,\n    'commer': 75, 'comnig': 76, 'comrav': 77, 'comred': 78, 'comter': 79,\n    'comyel': 80, 'coohaw': 81, 'coshum': 82, 'cowscj1': 83, 'daejun': 84,\n    'doccor': 85, 'dowwoo': 86, 'dusfly': 87, 'eargre': 88, 'easblu': 89,\n    'easkin': 90, 'easmea': 91, 'easpho': 92, 'eastow': 93, 'eawpew': 94,\n    'eucdov': 95, 'eursta': 96, 'evegro': 97, 'fiespa': 98, 'fiscro': 99,\n    'foxspa': 100, 'gadwal': 101, 'gcrfin': 102, 'gnttow': 103, 'gnwtea': 104,\n    'gockin': 105, 'gocspa': 106, 'goleag': 107, 'grbher3': 108, 'grcfly': 109,\n    'greegr': 110, 'greroa': 111, 'greyel': 112, 'grhowl': 113, 'grnher': 114,\n    'grtgra': 115, 'grycat': 116, 'gryfly': 117, 'haiwoo': 118, 'hamfly': 119,\n    'hergul': 120, 'herthr': 121, 'hoomer': 122, 'hoowar': 123, 'horgre': 124,\n    'horlar': 125, 'houfin': 126, 'houspa': 127, 'houwre': 128, 'indbun': 129,\n    'juntit1': 130, 'killde': 131, 'labwoo': 132, 'larspa': 133, 'lazbun': 134,\n    'leabit': 135, 'leafly': 136, 'leasan': 137, 'lecthr': 138, 'lesgol': 139,\n    'lesnig': 140, 'lesyel': 141, 'lewwoo': 142, 'linspa': 143, 'lobcur': 144,\n    'lobdow': 145, 'logshr': 146, 'lotduc': 147, 'louwat': 148, 'macwar': 149,\n    'magwar': 150, 'mallar3': 151, 'marwre': 152, 'merlin': 153, 'moublu': 154,\n    'mouchi': 155, 'moudov': 156, 'norcar': 157, 'norfli': 158, 'norhar2': 159,\n    'normoc': 160, 'norpar': 161, 'norpin': 162, 'norsho': 163, 'norwat': 164,\n    'nrwswa': 165, 'nutwoo': 166, 'olsfly': 167, 'orcwar': 168, 'osprey': 169,\n    'ovenbi1': 170, 'palwar': 171, 'pasfly': 172, 'pecsan': 173, 'perfal': 174,\n    'phaino': 175, 'pibgre': 176, 'pilwoo': 177, 'pingro': 178, 'pinjay': 179,\n    'pinsis': 180, 'pinwar': 181, 'plsvir': 182, 'prawar': 183, 'purfin': 184,\n    'pygnut': 185, 'rebmer': 186, 'rebnut': 187, 'rebsap': 188, 'rebwoo': 189,\n    'redcro': 190, 'redhea': 191, 'reevir1': 192, 'renpha': 193, 'reshaw': 194,\n    'rethaw': 195, 'rewbla': 196, 'ribgul': 197, 'rinduc': 198, 'robgro': 199,\n    'rocpig': 200, 'rocwre': 201, 'rthhum': 202, 'ruckin': 203, 'rudduc': 204,\n    'rufgro': 205, 'rufhum': 206, 'rusbla': 207, 'sagspa1': 208, 'sagthr': 209,\n    'savspa': 210, 'saypho': 211, 'scatan': 212, 'scoori': 213, 'semplo': 214,\n    'semsan': 215, 'sheowl': 216, 'shshaw': 217, 'snobun': 218, 'snogoo': 219,\n    'solsan': 220, 'sonspa': 221, 'sora': 222, 'sposan': 223, 'spotow': 224,\n    'stejay': 225, 'swahaw': 226, 'swaspa': 227, 'swathr': 228, 'treswa': 229,\n    'truswa': 230, 'tuftit': 231, 'tunswa': 232, 'veery': 233, 'vesspa': 234,\n    'vigswa': 235, 'warvir': 236, 'wesblu': 237, 'wesgre': 238, 'weskin': 239,\n    'wesmea': 240, 'wessan': 241, 'westan': 242, 'wewpew': 243, 'whbnut': 244,\n    'whcspa': 245, 'whfibi': 246, 'whtspa': 247, 'whtswi': 248, 'wilfly': 249,\n    'wilsni1': 250, 'wiltur': 251, 'winwre3': 252, 'wlswar': 253, 'wooduc': 254,\n    'wooscj2': 255, 'woothr': 256, 'y00475': 257, 'yebfly': 258, 'yebsap': 259,\n    'yehbla': 260, 'yelwar': 261, 'yerwar': 262, 'yetvir': 263, 'nocall': 264\n}\n\nINV_BIRD_CODE = {v: k for k, v in BIRD_CODE.items()}","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def mono_to_color(X: np.ndarray,\n                  mean=None,\n                  std=None,\n                  norm_max=None,\n                  norm_min=None,\n                  eps=1e-6):\n    \"\"\"\n    Code from https://www.kaggle.com/daisukelab/creating-fat2019-preprocessed-data\n    \"\"\"\n    # Stack X as [X,X,X]\n    X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    X = X - mean\n    std = std or X.std()\n    Xstd = X / (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    norm_max = norm_max or _max\n    norm_min = norm_min or _min\n    if (_max - _min) > eps:\n        # Normalize to [0, 255]\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255 * (V - norm_min) / (norm_max - norm_min)\n        V = V.astype(np.uint8)\n    else:\n        # Just zero\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n\n\nclass TestDataset(data.Dataset):\n    def __init__(self,\n                 df: pd.DataFrame,\n                 clip: np.ndarray,\n                 img_size=image_size,\n                 melspectrogram_parameters={}):\n        self.df = df\n        self.clip = clip\n        self.img_size = img_size\n        self.melspectrogram_parameters = melspectrogram_parameters\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx: int):\n        SR = 32000\n        sample = self.df.loc[idx, :]\n        site = sample.site\n        row_id = sample.row_id\n\n        if site == \"site_3\":\n            y = self.clip.astype(np.float32)\n            len_y = len(y)\n            start = 0\n            end = SR * 5\n            images = []\n            while len_y > start:\n                y_batch = y[start:end].astype(np.float32)\n                if len(y_batch) != (SR * 5):\n                    break\n                start = end\n                end = end + SR * 5\n\n                melspec = librosa.feature.melspectrogram(\n                    y_batch, sr=SR, **self.melspectrogram_parameters)\n                melspec = librosa.power_to_db(melspec).astype(np.float32)\n                image = mono_to_color(melspec)\n                height, width, _ = image.shape\n                image = cv2.resize(\n                    image,\n                    (int(width * self.img_size / height), self.img_size))\n                image = np.moveaxis(image, 2, 0)\n                image = (image / 255.0).astype(np.float32)\n                images.append(image)\n            images = np.asarray(images)\n            return images, row_id, site\n        else:\n            end_seconds = int(sample.seconds)\n            start_seconds = int(end_seconds - 5)\n\n            start_index = SR * start_seconds\n            end_index = SR * end_seconds\n\n            y = self.clip[start_index:end_index].astype(np.float32)\n\n            melspec = librosa.feature.melspectrogram(\n                y, sr=SR, **self.melspectrogram_parameters)\n            melspec = librosa.power_to_db(melspec).astype(np.float32)\n\n            image = mono_to_color(melspec)\n            height, width, _ = image.shape\n            image = cv2.resize(\n                image, (int(width * self.img_size / height), self.img_size))\n            image = np.moveaxis(image, 2, 0)\n            image = (image / 255.0).astype(np.float32)\n\n            return image, row_id, site","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_layer(layer):\n    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n    if hasattr(layer, 'weight'):\n        nn.init.xavier_uniform_(layer.weight)\n\n    if hasattr(layer, 'bias'):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)\n\ndef init_bn(bn):\n    \"\"\"Initialize a Batchnorm layer. \"\"\"\n    bn.bias.data.fill_(0.)\n    bn.weight.data.fill_(1.)\n\n\nclass CustomResNeSt(nn.Module):\n    def __init__(self, pretrained_model, n_classes):\n        \n        super(CustomResNeSt, self).__init__()\n\n        self.backbone = getattr(resnest_torch, pretrained_model)(pretrained=False)\n\n        self.backbone.fc = nn.Sequential(\n            nn.Linear(2048, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n            nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n            nn.Linear(1024, n_classes))\n        \n        self.init_weight()\n\n    def init_weight(self):\n        for layer in self.backbone.fc:\n            init_layer(layer)\n\n    def forward(self, input):\n        \"\"\"\n        Input: (batch_size, data_length)\"\"\"\n\n        if self.training:\n            out = self.backbone(x)\n        else:\n            out = self.backbone(input)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(pretrained_model, model_file_path):\n    model = CustomResNeSt(pretrained_model, n_classes)\n\n    print(f\"Loading model states from {model_file_path} ......\")\n    model.load_state_dict(torch.load(model_file_path))\n    device = torch.device(\"cuda\")\n    model.to(device)\n    model.eval()\n\n    print_model_info(model)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_resnest_model(pretrained_model, model_file_path):\n    model = getattr(resnest_torch, pretrained_model)(pretrained=False)\n    del model.fc\n    # # use the same head as the baseline notebook.\n    model.fc = nn.Sequential(nn.Linear(2048,\n                                       1024), nn.ReLU(), nn.Dropout(p=0.2),\n                             nn.Linear(1024, 1024), nn.ReLU(),\n                             nn.Dropout(p=0.2), nn.Linear(1024, n_classes))\n\n    print(f\"Loading model states from {model_file_path} ......\")\n    model.load_state_dict(torch.load(model_file_path))\n    device = torch.device(\"cuda\")\n    model.to(device)\n    model.eval()\n\n    print_model_info(model)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_nocall_resnest_model(pretrained_model, model_file_path):\n    model = CustomResNeSt(pretrained_model, n_classes+1)\n\n    print(f\"Loading model states from {model_file_path} ......\")\n    model.load_state_dict(torch.load(model_file_path))\n    device = torch.device(\"cuda\")\n    model.to(device)\n    model.eval()\n\n    print_model_info(model)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction_for_clip(test_df: pd.DataFrame,\n                        clip,\n                        models,\n                        public_model,\n                        resnest_models,\n                        nocall_models,\n                        mel_params: dict, \n                        threshold=0.5):\n\n    for m in models:\n        m.eval()\n    public_model.eval()\n    for m in resnest_models:\n        m.eval()\n    for m in nocall_models:\n        m.eval()\n        \n    prediction_dict = {}\n\n    dataset = TestDataset(df=test_df, \n                          clip=clip,\n                          img_size=image_size,\n                          melspectrogram_parameters=mel_params)\n\n    # Site 1 & 2: use normal batch size\n    if test_df.site.values[0] in [\"site_1\", \"site_2\"]:\n        loader = data.DataLoader(dataset,\n                                 batch_size=infer_batch_size,\n                                 shuffle=False,\n                                 num_workers=num_workers,\n                                 drop_last=False,\n                                 pin_memory=True)\n\n        for image, row_id, site in progress_bar(loader):\n            \n            with torch.no_grad():\n                image = image.to(device)\n                preds = np.zeros((image.size(0), n_classes+1))\n                for m in models:\n                    prediction = F.sigmoid(m(image))\n                    proba = prediction.detach().cpu().numpy()\n                    preds[:, :-1] += proba\n                preds /= len(models)\n                \n                public_preds = np.zeros((image.size(0), n_classes+1))\n                prediction = F.sigmoid(public_model(image))\n                public_proba = prediction.detach().cpu().numpy()\n                public_preds[:, :-1] = public_proba\n                \n                resnest_preds = np.zeros((image.size(0), n_classes+1))\n                for m in resnest_models:\n                    prediction = F.sigmoid(m(image))\n                    proba = prediction.detach().cpu().numpy()\n                    resnest_preds[:, :-1] += proba\n                resnest_preds /= len(resnest_models)\n                \n                nocall_preds = np.zeros((image.size(0), n_classes+1))\n                for m in nocall_models:\n                    prediction = F.sigmoid(m(image))\n                    proba = prediction.detach().cpu().numpy()\n                    nocall_preds += proba\n                nocall_preds /= len(nocall_models)\n                \n                preds = (preds + public_preds + resnest_preds + nocall_preds)/4\n                \n                events = preds >= threshold              \n                \n                for i in range(len(events)):\n                    event = events[i, :]\n                    labels = np.argwhere(event).reshape(-1).tolist()\n\n                    # Keep separate result per frame\n                    frame_id = row_id[i]\n                    if len(labels) == 0:\n                        prediction_dict[frame_id] = \"nocall\"\n                    else:\n                        if len(labels) > 1 and 264 in labels:\n                            labels.remove(264)\n                        labels_str_list = list(map(lambda x: INV_BIRD_CODE[x], labels))\n                        label_string = \" \".join(labels_str_list)\n                        prediction_dict[frame_id] = label_string\n\n    else:\n        # Site 3: batch size = 1\n        loader = data.DataLoader(dataset,\n                                 batch_size=1,\n                                 shuffle=False,\n                                 num_workers=num_workers,\n                                 drop_last=False,\n                                 pin_memory=True)\n    \n        for image, row_id, site in progress_bar(loader):\n            site = site[0]\n            row_id = row_id[0]\n\n            # to avoid prediction on large batch\n            image = image.squeeze(0)\n            batch_size = infer_batch_size\n            # batch_size = 16\n            whole_size = image.size(0)\n            if whole_size % batch_size == 0:\n                n_iter = whole_size // batch_size\n            else:\n                n_iter = whole_size // batch_size + 1\n                \n            all_events = set()\n            for batch_i in range(n_iter):\n                batch = image[batch_i * batch_size:(batch_i + 1) * batch_size]\n                if batch.ndim == 3:\n                    batch = batch.unsqueeze(0)\n\n                with torch.no_grad():\n                    batch = batch.to(device)\n                    \n                    preds = np.zeros((batch.size(0), n_classes+1))\n                    for m in models:\n                        prediction = F.sigmoid(m(batch))\n                        proba = prediction.detach().cpu().numpy()\n                        preds[:, :-1] += proba\n                    preds /= len(models)\n                    \n                    public_preds = np.zeros((batch.size(0), n_classes+1))\n                    prediction = F.sigmoid(public_model(batch))\n                    public_proba = prediction.detach().cpu().numpy()\n                    public_preds[:, :-1] = public_proba\n\n                    resnest_preds = np.zeros((batch.size(0), n_classes+1))\n                    for m in resnest_models:\n                        prediction = F.sigmoid(m(batch))\n                        proba = prediction.detach().cpu().numpy()\n                        resnest_preds[:, :-1] += proba\n                    resnest_preds /= len(resnest_models)\n\n                    nocall_preds = np.zeros((batch.size(0), n_classes+1))\n                    for m in nocall_models:\n                        prediction = F.sigmoid(m(batch))\n                        proba = prediction.detach().cpu().numpy()\n                        nocall_preds += proba\n                    nocall_preds /= len(nocall_models)\n\n                    preds = (preds + public_preds + resnest_preds + nocall_preds)/4\n\n                    events = preds >= threshold\n                    for i in range(len(events)):\n                        event = events[i, :]\n                        labels = np.argwhere(event).reshape(-1).tolist()\n\n                        for label in labels:\n                            all_events.add(label)\n                        \n            labels = list(all_events)\n\n            if len(labels) == 0:\n                prediction_dict[row_id] = \"nocall\"\n            else:\n                if len(labels) > 1 and 264 in labels:\n                    labels.remove(264)\n                labels_str_list = list(map(lambda x: INV_BIRD_CODE[x], labels))\n                label_string = \" \".join(labels_str_list)\n                prediction_dict[row_id] = label_string\n\n    del loader, dataset\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return prediction_dict","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def prediction(test_df: pd.DataFrame,\n               test_audio: Path,\n               mel_params: dict,\n               target_sr: int,\n               threshold=0.5):\n    models = []\n    for m, p in model_files:\n        models.append(get_model(m, p))\n        \n    public_model = get_resnest_model(public_model_file[0], public_model_file[1])\n    \n    resnest_models = []\n    for m, p in resnest_200_model_files:\n        resnest_models.append(get_resnest_model(m, p))\n    \n    nocall_models = []\n    for m, p in nocall_model_files:\n        nocall_models.append(get_nocall_resnest_model(m, p))\n    \n    unique_audio_id = test_df.audio_id.unique()\n\n    warnings.filterwarnings(\"ignore\")\n    prediction_dfs = []\n    for audio_id in unique_audio_id:\n        with timer(f\"Loading {audio_id}\"):\n            clip, _ = librosa.load(test_audio / (audio_id + \".mp3\"),\n                                   sr=target_sr,\n                                   mono=True,\n                                   res_type=\"kaiser_fast\")\n\n        test_df_for_audio_id = test_df.query(\n            f\"audio_id == '{audio_id}'\").reset_index(drop=True)\n        with timer(f\"Prediction on {audio_id}\"):\n            prediction_dict = prediction_for_clip(test_df_for_audio_id,\n                                                  clip=clip,\n                                                  models=models,\n                                                  public_model=public_model,\n                                                  resnest_models=resnest_models,\n                                                  nocall_models=nocall_models,\n                                                  mel_params=mel_params,\n                                                  threshold=threshold)\n        row_id = list(prediction_dict.keys())\n        birds = list(prediction_dict.values())\n        prediction_df = pd.DataFrame({\"row_id\": row_id, \"birds\": birds})\n        prediction_dfs.append(prediction_df)\n\n    prediction_df = pd.concat(prediction_dfs, axis=0,\n                              sort=False).reset_index(drop=True)\n    return prediction_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"submission = prediction(test_df=test,\n                        test_audio=TEST_AUDIO_DIR,\n                        mel_params=melspectrogram_parameters,\n                        target_sr=TARGET_SR,\n                        threshold=prob_threshold)\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"print(submission.shape)\nsubmission.head(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EOF"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}