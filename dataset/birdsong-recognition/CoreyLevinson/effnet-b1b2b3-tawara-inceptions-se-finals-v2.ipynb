{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install onnxruntime --no-index --find-links=file:///kaggle/input/save-out-pip-libraries-without-internet/onnxrunt/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport math\nimport random\nfrom pathlib import Path\n\nimport cv2\nimport librosa\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Conv2d, Module, Linear, BatchNorm2d, ReLU\nfrom torch.nn.modules.utils import _pair\nimport torch.utils.data as data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import onnxruntime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROOT = Path.cwd().parent\nINPUT_ROOT = ROOT / \"input\"\nRAW_DATA = INPUT_ROOT / \"birdsong-recognition\"\nTEST_AUDIO_DIR = RAW_DATA / \"test_audio\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not TEST_AUDIO_DIR.exists():\n    TEST_AUDIO_DIR = INPUT_ROOT / \"birdcall-check\" / \"test_audio\"\n    test = pd.read_csv(INPUT_ROOT / \"birdcall-check\" / \"test.csv\")\nelse:\n    test = pd.read_csv(RAW_DATA / \"test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/birdsong-recognition/sample_submission.csv\")\n#sub['ebird_code'] = 'evegro'\n#sub.to_csv(\"submission.csv\", index=False)  # this will be overwritten if everything goes well","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET_SR = 32000\n\nmodel_config = {\n    \"base_model_name\": \"resnest50_fast_1s1x64d\",\n    \"pretrained\": False,\n    \"num_classes\": 264,\n    \"trained_weights\": \"../input/training-birdsong-baseline-resnest50-fast/best_model.pth\"\n}\n\nmelspectrogram_parameters = {\n    \"n_mels\": 128,\n    \"fmin\": 20,\n    \"fmax\": 16000\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"BIRD_CODE = {\n    'aldfly': 0, 'ameavo': 1, 'amebit': 2, 'amecro': 3, 'amegfi': 4,\n    'amekes': 5, 'amepip': 6, 'amered': 7, 'amerob': 8, 'amewig': 9,\n    'amewoo': 10, 'amtspa': 11, 'annhum': 12, 'astfly': 13, 'baisan': 14,\n    'baleag': 15, 'balori': 16, 'banswa': 17, 'barswa': 18, 'bawwar': 19,\n    'belkin1': 20, 'belspa2': 21, 'bewwre': 22, 'bkbcuc': 23, 'bkbmag1': 24,\n    'bkbwar': 25, 'bkcchi': 26, 'bkchum': 27, 'bkhgro': 28, 'bkpwar': 29,\n    'bktspa': 30, 'blkpho': 31, 'blugrb1': 32, 'blujay': 33, 'bnhcow': 34,\n    'boboli': 35, 'bongul': 36, 'brdowl': 37, 'brebla': 38, 'brespa': 39,\n    'brncre': 40, 'brnthr': 41, 'brthum': 42, 'brwhaw': 43, 'btbwar': 44,\n    'btnwar': 45, 'btywar': 46, 'buffle': 47, 'buggna': 48, 'buhvir': 49,\n    'bulori': 50, 'bushti': 51, 'buwtea': 52, 'buwwar': 53, 'cacwre': 54,\n    'calgul': 55, 'calqua': 56, 'camwar': 57, 'cangoo': 58, 'canwar': 59,\n    'canwre': 60, 'carwre': 61, 'casfin': 62, 'caster1': 63, 'casvir': 64,\n    'cedwax': 65, 'chispa': 66, 'chiswi': 67, 'chswar': 68, 'chukar': 69,\n    'clanut': 70, 'cliswa': 71, 'comgol': 72, 'comgra': 73, 'comloo': 74,\n    'commer': 75, 'comnig': 76, 'comrav': 77, 'comred': 78, 'comter': 79,\n    'comyel': 80, 'coohaw': 81, 'coshum': 82, 'cowscj1': 83, 'daejun': 84,\n    'doccor': 85, 'dowwoo': 86, 'dusfly': 87, 'eargre': 88, 'easblu': 89,\n    'easkin': 90, 'easmea': 91, 'easpho': 92, 'eastow': 93, 'eawpew': 94,\n    'eucdov': 95, 'eursta': 96, 'evegro': 97, 'fiespa': 98, 'fiscro': 99,\n    'foxspa': 100, 'gadwal': 101, 'gcrfin': 102, 'gnttow': 103, 'gnwtea': 104,\n    'gockin': 105, 'gocspa': 106, 'goleag': 107, 'grbher3': 108, 'grcfly': 109,\n    'greegr': 110, 'greroa': 111, 'greyel': 112, 'grhowl': 113, 'grnher': 114,\n    'grtgra': 115, 'grycat': 116, 'gryfly': 117, 'haiwoo': 118, 'hamfly': 119,\n    'hergul': 120, 'herthr': 121, 'hoomer': 122, 'hoowar': 123, 'horgre': 124,\n    'horlar': 125, 'houfin': 126, 'houspa': 127, 'houwre': 128, 'indbun': 129,\n    'juntit1': 130, 'killde': 131, 'labwoo': 132, 'larspa': 133, 'lazbun': 134,\n    'leabit': 135, 'leafly': 136, 'leasan': 137, 'lecthr': 138, 'lesgol': 139,\n    'lesnig': 140, 'lesyel': 141, 'lewwoo': 142, 'linspa': 143, 'lobcur': 144,\n    'lobdow': 145, 'logshr': 146, 'lotduc': 147, 'louwat': 148, 'macwar': 149,\n    'magwar': 150, 'mallar3': 151, 'marwre': 152, 'merlin': 153, 'moublu': 154,\n    'mouchi': 155, 'moudov': 156, 'norcar': 157, 'norfli': 158, 'norhar2': 159,\n    'normoc': 160, 'norpar': 161, 'norpin': 162, 'norsho': 163, 'norwat': 164,\n    'nrwswa': 165, 'nutwoo': 166, 'olsfly': 167, 'orcwar': 168, 'osprey': 169,\n    'ovenbi1': 170, 'palwar': 171, 'pasfly': 172, 'pecsan': 173, 'perfal': 174,\n    'phaino': 175, 'pibgre': 176, 'pilwoo': 177, 'pingro': 178, 'pinjay': 179,\n    'pinsis': 180, 'pinwar': 181, 'plsvir': 182, 'prawar': 183, 'purfin': 184,\n    'pygnut': 185, 'rebmer': 186, 'rebnut': 187, 'rebsap': 188, 'rebwoo': 189,\n    'redcro': 190, 'redhea': 191, 'reevir1': 192, 'renpha': 193, 'reshaw': 194,\n    'rethaw': 195, 'rewbla': 196, 'ribgul': 197, 'rinduc': 198, 'robgro': 199,\n    'rocpig': 200, 'rocwre': 201, 'rthhum': 202, 'ruckin': 203, 'rudduc': 204,\n    'rufgro': 205, 'rufhum': 206, 'rusbla': 207, 'sagspa1': 208, 'sagthr': 209,\n    'savspa': 210, 'saypho': 211, 'scatan': 212, 'scoori': 213, 'semplo': 214,\n    'semsan': 215, 'sheowl': 216, 'shshaw': 217, 'snobun': 218, 'snogoo': 219,\n    'solsan': 220, 'sonspa': 221, 'sora': 222, 'sposan': 223, 'spotow': 224,\n    'stejay': 225, 'swahaw': 226, 'swaspa': 227, 'swathr': 228, 'treswa': 229,\n    'truswa': 230, 'tuftit': 231, 'tunswa': 232, 'veery': 233, 'vesspa': 234,\n    'vigswa': 235, 'warvir': 236, 'wesblu': 237, 'wesgre': 238, 'weskin': 239,\n    'wesmea': 240, 'wessan': 241, 'westan': 242, 'wewpew': 243, 'whbnut': 244,\n    'whcspa': 245, 'whfibi': 246, 'whtspa': 247, 'whtswi': 248, 'wilfly': 249,\n    'wilsni1': 250, 'wiltur': 251, 'winwre3': 252, 'wlswar': 253, 'wooduc': 254,\n    'wooscj2': 255, 'woothr': 256, 'y00475': 257, 'yebfly': 258, 'yebsap': 259,\n    'yehbla': 260, 'yelwar': 261, 'yerwar': 262, 'yetvir': 263\n}\n\nINV_BIRD_CODE = {v: k for k, v in BIRD_CODE.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"settings = {} # A dictionary to hold the settings, easier to understand than yaml...\n\n# Used for generating the MelSpectrogram image\nsettings['dataset'] = {}\nsettings['dataset']['params'] = {}\nsettings['dataset']['params']['img_size'] = 300 # The image size of the spectrogram that we are CNN'ing over\nsettings['dataset']['params']['melspectrogram_parameters'] = {} # https://librosa.org/doc/latest/generated/librosa.filters.mel.html#librosa.filters.mel\nsettings['dataset']['params']['melspectrogram_parameters']['n_mels'] = 300 # The number of Melspectrograms bands to create. the higher this is, the more compelx.\nsettings['dataset']['params']['melspectrogram_parameters']['fmin'] = 650 # Lowest frequency to use (usually 0)\nsettings['dataset']['params']['melspectrogram_parameters']['fmax'] = 16000 # Highest frequency to use (usually sr / 2)\nsettings['dataset']['params']['melspectrogram_parameters']['n_fft'] = 2400 # Usually n_mels * 8\nsettings['dataset']['params']['melspectrogram_parameters']['hop_length'] = 533 # usually len(sig) / n_mels, so 32000*5 / n_mels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"settings_effnetb1 = {} # A dictionary to hold the settings_effnetb1, easier to understand than yaml...\n\n# Used for generating the MelSpectrogram image\nsettings_effnetb1['dataset'] = {}\nsettings_effnetb1['dataset']['params'] = {}\nsettings_effnetb1['dataset']['params']['img_size'] = 240 # The image size of the spectrogram that we are CNN'ing over\nsettings_effnetb1['dataset']['params']['melspectrogram_parameters'] = {} # https://librosa.org/doc/latest/generated/librosa.filters.mel.html#librosa.filters.mel\nsettings_effnetb1['dataset']['params']['melspectrogram_parameters']['n_mels'] = 240 # The number of Melspectrograms bands to create. the higher this is, the more compelx.\nsettings_effnetb1['dataset']['params']['melspectrogram_parameters']['fmin'] = 1000 # Lowest frequency to use (usually 0)\nsettings_effnetb1['dataset']['params']['melspectrogram_parameters']['fmax'] = 12500 # Highest frequency to use (usually sr / 2)\nsettings_effnetb1['dataset']['params']['melspectrogram_parameters']['n_fft'] = 1920 # Usually n_mels * 8\nsettings_effnetb1['dataset']['params']['melspectrogram_parameters']['hop_length'] = 666 # usually len(sig) / n_mels, so 32000*5 / n_mels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"settings_thefirst = {}\n\n# Used for generating the MelSpectrogram image\nsettings_thefirst['dataset'] = {}\nsettings_thefirst['dataset']['params'] = {}\nsettings_thefirst['dataset']['params']['img_size'] = 260 # The image size of the spectrogram that we are CNN'ing over\nsettings_thefirst['dataset']['params']['melspectrogram_parameters'] = {} # https://librosa.org/doc/latest/generated/librosa.filters.mel.html#librosa.filters.mel\nsettings_thefirst['dataset']['params']['melspectrogram_parameters']['n_mels'] = 260 # The number of Melspectrograms bands to create. the higher this is, the more compelx.\nsettings_thefirst['dataset']['params']['melspectrogram_parameters']['fmin'] = 20 # Lowest frequency to use (usually 0)\nsettings_thefirst['dataset']['params']['melspectrogram_parameters']['fmax'] = 15000 # Highest frequency to use (usually sr / 2)\nsettings_thefirst['dataset']['params']['melspectrogram_parameters']['n_fft'] = 1500 # Usually n_mels * 8\nsettings_thefirst['dataset']['params']['melspectrogram_parameters']['hop_length'] = 800 # usually len(sig) / n_mels, so 32000*5 / n_mels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"settings_thesecond = {} # A dictionary to hold the settings_thesecond, easier to understand than yaml...\n\n# Used for generating the MelSpectrogram image\nsettings_thesecond['dataset'] = {}\nsettings_thesecond['dataset']['params'] = {}\nsettings_thesecond['dataset']['params']['img_size'] = 260 # The image size of the spectrogram that we are CNN'ing over\nsettings_thesecond['dataset']['params']['melspectrogram_parameters'] = {} # https://librosa.org/doc/latest/generated/librosa.filters.mel.html#librosa.filters.mel\nsettings_thesecond['dataset']['params']['melspectrogram_parameters']['n_mels'] = 260 # The number of Melspectrograms bands to create. the higher this is, the more compelx.\nsettings_thesecond['dataset']['params']['melspectrogram_parameters']['fmin'] = 260 # Lowest frequency to use (usually 0)\nsettings_thesecond['dataset']['params']['melspectrogram_parameters']['fmax'] = 16000 # Highest frequency to use (usually sr / 2)\nsettings_thesecond['dataset']['params']['melspectrogram_parameters']['n_fft'] = 2080 # Usually n_mels * 8\nsettings_thesecond['dataset']['params']['melspectrogram_parameters']['hop_length'] = 615 # usually len(sig) / n_mels, so 32000*5 / n_mels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"settings_inception = {} # A dictionary to hold the settings_inception, easier to understand than yaml...\n\n# Used for generating the MelSpectrogram image\nsettings_inception['dataset'] = {}\nsettings_inception['dataset']['params'] = {}\nsettings_inception['dataset']['params']['img_size'] = 299 # The image size of the spectrogram that we are CNN'ing over\nsettings_inception['dataset']['params']['melspectrogram_parameters'] = {} # https://librosa.org/doc/latest/generated/librosa.filters.mel.html#librosa.filters.mel\nsettings_inception['dataset']['params']['melspectrogram_parameters']['n_mels'] = 299 # The number of Melspectrograms bands to create. the higher this is, the more compelx.\nsettings_inception['dataset']['params']['melspectrogram_parameters']['fmin'] = 299 # Lowest frequency to use (usually 0)\nsettings_inception['dataset']['params']['melspectrogram_parameters']['fmax'] = 16000 # Highest frequency to use (usually sr / 2)\nsettings_inception['dataset']['params']['melspectrogram_parameters']['n_fft'] = 2392 # Usually n_mels * 8\nsettings_inception['dataset']['params']['melspectrogram_parameters']['hop_length'] = 535 # usually len(sig) / n_mels, so 32000*5 / n_mels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"settings_se = {} # A dictionary to hold the settings_se, easier to understand than yaml...\n\n# Used for generating the MelSpectrogram image\nsettings_se['dataset'] = {}\nsettings_se['dataset']['params'] = {}\nsettings_se['dataset']['params']['img_size'] = 224 # The image size of the spectrogram that we are CNN'ing over\nsettings_se['dataset']['params']['melspectrogram_parameters'] = {} # https://librosa.org/doc/latest/generated/librosa.filters.mel.html#librosa.filters.mel\nsettings_se['dataset']['params']['melspectrogram_parameters']['n_mels'] = 224 # The number of Melspectrograms bands to create. the higher this is, the more compelx.\nsettings_se['dataset']['params']['melspectrogram_parameters']['fmin'] = 224 # Lowest frequency to use (usually 0)\nsettings_se['dataset']['params']['melspectrogram_parameters']['fmax'] = 16000 # Highest frequency to use (usually sr / 2)\nsettings_se['dataset']['params']['melspectrogram_parameters']['n_fft'] = 1792 # Usually n_mels * 8\nsettings_se['dataset']['params']['melspectrogram_parameters']['hop_length'] = 714 # usually len(sig) / n_mels, so 32000*5 / n_mels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"settings_mixup = {} # A dictionary to hold the settings_mixup, easier to understand than yaml...\n\n# Used for generating the MelSpectrogram image\nsettings_mixup['dataset'] = {}\nsettings_mixup['dataset']['params'] = {}\nsettings_mixup['dataset']['params']['img_size'] = 260 # The image size of the spectrogram that we are CNN'ing over\nsettings_mixup['dataset']['params']['melspectrogram_parameters'] = {} # https://librosa.org/doc/latest/generated/librosa.filters.mel.html#librosa.filters.mel\nsettings_mixup['dataset']['params']['melspectrogram_parameters']['n_mels'] = 208 # The number of Melspectrograms bands to create. the higher this is, the more compelx.\nsettings_mixup['dataset']['params']['melspectrogram_parameters']['fmin'] = 2000 # Lowest frequency to use (usually 0)\nsettings_mixup['dataset']['params']['melspectrogram_parameters']['fmax'] = 15000 # Highest frequency to use (usually sr / 2)\nsettings_mixup['dataset']['params']['melspectrogram_parameters']['n_fft'] = 1664 # Usually n_mels * 8\nsettings_mixup['dataset']['params']['melspectrogram_parameters']['hop_length'] = 769 # usually len(sig) / n_mels, so 32000*5 / n_mels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"settings_ls = {} # A dictionary to hold the settings_ls, easier to understand than yaml...\n\n# Used for generating the MelSpectrogram image\nsettings_ls['dataset'] = {}\nsettings_ls['dataset']['params'] = {}\nsettings_ls['dataset']['params']['img_size'] = 260 # The image size of the spectrogram that we are CNN'ing over\nsettings_ls['dataset']['params']['melspectrogram_parameters'] = {} # https://librosa.org/doc/latest/generated/librosa.filters.mel.html#librosa.filters.mel\nsettings_ls['dataset']['params']['melspectrogram_parameters']['n_mels'] = 168 # The number of Melspectrograms bands to create. the higher this is, the more compelx.\nsettings_ls['dataset']['params']['melspectrogram_parameters']['fmin'] = 800 # Lowest frequency to use (usually 0)\nsettings_ls['dataset']['params']['melspectrogram_parameters']['fmax'] = 14000 # Highest frequency to use (usually sr / 2)\nsettings_ls['dataset']['params']['melspectrogram_parameters']['n_fft'] = 1344 # Usually n_mels * 8\nsettings_ls['dataset']['params']['melspectrogram_parameters']['hop_length'] = 1024 # usually len(sig) / n_mels, so 32000*5 / n_mels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def mono_to_color(X: np.ndarray,\n                  eps=1e-6):\n    \"\"\"\n    Code from https://www.kaggle.com/daisukelab/creating-fat2019-preprocessed-data\n    \"\"\"\n    # Stack X as [X,X,X]\n    X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = X.mean()\n    X = X - mean\n    std = X.std()\n    Xstd = X / (std + eps)\n    norm_min, norm_max = Xstd.min(), Xstd.max()\n    if (norm_max - norm_min) > eps:\n        # Normalize to [0, 255]\n        V = Xstd\n        V = 255 * (V - norm_min) / (norm_max - norm_min)\n        V = V.astype(np.uint8)\n    else:\n        # Just zero\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n\n\nclass TestDataset(data.Dataset):\n    def __init__(self, df: pd.DataFrame, clip: np.ndarray,\n                 img_size=300, melspectrogram_parameters={}, imgsize_resnest=224, resnest_melspec_params={},\n                 img_size_b1=240, melspec_b1= {}, img_size_thefirst=260, melspec_thefirst={}, \n                 img_size_thesecond=260, melspec_thesecond={},\n                 img_size_inception=299, melspec_inception={},\n                 #img_size_res50_melspec=224, melspec_res50_melspec={}):\n                 img_size_se=224, melspec_se={},\n                 img_size_mixup=260, melspec_mixup={},\n                 img_size_ls=260, melspec_ls={}\n                ):\n        self.df = df\n        self.clip = clip\n        self.img_size = img_size\n        self.melspectrogram_parameters = melspectrogram_parameters\n        self.imgsize_resnest = imgsize_resnest\n        self.resnest_melspec_params = resnest_melspec_params\n        self.imgsize_b1 = img_size_b1\n        self.b1_params = melspec_b1\n        self.imgsize_thefirst = img_size_thefirst\n        self.melspec_thefirst = melspec_thefirst\n        self.imgsize_thesecond = img_size_thesecond\n        self.melspec_thesecond = melspec_thesecond\n        self.imgsize_inception = img_size_inception\n        self.melspec_inception = melspec_inception\n        #self.imgsize_res50_melspec = img_size_res50_melspec\n        #self.melspec_res50_melspec = melspec_res50_melspec\n        self.imgsize_se = img_size_se\n        self.melspec_se = melspec_se\n        self.imgsize_mixup = img_size_mixup\n        self.melspec_mixup = melspec_mixup\n        self.imgsize_ls = img_size_ls\n        self.melspec_ls = melspec_ls\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx: int):\n        SR = 32000\n        sample = self.df.loc[idx, :]\n        site = sample.site\n        row_id = sample.row_id\n        \n        if site == \"site_3\":\n            y = self.clip.astype(np.float32)\n            len_y = len(y)\n            start = 0\n            end = SR * 5\n            images = []\n            resnest_images = []\n            images_b1 = []\n            images_thefirst = []\n            images_thesecond = []\n            images_inception = []\n            #images_res50_melspec = []\n            images_se = []\n            images_mixup = []\n            images_ls = []\n            while len_y > start:\n                y_batch = y[start:end].astype(np.float32)\n                if len(y_batch) != (SR * 5):\n                    break\n                start = end\n                end = end + SR * 5\n                \n                # Efficientnet B3 melspec+PCEN\n                melspec = librosa.feature.melspectrogram(y_batch,\n                                                         sr=SR,\n                                                         **self.melspectrogram_parameters)\n                melspec = librosa.pcen(melspec, sr=SR, hop_length=settings['dataset']['params']['melspectrogram_parameters']['hop_length'],gain=0.80,bias=10,power=0.5,time_constant=0.06,eps=1e-6)\n                image = mono_to_color(melspec)\n                height, width, _ = image.shape\n                image = cv2.resize(image, (int(width * self.img_size / height), self.img_size))\n                image = np.moveaxis(image, 2, 0)\n                image = (image / 255.0).astype(np.float32)\n                images.append(image)\n                \n                # Resnest melspec\n                melspec = librosa.feature.melspectrogram(y_batch,\n                                                         sr=SR,\n                                                         **self.resnest_melspec_params)\n                melspec = librosa.power_to_db(melspec).astype(np.float32)\n                image = mono_to_color(melspec)\n                height, width, _ = image.shape\n                image = cv2.resize(image, (int(width * self.imgsize_resnest / height), self.imgsize_resnest))\n                image = np.moveaxis(image, 2, 0)\n                image = (image / 255.0).astype(np.float32)\n                resnest_images.append(image)\n                \n                # Efficientnet B1 melspec+PCEN}\n                melspec = librosa.feature.melspectrogram(y_batch,\n                                                         sr=SR,\n                                                         **self.b1_params)\n                melspec = librosa.pcen(melspec, sr=SR, hop_length=settings_effnetb1['dataset']['params']['melspectrogram_parameters']['hop_length'],gain=0.80,bias=10,power=0.5,time_constant=0.06,eps=1e-6)\n                image = mono_to_color(melspec)\n                height, width, _ = image.shape\n                image = cv2.resize(image, (int(width * self.imgsize_b1 / height), self.imgsize_b1))\n                image = np.moveaxis(image, 2, 0)\n                image = (image / 255.0).astype(np.float32)\n                images_b1.append(image)\n                \n                # EfficientNet B2s\n                melspec = librosa.feature.melspectrogram(y_batch,\n                                                         sr=SR,\n                                                         **self.melspec_thefirst)\n                melspec = librosa.power_to_db(melspec).astype(np.float32)\n                image = mono_to_color(melspec)\n                height, width, _ = image.shape\n                image = cv2.resize(image, (int(width * self.imgsize_thefirst / height), self.imgsize_thefirst))\n                image = np.moveaxis(image, 2, 0)\n                image = (image / 255.0).astype(np.float32)\n                images_thefirst.append(image)\n                ####\n                melspec = librosa.feature.melspectrogram(y_batch,\n                                                         sr=SR,\n                                                         **self.melspec_thesecond)\n                melspec = librosa.power_to_db(melspec).astype(np.float32)\n                image = mono_to_color(melspec)\n                height, width, _ = image.shape\n                image = cv2.resize(image, (int(width * self.imgsize_thesecond / height), self.imgsize_thesecond))\n                image = np.moveaxis(image, 2, 0)\n                image = (image / 255.0).astype(np.float32)\n                images_thesecond.append(image)\n                \n                # Inception V4\n                melspec = librosa.feature.melspectrogram(y_batch,\n                                                         sr=SR,\n                                                         **self.melspec_inception)\n                melspec = librosa.power_to_db(melspec).astype(np.float32)\n                image = mono_to_color(melspec)\n                height, width, _ = image.shape\n                image = cv2.resize(image, (int(width * self.imgsize_inception / height), self.imgsize_inception))\n                image = np.moveaxis(image, 2, 0)\n                image = (image / 255.0).astype(np.float32)\n                images_inception.append(image)\n                \n                # Seresnext50\n                melspec = librosa.feature.melspectrogram(y_batch,\n                                                         sr=SR,\n                                                         **self.melspec_se)\n                melspec = librosa.power_to_db(melspec).astype(np.float32)\n                image = mono_to_color(melspec)\n                height, width, _ = image.shape\n                image = cv2.resize(image, (int(width * self.imgsize_se / height), self.imgsize_se))\n                image = np.moveaxis(image, 2, 0)\n                image = (image / 255.0).astype(np.float32)\n                images_se.append(image)\n                \n                # EfficientnetB2 Mixup\n                melspec = librosa.feature.melspectrogram(y_batch,\n                                                         sr=SR,\n                                                         **self.melspec_mixup)\n                melspec = librosa.pcen(melspec, sr=SR, hop_length=settings_mixup['dataset']['params']['melspectrogram_parameters']['hop_length'],gain=0.80,bias=10,power=0.5,time_constant=0.06,eps=1e-6)\n                image = mono_to_color(melspec)\n                height, width, _ = image.shape\n                image = cv2.resize(image, (int(width * self.imgsize_mixup / height), self.imgsize_mixup))\n                image = np.moveaxis(image, 2, 0)\n                image = (image / 255.0).astype(np.float32)\n                images_mixup.append(image)\n                \n                # EfficientnetB2 Labelsmooth\n                melspec = librosa.feature.melspectrogram(y_batch,\n                                                         sr=SR,\n                                                         **self.melspec_ls)\n                melspec = librosa.pcen(melspec, sr=SR, hop_length=settings_ls['dataset']['params']['melspectrogram_parameters']['hop_length'],gain=0.80,bias=10,power=0.5,time_constant=0.06,eps=1e-6)\n                image = mono_to_color(melspec)\n                height, width, _ = image.shape\n                image = cv2.resize(image, (int(width * self.imgsize_ls / height), self.imgsize_ls))\n                image = np.moveaxis(image, 2, 0)\n                image = (image / 255.0).astype(np.float32)\n                images_ls.append(image)\n                \n                # Resnest50 (Melspec + Powertodb)\n#                 melspec = librosa.feature.melspectrogram(y_batch,\n#                                                          sr=SR,\n#                                                          **self.melspec_res50_melspec)\n#                 melspec = librosa.power_to_db(melspec).astype(np.float32)\n#                 image = mono_to_color(melspec)\n#                 height, width, _ = image.shape\n#                 image = cv2.resize(image, (int(width * self.imgsize_res50_melspec / height), self.imgsize_res50_melspec))\n#                 image = np.moveaxis(image, 2, 0)\n#                 image = (image / 255.0).astype(np.float32)\n#                 images_res50_melspec.append(image)\n                \n                \n            images = np.asarray(images)\n            resnest_images = np.asarray(resnest_images)\n            images_b1 = np.array(images_b1)\n            images_thefirst = np.array(images_thefirst)\n            images_thesecond = np.array(images_thesecond)\n            images_inception = np.array(images_inception)\n            #images_res50_melspec = np.array(images_res50_melspec)\n            images_se = np.array(images_se)\n            images_mixup = np.array(images_mixup)\n            images_ls = np.array(images_ls)\n            return images, row_id, site, resnest_images, images_b1, images_thefirst, images_thesecond, images_inception, images_se, images_mixup, images_ls\n        else:\n            end_seconds = int(sample.seconds)\n            start_seconds = int(end_seconds - 5)\n            \n            start_index = SR * start_seconds\n            end_index = SR * end_seconds\n            \n            y = self.clip[start_index:end_index].astype(np.float32)\n\n            # Efficientnet B3 Melspec+PCEN\n            melspec = librosa.feature.melspectrogram(y, sr=SR, **self.melspectrogram_parameters)\n            #melspec = librosa.power_to_db(melspec).astype(np.float32)\n            melspec = librosa.pcen(melspec, sr=SR, hop_length=settings['dataset']['params']['melspectrogram_parameters']['hop_length'],gain=0.80,bias=10,power=0.5,time_constant=0.06,eps=1e-6)\n\n            image = mono_to_color(melspec)\n            height, width, _ = image.shape\n            image = cv2.resize(image, (int(width * self.img_size / height), self.img_size))\n            image = np.moveaxis(image, 2, 0)\n            image = (image / 255.0).astype(np.float32)\n            \n            # Resnest melspec\n            melspec = librosa.feature.melspectrogram(y, sr=SR, **self.resnest_melspec_params)\n            melspec = librosa.power_to_db(melspec).astype(np.float32)\n\n            image2 = mono_to_color(melspec)\n            height, width, _ = image2.shape\n            image2 = cv2.resize(image2, (int(width * self.imgsize_resnest / height), self.imgsize_resnest))\n            image2 = np.moveaxis(image2, 2, 0)\n            image2 = (image2 / 255.0).astype(np.float32)\n            \n            # Efficientnet B1 Melspec+PCEN\n            melspec = librosa.feature.melspectrogram(y, sr=SR, **self.b1_params)\n            #melspec = librosa.power_to_db(melspec).astype(np.float32)\n            melspec = librosa.pcen(melspec, sr=SR, hop_length=settings_effnetb1['dataset']['params']['melspectrogram_parameters']['hop_length'],gain=0.80,bias=10,power=0.5,time_constant=0.06,eps=1e-6)\n\n            image3 = mono_to_color(melspec)\n            height, width, _ = image3.shape\n            image3 = cv2.resize(image3, (int(width * self.imgsize_b1 / height), self.imgsize_b1))\n            image3 = np.moveaxis(image3, 2, 0)\n            image3 = (image3 / 255.0).astype(np.float32)\n            \n            # Efficientnet B2's\n            melspec = librosa.feature.melspectrogram(y, sr=SR, **self.melspec_thefirst)\n            melspec = librosa.power_to_db(melspec).astype(np.float32)\n\n            image4 = mono_to_color(melspec)\n            height, width, _ = image4.shape\n            image4 = cv2.resize(image4, (int(width * self.imgsize_thefirst / height), self.imgsize_thefirst))\n            image4 = np.moveaxis(image4, 2, 0)\n            image4 = (image4 / 255.0).astype(np.float32)\n            #####\n            melspec = librosa.feature.melspectrogram(y, sr=SR, **self.melspec_thesecond)\n            melspec = librosa.power_to_db(melspec).astype(np.float32)\n\n            image5 = mono_to_color(melspec)\n            height, width, _ = image5.shape\n            image5 = cv2.resize(image5, (int(width * self.imgsize_thesecond / height), self.imgsize_thesecond))\n            image5 = np.moveaxis(image5, 2, 0)\n            image5 = (image5 / 255.0).astype(np.float32)\n\n            # Inception V4\n            melspec = librosa.feature.melspectrogram(y, sr=SR, **self.melspec_inception)\n            melspec = librosa.power_to_db(melspec).astype(np.float32)\n\n            image6 = mono_to_color(melspec)\n            height, width, _ = image6.shape\n            image6 = cv2.resize(image6, (int(width * self.imgsize_inception / height), self.imgsize_inception))\n            image6 = np.moveaxis(image6, 2, 0)\n            image6 = (image6 / 255.0).astype(np.float32)\n            \n            # Seresnext50\n            melspec = librosa.feature.melspectrogram(y, sr=SR, **self.melspec_se)\n            melspec = librosa.power_to_db(melspec).astype(np.float32)\n\n            image7 = mono_to_color(melspec)\n            height, width, _ = image7.shape\n            image7 = cv2.resize(image7, (int(width * self.imgsize_se / height), self.imgsize_se))\n            image7 = np.moveaxis(image7, 2, 0)\n            image7 = (image7 / 255.0).astype(np.float32)\n            \n            # EfficientnetB2 Mixup\n            melspec = librosa.feature.melspectrogram(y,\n                                                     sr=SR,\n                                                     **self.melspec_mixup)\n            melspec = librosa.pcen(melspec, sr=SR, hop_length=settings_mixup['dataset']['params']['melspectrogram_parameters']['hop_length'],gain=0.80,bias=10,power=0.5,time_constant=0.06,eps=1e-6)\n            image12 = mono_to_color(melspec)\n            height, width, _ = image12.shape\n            image12 = cv2.resize(image12, (int(width * self.imgsize_mixup / height), self.imgsize_mixup))\n            image12 = np.moveaxis(image12, 2, 0)\n            image12 = (image12 / 255.0).astype(np.float32)\n            \n            # EfficientnetB2 Labelsmooth\n            melspec = librosa.feature.melspectrogram(y,\n                                                     sr=SR,\n                                                     **self.melspec_ls)\n            melspec = librosa.pcen(melspec, sr=SR, hop_length=settings_ls['dataset']['params']['melspectrogram_parameters']['hop_length'],gain=0.80,bias=10,power=0.5,time_constant=0.06,eps=1e-6)\n            image13 = mono_to_color(melspec)\n            height, width, _ = image13.shape\n            image13 = cv2.resize(image13, (int(width * self.imgsize_ls / height), self.imgsize_ls))\n            image13 = np.moveaxis(image13, 2, 0)\n            image13 = (image13 / 255.0).astype(np.float32)\n            \n#             # Resnest50 (Melspec + PowerToDb)\n#             melspec = librosa.feature.melspectrogram(y, sr=SR, **self.melspec_res50_melspec)\n#             melspec = librosa.power_to_db(melspec).astype(np.float32)\n\n#             image7 = mono_to_color(melspec)\n#             height, width, _ = image7.shape\n#             image7 = cv2.resize(image7, (int(width * self.imgsize_res50_melspec / height), self.imgsize_res50_melspec))\n#             image7 = np.moveaxis(image7, 2, 0)\n#             image7 = (image7 / 255.0).astype(np.float32)\n            return image, row_id, site, image2, image3, image4, image5, image6, image7, image12, image13","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### model\n\n* I forked this code from authors' original implementation. [GitHub](https://github.com/zhanghang1989/ResNeSt)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class SplAtConv2d(Module):\n    \"\"\"Split-Attention Conv2d\n    \"\"\"\n    def __init__(self, in_channels, channels, kernel_size, stride=(1, 1), padding=(0, 0),\n                 dilation=(1, 1), groups=1, bias=True,\n                 radix=2, reduction_factor=4,\n                 rectify=False, rectify_avg=False, norm_layer=None,\n                 dropblock_prob=0.0, **kwargs):\n        super(SplAtConv2d, self).__init__()\n        padding = _pair(padding)\n        self.rectify = rectify and (padding[0] > 0 or padding[1] > 0)\n        self.rectify_avg = rectify_avg\n        inter_channels = max(in_channels*radix//reduction_factor, 32)\n        self.radix = radix\n        self.cardinality = groups\n        self.channels = channels\n        self.dropblock_prob = dropblock_prob\n        if self.rectify:\n            from rfconv import RFConv2d\n            self.conv = RFConv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n                                 groups=groups*radix, bias=bias, average_mode=rectify_avg, **kwargs)\n        else:\n            self.conv = Conv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n                               groups=groups*radix, bias=bias, **kwargs)\n        self.use_bn = norm_layer is not None\n        if self.use_bn:\n            self.bn0 = norm_layer(channels*radix)\n        self.relu = ReLU(inplace=True)\n        self.fc1 = Conv2d(channels, inter_channels, 1, groups=self.cardinality)\n        if self.use_bn:\n            self.bn1 = norm_layer(inter_channels)\n        self.fc2 = Conv2d(inter_channels, channels*radix, 1, groups=self.cardinality)\n        if dropblock_prob > 0.0:\n            self.dropblock = DropBlock2D(dropblock_prob, 3)\n        self.rsoftmax = rSoftMax(radix, groups)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.use_bn:\n            x = self.bn0(x)\n        if self.dropblock_prob > 0.0:\n            x = self.dropblock(x)\n        x = self.relu(x)\n\n        batch, rchannel = x.shape[:2]\n        if self.radix > 1:\n            if torch.__version__ < '1.5':\n                splited = torch.split(x, int(rchannel//self.radix), dim=1)\n            else:\n                splited = torch.split(x, rchannel//self.radix, dim=1)\n            gap = sum(splited) \n        else:\n            gap = x\n        gap = F.adaptive_avg_pool2d(gap, 1)\n        gap = self.fc1(gap)\n\n        if self.use_bn:\n            gap = self.bn1(gap)\n        gap = self.relu(gap)\n\n        atten = self.fc2(gap)\n        atten = self.rsoftmax(atten).view(batch, -1, 1, 1)\n\n        if self.radix > 1:\n            if torch.__version__ < '1.5':\n                attens = torch.split(atten, int(rchannel//self.radix), dim=1)\n            else:\n                attens = torch.split(atten, rchannel//self.radix, dim=1)\n            out = sum([att*split for (att, split) in zip(attens, splited)])\n        else:\n            out = atten * x\n        return out.contiguous()\n\nclass rSoftMax(nn.Module):\n    def __init__(self, radix, cardinality):\n        super().__init__()\n        self.radix = radix\n        self.cardinality = cardinality\n\n    def forward(self, x):\n        batch = x.size(0)\n        if self.radix > 1:\n            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n            x = F.softmax(x, dim=1)\n            x = x.reshape(batch, -1)\n        else:\n            x = torch.sigmoid(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class DropBlock2D(object):\n    def __init__(self, *args, **kwargs):\n        raise NotImplementedError\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, inputs):\n        return nn.functional.adaptive_avg_pool2d(inputs, 1).view(inputs.size(0), -1)\n\nclass Bottleneck(nn.Module):\n    \"\"\"ResNet Bottleneck\n    \"\"\"\n    # pylint: disable=unused-argument\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 radix=1, cardinality=1, bottleneck_width=64,\n                 avd=False, avd_first=False, dilation=1, is_first=False,\n                 rectified_conv=False, rectify_avg=False,\n                 norm_layer=None, dropblock_prob=0.0, last_gamma=False):\n        super(Bottleneck, self).__init__()\n        group_width = int(planes * (bottleneck_width / 64.)) * cardinality\n        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(group_width)\n        self.dropblock_prob = dropblock_prob\n        self.radix = radix\n        self.avd = avd and (stride > 1 or is_first)\n        self.avd_first = avd_first\n\n        if self.avd:\n            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)\n            stride = 1\n\n        if dropblock_prob > 0.0:\n            self.dropblock1 = DropBlock2D(dropblock_prob, 3)\n            if radix == 1:\n                self.dropblock2 = DropBlock2D(dropblock_prob, 3)\n            self.dropblock3 = DropBlock2D(dropblock_prob, 3)\n\n        if radix >= 1:\n            self.conv2 = SplAtConv2d(\n                group_width, group_width, kernel_size=3,\n                stride=stride, padding=dilation,\n                dilation=dilation, groups=cardinality, bias=False,\n                radix=radix, rectify=rectified_conv,\n                rectify_avg=rectify_avg,\n                norm_layer=norm_layer,\n                dropblock_prob=dropblock_prob)\n        elif rectified_conv:\n            from rfconv import RFConv2d\n            self.conv2 = RFConv2d(\n                group_width, group_width, kernel_size=3, stride=stride,\n                padding=dilation, dilation=dilation,\n                groups=cardinality, bias=False,\n                average_mode=rectify_avg)\n            self.bn2 = norm_layer(group_width)\n        else:\n            self.conv2 = nn.Conv2d(\n                group_width, group_width, kernel_size=3, stride=stride,\n                padding=dilation, dilation=dilation,\n                groups=cardinality, bias=False)\n            self.bn2 = norm_layer(group_width)\n\n        self.conv3 = nn.Conv2d(\n            group_width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes*4)\n\n        if last_gamma:\n            from torch.nn.init import zeros_\n            zeros_(self.bn3.weight)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.dilation = dilation\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock1(out)\n        out = self.relu(out)\n\n        if self.avd and self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv2(out)\n        if self.radix == 0:\n            out = self.bn2(out)\n            if self.dropblock_prob > 0.0:\n                out = self.dropblock2(out)\n            out = self.relu(out)\n\n        if self.avd and not self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n    \"\"\"ResNet Variants\n    Parameters\n    ----------\n    block : Block\n        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n    layers : list of int\n        Numbers of layers in each block\n    classes : int, default 1000\n        Number of classification classes.\n    dilated : bool, default False\n        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n        typically used in Semantic Segmentation.\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    Reference:\n        - He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n        - Yu, Fisher, and Vladlen Koltun. \"Multi-scale context aggregation by dilated convolutions.\"\n    \"\"\"\n    # pylint: disable=unused-variable\n    def __init__(self, block, layers, radix=1, groups=1, bottleneck_width=64,\n                 num_classes=1000, dilated=False, dilation=1,\n                 deep_stem=False, stem_width=64, avg_down=False,\n                 rectified_conv=False, rectify_avg=False,\n                 avd=False, avd_first=False,\n                 final_drop=0.0, dropblock_prob=0,\n                 last_gamma=False, norm_layer=nn.BatchNorm2d):\n        self.cardinality = groups\n        self.bottleneck_width = bottleneck_width\n        # ResNet-D params\n        self.inplanes = stem_width*2 if deep_stem else 64\n        self.avg_down = avg_down\n        self.last_gamma = last_gamma\n        # ResNeSt params\n        self.radix = radix\n        self.avd = avd\n        self.avd_first = avd_first\n\n        super(ResNet, self).__init__()\n        self.rectified_conv = rectified_conv\n        self.rectify_avg = rectify_avg\n        if rectified_conv:\n            from rfconv import RFConv2d\n            conv_layer = RFConv2d\n        else:\n            conv_layer = nn.Conv2d\n        conv_kwargs = {'average_mode': rectify_avg} if rectified_conv else {}\n        if deep_stem:\n            self.conv1 = nn.Sequential(\n                conv_layer(3, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **conv_kwargs),\n                norm_layer(stem_width),\n                nn.ReLU(inplace=True),\n                conv_layer(stem_width, stem_width, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n                norm_layer(stem_width),\n                nn.ReLU(inplace=True),\n                conv_layer(stem_width, stem_width*2, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n            )\n        else:\n            self.conv1 = conv_layer(3, 64, kernel_size=7, stride=2, padding=3,\n                                   bias=False, **conv_kwargs)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        if dilated or dilation == 4:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n                                           dilation=2, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                           dilation=4, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n        elif dilation==2:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                           dilation=1, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                           dilation=2, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                           norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                           norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n        self.avgpool = GlobalAvgPool2d()\n        self.drop = nn.Dropout(final_drop) if final_drop > 0.0 else None\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, norm_layer):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None,\n                    dropblock_prob=0.0, is_first=True):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            down_layers = []\n            if self.avg_down:\n                if dilation == 1:\n                    down_layers.append(nn.AvgPool2d(kernel_size=stride, stride=stride,\n                                                    ceil_mode=True, count_include_pad=False))\n                else:\n                    down_layers.append(nn.AvgPool2d(kernel_size=1, stride=1,\n                                                    ceil_mode=True, count_include_pad=False))\n                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n                                             kernel_size=1, stride=1, bias=False))\n            else:\n                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n                                             kernel_size=1, stride=stride, bias=False))\n            down_layers.append(norm_layer(planes * block.expansion))\n            downsample = nn.Sequential(*down_layers)\n\n        layers = []\n        if dilation == 1 or dilation == 2:\n            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n                                radix=self.radix, cardinality=self.cardinality,\n                                bottleneck_width=self.bottleneck_width,\n                                avd=self.avd, avd_first=self.avd_first,\n                                dilation=1, is_first=is_first, rectified_conv=self.rectified_conv,\n                                rectify_avg=self.rectify_avg,\n                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n                                last_gamma=self.last_gamma))\n        elif dilation == 4:\n            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n                                radix=self.radix, cardinality=self.cardinality,\n                                bottleneck_width=self.bottleneck_width,\n                                avd=self.avd, avd_first=self.avd_first,\n                                dilation=2, is_first=is_first, rectified_conv=self.rectified_conv,\n                                rectify_avg=self.rectify_avg,\n                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n                                last_gamma=self.last_gamma))\n        else:\n            raise RuntimeError(\"=> unknown dilation size: {}\".format(dilation))\n\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes,\n                                radix=self.radix, cardinality=self.cardinality,\n                                bottleneck_width=self.bottleneck_width,\n                                avd=self.avd, avd_first=self.avd_first,\n                                dilation=dilation, rectified_conv=self.rectified_conv,\n                                rectify_avg=self.rectify_avg,\n                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n                                last_gamma=self.last_gamma))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        #x = x.view(x.size(0), -1)\n        x = torch.flatten(x, 1)\n        if self.drop:\n            x = self.drop(x)\n        x = self.fc(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def prediction_for_clip(test_df: pd.DataFrame, \n                        clip: np.ndarray, \n                        model, # EffNetB3 + PCEN, trained 50-60 epochs\n                        #model2: EfficientNet,\n                        #model3: EfficientNet,\n                        model4, # EffNetB3 + PCEN, trained 60-70 epochs + RohanRao External\n                        #model5: EfficientNet,\n                        #model6: EfficientNet,\n                        #model7: EfficientNet, # EffNetB3 + PCEN, trained 60-70 epochs\n                        #model8: EfficientNet,\n                        #model9: EfficientNet,\n                        #model10: EfficientNet, # EffNetB3 + PCEN, trained 40-50 epochs\n                        #model11: EfficientNet,\n                        #model12: EfficientNet,\n                        model13, # EffNetB1 + PCEN, trained 40-45 epochs\n                        #model14: EfficientNet,\n                        #model15: EfficientNet,\n                        model16, # EffnetB2\n                        #model17: EfficientNet,\n                        #model18: EfficientNet,\n                        model19, # EffnetB2\n                        #model20: EfficientNet,\n                        #model21: EfficientNet,\n                        model_inception,\n                        model_inception2,\n                        #model_res50_melspec,\n                        #model_res50_melspec2,\n                        model_se,\n                        model_mixup,\n                        model_ls,\n                        model_resnest,\n                        mel_params: dict, \n                        mel_params_resnest: dict,\n                        mel_params_b1: dict,\n                        mel_params_thefirst: dict,\n                        mel_params_thesecond: dict,\n                        mel_params_inception: dict,\n                        #mel_params_res50_melspec: dict,\n                        mel_params_se: dict,\n                        mel_params_mixup: dict,\n                        mel_params_ls: dict,\n                        threshold=0.5,\n                        maxpreds=3, # New param --> @kkiller\n                       ):\n    \n    \"\"\"\n    Original code:  @hidehisaarai1213\n    First refacto : @ttahara\n    Second refacto: @kkiller\n    \"\"\"\n\n    dataset = TestDataset(df=test_df, \n                          clip=clip,\n                          img_size=300,\n                          melspectrogram_parameters=mel_params,\n                         imgsize_resnest=224,\n                         resnest_melspec_params=mel_params_resnest,\n                         img_size_b1=240, melspec_b1= mel_params_b1,\n                         img_size_thefirst=260,melspec_thefirst=mel_params_thefirst,\n                         img_size_thesecond=260,melspec_thesecond=mel_params_thesecond,\n                         img_size_inception=299, melspec_inception=mel_params_inception,\n                         #img_size_res50_melspec=224, melspec_res50_melspec=mel_params_res50_melspec)\n                          img_size_se=224, melspec_se=mel_params_se,\n                          img_size_mixup=260, melspec_mixup = mel_params_mixup,\n                          img_size_ls=260, melspec_ls = mel_params_ls\n                         )\n    \n    loader = data.DataLoader(dataset, batch_size=1, shuffle=False)\n    \n    prediction_dict = {}\n    for image, row_id, site, image2, image3, image4, image5, image6, image7, image8, image9 in loader:\n        site = site[0]\n        row_id = row_id[0]\n        if site in {\"site_1\", \"site_2\"}:\n            #continue\n            #image = image.to(device)\n            #image2 = image2.to(device)\n            #image3 = image3.to(device)\n            #image4 = image4.to(device)\n            #image5 = image5.to(device)\n            \n            with torch.no_grad():\n            \n                inps1 = {model.get_inputs()[0].name: to_numpy(image)}\n                inps2 = {model4.get_inputs()[0].name: to_numpy(image)}\n                inps3 = {model13.get_inputs()[0].name: to_numpy(image3)}\n                inps4 = {model16.get_inputs()[0].name: to_numpy(image4)}\n                inps5 = {model19.get_inputs()[0].name: to_numpy(image5)}\n                inps6 = {model_resnest.get_inputs()[0].name: to_numpy(image2)}\n                inps7 = {model_inception.get_inputs()[0].name: to_numpy(image6)}\n                inps8 = {model_inception2.get_inputs()[0].name: to_numpy(image6)}\n                inps9 = {model_se.get_inputs()[0].name: to_numpy(image7)}\n                inps10 = {model_mixup.get_inputs()[0].name: to_numpy(image8)}\n                inps11 = {model_ls.get_inputs()[0].name: to_numpy(image9)}\n                #inps9 = {model_res50_melspec.get_inputs()[0].name: to_numpy(image7)}\n                #inps10 = {model_res50_melspec2.get_inputs()[0].name: to_numpy(image7)}\n\n                outs1 = model.run(None, inps1)\n                outs2 = model4.run(None, inps2)\n                outs3 = model13.run(None, inps3)\n                outs4 = model16.run(None, inps4)\n                outs5 = model19.run(None, inps5)\n                outs6 = model_resnest.run(None, inps6)\n                outs7 = model_inception.run(None, inps7)\n                outs8 = model_inception2.run(None, inps8)\n                outs9 = model_se.run(None, inps9)\n                outs10 = model_mixup.run(None, inps10)\n                outs11 = model_ls.run(None, inps11)\n                #outs9 = model_res50_melspec.run(None, inps9)\n                #outs10 = model_res50_melspec2.run(None, inps10)\n\n                outs1 = outs1[0][0]\n                outs2 = outs2[0][0]\n                outs3 = outs3[0][0]\n                outs4 = outs4[0][0]\n                outs5 = outs5[0][0]\n                outs6 = outs6[0][0]\n                outs7 = outs7[0][0]\n                outs8 = outs8[0][0]\n                outs9 = outs9[0][0]\n                outs10 = outs10[0][0]\n                outs11 = outs11[0][0]\n                #outs9 = outs9[0][0]\n                #outs10 = outs10[0][0]\n\n                # Sigmoid\n                outs1 = 1/(1 + np.exp(-outs1))\n                outs2 = 1/(1 + np.exp(-outs2))\n                outs3 = 1/(1 + np.exp(-outs3))\n                outs4 = 1/(1 + np.exp(-outs4))\n                outs5 = 1/(1 + np.exp(-outs5))\n                outs6 = 1/(1 + np.exp(-outs6))\n                outs7 = 1/(1 + np.exp(-outs7))\n                outs8 = 1/(1 + np.exp(-outs8))\n                outs9 = 1/(1 + np.exp(-outs9))\n                outs10 = 1/(1 + np.exp(-outs10))\n                outs11 = 1/(1 + np.exp(-outs11))\n                #outs9 = 1/(1 + np.exp(-outs9))\n                #outs10 = 1/(1 + np.exp(-outs10))\n\n                # Square\n                outs1 = outs1**2\n                outs2 = outs2**2\n                outs3 = outs3**2\n                outs4 = outs4**2\n                outs5 = outs5**2\n                outs6 = outs6**2\n                outs7 = outs7**2\n                outs8 = outs8**2\n                outs9 = outs9**2\n                outs10 = outs10**2\n                outs11 = outs11**2\n                #outs9 = outs9**2\n                #outs10 = outs10**2\n\n                # Mean\n                proba = np.sqrt((outs1+outs2+outs3+outs4+outs5+outs6+outs7+outs8+outs9+outs10+outs11)/11)\n\n            events = proba >= threshold\n            labels = np.argsort(-proba)[:events.sum()].tolist()\n\n        else:\n            # to avoid prediction on large batch\n            image = image.squeeze(0)\n            image2 = image2.squeeze(0)\n            image3 = image3.squeeze(0)\n            image4 = image4.squeeze(0)\n            image5 = image5.squeeze(0)\n            image6 = image6.squeeze(0)\n            image7 = image7.squeeze(0)\n            image8 = image8.squeeze(0)\n            image9 = image9.squeeze(0)\n            batch_size = 16\n            whole_size = image.size(0)\n            if whole_size % batch_size == 0:\n                n_iter = whole_size // batch_size\n            else:\n                n_iter = whole_size // batch_size + 1\n                \n            all_events = set()\n            probas = []\n            for batch_i in range(n_iter):\n                batch = image[batch_i * batch_size:(batch_i + 1) * batch_size]\n                batch2 = image2[batch_i * batch_size:(batch_i + 1) * batch_size]\n                batch3 = image3[batch_i * batch_size:(batch_i + 1) * batch_size]\n                batch4 = image4[batch_i * batch_size:(batch_i + 1) * batch_size]\n                batch5 = image5[batch_i * batch_size:(batch_i + 1) * batch_size]\n                batch6 = image6[batch_i * batch_size:(batch_i + 1) * batch_size]\n                batch7 = image7[batch_i * batch_size:(batch_i + 1) * batch_size]\n                batch8 = image8[batch_i * batch_size:(batch_i + 1) * batch_size]\n                batch9 = image9[batch_i * batch_size:(batch_i + 1) * batch_size]\n                if batch.ndim == 3:\n                    batch = batch.unsqueeze(0)\n                if batch2.ndim == 3:\n                    batch2 = batch2.unsqueeze(0)\n                if batch3.ndim == 3:\n                    batch3 = batch3.unsqueeze(0)\n                if batch4.ndim == 3:\n                    batch4 = batch4.unsqueeze(0)\n                if batch5.ndim == 3:\n                    batch5 = batch5.unsqueeze(0)\n                if batch6.ndim == 3:\n                    batch6 = batch6.unsqueeze(0)\n                if batch7.ndim == 3:\n                    batch7 = batch7.unsqueeze(0)\n                if batch8.ndim == 3:\n                    batch8 = batch8.unsqueeze(0)\n                if batch9.ndim == 3:\n                    batch9 = batch9.unsqueeze(0)\n\n#                 batch = batch.to(device)\n#                 batch2 = batch2.to(device)\n#                 batch3 = batch3.to(device)\n#                 batch4 = batch4.to(device)\n#                 batch5 = batch5.to(device)\n\n                with torch.no_grad():\n\n                    inps1 = {model.get_inputs()[0].name: to_numpy(batch)}\n                    inps2 = {model4.get_inputs()[0].name: to_numpy(batch)}\n                    inps3 = {model13.get_inputs()[0].name: to_numpy(batch3)}\n                    inps4 = {model16.get_inputs()[0].name: to_numpy(batch4)}\n                    inps5 = {model19.get_inputs()[0].name: to_numpy(batch5)}\n                    inps6 = {model_resnest.get_inputs()[0].name: to_numpy(batch2)}\n                    inps7 = {model_inception.get_inputs()[0].name: to_numpy(batch6)}\n                    inps8 = {model_inception2.get_inputs()[0].name: to_numpy(batch6)}\n                    inps9 = {model_se.get_inputs()[0].name: to_numpy(batch7)}\n                    inps10 = {model_mixup.get_inputs()[0].name: to_numpy(batch8)}\n                    inps11 = {model_ls.get_inputs()[0].name: to_numpy(batch9)}\n                    #inps9 = {model_res50_melspec.get_inputs()[0].name: to_numpy(batch7)}\n                    #inps10 = {model_res50_melspec2.get_inputs()[0].name: to_numpy(batch7)}\n\n                    outs1 = model.run(None, inps1)\n                    outs2 = model4.run(None, inps2)\n                    outs3 = model13.run(None, inps3)\n                    outs4 = model16.run(None, inps4)\n                    outs5 = model19.run(None, inps5)\n                    outs6 = model_resnest.run(None, inps6)\n                    outs7 = model_inception.run(None, inps7)\n                    outs8 = model_inception2.run(None, inps8)\n                    outs9 = model_se.run(None, inps9)\n                    outs10 = model_mixup.run(None, inps10)\n                    outs11 = model_ls.run(None, inps11)\n                    #outs9 = model_res50_melspec.run(None, inps9)\n                    #outs10 = model_res50_melspec2.run(None, inps10)\n\n                    del inps1,inps2,inps3,inps4,inps5,inps6,inps8,inps9,inps10,inps11\n                    gc.collect()\n\n                    outs1 = outs1[0]\n                    outs2 = outs2[0]\n                    outs3 = outs3[0]\n                    outs4 = outs4[0]\n                    outs5 = outs5[0]\n                    outs6 = outs6[0]\n                    outs7 = outs7[0]\n                    outs8 = outs8[0]\n                    outs9 = outs9[0]\n                    outs10 = outs10[0]\n                    outs11 = outs11[0]\n                    #outs9 = outs9[0]\n                    #outs10 = outs10[0]\n\n                    # Sigmoid\n                    outs1 = 1/(1 + np.exp(-outs1))\n                    outs2 = 1/(1 + np.exp(-outs2))\n                    outs3 = 1/(1 + np.exp(-outs3))\n                    outs4 = 1/(1 + np.exp(-outs4))\n                    outs5 = 1/(1 + np.exp(-outs5))\n                    outs6 = 1/(1 + np.exp(-outs6))\n                    outs7 = 1/(1 + np.exp(-outs7))\n                    outs8 = 1/(1 + np.exp(-outs8))\n                    outs9 = 1/(1 + np.exp(-outs9))\n                    outs10 = 1/(1 + np.exp(-outs10))\n                    outs11 = 1/(1 + np.exp(-outs11))\n                    #outs9 = 1/(1 + np.exp(-outs9))\n                    #outs10 = 1/(1 + np.exp(-outs10))\n\n                    # Square\n                    outs1 = outs1**2\n                    outs2 = outs2**2\n                    outs3 = outs3**2\n                    outs4 = outs4**2\n                    outs5 = outs5**2\n                    outs6 = outs6**2\n                    outs7 = outs7**2\n                    outs8 = outs8**2\n                    outs9 = outs9**2\n                    outs10 = outs10**2\n                    outs11 = outs11**2\n                    #outs9 = outs9**2\n                    #outs10 = outs10**2\n\n\n                    # Mean\n                    probas.append(np.sqrt((outs1+outs2+outs3+outs4+outs5+outs6+outs7+outs8+outs9+outs10+outs11)/11))\n                    del outs1,outs2,outs3,outs4,outs5,outs8,outs9,outs10,outs11\n                    gc.collect()\n                \n            probas = np.vstack(probas)\n            probas = probas.max(0)\n            events = (probas>=threshold)\n            labels = np.argsort(-probas)[:events.sum()].tolist()\n            \n        if len(labels) == 0:\n            prediction_dict[row_id] = \"nocall\"\n        else:\n            labels_str_list = list(map(lambda x: INV_BIRD_CODE[x], labels))\n            \n            # Only apply maxpreds to site1 and site2\n            if site in ('site_1','site_2'):\n                label_string = \" \".join(labels_str_list[:maxpreds])\n            else:\n                label_string = \" \".join(labels_str_list)\n            prediction_dict[row_id] = label_string\n    return prediction_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def prediction(test_df: pd.DataFrame,\n               test_audio: Path,\n               model_config: dict,\n               mel_params: dict,\n               resnest_melparams: dict,\n               b1_melparams: dict,\n               thefirst_melparams: dict,\n               thesecond_melparams: dict,\n               inception_melparams: dict,\n               #res50_melspec_melparams: dict,\n               se_melparams: dict,\n               mixup_melparams: dict,\n               ls_melparams: dict,\n               target_sr: int,\n               threshold=0.5,\n               maxpreds = 3, # New param --> @kkiller\n              ):\n    #model = get_model('../input/effnetb3-with-augs-v2-run3/epoch_51_valloss_3.985397423211764.pt')\n    model = onnxruntime.InferenceSession(\"../input/save-out-onnx-models/model.onnx\") #get_model('../input/effnetb3-with-augs-v2-run3/epoch_55_valloss_3.238097043361179.pt')\n    #model3 = get_model('../input/effnetb3-with-augs-v2-run3/epoch_59_valloss_3.8764151854444147.pt')\n    #model5 = get_model('../input/effnetb3-with-augs-v2-run4-vlad-rohan/epoch_71_valloss_0.1133052933314615.pt')\n    model5 = onnxruntime.InferenceSession(\"../input/save-out-onnx-models/model4.onnx\") # get_model('../input/effnetb3-with-augs-v2-run4-vlad-rohan/epoch_67_valloss_0.10827804547696074.pt')\n    #model7 = get_model('../input/effnetb3-with-augs-v2-run4-vlad-rohan/epoch_61_valloss_0.2174776398445949.pt')\n    #model8 = get_model('../input/effnetb3-with-augs-v2-run4/epoch_63_valloss_3.1640012733380876.pt')\n    #model9 = get_model('../input/effnetb3-with-augs-v2-run4/epoch_67_valloss_2.875856534533217.pt')\n    #model10 = get_model('../input/effnetb3-with-augs-v2-run4/epoch_71_valloss_3.2250966523378914.pt')\n    #model11 = get_model('../input/effnetb3-with-augs-v2-run3/epoch_41_valloss_3.654982790722728.pt')\n    #model12 = get_model('../input/effnetb3-with-augs-v2-run3/epoch_45_valloss_3.820303558173541.pt')\n    #model13 = get_model('../input/effnetb3-with-augs-v2-run3/epoch_49_valloss_3.7013871521043686.pt')\n    model14 = onnxruntime.InferenceSession(\"../input/save-out-onnx-models/model13.onnx\") # get_model_b1('../input/effnetb1-with-augs-v2-run2/epoch_47_valloss_0.144121603593846.pt')\n    #model15 = get_model_b1('../input/effnetb1-with-augs-v2-run2/epoch_41_valloss_0.13852256191663506.pt')\n    #model16 = get_model_b1('../input/effnetb1-with-augs-v2-run2/epoch_39_valloss_0.1459553754034121.pt')\n    #model17 = get_model_b2('../input/effnetb2-one/effnetb2_melspec_v2_epochepoch_49_valloss_0.060138213486710855.pt')\n    model17 = onnxruntime.InferenceSession(\"../input/save-out-onnx-models/model16.onnx\") # get_model_b2('../input/effnetb2-one/effnetb2_melspec_v2_epochepoch_47_valloss_0.062170032011575935.pt')\n    #model19 = get_model_b2('../input/effnetb2-one/effnetb2_melspec_v2_epochepoch_41_valloss_0.07619252517696254.pt')\n    #model20 = get_model_b2('../input/effnetb2-two/effnetb2_melspec_epochepoch_49_valloss_0.05903274682927723.pt')\n    model20 = onnxruntime.InferenceSession(\"../input/save-out-onnx-models/model19.onnx\") # get_model_b2('../input/effnetb2-two/effnetb2_melspec_epochepoch_47_valloss_0.06680513079018882.pt')\n    #model22 = get_model_b2('../input/effnetb2-two/effnetb2_melspec_epochepoch_43_valloss_0.050417204000717725.pt')\n    \n    model_inception = onnxruntime.InferenceSession('../input/save-out-onnx-models/model_inception.onnx')\n    model_inception2 = onnxruntime.InferenceSession('../input/save-out-onnx-models/model_inception2.onnx')\n    #model_res50_melspec = onnxruntime.InferenceSession('../input/save-out-onnx-models/model_res50_melspec.onnx')\n    #model_res50_melspec2 = onnxruntime.InferenceSession('../input/save-out-onnx-models/model_res50_melspec2.onnx')\n    model_se = onnxruntime.InferenceSession('../input/save-out-onnx-models/model_se.onnx')\n    model_mixup = onnxruntime.InferenceSession('../input/save-out-onnx-models/model_mixup.onnx')\n    model_ls = onnxruntime.InferenceSession('../input/save-out-onnx-models/model_ls.onnx')\n    model4 = onnxruntime.InferenceSession(\"../input/save-out-onnx-models/model_resnest.onnx\") #get_model_resnest(model_config)\n    unique_audio_id = test_df.audio_id.unique()\n\n    prediction_dfs = []\n    for audio_id in unique_audio_id:\n        clip, sr_native = librosa.core.audio.__audioread_load(test_audio / (audio_id + '.mp3'),\n                                                              offset=0,\n                                                              duration=None,\n                                                              dtype=np.float32)\n        clip = librosa.to_mono(clip)\n        cpmp_sr = 32000\n        if sr_native > 0:\n            clip = librosa.resample(clip, sr_native, cpmp_sr, res_type='kaiser_fast')\n        \n        test_df_for_audio_id = test_df.query(\n            f\"audio_id == '{audio_id}'\").reset_index(drop=True)\n        prediction_dict = prediction_for_clip(test_df_for_audio_id,\n                                              clip=clip,\n                                              model=model,\n                                              #model2=model2,\n                                              #model3=model3,\n                                              model4=model5,\n                                              #model5=model6,\n                                              #model6=model7,\n                                             # model7=model8,\n                                             # model8=model9,\n                                             # model9=model10,\n                                             # model10=model11,\n                                             # model11=model12,\n                                             # model12=model13,\n                                              model13=model14,\n                                              #model14=model15,\n                                              #model15=model16,\n                                              model16=model17,\n                                              #model17=model18,\n                                              #model18=model19,\n                                              model19=model20,\n                                              #model20=model21,\n                                              #model21=model22,\n                                              model_inception=model_inception,\n                                              model_inception2=model_inception2,\n                                              #model_res50_melspec=model_res50_melspec,\n                                              #model_res50_melspec2=model_res50_melspec2,\n                                              model_se=model_se,\n                                              model_mixup=model_mixup,\n                                              model_ls=model_ls,\n                                              model_resnest=model4,\n                                              mel_params=mel_params,\n                                              mel_params_resnest=resnest_melparams,\n                                              mel_params_b1 = b1_melparams,\n                                              mel_params_thefirst = thefirst_melparams,\n                                              mel_params_thesecond= thesecond_melparams,\n                                              mel_params_inception=inception_melparams,\n                                              #mel_params_res50_melspec=res50_melspec_melparams,\n                                              mel_params_se=se_melparams,\n                                              mel_params_mixup=mixup_melparams,\n                                              mel_params_ls=ls_melparams,\n                                              threshold=threshold,\n                                              maxpreds = maxpreds, # New param --> @kkiller\n                                             )\n\n            \n        row_id = list(prediction_dict.keys())\n        birds = list(prediction_dict.values())\n        prediction_df = pd.DataFrame({\n            \"row_id\": row_id,\n            \"birds\": birds\n        })\n        prediction_dfs.append(prediction_df)\n    \n    prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n    return prediction_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"submission = prediction(test_df=test,\n                        test_audio=TEST_AUDIO_DIR,\n                        model_config = model_config,\n                        mel_params=settings['dataset']['params']['melspectrogram_parameters'],\n                        resnest_melparams = melspectrogram_parameters,\n                        b1_melparams = settings_effnetb1['dataset']['params']['melspectrogram_parameters'],\n                        thefirst_melparams = settings_thefirst['dataset']['params']['melspectrogram_parameters'],\n                        thesecond_melparams = settings_thesecond['dataset']['params']['melspectrogram_parameters'],\n                        inception_melparams = settings_inception['dataset']['params']['melspectrogram_parameters'],\n                        #res50_melspec_melparams = settings_res50_melspec['dataset']['params']['melspectrogram_parameters'],\n                        se_melparams = settings_se['dataset']['params']['melspectrogram_parameters'],\n                        mixup_melparams=settings_mixup['dataset']['params']['melspectrogram_parameters'],\n                        ls_melparams=settings_ls['dataset']['params']['melspectrogram_parameters'],\n                        target_sr=TARGET_SR,\n                        threshold=0.33,\n                        maxpreds=3, # New param --> @kkiller\n                       )\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}