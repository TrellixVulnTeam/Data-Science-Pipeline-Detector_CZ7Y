{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align=\"center\">Categorical Feature Encoding Challenge</h1>\n\n<h2 align=\"center\"> Binary classification, with every feature a categorical</h2>\n\n<hr style=\"border: 1px solid #000\" width=\"70%\">\n\n<img src=\"https://storage.googleapis.com/kaggle-media/competitions/playground/cat_in_dat/cat7.jpg\" style=\"display: block; margin: 0 auto\" width=\"500px\" height=\"500px\"/>\n\n<hr style=\"border: 1px solid #000\">\n\n\n# Table of Content\n\n* [1. Introduction](#1.-Introduction)\n    * [1.1. Goal](#1.1.-Goal)\n    * [1.2. Libraries & Tools](#1.2.-Libraries-&-Tools)\n* [2. Exploratory Data Analysis](#2.-Exploratory-Data-Analysis)\n    * [2.1. Overview of the data](#2.1.-Overview-of-the-data)\n    * [2.2. The Categorical Variables](2.2.-The-Categorical-Variables)\n    * [2.3. Binary Variables](#2.3.-Binary-Variables)\n    * [2.4. Ordinal Variables](#2.4.--Ordinal-Variables)\n    * [2.5. Norminal Variables](#2.5.--Norminal-Variables)\n    * [2.6. Difference between Label Encoding and OneHotEncoding (OHE)](#2.6.-Difference-between-Label-Encoding-and-OneHotEncoding-(OHE))\n        * [2.6.1. Label Encoding](#2.6.1.-Label-Encoding)\n        * [2.6.2. OneHotEncoding](#2.6.2.-OneHotEncoding)\n    * [2.7. Day and Month Variables](#2.7.-Day-and-Month-Variables)\n* [3. Model Evaluation on Test-Set](#Model-Evaluation-on-Test-Set)\n* [4. Conclusion](#4.-Conclusion)\n* [References](#References)\n\n## Kindly upvote if you find the kernel helpful :) ","metadata":{}},{"cell_type":"markdown","source":"# 1. Introduction\n\nI started out this book [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) 2nd Edition by [Aurélien Géron](https://www.oreilly.com/people/aurelien-geron/), I wrote my first notebook on [housing prices]() after studing the second chapter, then me and my system were seperated for a while but now we're back together :). I hope to end this project as soon as I can. \n\nJust done with the thrid chapter and it all about classification in Machine Learning (<b style=\"color: #DE3163\">Classification</b>) Aurélien taught about binary and multi-class classification so like the last project am going to make use of this dataset for this challenge - []() - to practice on binary classification, and the second part of the project - []() - for multi-class clasification. So here we go again.\n\n**_Description and context:_**\n\n#### Is there a cat in your data?\n\nA common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured.\n\nBecause this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:\n\n* binary features\n* low- and high-cardinality nominal features\n* low- and high-cardinality ordinal features\n* (potentially) cyclical features\n\n## 1.1. Goals\n\nI want to start by appreciating [KDJ2020](https://www.kaggle.com/dkomyagin), [Shahules](https://www.kaggle.com/shahules) and [Prashant Manshani](https://www.kaggle.com/prazhant), their notebooks really helped me understand the various kinds of caregorical data and how to handle them. Their notebook also played no small part to my final score on the competition being <b style=\"color: #DE3163\">0.80464</b>. [Reference](#References) to their notebooks can be found below.\n\nMy goals in this **notebook** are to:\n\n1. Discover and visualize the data to gain insights.\n2. Prepare the data for Machine Learning algorithms.\n3. Select a model and train it.\n4. Fine-tune the model.\n5. Present my solution.\n\n## 1.2. Libraries & Tools","metadata":{}},{"cell_type":"code","source":"import scipy\nimport random\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gen_color():\n    \"\"\"\n    Generates a random a color of any sort.\n    \"\"\"\n    \n    return \"#\" + \"\".join([random.choice(list(\"0123456789abcdef\")) for _ in range(6)])\n\ndef pie_plot(column, data=None, title=None, ax=None, fontsize=15, explode=0,\n             autopct='%1.1f%%', shadow=None, figsize=(10, 6), colors=None, color_with_label=None):\n    \"\"\"\n    returns: a pie plot of the quantities of unique valiues from a column.\n    \"\"\"\n    \n    if type(column) == str:\n        target = data[column].value_counts()\n    else:\n        target = column.value_counts()\n    \n    explode = [explode for _ in range(len(target))]\n    \n    if color_with_label is not None:\n        colors = [color_with_label[key] for key in target.index]\n    elif color_with_label is None and colors is None:\n        colors = [gen_color() for _ in range(len(target))]\n        \n    if ax is not None:\n        if title is not None:\n            ax.set_title(title, fontsize=fontsize)\n\n        ax.pie(target, labels=target.index, autopct=autopct, shadow=shadow, explode=explode, colors=colors)\n    else:\n        fig = plt.figure(figsize=figsize)\n\n        if title is not None:\n            plt.title(title, fontsize=fontsize)\n\n        plt.pie(target, labels=target.index, autopct=autopct, shadow=shadow, explode=explode, colors=colors)\n        \n\ndef bar_plot(column, data=None, title=None, ax=None, fontsize=15, figsize=(10, 6), color='b'):\n    \"\"\"\n    returns: a bar plot of the quantities of unique valiues from a column.\n    \"\"\"\n    \n    if type(column) == str:\n        target = data[column].value_counts()\n    else:\n        target = column.value_counts()\n    \n    if ax is not None:\n        if title is not None:\n            ax.set_title(title, fontsize=fontsize)\n\n        ax.bar(target.index, target, color=color)\n    else:\n        fig = plt.figure(figsize=figsize)\n\n        if title is not None:\n            plt.title(title, fontsize=fontsize)\n\n        plt.bar(target.index, target, color=color)\n    \n\ndef compare_plots(shape, columns, titles=None, data=None, kind='pie', explode=0, color=None,\n                  fontsize=15, autopct='%1.1f%%', figsize=(20, 10), shadow=None, color_with_label=None):\n    \"\"\"\n    returns: a matplotlib.subplot consisting of several features as described by\n            the 'columns' using the 'data' DataFrame using a preferred kind.\n    \"\"\"\n    \n    fig, axes = plt.subplots(*shape, figsize=figsize)\n    \n    for i, ax in enumerate(axes.ravel()):\n        title = titles[i] if titles is not None else None\n        \n        if kind == 'pie':\n            pie_plot(columns[i], data=data, title=title, ax=ax, fontsize=fontsize, colors=color,\n                     autopct=autopct, figsize=figsize, explode=explode, shadow=shadow, \n                     color_with_label=color_with_label)\n        elif kind == 'bar':\n            if color is None:\n                color = 'b'\n                \n            bar_plot(columns[i], data=data, title=title, ax=ax, fontsize=fontsize, figsize=figsize, color=color)\n        else:\n            raise TypeError\n\ndef prep_data(train_data, test_data):\n    data = pd.concat([train_data, test_data])\n    \n    first_half = binary_data + ordinal_data\n    second_half = day_n_month\n    \n    # Handling binary data\n    data['bin_3'] = data['bin_3'].replace(to_replace=['F', 'T'], value=['0', '1']).astype(int)\n    data['bin_4'] = data['bin_4'].replace(to_replace=['Y', 'N'], value=['1', '0']).astype(int)\n    \n    # Handling ordinal data\n    data['ord_0'] = data['ord_0'] - 1\n    \n    for col, mapper in zip(\n        ['ord_1', 'ord_2', 'ord_3', 'ord_4'],        \n        [mapper_ord_1, mapper_ord_2, mapper_ord_3, mapper_ord_4]\n    ):\n        data[col] = data[col].replace(mapper)\n        \n    # Handling ord_5 high cardinality data\n    ord_5_matrix = data.ord_5.values.reshape(-1, 1)\n    data.ord_5 = ordinal_encoder.fit_transform(ord_5_matrix)\n    \n    # Scaling Ordinal Data\n    data[ordinal_data] = ordinal_scaler.fit_transform(data[ordinal_data])\n        \n    # One Hot Encoding on norminal data nom_0 - nom_4\n    nom_0_9_matrix = data[norminal_data].values\n    ohe_trans = ohe_encoder.fit_transform(nom_0_9_matrix)\n    \n    # Scaling Day and Month Data\n    data['day'] = data['day'] - 1\n    data['month'] = data['month'] - 1\n    \n    data[day_n_month] = dm_scaler.fit_transform(data[day_n_month])\n    \n    part_one_matrix = scipy.sparse.coo_matrix(\n        data.loc[:, first_half].to_numpy()\n    ).astype('float64')\n\n    part_two_matrix = scipy.sparse.coo_matrix(\n        data.loc[:, second_half].to_numpy()\n    ).astype('float64')\n    \n    \n    result =  scipy.sparse.hstack([\n        part_one_matrix, \n        ohe_trans, \n        part_two_matrix\n    ]).tocsr()\n    \n    return result[:train_data.shape[0]], result[train_data.shape[0]:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Exploratory Data Analysis\n\nThe housing data-set has already been divided into two distinctive set - the train and test set. We'll start by loading and performing data analysis on the training-set.\n\n\n## 2.1. Overview of the data\n\nThe data-set consists of only categorical data, and can be said to be broken into 3 distinct parts\n\n* **The Binary data** - labelled bin_: This are categorical variables consisting of binary data, i.e True/False, 1's/0's, Yes/No e.t.c\n\n* **The Ordinal Data** - labelled ord_: This are categorical variables consisting of ordered data. They are finite list of categories with a form of order related to it. Like in the days of the week, what come next after tuesday?\n\n* **The Norminal Data** - labelled nom_: This are categorical variables, finite like the ordinal data but with no order.\n\nThe day and month column of the data-set can be classified under the ordinal data.","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/cat-in-the-dat/train.csv\", index_col=\"id\")\ntest_data = pd.read_csv(\"../input/cat-in-the-dat/test.csv\", index_col=\"id\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_target = train_data.target\ntrain_data.drop('target', inplace=True, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.columns, len(train_data.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.duplicated().any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2. The Categorical Variables\n\nLet's list out all the information about the variables that we have\n","metadata":{}},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in train_data.columns:\n    print(f\"{col}: items_length = {len(train_data[col].unique())}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order for a model to effectively work with the data-set it has to be reduced/scaled to figures that are easy enough for the ML model to work with.\n\n## 2.3. Binary Variables\n\nWe start with the binary variables","metadata":{}},{"cell_type":"code","source":"binary_data = [f'bin_{i}' for i in range(5)]\n\nbinary_categorical_data = train_data.loc[:, binary_data]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"binary_categorical_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"binary_categorical_data['bin_3'] = binary_categorical_data['bin_3'].replace(to_replace=['F', 'T'], \n                                                                            value=['0', '1']).astype(int)\n\nbinary_categorical_data['bin_4'] = binary_categorical_data['bin_4'].replace(to_replace=['Y', 'N'], \n                                                                            value=['1', '0']).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"binary_categorical_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titles = [f\"ratio of 0's and 1's in {b}\" for b in binary_data]\n\ncolors = {\n    0: \"#FF5733\",\n    1: \"#2471A3\",\n}\n\ncompare_plots((1,5), binary_data, titles=titles, data=binary_categorical_data, color_with_label=colors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4.  Ordinal Variables\n\nGiven that ordinal data's are ordered. We try to maintain this order and then scale the features","metadata":{}},{"cell_type":"code","source":"ordinal_data = [f'ord_{i}' for i in range(6)]\n\nordinal_categorical_data = train_data.loc[:, ordinal_data]\n\nfor ordinal in ordinal_data:\n    print(f\"\"\"\nfor {ordinal}: uniques are {ordinal_categorical_data[ordinal].unique()}\n    \"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titles = [\n    f\"Distribution in ord_{i}\"\n    for i in range(6)\n]\n\ncompare_plots((3, 2), ordinal_data, titles=titles, data=ordinal_categorical_data, kind='bar', color=\"#2471A3\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal_categorical_data['ord_0'] = ordinal_categorical_data['ord_0'] - 1\n\nmapper_ord_1 = {'Novice': 0, 'Contributor': 1, 'Expert': 2, 'Master': 3, 'Grandmaster': 4}\n\nmapper_ord_2 = {'Freezing': 0, 'Cold': 1, 'Warm': 2, 'Hot': 3,'Boiling Hot': 4, 'Lava Hot': 5}\n\nmapper_ord_3 = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, \n                'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14}\n\nmapper_ord_4 = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, \n                'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14,\n                'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, \n                'W': 22, 'X': 23, 'Y': 24, 'Z': 25}\n\nfor col, mapper in zip(['ord_1', 'ord_2', 'ord_3', 'ord_4'], \n                       [mapper_ord_1, mapper_ord_2, mapper_ord_3, mapper_ord_4]):\n    ordinal_categorical_data[col] = ordinal_categorical_data[col].replace(mapper)\n\nordinal_categorical_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for ord_5, we have high cardinality. Lets apply  OrdinalEncoder with \"categories=’auto’\" to it.","metadata":{}},{"cell_type":"code","source":"# credits for ord_5 high cardinality code: \n# https://www.kaggle.com/gogo827jz/catboost-baseline-with-feature-importance through \n\nordinal_encoder = OrdinalEncoder(categories='auto')\n\nord_5_matrix = ordinal_categorical_data.ord_5.values.reshape(-1, 1)\n\nordinal_encoder.fit(ord_5_matrix)\n\nordinal_categorical_data.ord_5 = ordinal_encoder.transform(ord_5_matrix)\n\nordinal_categorical_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal_scaler = StandardScaler()\n\nordinal_categorical_data[ordinal_data] = ordinal_scaler.fit_transform(ordinal_categorical_data)\n\nordinal_categorical_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.5.  Norminal Variables\n\nThe norminal data on the other hand is a totally different case. Since they are not ordered either of Label encoding or OHEncoding should work great with them.","metadata":{}},{"cell_type":"code","source":"norminal_data = [f'nom_{i}' for i in range(10)]\n\nnorminal_categorical_data = train_data.loc[:, norminal_data]\n\nfor col in norminal_categorical_data.columns:\n    print(f\"{col}: items_length = {len(train_data[col].unique())}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.6. Difference between Label Encoding and OneHotEncoding (OHE)\n\nAlthough the label and one hot encoder performs similar function, they don't necessarily give the same result.\n\n### 2.6.1. Label Encoding\n\nLabel encoding assigns each unique variable to a different integer. This encoding scheme assumes the categories are ordered.\n\n![](https://miro.medium.com/max/996/1*K5JbqxIwwPmtiSNQhjLPRg.png)\n\n### 2.6.2. OneHotEncoding\n\nOne-hot encoding creates new columns indicating the presence/absence of each possible value in the original variable. Unlike label encoding, one-hot encoding does not assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical variables.\n\n![](https://miro.medium.com/max/878/1*WXpoiS7HXRC-uwJPYsy1Dg.png)\n\nFor the norminal variables, we'll be making use of OHE...\n\n","metadata":{}},{"cell_type":"code","source":"ohe_encoder = OneHotEncoder()\n\nnom_0_9_matrix = norminal_categorical_data[norminal_data].values\nohe_trans = ohe_encoder.fit_transform(nom_0_9_matrix)\n\nohe_trans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.7. Day and Month Variables\n\nNow let's evaluate the day and month variables","metadata":{}},{"cell_type":"code","source":"day_n_month = ['day', 'month']\n\nday_month_data = train_data.loc[:, day_n_month]\n\nfor dm in day_n_month:\n    print(f\"\"\"\nfor {dm}: uniques are {day_month_data[dm].unique()}\n    \"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"day_month_data['day'] = day_month_data['day'] - 1\nday_month_data['month'] = day_month_data['month'] - 1\n\nday_month_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dm_scaler = StandardScaler()\n\nday_month_data[day_n_month] = dm_scaler.fit_transform(day_month_data)\n\nday_month_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Putting it all together!!!**","metadata":{}},{"cell_type":"code","source":"train_d = train_data.copy()\ntest_d = test_data.copy()\n\ntrain_d, test_d = prep_data(train_d, test_d)\n\ntrain_d.shape, test_d.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation on Test-Set\n\nWe'll be making use of the Logistic classifier model for this evaluation.\n\nI already fine-tune and tested the model, so I will just apply it to the data to get the result.","metadata":{}},{"cell_type":"code","source":"log_clf = LogisticRegression(C=0.1, max_iter=1000, n_jobs=-2)\n\nlog_clf.fit(train_d, train_target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = log_clf.predict_proba(test_d)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = pd.Series(predictions, name=\"target\")\n\nresult = pd.DataFrame({\n    \"id\": test_data.index, \n    \"target\": target\n})\n\nresult = result.set_index(\"id\")\n\n# result.to_csv(\"cat-in-the-dat/my_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\n* https://www.kaggle.com/dkomyagin/cat-in-the-dat-0-80285-private-lb-solution\n* https://www.kaggle.com/shahules/an-overview-of-encoding-techniques\n* https://www.kaggle.com/prazhant/a-detailed-guide-to-different-encoding-schemes\n\n\n### Previous Notebook\n\n* [House Prices Prediction (Beginner)](https://www.kaggle.com/ganiyuolalekan/house-prices-prediction-beginner)\n\n<br><br>\n<b><a style=\"color: #283747\" href=\"#Table-of-Content\">Back to Top</a></b>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}