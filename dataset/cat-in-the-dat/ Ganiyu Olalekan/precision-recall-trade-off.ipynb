{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Precision - Recall Trade-off\n\nIn this notebook I will be explaining the relationship and difference between precision and recall and how changing one affect the other.\n\n# Outline\n\n* [1. Overview of the Data-set](#1.-Overview-of-the-Data-set)\n* [2. Confusion Matrix](#2.-Confusion-Matrix)\n* [3. Precision vs Recall](#3.-Precision-vs-Recall)\n* [4. Conclusion](4.-Conclusion)\n\n\n### Kindly upvote if you find the kernel helpful :) ","metadata":{}},{"cell_type":"code","source":"# Libraries\n\nimport scipy\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Variables\n\nbinary_data = [f'bin_{i}' for i in range(5)]\nordinal_data = [f'ord_{i}' for i in range(6)]\nnorminal_data = [f'nom_{i}' for i in range(10)]\nday_n_month = ['day', 'month']\n\nordinal_scaler = StandardScaler()\nordinal_encoder = OrdinalEncoder(categories='auto')\nohe_encoder = OneHotEncoder()\ndm_scaler = StandardScaler()\n\nmapper_ord_1 = {'Novice': 0, 'Contributor': 1, 'Expert': 2, 'Master': 3, 'Grandmaster': 4}\n\nmapper_ord_2 = {'Freezing': 0, 'Cold': 1, 'Warm': 2, 'Hot': 3,'Boiling Hot': 4, 'Lava Hot': 5}\n\nmapper_ord_3 = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, \n                'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14}\n\nmapper_ord_4 = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, \n                'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14,\n                'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, \n                'W': 22, 'X': 23, 'Y': 24, 'Z': 25}\n\n# Functions\n\ndef plot_conf_mx(mx, title=None, ax=None, fontsize=None):\n    \"\"\"\n    returns: a matplotlib black-white representation of the confusion matrix\n            The higher the greater the intensity of the white shade and vice.\n    \"\"\"\n    \n    if ax is not None:\n        if title is not None:\n            ax.set_title(title, fontsize=fontsize)\n            \n        ax.matshow(mx, cmap=plt.cm.gray)\n    else:\n        if title is not None:\n            plt.title(title, fontsize=fontsize)\n            \n        plt.matshow(mx, cmap=plt.cm.gray)\n        plt.show()\n            \ndef compare_conf_mx(shape, mx_s, titles=None, fontsize=15, figsize=(12, 10)):\n    \"\"\"\n    returns: a matplotlib.subplot comparing multiple matrices in order of their shape\n            and for easier comparision.\n    \"\"\"\n    \n    fig, axes = plt.subplots(*shape, figsize=figsize)\n    \n    for i, ax in enumerate(axes.ravel()):\n        title = titles[i] if titles is not None else None\n        \n        plot_conf_mx(mx_s[i], title=title, ax=ax, fontsize=fontsize)\n        \n    plt.show()\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds, perc=.8, on_perc=True):\n    \"\"\"\n    Code credits: Aurélien Geron\n    https://github.com/ageron/handson-ml2/blob/master/03_classification.ipynb\n    \n    plots the precision-recall curve. The perc parameter plots the percentage of\n    accuracy on the precision curve which is inverse to the recall curve, and also\n    states the corresponding threshold.\n    \"\"\"\n    \n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n    plt.xlabel(\"Threshold\")\n    plt.grid(True)\n    plt.legend()\n    \n    if on_perc:\n        recall_precision = recalls[np.argmax(precisions >= perc)]\n        threshold_precision = thresholds[np.argmax(precisions >= perc)]\n    \n        plt.plot([threshold_precision, threshold_precision], [0., perc], \"r:\")\n        plt.axis([-4, 4, 0, 1])\n        plt.plot([-4, threshold_precision], [perc, perc], \"r:\")\n        plt.plot([-4, threshold_precision], [recall_precision, recall_precision], \"r:\")\n        plt.plot([threshold_precision], [perc], \"ro\") \n        plt.plot([threshold_precision], [recall_precision], \"ro\")\n    \n    plt.show()\n\ndef plot_precision_vs_recall(precisions, recalls, perc=.8, on_perc=True):\n    \"\"\"\n    Code credits: Aurélien Geron\n    https://github.com/ageron/handson-ml2/blob/master/03_classification.ipynb\n    \n    plot the precision against the recall. The perc parameter plots the percentage of\n    accuracy on the precision curve which is inverse to the recall curve.\n    \"\"\"\n    \n    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n    plt.xlabel(\"Recall\", fontsize=16)\n    plt.ylabel(\"Precision\", fontsize=16)\n    plt.axis([0, 1, 0, 1])\n    plt.grid(True)\n    \n    if on_perc:\n        recall_precision = recalls[np.argmax(precisions >= perc)]\n        \n        plt.plot([recall_precision, recall_precision], [0., perc], \"r:\")\n        plt.plot([0.0, recall_precision], [perc, perc], \"r:\")\n        plt.plot([recall_precision], [perc], \"ro\")\n        \n    plt.show()\n    \ndef threshold_precision(predictions, precisions, thresholds, perc=.8):\n    t_precision = thresholds[np.argmax(precisions >= perc)]\n    \n    return (predictions >= t_precision)\n    \ndef prep_data(data):    \n    first_half = binary_data + ordinal_data\n    second_half = day_n_month\n    \n    # Handling binary data\n    data['bin_3'] = data['bin_3'].replace(to_replace=['F', 'T'], value=['0', '1']).astype(int)\n    data['bin_4'] = data['bin_4'].replace(to_replace=['Y', 'N'], value=['1', '0']).astype(int)\n    \n    # Handling ordinal data\n    data['ord_0'] = data['ord_0'] - 1\n    \n    for col, mapper in zip(\n        ['ord_1', 'ord_2', 'ord_3', 'ord_4'],        \n        [mapper_ord_1, mapper_ord_2, mapper_ord_3, mapper_ord_4]\n    ):\n        data[col] = data[col].replace(mapper)\n        \n    # Handling ord_5 high cardinality data\n    ord_5_matrix = data.ord_5.values.reshape(-1, 1)\n    data.ord_5 = ordinal_encoder.fit_transform(ord_5_matrix)\n    \n    # Scaling Ordinal Data\n    data[ordinal_data] = ordinal_scaler.fit_transform(data[ordinal_data])\n        \n    # One Hot Encoding on norminal data nom_0 - nom_4\n    nom_0_9_matrix = data[norminal_data].values\n    ohe_trans = ohe_encoder.fit_transform(nom_0_9_matrix)\n    \n    # Scaling Day and Month Data\n    data['day'] = data['day'] - 1\n    data['month'] = data['month'] - 1\n    \n    data[day_n_month] = dm_scaler.fit_transform(data[day_n_month])\n    \n    part_one_matrix = scipy.sparse.coo_matrix(\n        data.loc[:, first_half].to_numpy()\n    ).astype('float64')\n\n    part_two_matrix = scipy.sparse.coo_matrix(\n        data.loc[:, second_half].to_numpy()\n    ).astype('float64')\n    \n    \n    return  scipy.sparse.hstack([\n        part_one_matrix, \n        ohe_trans, \n        part_two_matrix\n    ]).tocsr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Overview of the Data-set\n\nI will be making use of the data from the [Categorical Feature Encoding Challenge](https://www.kaggle.com/ganiyuolalekan/eda-on-the-various-categorical-data-0-80464). The data-set consists of only categorical data, and can be said to be broken into 3 distinct parts\n\n* **The Binary data** - labelled bin_: This are categorical variables consisting of binary data, i.e True/False, 1's/0's, Yes/No e.t.c\n\n* **The Ordinal Data** - labelled ord_: This are categorical variables consisting of ordered data. They are finite list of categories with a form of order related to it. Like in the days of the week, what come next after tuesday?\n\n* **The Norminal Data** - labelled nom_: This are categorical variables, finite like the ordinal data but with no order.\n\n**checkout** this [notebook](https://www.kaggle.com/ganiyuolalekan/eda-on-the-various-categorical-data-0-80464) on my work in this challenge. In this notebook I will be introducing the logistic model I used to describe our objective for this course. \n\nTo understand the logistic model better visit https://machinelearningmastery.com/logistic-regression-for-machine-learning\n\nso let's start by loading the data!","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/cat-in-the-dat/train.csv\", index_col=\"id\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = data.target\ndata.drop('target', inplace=True, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will be skipping the exploratory data analysis of this data and head forward to preparing the data for the model. The exploratory data analysis of the data can be found [here](https://www.kaggle.com/ganiyuolalekan/eda-on-the-various-categorical-data-0-80464#2.-Exploratory-Data-Analysis).","metadata":{}},{"cell_type":"code","source":"td = data.copy()\n\ntd = prep_data(td)\n\ntd.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_clf = LogisticRegression(C=0.1, max_iter=1000, n_jobs=-2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = cross_val_predict(log_clf, td, target, cv=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\"\"\nPrecision: {precision_score(target, predictions)},\nRecall: {recall_score(target, predictions)}\nAccuracy: {accuracy_score(target, predictions)}\n\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.gifer.com/origin/96/96d4e0de7526f5343749be587062100e_w200.gif\" width=\"50%\" />\n\n# 2. Confusion Matrix\n\nThe accuracy score is the measure of **true-positives** plus **true-negatives** to the overall document. From the accuracy above we can tell approximately 76% of the data is predicted correctly, which tells us quite alot about how good the model is. The confusion matrix goes a step further to describe what the model predicts better or what it predicts worst in.\n\nThe confusion matrix displays - in the order left-right top-bottom - the **true-negative**, **false-positive**, **false-negative** and **true-positive**.\n\n- The **true-negative (TN)** is the number of negative predictions that were actually negative.\n- The **false-positive (FP)** is the number of positive predictions that were actually negative.\n- The **false-negative (FN)** is the number of negative predictions that were actually positive.\n- The **true-positive (TP)** is the number of positive predictions that were actually positive.\n\n![towardsdatascience.com](https://static.packt-cdn.com/products/9781838555078/graphics/C13314_06_05.jpg)\n\nSo let's see how the model truly performs with the confusion matrix\n","metadata":{}},{"cell_type":"code","source":"mx = confusion_matrix(target, predictions)\nmx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the model performs particularly well on the **true-negative** (top-left) with **186852** of the negative data being predicted negative. As good as this sounds **50174** of the positive data are also predicted as negative (**false-negative**) and consititute of the second largest share of the data in the confusion matrix. \n\nLet's use an image representation to view the confusion matrix","metadata":{}},{"cell_type":"code","source":"plot_conf_mx(mx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a cleaner representation of what is going on in the confusion matrix. The brighter the **true-positive** and **true-negative** and darker the **false-positive** and **false-negative** in the confusion matrix, the more accurate the model. A perfect model should be something like this.\n\n![](../input/perfect-conf-mx/conf_mx.png)\n\nWhat we see from our model is that we've only achieved half that. If we wish to manipulate the confusion matrix we have to adjust the precision and recall rate of the model. \n\n# 3. Precision vs Recall\n\n### What is precision?\n\nPrecision is the fraction of retrieved documents that are relevant to the query. \n\nPrecision = $\\frac{TP}{TP + FP}$\n\nIf you inspect the image of the confusion matrix above, you would notice the TP and FP are on the positive prediction half of the matrix (i.e on the RHS of the confusion matrix). \n\nThe summation of this two (2) consistutes the total positive predictions relevant to the documents, and from the precision equation above we can see that the precision is the measure of the actual positives from this documents.\n\nSimple put the precision focuses on how much of the actual positive predictions were rightly predicted.\n\n### What is recall?\n\nRecall (also referred to as sensitivity or True Positive Rate (TPR)) is the fraction of the relevant documents that are successfully retrieved.\n\nRecall = $\\frac{TP}{TP + FN}$\n\nIf you inspect the image of the confusion matrix above, you would notice the TP and FN are on the actual positive half of the matrix (i.e on the bottom of the confusion matrix). \n\nThe summation of this two (2) consistutes the total actual positive relevant to the documents and from the recall equation above we can tell that the recall is the measure of the actual positives from this documents.\n\nThe recall focuses on how much of the actual positive data were rightly predicted.","metadata":{}},{"cell_type":"code","source":"TN = mx[0][0]\nTP = mx[1][1]\nFP = mx[0][1]\nFN = mx[1][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\"\"\nPrecision: {TP / (TP + FP)},\nRecall: {TP / (TP + FN)}\nAccuracy: {(TP + TN) / (TP + FP + TN + FN)}\n\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing this to our previous scores we see that the results are the same. \n\nNow lets plot the relationship between the precision and the recall as a function of their threshold value.","metadata":{}},{"cell_type":"code","source":"prediction_scores = cross_val_predict(log_clf, td, target, cv=3, method=\"decision_function\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precisions, recalls, thresholds = precision_recall_curve(target, prediction_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_precision_recall_vs_threshold(precisions, recalls, thresholds, perc=.66)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another way to select a good precision/recall trade-off is to plot precision directly against recall.","metadata":{}},{"cell_type":"code","source":"plot_precision_vs_recall(precisions, recalls, perc=.66)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see what happens when we increase the precision by 10%","metadata":{}},{"cell_type":"code","source":"perc = 0.76","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_precision_recall_vs_threshold(precisions, recalls, thresholds, perc=perc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_precision_vs_recall(precisions, recalls, perc=perc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_76 = threshold_precision(prediction_scores, precisions, thresholds, perc=perc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\"\"\nPrecision: {precision_score(target, pred_76)},\nRecall: {recall_score(target, pred_76)}\nAccuracy: {accuracy_score(target, pred_76)}\n\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that as the precision rises to about 76% the recall rate drops to 25% and the accuracy drops slightly as well from 76% to 74%. \n\nBut what impart does it have on the model's confusion matrix? ","metadata":{}},{"cell_type":"code","source":"mx_76 = confusion_matrix(target, pred_76)\nmx_76","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_conf_mx(mx_76)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's almost as thou nothing changed. Let's compare it with the precision at 66%","metadata":{}},{"cell_type":"code","source":"titles = [\n    \"Confusion matrix at precision approx 66%\",\n    \"Confusion matrix at precision approx 76%\"\n]\n\ncompare_conf_mx((1, 2), [mx, mx_76], titles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This way we can see that there's a slight change, the model actually get slighty worst than before. (i.e in comparision to the perfect confusion matrix as shown above)\n\n**Note**: The darker the box in the confusion matrix the lower the value.\n\nSo its only logical that when precision increases the amount of **FP** should reduce while the **TP** should increase, but we notice here that the **TP** reduces slighlty and the **FN** slightly increases as well. This is where the precision/recall trade-off comes to play. \n\n> **The Recall rate is inversely proportional to the Precision rate**\n\nSo the condition for a rising recall is that as the **TP** increases the **FN** should reduce, the inverse of this is that as the **FN** increases the **TP** should reduces and we have a recall of about 25 which explains why the **TP** is the way it is.\n\nlet's increase the precision by another 10% and see how it affects the graph","metadata":{}},{"cell_type":"code","source":"perc = .86","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_precision_vs_recall(precisions, recalls, perc=perc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_86 = threshold_precision(prediction_scores, precisions, thresholds, perc=perc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\"\"\nPrecision: {precision_score(target, pred_86)},\nRecall: {recall_score(target, pred_86)}\nAccuracy: {accuracy_score(target, pred_86)}\n\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see how much the recall suffers and the accuracy is still good enough. This is a perfect model if your focus is on precision.\n\nLet's also see how this affect the confusion matrix.","metadata":{}},{"cell_type":"code","source":"mx_86 = confusion_matrix(target, pred_86)\nmx_86","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_conf_mx(mx_86)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What we see here is not a very perfect model, but it is highly precise. (about 86% worth)\n\nLet's see how we've progressed with the increase in precision.","metadata":{}},{"cell_type":"code","source":"titles = [\n    \"Confusion matrix at precision approx 66%\",\n    \"Confusion matrix at precision approx 76%\",\n    \"Confusion matrix at precision approx 86%\"\n]\n\ncompare_conf_mx((1, 3), [mx, mx_76, mx_86], titles, figsize=(20, 15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By reducing the precision we will increase the recall. Using this we can tell how the model performs with high recall. So we start by reducing the precision from 66% to 56%.","metadata":{}},{"cell_type":"code","source":"perc = 0.56","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_precision_recall_vs_threshold(precisions, recalls, thresholds, perc=perc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_precision_vs_recall(precisions, recalls, perc=perc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_56 = threshold_precision(prediction_scores, precisions, thresholds, perc=perc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\"\"\nPrecision: {precision_score(target, pred_56)},\nRecall: {recall_score(target, pred_56)}\nAccuracy: {accuracy_score(target, pred_56)}\n\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well this looks like a better model, if not for the reduced accuracy. \n\nBut according to the confusion matrix...","metadata":{}},{"cell_type":"code","source":"mx_56 = confusion_matrix(target, pred_56)\nmx_56","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_conf_mx(mx_56)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just like during the precision the **FP** suffers while the **FN** rises and the **TP** gets better.\n\nThe reason for this is because now the model becomes less focused on how precise its decision making becomes, and more focused on generating more positive outcomes, so this way the **FN** reduces because more of the actual positive are being predicted right and the **FP** increases as well because few of the actual negative are also predicted as positive.\n\nLet's compare this model with the main model (where precision equals 66%).","metadata":{}},{"cell_type":"code","source":"titles = [\n    \"Confusion matrix at precision approx 66%\",\n    \"Confusion matrix at precision approx 56%\"\n]\n\ncompare_conf_mx((1, 2), [mx, mx_56], titles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok... what happens if we reduce the precision by 10% more? Let's see...","metadata":{}},{"cell_type":"code","source":"perc = 0.46","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_precision_recall_vs_threshold(precisions, recalls, thresholds, perc=perc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_precision_vs_recall(precisions, recalls, perc=perc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_46 = threshold_precision(prediction_scores, precisions, thresholds, perc=perc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\"\"\nPrecision: {precision_score(target, pred_46)},\nRecall: {recall_score(target, pred_46)}\nAccuracy: {accuracy_score(target, pred_46)}\n\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can tell from the accuracy how important precision is to a good model. \n\nLet's see how the confusion matrix reacts to this.","metadata":{}},{"cell_type":"code","source":"mx_46 = confusion_matrix(target, pred_46)\nmx_46","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_conf_mx(mx_46)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see the effect of reduced precision. Although this model's confusion matrix is the closest to our target confusion matrix, it cares less about being very precise and more on predicting the positive data.\n\n# Conclusion\n\nYou can't have an absolutely perfect model, but you can trade precision/recall to make the model attain your objectives. \n\n**Think of it this way...**\n\n> The more **precise** the model the less **False Negatives**.\n\n> The more **recall** the model possess the less **False Positives**\n\nJust like in our experimentations.","metadata":{}},{"cell_type":"code","source":"titles = [\n    \"Confusion matrix at precision approx 46%\",\n    \"Confusion matrix at precision approx 86%\"\n]\n\ncompare_conf_mx((1, 2), [mx_46, mx_86], titles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}