{"cells":[{"metadata":{},"cell_type":"markdown","source":"## LGBM is Powerful!\n\nI want to prove LGBM is better than Logistic Regression. \n\nLet's try :)"},{"metadata":{},"cell_type":"markdown","source":"## Import Libarary & Read CSV"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nimport seaborn as sns\nimport numpy as np \nimport pandas as pd\nimport os, gc\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\ntrain_df = pd.read_csv('/kaggle/input/cat-in-the-dat/train.csv')\ntest_df = pd.read_csv('/kaggle/input/cat-in-the-dat/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train_df['target']\ntrain_id = train_df['id']\ntest_id = test_df['id']\ntrain_df.drop(['target', 'id'], axis=1, inplace=True)\ntest_df.drop('id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering (Target Encoding)"},{"metadata":{},"cell_type":"markdown","source":"and remaining loooooong features : target encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntraintest = pd.concat([train_df, test_df])\ndummies = pd.get_dummies(traintest, columns=traintest.columns, drop_first=True, sparse=True)\ntrain = dummies.iloc[:train_df.shape[0], :]\ntest = dummies.iloc[train_df.shape[0]:, :]\ntrain = train.sparse.to_coo().tocsr()\ntest = test.sparse.to_coo().tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.astype('float32')\ntest = test.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM model"},{"metadata":{},"cell_type":"markdown","source":"This is my first single LGBM Model (public leaderboard score low)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time \n# X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.2, random_state=97)\n\n# param = {   \n#     'boost': 'gbdt',\n#     'learning_rate': 0.005,\n#     'feature_fraction':0.3,\n#     'bagging_freq':1,\n#     'max_depth': -1,\n#     'num_leaves':18,\n#     'lambda_l2': 3,\n#     'lambda_l1': 3,\n#     'metric':{'auc'},\n#     'tree_learner': 'serial',\n#     'objective': 'binary',\n#     'verbosity': 1,\n#     'seed': 97,\n#     'feature_fraction_seed': 97,\n#     'bagging_seed': 97,\n#     'drop_seed': 97,\n#     'data_random_seed': 97,\n# }\n\n\n# evals_result = {}\n# predictions = np.zeros(test.shape[0])\n\n# lgb_train = lgb.Dataset(X_train, y_train)\n# lgb_valid = lgb.Dataset(X_test, y_test)\n\n# num_round = 20000\n# clf = lgb.train(param, lgb_train, num_round, valid_sets = [lgb_train, lgb_valid],\n#       verbose_eval=100, early_stopping_rounds = 1000, evals_result = evals_result)\n\n# ## Prediction\n# predictions = clf.predict(test, num_iteration=clf.best_iteration)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGBM with CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# CV function original : @Peter Hurford : Why Not Logistic Regression? https://www.kaggle.com/peterhurford/why-not-logistic-regression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score as auc\n\ndef run_cv_model(train, test, target, model_fn, params={}, label='model'):\n    kf = KFold(n_splits=5)\n    fold_splits = kf.split(train, target)\n\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0]))\n    i = 1\n    for dev_index, val_index in fold_splits:\n        print('Started {} fold {}/5'.format(label, i))\n        dev_X, val_X = train[dev_index], train[val_index]\n        dev_y, val_y = target[dev_index], target[val_index]\n        \n        pred_val_y, pred_test_y = model_fn(dev_X, dev_y, val_X, val_y, test, params)\n        \n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        \n        cv_score = auc(val_y, pred_val_y)\n        cv_scores.append(cv_score)\n        print(label + ' cv score {}: {}\\n'.format(i, cv_score))\n        i += 1\n        \n    print('{} cv scores : {}'.format(label, cv_scores))\n    print('{} cv mean score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv std score : {}'.format(label, np.std(cv_scores)))\n    pred_full_test = pred_full_test / 5.0\n    results = {'label': label, 'train': pred_train, 'test': pred_full_test, 'cv': cv_scores}\n    return results\n\n\ndef runLGBM(X_train, y_train, X_val, y_val, X_test, params):\n    predictions = np.zeros(test.shape[0])\n    lgb_train, lgb_valid = lgb.Dataset(X_train, y_train), lgb.Dataset(X_val, y_val)\n    num_round = 5000\n    clf = lgb.train(params, lgb_train, num_round, valid_sets = [lgb_train, lgb_valid], verbose_eval=1000, early_stopping_rounds = 1000)\n    pred_val_y = clf.predict(X_val, num_iteration=clf.best_iteration)\n    pred_test_y = clf.predict(X_test, num_iteration=clf.best_iteration)\n    return pred_val_y, pred_test_y\n\nparams = {   \n    'boost': 'gbdt',\n    'learning_rate': 0.005,\n    'feature_fraction':0.3,\n    'bagging_freq':1,\n    'max_depth': 1<<5,\n    'num_leaves':18,\n    'lambda_l2': 0.9,\n    'lambda_l1': 0.9,\n    'metric':{'auc'},\n    'tree_learner': 'serial',\n    'objective': 'binary',\n    'verbosity': 1,\n    'seed': 97,\n    'feature_fraction_seed': 97,\n    'bagging_seed': 97,\n    'drop_seed': 97,\n    'data_random_seed': 97,\n}\n\nresults = run_cv_model(train, test, target, runLGBM, params, 'LGBM')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Result"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.DataFrame({'id': test_id, 'target' : results['test']})\n\nsub_df.to_csv(\"lightgbm_onehotencoding_cv.csv\", index=False)\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}