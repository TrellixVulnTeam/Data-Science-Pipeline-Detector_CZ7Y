{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Is there a cat in your dat?\n\nA common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured.\n\nBecause this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:\n\nbinary features\nlow- and high-cardinality nominal features\nlow- and high-cardinality ordinal features\n(potentially) cyclical features"},{"metadata":{},"cell_type":"markdown","source":"# Reading Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read the data\ntrain_d = pd.read_csv('../input/cat-in-the-dat/train.csv') \ntest_d = pd.read_csv('../input/cat-in-the-dat/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring and understanding our Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_d.shape ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_d.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_d.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_d.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_d.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_d.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_d.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Is there any missing values?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_d.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"great, our data don't have any missing values"},{"metadata":{},"cell_type":"markdown","source":"# The number of unique values??"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train_d.columns[1:]:\n    print(col, train_d[col].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization and preprocessing of Binary Features :"},{"metadata":{"trusted":true},"cell_type":"code","source":"binary_col = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfor n, col in enumerate(train_d[binary_col]): \n    plt.figure(n)\n    sns.countplot(x=col, data=train_d, hue='target', palette='husl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_d['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(x=train_d.head(250)['bin_0'], y=train_d.head(250)['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(x=train_d.head(250)['target'], y=train_d.head(250)['month'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(x=train_d.head(250)['target'], y=train_d.head(250)['day'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(x=train_d.head(250)['day'], y=train_d.head(250)['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram \nsns.distplot(a=train_d['target'], kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=train_d['target'], shade=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" bin_d = train_d[['bin_0','bin_1','bin_2','bin_3', 'bin_4']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_d.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ntrain_df=pd.DataFrame()\nlabel=LabelEncoder()\nfor col in  train_d.columns:\n    if 'bin' in col:\n        train_df[col]=label.fit_transform(train_d[col])\n    else:\n        train_df[col]=train_d[col]\n        \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_d.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ntest_df=pd.DataFrame()\nlabel=LabelEncoder()\nfor col in  test_d.columns:\n    if 'bin' in col:\n        test_df[col]=label.fit_transform(test_d[col])\n    else:\n        test_df[col]=test_d[col]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(4) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"binary_cols = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfor n, col in enumerate(train_df[binary_cols]): \n    plt.figure(n)\n    sns.countplot(x=col, data=train_df, hue='target', palette='husl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape  ,  test_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization and preprocessing of nominal Features :"},{"metadata":{"trusted":true},"cell_type":"code","source":"nominal_cols = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for n, col in enumerate(train_df[nominal_cols]): \n    plt.figure(n)\n    sns.countplot(x=col, data=train_df, hue='target', palette='husl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# separating nominal Features"},{"metadata":{},"cell_type":"markdown","source":"As we see, we have a different range of nominal features and we know that is not good way to make OneHotencoder for variables taking more than 15 different values. so, we will separate them"},{"metadata":{"trusted":true},"cell_type":"code","source":"low_cardinality_nom_cols = []\nhigh_cardinality_nom_cols = []\n\n\nfor nom_col in range(10):\n    nom_col_name = \"nom_\"+str(nom_col)\n    if train_df[nom_col_name].nunique() < 10:\n        low_cardinality_nom_cols.append(nom_col_name)\n    else:\n        high_cardinality_nom_cols.append(nom_col_name)\n\nprint(\"Nominal columns low cardinality (<=10):\", low_cardinality_nom_cols)\nprint(\"Nominal columns with high cardinality (>10):\", high_cardinality_nom_cols)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_nom = train_df.columns[6:11]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_nom","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For (low) nominal features : using OneHotEencoder to encoding variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_df[low_cardinality_nom_cols]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(test_df[low_cardinality_nom_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train_df.index\nOH_cols_test.index = test_df.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = train_df.drop(low_cardinality_nom_cols, axis=1)\nnum_X_valid = test_df.drop(low_cardinality_nom_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_tset = pd.concat([num_X_valid, OH_cols_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OH_X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OH_X_tset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization and preprocessing of ordinal Features :"},{"metadata":{"trusted":true},"cell_type":"code","source":"ord_col = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfor n, col in enumerate(train_df[ord_col]): \n    plt.figure(n)\n    sns.countplot(x=col, data=train_df, hue='target', palette='husl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ntrain_n=pd.DataFrame()\nlabel=LabelEncoder()\nfor col in  ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']:\n   \n        train_n[col]=label.fit_transform(OH_X_train[col])\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \ndata_t = OH_X_train.drop(['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5'], axis=1) \ntrain_dd = pd.concat([data_t,train_n], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ntest_n=pd.DataFrame()\nlabel=LabelEncoder()\nfor col in  ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']:\n   \n        test_n[col]=label.fit_transform(OH_X_tset[col])\n    \n    \ndata_t = OH_X_tset.drop(['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5'], axis=1) \ntest_dd = pd.concat([data_t,test_n], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dd.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# back to high nominal : "},{"metadata":{},"cell_type":"markdown","source":"Trying some different ways to preprocessing high nominal like: hash, frequent, label encoder"},{"metadata":{},"cell_type":"markdown","source":"# Hash"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in high_cardinality_nom_cols:\n    train_dd[f'hash_{col}'] = train_dd[col].apply( lambda x: hash(str(x)) % 5000 )\n    test_dd[f'hash_{col}'] = test_dd[col].apply( lambda x: hash(str(x)) % 5000 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Frequent "},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in high_cardinality_nom_cols:\n    enc_nom_1 = (train_dd.groupby(col).size()) / len(train_dd)\n    train_dd[f'freq_{col}'] = train_dd[col].apply(lambda x : enc_nom_1[x])\n    #test_dd[f'enc_{col}'] = test_dd[col].apply(lambda x : enc_nom_1[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Label Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Label Encoding\nfor f in ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']:\n    if train_dd[f].dtype=='object' or test_dd[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train_dd[f].values) + list(test_dd[f].values))\n        train_dd[f'le_{f}'] = lbl.transform(list(train_dd[f].values))\n        test_dd[f'le_{f}'] = lbl.transform(list(test_dd[f].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_feat = ['hash_nom_5', 'hash_nom_6', 'hash_nom_7', 'hash_nom_8',\n            'hash_nom_9',  'freq_nom_5', 'freq_nom_6', 'freq_nom_7', \n            'freq_nom_8', 'freq_nom_9', 'le_nom_5', 'le_nom_6',\n            'le_nom_7', 'le_nom_8', 'le_nom_9']\n\nnew_da = (train_dd[high_cardinality_nom_cols + new_feat])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_da.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dd[['nom_5', 'hash_nom_5', 'freq_nom_5', 'le_nom_5']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dd.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dd.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dd.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# choosing just one type and Dropping other:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dd.drop([ \n                #'hash_nom_6', 'hash_nom_7', 'hash_nom_8', 'hash_nom_9',\n               'le_nom_5', 'le_nom_6', 'le_nom_7', 'le_nom_8', 'le_nom_9',\n                'freq_nom_5','freq_nom_6', 'freq_nom_7', 'freq_nom_8', 'freq_nom_9',\n              'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'\n         ], axis=1, inplace=True)\n\n#test_dd.drop([\n              #'hash_nom_6', 'hash_nom_7', 'hash_nom_8', 'hash_nom_9', \n #             'le_nom_5', 'le_nom_6', 'le_nom_7', 'le_nom_8', 'le_nom_9',\n  #            'freq_nom_5', 'freq_nom_6', 'freq_nom_7', 'freq_nom_8', 'freq_nom_9',\n   #           'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9',\n    #          ], axis=1, inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dd.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dd.drop([\n              #'hash_nom_6', 'hash_nom_7', 'hash_nom_8', 'hash_nom_9', \n            'le_nom_5', 'le_nom_6', 'le_nom_7', 'le_nom_8', 'le_nom_9',\n  #          'freq_nom_5', 'freq_nom_6', 'freq_nom_7', 'freq_nom_8', 'freq_nom_9',\n            'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'\n            ], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dd.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NOW OUR DATA IS NUMERIC ^_^"},{"metadata":{},"cell_type":"markdown","source":"# Let's make some visualizations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"date_cols = ['day', 'month']\n\nfor n, col in enumerate(train_df[date_cols]): \n    plt.figure(n)\n    sns.countplot(x=col, data=train_df, hue='target', palette='husl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=train_df['bin_0'], y=train_df['ord_3'], hue=train_df['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(x=train_dd.head(10)['hash_nom_5'],\n              y=train_dd.head(10)['day'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the width and height of the figure\nplt.figure(figsize=(10,6))\n\n# Add title\nplt.title(\"ord 3 , by Month\")\n\n# Bar chart showing average arrival delay for Spirit Airlines flights by month\nsns.barplot(x=train_dd.head(20)['month'], y=train_dd.head(20)['ord_3'])\n\n# Add label for vertical axis\nplt.ylabel(\"in \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the width and height of the figure\nplt.figure(figsize=(10,6))\n\n# Add title\nplt.title(\"hash_nom_6  , by day\")\n\n# Bar chart showing average arrival delay for Spirit Airlines flights by month\nsns.barplot(x=train_dd.head(20)['day'], y=train_dd.head(20)['hash_nom_6'])\n\n# Add label for vertical axis\nplt.ylabel(\"in \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the width and height of the figure\nplt.figure(figsize=(14,7))\n\n# Add title\nplt.title(\"Heatmap\")\n\n# Heatmap showing average arrival delay for each airline by month\nsns.heatmap(data=train_dd.head(15), annot=False)\n\n# Add label for horizontal axis\nplt.xlabel('data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cyclic_cols = ['day','month']\n\nfig, axs = plt.subplots(1, len(cyclic_cols), figsize=(8, 4))\n\nfor i in range(len(cyclic_cols)):\n    col = cyclic_cols[i]\n    ax = axs[i]\n    sns.barplot(x=col, y='target', data=train_dd, ax=ax)\n    ax.set_title(col, fontsize=14, fontweight='bold')\n    ax.legend(title=\"target\", loc='upper center')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# split data using : model_selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dd.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_dd = train_dd.drop([\"id\"],axis=1)\n\n\ntrain_dd.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Separate data into training and validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select  predictors\ncols_to_use = [     'bin_0',      'bin_1',      'bin_2',      'bin_3',      'bin_4',\n              'day',      'month',            0,            1,            2,\n                  3,            4,            5,            6,            7,\n                  8,            9,           10,           11,           12,\n                 13,           14,           15,           16,           17,\n                 18,           19,           20,           21,           22,\n                 23,           24,      'ord_0',      'ord_1',      'ord_2',\n            'ord_3',      'ord_4',      'ord_5', 'hash_nom_5', 'hash_nom_6',\n       'hash_nom_7', 'hash_nom_8', 'hash_nom_9']\n\nX = train_dd[cols_to_use]\n\n# Select target\ny = train_dd.target\n\n# Separate data into training and validation sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from xgboost import XGBRegressor\n\n \n#my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n#my_model.fit(X_train, y_train, \n #            early_stopping_rounds=5, \n  #           eval_set=[(X_valid, y_valid)], \n   #          verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.metrics import mean_absolute_error\n\n#predictions = my_model.predict(X_valid)\n\n# Calculate MAE\n#mae_1 = mean_absolute_error(y_valid, predictions) \n\n\n#print(\"Mean Absolute Error:\" , mae_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.metrics import accuracy_score\n#acc = accuracy_score(y_valid, predictions)\n\n#print(\"accuracy_score:\" , acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_X = test_dd[cols_to_use]\n\n# Use the model to make predictions\n#predicted_target = my_model.predict(test_X)\n# We will look at the predicted prices to ensure we have something sensible.\n#print(predicted_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#my_submission = pd.DataFrame({'Id': test_X.index, 'target': predicted_target})\n# you could use any filename. We choose submission here\n#my_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.linear_model import LogisticRegression\n\n#lr_m = LogisticRegression( solver=\"lbfgs\",max_iter=500,n_jobs=4)\n\n#lr_m.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.metrics import mean_absolute_error\n\n#predictions = lr_m.predict(test_X)\n\n# Calculate MAE\n#mae_1 = mean_absolute_error(y_valid, predictions) \n\n\n#print(\"Mean Absolute Error:\" , mae_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.metrics import accuracy_score\n#acc = accuracy_score(y_valid, predictions)\n\n#print(\"accuracy_score:\" , acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rf_model = RandomForestRegressor(n_estimators= 280,max_depth=40,max_features=11,max_leaf_nodes=350,random_state=1)\n#rf_model.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rf_val_predictions = rf_model.predict(test_X)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using XGBRegressor model with tuning parameter\n\n\ntrying some models but, I found the XGBRegressor the best until now"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nmy_model_2 = XGBRegressor(n_estimators=700, learning_rate=0.2, n_jobs=4)\nmy_model_2.fit(X_train,y_train)\n\ntest_preds = my_model_2.predict(test_X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generating one row  \n#X_rows = X_train.sample(frac =.03) \n  \n# checking if sample is 0.25 times data or not \n  \n#if (0.03*(len(X_train))== len(X_rows)): \n #   print( \"Cool\") \n  #  print(len(X_train))\n   # print('\\n')      \n    #print(len(X_rows))       \n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generating one row  \n#y_rows = y_train.sample(frac =.03) \n  \n# checking if sample is 0.25 times data or not \n  \n#if (0.03*(len(y_train))== len(y_rows)): \n #   print( \"Cool\") \n  #  print(len(y_train))\n   # print('\\n')      \n   # print(len(y_rows))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#parameters = [{'n_estimators': [ 800, 900, 1000], \n #                    'learning_rate': [0.05, 0.1, 0.15, 0.2]\n  #                  }]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import GridSearchCV\n#from xgboost import XGBRegressor\n#gsearch = GridSearchCV(estimator=XGBRegressor(),\n #                      param_grid = parameters, \n  #                     scoring='neg_mean_absolute_error',\n   #                    n_jobs=4,cv=3)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gsearch.fit(X_rows,y_rows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gsearch.best_params_.get('n_estimators'), gsearch.best_params_.get('learning_rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#final_model = XGBRegressor(n_estimators=gsearch.best_params_.get('n_estimators'), \n                          # learning_rate=gsearch.best_params_.get('learning_rate'), \n                           #n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#final_model.fit(X_rows,y_rows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_preds = final_model.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#submission = pd.read_csv('/kaggle/input/cat-in-the-dat/sample_submission.csv', index_col='id')\nsamplesubmission = pd.read_csv('/kaggle/input/cat-in-the-dat/sample_submission.csv', index_col='id')\n\noutput = pd.DataFrame({'Id': samplesubmission.index, 'target': test_preds})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}