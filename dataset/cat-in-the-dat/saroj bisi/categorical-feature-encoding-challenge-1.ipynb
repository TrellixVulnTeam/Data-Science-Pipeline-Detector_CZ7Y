{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import StandardScaler\nSC=StandardScaler()\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBRFClassifier,XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/cat-in-the-dat/train.csv')\ntest=pd.read_csv('/kaggle/input/cat-in-the-dat/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing of Train and test Data together\n# Concatenate Train and Test Data with a new column to identify Train (Type=0) VS Test Data(Type=1)  \ntrain['Type']=pd.DataFrame(np.zeros(len(train)).astype(int))\ntest['Type']=pd.DataFrame(np.ones(len(test)).astype(int))\n\nprint('Original Train Data shape:{} and Test Data shape:{}'.format(train.shape,test.shape))\nfeatures_Data=pd.concat([train.drop(columns=['target','id']),test.drop(columns=['id'])])\nprint('features_Data shape after combining Train and Test ',features_Data.shape)\nlabel=train.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets do some EDA\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfeatures_Data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count plot\nplt.figure(figsize=[12,14])\nfeatures=list(features_Data.columns)\nn=1\nfor f in features:\n    plt.subplot(10,4,n)\n    sns.countplot(features_Data[f])\n    sns.despine()\n    n=n+1\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 400)\nfeatures_Data.nom_5.value_counts().shape # 222 distinct categories\nfeatures_Data.nom_6.value_counts().shape # 522 distinct categories\nfeatures_Data.nom_7.value_counts().shape # 1220 distinct categories\nfeatures_Data.nom_8.value_counts().shape # 2219 distinct categories\nfeatures_Data.nom_9.value_counts().shape # 12068 distinct categories\nfeatures_Data.ord_5.value_counts().shape # 192 distinct categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_Data.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder,OrdinalEncoder,LabelBinarizer\n# Handling of Binary Features\nLB=LabelBinarizer()                           \nfeatures_Data['bin_3']=LB.fit_transform(features_Data.bin_3)\nfeatures_Data['bin_4']=LB.fit_transform(features_Data.bin_4)\n\n# Handling of Nominal Features\nLE=LabelEncoder()\nfeatures_nom=features_Data.loc[:,['nom_0','nom_1','nom_2','nom_3','nom_4','nom_5','nom_6','nom_7','nom_8','nom_9']]\nfeatures_nom=features_nom.apply(LE.fit_transform)\nfeatures_Data[['nom_0','nom_1','nom_2','nom_3','nom_4','nom_5','nom_6','nom_7','nom_8','nom_9']]=features_nom\n\n# Handling of Ordinal Features\nOE=OrdinalEncoder()\nfeatures_ord=features_Data.loc[:,['ord_1','ord_2','ord_3','ord_4','ord_5']]\nfeatures_ord=OE.fit_transform(features_ord)\nfeatures_Data[['ord_1','ord_2','ord_3','ord_4','ord_5']]=features_ord","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_Data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now that we have lableEncoded both Train and Test Data together , its time to split them apart\ntrain_data=features_Data[features_Data.Type==0].drop(columns=['Type'])\ntest_ata=features_Data[features_Data.Type==1].drop(columns=['Type'])\n\n# Its better to convert all label encoded data to int64\ntrain_data=train_data.astype('int64')\ntest_data=test_ata.astype('int64')\ntrain_data['target']=label\n\n#Extrat Features and Label\nfeatures=train_data.drop(columns='target').values\nlabel=train_data.loc[:,'target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets perform Cross validations considering all features and see what could be the best score\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score,classification_report,confusion_matrix\n#from sklearn import metrics\n\ndef stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y):\n    global df_model_selection\n    \n    skf = StratifiedKFold(n_splits, random_state=12,shuffle=True)\n    \n    weighted_f1_score = []\n    #print(skf.split(X,y))\n    for train_index, test_index in skf.split(X,y):\n        X_train, X_test = X[train_index], X[test_index] \n        y_train, y_test = y[train_index], y[test_index]\n        \n        \n        model_obj.fit(X_train, y_train)\n        test_ds_predicted = model_obj.predict( X_test )      \n        weighted_f1_score.append(round(f1_score(y_true=y_test, y_pred=test_ds_predicted , average='weighted'),2))\n        \n    sd_weighted_f1_score = np.std(weighted_f1_score, ddof=1)\n    range_of_f1_scores = \"{}-{}\".format(min(weighted_f1_score),max(weighted_f1_score))    \n    df_model_selection = pd.concat([df_model_selection,pd.DataFrame([[process,model_name,sorted(weighted_f1_score),range_of_f1_scores,sd_weighted_f1_score]], columns =COLUMN_NAMES) ])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import StandardScaler\nSC=StandardScaler()\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBRFClassifier,XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nCOLUMN_NAMES = [\"Process\",\"Model Name\", \"F1 Scores\",\"Range of F1 Scores\",\"Std Deviation of F1 Scores\"]\ndf_model_selection = pd.DataFrame(columns=COLUMN_NAMES)\n\nprocess='ALl Features'\nn_splits = 10\nX=SC.fit_transform(features)\ny=label\n\n# 1.Naive Bayes\nmodel_NB=BernoulliNB()\nmodel_obj=model_NB\nmodel_name='Naive Bayes'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # 2.Logistic Regression\nmodel_LR=LogisticRegression()\nmodel_obj=model_LR\nmodel_name='Logistic Regression'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # 3.Decesion Tree Classifier\nmodel_DTC=DecisionTreeClassifier()\nmodel_obj=model_DTC\nmodel_name='Decesion Tree Classifier'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# 4.Random Forest Classifier\nmodel_RFC=RandomForestClassifier()\nmodel_obj=model_RFC\nmodel_name='Random Forest Classifier'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# 5.XGBoost Classifier\nmodel_XGBC=XGBClassifier()\nmodel_obj=model_XGBC\nmodel_name='XGBoost Classifier'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# 6.Gradient Boosting Classifier\nmodel_GBC=GradientBoostingClassifier()\nmodel_obj=model_GBC\nmodel_name='Gradient Boosting Classifier'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# 7.XGBoost Random Forest Classifier\nmodel_XGBRFC=XGBRFClassifier()\nmodel_obj=model_XGBRFC\nmodel_name='XGBoost Random Forest Classifier'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# 8.Support Vector Machine Classifier\n# model_SVC=SVC()\n# model_obj=model_SVC\n# model_name='Support Vector Machine Classifier'\n# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n\n# 9.SGD Classifier\n# model_sgd = OneVsRestClassifier(SGDClassifier())\n# model_obj=model_sgd\n# model_name='Stochastic Gradient Descent Classifier'\n# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n#10.Gausian Process Classifier\n# model_GPC = GaussianProcessClassifier()\n# model_obj=model_GPC\n# model_name='Gausian Process Classifier'\n# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n#11.Gausian Process Classifier\n# model_KNNC=KNeighborsClassifier()\n# model_obj=model_KNNC\n# model_name='K Nearst Neighbour Classifier'\n# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n#12 Linear Discriminant Analysis\n# model_LDA=LinearDiscriminantAnalysis()\n# model_obj=model_LDA\n# model_name='Linear Discriminant Analysis'\n# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n#Exporting the results to csv\n#df_model_selection.to_csv(\"Model_statistics.csv\",index = False)\ndf_model_selection","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## From above test it seems that the best score could be around 70 to 73%.## We should look for improving the score by feature engineering approach.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets apply Chisquare test of independence and select K-best features\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\nmodelKBest=SelectKBest(score_func=chi2,k='all')\nfinalFeatures=modelKBest.fit_transform(features,label)\nprint(modelKBest.scores_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From above test we can drop below features from modeling and try  \n# ['bin_0','bin_2','bin_3','nom_2','nom_5','nom_8','nom_9','day']\n\n#Features from Chisquare Test\nfeatures=train_data.drop(columns=['bin_0','bin_2','bin_3','nom_2','nom_5','nom_8','nom_9','day','target']).values\n\nCOLUMN_NAMES = [\"Process\",\"Model Name\", \"F1 Scores\",\"Range of F1 Scores\",\"Std Deviation of F1 Scores\"]\ndf_model_selection = pd.DataFrame(columns=COLUMN_NAMES)\n\nprocess='ALl Features'\nn_splits = 10\nX=SC.fit_transform(features)\ny=label\n\n\n# # 5.XGBoost Classifier\nmodel_XGBC=XGBClassifier()\nmodel_obj=model_XGBC\nmodel_name='XGBoost Classifier'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\ndf_model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.stats as stats\n#Lets try to do Chisqueare test of independence one Feature at a time\n#if p_value < 0.05: we reject NULL hypothesis and it means two features are dependent on each other\n# ch2 , p_value , df, exp_freq = stats.chi2_contingency(pd.crosstab(train_data.bin_0,train_data.target))\n# print(p_value)\n# if p_value<0.05:\n#     print('bin_0 and target are related')\n# else:\n#     print('bin_0 variable can be dropped')\n\nfor col in train_data.columns:\n    ch2 , p_value , df, exp_freq = stats.chi2_contingency(pd.crosstab(train_data[col],train_data.target))\n    if p_value >=0.05:\n        print('p_value of the feature is: {} and feature to be dropped is: {}'.format(p_value,col))\n    #else:\n         #print('p_value of the feature is: {} and feature to be considered is: {}'.format(p_value,col))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From above test we can drop only one feature which is : bin_0\n\nfeatures=train_data.drop(columns=['bin_0','target']).values\n\nCOLUMN_NAMES = [\"Process\",\"Model Name\", \"F1 Scores\",\"Range of F1 Scores\",\"Std Deviation of F1 Scores\"]\ndf_model_selection = pd.DataFrame(columns=COLUMN_NAMES)\n\nprocess='ALl Features'\nn_splits = 10\nX=SC.fit_transform(features)\ny=label\n\n\n# # 5.XGBoost Classifier\nmodel_XGBC=XGBClassifier()\nmodel_obj=model_XGBC\nmodel_name='XGBoost Classifier'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\ndf_model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets apply SelectFromModel Feature Selection technique\nfrom sklearn.feature_selection import SelectFromModel\nmodel_XGBC=XGBClassifier()\nselectFeatures=SelectFromModel(estimator=model_XGBC)\nselectFeatures.fit(features,label)\nprint(selectFeatures.get_support())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SelectFromModel technique says the best features are below\n# ['bin_0','bin_4','nom_3','nom_9','ord_0','ord_1','ord_2','ord_3','ord_4']\n\nfeatures=train_data[['bin_0','bin_4','nom_3','nom_9','ord_0','ord_1','ord_2','ord_3','ord_4']].values\n\nCOLUMN_NAMES = [\"Process\",\"Model Name\", \"F1 Scores\",\"Range of F1 Scores\",\"Std Deviation of F1 Scores\"]\ndf_model_selection = pd.DataFrame(columns=COLUMN_NAMES)\n\nprocess='ALl Features'\nn_splits = 10\nX=SC.fit_transform(features)\ny=label\n\n\n# # 5.XGBoost Classifier\nmodel_XGBC=XGBClassifier()\nmodel_obj=model_XGBC\nmodel_name='XGBoost Classifier'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\ndf_model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets try to see feature importance method from Decesion Tree\nCOLUMN_NAMES = [\"Process\",\"Model Name\", \"F1 Scores\",\"Range of F1 Scores\",\"Std Deviation of F1 Scores\"]\ndf_model_selection = pd.DataFrame(columns=COLUMN_NAMES)\n\nfeatures=train_data.drop(columns='target').values\n\nprocess='ALl Features'\nn_splits = 10\nX=SC.fit_transform(features)\ny=label\n\n# # .Decesion Tree Classifier\nmodel_DTC=DecisionTreeClassifier()\nmodel_DTC.fit(X,y)\nprint('Feature Importance from Decesion Tree Clsssifier',model_DTC.feature_importances_)\n\n# # .Random Forest Classifier\nmodel_RFC=RandomForestClassifier()\nmodel_RFC.fit(X,y)\nprint('Feature Importance from RandomForest Classifir',model_RFC.feature_importances_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Importance from Decesion Tree Clsssifier and RandomForest Classifier. Below are the features\n#['nom_5','nom_6','nom_7','nom_8','nom_9','ord_3','ord_4','ord_5']\n\nCOLUMN_NAMES = [\"Process\",\"Model Name\", \"F1 Scores\",\"Range of F1 Scores\",\"Std Deviation of F1 Scores\"]\ndf_model_selection = pd.DataFrame(columns=COLUMN_NAMES)\n\nfeatures=train_data[['nom_5','nom_6','nom_7','nom_8','nom_9','ord_3','ord_4','ord_5']].values\n\nprocess='ALl Features'\nn_splits = 10\nX=SC.fit_transform(features)\ny=label\n\n# 1.Naive Bayes\nmodel_NB=BernoulliNB()\nmodel_obj=model_NB\nmodel_name='Naive Bayes'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # 2.Logistic Regression\nmodel_LR=LogisticRegression()\nmodel_obj=model_LR\nmodel_name='Logistic Regression'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# # 3.Decesion Tree Classifier\nmodel_DTC=DecisionTreeClassifier()\nmodel_obj=model_DTC\nmodel_name='Decesion Tree Classifier'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# 4.Random Forest Classifier\nmodel_RFC=RandomForestClassifier()\nmodel_obj=model_RFC\nmodel_name='Random Forest Classifier'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\n# 5.XGBoost Classifier\nmodel_XGBC=XGBClassifier()\nmodel_obj=model_XGBC\nmodel_name='XGBoost Classifier'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\ndf_model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Conclusion from Feature Engineering Approach SO Far: \n# Based on Chisquare Test of independence, Select From Model & Feature Importance it is obvious that almost all features are dependent on target. \n#Chisquare test of independence and select K-best features will reduce the number of features to some extent without compromising the accuracy too much\n\n#Features from Chisquare Test\nfeatures=train_data.drop(columns=['bin_0','bin_2','bin_3','nom_2','nom_5','nom_8','nom_9','day','target']).values\n\nCOLUMN_NAMES = [\"Process\",\"Model Name\", \"F1 Scores\",\"Range of F1 Scores\",\"Std Deviation of F1 Scores\"]\ndf_model_selection = pd.DataFrame(columns=COLUMN_NAMES)\n\nprocess='Chisquare test K-best features'\nn_splits = 10\nX=SC.fit_transform(features)\ny=label\n\n\n# # 5.XGBoost Classifier\nmodel_XGBC=XGBClassifier()\nmodel_obj=model_XGBC\nmodel_name='XGBoost Classifier'\nstratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n\ndf_model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=train_data.drop(columns=['bin_0','bin_2','bin_3','nom_2','nom_5','nom_8','nom_9','day','target']).values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets try to get the best split score using StratifiedKFold Cross Validation\n\n#Initialize the algo\nmodel=XGBClassifier()\n\n#Initialize StratifiedKFold Method\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=10, \n              random_state=1,\n              shuffle=True)\n\n#Initialize For Loop \n\ni=0\nfor train,test in kfold.split(features,label):\n    i = i+1\n    X_train,X_test = features[train],features[test]\n    y_train,y_test = label[train],label[test]\n    \n    model.fit(X_train,y_train)\n    test_ds_predicted=model.predict(X_test)\n    train_ds_predicted=model.predict(X_train)\n    \n    test_f1_score=round(f1_score(y_true=y_test, y_pred=test_ds_predicted , average='weighted'),2)\n    train_f1_score=round(f1_score(y_true=y_train, y_pred=train_ds_predicted , average='weighted'),2)\n    \n    #print(\"Train Score: {}, Test score: {}, for Sample Split: {}\".format(model.score(X_train,y_train),model.score(X_test,y_test),i))\n    print(\"Train f1-Score: {}, Test f1-score: {}, for Sample Split: {}\".format(train_f1_score,test_f1_score,i))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets extract the Train and Test sample for split 1\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=10, #n_splits should be equal to no of cv value in cross_val_score\n              random_state=1,\n              shuffle=True)\ni=0\nfor train,test in kfold.split(features,label):\n    i = i+1\n    if i == 1:\n        X_train,X_test,y_train,y_test = features[train],features[test],label[train],label[test]\n\n#Final Model\nmodel=XGBClassifier()\nmodel.fit(X_train,y_train)\n\ntest_ds_predicted=model.predict(X_test)\ntrain_ds_predicted=model.predict(X_train)\n\ntest_f1_score=round(f1_score(y_true=y_test, y_pred=test_ds_predicted , average='weighted'),2)\ntrain_f1_score=round(f1_score(y_true=y_train, y_pred=train_ds_predicted , average='weighted'),2)\nprint(\"Train f1-Score: {}, Test f1-score: {}\".format(train_f1_score,test_f1_score))\n\n\ntrain_score=np.round(model.score(X_train,y_train),2)\ntest_score=np.round(model.score(X_test,y_test),2)\nprint('Train Accuracy Score is:{} and  Test Accuracy Score:{}'.format(train_score,test_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix and Classification Report\nfrom sklearn.metrics import confusion_matrix,classification_report\ncm=confusion_matrix(y_true=label, y_pred=model.predict(features))\nCR=classification_report(y_true=label, y_pred=model.predict(features))\nprint('Confusion Matrix:\\n',cm)\nprint('\\n Classification Report:\\n',CR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.predict()\nfinal_test_data=test_data.drop(columns=['bin_0','bin_2','bin_3','nom_2','nom_5','nom_8','nom_9','day']).values\nSC_test_data=SC.fit_transform(final_test_data)\nsubmission=model.predict(SC_test_data)\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.DataFrame(submission,columns=['target'])\nsubmission.insert(0,'id',test['id'])\nsubmission.to_csv('submission.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}