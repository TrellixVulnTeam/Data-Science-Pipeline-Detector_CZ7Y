{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Categorical Feature Encoding\n\nThis notebook is to demonstrate Categorical Encoding Techniques and its performance.As per my keen observation the dataset is of Kaggle user competition data.There are couple of high cardinality and ordinal feature in addition to binary and cyclic features such as day and month.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n#Catagorical conversion libraries\nfrom sklearn.preprocessing import LabelEncoder\nfrom category_encoders import HashingEncoder,TargetEncoder\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold,train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom lightgbm import LGBMClassifier,plot_importance\nfrom sklearn.model_selection import GridSearchCV\nfrom imblearn.over_sampling import SMOTE\n\nimport xgboost as xgb\n\nfrom sklearn.metrics import roc_curve,roc_auc_score\n\nfrom sklearn.metrics import confusion_matrix,classification_report\n\nimport seaborn as sns\nsns.set(style='whitegrid')\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport gc\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/cat-in-the-dat/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/cat-in-the-dat/test.csv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train DF dimension:\",train_df.shape)\nprint(\"Test DF dimension:\",test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target variable value counts .There is a imbalance in target .Have to apply oversampling techniques such as SMOTe,Near Miss\ntrain_df.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Unique Values for each feature\n[(c,train_df[c].unique()) for c in train_df.columns[1:]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical feature encoding\nApplying TargetEncoder Technique which is faster as compared to HashingEncoder Technique."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint(\"Feature Engineering ....\")\n\ntrain_encoded_df = train_df[['id']].copy()\ntest_encoded_df = test_df[['id']].copy()\n\n\n# Target Encoder technique\ntarget_encoding_feat = ['bin_0','bin_1','bin_2','bin_3','bin_4','nom_0','nom_1','nom_2','nom_3','nom_4','nom_5','nom_6','nom_7','nom_8',\n                        'nom_9','ord_0','ord_1','ord_2','ord_3','ord_4','ord_5']\n\nprint(\"Starting Target Encoding .....\")\nte = TargetEncoder(cols=target_encoding_feat,smoothing=1.0)\nte_encoded_df = te.fit_transform(train_df[target_encoding_feat],train_df['target'])\nte_test_encoded_df = te.transform(test_df[target_encoding_feat])\n\nte_encoded_df.columns = 'te_' + te_encoded_df.columns\nte_test_encoded_df.columns = 'te_' + te_test_encoded_df.columns\n\ntrain_encoded_df = pd.concat([train_encoded_df,te_encoded_df],axis=1)\ntest_encoded_df = pd.concat([test_encoded_df,te_test_encoded_df],axis=1)\n\n#print(\"Target Encoding Done!..\")\n\n\n# Features day and month are in cyclic in nature . So using cyclic catagorical encoding technique \nprint(\"Cyclic Encoding begin!..\")\ndef cyclic_feat_encoding(df,col):\n    \n    df['sine_'+col] = np.sin(2 * np.pi * (df[col])/max(df[col]))\n    df['cos_'+col] = np.cos(2 * np.pi * (df[col])/max(df[col]))\n    return df\n\ntrain_df = cyclic_feat_encoding(train_df,'day')\ntrain_df = cyclic_feat_encoding(train_df,'month')\ntest_df = cyclic_feat_encoding(test_df,'day')\ntest_df = cyclic_feat_encoding(test_df,'month')\n\ntrain_encoded_df = pd.concat([train_encoded_df,train_df[['sine_day','cos_day','sine_month','cos_month']]],axis=1)\ntest_encoded_df = pd.concat([test_encoded_df,test_df[['sine_day','cos_day','sine_month','cos_month']]],axis=1)\n\nprint(\"Dimension of Train Encoded DF :\",train_encoded_df.shape)\nprint(\"Dimension of Test Encoded DF :\",test_encoded_df.shape)\n\nprint(\"Feature Engineering Done!..\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y= train_df['target']\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_encoded_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_encoded_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df\ndel test_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definition to plot roc curve\ndef roc_curve_plot(fpr,tpr,auc):\n    fig,ax = plt.subplots()\n    ax.plot(fpr,tpr,'b-',linewidth=2)\n    ax.plot([0,1],[0,1],color='navy',linestyle='--')\n    ax.set_title(f'AUC:{auc}')\n    ax.set(xlabel=\"False Positive Rate\",ylabel=\"True Positive Rate\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"with_target_encoded_cols = train_encoded_df.columns.values.tolist()[1:]\nwith_target_encoded_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_encoded_df[with_target_encoded_cols].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to fit and predict the classifier \ndef fit_clf(clf,X_train,y_train,X_valid,y_valid,t_df):\n        \n    clf.fit(X_train,y_train)\n    preds = clf.predict(X_valid)\n    auc = roc_auc_score(y_valid,preds)\n    print(\"roc_auc_score :\",auc)\n    prep_proba = clf.predict_proba(t_df)[:,1]\n    #fpr,tpr,threshold = roc_curve(y_valid,preds,pos_label=1)\n    #roc_curve_plot(fpr,tpr,auc)\n    return clf,auc,prep_proba","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to split df using startifiedkfold and apply SMOTE Upsampling technique\n\ndef train_model(clf,X,y,test_df,upsample=False,before_split=False):\n    kfold = 10\n    skf = StratifiedKFold(n_splits=kfold)\n    test_pred = 0.0\n    auc_score = 0.0\n    if upsample == False:\n        print(\"Only kfold split ...\")                    \n        for k,(train_idx,valid_idx) in enumerate(skf.split(X,y)):\n            print(\"Split =\",k+1)\n            X_train,X_valid = X.iloc[train_idx],X.iloc[valid_idx]\n            y_train,y_valid = y[train_idx],y[valid_idx]\n\n            ## call classifier \n            clfs,auc,test_pred_proba = fit_clf(clf,X_train,y_train,X_valid,y_valid,test_df)\n            test_pred += test_pred_proba\n            auc_score += auc\n        print(\"Average AUC Score :\",auc_score/kfold)\n        return test_pred/kfold\n        \n\n    elif upsample == True:\n        \n        smote = SMOTE(random_state=42)\n        if before_split == True:\n            print(\"Upsampling before kfold split.....\")\n            x_train_sm,y_train_sm =smote.fit_sample(X,y)\n        \n            for k,(train_idx,valid_idx) in enumerate(skf.split(x_train_sm,y_train_sm)):\n                print(\"Split =\",k+1)\n\n                X_train,X_valid = x_train_sm[train_idx],x_train_sm[valid_idx]\n                y_train,y_valid = y_train_sm[train_idx],y_train_sm[valid_idx]\n\n                ## call classifier \n                clfs,auc,test_pred_proba = fit_clf(clf,X_train,y_train,X_valid,y_valid,test_df)\n                test_pred += test_pred_proba\n                auc_score +=auc\n            print(\"Average AUC Score:\",auc_score/kfold)\n            return test_pred/kfold\n        \n        else:\n            print(\"Upsampling during kfold split.....\")\n            for k,(train_idx,valid_idx) in enumerate(skf.split(X,y)):\n                \n                print(\"Split =\",k+1)\n\n                X_train,X_valid = X.iloc[train_idx],X.iloc[valid_idx]\n                y_train,y_valid = y[train_idx],y[valid_idx]\n\n                x_train_sm,y_train_sm =smote.fit_sample(X_train,y_train)\n                x_valid_sm,y_valid_sm =smote.fit_sample(X_valid,y_valid)\n\n                ## call classifier \n                clfs,auc,test_pred_proba = fit_clf(clf,x_train_sm,y_train_sm,x_valid_sm,y_valid_sm,test_df)\n                test_pred += test_pred_proba\n                auc_score +=auc\n            print(\"Average AUC Score:\",auc_score/kfold)\n            return test_pred/kfold\n    else:\n        print(\"None options\")\n                 \n        \n    print(\"Training done!..\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# sm = SMOTE(random_state=42)\n# x_train_gc,y_train_gc =sm.fit_sample(train_encoded_df[with_target_encoded_cols],y)\n\n# print(\"x_train_gc dim :\",x_train_gc.shape)\n# print(\"y_train_gc dim :\",y_train_gc.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#np.logspace(0,4,10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gridsearch taking too much time so commented this code and noted the C value.\n# C = np.logspace(0,4,5)\n# param_grid = {\"penalty\":['l2'],\"C\":C}\n\n# glr = LogisticRegression(solver='lbfgs',max_iter=10000)\n# grid_search = GridSearchCV(glr,param_grid=param_grid,cv=10)\n# grid_search.fit(train_encoded_df[with_target_encoded_cols],y)\n\n# print(\"Best Parameters :\",grid_search.best_params_)\n# print(\"Best Score :\",grid_search.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best Parameters : {'C': 1.0, 'penalty': 'l2'}\nBest Score : 0.78286"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr_clf = LogisticRegression(solver='lbfgs',C=166.81,penalty='l2',max_iter=4000) \n# predict_prob = train_model(lr_clf,train_encoded_df[with_target_encoded_cols],y,test_encoded_df[with_target_encoded_cols],upsample=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict_prob1 = train_model(LogisticRegression(solver='lbfgs',C=1.0,penalty='l2',max_iter=5000),\n#                            train_encoded_df[with_target_encoded_cols],y,test_encoded_df[with_target_encoded_cols],upsample=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict_prob1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proba_ = train_model(LogisticRegression(solver='lbfgs',C=166.81,penalty='l2',max_iter=5000),\n                           train_encoded_df[with_target_encoded_cols],y,test_encoded_df[with_target_encoded_cols],upsample=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proba_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# AUC 0.76094191\n# lg_reg = LogisticRegression(solver='lbfgs',C=166.81,penalty='l2',max_iter=4000) \n# lg_reg_model = train_model(lg_reg,train_encoded_df[with_target_encoded_cols],y,upsample=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lg_reg_pred_prob = lg_reg_model.predict_proba(test_encoded_df[with_target_encoded_cols])[:,1]\n# lg_reg_pred_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# AUC 0.7598\n# lg_reg_model2 = train_model(lg_reg,train_encoded_df[with_target_encoded_cols],y,upsample=True,before_split=True)\n# lg_reg_pred_prob2 = lg_reg_model2.predict_proba(test_encoded_df[with_target_encoded_cols])[:,1]\n# lg_reg_pred_prob2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Increasing the Max Iteration to 10000  AUC 0.7612564509292037\n\n# lg_reg_10000 = LogisticRegression(solver='lbfgs',C=1.0,penalty='l2',max_iter=10000)\n# lg_reg_model_1000 = train_model(lg_reg_10000,train_encoded_df[with_target_encoded_cols],y,upsample=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lg_reg_pred_prob_10000 = lg_reg_model_1000.predict_proba(test_encoded_df[with_target_encoded_cols])[:,1]\n# lg_reg_pred_prob_10000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Predict Probability"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/cat-in-the-dat/sample_submission.csv')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression submission\n\n# submission['target'] = lg_reg_pred_prob\n# submission.head()\n\nsubmission['target'] = proba_\nsubmission.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}