{"cells":[{"metadata":{},"cell_type":"markdown","source":"> This competition is an  opportunity to study the effects of different categorical features on GBDT algorithms(Lgb/Xgb/CatBoost) and see how gradient boosting algorithms handle Cats.\n> So these kernel series will be an overview on Cats Vs Gbdts.\n> In this kernel we run Lgb by Label encoding and next will specify the cats for Lgb to benchmark the difference\n\n> In future kernels we will investigate different encodings on Lgb and CatBoost as well as XGB.\n Specially follwing encoding will be investigated:\n> - One-Hot-Encoder (OHE) (dummy encoding)\n> - Frequency Encoder\n> - Target/Mean Encoder (TE)\n> - Sum Encoder (Deviation Encoding or Effect Encoding)\n> - Weight Of Evidence Encoder (WOE)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nfrom sklearn.model_selection import TimeSeriesSplit, KFold, StratifiedKFold\n\nimport shap\nshap.initjs()\n\nimport warnings  \nwarnings.filterwarnings('ignore')\n\n\ntrain = pd.read_csv('../input/cat-in-the-dat/train.csv')\ntest = pd.read_csv('../input/cat-in-the-dat/test.csv')\n\ntarget = train['target']\ntrain_id = train['id']\ntest_id = test['id']\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\n\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head(10).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n\ncats_all =[c for c in train.columns if c not in ['day', 'month', 'target', 'id']] \ncats_obj = ['bin_3', 'bin_4', 'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5',\n       'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_1', 'ord_2', 'ord_3', 'ord_4',\n       'ord_5']\n\n\n\nfor col in cats_obj:\n    \n    le = LabelEncoder()\n    le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n    train[col] = le.transform(list(train[col].astype(str).values))\n    test[col] = le.transform(list(test[col].astype(str).values))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n\nlgb_params = {'num_leaves': 23,\n         # 'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.9,\n          'bagging_fraction': 0.9,\n          'min_data_in_leaf': 50,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.008,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3899927210061127,\n          #'reg_lambda': 0.6485237330340494,\n          'random_state': 47, \n            \n         }\n\n\nfolds =KFold(n_splits=3, shuffle=True, random_state=42)\nprint(folds.n_splits)\naucs = list()\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = train.columns\n\n#training_start_time = time()\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(train, target)):\n    #start_time = time()\n    print('Training on fold {}'.format(fold + 1))\n    \n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[test_idx], label=target.iloc[test_idx])\n    clf = lgb.train(lgb_params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds=300)\n    oof[test_idx] = clf.predict(train.iloc[test_idx], num_iteration=clf.best_iteration)\n    \n    feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n    aucs.append(clf.best_score['valid_1']['auc'])\n    \n    predictions += clf.predict(test, num_iteration=clf.best_iteration) / folds.n_splits\n    \n    #print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\nprint('-' * 50)\nprint('Training has finished.')\n\nprint('Mean auc:', np.mean(aucs))\nprint('-' * 50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"feature_importances['average'] = feature_importances[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(15, 10))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(25), x='average', y='feature');\nplt.title('25 TOP feature importance over {} folds average'.format(folds.n_splits));","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#shap_values = shap.TreeExplainer(clf).shap_values(train)\n\n#shap.summary_plot(shap_values, train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">##### here we specify categorical feats for lgb . The parameters didn't changed  but added 2 additive parameters (**'min_data_per_group' and 'cat_smooth'**)\n\n>Note: tuning lgb will get better result"},{"metadata":{"trusted":false},"cell_type":"code","source":"\n\nlgb_params = {'num_leaves': 23,\n         \n          'feature_fraction': 0.9,\n          'bagging_fraction': 0.9,\n          'min_data_in_leaf': 50,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.008,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.2,\n          'random_state': 42, \n              \n        'min_data_per_group': 200, # reduce overfitting when using categorical_features\n        'cat_smooth': 50 #reduce the effect of noises in categorical features\n            \n         }\n\n\nfolds =KFold(n_splits=3, shuffle=True, random_state=42)\nprint(folds.n_splits)\naucs = list()\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = train.columns\n\n#training_start_time = time()\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(train, target)):\n    #start_time = time()\n    print('Training on fold {}'.format(fold + 1))\n    \n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx], categorical_feature=cats_all)\n    val_data = lgb.Dataset(train.iloc[test_idx], label=target.iloc[test_idx], categorical_feature=cats_all)\n    clf = lgb.train(lgb_params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds=200)\n    oof[test_idx] = clf.predict(train.iloc[test_idx], num_iteration=clf.best_iteration)\n    \n    feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n    aucs.append(clf.best_score['valid_1']['auc'])\n    \n    predictions += clf.predict(test, num_iteration=clf.best_iteration) / folds.n_splits\n    \n    \nprint('-' * 50)\nprint('Mean auc:', np.mean(aucs))\nprint('-' * 50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"feature_importances['average'] = feature_importances[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(15, 10))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(25), x='average', y='feature');\nplt.title('TOP 25 feature importance over {} folds average'.format(folds.n_splits));","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#shap_values = shap.TreeExplainer(clf).shap_values(train)\n\n#shap.summary_plot(shap_values, train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub = pd.DataFrame({'id': test_id, 'target': predictions})\nsub.to_csv('sub_lgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">We can see the feature imporatance changed."},{"metadata":{},"cell_type":"markdown","source":">Lgb sorts the categories according to the training objective at each split. More specifically, LightGBM sorts the histogram (for a categorical feature) according to its accumulated values (sum_gradient / sum_hessian) and then finds the best split on the sorted histogram. So the split can be made based on the variable being of one specific level or any subset of levels. You have 2^N splits available in comparision with e.g of 4 for OHE.\n\n>The algorithm behind above mechanism is  Fisher (1958) to find the optimal split over categories.\nhttp://www.csiss.org/SPACE/workshops/2004/SAC/files/fisher.pdf\n\nTBD...\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}