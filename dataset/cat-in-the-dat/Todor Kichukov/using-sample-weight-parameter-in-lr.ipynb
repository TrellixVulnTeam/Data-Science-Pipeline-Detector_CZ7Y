{"cells":[{"metadata":{},"cell_type":"markdown","source":"LogisticRegression's fit() method has parameter sample_weights, which could be used to assign individual weights to the samples. According to documantation:\n> sample_weight : array-like, shape (n_samples,) optional\n\n> Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.\n\nIn this kernel we'll compare results with no sample weights and setting weights according to class distribution in train datatset. As our two classes are not evenly distributed in train dataset(but **they ARE in test dataset!**), I expect that model will benefit from setting \"balancing\" weights to the samples of minority class."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#First load train dataset\ntrain = pd.read_csv('/kaggle/input/cat-in-the-dat/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the purpose of this kernel is just to demonstrate use of sample_weight parameter, we'll just use One Hot Encoding for all columns, and no fancy data preprocessing."},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_cols=['bin_0','bin_1','bin_2','bin_3','bin_4','nom_0','nom_1','nom_2','nom_3','nom_4','nom_5','nom_6','nom_7','nom_8','nom_9','ord_0','ord_1','ord_2','ord_3','ord_4','ord_5','month', 'day']\ntrain = pd.get_dummies(train, columns=dummy_cols, sparse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because we want to test results with balanced dataset (as is test.csv), we'll pick first 3000 rows from each class as validation dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#validation dataset\nvalidate = pd.concat([train[train['target']==0].head(3000),train[train['target']==1].head(3000)]).reset_index(drop=True)\n\n#drop validation rows from train\ntrain=train[~train.id.isin(validate.id)].reset_index(drop=True)\n\n#get target column, then drop 'id' and  'target' from the two dataframes\ntarget = train['target']\ntrain = train.drop(['id','target'], axis=1)\ntarget_val = validate['target']\nvalidate = validate.drop(['id','target'], axis=1)\n\n#convert to sparse \ntrain = train.sparse.to_coo().tocsr()\nvalidate = validate.sparse.to_coo().tocsr()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's define a helper function - Logistic regression classifier and normalized confussion matrix plot(no need to write code twice)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def lr_classifier(x, y, x_val, y_val, sample_weight):\n    lr = LogisticRegression(solver = 'lbfgs', C = 0.1, max_iter=1000)\n    lr.fit(x,y,sample_weight) \n    y_pred = lr.predict(x_val)\n    \n    cm = confusion_matrix(y_val, y_pred )\n    cm = cm.astype('float') / cm.sum(axis=1)\n\n    plt.matshow(cm)\n    plt.title('Confusion matrix')\n    for (i, j), z in np.ndenumerate(cm):\n        plt.text(j, i, '{:0.2f}'.format(z), ha='center', va='center', bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we'll try LogisticRegression withouth sample_weights.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_classifier(train,target,validate,target_val,sample_weight = None)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's set weights. Since we have 208 236 samples for class 0 and 91 764 samples for class 1, I'll set weight 2.27 for all samples of minority class (we have 2.27 times less samples for class 1)."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_weight= target.apply( lambda x: 2.27 if x==1 else 1)\nlr_classifier(train,target,validate,target_val,sample_weight)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see clearly, that \"weighted\" example performs much better on balanced dataset. You can try it on the real data (test.csv, which, as I already mentioned, is balanced) and find the difference for yourself. \n\nHappy kaggling :))))"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}