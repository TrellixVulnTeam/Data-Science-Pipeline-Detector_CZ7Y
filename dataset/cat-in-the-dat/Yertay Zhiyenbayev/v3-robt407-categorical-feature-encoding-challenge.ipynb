{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt # for plots\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.preprocessing import OneHotEncoder\none_hot = OneHotEncoder()\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel_enc = LabelEncoder()\n\nfrom sklearn.feature_extraction import FeatureHasher\nfeature_hashing = FeatureHasher(input_type='string')\n\n'''\n Use Logistic Regression and return the accuracy score\n'''\ndef exec_logistic(X,y, Max_iter):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 1/5)\n    \n    log_reg = LogisticRegression(max_iter = Max_iter)\n    log_reg.fit(X_train, y_train)\n    y_pred = log_reg.predict(X_test)\n    \n    print('Accuracy score: ', accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"> Let us study the types of the data first. The most interesting fact is that **ALL** features are categorical."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"d_train = pd.read_csv(\"/kaggle/input/cat-in-the-dat/train.csv\")\nd_test = pd.read_csv(\"/kaggle/input/cat-in-the-dat/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We know that in most of the cases the missing values are represented by **NaN** and **None** in Pandas. Luckily, both types are treated interchangeably when it is applicable. For instance, we can check if our data set has any missing values in the following way.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of missing values in the train set:', d_train.isna().sum().sum())\nprint('Number of missing values in the test set:', d_test.isna().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the missing values are absent."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nsns.pairplot(d_train, height = 2.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distributions depict the presence of the order. Most importantly, we can clearly see that the problem is binary in nature and the data set is imbalanced. (You can see it from the histogram in the rightmost column of the bottom row)."},{"metadata":{},"cell_type":"markdown","source":"Let us separate columns of type 'object' from integers."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = d_train.drop(['target'],axis=1)\n\narray_objects = [] # Store the names of the columns of type 'o'\narray_integers = [] # Store the names of the columns of integer type \n\nfor col in zip(X.columns):\n  if(X[col[0]].dtype == 'object'):\n    array_objects.append(col[0])\n  else:\n    array_integers.append(col[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[array_integers].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[array_objects].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Label Encoding"},{"metadata":{},"cell_type":"markdown","source":"Let us label the data in the columns of type 'o' (object)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encoding of the columns of type 'object'\nfor col in zip(array_objects):\n  X[col[0]] = label_enc.fit_transform(X[col[0]])\n\nX","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see now that all the columns contain only numerical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nexec_logistic(X, d_train['target'], 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One-Hot Encoding"},{"metadata":{},"cell_type":"markdown","source":"Let us try One-Hot encoding now. Notice that in this case, we increase the number of features considerably."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = d_train.drop(['target'],axis = 1)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = one_hot.fit_transform(X)\n\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of features increases from just 24 to 316461. This is explained by high cardinalities of some features. For instance, 'nom_5' has the cardinality equal to 222,'nom_6' - 522, 'nom_7' - 1220 respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nexec_logistic(X, d_train['target'], 1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Partly One-Hot Encoding"},{"metadata":{},"cell_type":"markdown","source":"I choose some part of the features for One-Hot encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = d_train.drop(['target'],axis = 1)\n\n# Label encoding of the columns of type 'object'\nfor col in zip(array_objects):\n  X[col[0]] = label_enc.fit_transform(X[col[0]])\n\nnom_2_one_hot = one_hot.fit_transform(X.nom_2.values.reshape(-1,1)).toarray()\nnom_3_one_hot = one_hot.fit_transform(X.nom_3.values.reshape(-1,1)).toarray()\nnom_4_one_hot = one_hot.fit_transform(X.nom_4.values.reshape(-1,1)).toarray()\n\nday_one_hot = one_hot.fit_transform(X.day.values.reshape(-1,1)).toarray()\nmonth_one_hot = one_hot.fit_transform(X.month.values.reshape(-1,1)).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_nom_2 = pd.DataFrame(nom_2_one_hot, columns = [\"nom2_\"+str(int(i)) for i in range(nom_2_one_hot.shape[1])])\nX = X.drop(['nom_2'],axis = 1)\nX = pd.concat([X, df_nom_2], axis = 1)\n\ndf_nom_3 = pd.DataFrame(nom_3_one_hot, columns = [\"nom3_\"+str(int(i)) for i in range(nom_3_one_hot.shape[1])])\nX = X.drop(['nom_3'],axis=1)\nX = pd.concat([X, df_nom_3], axis=1)\n\ndf_nom_4 = pd.DataFrame(nom_4_one_hot, columns = [\"nom4_\"+str(int(i)) for i in range(nom_4_one_hot.shape[1])])\nX = X.drop(['nom_4'],axis=1)\nX = pd.concat([X, df_nom_4], axis=1)\n\ndf_day = pd.DataFrame(day_one_hot, columns = [\"day_\"+str(int(i)) for i in range(day_one_hot.shape[1])])\nX = X.drop(['day'],axis=1)\nX = pd.concat([X, df_day], axis=1)\n\ndf_month = pd.DataFrame(month_one_hot, columns = [\"month_\"+str(int(i)) for i in range(month_one_hot.shape[1])])\nX = X.drop(['month'],axis=1)\nX = pd.concat([X, df_month], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nexec_logistic(X, d_train['target'], 1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that by one-hot encoding part of the features, we do not achieve any improvement."},{"metadata":{},"cell_type":"markdown","source":"# Feature Hashing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = d_train.drop(['target'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nX_hash = X.copy()\nfor c in X.columns:\n    X_hash[c] = X[c].astype('str')      \n\nX_feature_hashed = feature_hashing.transform(X_hash.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nexec_logistic(X_feature_hashed, d_train['target'], 2000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that Feature Hashing can compete with One-Hot Encoding. "},{"metadata":{},"cell_type":"markdown","source":"# Cross-fold Target Encoding + Feature Hashing"},{"metadata":{},"cell_type":"markdown","source":"I want to avoid overfitting from usual Target Encoding. This is why I use cross-fold version. Recall that in usual Target Encoding we replace the categorical data by the probability or the mean value of the corresponding target value. This will bring the distribution of the categorical data closer to the the distribution of the target values. It may be helpful to recall the pair-plot that we produced earlier in the section 'Data Preparation'."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = d_train\nX_fold = X.copy()\n\nto_add = ['ord_0','day','month']\n\nfor add in zip(to_add):\n  array_objects.append(add[0])\n\nX_fold['bin_3'] = label_enc.fit_transform(X_fold['bin_3'])\nX_fold['bin_4'] = label_enc.fit_transform(X_fold['bin_4'])\nX_fold[array_objects] = X_fold[array_objects].astype('object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nkf = KFold(n_splits = 5, shuffle = False, random_state = 0)\nfor train_ind,val_ind in kf.split(X):\n    for col in zip(array_objects):\n        replaced=dict(X.iloc[train_ind][[col[0],'target']].groupby(col[0])['target'].mean())\n        X_fold.loc[val_ind,col[0]]=X_fold.iloc[val_ind][col[0]].replace(replaced).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_fold.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nX_hash = X.copy()\nfor c in X.columns:\n    X_hash[c] = X[c].astype('str')      \n\nX_feature_hashed = feature_hashing.transform(X_hash.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nexec_logistic(X_feature_hashed, d_train['target'], 2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nlinear_param_grid = {'C': np.array([1e-2, 1e-1, 1, 4]),\n                     'solver': ['newton-cg', 'lbfgs', 'saga'],\n                     'class_weight': ['balanced', None]}\n\nlog_reg = LogisticRegression(max_iter = 2000)\n\nclf = GridSearchCV(log_reg, linear_param_grid , cv = 3, n_jobs = -1, scoring = 'roc_auc', verbose = 1)\n\nbest_model = clf.fit(X_feature_hashed, d_train['target'])\n\nprint(best_model.best_estimator_)\nprint(best_model.best_score_)\nprint(best_model.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary"},{"metadata":{},"cell_type":"markdown","source":"I have tried different encoding techniques including Label encoding, One-Hot encoding, Feature Hashing and the combination of the cross-fold target encoding and Feature Hashing. The latter turned out to be the most accurate one notwithstanding its apparent complexity. The GridSearchCv 3-fold validation has yielded the best score **0.8814**."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}