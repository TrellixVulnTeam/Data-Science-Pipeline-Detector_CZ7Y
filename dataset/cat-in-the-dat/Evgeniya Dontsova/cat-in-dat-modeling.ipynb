{"cells":[{"metadata":{},"cell_type":"markdown","source":"This challenge is devoted to binary classification problem, with every feature being categorical. Please see the corresponding EDA here: https://www.kaggle.com/evgeniya1/eda-for-cat-in-dat.\n\n## **Preprocessing based on EDA**\n\nFirst, preprocess data by properly transforming it, as well as fixing an order in the couple ordinal variables and minmax normalizing it. Then split the data into the training and test sets to train and validate the model on different data samples."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport matplotlib.pyplot as plt # for plotting\nimport matplotlib.gridspec as gridspec # to do the grid of plots\n# jupyter cell magic for inline visualization\n%matplotlib inline \n\nimport seaborn as sns # for plotting\nsns.set(style='whitegrid') # for plotting style\n\nfrom IPython.display import display\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\n#target encoding\nfrom category_encoders.target_encoder import TargetEncoder\n#category encoding\nfrom category_encoders import WOEEncoder\n\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n\nfrom itertools import combinations\n\nimport gc; gc.enable()\n\n#setting to suppress SettingWithCopy\npd.set_option('mode.chained_assignment', None)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#set random seed\nSEED = 42\nnp.random.seed(SEED)\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#######preprocess data#######\ndef preprocess(df_train, add_interact = 0, add_all_combs = 0):\n    \"\"\"This function takes cat-in-dat dataframe \n    and preprocess the columns as follows:\n    map binary to T,F values,\n    MinMax normalizes cyclic_cols,\n    label encode the ordinal and fix some orders.\n    It can add selected \"best\" interactions between features, \n    i.e by setting add_interact=1,\n    or add interactions from all combinations of features,\n    i.e. add_all_combs=1.\n    \"\"\"\n    df = df_train.copy()\n    \n    #group columns by type\n    bin_cols = [col for col in df.columns if 'bin' in col]\n    nom_cols = [col for col in df.columns if 'nom' in col]\n    ord_cols = [col for col in df.columns if 'ord' in col]\n    cyclic_cols = ['day','month']\n   \n    #relabel the binary columns to 0,1\n    for col in ['bin_3', 'bin_4']:\n        df[col] = df[col].map({\"T\": 1, \"F\": 0, \"Y\":1, \"N\": 0})\n        \n    #transform bin columns to T,F for woe encoding\n    bin_cols = [col for col in df.columns if 'bin' in col]\n    for col in bin_cols:\n        df[col] = df[col].apply(lambda x: 'T' if x==1 else 'F')\n           \n    #transform cyclic columns for woe\n    for col in cyclic_cols:\n        df[col] = df[col].astype('str')\n        \n#     #Minmax normalization \n#     for col in cyclic_cols:\n#         df[col] = (df[col] - df[col].min()) / \\\n#                   (df[col].max() - df[col].min())\n\n    #ordinary variables: transform to categorical features\n    for col in ord_cols:\n        df[col] = df[col].astype('category').cat.as_ordered()\n      \n    #change order for the specific ordinal columns\n    df.ord_1 = df.ord_1.cat.reorder_categories(['Novice','Contributor',\n                                                'Expert','Master','Grandmaster'])\n    df.ord_2 = df.ord_2.cat.reorder_categories(['Freezing', 'Cold','Warm','Hot',\n                                                'Boiling Hot','Lava Hot'])\n\n\n    #Label encode the ordinary variables and normalize\n    for col in ord_cols:\n        df[col] = df[col].cat.codes\n        df[col] = df[col] / df[col].nunique()\n        \n\n    #Add interactions\n    if add_interact:\n        add_combs = [('nom_0', 'nom_5'), ('nom_0', 'nom_4'), ('nom_2', 'nom_3')] \\\n                    + [('bin_1', 'bin_4'), ('bin_0', 'bin_1'), ('bin_3', 'bin_4')] \\\n                    + [('bin_1', 'nom_0'), ('bin_1', 'nom_5'), ('bin_4', 'nom_4')] \\\n                    + [('month', 'nom_0'), ('month', 'bin_1'), ('day', 'nom_0')]\n        \n        if add_all_combs:\n            no_target_cols = [col for col in df.columns if col not in ['target']]\n            add_combs = combinations(no_target_cols,2)\n\n        for comb in add_combs:\n            df[str(comb)] = list(zip(df[comb[0]],df[comb[1]]))\n\n    return df\n\n\n#functions\ndef get_score(model, X_train, y_train, X_test, y_test, _print = 1, _gam = 0):\n    \"\"\"This function takes trained model instance, train and test data.\n    It fits the model and computes ROC AUC score and accuracy\n    for test and train respectively.\"\"\"\n        \n    #predicted probas, to account for different format for GAM model output:\n    if _gam:\n        y_test_prob = model.predict_proba(X_test)\n        y_train_prob = model.predict_proba(X_train)      \n    else:\n        y_test_prob = model.predict_proba(X_test)[:,1]\n        y_train_prob = model.predict_proba(X_train)[:,1]       \n  \n    #compute the scores\n    #roc auc score\n    auc_score_test = roc_auc_score(y_test, y_test_prob)\n    auc_score_train = roc_auc_score(y_train, y_train_prob)\n    auc = [auc_score_test, auc_score_train]\n\n    #accuracy\n    accuracy_test = accuracy_score(y_test, model.predict(X_test))    \n    accuracy_train = accuracy_score(y_train,model.predict(X_train))\n    acc = [accuracy_test, accuracy_train]\n    \n    if _print:\n        print(f'test ROC_AUC = {round(auc_score_test,5)}, \\\n              train ROC_AUC = {round(auc_score_train,5)}', \n              f'test Accuracy = {round(auc_score_train,5)}, \\\n              train Accuracy = {round(accuracy_train,5)}', sep='\\n')\n    return auc, acc\n\n\ndef get_score_cv(model, X, y, scoring='roc_auc',cv=5,_print = 1):\n    \"\"\"This function takes trained model, features and target\n    and scoring metric. It returns the cv score vector.\"\"\"\n\n    y_pred = model.predict_proba(X)[:,1]\n    crossValScores = cross_val_score(model, X, y, cv=10,scoring=scoring)\n    score_mean = crossValScores.mean()\n    score_std = crossValScores.std()\n    \n    if _print:\n        print(f\"CV {scoring} score is: \\\n              {score_mean.round(4)} +/- {score_std.round(4)}\") \n    \n    return crossValScores\n\n\ndef plot_target_dist(df, cols, figsize = (16,10), grid_r=3, grid_c=3):\n    \n    grid = gridspec.GridSpec(grid_r,grid_c) # The grid of chart\n    fig = plt.figure(figsize=figsize) # size of figure\n    total = df.shape[0] # total number of observations\n\n    # loop to get column and the count of plots\n    for n, col in enumerate(df[cols]): \n        ax = plt.subplot(grid[n]) # feeding the figure of grid\n        \n        #for low cardinality data\n        if df[col].nunique() < 14:\n            #count plot\n            sns.countplot(x=col, data=df, hue='target', palette='Paired',\n                          order=df[col].sort_values().unique(),ax=ax) \n            #df.groupby([col,'target'])[col].count().unstack(level=1)\\\n            #  .plot(kind='bar', color = [\"#a6cee3\", \"#1f78b4\"], width=0.8, ax=ax)\n            sizes=[] # Get highest values in y\n            for p in ax.patches: # loop to all objects\n                height = p.get_height()\n                sizes.append(height)\n                ax.text(p.get_x()+p.get_width()/2.,\n                        height * 1.02,\n                        '{:1.1f}%'.format(height/total*100),\n                        ha=\"center\", fontsize=14) \n            ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n\n        #for high cardinality data\n        else:\n            df_col = df.groupby([col,'target'])[col].count()\\\n                       .unstack(level=1).fillna(0).sort_index()\n            #define the plot type \n            if df[col].nunique() < 200:\n                df_col.plot(kind='bar', ax=ax, stacked=True,\n                            color = [\"#a6cee3\", \"#1f78b4\"], width=1)\n            else: \n                df_col.plot(kind='line', ax=ax, \n                            color = [\"#a6cee3\", \"#1f78b4\"])\n                        \n            #force number of xticks to show\n            ax.xaxis.set_major_locator(plt.MaxNLocator(20) )\n            \n            \n        #set labels\n        ax.set_ylabel('Count', fontsize=15) # y axis label\n        ax.set_title(f'{col} distribution by target', fontsize=16) # title label\n        ax.set_xlabel(f'{col} values', fontsize=15) # x axis label\n        _xlim = ax.get_xlim()\n        \n        #calculate pct of class 1\n        d = df.groupby([col,'target'])[col].count()\\\n              .unstack(level=1).fillna(0).sort_index()\n        d['class1_pct'] = d[1] / (d.sum(axis=1))\n        if (d.index.dtype == 'int'):\n            d.index = d.index - d.index.min()\n        elif (d.index.dtype == 'float'):\n            #to fix the scale\n            d.index = d.index * df[col].nunique()\n        \n        #add another y-axis to show the pct of class 1\n        ax2 = ax.twinx()\n        if df[col].nunique() < 200:\n            d.class1_pct.plot(marker='o',markersize=5,ax=ax2,color=[\"#6a3d9a\"])\n        else:\n            d.class1_pct.plot(marker='o',markersize=5,linewidth=0,\n                              ax=ax2,color=[\"#6a3d9a\"])\n            \n        ax2.set_ylabel('class 1 fraction', color=\"#6a3d9a\", fontsize=15)\n        ax2.set_xlim(_xlim)\n        ax2.set_ylim([-0.1,1.1])\n        ax2.grid(False)\n        \n    #!!!!!!!need to fix missing xlabels, problem appears when twinx axis is added\n    plt.tight_layout()\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Read in train and test data from csv files'''\ndf_train = pd.read_csv('../input/cat-in-the-dat/train.csv',index_col=0)\ndf_test = pd.read_csv('../input/cat-in-the-dat/test.csv',index_col=0)\n\n#group columns by type\ntarget = 'target'\nbin_cols = [col for col in df_train.columns if 'bin' in col]\ncyclic_cols = ['day','month']\nord_cols = [col for col in df_train.columns if 'ord' in col]\nnom_cols = [col for col in df_train.columns if 'nom' in col]\nno_target = [col for col in df_train.columns if 'target' not in col]\n\n#preprocess data \n###########preprocess###########\ndf = preprocess(df_train)\n#################################\n\nX = df[no_target]\ny = df[target]\n\n#split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.3333, \n                                                    random_state=SEED)\ndisplay(X_train.head())\n\n#delete df\ndel df; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Baseline model: using vtreat**\n\nOne of the simplest and good to try model for binary classification problem is a logistic regression model. To start, the model should be initialized first. Note that the imbalance in classification problem can be overcomed by using 'balanced' for class_weight keyword when initializing the model, see below."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic regression model\nlogit = LogisticRegression(random_state=SEED,class_weight='balanced',solver='liblinear')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How to choose the better enconing method for a given categorical variable? The quickest way is to use the vtreat package (github.com/WinVector/pyvtreat), that was developed to preprocess messy real world data for predictive modeling. It can handle missing values in the feature set, does not allow the missing values in the target variable. There is a version for binary classification problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"##install the package\n!pip install vtreat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the package\nimport vtreat\n\n#apply vtreat\nvtreat_enc = vtreat.BinomialOutcomeTreatment(\n    outcome_name='target',    # outcome variable\n    outcome_target=True) # outcome of interest\n\n#vtreat encoding using different techniques\n#here use all features, including high-cardinality\nX_train_vtreat = vtreat_enc.fit_transform(X_train, y_train)\nX_test_vtreat = vtreat_enc.transform(X_test)\n\n#feature selection: only recommended\nvtreat_fs = vtreat_enc.score_frame_[vtreat_enc.score_frame_.recommended == True]\\\n                      .sort_values(by = 'significance')\nprint(f'There are {len(vtreat_fs)} recommended features.')\n\n#display first 10 recommended features\nvtreat_fs.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use all recommended features\nused_cols = vtreat_fs.variable.to_list()\n\n#fit the model\nlogit.fit(X_train_vtreat[used_cols], y_train)\n#print the ROC-AUC score and Accuracy\nget_score(logit, X_train_vtreat[used_cols], \n                         y_train, X_test_vtreat[used_cols], y_test)\nprint('\\n')\nget_score_cv(logit, X_train_vtreat[used_cols], y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pd.DataFrame(vtreat_fs.orig_variable.value_counts())\nprint('Included recommended original features are: ', vtreat_fs.orig_variable.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it can be seen from the unique list of recommended original variables, bin_0 and bin_3 original features are not included in the recommended set, which is in agreement with their low contrast found in EDA. The package handled well the high-cardinality nominal features, given the high roc_auc score with all feature included. In case if you do WoE encoding to all features (except ordinal) and compare the logistic regression model results with and without high-cardinality nominal columns, not using them improves the test auc score noticeably: with high-card 0.7174 versus without 0.7660. \nLarge number of features suggests that there are multiple recommended features per original variable. Multiplicity of recommended original features is shown below for 'day' variable (there are 7 new encoded features)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#how many encoded features per original variable?\nprint(\"\\nExample for 'day' feature:\")\nvtreat_fs[vtreat_fs.orig_variable == 'day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#select only best among recomennded for a given original variable\nselect_cols = []\nfor var in vtreat_fs.orig_variable.value_counts().index:\n    #select only one encoded feature for \n    select_cols.append(vtreat_fs[vtreat_fs.orig_variable == var].iloc[0,0])\n    \n#print(select_cols)\nprint(f'There are {len(select_cols)} selected features out of {len(vtreat_fs)} recommended.')\nselect_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's use only 22 selected from the recommended set of features for modelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"#use all recommended features\nused_cols = select_cols\n\n#fit the model\nlogit.fit(X_train_vtreat[used_cols], y_train)\n#print the ROC-AUC score and Accuracy\nget_score(logit, X_train_vtreat[used_cols], \n                         y_train, X_test_vtreat[used_cols], y_test)\nprint('\\n')\nget_score_cv(logit, X_train_vtreat[used_cols], y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC score is roughly identical, but number of features is almost two times less than initially recommended, i.e. 21 instead of initial 59, which is a great improvemet for the model interpretability.\n\nTo summarize, vtreat package is a great way to build the initial baseline model with included feature preprocessing (i.e. cleaning and encoding) tool.\n\n### Handling the high-cardinality variables\n\nTo handle high-cardinality nominal features, Bayesian smoothing with sigmoidal logistic function is used."},{"metadata":{"trusted":true},"cell_type":"code","source":"#define feature sets\nno_high_card_cols = bin_cols + cyclic_cols + nom_cols[:5] + ord_cols \n#all features for modeling\nused_cols = bin_cols + cyclic_cols + nom_cols + ord_cols \n\n#WoE encoding\nwoe_enc = WOEEncoder(random_state=SEED, randomized=True).fit(X_train[used_cols], y_train)\nX_train_woe = woe_enc.transform(X_train[used_cols].reset_index(drop=True))\nX_test_woe = woe_enc.transform(X_test[used_cols].reset_index(drop=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#smoothing function\ndef sig(n,m,sh):\n    return 1/(1+np.exp(-m*(n-sh)))\n\n#manually optimized parameters\nsig_params = {'nom_5':[10000,0.0001], 'nom_6':[8000,0.00005],\n              'nom_7':[5000,0.002],'nom_8':[1000,0.0015],\n              'nom_9':[300,0.01]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Target smooting\ny_avg = y_train.mean()\n\nfor col in nom_cols[5:]:\n    #smothing parameters\n    m = sig_params[col][0]\n    sh = sig_params[col][1]\n    \n    #targer encoding\n    t_enc = TargetEncoder(smoothing=10.0)\n    X_train[col + '_te'] = t_enc.fit_transform(X_train[col],y_train)\n    X_test[col + '_te'] = t_enc.transform(X_test[col])\n\n    ### FREQUENCY ENCODING and weight\n    ccol = col + '_te'\n    #for ccol in [col + '_te']:\n    # size of each category\n    encoding = X_train.groupby(ccol).size()\n    # get frequency of each category\n    encoding = encoding/len(X_train)\n    X_train[ccol + '_freq'] = X_train[ccol].map(encoding)\n    X_train[ccol + '_weight'] = sig(X_train[ccol + '_freq'],m,sh)\n\n    #encoding the test set\n    encoding = X_test.groupby(ccol).size()\n    encoding = encoding/len(X_test)\n    X_test[ccol + '_freq'] = X_test[ccol].map(encoding)\n\n    #target smooting\n    n = X_train[ccol + '_freq']\n    y_est = np.exp(X_train[ccol])/(1+np.exp(X_train[ccol]))\n    y_adj = sig(n,m,sh)*y_est + (1-sig(n,m,sh))*y_avg\n    X_train[ccol.strip('_te') + '_adj'] = np.log(y_adj / (1-y_adj))\n\n    #test set\n    n = X_test[ccol + '_freq']\n    y_est = np.exp(X_test[ccol])/(1+np.exp(X_test[ccol]))\n    y_adj = sig(n,m,sh)*y_est + (1-sig(n,m,sh))*y_avg\n    X_test[ccol.strip('_te') + '_adj'] = np.log(y_adj / (1-y_adj))\n\n    #X_train[[col + '_te',col + '_adj',col + '_te_freq',col + '_te_weight']].head(10)\n\n    #visualize how low in number observations are suppressed for nom_9\n    if col == 'nom_9':\n        fig, (ax,ax3) = plt.subplots(1,2,figsize=(12,4))\n\n        #plot frequency of given observation\n        X_train[col + '_te_freq'].plot(kind='hist',bins=20, ax=ax)\n\n        #add second y-axis\n        x = np.linspace(0,X_train[col + '_te_freq'].max(),30)#ax.get_xticks()\n        ax2 = ax.twinx()\n        ax2.plot(x,sig(x,m,sh),color=\"#6a3d9a\")\n        ax2.set_ylabel('weight', color=\"#6a3d9a\", fontsize=15)\n        ax2.set_ylim([-0.1,1.1])\n        ax2.grid(False)\n        ax.set_xlabel('count_pct')\n        ax.set_title(f'Distribution of count percentage for {col}.')\n\n        X_train_woe[col].plot(kind='hist',ax=ax3,color=\"#fb9a99\",label='original WoE',alpha=0.5)\n        X_train[col + '_adj'].plot(kind='hist',ax=ax3,color=\"#33a02c\",label='adjusted WoE',alpha=0.5)\n        ax3.legend()\n        ax3.set_title(f'Comparison of initial and smoothed WoE for {col}')\n        plt.tight_layout()\n\n    #drop no longer needed columns\n    X_train.drop([ccol for ccol in X_train.columns if col + '_te' in ccol],axis=1,inplace=True); gc.collect()\n    X_test.drop([ccol for ccol in X_test.columns if col + '_te' in ccol],axis=1,inplace=True); gc.collect()\n\n#     #add adjusted\n#     X_tn = pd.concat([X_train_woe[no_high_card_cols],\n#                       X_train[[col + '_adj']].reset_index(drop=True)],axis=1)\n#     X_ts = pd.concat([X_test_woe[no_high_card_cols],\n#                       X_test[[col + '_adj']].reset_index(drop=True)],axis=1)\n\n#     #fit the model\n#     logit.fit(X_tn, y_train)\n#     #print the ROC-AUC score and Accuracy\n#     utility.get_score(logit, X_tn, y_train, X_ts, y_test)\n\n#     #delete temporary variables\n#     del X_tn, X_ts; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#adjusted nominal features with vtreat encoding for the rest\nnom_adj = [col for col in X_train.columns if '_adj' in col]\nno_high_card_vtreat_cols = [col for col in select_cols \n                            if col not in \n                            [col + '_logit_code' for col in nom_cols[5:]]]\n\nX_tn = pd.concat([X_train_vtreat[no_high_card_vtreat_cols],\n                      X_train[nom_adj].reset_index(drop=True)],axis=1)\nX_ts = pd.concat([X_test_vtreat[no_high_card_vtreat_cols],\n                      X_test[nom_adj].reset_index(drop=True)],axis=1)\n\n#fit the model\nlogit.fit(X_tn, y_train)\n#print the ROC-AUC score and Accuracy\nget_score(logit, X_tn, y_train, X_ts, y_test)\nprint('\\n')\nget_score_cv(logit, X_tn, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparison for WoE encoding with high-cardinality not-smoothed and smoothed."},{"metadata":{"trusted":true},"cell_type":"code","source":"#WoE encoding with high-cardinality data, no smoothing\n#to demonstrate that high-cardinality nominal feature cause overfitting\nall_cols = bin_cols + cyclic_cols + nom_cols + ord_cols \n\nused_cols = all_cols\n#fit the model\nlogit.fit(X_train_woe[used_cols], y_train)\n#print the ROC-AUC score and Accuracy\nget_score(logit, X_train_woe[used_cols], y_train, X_test_woe[used_cols], y_test)\nprint('\\n')\nget_score_cv(logit, X_train_woe[used_cols], y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#WoE encoding without high-cardinality data\nused_cols = no_high_card_cols\n\n#fit the model\nlogit.fit(X_train_woe[no_high_card_cols], y_train)\n#print the ROC-AUC score and Accuracy\nget_score(logit, X_train_woe[used_cols], y_train, X_test_woe[used_cols], y_test)\nprint('\\n')\nget_score_cv(logit, X_train_woe[used_cols], y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model trained without high-cardinality nominal features gives noticeable better roc_auc score for test set. Model does not suffer from overfitting, as confirmed by the very close roc_auc and accuracy scores between train and test sets. This problem with high-cardinality features can be spotted right from EDA, nevertheless, it is good to have confirmation from the model scores. To address the stability of the current model, 10-fold cross validation is used for roc_auc score. From the results shown above, logistic regresion model gives reasonably stable score."},{"metadata":{"trusted":true},"cell_type":"code","source":"#WoE encoding with high-cardinality data, with smoothing\nnom_adj = [col for col in X_train.columns if '_adj' in col]\nno_high_card_cols = bin_cols + cyclic_cols + nom_cols[:5] + ord_cols \n\nX_tn = pd.concat([X_train_woe[no_high_card_cols],\n                      X_train[nom_adj].reset_index(drop=True)],axis=1)\nX_ts = pd.concat([X_test_woe[no_high_card_cols],\n                      X_test[nom_adj].reset_index(drop=True)],axis=1)\n\n#fit the model\nlogit.fit(X_tn, y_train)\n#print the ROC-AUC score and Accuracy\nget_score(logit, X_tn, y_train, X_ts, y_test)\nprint('\\n')\nget_score_cv(logit, X_tn, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is an improvement in test score if compare simple WoE encoding with WoE high-cardinality, without high-cardinality, with smoothed WoE high-cardinality, but there is still some noticeable overfit. More work with smoothing is needed.\nThe graph of the receiver operating characteristic (ROC) curve, related to the scoring metric for the cat-in-dat competition, is shown below for completeness."},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the roc_auc curve\ny_score = logit.decision_function(X_ts)\nfpr, tpr, _ = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)\n    \nplt.figure()\nplt.plot(fpr, tpr, color='darkgreen',\n         lw=2, label='ROC curve (area = %0.3f)' %roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0]), plt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=15)\nplt.ylabel('True Positive Rate', fontsize=15)\nplt.title('Receiver operating characteristic', fontsize=15)\nplt.legend(loc=\"lower right\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}