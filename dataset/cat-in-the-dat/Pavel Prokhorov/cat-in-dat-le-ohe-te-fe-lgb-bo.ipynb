{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/cat-in-the-dat/train.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/cat-in-the-dat/test.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save columns that we will use for feature aggregates"},{"metadata":{"trusted":true},"cell_type":"code","source":"fa_features = [\n    'bin_0', 'bin_1',\n    'nom_5', 'nom_6'\n]\n\ntrain_fa = train[fa_features].copy()\ntest_fa = test[fa_features].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def summary(df):\n    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name', 'dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n    return summary\n\n\nsummary(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Idea:**\n* LabelEncoder for true/false features\n* OneHotEncoder for features with number of unique values <=30\n* TargetEncoder for features with many unique values\n* LabelEncoder for other features"},{"metadata":{"trusted":true},"cell_type":"code","source":"ohe_features = [\n    'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4',\n    'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4',\n    'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4',\n    'day', 'month'\n]\n\nle_features = list(set(test.columns) - set(ohe_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One hot encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_part = len(train)\ndf = pd.get_dummies(pd.concat([train, test], axis=0), columns=ohe_features)\ntrain = df[:train_part]\ntest = df[train_part:].drop('target', axis=1)\ndel df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n\ndef encode_categorial_features_fit(df, columns_to_encode):\n    encoders = {}\n    for c in columns_to_encode:\n        if c in df.columns:\n            encoder = LabelEncoder()\n            encoder.fit(df[c].astype(str).values)\n            encoders[c] = encoder\n    return encoders\n\ndef encode_categorial_features_transform(df, encoders):\n    out = pd.DataFrame(index=df.index)\n    for c in encoders.keys():\n        if c in df.columns:\n            out[c] = encoders[c].transform(df[c].astype(str).values)\n    return out\n\n\n# le_features = test.columns\n\ncategorial_features_encoders = encode_categorial_features_fit(\n    pd.concat([train, test], join='outer', sort=False), le_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = encode_categorial_features_transform(train, categorial_features_encoders)\ncolumns_to_drop = list(set(le_features) & set(train.columns))\ntrain = train.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = encode_categorial_features_transform(test, categorial_features_encoders)\ncolumns_to_drop = list(set(le_features) & set(test.columns))\ntest = test.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import TargetEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"te_features = [\n    'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"te = TargetEncoder(cols=te_features, drop_invariant=True, return_df=True, min_samples_leaf=2, smoothing=1.0)\nte.fit(train[te_features], train['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = te.transform(train[te_features])\ncolumns_to_drop = list(set(te_features) & set(train.columns))\ntrain = train.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = te.transform(test[te_features])\ncolumns_to_drop = list(set(te_features) & set(test.columns))\ntest = test.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature aggregates"},{"metadata":{"trusted":true},"cell_type":"code","source":"le_features = fa_features\n\ncategorial_features_encoders = encode_categorial_features_fit(\n    pd.concat([train_fa, test_fa], join='outer', sort=False), le_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = encode_categorial_features_transform(train_fa, categorial_features_encoders)\ncolumns_to_drop = list(set(le_features) & set(train_fa.columns))\ntrain_fa = train_fa.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = encode_categorial_features_transform(test_fa, categorial_features_encoders)\ncolumns_to_drop = list(set(le_features) & set(test_fa.columns))\ntest_fa = test_fa.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Add feature aggregates"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_aggregates(df, feature_to_group_by, feature):\n    out = pd.DataFrame(index=df.index)\n    agg = df.groupby([feature_to_group_by])[feature].value_counts(normalize=True)\n    freq = lambda row: agg.loc[row[feature_to_group_by], row[feature]]\n    out[feature + '__' + feature_to_group_by + '_freq'] = df.apply(freq, axis=1)\n    return out\n\n\nfor feature in ['nom_5__bin_0', 'nom_6__bin_1']:\n    feature_1, feature_2 = feature.split('__')\n    print('Add feature:', feature, '/ aggregates of', feature_2, 'by', feature_1)\n    \n    agg = make_aggregates(train_fa, feature_2, feature_1)\n    train = train.merge(agg, how='left', left_index=True, right_index=True)\n    del agg\n    \n    agg = make_aggregates(test_fa, feature_2, feature_1)\n    test = test.merge(agg, how='left', left_index=True, right_index=True)\n    del agg\n\ndel train_fa\ndel test_fa","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Free memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"# From https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#        else:\n#            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extract target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['target'].copy()\nx_train = train.drop('target', axis=1)\ndel train\n\nx_test = test.copy()\ndel test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom bayes_opt import BayesianOptimization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(num_leaves, min_data_in_leaf, max_depth, bagging_fraction, feature_fraction, lambda_l1, lambda_l2):\n    \n    params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'boosting_type': 'gbdt',\n        'is_unbalance': False,\n        'boost_from_average': True,\n        'num_threads': 4,\n        \n        'num_leaves': int(num_leaves),\n        'min_data_in_leaf': int(min_data_in_leaf),\n        'max_depth': int(max_depth),\n        'bagging_fraction' : bagging_fraction,\n        'feature_fraction' : feature_fraction,\n        'lambda_l1': lambda_l1,\n        'lambda_l2': lambda_l2\n    }\n    \n    scores = []\n    \n    cv = KFold(n_splits=10, random_state=42)\n    for train_idx, valid_idx in cv.split(x_train, y_train):\n        \n        x_train_train = x_train.iloc[train_idx]\n        y_train_train = y_train.iloc[train_idx]\n        x_train_valid = x_train.iloc[valid_idx]\n        y_train_valid = y_train.iloc[valid_idx]\n        \n        lgb_train = lgb.Dataset(data=x_train_train.astype('float32'), label=y_train_train.astype('float32'))\n        lgb_valid = lgb.Dataset(data=x_train_valid.astype('float32'), label=y_train_valid.astype('float32'))\n        \n        lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=100)\n        y = lgb_model.predict(x_train_valid.astype('float32'), num_iteration=lgb_model.best_iteration)\n        \n        score = roc_auc_score(y_train_valid.astype('float32'), y)\n        print('Fold score:', score)\n        scores.append(score)\n    \n    average_score = sum(scores) / len(scores)\n    print('Average score:', average_score)\n    return average_score\n\n\nbounds = {\n    'num_leaves': (31, 100),\n    'min_data_in_leaf': (20, 100),\n    'max_depth':(-1, 100),\n    'bagging_fraction' : (0.1, 0.9),\n    'feature_fraction' : (0.1, 0.9),\n    'lambda_l1': (0, 2),\n    'lambda_l2': (0, 2)\n}\n\nbo = BayesianOptimization(train_model, bounds, random_state=42)\nbo.maximize(init_points=20, n_iter=20, acq='ucb', xi=0.0, alpha=1e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bo.max","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'is_unbalance': False,\n    'boost_from_average': True,\n    'num_threads': 4,\n    \n    'num_iterations': 10000,\n    'learning_rate': 0.006,\n    'early_stopping_round': 100,\n    \n    'num_leaves': int(bo.max['params']['num_leaves']),\n    'min_data_in_leaf': int(bo.max['params']['min_data_in_leaf']),\n    'max_depth': int(bo.max['params']['max_depth']),\n    'bagging_fraction' : bo.max['params']['bagging_fraction'],\n    'feature_fraction' : bo.max['params']['feature_fraction'],\n    'lambda_l1': bo.max['params']['lambda_l1'],\n    'lambda_l2': bo.max['params']['lambda_l2']\n    \n#    'num_leaves': 94,\n#    'min_data_in_leaf': 61,\n#    'max_depth': 31,\n#    'bagging_fraction' : 0.12033530139527615,\n#    'feature_fraction' : 0.18631314159464357,\n#    'lambda_l1': 0.0628583713734685,\n#    'lambda_l2': 1.2728208225275608\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 10\n\ny = np.zeros(x_test.shape[0])\noof = np.zeros(x_train.shape[0])\nfeature_importances = []\n\ncv = KFold(n_splits=n_splits, random_state=42)\nfor train_idx, valid_idx in cv.split(x_train, y_train):\n    \n    x_train_train = x_train.iloc[train_idx]\n    y_train_train = y_train.iloc[train_idx]\n    x_train_valid = x_train.iloc[valid_idx]\n    y_train_valid = y_train.iloc[valid_idx]\n    \n    lgb_train = lgb.Dataset(data=x_train_train.astype('float32'), label=y_train_train.astype('float32'))\n    lgb_valid = lgb.Dataset(data=x_train_valid.astype('float32'), label=y_train_valid.astype('float32'))\n    \n    lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=100)\n    \n    y_part = lgb_model.predict(x_test.astype('float32'), num_iteration=lgb_model.best_iteration)\n    y += y_part / n_splits\n    \n    oof_part = lgb_model.predict(x_train_valid.astype('float32'), num_iteration=lgb_model.best_iteration)\n    oof[valid_idx] = oof_part\n    \n    score = roc_auc_score(y_train_valid.astype('float32'), oof_part)\n    print('Fold score:', score)\n    \n    feature_importances.append(lgb_model.feature_importance())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfeature_importance_df = pd.concat([\n    pd.Series(x_train.columns),\n    pd.Series(np.mean(feature_importances, axis=0))], axis=1)\nfeature_importance_df.columns = ['featureName', 'importance']\n\ntemp = feature_importance_df.sort_values(by=['importance'], ascending=False)\n\nplt.figure(figsize=(12, 20))\nsns.barplot(x=\"importance\", y=\"featureName\", data=temp)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature correlation map"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = feature_importance_df.sort_values(by=['importance'], ascending=False).head(15)\nmost_important_features = temp['featureName'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\ncor = x_train[most_important_features].corr()\nsns.heatmap(cor, annot=True, annot_kws={\"size\": 8}, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# From https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = pd.Series([0,1])\n\nplot_confusion_matrix(y_train, oof.round(), classes=classes, normalize=True, title='Confusion matrix')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submit predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/cat-in-the-dat/sample_submission.csv', index_col='id')\nsubmission['target'] = y\nsubmission.to_csv('lightgbm.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save OOF"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_df = pd.DataFrame(index=pd.read_csv('../input/cat-in-the-dat/train.csv', index_col='id').index)\noof_df['oof'] = oof\noof_df.to_csv('oof.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}