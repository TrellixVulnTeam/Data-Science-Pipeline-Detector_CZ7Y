{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import csr_matrix\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom tensorflow import keras\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.preprocessing import minmax_scale\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Reading into DataFrames"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/cat-in-the-dat/train.csv')\ntest = pd.read_csv('/kaggle/input/cat-in-the-dat/test.csv')\nsubmission = pd.read_csv('/kaggle/input/cat-in-the-dat/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = train.target\ntrain.drop('target', axis=1 , inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.concat([train, test]).reset_index(drop=True)\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Re-Defining Data Types:\n* bin columns: bool (binary), but will set them to unit8 for classifier feeding.\n* nom columns: should be one-hot encoded into bool (nominal)\n* ord columns: should be crafted into integers after find the correct sequence (ordinal)"},{"metadata":{},"cell_type":"markdown","source":"# Handling Binary Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_cols = [bin_col for bin_col in all_data.columns if bin_col.startswith('bin')]\nall_data['bin_3'] = all_data.bin_3.apply(lambda x: 1 if x == 'T' else 0)\nall_data['bin_4'] = all_data.bin_4.apply(lambda x: 1 if x == 'Y' else 0)\nall_data[bin_cols] = all_data[bin_cols].astype(np.uint8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Ordinal Columns\n> These columns needs to be handled one by one.."},{"metadata":{"trusted":true},"cell_type":"code","source":"ord_cols = [ord_col for ord_col in all_data.columns if ord_col.startswith('ord')]\n\n#ord_1\nall_data['ord_1'] = all_data.ord_1.map({'Novice': 0, 'Contributor': 1, 'Expert': 3, 'Master': 4, 'Grandmaster': 5}).astype(np.uint8)\n\n#ord_2\nall_data['ord_2'] = all_data.ord_2.map({'Freezing': 0, 'Cold': 1, 'Warm': 2, 'Hot': 3, 'Boiling Hot': 4, 'Lava Hot': 5}).astype(np.uint8)\n\n#ord_3\ndef ord_mapper(col, ord_map={}, df=all_data, i=0):\n    sorted_uniques = df[col].sort_values().unique()\n    for item in sorted_uniques:\n        ord_map[item] = i\n        i += 1\n    return ord_map\n\nall_data['ord_3'] = all_data.ord_3.map(ord_mapper('ord_3')).astype(np.uint8)\n\n#ord_4\nall_data['ord_4'] = all_data.ord_4.map(ord_mapper('ord_4')).astype(np.uint8)\n\n#ord_5\nall_data['ord_5'] = all_data.ord_5.map(ord_mapper('ord_5')).astype(np.uint8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize these ordinals\nall_data[ord_cols] = minmax_scale(all_data[ord_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Cyclical Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# changing month and day to cyclical feature\nall_data['day_sin'] = np.sin((all_data.day-1)*(2.*np.pi/30))\nall_data['day_cos'] = np.cos((all_data.day-1)*(2.*np.pi/30))\nall_data['month_sin'] = np.sin((all_data.month-1)*(2.*np.pi/12))\nall_data['month_cos'] = np.cos((all_data.month-1)*(2.*np.pi/12))\n\n# drop ordinal ones\nall_data.drop(['day', 'month'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Nominal Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"nom_cols = [nom_col for nom_col in all_data.columns if nom_col.startswith('nom')]\nsparse_nom = pd.get_dummies(all_data[nom_cols], drop_first=True, sparse=True)\nall_data.drop(nom_cols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Choosing A Classifier\nWhen dealing with large datasets on a limited RAM, you need to be extra cautious about what is happening behind the scenes. Most classifiers require a certain format/dtypes to work on the fly, otherwise, it will copy the data to the format it works on, Thus using more RAM\n\n> For LinearSVC (and LogisticRegression) any input passed as a numpy array will be copied and converted to the liblinear internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input we suggest to use the SGDClassifier class instead. The objective function can be configured to be almost the same as the LinearSVC model.\n\nAt this point you have four choices:\n* Change the np.array dtypes to match the format you would use (if it fits in RAM)\n* Use the SGD classifier with loss function similar to the original classifier you wanted to use as SGD uses stochastic gradient descent and works with sparse matrices without copying.\n* Use a totally different classifier that works with the data in your current format\n* Use feature selection to limit the amount of data you will feed to the model (I have done this)"},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(all_data.shape, sparse_nom.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sparse matrix is very large, we have 16k features which would make the classification very slow and memory intensive. I will attempt to choose only the most important ones, densify them and re-join with the main dataframe.\nWe are limited to feature selectors that can deal with sparse matrices directly which are: \n* chi2\n* mutual_info_classif\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a sparse csr matrix\nsparse_matrix = sparse_nom.to_sparse().to_coo().tocsr()\n\n# selecting top 7.5% effective features using chi2\nk = int(sparse_nom.shape[1] * 0.075)\nkbest_chi_nom_features_selector = SelectKBest(chi2, k=k).fit(sparse_matrix[:len(train)], targets)\nnom_features_mask = kbest_chi_nom_features_selector.get_support()\nnom_features_mask[:22] = True # Force dummies of nom_1 to nom_4 to be included as they are only 22 column\nsparse_nom_selected = sparse_nom.iloc[:, nom_features_mask]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combining all data again, the normal DF and the sparse backed DF\n> Be careful here, while the join would work directly I suggest to densify the sparse DF first as it produces weird results if you try to slice or sample from it for validation!"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = all_data.join(sparse_nom_selected.to_dense().astype(np.uint8))\ntrain = all_data.iloc[:len(train)].set_index('id')\ntest = all_data.iloc[len(train):].set_index('id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A Logit?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This will take a long time..\nclf = LogisticRegressionCV(Cs=50, cv=11, verbose=0, n_jobs=4, scoring='roc_auc', \n                           solver='sag', max_iter=200, tol=0.001)\nclf.fit(train, targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({\"id\": test.index, \"target\": clf.predict_proba(test)[:, 1]}).to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}