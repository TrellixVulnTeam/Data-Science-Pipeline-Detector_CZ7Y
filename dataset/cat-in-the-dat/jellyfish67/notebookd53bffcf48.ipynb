{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read **training and testing datasets** form csv as a dataframe"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(r'../input/cat-in-the-dat/train.csv')\ntestset = pd.read_csv(r'../input/cat-in-the-dat/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seperating the training set into Input and target."},{"metadata":{"trusted":true},"cell_type":"code","source":"X= dataset.loc[:,:'month']\nY= dataset.loc[:,'target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combining data for **OneHotEncoding**"},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata = pd.concat((X,testset))\nalldata.drop('id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(str(X.shape[0])+\" rows of X\")\nprint(str(testset.shape[0])+\" rows of testSet\")\nprint(str(alldata.shape[0])+\" rows of Combined\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying OneHotEncoding\nI chose One Hot Encoding because Label Encoding was giving lesser score(0.71) and loss was greater in the given no. of epochs ie. model was less accurate.\n\n<a href='#comparison'>**See comparison**</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"ohcInstance=OneHotEncoder()\nohcInstance.fit(alldata)\nalldata=ohcInstance.transform(alldata)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After Onehot encoding we get roughly 16000 columns,form 24."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"After one hot encoding no. of columns become \"+str(alldata.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reducing back to X and Test_X(testSet)."},{"metadata":{"trusted":true},"cell_type":"code","source":"X=alldata[0:300000]\nTest_X=alldata[300000:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining the Model\nUsed Multilayer perceptron. \nSince, Our problem is kind of a logistic regression problem. A single layered perceptron would have done the work. \nBut to increase the accuracy, multiple layers are used(for early convergence also) and  non-linearity is introduced by 'RELU' activation functions. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.Sequential([\n                                 tf.keras.layers.Dense(512,input_dim=X.shape[1],activation='relu'),\n                                tf.keras.layers.Dense(128,activation='relu'),\n                                 tf.keras.layers.Dense(64,activation='relu'),\n                                  tf.keras.layers.Dense(1, activation='sigmoid')\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model Fitting**\nIn order to avoid overfitting no. of epochs and batchsize are tweaked by hit and trial to get highest accuracy and minimum loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"history_OneHot=model.fit(X, np.asarray(Y).astype(np.int32), epochs=11, batch_size=10000, verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Writing in to the output file\nie ./submission.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(Test_X)\nsubmit = pd.concat([testset['id'], pd.Series(predictions[:,0]).rename('target')], axis=1)\nsubmit.to_csv('submission.csv', index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training same model,But Label Encoding is Used.\n**To prove our comparison** "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(r'../input/cat-in-the-dat/train.csv')\n\nX= dataset.iloc[:,1:24].values\nY= dataset.iloc[:,24].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def labelEncode(listData,index):\n    labelEncoder=LabelEncoder()\n    listData[:,index]=labelEncoder.fit_transform(listData[:,index]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.iloc[:,1:24]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"if We observe the dataset only columns 3-14 and 16-21 are having **Categorical Data**.\nHence, Applying encoding on that data only."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfor i in range(3,15):\n    labelEncode(X,i)\nfor i in range(16,21):\n    labelEncode(X,i)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.Sequential([\n                                 tf.keras.layers.Dense(512,input_dim=X.shape[1],activation='relu'),\n                                tf.keras.layers.Dense(128,activation='relu'),\n                                 tf.keras.layers.Dense(64,activation='relu'),\n                                  tf.keras.layers.Dense(1, activation='sigmoid')\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_label=model.fit(np.asarray(X).astype(np.int32), np.asarray(Y).astype(np.int32), epochs=11, batch_size=10000, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT LOSS AND ACCURACY\n%matplotlib inline\n\nimport matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\noneacc=history_OneHot.history['acc']\nlabelacc=history_label.history['acc']\noneloss=history_OneHot.history['loss']\nlabelloss=history_label.history['loss']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Label Encoding vs One Hot Encoding\n<a id=\"comparison\">**The Following is the comparison of Accuracy and Loss for Both the techniques.**</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=range(len(oneacc)) # Get number of epochs\n\nplt.plot(epochs, oneacc, 'b', \"Label Encoding Accuracy\")\nplt.plot(epochs, labelacc, 'r', \"OneHotEncoding Accuracy\")\nplt.title('Difference between accuracy of Label and OneHotEncoder')\nplt.figure()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see in the above plot, Accuracy of OneHotEncoder approches 1 nearby 6th epoch. Which in the case of Labelled Encoder has very low slope so will take more no. of epochs to reach."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs, oneloss, 'r', \"Label Encoding  Loss\")\nplt.plot(epochs, labelloss, 'b', \"OneHotEncoding Loss\")\nplt.title('Difference between Loss of Label and OneHotEncoder')\nplt.figure()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see in the above plot, Loss of OneHotEncoder is very less from the initial Epoch, which in the case of Labelled Encoder will take a large no. of epochs to reach so.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}