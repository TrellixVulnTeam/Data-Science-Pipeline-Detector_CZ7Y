{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Summary => <br>\nThis notebook includes the following topics. <br><br>\n\nThe notebook will be constructed in two stages. <br>\n* 1st Stage -> Complete python implementations along with brief descriptions. (Est. Date of Completion - 28-03-2021)\n* 2nd Stage -> Solving questions on these topics using python. (Est. Date of Completion - 31-04-2021)\n* Continuous Development and Improvisations....","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents\n\n* Understanding Data types\n    * Interval Scale\n    * Binary \n    * Categorical\n    * Ordinal \n    * Ratio Scaled\n    * Mixed Type\n* Different types of distances\n* Simmilarity and Dissimilarity Matrix\n* Familiarizing with different types of Error Metrics\n* Handling Missing data values\n* Central Tendency & Dispersion\n* Descriptive Statistics\n* Summary Statistics\n    * Central Tendency Statistics\n        * Arithmetic Mean\n        * Weighted Mean\n        * Median\n        * Percentile\n    * Dispersion\n        * Skewness\n        * Kurtosis\n        * Range\n        * Interquartile Range\n        * Variance\n        * Standard Score\n        * Coefficient of Variation\n* [Sample](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#16.-Sample-Statistics) vs [Population statistics](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#17.-Population-Statistics)\n* Random Variables\n* Probability Distribution Functions\n    * Uniform Distribution\n    * Exponential Distribution\n    * [Binomial Distribution](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#8.-Binomial-Distribution)\n    * [Normal Distributions](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#9.-Normal-Distribution)\n    * [Poisson Distributions](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#10.-Poisson-Distribution)\n    * [Bernoulli Distribution](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#11.-Bernoulli-Distribution)\n* [Measuring p-value](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#13.-Calculating-p-Value)\n* [Measuring Correlation](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#14.-Measuring-Correlation)\n* [Measuring Variance](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#15.-Measuring-Variance)\n* Expected Value\n\n* [z-score](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#5.-Z-Test)\n* Hypothesis Testing\n    * Null & Alternate Hypothesis\n    * Type 1 Error; Type 2 Error\n    * Various Approaches\n        * p-value\n        * critical value\n        * confidence interval value\n* z-stats vs t-stats\n\n* Two Sample Tests\n* Confidence Interval\n* Similarity & Dissimilarity Matrices\n* [Central Limit Theorem](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#12.-Central-Limit-Theorem)\n* [Chi Square Test](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#3.-Chi-Square-Test)\n* [T Test](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#4.-T-Test)\n* [ANOVA Test](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#6.-ANOVA-Test)\n    * [One Way Anova Test](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#6.1-One-Way-ANOVA-Test)\n        * F Test (LSD Test)\n        * Tukey Kramer Test\n    * [Two Way Anova Test](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#6.2-Two-Way-ANOVA-Test)\n        * Interaction Effects\n* [F Stats](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#7.-F-Stats-Test)\n* [Regressions (Linear, Multiple) + ROC](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#2.-Regressions)\n* Logistic Regression\n    * Python Implementation\n    * Calculating G Statistics\n* Residual Analysis\n* Maximum Likelihood Estimation\n* Cluster Analysis\n    * Partitioning Cluster Methods\n        * K-Means\n        * K Mediods\n    * Hierarchial Cluster Methods\n        * Agglomerative\n    * Density Based Cluster Methods\n        * DBSCAN\n* [CART Algorithms](https://www.kaggle.com/antoreepjana/statistics-for-ml-data-analysis/#1.-CART-Algorithms)\n    * Python Implementation\n    * various Calculations involved\n        * Information Gain\n        * Gain Ratio\n        * Gini Index\n* Confusion Metrics, ROC & Regression Analysis\n* Bonus Topics\n    * Classification Thresholding\n    * Prediction Bias\n    * Sampling Methods\n        * Simple\n        * Convenience\n        * Systematic\n        * Cluster\n        * Stratified","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport random\nimport statistics\nfrom scipy import stats","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.seed(2021)\nnp.random.seed(2021)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. CART Algorithms","metadata":{}},{"cell_type":"markdown","source":"Brief Description -> ","metadata":{}},{"cell_type":"markdown","source":"##### Tools Used\n\nDataset Used -> Boston Dataset (UCI Machine Learning Repository)","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_boston\nboston_dataset = load_boston()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boston = pd.DataFrame(boston_dataset.data, columns = boston_dataset.feature_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boston.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boston['MEDV'] = boston_dataset.target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = boston_dataset.feature_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"array = boston.values\n\nX = array[:, 0:13]\nY = array[:, 13]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = 1234)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DecisionTreeRegressor(max_leaf_nodes = 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"YHat = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r2 = r2_score(Y_test, YHat)\nprint(\"R2 Score -> \", r2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### plot the decision tree as a graph ","metadata":{}},{"cell_type":"code","source":"import graphviz\nfrom sklearn import tree","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"method 1","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(model, \n                   feature_names=names,  \n                   class_names=boston_dataset.target,\n                   filled=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"method 2","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (20,20))\ndot_data = tree.export_graphviz(model, out_file=None, \n                                feature_names=names,  \n                                class_names=boston_dataset.target,\n                                filled=True, rounded= True)\n\n# Draw graph\ngraph = graphviz.Source(dot_data, format=\"png\") \ngraph","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll learn how to custom paint your graph from the default settings (coming soon)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"import pydotplus\ngraph = pydotplus.graph_from_dot_data(dot_data)\nnodes = graph.get_node_list()\n\nfor node in nodes:\n    if node.get_label():\n        print(node.get_label())\n        node.set_fillcolor('yellow')\n        \n\ngraph.write_png('colored_tree.png')\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Regressions","metadata":{}},{"cell_type":"markdown","source":"Regression is a problem where you need to find a function that maps some features or variables to others sufficiently well.<br>\nDependent features are called the dependent variables. <br>\nIndependent features are called the independent variables. <br>\n\nRegression is useful to forecast a response using a set of predictors. <br>\nWhen implementing linear regression of some dependent variable y on set of independent variables x, 𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽ᵣ𝑥ᵣ + 𝜀. is called the regression equation. 𝜀 is the random error.","metadata":{}},{"cell_type":"markdown","source":"Useful Resources -> <br>\n\n* https://www.maths.usyd.edu.au/u/UG/SM/STAT3022/r/current/Lecture/lecture03_2020JC.html#1\n* https://towardsdatascience.com/maximum-likelihood-estimation-explained-normal-distribution-6207b322e47f#:~:text=%E2%80%9CA%20method%20of%20estimating%20the,observed%20data%20is%20most%20probable.%E2%80%9D&text=By%20assuming%20normality%2C%20we%20simply,the%20popular%20Gaussian%20bell%20curve.\n* https://online.stat.psu.edu/stat462/node/207/\n* https://psychscenehub.com/psychpedia/odds-ratio-2/\n* http://statkat.com/stat-tests/logistic-regression.php#:~:text=Logistic%20regression%20analysis%20tests%20the,%3D%CE%B2K%3D0","metadata":{}},{"cell_type":"markdown","source":"Regression analysis involves identifying the relationship between a dependent variable and one or more independent variables. <br>\nA model of the relationship is hypothesized, and estimates of the parameter values are used to develop an estimated regression equation. <br>\n<br><br>\n**Regression Model** <br>\nIn simple linear regression, the model used to describe the relationship between a single dependent variable y and a single independent variable x is y = β0 + β1x + ε. <br>\nβ0 and β1 are referred to as the model parameters,and ε is a probabilistic error term that accounts for the variability in y that cannot be explained by the linear relationship with x <br><br>\nThe difference between the observed value of y and the value of y predicted by the estimated regression equation is called a residual.<br>","metadata":{}},{"cell_type":"markdown","source":"**Key Assumptions ->** <br>\n* ε is a random variable with an expected value of 0\n* the variance of ε is the same for all values of x\n* the values of ε are independent,\n* ε is a normally distributed random variable.","metadata":{}},{"cell_type":"markdown","source":"**Residual Analysis ->** <br><br>\nThe analysis of residuals plays an important role in validating the regression model. <br>\nIf the error term satisfies the above 4 assumptions, then the regression model is valid. ","metadata":{}},{"cell_type":"markdown","source":"So-called dummy variables are used to represent qualitative variables in regression analysis. In general, k - 1 dummy variables are needed to model the effect of a qualitative variable that may assume k values.","metadata":{}},{"cell_type":"markdown","source":"An F-Test based on MSR / MSE can be used to test the statistical significance of the overall relationship between the dependent variable and the set of independent variables. <br>\nLarge values of F support the conclusion that the overall is statistically significant. ","metadata":{}},{"cell_type":"markdown","source":"**Key Terms ->** <br><br><br>\n* Coefficient of Determination -> R^2, tells you the amount of variation in y based on the dependence on x. Larger R^2 indicates a better fit and means that the model can \n* Coefficient of Correlation -> \n* SSE ->\n* SSR -> Sum of Squared Residuals\n* SST -> \n* Error Term ε -> \n* Regression Equation -> \n* Correlation Equation -> \n* Estimated Regression Equation -> \n* Regression Model ->\n* F Statistic -> \n* MSE\n* MSR\n* Dummy Variable -> A variable that takes values of 0 or 1 and is used to consider the effect of qualitative variables in a regression model\n* Correlation vs Causation -> \n* Standard Error ->\n* Confidence Interval Estimate -> The interval estimate of the mean value of y for a given value of x.\n* OLS -> Method of Ordinary Least Squares. To get the best model weights, we use the method of OLS to minimize the SSR. ","metadata":{}},{"cell_type":"markdown","source":"1. Linear Regression Analysis","metadata":{}},{"cell_type":"markdown","source":"Regression tries to search for relationships among variables. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\ny = np.array([5, 20, 14, 32, 22, 38])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LinearRegression()\n\nmodel.fit(X,y)\n\nr2_score = model.score(X,y)\nprint(\"Coefficient of Determination -> \" , r2_score)\n\nprint(\"Intercept -> \", model.intercept_)\nprint(\"Slope -> \", model.coef_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Multiple Regression Analysis","metadata":{}},{"cell_type":"markdown","source":"Multiple or multivariate linear regression is linear regression with two or more independent variables. <br>\nPolynomial Regression is considered a generalized case of linear regression. You assume polynomial dependence between the output and the input.  Which means, the regression equation can now include non-linear terms. ","metadata":{}},{"cell_type":"code","source":"x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]\ny = [4, 5, 20, 14, 32, 22, 38, 43]\n\nX = np.array(x)\ny = np.array(y)\n\n\nmodel = LinearRegression()\n\nmodel.fit(X,y)\n\n\nr2_score = model.score(X,y)\n\nprint(\"Coefficient of Determination -> \", r2_score)\n\nprint(\"Model Intercept => \", model.intercept_)\n\nprint(\"Slope => \", model.coef_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Linear Regression using statsmodels","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = [[0,1], [5,1], [15,2], [25,5], [35,11], [45,15], [55,34], [60,35]]\ny = [4,5,20,14,32,22,38,43]\n\nx, y = np.array(x), np.array(y)\n\n\nx = sm.add_constant(x)\n\nmodel = sm.OLS(y, x)\n\n\nresults = model.fit()\n\nprint(results.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Coefficient of Determination -> \", results.rsquared)\n\nprint(\"Adjusted Coefficient of Determination => \", results.rsquared_adj)\n\nprint(\"Regression coefficients => \", results.params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**(Bonus or Optional) Polynomial Regression with Scikit-Learn** <br>\nIn polynomial regression, you need to transform the inputs to include non-linear terms.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\ny = np.array([15, 11, 2, 8, 25, 32])\n\n\ntransformer = PolynomialFeatures(degree = 2, include_bias = False)\n\nx = transformer.fit_transform(x)\n\nmodel = LinearRegression().fit(x,y)\n\nr2_score = model.score(x,y)\n\nprint(\"Coefficient of Determination => \", r2_score)\n\nprint(\"Intercept -> \", model.intercept_)\nprint(\"Coefficients => \", model.coef_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Neither regression nor correlation analyses can be interpreted as establishing cause-and-effect relationships. <br><br> They can indicate only how or to what extent variables are associated with each other.<br><br> The correlation coefficient measures only the degree of linear association between two variables. Any conclusions about a cause-and-effect relationship must be based on the judgment of the analyst.","metadata":{}},{"cell_type":"markdown","source":"### 3. Chi Square Test","metadata":{}},{"cell_type":"markdown","source":"background -> \n\n* used to analyze the frequencies of two variables with multiple categories to determine their independency\n* qualitative variables\n* nominal data","metadata":{}},{"cell_type":"markdown","source":"degrees of freedom for the chi-squared distribution -> (rows -1) * (cols -1)","metadata":{}},{"cell_type":"markdown","source":"a. Understanding Contigency Tables (also known as crosstab)","metadata":{}},{"cell_type":"markdown","source":"* useful for multiple population proportions\n* classify sample observations according to two or more characterstics\n* also called cross-classification table","metadata":{}},{"cell_type":"markdown","source":"Contigency tables are the pivot tables obtained by utilizing the categorical variable. The contigency here is whether a variable affects the values of the caegorical variable. <br>\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"b. Performing Chi-Square Tests","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"c. Chi-Square Tests for Feature Selection","metadata":{}},{"cell_type":"markdown","source":"**Assumption** -> each cell in the contigency table has expected frequency atleast **5**","metadata":{}},{"cell_type":"markdown","source":"![](https://media.geeksforgeeks.org/wp-content/uploads/Capture-214.png)","metadata":{}},{"cell_type":"markdown","source":"#### Note:- Used only for Categorical Features.","metadata":{}},{"cell_type":"markdown","source":"Dataset used -> https://www.kaggle.com/c/cat-in-the-dat","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import chi2 \n\n\ndata = pd.read_csv('../input/cat-in-the-dat/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(['id'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in data.columns:\n    print(col, data[col].nunique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in data.columns:\n    print(col, '\\n\\n',data[col].value_counts())\n    print('-'*10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"bin_3, bin_4 has T/F values. <br>\nnom_0, nom_1, nom_2, nom_3, nom_4 have 3-6 unique values. <br>\nnom_5, nom_6, nom_7, nom_8, nom_9 have many unique values <br>\nTHen comes the ordinal variables","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['bin_3'] = data['bin_3'].map({\"T\" : 1, \"F\" : 0})\ndata['bin_4'] = data['bin_4'].map({\"Y\" : 1, \"N\" : 0})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We're done with dealing of binary variables. <br>\nNow we're left to deal with the nominals & ordinals.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 5 ordinal variables of which 4 have few unique values and can be dealt in a similar manner. <br>\nord_5 has multiple unique values and needs to be handled separately. ","metadata":{}},{"cell_type":"code","source":"for col in ['ord_1', 'ord_2', 'ord_3', 'ord_4']:\n    print(col, list(np.unique(data[col])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m1_ord1 = {'Novice' : 0, 'Contributor' : 1, 'Expert' : 2, 'Master' : 3, 'Grandmaster' : 4}\n\ndata['ord_1'] = data['ord_1'].map(m1_ord1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m2_ord2 = {'Boiling Hot' : 0, 'Cold' : 1, 'Freezing' : 2, 'Hot' : 3, 'Lava Hot' : 4, 'Warm' : 5}\n\ndata['ord_2'] = data['ord_2'].map(m2_ord2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['ord_3'] = data['ord_3'].apply(lambda x : ord(x) - ord('a'))\ndata['ord_4'] = data['ord_4'].apply(lambda x : ord(x) - ord('A'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['ord_5a'] = data['ord_5'].str[0]\ndata['ord_5b'] = data['ord_5'].str[1]\n\ndata['ord_5a'] = data['ord_5a'].map({val : idx for idx, val in enumerate(np.unique(data['ord_5a']))})\ndata['ord_5b'] = data['ord_5b'].map({val : idx for idx, val in enumerate(np.unique(data['ord_5b']))})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's deal the nominal variables.","metadata":{}},{"cell_type":"code","source":"data[['nom_0', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['nom_1'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['nom_2'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['nom_3'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['nom_4'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['nom_5'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['nom_6'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['nom_7'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(['ord_5', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"data['day'] = data['day'] / 7.0\n\ndata['month'] = data['month'] / 12.0\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's encode the remaining of the nominal values","metadata":{}},{"cell_type":"code","source":"data['nom_1'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m1_nom1 = {'Trapezoid' : 0, 'Square' : 1, 'Star' : 2, 'Circle' : 3, 'Polygon' : 4, 'Triangle' : 5}\n\ndata['nom_1'] = data['nom_1'].map(m1_nom1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['nom_2'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m2_nom2 = {'Lion' : 0, 'Cat' : 1, 'Snake' : 2, 'Dog' : 3, 'Axolotl' : 4, 'Hamster' : 5}\ndata['nom_2'] = data['nom_2'].map(m2_nom2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['nom_3'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m3_nom3 = {'Russia' : 0, 'Canada' : 1, 'China' : 2, 'Finland' : 3, 'Costa Rica' : 4, 'India' : 5}\n\ndata['nom_3'] = data['nom_3'].map(m3_nom3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['nom_4'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m4_nom4 = {'Oboe' : 0, 'Piano' : 1, 'Bassoon' : 2, 'Theremin' : 3}\n\ndata['nom_4'] = data['nom_4'].map(m4_nom4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['nom_0'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m0_nom0 = {'Green' : 0, 'Blue' : 1, 'Red' : 2}\n\ndata['nom_0'] = data['nom_0'].map(m0_nom0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perform One Hot Encoding of the ordinal features","metadata":{}},{"cell_type":"markdown","source":"Label Encoding multiple columns","metadata":{}},{"cell_type":"code","source":"df_copy = data.copy()\ndf_copy.drop(['target'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy = pd.get_dummies(df_copy, columns = df_copy.columns)\ndf_copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X = data.drop(['target'], axis = 1)\nX = df_copy\ny = data.target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# perform feature engineering to encode categorical variables so as to be processed by chi2_feature transform","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chi2_features = SelectKBest(chi2, k = 10)\nX_kbest_features = chi2_features.fit_transform(X,y)\n\nprint(\"Original Number of Features -> (shape)\", X.shape[1])\n\nprint(\"K Best Features (shape)-> \",X_kbest_features.shape[1])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_kbest_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### solving chi2 test generic questions and numericals using python ","metadata":{}},{"cell_type":"markdown","source":"q1. A survey of 500 respondents is depicted in the table below. <br> (In Making)\n\n\n\n|Type| No regular Exercise  | Sporadic Exercise | Regular Exercise | Total  |\n|----|-----|----|----|-----|\n|Dormitory| 32  | 30 | 28 | 90 |\n|On-Campus Apartment| 74 | 64 | 42 | 180 |\n|Off-Campus Apartment| 110  | 25  | 15  | 150  |\n|At home| 39 | 6 | 5 | 50 |\n|Total | 255 | 125 | 90 | 470\n\n\n<br>Based on the above data, is there any relationship between exercise and student's living arrangement? <br>\n\n\nSoln. \n\n\nH0 -> Living arrangement and exercise are independent <br><br>\nH1 -> Are dependent \n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. T-Test","metadata":{}},{"cell_type":"markdown","source":"t-test also known as Student's t-test compares the two averages (means) and tells you if they are different from each other. <br>\nCan also tell you how significant the differences are. ","metadata":{}},{"cell_type":"markdown","source":"**t-score**","metadata":{}},{"cell_type":"markdown","source":"**T-Values vs P-Values**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Types of T-Test <br>\n* Independent Samples t-test\n* Paired Sample t-test\n* One Sample t-test","metadata":{}},{"cell_type":"markdown","source":"### 5. Z-Test","metadata":{}},{"cell_type":"markdown","source":"Helps to determine whether the distribution of the test statistics can be approximated by a normal distribution. <br>\nHelps to determine whether two sample means are approximately the same when their variance is known and the sample size is large enough.","metadata":{}},{"cell_type":"markdown","source":"Q1. The grades on a physics midterm at Covington are roughly symmetric with μ=72, σ=2.0. <br>\nStephanie scored 74 on the exam. <br>\nZ-Score to stephanie's exam score. ","metadata":{}},{"cell_type":"code","source":"z = (74 - 72 )/ 2\n\nprint(z)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q2. Following are the summary statistics of life spans of two breeds of cats. <br>\n\n| Breed  |Mean Life Span (yrs)   | Standard Deviation  |  \n|---|---|---|\n| Abyssinian  | 12  | 1.5  | \n| Colorpoint shorthair  |  14 | 1  |   \n\nFluffy was an Abyssinian cat who lived for 13 years, and Mittens was a colorpoint shorthair cat who lived for 15 years.<br>\nRelative to the breed, which cat had a longer life span? <br><br>\n\n* Fluffy \n* Mittens\n* Fluffy and Mittens had equally long lifespans relative to their breeds.\n* It's impossible to say without seeing all of the individual lifespans.\n* It's impossible to say since we don't know the shape of either distribution.","metadata":{}},{"cell_type":"code","source":"z1 = (13 - 12) / 1.5\n\nz2 = (15 - 14) / 1\n\nprint(\"Z Score for Fluffy -> \", z1)\nprint(\"Z Score for Mittens -> \", z2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mittens had a longer lifespan","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. ANOVA Test","metadata":{}},{"cell_type":"markdown","source":"ANOVA -> Analysis of Variance. <br>\nHelps to compare the means of more than 2 groups. <br>\nANOVA F Test is also called omnibus test. <br><br><br>\n\nMain types of ANOVA Test -> \n* One-way or One-factor \n* Two-way or Two-factor","metadata":{}},{"cell_type":"markdown","source":"ANOVA Hypotheses -> <br>\n* Null Hypotheses = Group means are equal. No variation in the groups. \n* Alternative Hypothesis = At least, one group is different from other groups.","metadata":{}},{"cell_type":"markdown","source":"ANOVA Assumptions -> <br><br>\n* Residuals(experimental error) are normally distributed.(Shapiro-Wilks Test)\n* Homogenity of variances (variances are equal between treatment groups) (Levene's or Bartlett's Test)\n* Observations are sampled independently from each other. ","metadata":{}},{"cell_type":"markdown","source":"ANOVA Working -> <br><br>\n* Check sample sizes, i.e., Equal number of observations in each group. \n* Calculate Mean Square for each group (MS) (SS of group/degrees of freedom-1)\n* Calc Mean Sq. Error (SS Error / df of residuals)\n* Calc F value (MS of group / MSE)","metadata":{}},{"cell_type":"markdown","source":"#### 6.1 One-Way ANOVA Test","metadata":{}},{"cell_type":"code","source":"import random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.seed(2021)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame([random.sample(range(1, 1000), 4) , random.sample(range(1, 1000), 4), random.sample(range(1, 1000), 4), random.sample(range(1, 1000), 4)], columns = ['A', 'B', 'C', \"D\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_melt = pd.melt(df.reset_index(), id_vars = ['index'], value_vars = ['A','B','C','D'])\ndf_melt.columns = ['index', 'treatments', 'value']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_melt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x='treatments', y='value', data=df_melt, color='#99c2a2')\nsns.swarmplot(x=\"treatments\", y=\"value\", data=df_melt, color='#7d0013')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fvalue, pvalue = stats.f_oneway(df['A'], df['B'], df['C'], df['D'])\nprint(\"f Value -> \", fvalue)\nprint(\"p value -> \", pvalue)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n\nmodel = ols('value ~ C(treatments)', data = df_melt).fit()\n\nanova_table = sm.stats.anova_lm(model, typ = 2)\nanova_table","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Interpretation","metadata":{}},{"cell_type":"markdown","source":"p-value obtained from ANOVA Analysis is not significant (p > 0.05), and therefore, we conclude that there are no significant differences amongst the groups. ","metadata":{}},{"cell_type":"markdown","source":"#### 6.2 Two-Way ANOVA Test","metadata":{}},{"cell_type":"markdown","source":"In Two-Way ANOVA Test, we have 2 independent variables and their different levels","metadata":{}},{"cell_type":"code","source":"data = pd.DataFrame(list(zip(['A','A','A','B','B','B', 'C', 'C', 'C', 'D', 'D', 'D'], [np.random.ranf() for _ in range(12)], [np.random.ranf() for _ in range(12)], [np.random.ranf() for _ in range(12)])), columns = ['Genotype', '1_year', '2_year', '3_year'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_melt = pd.melt(data, id_vars = ['Genotype'], value_vars = ['1_year', '2_year', '3_year'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_melt.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_melt.columns = ['Genotype', 'years', 'value']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x = 'Genotype', y = 'value', hue = 'years', data = data_melt, palette = ['r', 'k', 'w'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ols('value ~ C(Genotype) + C(years) + C(Genotype) : C(years)', data = data_melt).fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anova_table = sm.stats.anova_lm(model, typ = 2)\n\nanova_table","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Post-Hoc Analysis (Tukey's Test)","metadata":{}},{"cell_type":"code","source":"!pip install -q bioinfokit\nfrom bioinfokit.analys import stat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = stat()\nres.tukey_hsd(df = df_melt, res_var = 'value', xfac_var = 'treatments', anova_model = 'value ~ C(treatments)')\noutput = res.tukey_summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the values are in accordance to the condition p > 0.05 <br>\nHence, aren't statistically significant.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. F Stats Test","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8. Binomial Distribution","metadata":{}},{"cell_type":"code","source":"from scipy.stats import binom\n\nn = 6\np = 0.6\n\nr_values = list(range(n + 1))\n\nmean, var = binom.stats(n, p)\n\n\ndist = [binom.pmf(r, n, p) for r in r_values]\n\ndf = pd.DataFrame(list(zip(r_values, dist)), columns = ['r', 'p(r)'], index = None)\n\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['p(r)'].plot.bar()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9. Normal Distribution","metadata":{}},{"cell_type":"markdown","source":"also known as \n* Gaussian Distribution\n* Bell Curve\n\n\n<br><br> Below is the probability distribution function (pdf) for Normal Distribution -> ","metadata":{}},{"cell_type":"markdown","source":"![](https://cdn.askpython.com/wp-content/uploads/2020/10/Probability-density-function-of-Normal-Distribution.jpg.webp)","metadata":{}},{"cell_type":"markdown","source":"* x -> input value\n* mu -> mean\n* sigma -> std deviation","metadata":{}},{"cell_type":"markdown","source":"![](https://cdn.askpython.com/wp-content/uploads/2020/10/Standard-deviation-around-mean.jpg.webp)","metadata":{}},{"cell_type":"code","source":"mu, sigma = 0.5, 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = np.random.normal(mu, sigma, 10000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count, bins, ignored = plt.hist(data, 20)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 10. Poisson Distribution","metadata":{}},{"cell_type":"markdown","source":"![](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1539784818/output_39_0_knqrjh.png)","metadata":{}},{"cell_type":"markdown","source":"Note -> Normal Distribution is a limiting case of poisson distribution when lambda -> inf. ","metadata":{}},{"cell_type":"code","source":"from scipy.stats import poisson\ndata_poisson =poisson.rvs(mu = 3, size = 10000, random_state= 2021)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(data_poisson, bins = 30, kde = False, \n            color = 'skyblue',\n             hist_kws = {'linewidth' : 15, 'alpha' : 1}\n            )\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 11. Bernoulli Distribution","metadata":{}},{"cell_type":"markdown","source":"A bernoulli distribution has only 2 possible outcomes, 1 (success) and 0 (failure). <br>\nEg. A coin toss. <br>","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 12. Central Limit Theorem","metadata":{}},{"cell_type":"markdown","source":"**What it states?** <br><br>\nEven when a sample is not normally distributed, if you draw multiple samples and take each of their averages, the averages will represent a normal distribution.<br><br>\nWhich means repeated sampling from a not normally distributed sample and taking the means of those repeated samples will end up being a normally distributed sample. <br><br>\n\n100 samples in total which are not normally distributed. Take random 10 samples say 50 times and take the mean of these samples. It will come out to be a normally distributed sample.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following is an experiment of dice roll for 1000 times. <br>\nfor 1000 times, we make samples of samples size 100 where possible outcomes are 1,2,3,4,5,6 <br><br>\nBy plotting the histogram of the sample means, we obtain a normally distributed plot. <br>\nThis is Central Limit Theorem","metadata":{}},{"cell_type":"code","source":"\nmeans = [np.mean(np.random.randint(1, 7, 100)) for _ in range(1000)]\n\nplt.hist(means)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Key Takeaways :- <br><br>\n\n![](https://miro.medium.com/max/366/1*RdIQG331j0tayi50asTOIw.png)\n\n![](https://miro.medium.com/max/418/1*dCxzo7E6lmKxHLEg2xZSoQ.png)","metadata":{}},{"cell_type":"markdown","source":"You can never experiment with all your customers (population). However, to draw a conclusion for an experiment which is a good representaion of your customers, you need to perform repeated experiments on different set of customers (different samples of the not normally distributed population/sample as per the context) and confirm your hypotheses. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 13. Calculating p-Value","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/963/1*0XXmFcatWBkagH3YeYdpig.png)","metadata":{}},{"cell_type":"markdown","source":"p-value is all about answering the question with certain confidence level. <br>\neg. I am 90% confident that I will get that job. ","metadata":{}},{"cell_type":"markdown","source":"**In p-value tests, our task might be to find the probability that a sample mean could be x, given the hypothesis that the population mean is y.** <br>\n\n<br>\nConclusion => <br>\nThe p-value gives us the probability of observing what we observed, given the hypothesis is true. It doesn't tell us the probability that the null hypothesis is true.","metadata":{}},{"cell_type":"markdown","source":"**How it works?** <br>\nStatement (Null Hypothesis) -> ","metadata":{}},{"cell_type":"code","source":"def pvalue(mu, sigma, samp_size, samp_mean = 0, deltam = 0):\n    \n    np.random.seed(2021)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"COnsidering Means -> Use t-test","metadata":{}},{"cell_type":"markdown","source":"![](https://media.geeksforgeeks.org/wp-content/uploads/20200503190751/Annotation-2020-05-03-190733-300x92.png)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Considering Proportions -> z test","metadata":{}},{"cell_type":"markdown","source":"![](https://media.geeksforgeeks.org/wp-content/uploads/20200503191805/Annotation-2020-05-03-191654-300x92.png)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Solving Questions on Distributions using python","metadata":{}},{"cell_type":"markdown","source":"Q1. Calculate the probability of getting 14 heads in 20 attempts from a fair coin.","metadata":{}},{"cell_type":"code","source":"# It is a problem of binomial distribution. N = 20, p = 0.5, q = 0.5, x = 14\n\n\n# use the probability mass function to evaluate the probability\nprint(stats.binom.pmf(k = 14, n = 20, p = 0.5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q2. A question paper contains 90 multiple choice questions. There are four alternatives answers to each question of which only one is correct. What is the probability to score atleast 22 marks without any preparation (random guessing).","metadata":{}},{"cell_type":"code","source":"## Again a question of binom distribution. N = 90, k = 22, p = 0.25\n\n\n# In this question, we'll be using cumulative distribution function and subtract the outcome from 1. \n# Find the cumulative probability till k = 21 and subtract it from 1 so that outcome is >= 22 marks\n1 - stats.binom.cdf(k = 21, n = 90, p = 0.25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q3. On an average 5% items supplied by a manufacturer are defective. If a batch of 10 items is inspected, what is the probability that 2 items are defective. ","metadata":{}},{"cell_type":"code","source":"### Binomial Distribution problem\n\n## Defective sample is to be considered as success. p = 0.05, N = 10, k = 2\n\nstats.binom.pmf(k = 2, n= 10, p = 0.05)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q4. A car distributor experiences on an average 3 car sales per day. Find the probability that on a randomly selected day they will sell \n1. 5 cars.\n2. 0 Cars\n3. At most 2 cars\n4. exactly 1 car","metadata":{}},{"cell_type":"code","source":"## Since we are dealing with discrete occurences over an interval, this is the case of poisson distribution.\n# x = 5, mu = 3\nstats.poisson.pmf(5, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Case ii. x = 0, mu = 3\n\nstats.poisson.pmf(0,3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Case iii. x <= 2, mu = 3\n\nstats.poisson.cdf(2, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Case iv. x = 1, mu = 3\n\nstats.poisson.pmf(1,3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q5. The weight of football players is normally distributed with mean of 200 pounds and a standard deviation of 25 pounds. Find the probability of a player weighing\n1. more than 241.25 pounds\n2. less than 250 pounds.","metadata":{}},{"cell_type":"code","source":"# part 1\n# x = 241.25\n# loc = 200\n# scale = 25\n\n# P(z > 241.25)\n\n1 - stats.norm.cdf(241.25, 200, 25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# part 2\n\n# x = 250\n# loc = 200\n# scale = 25\n\n# P(z < 250)\n\nstats.norm.cdf(250, 200, 25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q6. Assuming a binomial experiment with p = 0.5 and a sample size of 100. The expected value of this distribution is?","metadata":{}},{"cell_type":"code","source":"# Expected value of a binomial experiment is np\n\n0.5 * 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q7. Probability of acceptance of a student in a college is 0.3 <br>\nIf 5 students apply, probability that at most 2 are selected?","metadata":{}},{"cell_type":"code","source":"stats.binom.cdf(2, 5, 0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q8. Probability of obtaining 45 or fewer heads in 100 tosses of a coin?","metadata":{}},{"cell_type":"code","source":"stats.binom.cdf(45, 100, 0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q9. Suppose a die is tossed 5 times. Probability of getting exactly 2 fours?","metadata":{}},{"cell_type":"code","source":"stats.binom.pmf(2,5, 0.167)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q10. An average light bulb manufactured by the Acme Corporation lasts 300 days with a standard deviation of 50 days. Assuming that bulb life is normally distributed, what is the probability that an Acme light bulb will last at most 365 days?","metadata":{}},{"cell_type":"code","source":"stats.norm.cdf(365, 300, 50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q11. Suppose scores on an IQ test are normally distributed. If the test has a mean of 100 and a standard deviation of 10, what is the probability that a person who takes the test will score between 90 and 110?","metadata":{}},{"cell_type":"code","source":"stats.norm.cdf(110, 100, 10) - stats.norm.cdf(90, 100, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q12. Suppose the average number of lions seen on a 1-day safari is 5. What is the probability that tourists will see fewer than four lions on the next 1-day safari?","metadata":{}},{"cell_type":"code","source":"stats.poisson.cdf(3, 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q13. The average number of homes sold by the Acme Realty company is 2 homes per day. What is the probability that exactly 3 homes will be sold tomorrow?","metadata":{}},{"cell_type":"code","source":"stats.poisson.pmf(2,3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q14. Suppose scores on an IQ test are normally distributed, with a mean of 100. Suppose 20 people are randomly selected and tested. The standard deviation in the sample group is 15. What is the probability that the average test score in the sample group will be at most 110?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q15. Acme Corporation manufactures light bulbs. The CEO claims that an average Acme light bulb lasts 300 days. A researcher randomly selects 15 bulbs for testing. The bulbs last an average of 290 days, with a standard deviation of 50 days. If the CEO's claim were true, what is the probability that 15 randomly selected bulbs would have an average life of no more than 290 days?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q16. Suppose we select 5 cards from an ordinary deck of playing cards. What is the probability of obtaining 2 or fewer hearts?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 14. Measuring Correlation","metadata":{}},{"cell_type":"markdown","source":"The strength of the association between two variables is known as correlation test. <br>\nIf we want to know the relation between height and weight of human beings, a dataset of the same is to be obtain and correlation is to be found to justify or reject the above hypothesis. ","metadata":{}},{"cell_type":"markdown","source":"* r takes values -1 to +1 \n* r = 0 means no correlation\n* can't be applied to ordinal variables\n* the sample size should be moderate 20 to 30. \n* outliers can lead to misleading calculations","metadata":{}},{"cell_type":"markdown","source":"![](https://media.geeksforgeeks.org/wp-content/uploads/20200311233526/formula6.png)","metadata":{}},{"cell_type":"code","source":"random.seed(2021)\nlst1 = random.sample(range(100), 50)\nprint(\"Elements of 1st list -> \", lst1, \"\\n\")\n\n\nlst2 = random.sample(range(100), 50)\nprint(\"Elements of 2nd list -> \", lst2, \"\\n\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr, _ = stats.pearsonr(lst1, lst2)\n\nprint(\"Pearsons correlation : \", corr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inference -> A value close to 0 means there is no correlation between the values of the elements. We can say there is slight but insignificant correlation between the values","metadata":{}},{"cell_type":"markdown","source":"### 15. Measuring Variance","metadata":{}},{"cell_type":"markdown","source":"Variance -> Measures how far from their mean the individual observations in dataset are. <br>\nStd Deviation -> Square root of variance is std deviation which measures the amount of dispersion of the dataset.","metadata":{}},{"cell_type":"code","source":"def variance(data):\n    \n    n = len(data)\n    \n    mean = sum(data)/n\n    \n    deviations = [(x - mean) ** 2 for x in data]\n    \n    variance = sum(deviations) / n\n    \n    return variance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.seed(2021)\ndata = random.sample(range(1000), 10)\n\nvariance(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Variance estimate of the population using sample data","metadata":{}},{"cell_type":"code","source":"def variance(data , dof = 0):\n    \n    n = len(data)\n    mean = sum(data) / n\n    \n    return sum((x - mean)** 2 for x in data) / (n - dof)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variance(data, dof = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Standard deviation is the square root of the variance value calculated. <br><br><br>\nValues that are within one standard deviation of the mean can be thought of as fairly typical. <br>\nThose values which are three or more standard deviations away from the mean can be considered as **outliers**. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 16. Sample Statistics","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 17. Population Statistics","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 18. Maximum Likehood Estimation","metadata":{}},{"cell_type":"markdown","source":"MLE is a method to find the most likely density function that would have generated the data. <br>\nThe likelihood function depends on mean 'mu' and variance σ2 which is found through an iterative process using calculators or computers.<br>\n","metadata":{}},{"cell_type":"markdown","source":"|  S.No | Likelihood   | Probability  |\n|---|---|---|\n| 1  | Refers to past events with known outcomes  | Refers to the occurence of future outcomes  |\n| 2  | eg. A coin is flipped 10 times and 10 heads occur. Likelihood the coin is an unbiased coin?  | eg. A coin flipped n times. Probability of getting heads.| \n| 3  | Sum of Likelihoods != 1  | Sum of Probabilities = 1  |  ","metadata":{}},{"cell_type":"markdown","source":"**Steps to perform MLE:** <br>\n* Perform a certain experiment to collect data\n* Choose parametric model of the data\n* Formulate the likelihood as an objective function to be maximized \n* Maximize the objective func and derive the parameters of the model\n\n\n**Examples** -> <br>\n* Coin Toss to find the probabilities of heads and tails\n* Dart throwing","metadata":{}},{"cell_type":"markdown","source":"Note :- For linear regression models, we use Ordinary Least Squares (OLS) to fit the regression model and estimate the parameters B0 & B1. <br>\n**MLE** is based on the data we observe, what are the model parameters that maximize the likelihood of the observed data occuring?","metadata":{}},{"cell_type":"markdown","source":"**Applications ->** <br>\nThe parameters of a logistic regression model can be estimated by the probabilistic framework called Maximum Likelihood Estimation.","metadata":{}},{"cell_type":"markdown","source":"The outputs of a logistic regression are class probabilities.","metadata":{}},{"cell_type":"markdown","source":"**Brief Overview** -> <br>\n![](https://miro.medium.com/max/421/1*ayxQCn3xz6sm41KRjf3Ygw.gif)\n\n![](https://miro.medium.com/max/721/1*6MTXtB4zipiDMguZrlXSlA.gif)\n\nIn statistics, MLE is widely used to obtain the parameter for a distribution. In this paradigm, ","metadata":{}},{"cell_type":"markdown","source":"In this paradigm, to maximize log likelihood, we need to minimize the cost function. <br>\n![](https://miro.medium.com/max/774/1*VAb-6NSg2vwUtqCtfNdjrA.gif)\n\nGradient Descent algorithm is used to tweak the values of the cost function using MLE. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 19. Cluster Analysis","metadata":{}},{"cell_type":"markdown","source":"Cluster Analysis is an unsupervised technique of grouping objects together based on their properties. By doing so, the objects in one group are more similar to each other than to those in other groups.","metadata":{}},{"cell_type":"markdown","source":"Cluster Analysis vs Discriminant Analysis","metadata":{}},{"cell_type":"markdown","source":"Cluster Analysis assigns objects to various groups without any prior object labels whereas Discriminant Analysis uses such knowledge which was defined in advance.","metadata":{}},{"cell_type":"markdown","source":"**Distance Functions**","metadata":{}},{"cell_type":"markdown","source":"Minkowski Distance <br><br>\nA generalization of both the Euclidean and the manhattan metric is the Minowski distance given by :- <br>\n\n\n![](https://slideplayer.com/slide/5070455/16/images/16/Minkowski+Distance+Minkowski+distance%3A+a+generalization.jpg)\n\n![](https://www.researchgate.net/publication/349155159/figure/fig1/AS:989596292767746@1612949550717/Three-typical-Minkowski-distances-ie-Euclidean-Manhattan-and-Chebyshev-distances.png)","metadata":{}},{"cell_type":"markdown","source":"Understanding various distance functions -> <br>\nBoth euclidean and manhattan distance satisfy the following -> \n* d(i,j) >= 0\n* d(i,j) = d(j,i)\n* d(i,j) >= d(i,h) + d(h,j)\n* d(i,i) != 0","metadata":{}},{"cell_type":"markdown","source":"#### 19. i. K Means Clustering","metadata":{}},{"cell_type":"markdown","source":"![](https://files.realpython.com/media/kmeans-algorithm.a94498a7ecd2.png)","metadata":{}},{"cell_type":"markdown","source":"**Ways to choose optimal number of clusters** -> <br>\n1. Elbow Method\n2. Silhouette coefficient","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = make_blobs(n_samples = 300, centers = 4, cluster_std = 0.60, random_state = 0)\n\n\nplt.scatter(X[:,0], X[:,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Elbow Method","metadata":{}},{"cell_type":"code","source":"wcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    \n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n    \n    \nplt.plot(range(1,11), wcss)\nplt.title('Elbow method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, n_init=10, random_state=0)\npred_y = kmeans.fit_predict(X)\nplt.scatter(X[:,0], X[:,1])\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Silhouette Analysis Method","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import silhouette_samples, silhouette_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_clusters = [2,3,4,5,6,7,8,9]\n\n\nfor n_cluster in n_clusters:\n    \n    clusterer = KMeans(n_clusters = n_cluster, random_state = 10)\n    cluster_labels = clusterer.fit_predict(X)\n    \n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"\\nFor n_clusters = \", n_cluster, \"\\nThe average silhouette_score is : \", silhouette_avg)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, for n = 4, we obtain the highest silhouette score. <br>\nThis is exactly the same number we obtained using elbow method as well as it is the number of clusters we have defined in our dataset.","metadata":{}},{"cell_type":"markdown","source":"#### 19. ii. K-Mediods","metadata":{}},{"cell_type":"markdown","source":"**K-Means** clustering algorithm is sensitive to **outliers** as mean value is easily influenced by **extreme values**. <br>\n**K-Mediods** is a variant of K-Means which is more robust to noises and outliers.","metadata":{}},{"cell_type":"code","source":"!pip install -q https://github.com/scikit-learn-contrib/scikit-learn-extra/archive/master.zip\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn_extra.cluster import KMedoids\n\nkmediods = KMedoids(n_clusters = 3, random_state = 0).fit(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmediods.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 19. iii. Agglomerative Clustering","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering \nimport scipy.cluster.hierarchy as shc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/ccdata/CC GENERAL.csv')\n\nX = df.drop(['CUST_ID'], axis = 1)\n\nX.fillna(method = 'ffill', inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, normalize\n\nscaler = StandardScaler()\n\nX_scaled = scaler.fit_transform(X)\n\n\nX_normalized = normalize(X_scaled)\n\n\nX_normalized = pd.DataFrame(X_normalized)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_normalized","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA \n\n\npca = PCA(n_components = 2)\n\n\nx_pca = pca.fit_transform(X_normalized)\n\nx_pca = pd.DataFrame(x_pca)\nx_pca.columns = ['P1', 'P2']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize =(8, 8))\nplt.title('Visualising the data')\nDendrogram = shc.dendrogram((shc.linkage(x_pca, method ='ward')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use silhouette scores to find the optimal number of clusters.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 20. Hypothesis Testing","metadata":{}},{"cell_type":"markdown","source":"It is a statistical method used for making statistical decisions using experimental data. <br>\nUsed to evaluate two mutually exclusive statements in a population to determine which statement is best supported using the sample data.","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/481/1*2vTwIrqdELKJY-tpheO7GA.jpeg)\n\nStandardized Normal Curve <br>\nWhich means it is a normal distribution curve which is standardized (mean = 0, std = 1).","metadata":{}},{"cell_type":"markdown","source":"Normal Curve -> \n![](https://miro.medium.com/max/626/1*gBnxoTRwo9sDovvegHfm6g.png)","metadata":{}},{"cell_type":"markdown","source":"**Important Terminologies ->** <br><br><br>\n**Level of Significance** => the degree of significance in which we accept or reject the nuol hypothesis. A 5% significance means the the result should be atleast 95% confident to give a similar result in each sample. ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 21. Type-I Error & Type-II Error","metadata":{}},{"cell_type":"markdown","source":"Type I Error","metadata":{}},{"cell_type":"markdown","source":"When you reject the null hypothesis but thay hypothesis was true. Type I error is denoted by alpha. The region that shows the critical region , is called the alpha region. <br>","metadata":{}},{"cell_type":"markdown","source":"Type II Error","metadata":{}},{"cell_type":"markdown","source":"When we accept null hypothesis but it is false. Denoted by Beta. The normal curve that shows the acceptance region is called the beta region. ","metadata":{}},{"cell_type":"markdown","source":"One tailed Test","metadata":{}},{"cell_type":"markdown","source":"A statistical test in which the region of **rejection** is only on **one** side of the sampling distribution. ","metadata":{}},{"cell_type":"markdown","source":"Two Tailed Test","metadata":{}},{"cell_type":"markdown","source":"A **two-tailed** test is a statistical **test** in which the critical area of distribution is **two-sided** and tests are adopted as whether the samples aare greater than or less than some critical values. If the sample being tested falls in either of the critical values, the alternate hypothesis is accepted instead of the null hypothesis. ","metadata":{}},{"cell_type":"markdown","source":"P-Value","metadata":{}},{"cell_type":"markdown","source":"The **p-Value**, or calculated probability, of finding extreme results when the null hypothesis is true. <br>\nIf your p value is less than the chosen significance level, you reject the null hypothesis. ","metadata":{}},{"cell_type":"markdown","source":"**Degree of Freedom** -> <br>\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Widely Used hypothesis testing types ->** <br>\n* T Test (Student T Test)\n* Z Test\n* ANOVA Test\n* Chi-Square Test","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 22. Z-Stats & T-Stats","metadata":{}},{"cell_type":"markdown","source":"T-Test <br>\nType of inferential statistics used to determine if there is a significant difference between means of two groups which may be related in certain features. <br>\nUsed when the datasets would follow a normal distribution and may have unknown variances. ","metadata":{}},{"cell_type":"markdown","source":"We have two types of T-Tests -> <br>\n* 1 sample t-test\n* 2 sample t-test","metadata":{}},{"cell_type":"markdown","source":"One Sample t-test -> <br>\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2 Samples t-test -> <br>\n        Compares the means of two independent groups in order to determine whether there is statistical evidence that the associated population means are statistically different. <br>\n        Also known as **Independent t-test**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Paired sampled t-test** -> <br>\nAlso known as dependent sample t-test. <br>\nUnivariate test that tests for a significant difference between 2 related variables. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Z-Test <br>\nUse Z-test if -> <br>\n* Sample Size > 30\n* Data points **independent** from each other. \n* Data should be normally distributed. Sometimes, if the sample size is large enough, this doesn't matter. \n* Data items have equal chance of getting selected. ","metadata":{}},{"cell_type":"markdown","source":"One-Sample Z-Test","metadata":{}},{"cell_type":"code","source":"from statsmodels.stats import weightstats as stests\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Two Sample Z-Test","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ANOVA test or F-Test","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One-Way F-Test (Anova) -> <br>\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Two-Way F-Test (Anova) -> ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Chi-Sq Test","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"KS Test","metadata":{}},{"cell_type":"markdown","source":"KS Test is used to check if given values follow a distribution. <br>\n","metadata":{}},{"cell_type":"code","source":"np.random.seed(2021)\nv = np.random.normal(size = 100)\n\nres = stats.kstest(v, 'norm')\n\nprint(res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 23. Confidence Interval","metadata":{}},{"cell_type":"markdown","source":"It is a range of values. <br>\n95% confidence interval is the most common. <br>\nNote -> 95% confidence interval doesn't mean 95% probability. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculation of confidence interval** <br>\n","metadata":{}},{"cell_type":"code","source":"def mean_confidence_interval(data, confidence = 0.95):\n    a = 1.0 * np.array(data)\n    n = len(a)\n    \n    m, se = np.mean(a), stats.sem(a)\n    h = se * stats.t.ppf((1 + confidence) / 2., n - 1)\n    \n    return m, m - h, m + h","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I. Confidence interval for a sample","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"II. Confidence Interval with a small sample","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"III. Confidence Interval with the Normal Distribution / Z-Distribution","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"IV. Confidence Interval for a proportion","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"V. Confidence Interval for 2 populations or (proportions)","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In a local teaching district a technology grant is available to teachers in order to install a cluster of four computers in their classrooms. From 6250 teachers in the district, 250 were randly selected and asked if they felt that computers were an essential teaching tool for their classroom. Of those selected, 142 teachers felt that computers were an essential teaching tool. <br>\n1. Calculate a 99% confidence interval for the propertion of teachers who felt that computers are an essential teaching tool. <br>\n2. How could the survey be changed to narrow the confidence interval but to maintain the 99% confidence interval?","metadata":{}},{"cell_type":"markdown","source":"### 24. Confusion Matrix, ROC & Regression Analysis","metadata":{}},{"cell_type":"markdown","source":"**Confusion Matrix** <br><br>\n\nWe'll learn confusion matrix for both Binary Classifiers as well as Multi Class Classifiers. ","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/proxy/0*-oGC3SE8sPCPdmxs.jpg)","metadata":{}},{"cell_type":"markdown","source":"I. Binary Classifiers ","metadata":{}},{"cell_type":"code","source":"# Let's assume the following was a confusion matrix obtained for a Binary Dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = [0,1,0,1,0,1]\ny_pred = [0,0,1,1,0,1]\n\nconfusion_matrix(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the help of confusion matrix, we can obtain values for TN, FP, FN, TP","metadata":{}},{"cell_type":"code","source":"tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\nprint(\"True Negatives -> \", tn)\nprint(\"False Positives -> \", fp)\nprint(\"False Negatives -> \", fn)\nprint(\"True Positives -> \", tp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define a function to output all the metrics needed to gauge model performance. <br>","metadata":{}},{"cell_type":"code","source":"def calculate_performance(tn, fp, fn, tp):\n    \n    \n    accuracy = (tp + tn)/ (tp + tn + fp + fn)\n    \n    precision = tp / (tp + fp)\n    \n    recall = tp / (tp + fn)\n    \n    f1 = ( 2 * precision * recall ) / (precision + recall)\n    \n    \n    return accuracy, precision, recall, f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc, precision, recall, f1 = calculate_performance(tn, tp, fn, tp)\n\nprint(\"Accuracy of the hypothetical model -> \", acc)\nprint(\"Precision of the hypothetical model -> \", precision)\nprint(\"Recall of the hypothetical model -> \", recall)\nprint(\"F1-score of the hypothetical model -> \", f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can also display the output of your confusion matrix in a better visual format","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_true, y_pred),\n                               display_labels=[0,1])\n\ndisp.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"II. Multi-Class Classifiers","metadata":{}},{"cell_type":"code","source":"# For multi-class classifiers, let's understand how to obtain the metrics given the hypothetical confusion matrix.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = [0,1,2, 0,1,2,0,1,2]\ny_pred = [0,2,1,0,1,2,1,0,1]\n\nconfusion_matrix(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_true, y_pred),\n                               display_labels=[0,1,2])\n\ndisp.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How to read a multi-class confusion matrix?\n\n![](https://miro.medium.com/max/875/1*uQDpo9iISx00ucl3gftLVA.png)","metadata":{}},{"cell_type":"markdown","source":"Now we need to calculate TP, FN, FP, TN values. ","metadata":{}},{"cell_type":"code","source":"def calculate_metrics_multi(cnf_matrix):\n    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n    TP = np.diag(cnf_matrix)\n    TN = cnf_matrix.sum() - (FP + FN + TP)\n    \n    FP = FP.astype(float)\n    FN = FN.astype(float)\n    TP = TP.astype(float)\n    TN = TN.astype(float)\n    \n    # Sensitivity, hit rate, recall, or true positive rate\n    TPR = TP/(TP+FN)\n    # Specificity or true negative rate\n    TNR = TN/(TN+FP) \n    # Precision or positive predictive value\n    PPV = TP/(TP+FP)\n    # Negative predictive value\n    NPV = TN/(TN+FN)\n    # Fall out or false positive rate\n    FPR = FP/(FP+TN)\n    # False negative rate\n    FNR = FN/(TP+FN)\n    # False discovery rate\n    FDR = FP/(TP+FP)\n    # Overall accuracy for each class\n    ACC = (TP+TN)/(TP+FP+FN+TN)\n    \n    f1 = 2 * precision * recall / (precision + recall)\n    \n    print(\"The calculated metrics are as follows -> \\n\\n\")\n    print(f\"\\n1. Accuracy = {ACC} \\n2. Recall or Sensitivity = {TPR} \\n3. Specificity or True Negative Rate = {TNR} \\n4. Precision = {PPV} \\n5. Negative Predictive Value = {NPV} \\n6. Fall out or False Positive Rate = {FPR} \\n7. False Negative Rate = {FNR} \\n8. False Discovery Rate = {FDR} \\n9. F1-Score = {f1}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_metrics_multi(confusion_matrix(y_true, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Receiver Operating Characteristic** <br>\n","metadata":{}},{"cell_type":"markdown","source":"**What is it?** <br>\nROC is a plot useful for predicting the probability of a binary outcome. <br>\nIt is the plot of the false positive rate (x-axis) versus the true positive rate (y-axis) for a number of different candidate threshold values between 0.0 and 1.0 <br>","metadata":{}},{"cell_type":"markdown","source":"![](https://developers.google.com/machine-learning/crash-course/images/ROCCurve.svg)","metadata":{}},{"cell_type":"markdown","source":"**Area Under the ROC Curve** -> <br>\nAUC provides aggregate measure of performance across all possible classification thresholds. ","metadata":{}},{"cell_type":"markdown","source":"**AUC is desirable for 2 reasons** -> <br>\n* Scale invariant \n* classification-model-threshold invariant","metadata":{}},{"cell_type":"markdown","source":"**Q. How would multiplying all of the predictions from a given model by 2.0 (for example, if the model predicts 0.4, we multiply by 2.0 to get a prediction of 0.8) change the model's performance as measured by AUC?** <br><br>\nAns. No change. AUC only cares about relative prediction scores.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Thresholding in ROC**","metadata":{}},{"cell_type":"markdown","source":"The aim of the plot is to analyze the predictive power of the predictor ","metadata":{}},{"cell_type":"markdown","source":"While selecting threshold, you should visualize the following graph for threshold selection and\n\n![](https://miro.medium.com/max/644/1*P2qKi7w1UHF7zg6SnCGTag.png)","metadata":{}},{"cell_type":"markdown","source":"analyze the performance based on the following ROC curve generated.","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/601/1*QkWHqoSHSBig31InTzr8TA.jpeg)","metadata":{}},{"cell_type":"markdown","source":"* If you aim for very low FPR, you might pick up a ","metadata":{}},{"cell_type":"markdown","source":"**Poor Classifier** <br><br>\n![](https://miro.medium.com/max/764/1*HVvNkWufhzGj2s0sc4CyIw.png)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Understanding the influence of threshold on the ROC Curve -> <br>\nThere are 2 possible cases for ROC Curve's threshold movement -> <br>\n* Shifting the threshold to the right\n* Shifting the threshold to the left","metadata":{}},{"cell_type":"markdown","source":"In such a scenario, never remember the standard TPR vs FPR diagram for ROC Curve. Rather Refer to the following diagram -> <br>\n![](https://lukeoakdenrayner.files.wordpress.com/2018/01/threshold2.png?w=656)\n\n**Case I : Shifting the threshold to the right**<br>\nThis would case TP to decrease, FP to decrease, FN to increase and TN to increase. <br>\nBy doing so, TPR or Sensitivity (which is TP / Sum(+ves)) decreases. FPR (which is (FP / Sum(-ves) ) decreases. If FPR decreases, specificity increases as FPR = 1 - specificity.","metadata":{}},{"cell_type":"markdown","source":"**Case I : Shifting the threshold to the left**<br>\nThis would case TP to increase, FP to increase, FN to decrease and TN to decrease. <br>\nBy doing so, TPR or Sensitivity (which is TP / Sum(+ves)) increases. FPR (which is (FP / Sum(-ves) ) increases. If FPR increases, specificity decreases as FPR = 1 - specificity.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Concluding Remarks about ROC & AUC Curves -> <br>\n* can be used as a summary of the model skill\n* ROC Curves of different models can be compared directly in general or for different thresholds.\n* Shape of ROC curves contains info about the predictive power of the model.\n* for imbalanced class distribution, ROC curves are very helpful. Helps to visualize the trade-off between TPR and FPR and thus help us to arrive at a threshold that minimizes the mis-classification cost.","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/603/1*D05sMUrwZIgvwsQVF_CJdg.jpeg)","metadata":{}},{"cell_type":"markdown","source":"**Regression Analysis**","metadata":{}},{"cell_type":"markdown","source":"Output of a regression analysis is usually a summary statistic that includes: <br>\n* R \n* R squared\n* adjusted R-squared\n* standard error of the estimate","metadata":{}},{"cell_type":"markdown","source":"**Use regression analysis to ->** <br><br>\n* Model multiple independent variables\n* use polynomial terms to model the curvature. \n* include continuous and categorical variables\n* assess interaction terms to determine affect of one independent variable on the value of another variable.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 25. Summary Statistics","metadata":{}},{"cell_type":"markdown","source":"#### I. Central Tendency Statistics","metadata":{}},{"cell_type":"markdown","source":"a. Arithmetic Mean","metadata":{}},{"cell_type":"markdown","source":"Given a set of numbers -> [n1,n2,n3,n4,n5]<br><br>\n\nAverage or **Arithmetic Mean** -> Sum of dataset / num of data items <br>\n-> (n1 + n2 + n3 + n4 + n5) / 5 \n","metadata":{}},{"cell_type":"code","source":"data1 = np.arange(1, 10, 2)\ndata1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mean of the above data set is => \", np.mean(data1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"b. Weighted Mean","metadata":{}},{"cell_type":"markdown","source":"For calculating weighted mean, <br>\nyou would require two lists. <br>\n* The data items list\n* The corresponding weights list","metadata":{}},{"cell_type":"code","source":"data1 = np.arange(1, 21, 2)\ndata1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = np.random.random(len(data1))\nweights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighted_mean(data_lst, weights_lst):\n    return np.average(data_lst, weights = weights_lst)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Weighted Mean is -> \", weighted_mean(data1, weights))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"c. Median","metadata":{}},{"cell_type":"markdown","source":"Median for **odd** number of elements is the middle most element. <br>\nMedian for **even** number of elements is the average of the middle two elements. <br>","metadata":{}},{"cell_type":"markdown","source":"case 1 : even number of elements","metadata":{}},{"cell_type":"code","source":"data1 = np.arange(1, 21, 2)\ndata1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.median(data1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"case 2: odd number of elements","metadata":{}},{"cell_type":"code","source":"data1 = np.arange(1, 10, 2)\ndata1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Median of the above dataset is -> \", np.median(data1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"d. Percentile","metadata":{}},{"cell_type":"markdown","source":"Percentile is calculated by assuming the highest or maximum value in a dataset as the upper limit and relative to that value other values are calculated indicating how far or near they are.","metadata":{}},{"cell_type":"markdown","source":"We'll be generating the following percentile values for the dataset -> <br>\n* 50th Percentile / Median\n* 25th Percentile\n* 75th Percentile","metadata":{}},{"cell_type":"markdown","source":"case 1. 1D dataset","metadata":{}},{"cell_type":"code","source":"data1 = random.sample(range(1000), 20)\ndata1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('25th Percentile -> ', np.percentile(data1, 25))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('50th Percentile -> ', np.percentile(data1, 50))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('75th Percentile -> ', np.percentile(data1, 75))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"case 2. 2D dataset","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.seed(2021)\n\ndata = [random.sample(range(100), 4), \n       random.sample(range(100), 4),\n       random.sample(range(100), 4)]\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"25th percentile value for axis = None -> \", np.percentile(data, 25,))\nprint(\"25th percentile value for axis = 0 -> \", np.percentile(data, 25,axis = 0))\nprint(\"25th percentile value for axis = 1 -> \", np.percentile(data, 25,axis = 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"50th percentile value for axis = None -> \", np.percentile(data, 50,))\nprint(\"50th percentile value for axis = 0 -> \", np.percentile(data, 50,axis = 0))\nprint(\"50th percentile value for axis = 1 -> \", np.percentile(data, 50,axis = 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"75th percentile value for axis = None -> \", np.percentile(data, 75,))\nprint(\"75th percentile value for axis = 0 -> \", np.percentile(data, 75,axis = 0))\nprint(\"75th percentile value for axis = 1 -> \", np.percentile(data, 75,axis = 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### II. Dispersion","metadata":{}},{"cell_type":"markdown","source":"a. Skewness","metadata":{}},{"cell_type":"markdown","source":"* skewness = 0, normally distributed\n* skewness > 0, more weight in the left tail of the distribution\n* skewness < 0, more weight in the right tail of the distribution","metadata":{}},{"cell_type":"markdown","source":"![](https://media.geeksforgeeks.org/wp-content/uploads/skewness.jpg)","metadata":{}},{"cell_type":"code","source":"from scipy.stats import skew\nimport pylab\nx1 = np.linspace(-10, 10, 1000)\ny1 = 1./ (np.sqrt(2. * np.pi)) * np.exp(-.5 * (x1) ** 2)\n\npylab.plot(x1, y1)\n\nprint(\"Skewness of the data --> \", skew(y1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1 = np.linspace(-5, 10, 1000)\ny1 = 1./ (np.sqrt(2. * np.pi)) * np.exp(-.5 * (x1) ** 2)\n\npylab.plot(x1, y1)\n\nprint(\"Skewness of the data -> \", skew(y1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.random.normal(0, 2, 10000)\n\n\nprint(\"X: \\n\", x)\n\nprint(\"\\nSkewness for data : \", skew(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"b. Kurtosis","metadata":{}},{"cell_type":"markdown","source":"It helps in the measure of how heavy the tail is in compared to a normal distribution.","metadata":{}},{"cell_type":"markdown","source":"![](https://media.geeksforgeeks.org/wp-content/uploads/kurtosis.jpg)","metadata":{}},{"cell_type":"code","source":"x = np.linspace(-10, 10, 1000)\ny1 = 1./(np.sqrt(2.*np.pi)) * np.exp( -.5*(x1)**2  )\n\n\npylab.plot(x,y1, '*')\n\nprint(\"Kurtosis for normal distribution : \", stats.kurtosis(y1))\n\nprint(\"Kurtosis for normal distribution : \", stats.kurtosis(y1, fisher = False))\n\nprint(\"Kurtosis for normal distribution : \", stats.kurtosis(y1, fisher = True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"c. Range","metadata":{}},{"cell_type":"markdown","source":"Range is the span of values in the entire dataset. <br>\nRange is denoted by [min_value, max_value]","metadata":{}},{"cell_type":"code","source":"random.seed(2021)\ndata1 = random.sample(range(-1000, 1000), 100)\n#data1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_value = min(data1)\n\nmax_value = max(data1)\n\nprint(f\"Range of the dataset -> [{min_value},{max_value}]\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"d. Interquartile Range","metadata":{}},{"cell_type":"markdown","source":"Interquartile range also called as midspread, or middle 50% or technically H-spread. <br>\nTechnically, it is the Q3 - Q1 where Q3 is the third quartile and the first quartile. <br>\nIt covers the center of the distribution and contains 50% of the observations. <br><br>\n\n**IQR** = **Q3 - Q1**","metadata":{}},{"cell_type":"markdown","source":"##### Applications -> <br>\n* helps in easy identification of outlier values\n* gives the central tendency of the data.\n* higher the IQR, higher the variablity.\n* lower the IQR, the preferable the dataset is.","metadata":{}},{"cell_type":"code","source":"\n\n\nrandom.seed(2021)\n\ndata = random.sample(range(1000), 30)\n\nprint(f\"Dataset -> {data}\\n\\n\")\nIQR = stats.iqr(data, interpolation = 'midpoint')\n\nprint(IQR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"e. Variance","metadata":{}},{"cell_type":"markdown","source":"Variance is the square of the difference of a variable from its mean. <br>\nIt measures the spread of random data in a set from its mean or median value. <br>\n* Low value for variance indicates the data are clustered together.\n* High value for variance indicates the data are spread widely.","metadata":{}},{"cell_type":"markdown","source":"![](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-7b0fdc0b3c4d7ef2aeeba85f690456c2_l3.svg)","metadata":{}},{"cell_type":"code","source":"print(f\"Variance of the sample set --> {statistics.variance(data)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"f. Standard Score or Z-Score","metadata":{}},{"cell_type":"markdown","source":"**z-score** tells us how many standard deviations away a value is from the mean.\n\nz = (X – μ) / σ\n","metadata":{}},{"cell_type":"code","source":"import scipy.stats as stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stats.zscore(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here each z-score tells us how many std. deviations each value is away from mean.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"g. Coefficient of Variation","metadata":{}},{"cell_type":"markdown","source":"It is the ratio of standard deviation to mean. ","metadata":{}},{"cell_type":"code","source":"np.random.seed(2021)\ndata1 = np.random.randn(5,5)\n\nprint(\"\\nVariation at axis = 0: \\n\", stats.variation(data1, axis = 0))\nprint(\"\\nVariation at axis = 1: \\n\", stats.variation(data1, axis = 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 26. Familiarizing with different error metrics","metadata":{}},{"cell_type":"markdown","source":"SST ","metadata":{}},{"cell_type":"markdown","source":"Sum of Squares Total - the squared difference between the observed dependent variable and its mean.<br>\nSST is also denoted as TSS or total sum of squares.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SSR","metadata":{}},{"cell_type":"markdown","source":"Sum of differences between the predicted value and the mean of the dependent variable.","metadata":{}},{"cell_type":"markdown","source":"if SSR = SST, our regression model captures all the observed variablity and its perfect. <br>\nESS -> Explained sum of squares.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"SSE","metadata":{}},{"cell_type":"markdown","source":"The error is the difference between the observed value and the predicted value. <br>\nWe want to minimize the error. The smaller the error, the better the estimation power of the regression. <br>\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Relation between SST, SSR, SSE","metadata":{}},{"cell_type":"markdown","source":"SST = SSR + SSE <br>\nThe total variability = Explained Variability + Unexplained Variability","metadata":{}},{"cell_type":"markdown","source":"**Mean Squared Error** -> <br>\n1/n * SSE <br><br>\n**Root Mean Squared Error** -> <br>\nsqrt(MSE) <br><br>\n**R Squared** -> <br>\n1 - SSE / SST <br><br>\n**Adjusted R-squared** -> <br>\n1 - (n + k / n - k) * (1 - R**2) ","metadata":{}},{"cell_type":"markdown","source":"**MSR ->** <br><br>\n\nMean Square due to regression, <br>\nSSR / ","metadata":{}},{"cell_type":"markdown","source":"### 27. Simmilarity and Dissimilarity Index","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 28. Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"**Assumptions of Logistic Regression ->** <br>\n* Binary Logistic Regression requires the dependent variable to be binary\n* The variables should be independent of each other. That is, the model should have little or no multicollinearity. \n* Samples sizes should be preferably large. \n* ","metadata":{}},{"cell_type":"markdown","source":"#### I. Python Implementation","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/logistic-regression-stats-dataset/logit_train1.csv\", index_col = 0)\n\nXtrain = df[['gmat', 'gpa', 'work_experience']]\nytrain = df[['admitted']]\n\n\nlog_reg = sm.Logit(ytrain, Xtrain).fit()\n\n\nprint(log_reg.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### II. Calculating G-Statistic","metadata":{}},{"cell_type":"markdown","source":"G - statistic in logistic regression is -> <br>\n-2ln[ likelihood without variable/ likelihood with variable]","metadata":{}},{"cell_type":"markdown","source":"Applications of G-Statistic -> <br>\n* Helps to verify the overall significance of the model\n* ","metadata":{}},{"cell_type":"markdown","source":"Wald Test -> <br>\nMeasures an individual independent variable's significance","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Notebook in Making.  <br>\nEst. Date of Completion - 28-03-2021","metadata":{}}]}