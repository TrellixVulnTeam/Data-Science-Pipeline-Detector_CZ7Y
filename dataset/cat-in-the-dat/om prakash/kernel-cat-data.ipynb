{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\ntrain=pd.read_csv(\"/kaggle/input/cat-in-the-dat/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/cat-in-the-dat/test.csv\")\nsample=pd.read_csv(\"/kaggle/input/cat-in-the-dat/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for  unique values binary columns in each columns\nprint(\"Binary Unique Values . . . . .\")\ndef bin_val(dataframe):\n    l=[]\n    for i in (dataframe.iloc[:,1:6]).columns:\n        \n        l.append([i,pd.Series(dataframe[i].unique())])\n    return pd.Series(l)\nprint(bin_val(train))\nprint('\\n')\n\n#Cheking for nominal  values\nprint(\"Nominal Unique Values . . . .\")\n\ndef nom_val(dataframe):\n    l=[]\n    for i in (dataframe.iloc[:,6:16]).columns:\n        l.append([i,pd.Series(dataframe[i].unique()).count()])\n    return (pd.Series(l))\n\nprint(nom_val(train))\nprint('\\n')\n\n#Checking for ordinal values\n\nprint(\"Ordinal Unique Values . . . .\")\ndef ord_val(dataframe):\n    l=[]\n    for i in (dataframe.iloc[:,16:22]).columns:\n        l.append([i,pd.Series(dataframe[i].unique()).count()])\n    return pd.Series(l)\nprint(ord_val(train))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for type for columns\nprint('To Know Data Type Of Columns . . . . ')\ntyp=[]\nfor i in (train.iloc[:,1:24]).columns:\n    typ.append(type(train[i][0]))\ntyp=pd.DataFrame(typ)\ntyp[0].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we know binary values don't have relationships each other we us dummies from pandas \ntrain=pd.get_dummies(data=train,columns=['bin_0','bin_1','bin_2','bin_3','bin_4'])\ntest=pd.get_dummies(data=test,columns=['bin_0','bin_1','bin_2','bin_3','bin_4'])\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nominal=train.iloc[:,1:6]\nfig = plt.figure(figsize=(25,6))\nfig.subplots_adjust(hspace=0.4,wspace=0.4)\nfor i in range(pd.Series(nominal.columns).count()):\n    ax = fig.add_subplot(1,6,i+1)\n    sns.countplot(y=nominal.iloc[:,i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(10,7))\nfig.subplots_adjust(hspace=1,wspace=1)\nnominal_2=train.iloc[:,6:11]\nfor i in range(pd.Series((nominal_2).columns).count()):\n    ax=fig.add_subplot(3,2,i+1)\n    z=pd.DataFrame(nominal_2.iloc[:,i].value_counts()).head(5)\n    z.reset_index(inplace=True)\n    sns.barplot(y=z['index'],x=z.iloc[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For Nominal we are going to apply for label encoder\n\nfrom sklearn.preprocessing import LabelEncoder\nencoder =LabelEncoder()\ndef nom_val_lab_encoder(dataframe):\n    for i in (dataframe.iloc[:,1:11]).columns:\n        e=encoder.fit(dataframe[i])\n        dataframe[i]=pd.Series(e.transform(dataframe[i]))\n    return dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=nom_val_lab_encoder(train)\ntest=nom_val_lab_encoder(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.iloc[:,11:17]\ntrain=pd.get_dummies(data=train,columns=['ord_0','ord_1','ord_2','ord_3','ord_4','ord_5'])\ntest=pd.get_dummies(data=test,columns=['ord_0','ord_1','ord_2','ord_3','ord_4','ord_5'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['target'].copy()\nx_train = train.drop('target', axis=1)\ndel train\n\nx_test = test.copy()\ndel test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nimport lightgbm as  lgb\nfrom sklearn.metrics import roc_auc_score\nfrom bayes_opt import BayesianOptimization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_mod(n_leaves, min_data_in_leaf, max_depth, bagging, feature, l1, l2):\n    \n    params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'boosting_type': 'gbdt',\n        'is_unbalance': False,\n        'boost_from_average': True,\n        'num_threads': 4,\n        \n        'n_leaves': int(n_leaves),\n        'min_data_in_leaf': int(min_data_in_leaf),\n        'max_depth': int(max_depth),\n        'bagging' : bagging,\n        'feature' : feature,\n        'l1': l1,\n        'l2': l2\n    }\n    \n    scores = []\n    \n    cv = StratifiedKFold(n_splits=10)\n    for train_idx, valid_idx in cv.split(x_train, y_train):\n        \n        x_train_train = x_train.iloc[train_idx]\n        y_train_train = y_train.iloc[train_idx]\n        x_train_valid = x_train.iloc[valid_idx]\n        y_train_valid = y_train.iloc[valid_idx]\n        \n        lgb_train = lgb.Dataset(data=x_train_train.astype('float32'), label=y_train_train.astype('float32'))\n        lgb_valid = lgb.Dataset(data=x_train_valid.astype('float32'), label=y_train_valid.astype('float32'))\n        lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=100)\n        y = lgb_model.predict(x_train_valid.astype('float32'), num_iteration=lgb_model.best_iteration)\n        score = roc_auc_score(y_train_valid.astype('float32'), y)\n        print('Fold score:', score)\n        scores.append(score)\n    average_score = sum(scores) / len(scores)\n    print('Average score:', average_score)\n    return average_score\n\n\nbounds = {\n    'n_leaves': (31, 100),\n    'min_data_in_leaf': (20, 100),\n    'max_depth':(-1, 100),\n    'bagging' : (0.1, 0.9),\n    'feature' : (0.1, 0.9),\n    'l1': (0, 2),\n    'l2': (0, 2)\n}\n\nbo = BayesianOptimization(train_mod, bounds, random_state=42)\nbo.maximize(init_points=20, n_iter=20, acq='ucb', xi=0.0, alpha=1e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nparams = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'is_unbalance': False,\n    'boost_from_average': True,\n    'num_threads': 4,\n    \n    'bagging_fraction': 0.12033530139527615,\n    'feature_fraction': 0.18631314159464357,\n    'lambda_l1': 0.0628583713734685,\n    'lambda_l2': 1.2728208225275608,\n    'max_depth': int(30.749954088708993),\n    'min_data_in_leaf': int(60.685655293176225),\n    'num_leaves': int(93.62208670090041),\n    \n    'num_iterations': 10000,\n    'learning_rate': 0.006,\n    'early_stopping_round': 100\n}\nn_splits = 10\n\ny = np.zeros(x_test.shape[0])\n\nbest_score = 0\nbest_y = []\n\nfeature_importances = []\n\ncv = StratifiedKFold(n_splits=n_splits)\nfor train_idx, valid_idx in cv.split(x_train, y_train):\n    \n    x_train_train = x_train.iloc[train_idx]\n    y_train_train = y_train.iloc[train_idx]\n    x_train_valid = x_train.iloc[valid_idx]\n    y_train_valid = y_train.iloc[valid_idx]\n    \n    lgb_train = lgb.Dataset(data=x_train_train.astype('float32'), label=y_train_train.astype('float32'))\n    lgb_valid = lgb.Dataset(data=x_train_valid.astype('float32'), label=y_train_valid.astype('float32'))\n    \n    lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=100)\n    \n    y_part = lgb_model.predict(x_test.astype('float32'), num_iteration=lgb_model.best_iteration)\n    y += y_part / n_splits\n    \n    temp = lgb_model.predict(x_train_valid.astype('float32'), num_iteration=lgb_model.best_iteration)\n    score = roc_auc_score(y_train_valid.astype('float32'), temp)\n    print('Fold score:', score)\n    if score > best_score:\n        best_score = score\n        best_y = y_part\n        print('Best Y updated. Score =', score)\n    \n    feature_importances.append(lgb_model.feature_importance())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample['target']=y\nsample.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}