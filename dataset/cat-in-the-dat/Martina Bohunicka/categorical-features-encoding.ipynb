{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Is there a cat in your dat?**"},{"metadata":{},"cell_type":"markdown","source":"### Overview\n\nA common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured.\n\nBecause this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:\n\n- binary features\n- low- and high-cardinality nominal features\n- low- and high-cardinality ordinal features\n- (potentially) cyclical features <br>\n\nThis Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. \n\n![cat](https://i.kinja-img.com/gawker-media/image/upload/s--rqCW9nxC--/c_scale,f_auto,fl_progressive,q_80,w_800/p4b69sblvgebowkdhnfy.jpg)"},{"metadata":{},"cell_type":"markdown","source":"### Table of Content"},{"metadata":{},"cell_type":"markdown","source":"- [Importing Libraries](#imports)\n- [Exploring the Data](#explore_data)\n   - [Binary features](#binary_features)\n   - [Nominal features](#nominal_features)\n   - [Ordinal features](#ordinal_features)\n   - [Cyclical features](#cyclical_features)\n- [Categorical Feature Encoding](#cat)  \n   - [Binary features encoding](#bin_cat)\n   - [Nominal features encoding](#nom_cat)\n   - [Ordinal features encoding](#ord_cat)\n   - [Cyclical features encoding](#cyc_cat)"},{"metadata":{},"cell_type":"markdown","source":"### Importing Libraries <a class=\"anchor\" id=\"imports\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing numpy (linear algebra) and pandas (data processing): \nimport numpy as np \nimport pandas as pd \n\n# Imports for plotting:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\nimport os\nimport matplotlib.ticker as ticker","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring the Data <a class=\"anchor\" id=\"explore_data\"></a>"},{"metadata":{},"cell_type":"markdown","source":"In this competition, we will be predicting the probability [0, 1] of a binary target column.\n\nThe data contains binary features (bin_), nominal features (nom_), ordinal features (ord_) as well as (potentially cyclical) day (of the week) and month features. The string ordinal features ord_{3-5} are lexically ordered according to string.ascii_letters.\n\nSince the purpose of this competition is to explore various encoding strategies, the data has been simplified in that (1) there are no missing values, and (2) the test set does not contain any \"unseen\" feature values. (Of course, in real-world settings both of these factors are often important to consider!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore what's in the cat-in-the-dat folder:\nprint(os.listdir(\"../input/cat-in-the-dat\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read train, test and sample_submission data:\ntrain_df = pd.read_csv(\"../input/cat-in-the-dat/train.csv\")\ntest_df = pd.read_csv(\"../input/cat-in-the-dat/test.csv\")\nsubmission = pd.read_csv(\"../input/cat-in-the-dat/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of the train and testdataset:\nprint(train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To display first 5 rows of the train_df:\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Names of all columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the names of all columns in train DataFrame:\nprint(train_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking for missing data (nan)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Are there any missing values in train_df?\n# train_df.apply(axis=0, func=lambda x : any(pd.isnull(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to describe variables\ndef desc(df):\n    summ = pd.DataFrame(df.dtypes,columns=['Data_Types'])\n    summ = summ.reset_index()\n    summ['Columns'] = summ['index']\n    summ = summ[['Columns','Data_Types']]\n    summ['Missing'] = df.isnull().sum().values    \n    summ['Uniques'] = df.nunique().values\n    return summ\n\n# Function to analyse missing values\ndef nulls_report(df):\n    nulls = df.isnull().sum()\n    nulls = nulls[df.isnull().sum()>0].sort_values(ascending=False)\n    nulls_report = pd.concat([nulls, nulls / df.shape[0]], axis=1, keys=['Missing_Values','Missing_Ratio'])\n    return nulls_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use desc function to describe test data:\ndesc(train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Target distribution**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bar chart of frequency of digit occurance in our train dataset:\ntotal = float(len(train_df))\n\nplt.figure(figsize=(16,4))\nax = sns.countplot(x = 'target', data=train_df,  palette = 'rocket_r')\n\n# Make twin axis\nax2=ax.twinx()\nax2.set_ylabel('Frequency [%]')\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}'.format(height*100/total),\n           # '{0:.0%}'.format(height/total),\n            ha=\"center\") \n\n\n# Use a LinearLocator to ensure the correct number of ticks\nax.yaxis.set_major_locator(ticker.LinearLocator(11))\n\n# Fix the Frequency [%] range to 0-100\nax2.set_ylim(0,100)\nax.set_ylim(0,300000)\n\n# And use a MultipleLocator to ensure a tick spacing of 10\nax.yaxis.set_major_locator(ticker.MultipleLocator(25000))\nax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\n\n# Turn the grid on ax2 off, otherwise the gridlines will cut through percentages %:\nax.grid(False)\nax2.grid(False)   \n    \nplt.title('Target Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df['target'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In our train_df we have 300,000 rows of data with 208,236 (69.41%) rows with the target of 0 and 91,764 (30.59%) rows with the target of 1. "},{"metadata":{},"cell_type":"markdown","source":"####Â **Binary (bin_) features** <a class=\"anchor\" id=\"binary_features\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define bin list:\nbin = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bar charts for binary features, split according to the target:\nfor i in bin:\n    plt.figure(figsize=(16,4))\n    ax = sns.countplot(x=i, \n                       hue=\"target\", \n                       palette= 'ocean_r',\n                       data=train_df\n                       )\n    \n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}'.format(height*100/total),\n                #'{0:.0%}'.format(height/total),\n                ha=\"center\") \n       \n        ax.set_ylim(0,200000)\n        ax.grid(False)\n\n        plt.title('Target Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns bin_3 and bin_4 contain T,F and Y,N respectively, isntead of numerical values 0,1."},{"metadata":{},"cell_type":"markdown","source":"#### **Nominal (nom_) features** <a class=\"anchor\" id=\"nominal_features\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define nom as:\nnom = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bar charts for nominal features, split according to the target:\nfor i in nom[0:5]:\n    plt.figure(figsize=(16,4))\n    ax = sns.countplot(x=i, \n                       hue=\"target\", \n                       palette= 'gist_heat_r',\n                       data=train_df\n                       )\n    \n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height*100/total),\n                #'{0:.0%}'.format(height/total),\n                ha=\"center\") \n       \n        ax.set_ylim(0,100000)\n        ax.grid(False)\n\n        plt.title('Target Distribution')\nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is interesting: there are some similarities in Target Distributions for nom1, nom2 and nom3. To be more at target distribution rounded to the nearest integer and compare the following: \n- Trapezoid, Lion, Russia (24%,10%)\n- Square, Cat, Canada (11%, 6%)\n- Star, Snake, China (11%, 5%)\n- Circle, Dog, Finaland (9%, 3%)\n- Polygon, Axolotl, Costa Rica (8%, 4%)\n- Triangle, Hamster, India (6%,4%)\n\nLet's have a look at the value tables for nom_1, nom_2 and nom_3, just to confirm that Target Distribution is very similar for all three features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a crosstab with nom_1 and target:\nprint('Crosstab for numerical target distribution in nom_1:')\n\npd.crosstab([train_df.target], \n            [train_df.nom_1],\n             margins=True).style.background_gradient(cmap='autumn_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a crosstab with nom_2 and target:\nprint('Crosstab for numerical target distribution in nom_2:')\n\npd.crosstab([train_df.target], \n            [train_df.nom_2],\n             margins=True).style.background_gradient(cmap='autumn_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a crosstab with nom_3 and target:\nprint('Crosstab for numerical target distribution in nom_3:')\n\npd.crosstab([train_df.target], \n            [train_df.nom_3],\n             margins=True).style.background_gradient(cmap='autumn_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We still have columns from nom_5 to nom_9, those hold from 222 to 11,981 categories respectively. Let's have a look at how many categories each of the columns hold:"},{"metadata":{},"cell_type":"markdown","source":"#### **Ordinal (ord_) features** <a class=\"anchor\" id=\"ordinal_features\"></a>"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"ord = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bar charts for ordinal features, split according to the target:\n\nfor i in ord[0:3]:\n    plt.figure(figsize=(16,4))\n    ax = sns.countplot(x=i, \n                       hue=\"target\", \n                       palette= 'winter_r',\n                       data=train_df\n                       )\n    \n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.1f}%'.format(height*100/total),\n                #'{0:.0%}'.format(height/total),\n                ha=\"center\") \n       \n        ax.set_ylim(0,150000)\n        ax.grid(False)\n\n        plt.title('Target Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ord[3:5]:\n    plt.figure(figsize=(16,4))\n    ax = sns.countplot(x=i, \n                       hue=\"target\", \n                       palette= 'winter_r',\n                       data=train_df\n                       )\n    \n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                #'{:1.1f}%'.format(height*100/total),\n                '{0:.0%}'.format(height/total),\n                ha=\"center\") \n       \n        ax.set_ylim(0,35000)\n        ax.grid(False)\n\n        plt.title('Target Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of unique values in ord_5:\nprint('Number of unique values for ord_5: ' + str(train_df['ord_5'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For ord_5 we have 192 unique values, all of them consist of 2 alphabet letters."},{"metadata":{},"cell_type":"markdown","source":"#### **Cyclical features** <a class=\"anchor\" id=\"cyclical_features\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Hours of the day, days of the week, months in a year are all examples of features that are cyclical. In our DataFrame we have days and months, let's have a look at unique values for those features."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Unique values of day:',train_df.day.unique())\nprint('Unique values of month:',train_df.month.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we could expect, we have 1-7 values for day and 1-12 values for month feature. "},{"metadata":{"trusted":true},"cell_type":"code","source":"cyc = ['day', 'month']\n\n\nfor i in cyc:\n    plt.figure(figsize=(16,4))\n    ax = sns.countplot(x=i, \n                       hue=\"target\", \n                       palette= 'cool_r',\n                       data=train_df\n                       )\n    \n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                #'{:1.1f}%'.format(height*100/total),\n                '{0:.0%}'.format(height/total),\n                ha=\"center\") \n       \n        ax.set_ylim(0,60000)\n        ax.grid(False)\n\n        plt.title('Target Distribution')\nplt.show()      ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting! We don't have much data for June and for Saturdays. "},{"metadata":{},"cell_type":"markdown","source":"## Categorical Features Encoding <a class=\"anchor\" id=\"cat\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Before we start working on feature encoding, we will combine train_df and test_df into one DataFrame called tetra_df and separate target column. This will allow us to make changes to both DataFrames at the same time."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign output target to the following variable:\ntarget = train_df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge train and test data into tetra_df and drop target and id column:\ntetra_df = train_df.append(test_df, ignore_index = True, sort = 'True')\ntetra_df = tetra_df.drop(['target', 'id'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if merge worked (must have 500,000 entries):\ntetra_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create indexes to separate data later:\ntrain_df_idx = len(train_df)\ntest_df_idx = len(tetra_df) - len(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binary features encoding"},{"metadata":{},"cell_type":"markdown","source":"Since bin_3 and bin_4 contain only two values, we can convert them to a binary columns. Let's assume that: <br>\n => T = True and F = False, <br>\n => Y = Yes and N = No <br>\nWe can just simply replace T by1 in bin_3, F by 0 and Y by 1, N by 0 in bin_4.  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert T, F in bin_3 to binary values (0,1):\ntetra_df['bin_3'] = tetra_df['bin_3'].map({'T':1, 'F':0})\n\n# Similarly convert Y, N in bin_4 to binary values:\ntetra_df['bin_4'] = tetra_df['bin_4'].map({'Y':1, 'N':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the outcome:\ntetra_df[bin].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Nominal features encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# One hot encoding for column : nom_0 to nom_4\ntetra_df = pd.get_dummies(tetra_df, columns = nom[0:5],\n                        prefix = nom[0:5], \n                        drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding hex features\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nfeatures_hex = nom[5:]\n\nfor col in features_hex:\n    labelencoder.fit(tetra_df[col])\n    tetra_df[col] = labelencoder.transform(tetra_df[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ordinal features encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"#tetra_df[ord].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert ord_1 by dictionary mapping as follows:\ntetra_df['ord_1'] = tetra_df['ord_1'].map({\n    'Novice': 0,\n    'Contributor': 1,\n    'Master': 2,\n    'Expert' : 3,\n    'Grandmaster': 4\n})\n\n# Similarly convert ord_2:\ntetra_df['ord_2'] = tetra_df['ord_2'].map({\n    'Freezing': 0,\n    'Cold': 1,\n    'Warm': 2,\n    'Hot' : 3,\n    'Boiling Hot': 4,\n    'Lava Hot' : 5\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change type of ord_3 to category, create a dictionary alph that orders letters alphabetically:\ntetra_df['ord_3'] = tetra_df['ord_3'].astype('category')\nalph = dict(zip(tetra_df['ord_3'],tetra_df['ord_3'].cat.codes))\n# Map alphord to ord_3 and change type of ord_3 to integer:\ntetra_df['ord_3'] = tetra_df['ord_3'].map(alph)\ntetra_df['ord_3'] = tetra_df['ord_3'].astype(int)\n\n# Similarly change ord_4:\ntetra_df['ord_4'] = tetra_df['ord_4'].astype('category')\nalph1 = dict(zip(tetra_df['ord_4'],tetra_df['ord_4'].cat.codes))\ntetra_df['ord_4'] = tetra_df['ord_4'].map(alph1)\ntetra_df['ord_4'] = tetra_df['ord_4'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create sorted list of ord_5 values (ordered alphabetically):\nordli = sorted(list(set(tetra_df['ord_5'].values)))\n\n# Create mapping dictionary alph2 for ord_5\nalph2 = dict(zip(ordli, range(len(ordli))))  \n\n# Map alph2 dictionary to ord_5\ntetra_df['ord_5'] = tetra_df['ord_5'].map(alph2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cyclical features encoding"},{"metadata":{},"cell_type":"markdown","source":"One of the methods for cyclical features encoding is to perform sine and cosine transformation of the feature by using the following formulas:\n\n$$x_{sin} = sin(\\frac{2*\\pi*x}{max(x)})$$\n\n$$x_{cos} = cos(\\frac{2*\\pi*x}{max(x)})$$\n\nSince both trigonometric functions are periodical, it's not a good idea to use only one of them for encoding. The reason is simple: two different features can be encoded as the same value. <br>\nBy using sin and cos function we will avoid this and assign an unique position on a [unit circle](http://mathworld.wolfram.com/UnitCircle.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cyclical encoding for day:\ntetra_df['day_sin'] = np.sin(2 * np.pi * tetra_df['day']/7.0)\ntetra_df['day_cos'] = np.cos(2 * np.pi * tetra_df['day']/7.0)\n\n# Cyclical encoding for month:\ntetra_df['month_sin'] = np.sin(2 * np.pi * tetra_df['month']/12.0)\ntetra_df['month_cos'] = np.cos(2 * np.pi * tetra_df['month']/12.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both sin and cos values will be in the range between -1 and 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show that Encoded values are now placed on the circle with radius 1 and origing at [0,0]:\nx = tetra_df.day_sin\ny = tetra_df.day_cos\n\ntetra_df.sample(5000).plot.scatter('day_sin','day_cos').set_aspect('equal')\ntetra_df.sample(5000).plot.scatter('month_sin','month_cos').set_aspect('equal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tetra_df = tetra_df.drop(['day', 'month'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the names of all columns in tetra_df DataFrame:\n print(tetra_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalize data columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.preprocessing import MinMaxScaler\n#min_max_scaler = MinMaxScaler()\n\n# x returns a numpy array\n#x = tetra_df.values \n\n\n#x_scaled = min_max_scaler.fit_transform(x)\n#tetra_df = pd.DataFrame(x_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tetra_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating training and testing data:\ntraining = tetra_df[ : train_df_idx]\ntesting = tetra_df[test_df_idx :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For splitting data we will be using train_test_split from sklearn:\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = training\ny = target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the training data into test and train, we are testing on 0.20 = 20% of dataset:\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=13)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**USE XGBoost CLASSIFIER**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_validate, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(objective= 'binary:logistic'\n                    , learning_rate=0.7\n                    , max_depth=3\n                    , n_estimators=250\n                    , scale_pos_weight=2\n                    , random_state=42\n                    , colsample_bytree=0.5\n                    )\n    \nxgb.fit(X_train, y_train)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = xgb.predict(X_test)\nprint(classification_report(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix cm:\ncm = confusion_matrix(y_test,y_predict)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quick overview of our confusion matrix:\nsns.heatmap(cm, annot = True, square = True, fmt='g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = xgb.predict(testing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine ImageID and Label into one DataFrame:\nfinal_result = pd.DataFrame({'target': prediction, 'id': submission.id})\nfinal_result = final_result[['id', 'target']]\n\n# Downloading final_result dataset as digit_output.csv:\nfinal_result.to_csv('cat_output.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}