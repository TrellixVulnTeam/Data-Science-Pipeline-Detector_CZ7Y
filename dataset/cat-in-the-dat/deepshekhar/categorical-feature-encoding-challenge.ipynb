{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/cat-in-the-dat/train.csv')\ntest = pd.read_csv('../input/cat-in-the-dat/test.csv')\n\nprint(train.shape)\nprint(test.shape)\n\ntrain['ord_5_a'] = train['ord_5'].str[0]\ntrain['ord_5_b'] = train['ord_5'].str[1]\ntest['ord_5_a'] = test['ord_5'].str[0]\ntest['ord_5_b'] = test['ord_5'].str[1]\n\ntrain.drop(['ord_5'],inplace=True,axis=1)\ntest.drop(['ord_5'],inplace=True,axis=1)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing categorical options of pandas\nfrom pandas.api.types import CategoricalDtype \n# seting the orders of our ordinal features\nord_3 = CategoricalDtype(categories=['a', 'b', 'c', 'd', 'e', 'f', 'g',\n                                     'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'], ordered=True)\nord_4 = CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n                                     'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n                                     'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], ordered=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming ordinal Features\ntrain.ord_3 = train.ord_3.astype(ord_3).cat.codes\ntrain.ord_4 = train.ord_4.astype(ord_4).cat.codes\n\n# test dataset\ntest.ord_3 = test.ord_3.astype(ord_3).cat.codes\ntest.ord_4 = test.ord_4.astype(ord_4).cat.codes\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def date_cyc_enc(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_vals)\n    return df\n\ntrain = date_cyc_enc(train, 'day', 7)\ntest = date_cyc_enc(test, 'day', 7) \n\ntrain = date_cyc_enc(train, 'month', 12)\ntest = date_cyc_enc(test, 'month', 12)\n\n#train.drop(['month','day'],inplace=True,axis=1)\n#test.drop(['month','day'],inplace=True,axis=1)\n# NOTE, I discovered it on: kaggle.com/gogo827jz/catboost-baseline-with-feature-importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Subset\ntarget = train['target']\ntrain_id = train['id']\ntest_id = test['id']\ntrain.drop(['target', 'id','bin_0'], axis=1, inplace=True)\ntest.drop(['id','bin_0'], axis=1, inplace=True)\n\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_cols = ['bin_3','bin_4','nom_0', 'nom_1', 'nom_2','ord_0', 'ord_1','ord_2',\n       'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_5_a', 'ord_5_b','day','month']\ntraintest = pd.concat([train, test])\n\ncols_to_scale = ['day_sin','day_cos','month_sin','month_cos','ord_3','ord_4'] \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntraintest[cols_to_scale] = scaler.fit_transform(traintest[cols_to_scale])\n\ndummies = pd.get_dummies(traintest, columns=traintest.columns, drop_first=True, sparse=True)\ntrain_ohe = dummies.iloc[:train.shape[0], :]\ntest_ohe = dummies.iloc[train.shape[0]:, :]\n\nprint(train_ohe.shape)\nprint(test_ohe.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_ohe = train_ohe.sparse.to_coo().tocsr()\ntest_ohe = test_ohe.sparse.to_coo().tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''print(train_df_1['ord_1'].unique())\nnew_dict = {'Grandmaster':5 ,'Expert':4 ,'Novice':1 ,'Contributor':2 ,'Master':3} \ntrain_df_1['ord_1'] = train_df_1['ord_1'].apply(lambda x: new_dict[x])\ntest_df_1['ord_1'] = test_df_1['ord_1'].apply(lambda x: new_dict[x])\ntrain_df_1['ord_1'].head()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''train_df_1['ord_3'] = train_df_1[['ord_3', 'ord_4']].apply(lambda x: ''.join(x), axis=1)\ntrain_df_1.drop(['ord_4'], axis=1, inplace=True)\ntrain_df_1.loc[:,'ord_1':'target'].head()\n\ntest_df_1['ord_3'] = test_df_1[['ord_3', 'ord_4']].apply(lambda x: ''.join(x), axis=1)\ntest_df_1.drop(['ord_4'], axis=1, inplace=True)\n\nval_ord3 = train_df_1['ord_3'].value_counts()\nval_ord5 = train_df_1['ord_5'].value_counts()\ntrain_df_1['ord_3'] = train_df_1.loc[:,'ord_3'].apply(lambda x: val_ord3[x])\ntrain_df_1['ord_5'] = train_df_1.loc[:,'ord_5'].apply(lambda x: val_ord5[x])\nprint(train_df_1.loc[:,'ord_1':].head())\n\nval_ord3 = test_df_1['ord_3'].value_counts()\nval_ord5 = test_df_1['ord_5'].value_counts()\ntest_df_1['ord_3'] = test_df_1.loc[:,'ord_3'].apply(lambda x: val_ord3[x])\ntest_df_1['ord_5'] = test_df_1.loc[:,'ord_5'].apply(lambda x: val_ord5[x])'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score as auc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\n\n# Model\ndef run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n    kf = KFold(n_splits=5)#,shuffle=True,random_state=0)\n    fold_splits = kf.split(train, target)\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0]))\n    i = 1\n    for dev_index, val_index in fold_splits:\n        print('Started ' + label + ' fold ' + str(i) + '/5')\n        dev_X, val_X = train[dev_index], train[val_index]\n        dev_y, val_y = target[dev_index], target[val_index]\n        params2 = params.copy()\n        pred_val_y, pred_test_y = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        if eval_fn is not None:\n            cv_score = eval_fn(val_y, pred_val_y)\n            cv_scores.append(cv_score)\n            print(label + ' cv score {}: {}'.format(i, cv_score))\n        i += 1\n    print('{} cv scores : {}'.format(label, cv_scores))\n    print('{} cv mean score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv std score : {}'.format(label, np.std(cv_scores)))\n    #print('\\n\\n\\n',pred_full_test,'\\n\\n')\n    pred_full_test = pred_full_test / 5.0\n    results = {'label': label,\n              'train': pred_train, 'test': pred_full_test,\n              'cv': cv_scores}\n    return results\n\n\ndef runLR(train_X, train_y, test_X, test_y, test_X2, params):\n    print('Train LR')\n    model = LogisticRegression(**params)\n    model.fit(train_X, train_y)\n    print('Predict 1/2')\n    pred_test_y = model.predict_proba(test_X)[:, 1]\n    print('Predict 2/2')\n    pred_test_y2 = model.predict_proba(test_X2)[:, 1]\n    return pred_test_y, pred_test_y2\n\ndef runSGD(train_X, train_y, test_X, test_y, test_X2, params):\n    print('Train LR')\n    model = SGDClassifier(**params)\n    model.fit(train_X, train_y)\n    print('Predict 1/2')\n    pred_test_y = model.predict_proba(test_X)[:, 1]\n    print('Predict 2/2')\n    pred_test_y2 = model.predict_proba(test_X2)[:, 1]\n    return pred_test_y, pred_test_y2\n\nlr_params = {'solver': 'lbfgs', 'C': 0.096, 'max_iter':500, 'class_weight':'balanced'}\nsgd_params = {'loss': 'log', 'penalty': 'l2', 'max_iter':1000}\nresults = run_cv_model(train_ohe, test_ohe, target, runLR, lr_params, auc, 'lr')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id': test_id, 'target': results['test']})\nsubmission.to_csv('submission_11.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500,verbose=True,activation='tanh',solver='sgd',alpha=0.003,learning_rate='adaptive')\nmlp.fit(xTrain,yTrain)\n\nfrom sklearn.metrics import roc_auc_score\nprediction = mlp.predict(test_df_1[:])\nroc_auc_score(yTest, predict_yTest)\nprint(prediction.sum())'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}