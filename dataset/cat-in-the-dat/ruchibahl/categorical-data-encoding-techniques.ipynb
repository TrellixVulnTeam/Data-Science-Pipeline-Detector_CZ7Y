{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Problem Statement:-**\n\nA common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured.\n\nWe have to handle different types of categorical data columns using multiple techniques in order to get best results.\n\n![Lets Categorize](https://media3.giphy.com/media/WYEWpk4lRPDq0/giphy.gif)\nLets begin.\n\nTypes of categorical data given to us \n- binary features\n- low- and high-cardinality nominal features\n- low- and high-cardinality ordinal features\n- (potentially) cyclical features"},{"metadata":{},"cell_type":"markdown","source":"# Importing libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install logitboost","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom logitboost import LogitBoost\nimport seaborn as sns\nfrom category_encoders import TargetEncoder, HashingEncoder, LeaveOneOutEncoder\nfrom sklearn.naive_bayes import ComplementNB, GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nimport string\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets load the datasets first"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/cat-in-the-dat/train.csv')\ntest = pd.read_csv('../input/cat-in-the-dat/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see how the train dataset looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysing Categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get list of categorical variables\ns = (train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we observed somethings\n\n1. There are no null values in train dataset\n2. There are multiple categorical variables which are as follows\n    1. bin_3, bin_4 :-  binary cols\n    2. nom_0 -  nom_4 :-  nominal columns ( with no order)\n    3. nom_5 - nom_9 :- nominal columns with high cardinality\n    4. ord_1 - ord_5 :-  Ordered columns\n    \nWe have to use different ways to treat these columns and convert them into numerical data"},{"metadata":{},"cell_type":"markdown","source":"# Encoding techniques\n\nTaken reference from https://www.kaggle.com/discdiver/category-encoders-examples\n\n1. bin_3, bin_4 :- Convert Y/N and T/F to 1/0\n2. nom_0 -  nom_4 :-  Encode using One hot encoding\n3. nom_5 - nom_9 :- Target encode them as they are high cardinal variables\n4. ord_1, ord_2 :- Convert into numerical order using hard coded values as Label encoder might not be able to understand the order\n5. ord_3 - ord_4 :-  Encode using ascii as they are alphabetical values\n6. ord_5 :- Separate two alphabets and then do label encoding\n7. day, month:- Encode using sin and cosine values as they are cyclic in nature\n"},{"metadata":{},"cell_type":"markdown","source":"# Encoding data"},{"metadata":{},"cell_type":"markdown","source":"Lets save target variable somewhere"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets identify the uncommon columns between test and train data. Replace uncommon columns with a common value"},{"metadata":{"trusted":true},"cell_type":"code","source":"replace_xor = lambda x: 'xor' if x in xor_values else x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(set(train['ord_4'].unique()))\nprint(set(test['ord_4'].unique()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_test = ['ord_5', 'ord_4', 'ord_3']\nfor column in columns_to_test:\n    xor_values = set(train[column].unique()) ^ set(test[column].unique())\n    if xor_values:\n        print('Column', column, 'has', len(xor_values), 'XOR values')\n        train[column] = train[column].apply(replace_xor)\n        test[column] = test[column].apply(replace_xor)\n    else:\n        print('Column', column, 'has no XOR values')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train[\"ord_5a\"]=train[\"ord_5\"].str[0]\n#train[\"ord_5b\"]=train[\"ord_5\"].str[1]\n#train.drop(['ord_5'], axis=1, inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test[\"ord_5a\"]=test[\"ord_5\"].str[0]\n#test[\"ord_5b\"]=test[\"ord_5\"].str[1]\n#test.drop(['ord_5'], axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def date_cyc_enc(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_vals)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"map_to_ascii_index = lambda x: string.ascii_letters.index(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replace_xor = lambda x: 'xor' if x in xor_values else x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before starting with encoding lets create some other features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['merge_col1'] =  train[['nom_0', 'nom_1']].apply(lambda x: ''.join(x), axis=1)\ntest['merge_col1'] =  test[['nom_0', 'nom_1']].apply(lambda x: ''.join(x), axis=1)\n\ntrain['merge_col2'] =  train[['nom_1', 'nom_2']].apply(lambda x: ''.join(x), axis=1)\ntest['merge_col2'] =  test[['nom_1', 'nom_3']].apply(lambda x: ''.join(x), axis=1)\n\ntrain['merge_col3'] =  train[['nom_2', 'nom_3']].apply(lambda x: ''.join(x), axis=1)\ntest['merge_col3'] =  test[['nom_2', 'nom_3']].apply(lambda x: ''.join(x), axis=1)\n\ntrain['merge_col4'] =  train[['nom_3', 'nom_4']].apply(lambda x: ''.join(x), axis=1)\ntest['merge_col4'] =  test[['nom_3', 'nom_4']].apply(lambda x: ''.join(x), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Binary encoding\ntrain['bin_3'] = [0 if x == 'F' else 1 for x in train['bin_3']]\ntrain['bin_4'] = [0 if x == 'N' else 1 for x in train['bin_4']]\n\n#Hard coded Label encoding\ntrain['ord_1'] = [0 if x == 'Novice' else 1 if x == 'Contributor' else 2 if x == 'Expert' else 3 if x == 'Master' else 4 for x in train['ord_1']]\ntrain['ord_2'] = [0 if x == 'Freezing' else 1 if x == 'Cold' else 2 if x == 'Warm' else 3 if x == 'Hot' else 4 if x == 'Boiling Hot' else 5 for x in train['ord_2']]\n\n# Label encoding via LabelEncoder class\nlabel_encoder = LabelEncoder()\ntrain['ord_3'] = label_encoder.fit_transform(train['ord_3'])\ntest['ord_3'] = label_encoder.transform(test['ord_3'])\n\ntrain['ord_4'] = label_encoder.fit_transform(train['ord_4'])\ntest['ord_4'] = label_encoder.transform(test['ord_4'])\n\ntrain['ord_5'] = label_encoder.fit_transform(train['ord_5'])\ntest['ord_5'] = label_encoder.transform(test['ord_5'])\n\n#train['ord_5b'] = label_encoder.fit_transform(train['ord_5b'])\n\n#train['ord_3'] = train['ord_3'].apply(map_to_ascii_index)\n#train['ord_4'] = train['ord_4'].apply(map_to_ascii_index)\n#train['ord_5'] = label_encoder.fit_transform(train['ord_5'])\n\n\n#train = train.drop('ord_5b', axis=1)\n\ntrain = date_cyc_enc(train, 'day', 7)\ntrain = date_cyc_enc(train, 'month', 12)\ntrain.drop(['day', 'month'], axis=1, inplace = True)\n\n#Leave one out encoding high cardinal variables\nhigh_cardinal_vars = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n\n#trgt_encoder = TargetEncoder(cols=high_cardinal_vars, smoothing=0, return_df=True)\n#hashing_encoder = HashingEncoder(cols = high_cardinal_vars)\nloo_encoder = LeaveOneOutEncoder(cols=high_cardinal_vars)\ntrain = loo_encoder.fit_transform(train.drop(['target'], axis = 1), train['target'])\n\n# Same for test data\ntest['bin_3'] = [0 if x == 'F' else 1 for x in test['bin_3']]\ntest['bin_4'] = [0 if x == 'N' else 1 for x in test['bin_4']]\ntest['ord_1'] = [0 if x == 'Novice' else 1 if x == 'Contributor' else 2 if x == 'Expert' else 3 if x == 'Master' else 4 for x in test['ord_1']]\ntest['ord_2'] = [0 if x == 'Freezing' else 1 if x == 'Cold' else 2 if x == 'Warm' else 3 if x == 'Hot' else 4 if x == 'Boiling Hot' else 5 for x in test['ord_2']]\n\n#test = test.drop('ord_5b', axis=1)\n\n#test['ord_3'] = test['ord_3'].apply(map_to_ascii_index)\n#test['ord_4'] = test['ord_4'].apply(map_to_ascii_index)\n#test['ord_5b'] = label_encoder.fit_transform(test['ord_5b'])\n\n#for column in ['ord_3', 'ord_4', 'ord_5a']:\n#    train[column] = train[column].apply(map_to_ascii_index)\n#    test[column] = test[column].apply(map_to_ascii_index)\n\n#For cyclic data we convert it into sin and cosine values\ntest = date_cyc_enc(test, 'day', 7)\ntest = date_cyc_enc(test, 'month', 12)\ntest.drop(['day', 'month'], axis=1, inplace = True)\n\ntest = loo_encoder.transform(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One Hot encoding other nominal columns\ntrain_df = pd.get_dummies(train, drop_first=True)\n\n# Same for test data\n#test_modified = test.drop(nominal_variables, axis = 1)\ntest_df = pd.get_dummies(test, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor = train_df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(25, 25))\nsns.heatmap(cor, annot=False, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that correlation between data points is quite less here. So lets keep all these features and move ahead with our classification\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df.drop(['id'], axis=1)\ny = target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysing imbalanace in dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=y.value_counts()\nplt.bar(x.index,x)\nplt.gca().set_xticks([0,1])\nplt.title('distribution of target variable')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly there is imbalance in dataset. We need to cater this implance using SMOTE technique"},{"metadata":{},"cell_type":"markdown","source":"Now lets do the necessary train_test_split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets cater the imbabalnce in dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(kind = \"regular\")\nX_tr,y_tr = sm.fit_sample(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets create a function to test our dataset and calculate ROC-AUC score for multiple models as param"},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for comparing different approaches\ndef score_dataset(X_train, X_test, y_train, y_test, model):\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    draw_roc(y_test, preds)\n    return roc_auc_score(y_test, preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scaling data"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_tr = scaler.fit_transform(X_tr)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing with vanilla version of models"},{"metadata":{},"cell_type":"markdown","source":"Lets plot roc curver and calculate score for all models"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(random_state=0, solver = 'lbfgs')\nprint('AUC score with Logistic Regression :- ', score_dataset(X_tr, X_test, y_tr, y_test, lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rft = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\nprint('AUC score with RandomForest :- ', score_dataset(X_tr, X_test, y_tr, y_test, rft))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(random_state=0)\nprint('AUC score with Decision Tree :- ', score_dataset(X_tr, X_test, y_tr, y_test, dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gaussianNB = GaussianNB(priors=None, var_smoothing=1e-09)\nprint('AUC score with gausian Naive Bayes :- ', score_dataset(X_tr, X_test, y_tr, y_test, gaussianNB))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Refinement"},{"metadata":{"trusted":true},"cell_type":"code","source":"# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparam_grid = {\n    'max_depth': range(1, 5),\n    'min_samples_leaf': range(25, 175, 50),\n    'min_samples_split': range(50, 150, 50)\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decison Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# uncomment if you want to see hyper parameter tuning. Although it takes some good amount of time\n'''\n# instantiate the model\ndt = DecisionTreeClassifier()\n\n# fit tree on training data\ngrid_search_dt = GridSearchCV(estimator = dt, param_grid = param_grid, \n                          cv = n_folds, verbose = 1, n_jobs = -1, scoring=\"roc_auc\")\ngrid_search_dt.fit(X_tr, y_tr)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# uncomment to see the results\n'''\ncv_results_dt = pd.DataFrame(grid_search_dt.cv_results_)\n# printing the optimal accuracy score and hyperparameters\nprint(\"Decison Tree grid search Accuracy : \", grid_search_dt.best_score_)\nprint(grid_search_dt.best_estimator_)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid['n_estimators']  = range(50, 200, 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# uncomment if you want to see hyper parameter tuning. Although it takes some good amount of time\n'''\n# instantiate the model\nrft = RandomForestClassifier(n_jobs= -1)\n\n# fit tree on training data\ngrid_search_rft = GridSearchCV(estimator = rft, param_grid = param_grid, \n                          cv = n_folds, verbose = 1, n_jobs = -1, scoring=\"roc_auc\")\ngrid_search_rft.fit(X_tr, y_tr)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# uncomment to see the results\n'''\ncv_results_rft = pd.DataFrame(grid_search_rft.cv_results_)\n# printing the optimal accuracy score and hyperparameters\nprint(\"Random Forest grid search Accuracy : \", grid_search_rft.best_score_)\n# Best estimators\nprint(grid_search_rft.best_estimator_)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlogit_param_grid = {\n    'C': [0.100, 0.150, 0.120, 0.125, 0.130, 0.135, 0.140, 0.145, 0.150]\n}\n\nlogit_grid = GridSearchCV(estimator = lr, param_grid = logit_param_grid,\n                          scoring='roc_auc', cv=5, n_jobs=-1, verbose=0)\nlogit_grid.fit(X_tr, y_tr)\n\nbest_C = logit_grid.best_params_['C']\n# best_C = C = 0.125\n\nprint('Best C:', best_C)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have got our tuned Random Forest and Decison Tree. Lets take them and fit with our data"},{"metadata":{},"cell_type":"markdown","source":"## Model fitting with tuned hyper parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"rft = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                       max_depth=4, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=25, min_samples_split=50,\n                       min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n                       oob_score=False, random_state=None, verbose=0,\n                       warm_start=False)\nprint('AUC score with RandomForest :- ', score_dataset(X_tr, X_test, y_tr, y_test, rft))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=25, min_samples_split=50,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=None, splitter='best')\nprint('AUC score with Decision Tree :- ', score_dataset(X_tr, X_test, y_tr, y_test, dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(solver='lbfgs', random_state = 0, C=best_C)\nprint('AUC score with Losgistic Regression :- ', score_dataset(X_tr, X_test, y_tr, y_test, lr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Boosting algorithms"},{"metadata":{},"cell_type":"markdown","source":"## Catboost"},{"metadata":{},"cell_type":"markdown","source":"Lets try this with a boosting algorithm also . We will use **CatBoostClassifier** . First lets try with Vanilla model version"},{"metadata":{"trusted":true},"cell_type":"code","source":"catboost = CatBoostClassifier(iterations=20,learning_rate=1,depth=2, custom_metric=['AUC'])\nprint('AUC score with Catboost classifier :- ', score_dataset(X_tr, X_test, y_tr, y_test, catboost))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logitboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"lboost = LogitBoost(n_estimators=200, random_state=0)\nprint('AUC score with Logitboost classifier :- ', score_dataset(X_tr, X_test, y_tr, y_test, lboost))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Xgboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost = XGBClassifier(random_state=0)\nprint('AUC score with Xgboost classifier :- ', score_dataset(X_tr, X_test, y_tr, y_test, xgboost))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets predict for test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.drop(['id'], axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets scale test data first"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = scaler.transform(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets fit for entire training set before making predictions. But before that we have to scale entire train data also"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = scaler.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = lr.fit(X,y)\ndt = dt.fit(X,y)\nrft = rft.fit(X,y)\ncatboost = catboost.fit(X,y)\nlboost = lboost.fit(X,y)\ngaussianNB = gaussianNB.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_final_lr = lr.predict(test_df)\ny_test_final_dt = dt.predict(test_df)\ny_test_final_rft = rft.predict(test_df)\ny_test_NB = gaussianNB.predict(test_df)\ny_test_final_catboost = catboost.predict(test_df)\ny_test_final_logitboost = lboost.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be using all these models to calculate scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_prob_lr = lr.predict_proba(test_df)[:, 1]\ny_test_prob_dt = dt.predict_proba(test_df)[:, 1]\ny_test_prob_rft = rft.predict_proba(test_df)[:, 1]\ny_test_prob_NB = gaussianNB.predict_proba(test_df)[:, 1]\ny_test_prob_catboost = catboost.predict_proba(test_df)[:, 1]\ny_test_prob_logitboost = lboost.predict_proba(test_df)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_test_prob_lr\n    })\nsubmission.to_csv('LogisticRegression.csv',header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_test_prob_dt\n    })\nsubmission.to_csv('DecisonTree.csv',header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_test_prob_rft\n    })\nsubmission.to_csv('RandomForest.csv',header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_test_prob_NB\n    })\nsubmission.to_csv('GaussianNB.csv',header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_test_prob_catboost\n    })\nsubmission.to_csv('Catboost.csv',header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_test_prob_logitboost\n    })\nsubmission.to_csv('Logitboost.csv',header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the best submission file to submit your score. any more suggestions welcome. Will still try to imporve this kernel"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}