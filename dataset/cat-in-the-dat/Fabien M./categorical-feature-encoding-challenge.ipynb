{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Categorical Feature Encoding Challenge\n\n# Table of contents\n\n1. [Context](#context)  \n2. [Importations](#importations)  \n3. [Informations](#informations)\n4. [Set parameters](#set_parameters)\n5. [Data exploration](#data_exploration)  \n    5.1 [Import data](#import_data)  \n    5.2 [General analysis](#general_analysis)   \n    5.3 [Pre-processing](#pre_processing)  \n    5.4 [Univariant analysis](#univariant_analysis)  \n6. [Modelisation](#modelisation)  \n    6.1 [Learning](#learning)    \n    6.2 [Results](#results)   \n7. [Submission](#submission)\n8. [Prediction](#prediction)\n9. [Submission](#submission)\n10. [References](#references)\n\n# 1. Context <a id=\"context\"></a>\n\n<p style=\"text-align:center;\">\n    <img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/14999/logos/header.png?t=2019-08-22-18-17-37\" style=\"height:100%; width:100%\"/>\n</p>\n\nThe goal of this \"Playground competition\" is to give the opportunity to try different encoding schemes for different algorithms and to compare how they perform."},{"metadata":{},"cell_type":"markdown","source":"# 2. Importations <a id=\"importations\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Get version python/keras/tensorflow/sklearn\nfrom platform import python_version\nimport sklearn\n\n# Folder manipulation\nimport os\n\n# Garbage collector\nimport gc\n\n# Linear algebra and data processing\nimport numpy as np\nimport pandas as pd\nfrom pandas import datetime\n\n# Visualisation of picture and graph\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n# Sklearn importation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, MinMaxScaler\nfrom sklearn.metrics import roc_auc_score, classification_report, roc_curve\nfrom sklearn.base import clone\nfrom sklearn import base","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Informations <a id=\"informations\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))\nprint(\"Python version : \" + python_version())\nprint(\"Sklearn version : \" + sklearn.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Set parameters <a id=\"set_parameters\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAIN_DIR = \"../input/cat-in-the-dat/\"\n\nTRAIN_DIR = MAIN_DIR + \"train.csv\"\nTEST_DIR = MAIN_DIR + \"test.csv\"\n\nBINS_FEAT = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']\nNAMES_FEAT = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\nORDINALS_FEAT = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']\nOTHERS_FEAT = ['day', 'month']\n\n# Set graph font size\nsns.set(font_scale=1.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Data exploration <a id=\"data_exploration\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Import data <a id=\"import_data\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data():\n    df_train = pd.read_csv(TRAIN_DIR)\n    df_test = pd.read_csv(TEST_DIR)\n    return df_train, df_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_train_raw, data_test_raw = load_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape training data : {data_train_raw.shape}\")\nprint(f\"Shape test data : {data_test_raw.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 General analysis <a id=\"general_analysis\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_raw.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_raw.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3 Pre processing <a id=\"pre_processing\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduce memory used on RAM\ndef pre_processing(data_train, data_test):\n    df_train = data_train.copy()\n    df_test = data_test.copy()\n    \n    dtypes = {'bin_0': 'int8',\n              'bin_1': 'int8',\n              'bin_2': 'int8',\n              'ord_0': 'int8',\n              'day': 'int8',\n              'month': 'int8'}\n    \n    df_train = df_train.astype(dtypes)\n    df_train['target'] = df_train['target'].astype('int8')\n    \n    df_test = df_test.astype(dtypes)\n    \n    return df_train, df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_pre, data_test_pre = pre_processing(data_train_raw, data_test_raw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.4 Univariant analysis <a id=\"univariant_analysis\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_plot(data_train, data_test,\n               feats,\n               title=\"\",\n               n_cols=3,\n               figsize=(20, 10)):\n    \n    df_train = data_train.copy()\n    df_test = data_test.copy()\n    \n    df_train['dataset'] = 'train'\n    df_test['dataset'] = 'test'\n    \n    df_train_test = pd.concat([df_train, df_test], axis=0)\n    \n    n_axes = int(np.ceil(len(feats)/n_cols))\n    n_axes_last = len(feats)%3\n    \n    if(n_axes > 1):\n        fig, axes = plt.subplots(n_axes, n_cols, figsize=figsize)\n        # Delete useless ax\n        for ax in axes[-1,n_axes_last:]:\n            fig.delaxes(ax)\n    else:\n        fig, axes = plt.subplots(n_axes, len(feats), figsize=figsize)\n    \n    for ax, feat in zip(axes.ravel()[0:len(feats)], feats):\n        sns.countplot(x=feat, hue=\"dataset\", data=df_train_test, ax=ax)\n        \n    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.25, hspace=None)\n    fig.suptitle(title, fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binary features"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_plot(data_train_raw, data_test_raw, \n           feats=BINS_FEAT, \n           title=\"Binary features\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Name Feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_pre[NAMES_FEAT].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_plot(data_train_pre, data_test_pre, \n           feats=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'],\n           title=\"Names features\",\n           figsize=(20, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ordinal feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_pre[ORDINALS_FEAT].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_plot(data_train_pre, data_test_pre, \n           feats=['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4'],\n           title=\"Ordinals features\",\n           figsize=(20, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Others features"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_plot(data_train_pre, data_test_pre, \n           feats=OTHERS_FEAT,\n           title=\"Others features\",\n           n_cols=2,\n           figsize=(13, 4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(5, 4))\nsns.countplot(x='target', data=data_train_pre, ax=ax)\nfig.suptitle('Target feature', fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Feature Engineering <a id=\"feature_engineering\"></a>"},{"metadata":{},"cell_type":"markdown","source":"For more information about KFold target encoding class see [[1]](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b).  \nFor some useful part for feature engineering see [[2]](https://www.kaggle.com/pavelvpster/cat-in-dat-ohe-vs-thermometer-logit)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class KFoldTargetEncoderTrain(base.BaseEstimator,\n                               base.TransformerMixin):\n    def __init__(self,colnames,targetName,\n                  n_fold=5, verbosity=True,\n                  discardOriginal_col=False):\n        self.colnames = colnames\n        self.targetName = targetName\n        self.n_fold = n_fold\n        self.verbosity = verbosity\n        self.discardOriginal_col = discardOriginal_col\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self,X):\n        assert(type(self.targetName) == str)\n        assert(type(self.colnames) == str)\n        assert(self.colnames in X.columns)\n        assert(self.targetName in X.columns)\n        mean_of_target = X[self.targetName].mean()\n        kf = KFold(n_splits = self.n_fold,\n                   shuffle = False, random_state=2019)\n        col_mean_name = self.colnames + '_' + 'Kfold_Target_Enc'\n        X[col_mean_name] = np.nan\n        for tr_ind, val_ind in kf.split(X):\n            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\n            X.loc[X.index[val_ind], col_mean_name] = X_val[self.colnames].map(X_tr.groupby(self.colnames)\n                                     [self.targetName].mean())\n            X[col_mean_name].fillna(mean_of_target, inplace = True)\n        if self.verbosity:\n            encoded_feature = X[col_mean_name].values\n            print('Correlation between the new feature, {} and, {} is {}.'.format(col_mean_name,self.targetName,                    \n                   np.corrcoef(X[self.targetName].values,\n                               encoded_feature)[0][1]))\n        if self.discardOriginal_col:\n            X = X.drop(self.targetName, axis=1)\n        return X\n    \n    \nclass KFoldTargetEncoderTest(base.BaseEstimator,\n                             base.TransformerMixin):\n    def __init__(self,train,colNames,encodedName):\n        \n        self.train = train\n        self.colNames = colNames\n        self.encodedName = encodedName\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self,X):\n        mean =  self.train[[self.colNames,\n                self.encodedName]].groupby(\n                                self.colNames).mean().reset_index() \n        \n        dd = {}\n        for index, row in mean.iterrows():\n            dd[row[self.colNames]] = row[self.encodedName]\n        X[self.encodedName] = X[self.colNames]\n        X = X.replace({self.encodedName: dd})\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(data_train, data_test):\n    df_train = data_train.copy()\n    df_test = data_test.copy()\n    \n    df_traintest = pd.concat([df_train, df_test])\n    \n    print(\"# BINS FEATURES\")\n    \n    print(\"\\t# bin_3\")\n    df_traintest.loc[df_train['bin_3'] == 'T', 'bin_3'] = 1\n    df_traintest.loc[df_train['bin_3'] == 'F', 'bin_3'] = 0\n    \n    print(\"\\t# bin_4\")\n    df_traintest.loc[df_train['bin_4'] == 'Y', 'bin_4'] = 1\n    df_traintest.loc[df_train['bin_4'] == 'N', 'bin_4'] = 0\n    \n    print(\"# ORDINALS FEATURES\")\n    \n    print(\"\\t# Label encoding\")\n    \n    ord_1_map = {\n        'Grandmaster':'4',\n        'Master':'3',\n        'Expert':'2',\n        'Contributor':'1',\n        'Novice':'0'\n    }\n    df_traintest['ord_1'] = df_traintest['ord_1'].map(ord_1_map)\n    \n    ord_2_map = {\n        'Lava Hot': '5',\n        'Boiling Hot':'4',\n        'Hot':'3',\n        'Warm':'2',\n        'Cold':'1',\n        'Freezing':'0'\n    }\n    df_traintest['ord_2'] = df_traintest['ord_2'].map(ord_2_map)\n    \n    df_traintest['ord_5a'] = df_traintest['ord_5'].apply(lambda x : list(x)[0])\n    df_traintest['ord_5b'] = df_traintest['ord_5'].apply(lambda x : list(x)[1])\n    df_traintest['ord_5'] = df_traintest.drop(['ord_5'], axis=1)\n    \n    for feat in ['ord_3', 'ord_4', 'ord_5a', 'ord_5b']:\n        dict_ord = dict()\n        values = df_traintest[feat].apply(lambda x : list(x)[0]).value_counts().index.sort_values().values\n\n        for value, label in zip(values, range(values.shape[0])):\n            dict_ord[value] = label\n            \n        df_traintest[feat] = df_traintest[feat].map(dict_ord)\n        \n    feat_to_encode = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5a', 'ord_5b']\n    df_traintest[feat_to_encode] = MinMaxScaler().fit_transform(df_traintest[feat_to_encode])\n    \n    print(\"\\t# Target encoding\")\n\n    # Split data for encoding\n    df_train_enc = df_traintest.iloc[:df_train.shape[0]]\n    df_test_enc = df_traintest.iloc[df_train.shape[0]:]\n    \n    for feat in feat_to_encode:\n        print(f\"\\t\\t# {feat}\")\n        \n        # For train\n        targetc = KFoldTargetEncoderTrain(feat,'target',n_fold=5)\n        df_train_enc = targetc.fit_transform(df_train_enc)\n        \n        # For test\n        test_targetc = KFoldTargetEncoderTest(df_train_enc,\n                                       feat,\n                                       feat+'_Kfold_Target_Enc')\n        df_test_enc = test_targetc.fit_transform(df_test_enc)\n        \n    df_traintest = pd.concat([df_train_enc, df_test_enc], axis=0)\n    \n    print(\"# NAMESS FEATS and OTHERS_FEAT\")\n\n    df_traintest = pd.get_dummies(df_traintest,\n                                  columns=NAMES_FEAT+OTHERS_FEAT,\n                                  sparse=True,\n                                  drop_first=True)\n    \n    df_train = df_traintest.iloc[:df_train.shape[0], :]\n    df_test = df_traintest.iloc[df_train.shape[0]:, :]\n    \n    df_test = df_test.drop(['target'], axis=1)\n    \n    print(\"# DROP FEATURES\")\n    \n    df_traintest = df_traintest.drop(['bin_0', 'ord_5b'], axis=1)\n    \n    gc.collect()\n    \n    return df_train, df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndata_train_raw, data_test_raw = load_data()\ndata_train_pre, data_test_pre = pre_processing(data_train_raw, data_test_raw)\ndata_train, data_test = feature_engineering(data_train_pre, data_test_pre)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape training data : {data_train.shape}\")\nprint(f\"Shape test data : {data_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 7. Modelisation <a id=\"modelisation\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_matrices(data_train, data_test):\n    X = data_train.drop(['target', 'id'], axis=1)\n    X_test = data_test.drop(['id'], axis=1)\n    \n    # Transform to sparse matrice\n    X = X.astype(float).to_sparse().to_coo().tocsr()\n    X_test = X_test.astype(float).to_sparse().to_coo().tocsr()\n    \n    y = data_train['target']\n    return X, y, X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_auc_folds(y_true_folds, y_pred_folds, n_cols=3, figsize=(15, 10), title=\"\"):\n    \n    def plot_roc_auc(y_true, y_pred, ax, i):\n        fpr, tpr, thresholds = roc_curve(y_true, y_pred,\n                                         drop_intermediate=False)\n        auc_score = roc_auc_score(y_true, y_pred)\n        \n        ax.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n        ax.plot([0, 1], [0, 1], 'k--')\n\n        ax.set_xlim([0.0, 1.0])\n        ax.set_ylim([0.0, 1.05])\n        ax.set_xlabel('FPR or [1 - TPR]')\n        ax.set_ylabel('TPR')\n        ax.set_title(f'ROC fold {i}')\n        ax.legend(loc=\"lower right\")\n        return ax\n    \n    n_axes = int(np.ceil(len(y_true_folds)/n_cols))\n    n_axes_last = len(y_true_folds)%3\n    \n    if(n_axes > 1):\n        fig, axes = plt.subplots(n_axes, n_cols, figsize=figsize)\n    else:\n        fig, axes = plt.subplots(n_axes, len(y_true_folds), figsize=figsize)\n    \n    for y_true, y_pred, ax, fold in zip(y_true_folds, \n                                        y_pred_folds,\n                                        axes.ravel()[0:len(y_true_folds)],\n                                        range(len(y_true_folds))):\n        plot_roc_auc(y_true, y_pred, ax=ax, i=fold)\n        \n    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.25, hspace=0.3)\n    fig.suptitle(title, fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_classification_report_folds(y_true_folds, y_pred_folds):\n    print(\"Detailed classification report:\\n\")\n    for y_true, y_pred, fold in zip(y_true_folds, \n                              y_pred_folds, \n                              range(len(y_true_folds))):\n        print(f\"FOLDS : {fold}\")\n        print(classification_report(y_true=y_true, y_pred=y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_eval(X, y, n_folds=6):\n    \n    kf = KFold(n_splits=n_folds, shuffle=True)\n    print(f\"Numer of folds is {kf.get_n_splits(X)}\")\n    \n    y_true_folds = []\n    y_pred_folds = []\n    \n    fold = kf.get_n_splits(X)\n    \n    for train_index, test_index in kf.split(X):\n        print(f\"FOLD : {fold}\")\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n    \n        model = LogisticRegression(n_jobs=-1, solver='lbfgs', max_iter=3000, C=0.1)\n        model.fit(X_train, y_train)\n        \n        y_true, y_pred = y_test.values, model.predict(X_test)\n        \n        y_true_folds.append(y_true)\n        y_pred_folds.append(y_pred)\n        \n        fold -= 1\n    \n    return model, y_true_folds, y_pred_folds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7.1 Learning <a id=\"learning\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX, y, X_test = get_matrices(data_train, data_test)\nmodel, y_true_folds, y_pred_folds = train_eval(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7.2 Results <a id=\"results\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_auc_folds(y_true_folds, y_pred_folds, title=\"ROC and ROC-AUC for each folds\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_classification_report_folds(y_true_folds, y_pred_folds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Retrain the model on all the data\nmodel = clone(model, safe=True)\nmodel.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Prediction <a id=\"prediction\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_pred_ratio(y_pred):\n    df = pd.DataFrame(y_pred, columns=['y_pred'])\n    \n    fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n    sns.countplot(df['y_pred'], ax=ax)\n    ax.set_title(\"Prediction repartition\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\nplot_pred_ratio(y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. Submission <a id=\"submission\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def submission(y_pred):\n    df_test = pd.read_csv(TEST_DIR)\n    sub = pd.DataFrame({\"id\": df_test[\"id\"], \"target\": y_pred})\n    sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict_proba(X_test)[:,1]\nsubmission(y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a href=\"./submission.csv\">Download submission</a>"},{"metadata":{},"cell_type":"markdown","source":"# 10. References\n\n[[1]](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b) Source code about KFold encoding.  \n[[2]](https://www.kaggle.com/pavelvpster/cat-in-dat-ohe-vs-thermometer-logit) Useful part for feature engineering."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}