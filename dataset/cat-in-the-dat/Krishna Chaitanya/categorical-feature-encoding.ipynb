{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nsns.set(style='darkgrid')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/cat-in-the-dat/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/cat-in-the-dat/test.csv\")\nsubm_data = pd.read_csv(\"/kaggle/input/cat-in-the-dat/sample_submission.csv\")\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To CHeck \ntrain_data.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Id column is not necessary \ntrain_data=train_data.drop(['id'], axis = 1)\ntest_data=test_data.drop(['id'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"All_features=train_data.columns.tolist()\nNumerical_features=['bin_0','bin_1','bin_2','ord_0','day','month','target']\ncategorical_features=list(set(All_features) - set(Numerical_features))\nNumerical_features.remove('target')\nprint(categorical_features)\nprint(Numerical_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n<h3>Working on Numerical Data</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# to get description of numerical data in dataset\ntrain_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, 3, figsize=(15, 10))\nfor variable, subplot in zip(Numerical_features, ax.flatten()):\n    sns.boxplot(train_data[variable], ax=subplot, color='black')\n    for label in subplot.get_xticklabels():\n        label.set_rotation(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No Need of outlier Treatment"},{"metadata":{},"cell_type":"markdown","source":"<hr>\n<h3>Working on Categorical Data</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for cname in categorical_features:\n    print(cname+\" : \"+str(len(train_data[cname].unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can divide the categorical variables available into 3 sub categories\n1. Binary Categorical variables - Converting them into 1's and 0's\n2. Ordinal Variables - Converting them into numerical data starting from 1 to N\n3. Nominal Variables - One hot encoding\n4. day and month "},{"metadata":{"trusted":true},"cell_type":"code","source":"binary = {'T': 1,'F': 0}\ntrain_data[\"bin_3\"]= [binary[item] for item in train_data[\"bin_3\"]]\ntest_data[\"bin_3\"]= [binary[item] for item in test_data[\"bin_3\"]]\nbinary = {'Y': 1,'N': 0}\ntrain_data[\"bin_4\"]= [binary[item] for item in train_data[\"bin_4\"]]\ntest_data[\"bin_4\"]= [binary[item] for item in test_data[\"bin_4\"]]\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nominal_col = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']\nordinal_col = ['ord_0', 'ord_1', 'ord_2', 'ord_3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keeping the ord_5 features aside as it has higher amount of cardinality\n# Importing categorical options of pandas\nfrom pandas.api.types import CategoricalDtype \n\n# seting the orders of our ordinal features\nord_1 = CategoricalDtype(categories=['Novice', 'Contributor','Expert', \n                                     'Master', 'Grandmaster'], ordered=True)\nord_2 = CategoricalDtype(categories=['Freezing', 'Cold', 'Warm', 'Hot',\n                                     'Boiling Hot', 'Lava Hot'], ordered=True)\nord_3 = CategoricalDtype(categories=['a', 'b', 'c', 'd', 'e', 'f', 'g',\n                                     'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'], ordered=True)\nord_4 = CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n                                     'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n                                     'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], ordered=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming ordinal Features\ntrain_data.ord_1 = train_data.ord_1.astype(ord_1)\ntrain_data.ord_2 = train_data.ord_2.astype(ord_2)\ntrain_data.ord_3 = train_data.ord_3.astype(ord_3)\ntrain_data.ord_4 = train_data.ord_4.astype(ord_4)\ntrain_data.ord_1 = train_data.ord_1.cat.codes\ntrain_data.ord_2 = train_data.ord_2.cat.codes\ntrain_data.ord_3 = train_data.ord_3.cat.codes\ntrain_data.ord_4 = train_data.ord_4.cat.codes\ntrain_data.head()\n\ntest_data.ord_1 = test_data.ord_1.astype(ord_1)\ntest_data.ord_2 = test_data.ord_2.astype(ord_2)\ntest_data.ord_3 = test_data.ord_3.astype(ord_3)\ntest_data.ord_4 = test_data.ord_4.astype(ord_4)\ntest_data.ord_1 = test_data.ord_1.cat.codes\ntest_data.ord_2 = test_data.ord_2.cat.codes\ntest_data.ord_3 = test_data.ord_3.cat.codes\ntest_data.ord_4 = test_data.ord_4.cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(str(train_data.day.unique())+\" \"+str(train_data.month.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['ord_5_ot'] = 'Others'\ntrain_data.loc[train_data['ord_5'].isin(train_data['ord_5'].value_counts()[:25].sort_index().index), 'ord_5_ot'] = train_data['ord_5']\n\ntest_data['ord_5_ot'] = 'Others'\ntest_data.loc[test_data['ord_5'].isin(test_data['ord_5'].value_counts()[:25].sort_index().index), 'ord_5_ot'] = test_data['ord_5']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nsns.countplot(x='ord_5_ot', data=train_data,\n                   order=list(train_data['ord_5_ot'].value_counts().sort_index().index) ,\n                   color='black') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ord_5_count = train_data['ord_5'].value_counts().reset_index()['ord_5'].values\nplt.figure(figsize=(20,5))\ng = sns.distplot(ord_5_count, bins= 50,color='black')\ng.set_title(\"Frequency\", fontsize=22)\ng.set_xlabel(\"Total\", fontsize=18)\ng.set_ylabel(\"Density\", fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Credit of this features to: \n## https://www.kaggle.com/gogo827jz/catboost-baseline-with-feature-importance\nimport string\n# Then encode 'ord_5' using ACSII values\n# Add up the indices of two letters in string.ascii_letters\ntrain_data['ord_5_new'] = train_data['ord_5_ot'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\ntest_data['ord_5_new'] = test_data['ord_5_ot'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\n#train_data['ord_5_new']= train_data['ord_5_new'].astype('float64')\n                                                    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=train_data.drop(['ord_5_ot','ord_5'], axis = 1) \ntest_data=test_data.drop(['ord_5_ot','ord_5'], axis = 1) \ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> We have successfully converted all the ordinal data into numerical data</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"nominal_col = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']\nfig, ax = plt.subplots(2, 3, figsize=(20, 10))\nfor variable, subplot in zip(nominal_col, ax.flatten()):\n    sns.countplot(train_data[variable], ax=subplot, color='black')\n    for label in subplot.get_xticklabels():\n        label.set_rotation(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"high_card_feats = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\nfor x in high_card_feats:\n    print(x+\"-\"+str(len(train_data[x].unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in high_card_feats:\n    train_data[f'hash_{col}'] = train_data[col].apply( lambda x: hash(str(x)) % 5000 )\n    test_data[f'hash_{col}'] = test_data[col].apply( lambda x: hash(str(x)) % 5000 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in high_card_feats:\n    enc_nom_1 = (train_data.groupby(col).size()) / len(train_data)\n    train_data[f'freq_{col}'] = train_data[col].apply(lambda x : enc_nom_1[x])\n    #df_test[f'enc_{col}'] = df_test[col].apply(lambda x : enc_nom_1[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Label ENcoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Label Encoding\nfor f in ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']:\n    if train_data[f].dtype=='object' or test_data[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_data[f].values) + list(test_data[f].values))\n        train_data[f'le_{f}'] = lbl.transform(list(train_data[f].values))\n        test_data[f'le_{f}'] = lbl.transform(list(test_data[f].values))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\nsns.countplot(x='le_nom_5', data=train_data,\n                   order=list(train_data['le_nom_5'].value_counts().sort_index().index) ,\n                   color='black') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.drop([ 'hash_nom_5','hash_nom_6', 'hash_nom_7', 'hash_nom_8', 'hash_nom_9','freq_nom_5','freq_nom_6', 'freq_nom_7', 'freq_nom_8', 'freq_nom_9',\n                'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'], axis=1, inplace=True)\n\ntest_data.drop([ 'hash_nom_5','hash_nom_6', 'hash_nom_7', 'hash_nom_8', 'hash_nom_9','nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# credits to eda-feat-engineering-encode-conquer kernal\ntest_data['target'] = 'test'\ndf = pd.concat([train_data, test_data], axis=0, sort=False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape before dummy transformation: {df.shape}')\ndf = pd.get_dummies(df, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'],\\\n                          prefix=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], drop_first=True)\nprint(f'Shape after dummy transformation: {df.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data = df[df['target'] != 'test'], df[df['target'] == 'test'].drop('target', axis=1)\ndel df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the features are now handled"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx = train_data.drop([\"target\"], axis=1)\ny = train_data[\"target\"]\ny = y.astype(bool)\ntest_X = test_data.drop([],axis=1)\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 15)             # running RFE with 10 variables as output\nrfe = rfe.fit(x_train,y_train)\nprint(rfe.support_)           # Printing the boolean results\nprint(rfe.ranking_)           # Printing the ranking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = x_train.columns[rfe.support_]\nprint(col)\nUpdatedTrain_X=x_train[col]\nprint(UpdatedTrain_X.shape)\nUpdatedTest_X=x_test[col]\nprint(UpdatedTest_X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\ndf_train_rfe = sm.add_constant(UpdatedTrain_X)\nlog_mod_rfe = sm.GLM(y_train,df_train_rfe,family = sm.families.Binomial())\nmod_res_rfe = log_mod_rfe.fit()\nlog_mod_rfe.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting the Test Data\nUpdatedTestCoef_X = sm.add_constant(UpdatedTest_X[col])\npredictions = mod_res_rfe.predict(UpdatedTestCoef_X)\nY_pred= predictions.map(lambda x: 1 if x > 0.5 else 0)\nY_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.confusion_matrix(y_test, Y_pred), \"\\n\")\nprint(\"accuracy\", metrics.accuracy_score(y_test, Y_pred))\nprint(\"precision\", metrics.precision_score(y_test,Y_pred))\nprint(\"recall\", metrics.recall_score(y_test,Y_pred))\nconfusion=confusion_matrix(y_test,Y_pred)    \nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n# Let's see the sensitivity of our logistic regression model\nprint(\"Sensitivity\",TP / float(TP+FN))\n# positive predictive value \nprint (\"Positive Predection Rate\",TP / float(TP+FP))\n# Negative predictive value\nprint (\"Negative Predection rate\",TN / float(TN+ FN))\n# Calculate false postive rate - predicting churn when customer does not have churned\nprint(\"False positive Predection Rate\",FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**KNN classification alogrithm**"},{"metadata":{"trusted":true},"cell_type":"code","source":"k_range = [1,3,5,7,9,10,15]\nscores=[]\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n    knn.fit(UpdatedTrain_X, y_train)\n    y_pred = knn.predict(UpdatedTest_X)\n    scores.append(metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(k_range, scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Testing Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\nknn.fit(UpdatedTrain_X, y_train)\ny_pred = knn.predict(UpdatedTest_X)\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(\"accuracy\", metrics.accuracy_score(y_test, y_pred))\nprint(\"precision\", metrics.precision_score(y_test,y_pred))\nprint(\"recall\", metrics.recall_score(y_test,y_pred))\nconfusion=confusion_matrix(y_test,y_pred)    \nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n# Let's see the sensitivity of our logistic regression model\nprint(\"Sensitivity\",TP / float(TP+FN))\n# positive predictive value \nprint (\"Positive Predection Rate\",TP / float(TP+FP))\n# Negative predictive value\nprint (\"Negative Predection rate\",TN / float(TN+ FN))\n# Calculate false postive rate - predicting churn when customer does not have churned\nprint(\"False positive Predection Rate\",FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_data=test_data[col]\ny_pred = knn.predict(sub_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm_data['target'] = y_pred\nsubm_data.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}