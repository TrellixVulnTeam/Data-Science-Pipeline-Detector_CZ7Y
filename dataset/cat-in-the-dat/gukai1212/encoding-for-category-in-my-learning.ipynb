{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Reference\n\n1. [Handling Categorical Variables:Encoding & Modeling](https://www.kaggle.com/vikassingh1996/handling-categorical-variables-encoding-modeling)\n2. [An Overview of Encoding Techniques](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques/notebook)\n3. [Categorical Data encoding techniques](https://www.kaggle.com/ruchibahl18/categorical-data-encoding-techniques)\n4. [Category Encoders Examples](https://www.kaggle.com/discdiver/category-encoders-examples)\n5. [Entity embeddings to handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories)\n6. [Why Not Logistic Regression?](https://www.kaggle.com/peterhurford/why-not-logistic-regression)"},{"metadata":{},"cell_type":"markdown","source":"### Backgroud"},{"metadata":{},"cell_type":"markdown","source":"* **Binary data**: A binary variable is a variable with only two values, like 1/0, such as bin_0,bin_1,bin_2.\n* **Categorical data** \n    * **Ordinal data**: An ordinal variable is a categorical variable with a ordering. For low ordinal features, like ord_1, ord_2, ord_3, ord_4. For high ordinal data, like ord_5.\n    * **Nominal data**: Nominal variables contain two or more categories without a natural ordering. For low nominal features, like nom_0, nom_1, nom_2, nom_3, nom_4. For high-cardinality nominal features, like nom_5, nom_6, nom_7, nom_8, nom_9.\n* **Timeseries data**: Time series data, like day or moth, it seems to be a cyclical continuous features."},{"metadata":{},"cell_type":"markdown","source":"### Which Encoding methods is suitable to deal with above categorical features?"},{"metadata":{},"cell_type":"markdown","source":"I pick up some methods from excellent notebooks that have published in this kaggle competition, and then try it as the following.\n\n* To binary data, I will do logical judgement.\n* To low ordial data, I will try LabelEncoder or simple replacement. But to high ordial data, I maybe use OrdinalEncoder.\n* To low nominal data, dummy variable is a common method. But to high nonmial data, I maybe use LeaveOneOutEncoder or FeatureHasher"},{"metadata":{},"cell_type":"markdown","source":"### Import packages and data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction import FeatureHasher\nfrom category_encoders import LeaveOneOutEncoder\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler, LabelEncoder, OrdinalEncoder\nfrom sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold, cross_val_score, RandomizedSearchCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, LogisticRegressionCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom mlxtend.classifier import StackingCVClassifier\nimport copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/cat-in-the-dat/train.csv\", index_col=['id'])\ntest = pd.read_csv(\"/kaggle/input/cat-in-the-dat/test.csv\", index_col=['id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(\"target\", axis = 1)\ny = train.loc[:,\"target\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding for catagory "},{"metadata":{},"cell_type":"markdown","source":"#### Binary data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.bin_3 = X.bin_3.apply(lambda x: 1 if x == \"T\" else 0)\nX.bin_4 = X.bin_4.apply(lambda x: 1 if x == \"Y\" else 0)\n\nprint(X.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Nominal data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# h = FeatureHasher(input_type='string', n_features=1000)\n# X[['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']].values\n# hash_X = h.fit_transform(X[['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']].values)\n# hash_X = pd.DataFrame(hash_X.toarray())\n\n# hash_X.columns\n# X = X.drop([\"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\"], axis=1).join(hash_X)\n\nloo_encoder = LeaveOneOutEncoder(cols=[\"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\"])\nloo_X = loo_encoder.fit_transform(X[[\"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\"]], y)\nX = X.drop([\"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\"], axis=1).join(loo_X)\n\nX = X.drop([\"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"], axis=1) \\\n        .join(pd.get_dummies(X[[\"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"]]))\n\nprint(X.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ordinal data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.ord_1.replace(to_replace = ['Novice', 'Contributor','Expert', 'Master', 'Grandmaster'],\n                         value = [0, 1, 2, 3, 4], inplace = True)\nX.ord_2.replace(to_replace = ['Freezing', 'Cold', 'Warm', 'Hot','Boiling Hot', 'Lava Hot'],\n                         value = [0, 1, 2, 3, 4, 5], inplace = True)\n\nfor i in [\"ord_3\", \"ord_4\"]:\n   le = LabelEncoder()\n   X[[i]] = le.fit_transform(X[[i]])\n\noe = OrdinalEncoder(categories='auto')\nX.ord_5 = oe.fit_transform(X.ord_5.values.reshape(-1,1))\n\nprint(X.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Timeseries data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def date_cyc_enc(df, col, max_vals):\n   df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_vals)\n   df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_vals)\n   return df\n\nX = date_cyc_enc(X, 'day', 7)\nX = date_cyc_enc(X, 'month', 12)\nX.drop(['day', 'month'], axis=1, inplace = True)\n\nprint(X.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model refinement\n\n#### Try it on linear model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr = LogisticRegression()\n# scores_lr = cross_val_score(lr, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_lr.mean(), scores_lr.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rc = RidgeClassifier()\n# scores_rc = cross_val_score(rc, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_rc.mean(), scores_rc.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lda = LinearDiscriminantAnalysis()\n# scores_lda = cross_val_score(lda, X_new, y, cv=5, n_jobs=1)\n# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_lda.mean(), scores_lda.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# linear_svm = LinearSVC(penalty=\"l2\")\n# scores_linear_svm = cross_val_score(linear_svm, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_linear_svm.mean(), scores_linear_svm.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LogisticRegression, RidgeClassifier and LinearDiscriminantAnalysis revel better accuracy than LinearSVC."},{"metadata":{},"cell_type":"markdown","source":"#### Try it on classification model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fr = DecisionTreeClassifier(random_state=0)\n# scores_dt = cross_val_score(fr, X, y, cv=5, n_jobs=2)\n# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_dt.mean(), scores_dt.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sgdc = SGDClassifier()\n# scores_sgdc = cross_val_score(sgdc, X, y, cv=5, n_jobs=4)\n# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_sgdc.mean(), scores_sgdc.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ab = AdaBoostClassifier()\n# scores_ab= cross_val_score(ab, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_ab.mean(), scores_ab.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gbm = GradientBoostingClassifier()\n# scores_gbm= cross_val_score(gbm, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_gbm.mean(), scores_gbm.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rf = RandomForestClassifier()\n# scores_rf= cross_val_score(rf, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_rf.mean(), scores_rf.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# et = ExtraTreesClassifier()\n# scores_et= cross_val_score(et, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_et.mean(), scores_et.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb = XGBClassifier()\n# scores_xgb= cross_val_score(xgb, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_xgb.mean(), scores_xgb.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that GradientBoostingClassifier, XGBClassifier maybe have the best accuracy in above methods, which the accuracy of AdaBoostClassifier is also close to. Otherwise, others maybe performent not good."},{"metadata":{},"cell_type":"markdown","source":"#### Grid Search for best params of XGBClassifier model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# params = {\n#         'min_child_weight': [1, 5, 10, 13, 15],\n#         'gamma': [0.5, 1, 1.5, 2, 5],\n#         'subsample': [0.2, 0.4, 0.6, 0.8, 1.0],\n#         'colsample_bytree': [0.6, 0.8, 1.0],\n#         'max_depth': [3, 4, 5, 10, 20]\n#         }\n# xgb = XGBClassifier(silent=True, nthread=1)\n# folds = 3\n# param_comb = 5\n\n# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\n# random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, \n#                                    scoring='accuracy', n_jobs=-1, cv=skf.split(X, y), \n#                                    verbose=3, random_state=1001 )\n# random_search.fit(X, y)\n\n# print('\\n All results:')\n# print(random_search.cv_results_)\n# print('\\n Best estimator:')\n# print(random_search.best_estimator_)\n# print('\\n Best hyperparameters:')\n# print(random_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# means = random_search.cv_results_['mean_test_score']\n# stds = random_search.cv_results_['std_test_score']\n# for mean, std, params in zip(means, stds, random_search.cv_results_['params']):\n#     print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Grid Search for best params of GradientBoostingClassifier model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# params2 = {\n#         'n_estimators': [50, 100, 300, 800],\n#         'learning_rate': [0.01, 0.1, 0.5, 1],\n#         'max_depth': [3, 10, 20, 50],\n#         'min_samples_split': [100, 200, 500, 800],\n#         'subsample': [0.2, 0.4, 0.6, 0.8, 1.0]\n#         }\n# gbm = GradientBoostingClassifier(random_state=1001)\n\n# random_search2 = RandomizedSearchCV(gbm, param_distributions=params2, n_iter=param_comb, \n#                                    scoring='accuracy', n_jobs=-1, cv=skf.split(X, y), \n#                                    verbose=3, random_state=1001 )\n# random_search2.fit(X, y)\n\n# print('\\n All results:')\n# print(random_search2.cv_results_)\n# print('\\n Best estimator:')\n# print(random_search2.best_estimator_)\n# print('\\n Best hyperparameters:')\n# print(random_search2.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# means = random_search2.cv_results_['mean_test_score']\n# stds = random_search2.cv_results_['std_test_score']\n# for mean, std, params in zip(means, stds, random_search2.cv_results_['params']):\n#     print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Grid Search for best params of LogisticRegression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr_params = {'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'], \n#              'C': [0.01, 0.1, 0.5, 1]\n#             }\n# lr = LogisticRegression(random_state=1001)\n\n# random_search3 = RandomizedSearchCV(lr, param_distributions=lr_params, n_iter=param_comb, \n#                                    scoring='accuracy', n_jobs=-1, cv=skf.split(X, y), \n#                                    verbose=3, random_state=1001 )\n# random_search3.fit(X, y)\n\n# print('\\n All results:')\n# print(random_search3.cv_results_)\n# print('\\n Best estimator:')\n# print(random_search3.best_estimator_)\n# print('\\n Best hyperparameters:')\n# print(random_search3.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# means = random_search3.cv_results_['mean_test_score']\n# stds = random_search3.cv_results_['std_test_score']\n# for mean, std, params in zip(means, stds, random_search3.cv_results_['params']):\n#     print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stacked model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_clf = XGBClassifier(booster='gbtree', gamma=5, colsample_bytree=0.8,\n#                         learning_rate=0.1, max_depth=10, \n#                         min_child_weight=10, n_estimators=100, \n#                         silent=True, subsample=0.8)\n\n# ab_clf = AdaBoostClassifier(n_estimators=200,\n#                             base_estimator=DecisionTreeClassifier(\n#                                 min_samples_leaf=2,\n#                                 random_state=1001),\n#                             random_state=1001)\n\n# gbm_clf = GradientBoostingClassifier(n_estimators=300, min_samples_split=100,\n#                                  max_depth=50, learning_rate=1, subsample=0.8,\n#                                  random_state=1001)\n\n# lr = LogisticRegression()\n\n# stack = StackingCVClassifier(classifiers=[xgb_clf, gbm_clf, ab_clf], \n#                             meta_classifier=lr,\n#                             cv=5,\n#                             stratify=True,\n#                             shuffle=True,\n#                             use_probas=True,\n#                             use_features_in_secondary=True,\n#                             verbose=1,\n#                             random_state=1001,\n#                             n_jobs=-1)\n# stack = stack.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions"},{"metadata":{},"cell_type":"markdown","source":"In fact, as above analysis, I found the stack model in test data is very bad !\n\nAt least, I found if I use LogisticRegression model only or other linear model, the result is better. **For example, if I preprocess all features as above steps and perform LR model with default parameters, the score of this competition is 0.80266**\n\nSo I decided to preprocess the train data again with one-hot-encoding, and build model of LR. The result is shown below."},{"metadata":{},"cell_type":"markdown","source":"#### Encoding for train/test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(\"target\", axis = 1)\ny_train = train.loc[:,\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.bin_3 = X_train.bin_3.apply(lambda x: 1 if x == \"T\" else 0)\nX_train.bin_4 = X_train.bin_4.apply(lambda x: 1 if x == \"Y\" else 0)\n\nX_train.ord_1.replace(to_replace = ['Novice', 'Contributor','Expert', 'Master', 'Grandmaster'],\n                         value = [0, 1, 2, 3, 4], inplace = True)\nX_train.ord_2.replace(to_replace = ['Freezing', 'Cold', 'Warm', 'Hot','Boiling Hot', 'Lava Hot'],\n                         value = [0, 1, 2, 3, 4, 5], inplace = True)\n\nfor i in [\"ord_3\", \"ord_4\"]:\n   le = LabelEncoder()\n   X_train[[i]] = le.fit_transform(X_train[[i]])\n\noe = OrdinalEncoder(categories='auto')\nX_train.ord_5 = oe.fit_transform(X_train.ord_5.values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = copy.deepcopy(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.bin_3 = X_test.bin_3.apply(lambda x: 1 if x == \"T\" else 0)\nX_test.bin_4 = X_test.bin_4.apply(lambda x: 1 if x == \"Y\" else 0)\n\nX_test.ord_1.replace(to_replace = ['Novice', 'Contributor','Expert', 'Master', 'Grandmaster'],\n                         value = [0, 1, 2, 3, 4], inplace = True)\nX_test.ord_2.replace(to_replace = ['Freezing', 'Cold', 'Warm', 'Hot','Boiling Hot', 'Lava Hot'],\n                         value = [0, 1, 2, 3, 4, 5], inplace = True)\n\nfor i in [\"ord_3\", \"ord_4\"]:\n    le = LabelEncoder()\n    X_test[[i]] = le.fit_transform(X_test[[i]])\n\noe = OrdinalEncoder(categories='auto')\nX_test.ord_5 = oe.fit_transform(X_test.ord_5.values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([X_train, X_test])\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = data.columns\ndummies = pd.get_dummies(data,\n                         columns=columns,\n#                          drop_first=True,\n                         sparse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dummies.shape)\nprint(X_train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = dummies.iloc[:X_train.shape[0], :]\nX_test = dummies.iloc[X_train.shape[0]:, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del dummies\ndel data\nprint (X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.sparse.to_coo().tocsr()\nX_test = X_test.sparse.to_coo().tocsr()\n\nX_train = X_train.astype(\"float32\")\nX_test = X_test.astype(\"float32\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Making predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\npred = lr.predict_proba(X_test)\npred[:10,1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">**In fact, as above preprocess, I first transform some features with some normal methods ,and then I preprocess all features with get_dummies function, the score of competition drops from 0.80266 to 0.80184.**\n**However, if I adjust LR model with tuning parameters, the score will be upgrade to 0.80801.**\n**But I think these three scores are keeping in same level, so those scores can not indicate that which preprocess or encoding way is better.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr = LogisticRegression(solver=\"lbfgs\", C=0.1, max_iter=10000)\n# lr.fit(X_train, y_train)\n# pred2 = lr.predict_proba(X_test)\n# pred2[:10,1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Export predictions and submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pd.Series(pred[:,1], index=test.index, dtype=y.dtype)\npredictions.to_csv(\"./submission.csv\", header=['target'], index=True, index_label='id')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}