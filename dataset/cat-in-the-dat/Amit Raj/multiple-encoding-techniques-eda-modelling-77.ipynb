{"cells":[{"metadata":{"id":"NPQt90yNRh79","_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"#================================================================#\n### Installing relevant libraries ###\n#================================================================#\n\n!pip install category_encoders","execution_count":null,"outputs":[]},{"metadata":{"id":"A8sQESwxRSiw","trusted":true},"cell_type":"code","source":"#================================================================#\n### Importing all the relevant libraries ###\n#================================================================#\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nimport category_encoders as ce\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, ShuffleSplit, cross_val_score\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nfrom sklearn.metrics import roc_curve, plot_roc_curve, auc, classification_report, accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom xgboost import plot_importance\n\nimport scipy.stats as stats\nfrom imblearn.over_sampling import RandomOverSampler\n\nfrom sklearn.ensemble import RandomForestClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"id":"EmPTqDuoRj13","outputId":"aac3156d-8a43-4351-fee6-c2b20feee816","trusted":true},"cell_type":"code","source":"#================================================================#\n### Reading Train Dataset ###\n#================================================================#\n\ndata = pd.read_csv('../input/cat-in-the-dat/train.csv')\n\nprint(data.shape)\n\ndata.head(2)","execution_count":null,"outputs":[]},{"metadata":{"id":"lajh4KBzSNY3","outputId":"0cafc5b0-58dd-4afa-e9bd-bea51e4f2410","trusted":true},"cell_type":"code","source":"#================================================================#\n### Train Dataset Distribution ###\n#================================================================#\n\ndata.describe(include='all')\n\n# From Nom 5 to Nom 9, we notice that the variables have high cardinality\n# Similar is the case with Ord 5","execution_count":null,"outputs":[]},{"metadata":{"id":"zpPETer-p7Ub","outputId":"c5f0a23d-6d70-449b-81da-a513936e75b9","trusted":true},"cell_type":"code","source":"#================================================================#\n### Reading Test Dataset ###\n#================================================================#\ntest = pd.read_csv('../input/cat-in-the-dat/test.csv')\n\nprint(test.shape)\n\ntest.head(2)","execution_count":null,"outputs":[]},{"metadata":{"id":"09fKgdOzqE1_","outputId":"81edbcf3-8c82-4d18-b8f4-01a9090a13fe","trusted":true},"cell_type":"code","source":"test.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"id":"bVYQsCapis8U"},"cell_type":"markdown","source":"# Exploratory Data analysis"},{"metadata":{"id":"8fY1ZMlfhXQn","outputId":"96b55ba8-ff0c-4e33-ee9c-03dbc8d93dba","trusted":true},"cell_type":"code","source":"#================================================================#\n### Distribution of Target Variable ###\n#================================================================#\n\ndata['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"83rWjGWXTBDa","outputId":"407e8778-105b-4f18-9570-030ff401ccc7","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Bins vs Target Variable ###\n#===========================================================================#\n\nfig, ax = plt.subplots(nrows=1, ncols=5, figsize=(20,5))\n\nsmallPlots('bin_0' , 'Bin 0', 0)\nsmallPlots('bin_1' , 'Bin 1', 1)\nsmallPlots('bin_2' , 'Bin 2', 2)\nsmallPlots('bin_3' , 'Bin 3', 3)\nsmallPlots('bin_4' , 'Bin 4', 4)\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"I2GkB36Rbw3L","outputId":"93c6fef2-93a5-4990-b901-d10a99c3598c","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Noms vs Target Variable ###\n#===========================================================================#\n\nfig, ax = plt.subplots(nrows=1, ncols=5, figsize=(25,5))\n\nsmallPlots('nom_0' , 'Nom 0', 0)\nsmallPlots('nom_1' , 'Nom 1', 1)\nsmallPlots('nom_2' , 'Nom 2', 2)\nsmallPlots('nom_3' , 'Nom 3', 3)\nsmallPlots('nom_4' , 'Nom 4', 4)\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"esryfcO8elQM","outputId":"a0a74f21-3f81-4c18-dd16-0b1fd9885242","trusted":true},"cell_type":"code","source":"#================================================================#\n### Nom 5's Pareto chart ###\n#================================================================#\n\n# Nom 5 had high cardinality, first logical check is to see if the data\n# distribution is skewed or not, if skewed then we can replace less frequent entries\n# In this case the data is distributed linearly i.e. all entries in Nom 5 have \n# almost equal # of IDs corresponding to them\n\nparetoPlots('nom_5', 'id')","execution_count":null,"outputs":[]},{"metadata":{"id":"dluY2rRNfWdE","outputId":"55248dfc-52d8-4a9c-d5c9-081e41e995ef","trusted":true},"cell_type":"code","source":"#================================================================#\n### Nom 6's Pareto chart ###\n#================================================================#\n\n# Nom 6 had high cardinality, first logical check is to see if the data\n# distribution is skewed or not, if skewed then we can replace less frequent entries\n# In this case the data is distributed linearly i.e. all entries in Nom 6 have \n# almost equal # of IDs corresponding to them\n\nparetoPlots('nom_6', 'id')","execution_count":null,"outputs":[]},{"metadata":{"id":"wn4TsqsYfb4p","outputId":"b079a288-bccc-405c-ccf3-fe82f8ecd199","trusted":true},"cell_type":"code","source":"#================================================================#\n### Nom 7's Pareto chart ###\n#================================================================#\n\n# Nom 7 had high cardinality, first logical check is to see if the data\n# distribution is skewed or not, if skewed then we can replace less frequent entries\n# In this case the data is distributed linearly i.e. all entries in Nom 7 have \n# almost equal # of IDs corresponding to them\n\nparetoPlots('nom_7', 'id')","execution_count":null,"outputs":[]},{"metadata":{"id":"YicjIptqffHS","outputId":"9c8294cc-b7a2-4ebf-dc43-adc09250bc9a","trusted":true},"cell_type":"code","source":"#================================================================#\n### Nom 8's Pareto chart ###\n#================================================================#\n\n# Nom 8 had high cardinality, first logical check is to see if the data\n# distribution is skewed or not, if skewed then we can replace less frequent entries\n# In this case the data is distributed linearly i.e. all entries in Nom 8 have \n# almost equal # of IDs corresponding to them\n\nparetoPlots('nom_8', 'id')","execution_count":null,"outputs":[]},{"metadata":{"id":"F0JCCL-Acdjx","outputId":"34c37705-8191-4159-a99c-a78900853e27","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Ord vs Target Variable ###\n#===========================================================================#\n\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n\nsmallPlots('ord_0' , 'Ord 0', 0)\nsmallPlots('ord_1' , 'Ord 1', 1)\nsmallPlots('ord_2' , 'Ord 2', 2)\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"XtoQahmgc-C8","outputId":"f23d526a-608d-4588-8cad-f613386f6659","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Ord vs Target Variable ###\n#===========================================================================#\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(15,10))\n\nsmallPlots('ord_3' , 'Ord 3', 0)\nsmallPlots('ord_4' , 'Ord 4', 1)\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"12jBjuVSf7pA","outputId":"4980d230-ed66-4705-ed25-8b3593342fce","trusted":true},"cell_type":"code","source":"#================================================================#\n### Ord 5's Pareto chart ###\n#================================================================#\n\n# Ord 5 had high cardinality, first logical check is to see if the data\n# distribution is skewed or not, if skewed then we can replace less frequent entries\n# In this case the data is distributed linearly i.e. all entries in Ord 5 have \n# almost equal # of IDs corresponding to them\n\nparetoPlots('ord_5', 'id')","execution_count":null,"outputs":[]},{"metadata":{"id":"z9ExSLPtgHQk","outputId":"ce021144-55cb-404b-f578-a9d93b6e9d4e","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Day and Month vs Target Variable ###\n#===========================================================================#\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(15,10))\n\nsmallPlots('day' , 'Day', 0)\nsmallPlots('month' , 'Month', 1)\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"7YqyJuaiixm2"},"cell_type":"markdown","source":"# Encoding\n\nThere are 3 approaches we can take:\n\n**Count Encoding:**\n\nIn count encoding the categorical value is replaced by it's frequency in the dataset i.e. say an entry - 'Green' in column nom_0 has a frequency of 100 then at every place in nom_0 the entry as 'Green' will be replaced by 100\n\nThis will be done throughout the columns specified i.e. the categorical entries will be replaced by it's count in the column\n\n**Target Encoding:**\n\nTarget encoding is similar to Count encoding but has the *Target Variable* involved. Say for column nom_5 the value green has a frequency of 100 and at a target level, it has the value of 60 for 1 and 40 for 0 then at all the places where the row has value nom_5 = 'Green' and 'Target' = 1, it'll replace it by 60 and for 'Target' = 0 it'll return 40\n\n**One Hot Encoding**\n\nOne Hot encoding is the most common and widely used criteria but it'll suffer in case of high cardinality, here we see *nom_9* has more than 11K distinct values, in that case OHE will make 11K columns which will lead to system crash\n\nOHE can be used in combination with either Count or Target Encoding, for high cardinal variables, we can use the above mentioned approaches while for the low cardinal variables, we can use OHE"},{"metadata":{"id":"KSh4zSFzkspU","outputId":"edab8da4-b8e0-45c7-cbf5-be5d01b95460","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Running Count encoding on Dataset ###\n#===========================================================================#\n\ndata_copy = data.copy()\n\ndata_copy = data_copy.astype('str')\n\n# Create the encoder\ncount_enc_all = ce.CountEncoder()\n\n# Transform the features\ncount_encoded_all = count_enc_all.fit_transform(data_copy.iloc[:, 1:-1])\ncount_encoded_all = count_encoded_all/count_encoded_all.shape[0]\n\ncount_test = count_enc_all.transform(test.iloc[:,1:])\n\nprint(count_encoded_all.shape)\nprint(count_test.shape)\n\ncount_encoded_all.head(3)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ias6T8lCW-9L","outputId":"7f196335-a94d-4582-e515-7673cc2afc51","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Running Target Encoding on Dataset ###\n#===========================================================================#\n\ndata_copy = data.copy()\n\ncat_features = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1',\n       'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9',\n       'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5', 'day', 'month']\n\n# Create the encoder\ntarget_enc = ce.TargetEncoder(cols=cat_features)\ntarget_enc.fit(data_copy[cat_features], data_copy['target'])\n\n# Transform the features, rename the columns with _target suffix, and join to dataframe\ntrain_TE = target_enc.transform(data_copy[cat_features])\ntest_TE = target_enc.transform(test[cat_features])\n\nprint(train_TE.shape)\n\ntrain_TE.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"9z8cBvwKiwou","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Running Count encoding and OHE on Dataset ###\n#===========================================================================#\n\ndata_copy = data.copy()\n\ndata_copy['ord_0'] = data_copy['ord_0'].astype('str')\ndata_copy['day'] = data_copy['day'].astype('str')\ndata_copy['month'] = data_copy['month'].astype('str')\n\ncol_list_cat = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_5']\n\ncol_list_oh = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1',\n       'nom_2', 'nom_3', 'nom_4','ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'day', 'month']\n\n\n# Create the encoder\ncount_enc = ce.CountEncoder()\n\n# Transform the features\ncount_encoded = count_enc.fit_transform(data_copy[col_list_cat])\ncount_encoded = count_encoded/count_encoded.shape[0]\n\noh_encoded = pd.get_dummies(data_copy[col_list_oh])\n\ndata_enc = pd.concat([oh_encoded, count_encoded, data_copy['target']], axis=1)\n\nprint(data_enc.shape)\n\ndata_enc.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Running Count encoding and OHE on Dataset ###\n#===========================================================================#\n\ntest_copy = test.copy()\n\ntest_copy['ord_0'] = test_copy['ord_0'].astype('str')\ntest_copy['day'] = test_copy['day'].astype('str')\ntest_copy['month'] = test_copy['month'].astype('str')\n\n# Transform the features\ncount_encoded_test = count_enc.transform(test_copy[col_list_cat])\ncount_encoded_test = count_encoded_test/count_encoded.shape[0]\n\noh_encoded_test = pd.get_dummies(test_copy[col_list_oh])\n\ndata_enc_test = pd.concat([oh_encoded_test, count_encoded_test], axis=1)\n\nprint(data_enc_test.shape)\n\ndata_enc_test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"id":"vB5FaY2HJQII","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Eleminating variables using Chisq ###\n#===========================================================================#\n\nchi2_output = chi2(oh_encoded, data_copy['target'])\ncol_1 = []\nfor i in range(0,len(chi2_output[1])):\n  if chi2_output[1][i] < 0.05:\n    col_1.append(oh_encoded.columns[i])\n\ncol_0 = []\nfor i in range(0,len(chi2_output[0])):\n  if chi2_output[0][i] < 0.05:\n    col_0.append(oh_encoded.columns[i])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"EGW07CZuRhGs","outputId":"8fec57f2-1461-4ebb-cb51-1db955186900","trusted":true},"cell_type":"code","source":"contigency_tab = pd.crosstab(data_enc['nom_0_Green'], data_enc['target']) \ncontigency_tab","execution_count":null,"outputs":[]},{"metadata":{"id":"LqYldfvSTNCC","outputId":"b2801dcc-69cd-4699-bbda-f0fb9a45c43a","trusted":true},"cell_type":"code","source":"c, p, dof, expected = stats.chi2_contingency(contigency_tab) \n\n# Print the p-value\nprint(p)\nprint(expected)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"oa2OpuIPkHF7"},"cell_type":"markdown","source":"# Model"},{"metadata":{"id":"Bua8qwbWYeeS"},"cell_type":"markdown","source":"## Target Encoded"},{"metadata":{"id":"TgKTjr3dudV1","outputId":"10a5ae91-864b-41c4-cd5b-2f18b5067366","trusted":false},"cell_type":"code","source":"# model_tuned, dict_model = model_tuning(train_TE, data['target'])","execution_count":null,"outputs":[]},{"metadata":{"id":"D5P-npwesMHg","outputId":"8049ac30-6b2d-4514-f385-aa4cce3023c1","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Training XG Boost Model ###\n#===========================================================================#\n\nmodelXgb_te = XGBClassifier(max_depth=12, n_estimators=400, reg_lambda=0.1, tree_method='gpu_hist')\n\nmodelXgb_te.fit(train_TE, data['target'])\n\npred_xgb_train = modelXgb_te.predict(train_TE)\n\nprint('### Train Summary ###', '\\n')\nprint(classification_report(data['target'], pred_xgb_train), '\\n')\nprint(confusion_matrix(data['target'], pred_xgb_train), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"id":"WZLdEY0Fs1JK","outputId":"727ef0f1-cea3-4bad-df4e-a05944e3d73f","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Test Prediction ### ~ 77%\n#===========================================================================#\n\npred_xgb_test = modelXgb_te.predict_proba(test_TE)\n\ntest_pred_xgb = pd.DataFrame(pred_xgb_test[:,1], columns=['target'])\n\ntest_xgb = test_pred_xgb.copy()\ntest_xgb['pred'] = np.where(test_xgb['target'] >=0.5,1,0)\ntest_xgb['pred'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"83IslU9et3uL","trusted":true},"cell_type":"code","source":"test_pred_te_xgb = pd.concat([test['id'], test_pred_xgb], axis=1)\ntest_pred_te_xgb.to_csv('pred_te_xgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"HfbJfRFKluHJ"},"cell_type":"markdown","source":"## Only Cat Encoded"},{"metadata":{"id":"PDhT-lbvlyQo","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Test Train Split ###\n#===========================================================================#\n\nX_train_ce, X_test_ce, y_train_ce, y_test_ce = train_test_split(count_encoded_all, data['target'], test_size=0.01, random_state=12)\n\nros = RandomOverSampler(random_state=0)\nX_resampled_ce, y_resampled_ce = ros.fit_resample(X_train_ce, y_train_ce)\n\nX_resampled_ce = pd.DataFrame(X_resampled_ce, columns= X_train_ce.columns)\nprint(X_resampled_ce.shape)\nX_resampled_ce.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Hjx2K5PtnHAu","outputId":"ee39764d-472f-428d-e47e-68a8cd3f7c68","trusted":false},"cell_type":"code","source":"# model_tuned, dict_model = model_tuning(count_encoded_all, data['target'])","execution_count":null,"outputs":[]},{"metadata":{"id":"RnRxWD_xGCKc","outputId":"2b8a3451-3af4-4609-9b11-2a3f7d8f043b","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Training XG Boost Model ###\n#===========================================================================#\n\nmodelXgb_ce = XGBClassifier(n_estimators=200, max_depth=12, reg_lambda=0.05, tree_method='gpu_hist')\n\nmodelXgb_ce.fit(X_resampled_ce[col_list_oh], y_resampled_ce)\n\npred_xgb_train_ce = modelXgb_ce.predict(X_train_ce[col_list_oh])\npred_xgb_test_ce = modelXgb_ce.predict(X_test_ce[col_list_oh])\npred_xgb_test_ce_f = modelXgb_ce.predict_proba(count_test[col_list_oh])\n\nprint('### Train Summary ###', '\\n')\nprint(classification_report(y_train_ce, pred_xgb_train_ce), '\\n')\nprint(confusion_matrix(y_train_ce, pred_xgb_train_ce), '\\n')\n\nprint('\\n', '### Test Summary ###', '\\n')\nprint(classification_report(y_test_ce, pred_xgb_test_ce), '\\n')\nprint(confusion_matrix(y_test_ce, pred_xgb_test_ce), '\\n')\n\n# print('\\n', '### Provider 2 Summary ###', '\\n')\n# print(classification_report(y_cat_enc_p2_broad_1, pred_xgb_test_p2_cat_broad_1), '\\n')\n# # print(confusion_matrix(y_cat_enc_p2_broad_1, pred_xgb_test_p2_cat_broad_1), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"id":"IWVv6i2GnKwA","outputId":"616a9989-5d7f-4332-b200-268f50c198a8","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Plotting an ROC curve to check performance ###\n#===========================================================================#\n\nfalse_positive_rate, true_positive_rate, _ = roc_curve(y_test_ce, pred_xgb_test_ce)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc\n\n# plot the curve\nplt.plot(false_positive_rate, true_positive_rate, \n    'b', label='AUC = %0.3f'% roc_auc)\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"bSHmejmBnTD6","outputId":"48e5fef1-4d63-4438-8926-2680146df30f","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Identifying Important Features ###\n#===========================================================================#\n\n_, ax = plt.subplots(figsize=(8, 8))\nplot_importance(modelXgb_ce, max_num_features=10, ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"OlvRW4XCEA-J","outputId":"52c268e7-fc7a-4be8-9e93-fa97eff98fa0","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Test Prediction ###\n#===========================================================================#\n\ntest_pred = pd.DataFrame(pred_xgb_test_ce_f[:,1], columns=['target'])\ntest_pred.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"RaE-W6L4Aqui","trusted":false},"cell_type":"code","source":"# test_pred_cat = pd.concat([test['id'], test_pred], axis=1)\n# test_pred_cat.to_csv('pred_cat_only.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"ZkEekBlxlyto"},"cell_type":"markdown","source":"## Cat and OH"},{"metadata":{"id":"y1LgdbvOmEFd","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Test Train Split ###\n#===========================================================================#\n\nX_train, X_test, y_train, y_test = train_test_split(data_enc.iloc[:,:-1],\n                                                    data_enc['target'], test_size=0.05, random_state=12)\n\n### Random Oversampling to Create a balanced Dataset ###\n\nros = RandomOverSampler(random_state=0)\nX_resampled, y_resampled = ros.fit_resample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"j_Pa6TNdoz4c","outputId":"3f4c04f2-a985-4918-8d37-0c6ce4802efe","trusted":false},"cell_type":"code","source":"# model_tuned, dict_model = model_tuning(data_enc.iloc[:,:-1], data_enc['target'])","execution_count":null,"outputs":[]},{"metadata":{"id":"Q5r0sQUEmAfS","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Defining and running the Model ###\n#===========================================================================#\n\nmodelXgb = XGBClassifier(n_estimators=300, max_depth=10, min_child_weight=1,tree_method='gpu_hist')\n\nmodelXgb.fit(X_resampled, y_resampled)\n\npred_xgb_train = modelXgb.predict(X_train)\npred_xgb_test = modelXgb.predict(X_test)\npred_xgb_test_f = modelXgb.predict_proba(data_enc_test) \n\nprint('### Train Summary ###', '\\n')\nprint(classification_report(y_train, pred_xgb_train), '\\n')\nprint(confusion_matrix(y_train, pred_xgb_train), '\\n')\n\nprint('\\n', '### Test Summary ###', '\\n')\nprint(classification_report(y_test, pred_xgb_test), '\\n')\nprint(confusion_matrix(y_test, pred_xgb_test), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"id":"2V230QHtrKU4","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Plotting an ROC curve to check performance ###\n#===========================================================================#\n\nfalse_positive_rate, true_positive_rate, _ = roc_curve(y_test, pred_xgb_test)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc\n\n# plot the curve\nplt.plot(false_positive_rate, true_positive_rate, \n    'b', label='AUC = %0.3f'% roc_auc)\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"9xIZUFWrrXoV","outputId":"e8208efd-77d0-4c9c-a4d8-70da68bce88d","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Identifying Important Features ###\n#===========================================================================#\n\n_, ax = plt.subplots(figsize=(8, 8))\nplot_importance(modelXgb, max_num_features=10, ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#===========================================================================#\n### Test Prediction ### 73%\n#===========================================================================#\n\ntest_pred_oh_ce = pd.DataFrame(pred_xgb_test_f[:,1], columns=['target'])\ntest_pred_oh_ce = pd.concat([test['id'], test_pred_oh_ce], axis=1)\ntest_pred_oh_ce.to_csv('test_xgb_oh_ce.csv', index=False)\n\ntest_xgb_oh_ce = test_pred_oh_ce.copy()\ntest_xgb_oh_ce['pred'] = np.where(test_xgb_oh_ce['target'] >=0.5,1,0)\nprint(test_xgb_oh_ce['pred'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"id":"AAMOIGxegsV8"},"cell_type":"markdown","source":"# Functions"},{"metadata":{"id":"Dj7tc6eXTj1h","trusted":true},"cell_type":"code","source":"#============================================================================#\n## Bar Plots\n#============================================================================#\n\ndef smallPlots(col, title, i):\n\n  plot_data = data.groupby([col, 'target']).agg({'id':'count'}).reset_index()\n\n  sns.barplot(x=plot_data[col],\n              y=plot_data['id'],\n              hue=plot_data['target'], ax= ax[i])\n  \n  ax[i].set_title(title)","execution_count":null,"outputs":[]},{"metadata":{"id":"SEESs9lPWs0n","trusted":true},"cell_type":"code","source":"#============================================================================#\n## Pareto Analysis: What % of Column == 80% or 90% of Users\n#============================================================================#\n\ndef paretoPlots(col, agg_col):\n\n  df_pareto = data.groupby([col]).agg({agg_col: 'count'}).reset_index()\n\n  df_pareto = df_pareto.sort_values(by=[agg_col], ascending=False, ignore_index=True)\n\n  df_pareto['dummy'] = 1\n\n  df_pareto['dummy_cum'] = df_pareto.groupby(['dummy'])['dummy'].apply(lambda x: x.cumsum())\n  df_pareto[agg_col+'_cumsum'] = df_pareto.groupby(['dummy'])[agg_col].apply(lambda x: x.cumsum())\n\n  df_pareto['dummay_cum_max'] = df_pareto['dummy_cum'].max()\n  df_pareto[agg_col+'_cumsum_max'] = df_pareto[agg_col+'_cumsum'].max()\n\n  df_pareto['dummay_ratio'] = df_pareto['dummy_cum']/df_pareto['dummay_cum_max'] * 100\n  df_pareto[agg_col+'_ratio'] = df_pareto[agg_col+'_cumsum']/df_pareto[agg_col+'_cumsum_max'] *100\n\n\n  fig, ax = plt.subplots(ncols = 1, nrows = 1, figsize = (10, 8))\n\n  ax.plot(df_pareto['dummay_ratio'], df_pareto[agg_col+'_ratio'])\n\n  plt.xlabel('% of ' + col)\n  plt.ylabel('% of ' + agg_col)\n\n  plt.show()\n  ax.set_title('Pareto Analysis for ' + col)\n  ax.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"cL4luL4YrnxT","trusted":true},"cell_type":"code","source":"#===========================================================================#\n### XG Boost Hyper Parameter Tuning ###\n#===========================================================================#\n\nparameters_xgb_1 = {\n 'max_depth':range(7, 11, 1),\n 'min_child_weight':range(0, 3, 1)\n}\n\nparameters_xgb_2 = {\n 'gamma':[i/5.0 for i in range(0,1)]\n}\n\nparameters_xgb_3 = {\n 'n_estimators':range(200,600,100)\n}\n\nparameters_xgb_4 = {\n 'reg_lambda':[0.0, 0.1, 0.2]\n}\n\nparameters_xgb_5 = {\n 'learning_rate':[0.01, 0.05, 0.1, 0.2]\n}\n\ndef gridSearchFunction(model, model_params, X, y):\n    %%time\n    grid_search_xgb = GridSearchCV(estimator = model,\n                           param_grid = model_params,\n                           scoring = 'accuracy',\n                           cv = 3,\n                           n_jobs = 4)\n    grid_search = grid_search_xgb.fit(X, y)\n    return grid_search\n\n\ndef model_tuning(X, y):\n    \"\"\"\n    Please Note: This function requires a lot of resources and time to run\n    \n    This functions is used to tune the hyperparameters for XG Boost.\n    It starts with min_depth and min_child_weight then moves to gamma, n_estimators,\n    reg_lambda and finally to learning_rate. The tuned parameters are finally stored\n    in dict_\n    \"\"\"\n    \n    param_array = [parameters_xgb_1, parameters_xgb_2, parameters_xgb_3,\n                   parameters_xgb_4, parameters_xgb_5]\n    \n    xgModel = XGBClassifier(tree_method='gpu_hist')    \n    dict_ = {}\n    \n    \n    for i in range(0,5):\n        model_params = param_array[i]\n        grid_search_op = gridSearchFunction(xgModel, model_params, X, y)\n        dict_.update(grid_search_op.best_params_)\n        print(dict_)\n        xgModel = XGBClassifier(**dict_)\n    \n    return xgModel, dict_","execution_count":null,"outputs":[]},{"metadata":{"id":"xEYmJDxkNYGh","trusted":true},"cell_type":"code","source":"def evaluate(y,y_hat,labels, title):\n  print(classification_report(y,y_hat))\n  cm = confusion_matrix(y,y_hat)\n  cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n  cmat = pd.DataFrame(cm)\n  cmat.columns = labels\n  cmat.set_index([pd.Index(labels)],inplace=True)\n  sns.heatmap(cmat,cmap=\"YlGnBu\", annot=True)\n  plt.title(title)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}