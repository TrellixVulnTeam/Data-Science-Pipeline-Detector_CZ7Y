{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn import base\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read the data\ndf_train=pd.read_csv('../input/cat-in-the-dat/train.csv', index_col='id')\ndf_test=pd.read_csv('../input/cat-in-the-dat/test.csv', index_col='id')\n\n# Remove rows with missing target, separate target from predictors\ny = df_train.target\ndf_train.drop(['target'], axis=1, inplace=True)\ndf_train.shape\n\n# Remove rows with missing target, separate target from predictors\ndf_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define function to calculate MAE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.svm import LinearSVC\nimport xgboost as xgb\n\n# function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    global model\n    #model = LinearSVC(random_state=0, tol=1e-5)\n    #RandomForestRegressor(n_estimators=100, random_state=0)\n    model = xgb.XGBClassifier(\n        learning_rate =0.1,\n        n_estimators=1000,\n        max_depth=5,\n        min_child_weight=1,\n        gamma=0,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        objective= 'binary:logistic')\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\n\ndef logistic(X,y):\n    X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.2)\n    lr=LogisticRegression()\n    lr.fit(X_train,y_train)\n    y_pre=lr.predict(X_test)\n    print('Accuracy : ',accuracy_score(y_test,y_pre))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train data set has got {} rows and {} columns'.format(df_train.shape[0],df_train.shape[1]))\nprint('test data set has got {} rows and {} columns'.format(df_test.shape[0],df_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Get all categorical columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# All categorical columns\nobject_cols = [col for col in df_train.columns if df_train[col].dtype == \"object\"]\nprint(object_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Divide categorical columns on ordinal (where order is important) and nominal**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[object_cols].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ordinal_col=['ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']\nnominal_col=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using custom order for ORD_1, ODR_2, ORD_3, ORD_4**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing categorical options of pandas\nfrom pandas.api.types import CategoricalDtype \n\n# seting the orders of our ordinal features\nord_1 = CategoricalDtype(categories=['Novice', 'Contributor','Expert', \n                                     'Master', 'Grandmaster'], ordered=True)\nord_2 = CategoricalDtype(categories=['Freezing', 'Cold', 'Warm', 'Hot',\n                                     'Boiling Hot', 'Lava Hot'], ordered=True)\nord_3 = CategoricalDtype(categories=['a', 'b', 'c', 'd', 'e', 'f', 'g',\n                                     'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'], ordered=True)\nord_4 = CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n                                     'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n                                     'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], ordered=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming ordinal Features\ndf_train.ord_1 = df_train.ord_1.astype(ord_1)\ndf_train.ord_2 = df_train.ord_2.astype(ord_2)\ndf_train.ord_3 = df_train.ord_3.astype(ord_3)\ndf_train.ord_4 = df_train.ord_4.astype(ord_4)\n\n# test dataset\ndf_test.ord_1 = df_test.ord_1.astype(ord_1)\ndf_test.ord_2 = df_test.ord_2.astype(ord_2)\ndf_test.ord_3 = df_test.ord_3.astype(ord_3)\ndf_test.ord_4 = df_test.ord_4.astype(ord_4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.ord_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Geting the codes of ordinal categoy's - train\ndf_train.ord_1 = df_train.ord_1.cat.codes\ndf_train.ord_2 = df_train.ord_2.cat.codes\ndf_train.ord_3 = df_train.ord_3.cat.codes\ndf_train.ord_4 = df_train.ord_4.cat.codes\n\n# Geting the codes of ordinal categoy's - test\ndf_test.ord_1 = df_test.ord_1.cat.codes\ndf_test.ord_2 = df_test.ord_2.cat.codes\ndf_test.ord_3 = df_test.ord_3.cat.codes\ndf_test.ord_4 = df_test.ord_4.cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[['ord_0', 'ord_1', 'ord_2', 'ord_3']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For ORD_5 use the Label encoding**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Apply label encoder \n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in ['ord_5']:\n    df_train[col] = label_encoder.fit_transform(df_train[col])\n    df_test[col] = label_encoder.transform(df_test[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LeaveOneOutEncoder for nominal features with high cardinality**"},{"metadata":{"trusted":true},"cell_type":"code","source":"high_card_feats = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n#df_train.drop(high_card_feats, axis=1, inplace=True)\n#df_test.drop(high_card_feats, axis=1, inplace=True)\nfrom category_encoders import  LeaveOneOutEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leaveOneOut_encoder = LeaveOneOutEncoder()\nfor col in high_card_feats:\n    df_train[col] = leaveOneOut_encoder.fit_transform(df_train[col], y)\n    df_test[col] = leaveOneOut_encoder.transform(df_test[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Working on binary Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dictionary to map the feature\nbin_dict = {'T':1, 'F':0, 'Y':1, 'N':0}\n# Maping the category values in our dict\ndf_train['bin_3'] = df_train['bin_3'].map(bin_dict)\ndf_train['bin_4'] = df_train['bin_4'].map(bin_dict)\ndf_test['bin_3'] = df_test['bin_3'].map(bin_dict)\ndf_test['bin_4'] = df_test['bin_4'].map(bin_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Nominal cols with low cardinality modify with One-hot encoding**"},{"metadata":{"trusted":true},"cell_type":"code","source":"low_card_feats = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']\ndf_train[low_card_feats].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\n#---------------------------------------------------\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_df_train = pd.DataFrame(OH_encoder.fit_transform(df_train[low_card_feats]))\nOH_df_test = pd.DataFrame(OH_encoder.transform(df_test[low_card_feats]))\n\n# One-hot encoding removed index; put it back\nOH_df_train.index = df_train.index\nOH_df_test.index = df_test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_df_train = df_train.drop(low_card_feats, axis=1)\nnum_df_test = df_test.drop(low_card_feats, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_df_train_fin = pd.concat([num_df_train, OH_df_train], axis=1)\nOH_df_test_fin = pd.concat([num_df_test, OH_df_test], axis=1)\n#---------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OH_df_train_fin.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Use special encoding for cyclic**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cyclic_columns_encode(columns, df):\n    for col in columns:\n        df[col+'_sin']=np.sin((2*np.pi*df[col])/max(df[col]))\n        df[col+'_cos']=np.cos((2*np.pi*df[col])/max(df[col]))\n    df=df.drop(columns,axis=1)\n    return df\n\ncolumns=['day','month']\n\nX_train=cyclic_columns_encode(columns, OH_df_train_fin)\nX_test=cyclic_columns_encode(columns, OH_df_test_fin)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.info())\nprint(X_test.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Validate model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic(X_train,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf=LogisticRegression()  # MODEL\nclf.fit(X_train, y)\npred=clf.predict_proba(X_test)[:,1]\npd.DataFrame({\"id\": X_test.index, \"target\": pred}).to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}