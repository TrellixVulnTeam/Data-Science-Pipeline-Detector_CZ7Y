{"cells":[{"metadata":{},"cell_type":"markdown","source":"Target encoding&mdash;as implemented in [contrib.scikit-learn.org/categorical-encoding](https://contrib.scikit-learn.org/categorical-encoding/targetencoder.html)&mdash;can prove powerful especially to encode high cardinality categorical features. This implementation assumes that the target is ordinal (which is the case here as it is a binary outcome, but for many multiclass classification that is often not the case).\n\nHere we use it for all features as a starting point, but many of those features might better contribute to the overall predictive power when encoded with alternative techniques.\n\nWe use *k-fold* to mitigate data leaks that would otherwise almost certainly lead to overfitting. Alternatively, we could split the train set, but given the small size of it, a resampling technique sounds preferable.\n\nNote: this is a modification of the notebook [kaggle.com/merckel/target-encoding](https://www.kaggle.com/merckel/target-encoding), where LightGBM is used in lieu of a logit method."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport category_encoders as ce\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/cat-in-the-dat/train.csv')\ntest = pd.read_csv('../input/cat-in-the-dat/test.csv')\n\ntrain.sort_index(inplace=True)\ntrain_y = train['target']\ntest_id = test['id']\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feat_to_encode = train.columns.tolist()\nsmoothing=50.0\n\noof = pd.DataFrame([])\nfor tr_idx, oof_idx in StratifiedKFold(\n    n_splits=5, random_state=1, shuffle=True).split(\n        train, train_y):\n    ce_target_encoder = ce.TargetEncoder(cols = cat_feat_to_encode, smoothing=smoothing)\n    ce_target_encoder.fit(train.iloc[tr_idx, :], train_y.iloc[tr_idx])\n    oof = oof.append(ce_target_encoder.transform(train.iloc[oof_idx, :]), ignore_index=False)\n\nce_target_encoder = ce.TargetEncoder(cols = cat_feat_to_encode, smoothing=smoothing)\nce_target_encoder.fit(train, train_y)\ntrain = oof.sort_index() \ntest = ce_target_encoder.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = lgb.train(\n    params={\n        'max_depth': 2, \n        'num_leaves': 150,\n        'reg_alpha': 0.6, \n        'reg_lambda': 0.6,\n        'objective': 'binary',\n        \"boosting_type\": \"gbdt\",\n        \"metric\": 'auc',\n        \"verbosity\": -1,\n        'random_state': 1},\n    train_set=lgb.Dataset(train, label=train_y),\n    num_boost_round=700)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from datetime import datetime\npd.DataFrame({'id': test_id, 'target': clf.predict(test)}).to_csv(\n    'sub_' + str(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')) + '.csv', \n    index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}