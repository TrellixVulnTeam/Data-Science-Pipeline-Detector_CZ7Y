{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Categorical Feature Encoding Introduction\n\nA common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured. Because this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:\n\n* binary features\n* low- and high-cardinality nominal features\n* low- and high-cardinality ordinal features\n* (potentially) cyclical features"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.kdnuggets.com/wp-content/uploads/woman-yelling-cat-data-science-business.jpg)\n"},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use(\"ggplot\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read the data\n\ntrain_df = pd.read_csv(\"../input/cat-in-the-dat/train.csv\")\ntest_df = pd.read_csv(\"../input/cat-in-the-dat/test.csv\")\nsubmission_df = pd.read_csv(\"../input/cat-in-the-dat/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of observations in the train data: \", train_df.shape[0])\nprint(\"Number of observations in the train data: \", test_df.shape[0])\nprint(\"Number of columns in the train data: \", train_df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://pbs.twimg.com/media/EJ9uzwgUYAINWfK?format=jpg&name=medium\" alt=\"Drawing\" style=\"width: 700px;\"/>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking for missing values\n\ntrain_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#basic stats of the data\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#basic stats of the string data\n\ntrain_df.describe(include = \"object\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper Functions to plot the Numerical and Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"def NumericalVariables_targetPlots(df,segment_by,target_var = \"Attrition\"):\n    \"\"\"A function for plotting the distribution of numerical variables and its effect on attrition\"\"\"\n    \n    fig, ax = plt.subplots(ncols= 2, figsize = (14,6))    \n\n    #boxplot for comparison\n    sns.countplot(x = segment_by, hue = target_var, data=df, ax=ax[0])\n    ax[0].set_title(\"Comparision of \" + segment_by + \" vs \" + target_var)\n    \n    #distribution plot\n    ax[1].set_title(\"Distribution of \"+segment_by)\n    ax[1].set_ylabel(\"Frequency\")\n    sns.distplot(a = df[segment_by], ax=ax[1], kde=False)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def CategoricalVariables_targetPlots(df, segment_by,invert_axis = False, target_var = \"target\"):\n    \n    \"\"\"A function for Plotting the effect of variables(categorical data) on attrition \"\"\"\n    \n    fig, ax = plt.subplots(ncols= 2, figsize = (14,6))\n    \n    #countplot for distribution along with target variable\n    #invert axis variable helps to inter change the axis so that names of categories doesn't overlap\n    if invert_axis == False:\n        sns.countplot(x = segment_by, data=df,hue=\"target\",ax=ax[0])\n    else:\n        sns.countplot(y = segment_by, data=df,hue=\"target\",ax=ax[0])\n        \n    ax[0].set_title(\"Comparision of \" + segment_by + \" vs \" + \"Target\")\n    \n    #plot the effect of variable on attrition\n    if invert_axis == False:\n        sns.barplot(x = segment_by, y = target_var ,data=df,ci=None)\n    else:\n        sns.barplot(y = segment_by, x = target_var ,data=df,ci=None)\n        \n    ax[1].set_title(\"Target rate by {}\".format(segment_by))\n    ax[1].set_ylabel(\"Relative Target Representation\")\n    plt.tight_layout()\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Binary Variables - Numerical & Categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"bin_0\"\n\nNumericalVariables_targetPlots(train_df, \"bin_0\", \"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"bin_1\"\n\nNumericalVariables_targetPlots(train_df, \"bin_1\", \"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"bin_2\"\n\nNumericalVariables_targetPlots(train_df, \"bin_2\", \"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"bin_3\"\n\nCategoricalVariables_targetPlots(train_df, \"bin_3\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"bin_4\"\n\nCategoricalVariables_targetPlots(train_df, \"bin_4\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Nominal Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"nom_0\"\n\nCategoricalVariables_targetPlots(train_df, \"nom_0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"nom_1\"\n\nCategoricalVariables_targetPlots(train_df, \"nom_1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"nom_2\"\n\nCategoricalVariables_targetPlots(train_df, \"nom_2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"nom_3\"\n\nCategoricalVariables_targetPlots(train_df, \"nom_3\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"nom_4\"\n\nCategoricalVariables_targetPlots(train_df, \"nom_4\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ordinal Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#changing the color scheme\nplt.style.use(\"fivethirtyeight\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://matplotlib.org/3.1.0/_images/sphx_glr_style_sheets_reference_008.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n<img src=\"https://matplotlib.org/3.1.0/_images/sphx_glr_style_sheets_reference_009.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"ord_0\"\nNumericalVariables_targetPlots(train_df, \"ord_0\", \"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"ord_1\"\nCategoricalVariables_targetPlots(train_df,\"ord_1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"ord_2\"\nCategoricalVariables_targetPlots(train_df,\"ord_2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"day\"\n\nNumericalVariables_targetPlots(train_df, \"day\", \"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing the variable \"day\"\n\nNumericalVariables_targetPlots(train_df, \"month\", \"target\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(\"seaborn\")\ntrain_df.target.value_counts(normalize = True).plot(kind = \"barh\")\nplt.title(\"Distribution of Target Variable\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The Target variable is imbalanced in the ratio of 7:3. We can use any of the sampling techniques like Undersampling, oversampling and SMOTE to handle the imbalance data."},{"metadata":{},"cell_type":"markdown","source":"**Data Analysis Observations:**\n* `bin_3`and `bin_4` are binary variables representing T/F and Y/N. We can convert them to 0 or 1.\n* `nom_0` to `nom_4` are not ordinal variables. We can create dummy variables for these columns using one hot encoding.\n* `nom_5` to `nom_9` has high cardinaty in the variables. \n* `ord_1` and `ord_2` has ordinal data. We can manually encode these variables.\n* `ord_3` to `ord_5` encode using Label encoder.\n* `day` and `month` encode using sin and cosine values as they are cyclic in nature."},{"metadata":{},"cell_type":"markdown","source":"# Feature Encoding Techniques"},{"metadata":{},"cell_type":"markdown","source":"## Binary Encoding \n- Manually converting T/F --> 1/0"},{"metadata":{"trusted":true},"cell_type":"code","source":"# changing the binary values T/F to 1/0 and Y/N to 1/0.\n\ntrain_df[\"bin_3\"] = train_df[\"bin_3\"].apply(lambda x: 0 if x == 'F' else 1)\ntrain_df[\"bin_4\"] = train_df[\"bin_4\"].apply(lambda x: 0 if x == 'N' else 1)\n\n#test data\ntest_df[\"bin_3\"] = test_df[\"bin_3\"].apply(lambda x: 0 if x == \"F\" else 1)\ntest_df[\"bin_4\"] = test_df[\"bin_4\"].apply(lambda x: 0 if x == \"N\" else 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One Hot Encoding - Nominal Features\n- Converting columns that contain numbers of no specific order of preference. The data in the column usually denotes a category or value of the category and also when the data in the column is label encoded. \n- This confuses the machine learning model, to avoid this the data in the column should be One Hot encoded.\n- Get k-1 dummies out of k categorical levels by removing the first level."},{"metadata":{"trusted":true},"cell_type":"code","source":"nominal_cat_var = [\"nom_\" + str(i) for i in range(0,5)]\n\ntrain_temp_df = pd.get_dummies(train_df, columns = nominal_cat_var, drop_first = True)\ntest_temp_df = pd.get_dummies(test_df, columns = nominal_cat_var, drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of columns in the train data after one hot encoding: \", train_temp_df.shape[1])\nprint(\"Number of columns in the test data after one hot encoding: \", test_temp_df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ordinal Encoding\n- OrdinalEncoder converts each string value to a whole number. The first unique value in your column becomes 1, the second becomes 2, the third becomes 3, and so on."},{"metadata":{"trusted":true},"cell_type":"code","source":"#for ordinal variables - ord_1 and ord_2. we will manually replace the columns with integer values\n\nord1_mapping = {'Grandmaster': 5, 'Expert': 4 , 'Novice':1 , 'Contributor':2 , 'Master': 3}\nord2_mapping = {'Cold': 2, 'Hot':4, 'Lava Hot': 6, 'Boiling Hot': 5, 'Freezing': 1, 'Warm': 3}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_temp_df[\"ord_1\"] = train_temp_df[\"ord_1\"].map(ord1_mapping)\ntrain_temp_df[\"ord_2\"] = train_temp_df[\"ord_2\"].map(ord2_mapping)\n\ntest_temp_df[\"ord_1\"] = test_temp_df[\"ord_1\"].map(ord1_mapping)\ntest_temp_df[\"ord_2\"] = test_temp_df[\"ord_2\"].map(ord2_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting \"ord_3\" and \"ord_4\" as category type and getting the category codes.\n\nfor col in [\"ord_3\", \"ord_4\"]:\n    train_temp_df[col] = train_temp_df[col].astype('category')\n    ord_map = dict( zip(train_temp_df[col], train_temp_df[col].cat.codes))\n    train_temp_df[col] = train_temp_df[col].map(ord_map)\n    test_temp_df[col] = test_temp_df[col].map(ord_map)\n    train_temp_df[col] = train_temp_df[col].astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ordinal - High Cardinality Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode 'ord_5' using ACSII values\n# Source:- https://www.kaggle.com/c/cat-in-the-dat/discussion/105702#latest-607652\n\n# # Option 1: Add up the indices of two letters in string.ascii_letters\ntrain_temp_df['ord_5_oe_add'] = train_temp_df['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\ntest_temp_df['ord_5_oe_add'] = test_temp_df['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\n\ntrain_temp_df.drop('ord_5', axis=1, inplace=True)\ntest_temp_df.drop('ord_5', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Nominal - High Cardinality Variables"},{"metadata":{},"cell_type":"markdown","source":"### Feature hashing (a.k.a the hashing trick)\n\n- Feature hashing is a very cool technique to represent categories in a “one hot encoding style” as a sparse matrix but with a much lower dimensions. In feature hashing we apply a hashing function to the category and then represent it by its indices.\n\n- [Using categorical data in machine learning with python](https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512)\n- [Don’t be tricked by the Hashing Trick](https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087)"},{"metadata":{"trusted":true},"cell_type":"code","source":"nominal_highcat_var  = [\"nom_\" + str(i) for i in range(5,10)]\nnominal_highcat_var","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders  as ce\nfrom sklearn.feature_extraction import FeatureHasher\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I got this solution from @kabure and @Giba\n\nfor col in nominal_highcat_var:\n    train_temp_df[f'hash_{col}'] = train_temp_df[col].apply( lambda x: hash(str(x)) % 5000 )\n    test_temp_df[f'hash_{col}'] = test_temp_df[col].apply( lambda x: hash(str(x)) % 5000 )  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the variables after transformation\n   \ntrain_temp_df.drop(['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'], axis=1, inplace=True)\ntest_temp_df.drop(['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_temp_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_temp_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_temp_df.drop(['id', 'target'],axis = 1)\ny_train = train_temp_df['target']\nX_test = test_temp_df.drop(['id'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Input training dimension:', X_train.shape)\nprint('Test data dimension:', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling & Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import roc_auc_score,roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#declare hyperparameters dictionary\n\npipelines = {\n    'logistic' : make_pipeline(LogisticRegression(random_state = 123)),\n    'decisiontree' : make_pipeline(DecisionTreeClassifier(random_state = 123)),\n    'randomforest': make_pipeline(RandomForestClassifier(random_state = 123)),\n    'adaboost': make_pipeline(AdaBoostClassifier(random_state = 123))\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the all possible parameters for a model\n\n#pipelines[\"adaboost\"].get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistic hyperparameters\nlogistic_hyperparameters = {\n    'logisticregression__C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n    'logisticregression__penalty' : ['l1', 'l2']\n}\n\n#ada boost hyperparameters\nab_hyperparameters = {\n    'adaboostclassifier__n_estimators' : [100, 200, 400, 600, 800],\n    'adaboostclassifier__learning_rate' : [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1],\n    'adaboostclassifier__algorithm': ['SAMME', 'SAMME.R']\n}\n\ndecisiontree_hyperparameters = {\n    \"decisiontreeclassifier__max_depth\": np.arange(3,12),\n    \"decisiontreeclassifier__max_features\": np.arange(3,10),\n    \"decisiontreeclassifier__min_samples_split\": [2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n    \"decisiontreeclassifier__min_samples_leaf\" : np.arange(1,3)\n}\n\n#random forest hyperparameters\n\nrf_hyperparameters = {\n    'randomforestclassifier__n_estimators' : [100, 200, 400, 600, 800],\n    'randomforestclassifier__max_features' : ['auto', 'sqrt', 'log2'],\n    'randomforestclassifier__max_depth' : [int(x) for x in np.linspace(3, 10, num = 1)],\n    'randomforestclassifier__min_samples_split' : np.arange(2, 10)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperparameters = {\n    'adaboost' : ab_hyperparameters,\n    'randomforest' : rf_hyperparameters,\n    'logistic' : logistic_hyperparameters,\n    'decisiontree' : decisiontree_hyperparameters\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit and Tune Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"k = StratifiedKFold(n_splits=3, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fitted_models = {}\n\nfor name, pipeline in pipelines.items():\n    print(\"------- \", name, ' ---------')\n    #create a cross validation object from pipelines and hyperparameters\n    model = GridSearchCV(pipeline, hyperparameters[name], cv = k , n_jobs=-1,return_train_score=True, verbose = 2,scoring=\"roc_auc\")\n    \n    #fit model on X train and y train\n    model.fit(X_train, y_train)\n    \n    fitted_models[name] = model\n    \n    # Print '{name} has been fitted'\n    print(name, 'has been fitted.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Here we are evaluating based on model roc score\n\nresults, names  = [], [] \nbest_estimator_dict = {}\n\nfor name,model_built in fitted_models.items():\n    names.append(name)\n    results.append(np.round(model_built.best_score_,4))\n    print(\"Mean AUC Score of \"+ name + \" :\", np.round(model_built.cv_results_[\"mean_train_score\"].mean(),4))\n    print(\"Best AUC Score of \"+ name + \" :\", np.round(model_built.best_score_,4))\n    best_estimator_dict[model_built] = np.round(model_built.best_score_,4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_importance = pd.Series(results, names)\nroc_auc_importance.plot(kind='barh', cmap = \"viridis\")\nplt.title(\"Mean AUC_ROC Score Based Cross Validation for Different Models\")\nplt.xlabel(\"AUC ROC Score\")\nplt.ylabel(\"Model Name\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding the best model based on AUC ROC Score\n\nbest_fittedobject = max(best_estimator_dict, key=best_estimator_dict.get)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the best model from the best fittedobject\n\nbest_model = best_fittedobject.best_estimator_.steps[0][1]\nbest_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best model params\n\nbest_fittedobject.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot roc curve\n\ndef plot_roc( actual, probs ):\n    fpr, tpr, thresholds = roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = roc_auc_score( actual, probs )\n    plt.figure(figsize=(7, 7))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic plot')\n    plt.legend(loc=\"lower right\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#AUC ROC Curve on Training Data based on \n\nplot_roc(y_train, best_model.predict_proba(X_train)[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#plot the feature importance\n\ntry:\n    feat_importances = pd.Series(best_model.feature_importances_, index=X_train.columns)\n    feat_importances.nlargest(5).plot(kind='barh')\n    plt.title(\"Feature Importance\")\n    plt.xlabel(\"Relative Importance\")\n    plt.ylabel(\"Variable Name\")\n    plt.show()\nexcept:\n    print(\"Best Model doesn't support Feature Importance Plot (Logisitc Regression)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_fittedobject.cv_results_[\"mean_train_score\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions\n\ny_preds = best_model.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#appending the predictions to submission data\n\nsubmission_df[\"target\"] = y_preds\nsubmission_df.to_csv('best_submission.csv',header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Upvote the Kernel if you liked it. "},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://pbs.twimg.com/media/EJ9LyBZXUAAhk0-?format=jpg&name=900x900\" alt=\"Drawing\" style=\"width: 500px;\"/>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}