{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"border:2px solid purple; border-radius:10px; color:white; background-color:green\"><center> Tabular Playground Series(feb) </center></h2>\n\nTable Playground Series are beginner friendly monthly competitions organised by kaggle.<br/>\n    \nIn this competition we have to make a regrssion model based on categorical and continous features provided<br/>\n\nThis notebook is beginner friendly guide for creating supercool EDA and making baseline model.\n    \n**Feel free to ask any question and if you find any mistake tell me in the comments**"},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries üìó."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datatable as dt\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.manifold import TSNE\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport lightgbm as lgb\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\n\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.preprocessing import (StandardScaler,PowerTransformer,\n                                   QuantileTransformer,LabelEncoder, \n                                   OneHotEncoder, OrdinalEncoder,\n                                  RobustScaler)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:yellow\"><center> 1. Given Data üíΩ </center></h2>\n\nwe are provided with 3 files.<br/>\n1. sample_submission.csv\n2. test.csv\n3. train.csv\n\nTrain data contains 10 categorical featues and 14 continous features<br/>\nwe have to make prediction on test data and make submission using sample_submission's format<br/>"},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:yellow\"><center> 2. Metric üìê</center></h2>\n\nHere metric used is very simple which is root mean squared error.<br/>\n\nrmse = root(1/n (yit-yip)^2)\n\nHere yit is true target values<br/>\nand yip is predicted target values.<br/>"},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:yellow\"><center> 3. Loading Data üíΩ</center></h2>\n\nWe will use pandas to load data<br/>\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"folder_path = '../input/tabular-playground-series-feb-2021'\ntrain_data = pd.read_csv(f'{folder_path}/train.csv')\ntest_data = pd.read_csv(f'{folder_path}/test.csv')\nsample = pd.read_csv(f'{folder_path}/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"{0}Number of rows in train data: {1}{2}\\n{0}Number of columns in train data: {1}{3}\".format(y_,r_,train_data.shape[0],train_data.shape[1]))\nprint(\"{0}Number of rows in test data: {1}{2}\\n{0}Number of columns in test data: {1}{3}\".format(m_,r_,test_data.shape[0],test_data.shape[1]))\nprint(\"{0}Number of rows in sample : {1}{2}\\n{0}Number of columns in sample : {1}{3}\".format(c_,r_,sample.shape[0],sample.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:yellow\"><center> 4. Exploratory Data Analysis üìäüìàüìâüëÄ</center></h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_features = [f'cont{i}' for i in range(14)]\ncat_features = [f'cat{i}' for i in range(10)]\nall_features = cont_features + cat_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.1 Checking for Null Values\n</center></h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center>4.2 Distribution of cont0</center></h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\ndef distribution1(feature,color1,color2,df=train_data):\n    plt.figure(figsize=(15,7))\n    \n    plt.subplot(121)\n    dist = sns.distplot(df[feature],color=color1)\n    a = dist.patches\n    xy = [(a[i].get_x() + a[i].get_width() / 2,a[i].get_height()) \\\n          for i in range(1,len(a)-1) if (a[i].get_height() > a[i-1].get_height() and a[i].get_height() > a[i+1].get_height())]\n    \n    for i,j in xy:\n        dist.annotate(\n            s=f\"{i:.3f}\",\n            xy=(i,j), \n            xycoords='data',\n            ha='center', \n            va='center', \n            fontsize=11, \n            color='black',\n            xytext=(0,7), \n            textcoords='offset points',\n        )\n    \n    qnt = df[feature].quantile([.25, .5, .75]).reset_index(level=0).to_numpy()\n    plt.subplot(122)\n    box = sns.boxplot(df[feature],color=color2)\n    for i,j in qnt:\n        box.annotate(str(j)[:4],xy= (j-.05,-0.01),horizontalalignment='center')\n        \n    print(\"{}Max value of {} is: {} {:.2f} \\n{}Min value of {} is: {} {:.2f}\\n{}Mean of {} is: {}{:.2f}\\n{}Standard Deviation of {} is:{}{:.2f}\"\\\n      .format(y_,feature,r_,df[feature].max(),g_,feature,r_,df[feature].min(),b_,feature,r_,df[feature].mean(),m_,feature,r_,df[feature].std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution1('cont0','yellow','red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.3 Distribution of targets</center></h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution1('target','red','blue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target ranges from 0 to 10.31<br/>\nbut distribution really starts from target values >4<br/>\n\nWe can see 2 peaks in target distribution<br/>\nOne may assume that data is taken from 2 different distribution and then combined.<br/>\nso we are seeing two peaks<br/>\n\nBut judging from TPS(jan) actually target is from same distribution but some values of targets<br/>\nbelong to test data so we will most probably see the distribution of predictions with one peak around value 7<br/>"},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.3 Distribution of all the continous features. </center></h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\ncolors = ['#8ECAE6','#219EBC','#023047',\n          '#023047','#023047','#0E402D',\n          '#023047','#023047','#F77F00',\n          '#D62828','#4285F4','#EA4335',\n          '#FBBC05','#34A853']\nfor i,feature in enumerate(cont_features):\n    plt.subplot(2,7,i+1)\n    sns.distplot(train_data[feature],color=colors[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear that all the features have different disbtributions and none of them looks like normal distribution<br/>\n\ncont1 is very interesting as there are gaps in cont1 as if it is partly continous and partly categorical<br/>\n\nMaybe we should look into correlations between this features to get better insight"},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.4 Correlation Matrix</center></h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"corr = train_data[all_features+['target']].corr()\nfig = px.imshow(corr)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that there is some correlation between      featues cont5 to cont12 but they are not very high.\n    \nOne interesting thing is no features have any    correlation with target"},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center>4.5 Pairplot of features</center></h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_grid(data,color1,color2,color3):\n    f = sns.PairGrid(data);\n    plt.figure(figsize=(10,10));\n    f.map_upper(plt.scatter,color = color1);\n    f.map_lower(sns.kdeplot,color = color2);\n    #f.map_diag(sns.histplot,color = c3 );\n    f.map_diag(sns.kdeplot, lw=3, legend=False,color = color3);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_grid(train_data.loc[:1000,all_features+['target']],'#EA4335','#FBBC05','#34A853');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pair plot gives very good idea about relationship between the features<br/>\nall the features have a square relationship means for every value in one feature<br/>\nthere are enough values in entire range of other feature which makes this data little challanging<br/>"},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.7 Let's play with PCA</center></h3>"},{"metadata":{},"cell_type":"markdown","source":"<h4 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:lightblue\"><center> 4.7.1 How good are 4 components of pca at seprating data</center></h4>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def pca_plot1(features,n_components,target,nrows=10**4):\n    pca = PCA(n_components=n_components)\n    train_d = train_data.sample(n=nrows).fillna(train_data.mean())\n    train_g_pca = pca.fit_transform(train_d[features])\n\n    total_var = pca.explained_variance_ratio_.sum()*100\n    labels = {str(i): f\"PC {i+1}\" for i in range(n_components)}\n\n    fig = px.scatter_matrix(\n        train_g_pca,\n        dimensions=range(n_components),\n        labels=labels,\n        title=f\"Total explained variance ratio{total_var:.2f}%\",\n        color=train_d[target].values\n    )\n\n    fig.update_traces(diagonal_visible=True,opacity=0.5)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_plot1(cont_features,4,'target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:lightblue\"><center> 4.7.2 How good are 3 components of pca at seprating data (3d plot)</center></h4>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def pca_plot_3d(features,target,nrows=10**4):\n    pca = PCA(n_components=3)\n    train_d = train_data.sample(n=nrows).fillna(train_data.mean())\n    train_g_pca = pca.fit_transform(train_d[features])\n\n    total_var = pca.explained_variance_ratio_.sum()*100\n\n    fig = px.scatter_3d(\n        train_g_pca,x=0,y=1,z=2,\n        title=f\"Total explained variance ratio{total_var:.2f}%\",\n        color=train_d[target].values,\n        labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n    )\n\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_plot_3d(cont_features,'target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:lightblue\"><center> 4.7.3 Ploting explained variance </center></h4>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_exp_var(features,nrows=10**4):\n    pca = PCA()\n    train_d = train_data.sample(n=nrows).fillna(train_data.mean())\n    pca.fit(train_d[features])\n    exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n\n    fig = px.area(\n        x=range(1, exp_var_cumul.shape[0] + 1),\n        y=exp_var_cumul,\n        labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"},\n    )\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_exp_var(cont_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:lightblue\"><center> 4.8 Countplot of all categorical fearues </center></h4>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.style.use(\"ggplot\")\nplt.figure(figsize=(25,20))\nfor i,feature in enumerate(cat_features):\n    plt.subplot(2,5,i+1)\n    sns.countplot(train_data[feature])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear that distribution of categories are evry skewed<br/>\nIt would be good to remove some of the cat features like<br/>\ncat4,cat0, cat2, cat6, cat7, <br/>\nThe reason for this is that almost all rows will have same values for these columns<br/>\nso it will not provide any usefull information to model."},{"metadata":{},"cell_type":"markdown","source":" <h2 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.9 cont0 distribution based on cat9</center></h2>\nAs cat9 has most categories let us see distribution of one feature based on cat9."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def distribution3(feature,category,df=train_data):\n    plt.subplots(figsize=(15, 7))\n    sns.histplot(train_data,x=feature,hue=category)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution3('cont0','cat9')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.10 cont0 box-plot based on cat9</center></h3>\nI think box-plot will give use better idea than histplot"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def boxploting1(feature,category,df=train_data,figure_size=(15,7)):\n    plt.subplots(figsize=figure_size)\n    sns.boxplot(x=feature, y=category, data=df,whis=[0, 100], width=.6, palette=\"vlag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxploting1('cont0','cat9')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.11 Boxenplot for cont1 and cat8\n </center></h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def boxploting2(feature,category,df=train_data,figure_size=(15,7)):\n    plt.subplots(figsize=figure_size)\n    sns.boxenplot(y=feature, x=category,color=\"pink\",scale=\"linear\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxploting2('cont0','cat8',figure_size=(10,7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.12 Trivariate histogram with 2 category</center></h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def distribution3(feature,category1,category2,df=train_data,figure_size=(15,15)):\n    sns.set_theme(style=\"dark\")\n    sns.displot(\n        data=df, x=feature, y=category1, col=category2,\n        log_scale=(True, False), col_wrap=4, height=4, aspect=.7,\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution3('cont5','cat8','cat9')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.13 swarmplot of target</center></h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def swarmplot(feature,category1,category2,df=train_data,figure_size=(15,7)):\n    sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n    plt.figure(figsize=figure_size)\n    ax = sns.swarmplot(data=df, x=feature, y=category1, hue=category2)\n    ax.set(ylabel=\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"swarmplot('target','cat8','cat9',df=train_data.sample(n=10000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center>4.13 Trend lines and templates plotly</center></h3>\nNow I will use plotly for plotting"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def scatterplot1(feature1,feature2,category,df=train_data):\n    fig = px.scatter(train_data, x=feature1, y=feature2, color=category, marginal_y=\"violin\",\n               marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatterplot1('cont6','cont7','cat1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.14 Error bar </center></h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def errorbar(feature1,feature2,feature3,category,df=train_data):\n    df['e'] = df[feature3]/100\n    fig = px.scatter(df, x=feature1, y=feature2, color=category, error_x=\"e\", error_y=\"e\")\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"errorbar('cont1','cont2','cont3','cat9',df=train_data.sample(n=1000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.15 parellel categories</center></h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def parallelcat(feature,df=train_data):\n    fig = px.parallel_categories(df, color=feature, color_continuous_scale=px.colors.sequential.Inferno)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parallelcat('cont2',df=train_data.sample(n=1000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.16 Parallel lines</center></h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def parallellines(feature,df=train_data):\n    fig = px.parallel_coordinates(df, color=feature,\n                    color_continuous_scale=px.colors.diverging.Tealrose, color_continuous_midpoint=2)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parallellines('target',df=train_data.sample(n=1000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.17 sunburst Chart</center></h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def sunburst(category1,category2,df=train_data):\n    fig = px.sunburst(df, path=[category1, category2],\n                  color='target')\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sunburst('cat8','cat9',df=train_data.sample(n=1000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:brown\"><center> 5. Data Preprocessing</center></h2>\nI am using StratifiedKFold for regression by dividing target into bins\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"folder_path = '../input/tabular-playground-series-feb-2021'\ntrain_data = pd.read_csv(f'{folder_path}/train.csv')\ntest_data = pd.read_csv(f'{folder_path}/test.csv')\nsample = pd.read_csv(f'{folder_path}/sample_submission.csv')\n\ncont_features = [f'cont{i}' for i in range(14)]\ncat_features = [f'cat{i}' for i in range(10)]\n# cat_features = ['cat1','cat3','cat5','cat8','cat9']\nall_features =   cat_features + cont_features\ntarget_feature = 'target'\n\nnum_bins = int(1 + np.log2(len(train_data)))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'].to_numpy(),bins=num_bins,labels=False)\nbins = train_data['bins'].to_numpy()\n\nfor feat in cat_features:\n    le = LabelEncoder()\n    train_data.loc[:,feat] = le.fit_transform(train_data[feat].fillna(\"-1\"))\n    test_data.loc[:,feat] = le.transform(test_data[feat].fillna(\"-1\"))\n\n\nqt = QuantileTransformer()\ntrain_data.loc[:,cont_features] = qt.fit_transform(train_data.loc[:,cont_features])\ntest_data.loc[:,cont_features] = qt.transform(test_data.loc[:,cont_features])\n\nemb_c = {cat: int(train_data[cat].nunique()) for cat in cat_features if int(train_data[cat].nunique()) >2}\nemb_cols = emb_c.keys()\nembedding_sizes = [(c, min(50, (c+1)//2)) for _,c in emb_c.items()]\ncont_features = cont_features + [cat for cat in cat_features if cat not in emb_cols]\ncat_features = emb_cols\n\n# target = train_data[target_feature].to_numpy()\n# train_data = train_data[all_features].to_numpy()\n# test_data = test_data[all_features].to_numpy()\n\ntrain_data.shape, test_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"border:2px solid red; border-radius:10px; color:black; background-color:orange\"><center> 6. Pytorch Model</center></h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 2021\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = {\n    \"epochs\":15,\n    \"train_batch_size\":1024,\n    \"valid_batch_size\":1024,\n    \"test_batch_size\":1024,\n    \"nfolds\":5, \n    \"learning_rate\":0.001,\n    \n    \"input_size\":len(all_features), \n    'cont_size':len(cont_features),\n    'hidden_sizes':[128,64,32,16],\n    'output_size':1\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TPSDataset(Dataset):\n    def __init__(self,df,cat_features,cont_features):\n        self.cat_data = df.loc[:,cat_features].to_numpy()\n        self.cont_data = df.loc[:,cont_features].to_numpy()\n        self.target = df.loc[:,target_feature].to_numpy()\n    \n    def __getitem__(self,idx):\n        input1 = torch.tensor(self.cat_data[idx],dtype=torch.long)\n        input2 = torch.tensor(self.cont_data[idx],dtype=torch.long)\n        target = torch.tensor(self.target[idx],dtype=torch.float)\n        return input1,input2, target\n    \n    def __len__(self):\n        return len(self.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,cont_size,output_size,hidden_sizes,embedding_sizes):\n        super().__init__()\n        self.embeddings = nn.ModuleList([nn.Embedding(cat,size) for (cat,size) in embedding_sizes])\n        self.emb_size = sum(e.embedding_dim for e in self.embeddings)\n        self.cont_size = cont_size\n            \n        self.layer1 = self.batch_linear_drop(self.emb_size+self.cont_size,hidden_sizes[0],0.1,activation=nn.ReLU)\n        self.layer2 = self.batch_linear_drop(hidden_sizes[0],hidden_sizes[1],0.1,activation=nn.ReLU)\n        self.layer3 = self.batch_linear_drop(hidden_sizes[1],hidden_sizes[2],0.1,activation=nn.ReLU)\n        self.layer4 = self.batch_linear_drop(hidden_sizes[2],hidden_sizes[3],0.1,activation=nn.ReLU)\n        self.layer5 = self.batch_linear(hidden_sizes[3],output_size)\n        \n    def batch_linear_drop(self,inp,out,drop,activation=None):\n        if activation:\n            return nn.Sequential(nn.BatchNorm1d(inp),nn.Dropout(drop),nn.Linear(inp,out),activation())\n        else:\n            return nn.Sequential(nn.BatchNorm1d(inp),nn.Dropout(drop),nn.Linear(inp,out))\n            \n    def batch_linear(self,inp,out,activation=None):\n        if activation:\n            return nn.Sequential(nn.BatchNorm1d(inp),nn.Linear(inp,out),activation())\n        else:\n            return nn.Sequential(nn.BatchNorm1d(inp),nn.Linear(inp,out))\n    \n    def forward(self,input1,input2):\n        x = [e(input1[:,i]) for i,e in enumerate(self.embeddings)]\n        x = torch.cat(x,1)\n        x = torch.cat([x,input2],1)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run(plot_losses=True,verbose=True):\n    \n    def loss_fn(outputs,targets):\n        loss = nn.MSELoss()(outputs,targets)\n        return loss\n  \n    def train_loop(train_loader,model,loss_fn,device,optimizer,lr_scheduler=None):\n        model.train()\n        total_loss = 0\n        for i, (inputs1,inputs2, targets) in enumerate(train_loader):\n            inputs1 = inputs1.to(device)\n            inputs2 = inputs2.to(device)\n            targets = targets.to(device)\n            optimizer.zero_grad()\n            \n            outputs = model(inputs1,inputs2)\n            loss = loss_fn(outputs,targets)\n            loss.backward()\n                \n            total_loss += loss.item()\n\n            optimizer.step()\n                    \n        total_loss /= len(train_loader)\n        return total_loss\n    \n    def valid_loop(valid_loader,model,loss_fn,device):\n        model.eval()\n        total_loss = 0\n        predictions = list()\n        \n        for i, (inputs1,inputs2,targets) in enumerate(valid_loader):\n            inputs1 = inputs1.to(device)\n            inputs2 = inputs2.to(device)\n            targets = targets.to(device)\n            \n            outputs = model(inputs1,inputs2)\n            loss = loss_fn(outputs,targets)\n\n            predictions.extend(outputs.detach().cpu().numpy())\n            \n            total_loss += loss.item()\n        total_loss /= len(valid_loader)\n            \n        return total_loss,np.array(predictions)\n    \n    \n    fold_train_losses = list()\n    fold_valid_losses = list()\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n    \n#     kfold = KFold(n_splits=config['nfolds'])\n    kfold = StratifiedKFold(n_splits=config['nfolds'])\n    for k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_data,y=bins)):\n        x_train,x_valid = train_data.iloc[train_idx,:],train_data.iloc[valid_idx,:]\n\n        input_size = config['cont_size']\n        hiddens_sizes = config['hidden_sizes']\n        output_size = 1\n\n        model = Model(input_size,output_size,hiddens_sizes,embedding_sizes)\n        model.to(device)\n\n        train_ds = TPSDataset(x_train,cat_features,cont_features)\n        train_dl = DataLoader(train_ds,\n                             batch_size = config[\"train_batch_size\"],\n                              shuffle=True,\n                              num_workers = 4,\n                              pin_memory=True\n                             )\n\n        valid_ds = TPSDataset(x_valid,cat_features,cont_features)\n        valid_dl = DataLoader(valid_ds,\n                              batch_size =config[\"valid_batch_size\"],\n                              shuffle=False,\n                              num_workers = 4,\n                              pin_memory=True,\n                             )\n        \n        optimizer = optim.Adam(model.parameters(),lr=config['learning_rate'])\n        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=2, verbose=True)\n#         lr_scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.8)\n#         lr_scheduler = None\n\n        print(f\"Fold {k}\")\n        best_loss = 999\n        \n        train_losses = list()\n        valid_losses = list()\n        start = time.time()\n        for i in range(config[\"epochs\"]):\n            train_loss = train_loop(train_dl,model,loss_fn,device,optimizer,lr_scheduler=lr_scheduler)\n            valid_loss,predictions = valid_loop(valid_dl,model,loss_fn,device)\n            \n            if lr_scheduler:\n                lr_scheduler.step(valid_loss)\n\n            train_losses.append(train_loss)\n            valid_losses.append(valid_loss)\n            \n            end = time.time()\n            epoch_time = end - start\n            start = end\n            \n            score = rmse_score(x_valid[target_feature],predictions)\n                          \n            if verbose:\n                print(f\"epoch:{i} Training loss:{train_loss} | Validation loss:{valid_loss}| Score: {score}  | epoch time {epoch_time:.2f}s \")\n\n            if valid_loss <= best_loss:\n                if verbose:\n                    print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n                best_loss = valid_loss\n                torch.save(model.state_dict(),f'model{k}.bin')\n                \n        fold_train_losses.append(train_losses)\n        fold_valid_losses.append(valid_losses)\n#         break\n      \n        \n    if plot_losses == True:\n        plt.figure(figsize=(20,14))\n        for i, (t,v) in enumerate(zip(fold_train_losses,fold_valid_losses)):\n            plt.subplot(2,5,i+1)\n            plt.title(f\"Fold {i}\")\n            plt.plot(t,label=\"train_loss\")\n            plt.plot(v,label=\"valid_loss\")\n            plt.legend()\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"border:2px solid red; border-radius:10px; color:white; background-color:orange\"><center> Inference </center></h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TPSDataset(Dataset):\n    def __init__(self,df,cat_features,cont_features):\n        self.cat_data = df.loc[:,cat_features].to_numpy()\n        self.cont_data = df.loc[:,cont_features].to_numpy()\n    \n    def __getitem__(self,idx):\n        input1 = torch.tensor(self.cat_data[idx],dtype=torch.long)\n        input2 = torch.tensor(self.cont_data[idx],dtype=torch.long)\n        return input1,input2\n    \n    def __len__(self):\n        return len(self.cat_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = list()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninput_size = config['cont_size']\nhiddens_sizes = config['hidden_sizes']\noutput_size = 1\n\nfor i in range(config['nfolds']):\n    model = Model(input_size,output_size,hiddens_sizes,embedding_sizes)\n    model.load_state_dict(torch.load(f\"./model{i}.bin\",map_location=device))\n    model.to(device)\n    model.eval()\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference(test):\n    all_prediction = np.zeros((test.shape[0],1))\n    test_ds = TPSDataset(test,cat_features,cont_features)\n    test_dl =  DataLoader(test_ds,batch_size = config[\"test_batch_size\"],shuffle=False,num_workers = 4,pin_memory=True)\n    \n    for model in models:\n        prediction = list()\n        for inputs1,inputs2 in test_dl:\n            inputs1 = inputs1.to(device)\n            inputs2 = inputs2.to(device)\n            outputs = model(inputs1,inputs2) \n            prediction.extend(outputs.detach().cpu().numpy())\n        all_prediction += prediction\n\n    return all_prediction/config['nfolds']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = inference(test_data)\nsample.target = predictions\nsample.to_csv('submission.csv',index=False)\nsample.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nplt.subplot(131)\nsns.distplot(sample.target)\nplt.title(\"Distribution of test target\")\nplt.subplot(132)\nsns.distplot(train_data[target_feature])\nplt.title(\"Distribution of train target\")\nplt.subplot(133)\nsns.distplot(sample.target,label='test')\nsns.distplot(train_data[target_feature],label='train')\nplt.legend()\nplt.title(\"Distribution of train-test target\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}