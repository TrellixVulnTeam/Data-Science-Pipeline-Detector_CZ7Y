{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kaggle Tabular Feb Challenge - Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I will explore various feature engineering techniques to improve the model performance."},{"metadata":{},"cell_type":"markdown","source":"## Data Import"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom category_encoders import MEstimateEncoder\nimport optuna","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data(data_dir):\n    train = pd.read_csv(os.path.join(data_dir, 'train.csv'), index_col='id')\n    test = pd.read_csv(os.path.join(data_dir, 'test.csv'), index_col='id')\n    sample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'), index_col='id')\n    return train, test, sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DAT_DIR = '../input/tabular-playground-series-feb-2021'\ntrain, test, sample_submission = read_data(DAT_DIR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{},"cell_type":"markdown","source":"First let's examine the variable's influnce on the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"disc_features = [train[c].dtype == 'object' for c in train.drop('target', axis=1).columns]\ncat_cols = train.select_dtypes(include='object')\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in cat_cols:\n    enc = LabelEncoder()\n    train[c] = enc.fit_transform(train[c])\n    test[c] = enc.transform(test[c])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop('target', axis=1)\ny_train = train.target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also re-introduce utility functions to make model assessment easier."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params = {'max_depth': 7, \n              'learning_rate': 0.002368706913117573, \n              'n_estimators': 3842, \n              'min_child_weight': 4, \n              'colsample_bytree': 0.6612496396706031, \n              'subsample': 0.6060764549240347, \n              'reg_alpha': 0.18899174723187226, \n              'reg_lambda': 30.33470416661318}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def score_dataset(X, y, model=XGBRegressor(), cv_folds=2):\n    # Label encoding for categoricals\n    #\n    # Label encoding is good for XGBoost and RandomForest, but one-hot\n    # would be better for models like Lasso or Ridge. The `cat.codes`\n    # attribute holds the category levels.\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(\n        model, X, y, cv=cv_folds, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"def make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=disc_features) \n    mi_scores = pd.Series(mi_scores, name='MI Scores', index=X.columns)    \n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmi_scores = make_mi_scores(X_train, y_train, discrete_features = disc_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mi_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems the top 4 variables, i.e., cont8, cat1, cont0, and cat9, are more significant than others. Let us try creating products of these variables. "},{"metadata":{},"cell_type":"markdown","source":"### Create pairwise product features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_pairwise_product(X):\n    X2 = pd.DataFrame()\n    for idx1, c1 in enumerate(X.columns):\n        for idx2, c2 in enumerate(X.columns):\n            if idx1 >= idx2: continue\n                \n            new_var = pd.Series(X[c1] * X[c2], name=f'{c1}-{c2}', index=X.index)\n            X2 = pd.concat([X2, new_var], axis=1)\n                \n    return X2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pw_cols = ['cont8', 'cat1', 'cont0', 'cat9']\nX_train_pw = create_pairwise_product(X_train[pw_cols])\nX_train2 = X_train.join(X_train_pw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = score_dataset(X_train2, y_train, model=XGBRegressor(**xgb_params), cv_folds=2)\nprint(f'RMSE: = {score:.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It does not seem to make a difference. Now, let's try adding a little more variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"pw_cols = [c for c in X_train.columns if mi_scores[c] > 0.005]\nX_train_pw = create_pairwise_product(X_train[pw_cols])\nX_train2 = X_train.join(X_train_pw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = score_dataset(X_train2, y_train, model=XGBRegressor(**xgb_params), cv_folds=2)\nprint(f'RMSE: = {score:.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pairwise product variables do not seem to make much difference."},{"metadata":{},"cell_type":"markdown","source":"### PCA Transformation"},{"metadata":{},"cell_type":"markdown","source":"Next, we will try PCA transformation approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create PCA\n\npca = PCA()\nX_train_pca = pca.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"component_names = [f'PC{i+1}' for i in range(X_train_pca.shape[1])]\nX_train_pca = pd.DataFrame(X_train_pca, columns=component_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loadings = pd.DataFrame(pca.components_.T, columns=component_names, index=X_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loadings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1,2)\n    n = pca.n_components_\n    grid = np.arange(1, n+1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(xlabel='Component', title='% Explained Variance', ylim=(0.0, 1.0))\n    \n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0,cv], 'o-')\n    axs[1].set(xlabel='Component', title='% Cumulative Variance', ylim=(0.0, 1.0))\n    \n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = plot_variance(pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first 2 PCA components explain 80% of total variances. Next, let's check their MI scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train2 = X_train.join(X_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = score_dataset(X_train2, y_train, model=XGBRegressor(**xgb_params), cv_folds=2)\nprint(f'RMSE: = {score:.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now confirm that adding new PCA variables do not help."},{"metadata":{},"cell_type":"markdown","source":"### Target Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cat_cnts = list(map(lambda c: X_train[c].value_counts(), cat_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cat_cnts = pd.DataFrame(X_train_cat_cnts).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cat_cnts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems variables cat9 has many levels, followed by cat6, cat7, and cat8. Let's use them as candicates for target encoding. To avoid overfitting, we split the training data into encoding and rest."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding split\nX_train_enc = X_train.sample(frac=0.2, random_state=0)\ny_train_enc = y_train.loc[X_train_enc.index]\n\n# Training split\nX_train_res = X_train.drop(index=X_train_enc.index)\ny_train_res = y_train.drop(X_train_enc.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'X_train_enc.shape = {X_train_enc.shape}, y_train_enc = {y_train_enc.shape}')\nprint(f'X_train_res.shape = {X_train_res.shape}, y_train_res = {y_train_res.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = MEstimateEncoder(cols=['cat6', 'cat7', 'cat8', 'cat9'], m=1.0)\nenc.fit(X_train_enc, y_train_enc)\nX_train2 = enc.transform(X_train_res, y_train_res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params = {'max_depth': 7, \n              'learning_rate': 0.002368706913117573, \n              'n_estimators': 3842, \n              'min_child_weight': 4, \n              'colsample_bytree': 0.6612496396706031, \n              'subsample': 0.6060764549240347, \n              'reg_alpha': 0.18899174723187226, \n              'reg_lambda': 30.33470416661318}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = score_dataset(X_train2, y_train_res, model=XGBRegressor(**xgb_params), cv_folds=2)\nprint(f'RMSE: = {score:.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusions"},{"metadata":{},"cell_type":"markdown","source":"We have tried a few common feature engineering approaches but none of them seem useful. In the next steps, we will survey the existing work to collect new ideas."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}