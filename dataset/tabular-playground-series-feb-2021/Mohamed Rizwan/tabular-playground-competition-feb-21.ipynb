{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tabular Playground Series Competition - Feb 2021 \n\n\n### It's multivariate regression problem!\n\nIn this kernel, I am exploring the statistical regression models to predict the target with the given features (with categorical and continous variables) with the following libraries/modules:\n\n> ### Benchmark models:\n- Artificial Neural Network\n- Scikit-Learn (Python ML Library)\n- Tabular Fastai\n\n> ### With Hyperparameter Tuning\n\n- CatBoost and XGBoost algorithms with Optuna Hyperparameter Tuning\n- PyCaret's Regression module\n\n\n__Bonus: AutoViML library__\n\nOur evaluation metric is \"Root Mean Squared Error' (RMSE). The lower is the RMSE, the better fit is the model.\n\n### Let's Start!! DO CARE TO UPVOTE😁"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# general libraries\nimport os\nimport gc\nimport numpy as np\nfrom numpy import mean\nfrom numpy import std\nimport pandas as pd\nfrom pathlib import Path\n\n# plotting\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport seaborn as sns\nfrom IPython.display import display\n%matplotlib inline\n\n# sklearn - metric, train test split\nfrom sklearn.model_selection import KFold,train_test_split,cross_val_score, RepeatedKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read in the data files"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data():\n    train      = pd.read_csv(\"../input/tabular-playground-series-feb-2021/train.csv\",index_col='id')\n    test       = pd.read_csv(\"../input/tabular-playground-series-feb-2021/test.csv\",index_col='id')\n    submission = pd.read_csv(\"../input/tabular-playground-series-feb-2021/sample_submission.csv\")\n    return train,test,submission\n\ntrain,_,_ = load_data()\ndisplay(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regression on Deep Artificial Neural Networks\n\n\n[READ: Linear Regression Deep Neural Network](https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom tensorflow.keras import layers\nfrom keras.layers import Dense\nfrom keras.models import Sequential, Model\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.values.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encoding\n\ntarget = train.pop('target')\n\nfor c in train.columns:\n    if train[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values))\n        train[c] = lbl.transform(train[c].values)\n        \n# Normalisation of the data\nnormalizer = preprocessing.Normalization()\nnormalizer.adapt(np.array(train))\n\n\nX_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.80)\n\n# define the model\ndef ann_model(norm):\n    model = keras.Sequential([\n        norm,\n        layers.Dense(24, input_dim=24, kernel_initializer='normal', activation='relu'),\n        layers.Dense(10,activation='relu'),\n        layers.Dense(5),\n        layers.Dense(1)\n    ])\n    model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(0.01))\n    return model\n\ndnn_model = ann_model(normalizer)\ndnn_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = dnn_model.fit(X_train,y_train, validation_split =0.2,batch_size=128, epochs=50,verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generic function to visualise the fit of the model and MSE\ndef plot_results(name, y, yhat, num_to_plot=25000, lims=(0,15), figsize=(10,7)):\n    MSE = math.sqrt(((yhat-y)**2).mean())\n    RMSE = np.sqrt(MSE)\n    plt.figure(figsize=figsize)\n    a = plt.axes(aspect='equal')\n    plt.scatter(y[:num_to_plot], yhat[:num_to_plot])\n    plt.ylim(lims)\n    plt.xlim(lims)\n    _ = plt.plot(lims, lims)\n    plt.title(f'{name}: {RMSE:0.6f}', fontsize=16)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions= dnn_model.predict(X_test).flatten()\ntest_labels = y_test\n\na = plt.axes(aspect='equal')\nplt.scatter(test_labels, predictions)\nplt.xlabel('True Values')\nplt.ylabel('Predictions')\nlims = [0, 14]\nplt.xlim(lims)\nplt.ylim(lims)\n_ = plt.plot(lims, lims)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# error distributiion\n\nerror = predictions - y_test\nplt.hist(error, bins=2000)\nplt.xlabel('Prediction Error')\n_ = plt.ylabel('Count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarizing the fit of the model\ndnn_MSE      = round(mean_squared_error(y_test, predictions, squared=False),6)\ndnn_RMSE     = round(np.sqrt(mean_squared_error(y_test, predictions, squared=False)),6)\ndnn_R2       = round(metrics.r2_score(y_test, predictions),6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models =['Neural Network','Decision Tree','Random Forest','LightGBM','XGBoost','CatBoost','Decision Tree-fastai','Random Forest-fastai',\n         'CatBoost-fastai','XGBoost-Optuna','CatBoost-Optuna-gpu','CatBoost-Optuna-cpu']\nresults = pd.DataFrame(index=models,columns=['MSE','RMSE','R2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.iloc[0:1,0:1] = dnn_MSE\nresults.iloc[0:1,1:2] = dnn_RMSE\nresults.iloc[0:1,2:3] = dnn_R2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train, target, history\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scikit-Learn Library of Python"},{"metadata":{},"cell_type":"markdown","source":"## Encode the categoricals\n\nThere are different strategies to accomplish this, and different approaches will have different performance when using different algorithms."},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test,_ = load_data()\n\nfor c in train.columns:\n    if train[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values))\n        train[c] = lbl.transform(train[c].values)\n              \nfor c in test.columns:\n    if test[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(test[c].values))\n        test[c] = lbl.transform(test[c].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data       \ntarget = train.pop('target')\n\nX_train,X_test,y_train,y_test = train_test_split(train,target,test_size=0.2)\nX_train.shape,y_train.shape,X_test.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Decision Tree\n\nfrom sklearn.tree import DecisionTreeRegressor\nimport math\n\n\nmodel_tree = DecisionTreeRegressor(\n    criterion='mse',splitter='best',min_samples_split=1.0,min_samples_leaf=5, max_features=1.0,random_state=42,max_leaf_nodes=20\n)\n\nmodel_tree.fit(X_train, y_train)\ny_tree = model_tree.predict(X_test)\n\n# summarizing the fit of the model\ntree_MSE      = round(mean_squared_error(y_test, y_tree, squared=False),6)\ntree_RMSE     = round(np.sqrt(mean_squared_error(y_test, y_tree, squared=False)),6)\ntree_R2       = round(metrics.r2_score(y_test, y_tree),6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.iloc[1:2,0:1] = tree_MSE\nresults.iloc[1:2,1:2] = tree_RMSE\nresults.iloc[1:2,2:3] = tree_R2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\ndef rf(xs,y,n_estimators=100, n_jobs=-1,max_samples = 240000,max_features = 0.5,min_samples_leaf=0.5,**kwargs):\n    return RandomForestRegressor(n_jobs=-1,n_estimators=100,max_samples=max_samples,\n                                 max_features=max_features,min_samples_leaf=min_samples_leaf,oob_score=True).fit(xs,y)\n\n                              \nmodelRF = rf(X_train, y_train,n_estimators=100)\ny_pred_rf = modelRF.predict(X_test)\n\n# summarizing the fit of the model\n\nrf_MSE      = round(mean_squared_error(y_test, y_pred_rf, squared=False),6)\nrf_RMSE     = round(np.sqrt(mean_squared_error(y_test, y_pred_rf, squared=False)),6)\nrf_R2       = round(metrics.r2_score(y_test, y_pred_rf),6)\n\nresults.iloc[2:3,0:1] = rf_MSE\nresults.iloc[2:3,1:2] = rf_RMSE\nresults.iloc[2:4,2:3] = rf_R2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Light GBM Regressor\n\n> ### The parameters need to tune to get good results on a leaf-wise tree algorithm:\n\n1. num_leaves      : the number of leaves should be smaller than 2^(max_depth)\n2. min_data_in_leaf: For a large dataset, it can be set to hundreds or thousands\n3. max_depth       : limit the depth of the tree\n\n> ### Faster speeds on the algorithm can be obtained by using:\n\n1. small max_bin\n2. save_binary to speed up data loading in future learning\n3. optimal bagging_freq and bagging_fraction\n4. feature_fraction for feature sub-sampling\n5. Use a small learning rate with large num_iterations\n\n> ### Avoid Overfitting\n\n1. Trying lambda_l1, lambda_l2, and min_gain_to_split for regularization\n2. Avoid growing a very deep tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as ltb\n\nmodel_ltb= ltb.LGBMRegressor(boosting_type='gbdt',num_leaves=50,min_data_in_leaf=1000,max_depth=7,learning_rate=0.009,n_estimators=500)\nmodel_ltb.fit(X_train, y_train)\ny_pred_lg = model_ltb.predict(X_test)\n\n# summarizing the fit of the model\n\nlg_MSE      = round(mean_squared_error(y_test, y_pred_lg, squared=False),6)\nlg_RMSE     = round(np.sqrt(mean_squared_error(y_test, y_pred_lg, squared=False)),6)\nlg_R2       = round(metrics.r2_score(y_test, y_pred_lg),6)\n\nresults.iloc[3:4,0:1] = lg_MSE\nresults.iloc[3:4,1:2] = lg_RMSE\nresults.iloc[3:4,2:3] = lg_R2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nxgbmodel = xgb.XGBRegressor(objective='reg:squarederror') \n\n# Fitting the model \nxgbmodel.fit(X_train, y_train)\n\n# Predict the model \ny_pred_xgb = xgbmodel.predict(X_test)\n\n# summarizing the fit of the model\nxg_MSE      = round(mean_squared_error(y_test, y_pred_xgb, squared=False),6)\nxg_RMSE     = round(np.sqrt(mean_squared_error(y_test, y_pred_xgb, squared=False)),6)\nxg_R2       = round(metrics.r2_score(y_test, y_pred_xgb),6)\n\nresults.iloc[4:5,0:1] = xg_MSE\nresults.iloc[4:5,1:2] = xg_RMSE\nresults.iloc[4:5,2:3] = xg_R2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model on the whole dataset\ncatmodel = CatBoostRegressor(verbose=0, n_estimators=1000)\n# Fitting the model \ncatmodel.fit(X_train, y_train)\n\n# Predict the model \ny_pred_cat = catmodel.predict(X_test)\n# summarizing the fit of the model\n\ncat_MSE      = round(math.sqrt(((y_pred_cat-y_test)**2).mean()),6)\ncat_RMSE     = round(np.sqrt(cat_MSE),6)\ncat_R2       = metrics.r2_score(y_pred_cat,y_test)\n\nresults.iloc[5:6,0:1] = cat_MSE\nresults.iloc[5:6,1:2] = cat_RMSE\nresults.iloc[5:6,3:4] = cat_R2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_results(\"Cat Boost Regressor\", y_test, y_pred_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train,target,X_train,X_test,y_train,y_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PyCaret - Statistical Models\n\n#### Read [**Tutorial on PyCaret Library**](https://github.com/pycaret/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pycaret\nfrom pycaret.regression import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test,_ = load_data()\n\nfor c in train.columns:\n    if train[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values))\n        train[c] = lbl.transform(train[c].values)\n              \nfor c in test.columns:\n    if test[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(test[c].values))\n        test[c] = lbl.transform(test[c].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Unseen Data for Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train.sample(frac=0.9, random_state=42)\ndata_unseen = train.drop(data.index)\n\ntrain.reset_index(drop=True, inplace=True)\ndata_unseen.reset_index(drop=True, inplace=True)\n\nprint('Data for Modeling          : ' + str(train.shape))\nprint('Unseen Data For Predictions: ' + str(data_unseen.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pull out the target, and make a validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = setup(train,target='target',session_id=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best = compare_models(exclude = ['rf','xgboost','lightgbm','br','ransac','lr','dt','lar','huber','par','omp','knn','ridge','et','ada','en'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ncat = create_model('catboost',verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# untuned model is efficient tha untuned\n#tuned_model = tune_model(cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(cat, plot = 'error')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict on Test / Hold-out Sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_model(cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finalize Mode"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = finalize_model(cat)\npredict_model(final_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict on Unseen Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"unseen_predictions = predict_model(final_model,data=data_unseen)\nunseen_predictions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pycaret.utils import check_metric\ncheck_metric(unseen_predictions.target, unseen_predictions.Label, 'RMSE')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Saving the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"save_model(final_model,'CatBoost_Model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyper parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = final_model.get_all_params()\nprint(best_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pycart - CatBoost Model - Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/tabular-playground-series-feb-2021/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_catboost = predict_model(final_model,data=test)\n\nid = sample_submission['id'].values\nlabel = pred_catboost.Label.values\n\n\nout_df=pd.DataFrame({'id':id,'target':label})\n\n# round the predictions to 6 decimal values\nfor c in out_df.columns:\n    if out_df[c].dtype=='float64':\n        out_df[c]= round(out_df[c],6)\n        \ndisplay(out_df.head())\nout_df.to_csv('submission_catboost_pycaret.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Statistical Models with Fastai"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This file contains all the main external libs we'll use\nimport fastai\nfrom fastai.imports  import *\nfrom fastai.tabular.all import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_path = Path('/kaggle/input/tabular-playground-series-feb-2021/')\ntrain = pd.read_csv(input_path / 'train.csv', index_col='id')\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Samples              :\",len(train))\nprint(\"Number of Categorical variables:\",10)\nprint(\"Number of Continuos variables  :\",14)\nprint(\"Max Features                   :\",24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['cat0'] = train['cat0'].astype('category')\ntrain['cat1'] = train['cat1'].astype('category')\ntrain['cat2'] = train['cat2'].astype('category')\ntrain['cat3'] = train['cat3'].astype('category')\ntrain['cat4'] = train['cat4'].astype('category')\ntrain['cat5'] = train['cat5'].astype('category')\ntrain['cat6'] = train['cat6'].astype('category')\ntrain['cat7'] = train['cat7'].astype('category')\ntrain['cat8'] = train['cat8'].astype('category')\ntrain['cat9'] = train['cat9'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_names = ['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9']\ncont_names = ['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10','cont11','cont12','cont13']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sizes ='A','B','C','D','E','F','G','H','I','J','K','L','M','N','O'\n\ntrain['cat0'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat1'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat2'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat3'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat4'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat5'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat6'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat7'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat8'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat9'].cat.set_categories(sizes, ordered=False, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"splits = RandomSplitter(valid_pct=0.2)(range_of(train))\n\ntp = TabularPandas(train,cat_names=cat_names,cont_names=cont_names,procs=[Categorify,FillMissing, Normalize],y_names='target',splits=splits)\nlen(tp.train),len(tp.valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tp.show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tp.items.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test= pd.read_csv(input_path / 'test.csv', index_col='id')\n\nto = TabularPandas(test,cat_names=cat_names,cont_names=cont_names,procs=[Categorify,FillMissing, Normalize])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining independent and dependent variables\nxs,y = tp.train.xs,tp.train.y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.imports import *\nfrom sklearn.tree import DecisionTreeRegressor\n\nm = DecisionTreeRegressor(max_features=24,max_leaf_nodes=25,max_depth=10)\nm.fit(xs,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nimport graphviz\nfeature_names = xs.columns.values\ndot_data = tree.export_graphviz(m, out_file=None,feature_names=feature_names,class_names=y,filled=True, rounded=True) \ngraph = graphviz.Source(dot_data) \ngraph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining independent and dependent variables\nvalid_xs,valid_y = tp.valid.xs,tp.valid.y\ny_pred_dt   = m.predict(valid_xs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cal_rmse(pred,y):return round(math.sqrt(((pred-y)**2).mean()),6)\n\ndef model_rmse(m,xs,y):return cal_rmse(m.predict(xs),y)\n\ndt_MSE      = round(mean_squared_error(valid_y, y_pred_dt, squared=False),6)\ndt_RMSE     = np.sqrt(dt_MSE)\ndt_R2       = round(metrics.r2_score(valid_y, y_pred_dt),6)\n\nresults.iloc[6:7,0:1] = dt_MSE\nresults.iloc[6:7,1:2] = dt_RMSE\nresults.iloc[6:7,2:3] = dt_R2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\ndef rf(xs,y,n_estimators=250, n_jobs=-1,max_samples = 240000,max_features =1.0,min_samples_leaf=0.5,**kwargs):\n    return RandomForestRegressor(n_jobs=-1,n_estimators=100,max_samples=max_samples,max_features=max_features,min_samples_leaf=min_samples_leaf,oob_score=True).fit(xs,y)\n\nm_rf = rf(xs,y,min_samples_leaf=4,max_leaf_nodes=250,max_depth=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cal_rmse(pred,y):return round(math.sqrt(((pred-y)**2).mean()),6)\ndef model_rmse(m,xs,y): return cal_rmse(m.predict(xs),y)\n\n# defining independent and dependent variables\nvalid_xs,valid_y = tp.valid.xs,tp.valid.y\n\ny_pred_rf = m_rf.predict(valid_xs)\n\nrfa_MSE      = round(model_rmse(m_rf,valid_xs,valid_y),6)\nrfa_RMSE     = np.sqrt(rfa_MSE)\nrfa_R2       = round(metrics.r2_score(valid_y,y_pred_rf),6)\n\nresults.iloc[7:8,0:1] = rfa_MSE\nresults.iloc[7:8,1:2] = rfa_RMSE\nresults.iloc[7:8,2:3] = rfa_R2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CatBoost with fastai"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model on the whole dataset\n\ndef cat(xs,y,n_estimators=1000, n_jobs=-1,**kwargs):\n    return CatBoostRegressor(verbose=0, n_estimators=1000).fit(xs,y)\n\nparams = {'nan_mode': 'Min','eval_metric': 'RMSE','iterations': 2500,'sampling_frequency': 'PerTree',\n          'leaf_estimation_method': 'Newton','grow_policy': 'SymmetricTree','penalties_coefficient': 1,'boosting_type': 'Plain',\n          'model_shrink_mode': 'Constant','feature_border_type': 'GreedyLogSum','bayesian_matrix_reg': 0.10000000149011612,'l2_leaf_reg': 3,\n          'random_strength': 1,'rsm':1,'boost_from_average': True,'model_size_reg': 0.5,'subsample': 0.800000011920929,'use_best_model': False,\n          'random_seed':14,'depth': 10,'posterior_sampling': False,'border_count': 254,'classes_count': 0,'auto_class_weights': 'None',\n          'sparse_features_conflict_fraction': 0,'leaf_estimation_backtracking': 'AnyImprovement','best_model_min_trees': 1,'model_shrink_rate': 0,\n          'min_data_in_leaf': 300,'loss_function': 'RMSE','learning_rate': 0.010290546311954876,'score_function': 'Cosine','task_type': 'CPU',\n          'leaf_estimation_iterations': 1,'bootstrap_type': 'MVS','max_leaves': 64}\n\n# Fitting the model \nm_cat = cat(xs,y,**params)\n\ndef cal_rmse(pred,y):return round(math.sqrt(((pred-y)**2).mean()),6)\ndef model_rmse(m,xs,y): return cal_rmse(m.predict(xs),y)\n\n# defining independent and dependent variables\nvalid_xs,valid_y = tp.valid.xs,tp.valid.y\n\ny_pred_cat = m_cat.predict(valid_xs)\n\ncata_MSE      = round(model_rmse(m_cat,valid_xs,valid_y),6)\ncata_RMSE     = np.sqrt(cata_MSE)\ncata_R2       = round(metrics.r2_score(valid_y,y_pred_cat),6)\n\nresults.iloc[8:9,0:1] = cata_MSE\nresults.iloc[8:9,1:2] = cata_RMSE\nresults.iloc[8:9,2:3] = cata_R2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rf_imp_features(m,df):\n    return pd.DataFrame({'cols':df.columns,'imp_features':m.feature_importances_}).sort_values('imp_features',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ffig = rf_imp_features(m,xs)\nffig[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_fig(ffig):\n    return ffig.plot('cols','imp_features','barh',figsize=(12,8),legend=False)\n\nplot_fig(ffig)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models with Optuna Hyperparameter Tuning\nOptuna is a black-box optimizer that needs an objective function. It returns a numerical value to evaluate the performance of the hyperparameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install optuna \nimport optuna","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost with Optuna"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(input_path / 'train.csv', index_col='id')\n\nfor c in train.columns:\n    if train[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values))\n        train[c] = lbl.transform(train[c].values)\n        \ntarget = train.pop('target')\nX_train,X_test,y_train,y_test = train_test_split(train,target,test_size=0.2)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\ndef objective(trial):\n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.001,0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': 4000,\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight',1,500)\n    }\n    model = xgb.XGBRegressor(**param)  \n    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n    preds = model.predict(X_test)\n    rmse = round(np.sqrt(mean_squared_error(y_test,preds)),6)\n    return rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n#study = optuna.create_study(direction='minimize')\n#study.optimize(objective, n_trials=25)\n#print('Number of finished trials:', len(study.trials))\n#print('Best trial:', study.best_trial.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_optimization_history: shows the scores from all trials as well as the best score so far at each point\n\n#optuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model on the whole dataset\n\nbest_trial =  {'lambda': 0.07768755871021779, 'alpha': 9.52276768372669,\n             'colsample_bytree': 0.3, 'subsample': 0.7, 'learning_rate': 0.02, 'max_depth': 7, 'random_state': 24, 'min_child_weight': 117}\n\nmodel = xgb.XGBRegressor(**best_trial)\nmodel.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=False)\n\n# Predict the model \npreds = model.predict(X_test)\nmse = round(mean_squared_error(y_test, preds,squared=False),6)\n\nxgopt_MSE      = mse\nxgopt_RMSE     = np.sqrt(mse)\nxgopt_R2       = round(metrics.r2_score(y_test, preds),6)\n\nresults.iloc[9:10,0:1] = xgopt_MSE\nresults.iloc[9:10,1:2] = xgopt_RMSE\nresults.iloc[9:10,2:3] = xgopt_R2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CatBoost with Optuna"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    param = {\n        'loss_function': 'RMSE',\n        'task_type': 'GPU',\n        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n        'max_bin': trial.suggest_int('max_bin', 200, 400),\n        #'rsm': trial.suggest_uniform('rsm', 0.3, 1.0),\n        'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n        'n_estimators': 25000,\n        'max_depth': trial.suggest_categorical('max_depth', [7,10,14,16]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300)\n    }\n    model = CatBoostRegressor(**param)  \n    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=200,verbose=False)\n    preds = model.predict(X_test)\n    rmse = mean_squared_error(y_test, preds,squared=False)\n    return rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna\n\n\nclass StopWhenTrialKeepBeingPrunedCallback:\n    def __init__(self, threshold: int):\n        self.threshold = threshold\n        self._consequtive_pruned_count = 0\n\n    def __call__(self, study: optuna.study.Study, trial: optuna.trial.FrozenTrial) -> None:\n        if trial.state == optuna.trial.TrialState.PRUNED:\n            self._consequtive_pruned_count += 1\n        else:\n            self._consequtive_pruned_count = 0\n\n        if self._consequtive_pruned_count >= self.threshold:\n            study.stop()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nimport logging\nimport sys\n\n# Add stream handler of stdout to show the messages\noptuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\nstudy = optuna.create_study(direction='minimize',pruner=optuna.pruners.MedianPruner())\n\nstudy_stop_cb = StopWhenTrialKeepBeingPrunedCallback(2)\nstudy.optimize(objective, n_trials=20,callbacks=[study_stop_cb])\n\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_optimization_history: shows the scores from all trials as well as the best score so far at each point\n\noptuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### With CPU\n\nbest_trial = {'l2_leaf_reg': 0.02247766515106271, 'max_bin': 364, 'subsample': 0.6708650091202213,\n             'learning_rate': 0.010290546311954876, 'max_depth': 10, 'random_state': 24, 'min_data_in_leaf': 300,\n            'loss_function': 'RMSE','n_estimators':  25000,'rsm':0.5}"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_trial = {'l2_leaf_reg': 0.02247766515106271, 'max_bin': 364, 'subsample': 0.6708650091202213,\n             'learning_rate': 0.010290546311954876, 'max_depth': 10, 'random_state': 24, 'min_data_in_leaf': 300,\n            'loss_function': 'RMSE','n_estimators':  25000,'rsm':0.5}\n\nmodel_catopt_cpu = CatBoostRegressor(**best_trial)  \nmodel_catopt_cpu.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=200,verbose=False)\npreds = model_catopt_cpu.predict(X_test)\nmse = round(mean_squared_error(y_test, preds,squared=False),6)\n\ncatboost_MSE      = mse\ncatboost_RMSE     = np.sqrt(mse)\ncatboost_R2       = round(metrics.r2_score(y_test, preds),6)\n\n\nresults.iloc[11:12,0:1] = catboost_MSE\nresults.iloc[11:12,1:2] = catboost_RMSE\nresults.iloc[11:12,2:3] = catboost_R2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test,_ = load_data()\n\nfor c in test.columns:\n    if test[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(test[c].values))\n        test[c] = lbl.transform(test[c].values)\n        \ntest_features = test.values\n\nsubmission = pd.read_csv(\"../input/tabular-playground-series-feb-2021/sample_submission.csv\")\n\npred3 = model_catopt_cpu.predict(test_features).flatten()\n\nsubmission['target'] = pred3\n\n# round the predictions to 6 decimal values\nfor c in submission.columns:\n    if submission[c].dtype=='float64':\n        submission[c]= round(submission[c],6)\n    \nsubmission.to_csv('submission_catboost_optuna_cpu.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### with GPU\n\nbest_params= {'l2_leaf_reg': 0.013856725926090555,'max_bin': 257,'bagging_fraction': 0.6788425346135741,'learning_rate': 0.010983813229740517,\n'max_depth': 10,'random_state': 24,'min_data_in_leaf': 300}"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_trial = study.best_trial.params\n\nmodel = CatBoostRegressor(**best_trial)  \nmodel.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=200,verbose=False)\npreds = model.predict(X_test)\nmse = round(mean_squared_error(y_test, preds,squared=False),6)\n\ncatboost_MSE_gpu     = mse\ncatboost_RMSE_gpu    = np.sqrt(mse)\ncatboost_R2_gpu      = round(metrics.r2_score(y_test, preds),6)\n\n\nresults.iloc[10:11,0:1] = catboost_MSE_gpu\nresults.iloc[10:11,1:2] = catboost_RMSE_gpu\nresults.iloc[10:11,2:3] = catboost_R2_gpu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results and Submissions\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = results.sort_values(by=['MSE'], ascending=True)\ndisplay(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_cat_fastai = m_cat.predict(to.items)\n# model: CatBoost-fastai\n\nsubmission['target'] = y_pred_cat_fastai\n\n# round the predictions to 6 decimal values\nfor c in submission.columns:\n    if submission[c].dtype=='float64':\n        submission[c]= round(submission[c],6)\n    \nsubmission.to_csv('submission_catboost_fastai.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test,_ = load_data()\n\nfor c in test.columns:\n    if test[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(test[c].values))\n        test[c] = lbl.transform(test[c].values)\n        \ntest_features = test.values\n\nsubmission = pd.read_csv(\"../input/tabular-playground-series-feb-2021/sample_submission.csv\"\n\n# model: CatBoost-Optuna-GPU\n\npred2 = model.predict(test_features).flatten()\n\nsubmission['target'] = pred2\n\n# round the predictions to 6 decimal values\nfor c in submission.columns:\n    if submission[c].dtype=='float64':\n        submission[c]= round(out_df[c],6)\n    \nsubmission.to_csv('submission_catboost_optuna_gpu.csv',index=False)\n\n# model: CatBoost-Optuna-CPU\n\npred3 = model_catopt_cpu(test_features).flatten()\n\nsubmission['target'] = pred3\n\n# round the predictions to 6 decimal values\nfor c in submission.columns:\n    if submission[c].dtype=='float64':\n        submission[c]= round(out_df[c],6)\n    \nsubmission.to_csv('submission_catboost_optuna_cpu.csv',index=False)\n\n# model: - CatBoost-pycaret\n\npred_catboost = predict_model(final_model,data=test)\n\nid = sample_submission['id'].values\nlabel = pred_catboost.Label.values\nout_df=pd.DataFrame({'id':id,'target':label})\n\n# round the predictions to 6 decimal values\nfor c in out_df.columns:\n    if out_df[c].dtype=='float64':\n        out_df[c]= round(out_df[c],6)\n        \nout_df.to_csv('submission_catboost_pycaret.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using AutoViML \n\nAutoVIML is an open-source python package that makes machine learning easy."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install autoviml\nfrom autoviml.Auto_ViML import Auto_ViML\n!pip install autoviml --no-cache-dir --ignore-installed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip\n!pip install SHAP\n!pip3 install --upgrade Pillow\nimport PIL\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test,submission = load_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the data sets\n\nfrom catboost import CatBoostRegressor\n\nmodel, features, trainm, testm = Auto_ViML(\n    train=train,\n    target=\"target\",\n    test=test,\n    sample_submission=\"\",\n    hyper_param=\"RS\",\n    feature_reduction=True,\n    scoring_parameter=\"mse\",\n    KMeans_Featurizer=False,\n    Boosting_Flag=\"CatBoost\",\n    Binning_Flag=True,\n    Add_Poly=False,\n    Stacking_Flag=False,\n    Imbalanced_Flag=True,\n    verbose=0\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test,submission = load_data()\n\nfor c in train.columns:\n    if train[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values))\n        train[c] = lbl.transform(train[c].values)\n\nfor c in test.columns:\n    if test[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(test[c].values))\n        test[c] = lbl.transform(test[c].values)\n\n\ntarget = train.pop('target')\n\nX_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.80)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"automodel = model.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=False)\n\npreds = automodel.predict(X_test)\n\nmse = round(mean_squared_error(y_test, preds),6)\n\n\nprint(mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(results)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}