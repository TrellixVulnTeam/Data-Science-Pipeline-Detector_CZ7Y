{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os, random, math, glob\nfrom IPython.display import Image as IM\nfrom IPython.display import clear_output\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold, GroupKFold, train_test_split\nimport re\nfrom sklearn.preprocessing import LabelEncoder\n#from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\nfrom lightgbm import LGBMRegressor\nfrom sklearn.cluster import KMeans\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nplt.rcParams['figure.figsize'] = [5, 5]\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/test.csv')\nsub = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## shape of the train and test set\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## storing the id\ntest_id = test['id']\n\ntest = test.drop('id', axis=1)\ntrain = train.drop('id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## check missing value\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing value so far"},{"metadata":{"trusted":true},"cell_type":"code","source":"## check the target variable distribution using histogram\ntrain['target'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### I will be using lightgbm encoding method to see the performance of the model.\nLightGBM can use categorical features directly (without one-hot encoding). The experiment on Expo data shows about 8x speed-up compared with one-hot encoding.\nit's shows is super fast than catboost and will also be super fast and more suitable for 300K rows of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_feats = [\n    f for f in train.columns if train[f].dtype == 'object'\n]\n\ncategorical_feats\nfor f_ in categorical_feats:\n    train[f_] = train[f_].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_feats = [\n    f for f in test.columns if test[f].dtype == 'object'\n]\n\ncategorical_feats\nfor f_ in categorical_feats:\n    test[f_] = test[f_].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop('target', axis=1)\ny = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {'num_leaves': 31,\n         'min_data_in_leaf': 5, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"min_child_samples\": 5,\n         \"min_sum_hessian_in_leaf\":10,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,'early_stopping':100,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.85 ,\n         \"bagging_seed\": 224,\n         \"metric\": 'rmse',\n         \"verbosity\": -1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GroupKFold, RepeatedKFold\nerrlgb=[]\ny_pred_totlgb=[]\nfold= KFold(n_splits=5, shuffle=True, random_state=10)\ni=1\nfor train_index, test_index in fold.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    m = LGBMRegressor(**param, n_estimators=2000)\n    m.fit(X_train,y_train,categorical_feature=categorical_feats,eval_set=[(X_train,y_train),(X_test, y_test)],verbose=100, early_stopping_rounds=100)#,verbose=100)\n    preds=m.predict(X_test)\n    print(\"err: \",np.sqrt(mean_squared_error(y_test,preds)))\n    errlgb.append(np.sqrt(mean_squared_error(y_test,preds)))\n    p = m.predict(test)\n    y_pred_totlgb.append(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(errlgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fea_imp = pd.DataFrame({'imp':m.feature_importances_, 'col': X.columns})\nfea_imp = fea_imp.sort_values(['imp', 'col'], ascending=False).iloc[-30:]\n_ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))\nplt.savefig('catboost_feature_importance.png')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {\"id\": test_id, 'target': (np.mean(y_pred_totlgb, 0))}\nsub = pd.DataFrame(data=d)\nsub = sub[[\"id\", 'target']].round(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('base.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**A great way to start.**\n\n**fine tune the lightgbm model**\n\n\n**more to come**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}