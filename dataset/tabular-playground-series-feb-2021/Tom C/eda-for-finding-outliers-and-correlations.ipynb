{"cells":[{"metadata":{},"cell_type":"markdown","source":"# EDA for Finding Outliers and Correlations\n\nHere we will perform some exploratory data anaylsis to get an idea of any patterns in the dataset. We will also find (and remove) any outliers.\n\nFinally we will look at correlations between the features to aid the development of models into the future.\n\n**Please give an update if you find this helpful. Thanks!**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\nfrom scipy.stats import norm\nimport scipy.stats as st\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the Data\n\nHere we will load the data into a pandas dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-feb-2021/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-feb-2021/test.csv')\ndisplay(train_df.head())\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we have 14 continuous variables and 10 categorical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_FEATURES = ['cont%d' % (i) for i in range(0, 14)]\ncat_FEATURES = ['cat%d' % (i) for i in range(0, 10)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning the Dataset\n\nFollowing the steps of the Machine Learning Checklist we will start by cleaning out invalid values and outliers from the dataset."},{"metadata":{},"cell_type":"markdown","source":"### Invalid Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that there are no *non-null* values so there is nothing to remove here.\n\n### Outliers\n\n#### **Removing outliers is less of a science and more of an art form. So I will leave the choice up to you, but show you how to visualise these points.**\n\nFirst we will look at outliers for the *target*.\n\nWe will add noise to the one dimensional features in order to \"explode\" the points out, helping us see the distributions and potential outliers.\n\nWe will use two methods for finding outliers:\n* The first will consider a point to be an outlier if it is N standard deviations from the mean. N is defined as the threshold.\n* A more complex form of outlier detection is LOF (Local Outlier Factor) which uses a points 20 nearest neighbours to determine if it is in a low density region (and therefore potentially and outlier)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_outliers(df, feature, threshold=3):\n    mean, std = np.mean(df), np.std(df)\n    z_score = np.abs((df-mean) / std)\n    good = z_score < threshold\n\n    print(f\"Rejection {(~good).sum()} points\")\n    visual_scatter = np.random.normal(size=df.size)\n    plt.scatter(df[good], visual_scatter[good], s=2, label=\"Good\", color=\"#4CAF50\")\n    plt.scatter(df[~good], visual_scatter[~good], s=8, label=\"Bad\", color=\"#F44336\")\n    plt.legend(loc='upper right')\n    plt.title(feature)\n    plt.show();\n    \n    return good\n    \ndef plot_lof_outliers(df, feature):\n    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.001, p=1)\n    good = lof.fit_predict(df) > 0.5 # change this value to set the threshold for outliers\n    print(f\"Rejection {(~good).sum()} points\")\n    \n    visual_scatter = np.random.normal(size=df.size)\n    plt.scatter(df[good], visual_scatter[good], s=2, label=\"Good\", color=\"#4CAF50\")\n    plt.scatter(df[~good], visual_scatter[~good], s=8, label=\"Bad\", color=\"#F44336\")\n    plt.legend(loc='upper right')\n    plt.title(feature)\n    plt.show();\n    \n    return good","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"good = plot_outliers(train_df['target'], 'target', threshold=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above we can see that these points are very reasonable outliers. There is a clear grouping for the target values however these points marked in red fall outside this grouping. I will therefore remove these 24 rejected points."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[good]\nprint('Now train_df has %d rows.' % (train_df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we will look at the LOF outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"good = plot_lof_outliers(train_df['target'].values.reshape(train_df['target'].shape[0], -1), 'target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above is harder to read as it has picked some points inside grouping. However, since there are only 300 points and I trust the LOF measurement, I am going to remove these points from dataset as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[good]\nprint('Now train_df has %d rows.' % (train_df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Outliers\n\nFirst we will look at the threshold outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in cont_FEATURES:\n    plot_outliers(train_df[feature], feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So above we can see that the majority of the features do not contain outliers, however features *cont5* and *cont12* do contain some points that are could be considered as outliers."},{"metadata":{},"cell_type":"markdown","source":"We will now look at the **LOF (Local Outlier Factor)** outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in cont_FEATURES:\n    # There some reshaping done here for syntax sake\n    plot_lof_outliers(train_df[feature].values.reshape(train_df[feature].shape[0], -1), feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the above that there are a small number of reasonable outliers selected here. I am therefore not going to remove any of these points as outliers."},{"metadata":{},"cell_type":"markdown","source":"# Analysing Distributions\n\nHere we will look at correlations between the features, distributions of the features."},{"metadata":{},"cell_type":"markdown","source":"### Continuous Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in cont_FEATURES:\n    sns.violinplot(x=train_df[feature], inner='quartile', bw=0.1)\n    plt.title(feature)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above shows us that each feature has a unique distribution which could likely be used to help our models make predictions.\n\nWe can also see that contain features contain points that are most likely outliers (looking at the tails/heads), namely *cont3*, *cont4*, *cont5*, *cont6* and *cont12*."},{"metadata":{},"cell_type":"markdown","source":"### Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in cat_FEATURES:\n    sns.violinplot(x=feature, y='target', data=train_df, inner='quartile');\n    plt.title(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The takeaway from this is that the categorical variables are not a silver bullet for determining the target. The models will need to receive a combination of these variables in order to make accurate predictions."},{"metadata":{},"cell_type":"markdown","source":"# Empirical CDFs\n\nThe below graphs show us where the 10th/20th/..../90th percentiles lie for each of the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_cdf(df, feature):\n    ps = 100 * st.norm.cdf(np.linspace(-4, 4, 10)) # The last number in this tuple is the number of percentiles\n    x_p = np.percentile(df, ps)\n\n    xs = np.sort(df)\n    ys = np.linspace(0, 1, len(df))\n\n    plt.plot(xs, ys * 100, label=\"ECDF\")\n    plt.plot(x_p, ps, label=\"Percentiles\", marker=\".\", ms=10)\n    plt.legend()\n    plt.ylabel(\"Percentile\")\n    plt.title(feature)\n    plt.show();\n\nfor feature in cont_FEATURES:\n    plot_cdf(train_df[feature], feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is perhaps the most revealing visualisations. It shows us that our features (especially '*cont1*' and '*cont5*') have unusual distributions. '*cont1*' appears to turn into an categorical variable when greater than 0.4 and '*cont4*' is a linear distribution once above 0.3. \n\nThis could suggest that these variables need to split into additional features or have functions applied to their values to create a bigger distinction between very similar values."},{"metadata":{},"cell_type":"markdown","source":"# Correlation\n\nHere we can look at the correlation between the features and each other (and the target)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This plots a 16x16 matrix of correlations between all the features and the target\n# Note: I sometimes comment this out because it takes a few minutes to run and doesn't show any useful information.\n\n# pd.plotting.scatter_matrix(train_df, figsize=(10, 10));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the above graph is far too busy to show us any useful information. However, at least we know that there isn't any clear correlations between a particular variable and the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(train_df.drop(columns=['id']).corr(), annot=True, cmap='viridis', fmt='0.2f', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above we can see a cluster of features (cont1, cont5-cont12) that appear to be quite highly correlated together. This suggests that dimensionality reduction techniques could be used to reduce these features to a smaller set."},{"metadata":{},"cell_type":"markdown","source":"# Analyse the Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x=train_df['target'], inner='quartile', bw=0.1)\nplt.title('target')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This doesn't show us much that is interesting other than the target is grouped around it's mean of 7.5, with some long tails out to either side."},{"metadata":{},"cell_type":"markdown","source":"Finally we will look at the 2D histogram plots for each features vs. the target, this can be a clue of unusual correlations between the target and features. \n\n**Note:** There is also code for a KDE plot but these take a long time to run."},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in cont_FEATURES:\n    #sns.kdeplot(x=train_df['target'], y=train_df[feature], bins=20, cmap='magma', shade=True) \n    plt.hist2d(x=train_df['target'], y=train_df[feature], bins=20)\n    plt.xlabel(feature)\n    plt.ylabel('target')\n    plt.title(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Takeaways + Future Work"},{"metadata":{},"cell_type":"markdown","source":"Takeaways:\n* None of the features are \"silver bullets\" for making accurate predictions\n* Outliers exist in the dataset but aren't common\n* A number of features could be combined using dimensionality reduction\n* Other features could be split into multiple features\n\nFuture Work:\n* I am going to create another notebook for feature engineering before feeding these into a number of different models.\n\nYou can find the next notebook here https://www.kaggle.com/tjcdev/baselines-with-eda-and-feature-engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}