{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"Table-Of-Contents\"></a>\n# Table Of Contents\n* [Table Of Contents](#Table-Of-Contents)\n* [Problem Statement](#Problem-Statement)\n    - [Introduction](#Introduction)\n    - [Goal](#Goal)\n    - [Evaluation Metrics](#Evaluation-Metrics)\n* [Importing Libraries](#Importing-Libs)\n* [Descriptive Statistics](#Descriptive-Statistics)\n* [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n    - [Categorical Features](#Categorical-Features)\n    - [Continuous Features](#Continuous-Features)\n    - [Target](#Target)\n* [Data Preprocessing](#Data-Preprocessing)\n    - [Train Test Split](#Train-Test-Split)\n    - [Transforms and Pipelines](#Transforms-and-Pipelines)\n* [Modelling](#Modelling)\n    - [Lasso Regression](#Lasso-Regression)\n    - [Random-forest Regression](#Random-Forest)\n    - [LightGBM Regression](#Light-GBM-Regressor)\n* [Submission](#Submission)"},{"metadata":{},"cell_type":"markdown","source":"# Problem Statement"},{"metadata":{},"cell_type":"markdown","source":"### Introduction\nThe dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features."},{"metadata":{},"cell_type":"markdown","source":"### Goal\nFor this competition, we will be predicting a continuous target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat9 are categorical, and the feature columns cont0 - cont13 are continuous. Hence, this is a regression task."},{"metadata":{},"cell_type":"markdown","source":"### Evaluation Metrics\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n \nwhere is the predicted value, is the original value, and is the number of rows in the test data."},{"metadata":{},"cell_type":"markdown","source":"# Importing Libs"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, LabelBinarizer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nfrom lightgbm import LGBMRegressor \nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score, cross_val_predict,KFold, GridSearchCV, train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.base import BaseEstimator, TransformerMixin, clone\nfrom datetime import datetime","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Descriptive Statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')\ndata_f = pd.read_csv(\"../input/tabular-playground-series-feb-2021/train.csv\")\ndata = data_f.copy()\ndata = data.iloc[:,1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cat = data.iloc[:,:10]\ndata_cont = data.iloc[:,10:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cat.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cont.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Null values in continuos variables:{}\\nNull values in categorical variables:{}\".format(data_cont.isna().sum().sum(), data_cat.isna().sum().sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Looking for trends in continuous features."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data_cont)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### No. of categories in each categorical variable.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale = 2)\nfig,ax = plt.subplots(5,2, figsize=(20,30), sharex=True)\naxes = ax.flatten()\nobject_bol = data_cat.dtypes == 'object'\nfor ax, catplot in zip(axes, data_cat.dtypes[object_bol].index):\n    sns.countplot(y=catplot, data=data_cat, ax=ax, order=data_cat[catplot].value_counts().index)\n    ax.xaxis.label.set_size(30)\n    ax.yaxis.label.set_size(30)\n    ax.tick_params(axis=\"x\", labelsize=15)\n    ax.tick_params(axis=\"x\", labelsize=15)\nplt.tight_layout()  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cat 9 and Cat 8 accommodate a larger variety compared to the rest of the features."},{"metadata":{},"cell_type":"markdown","source":"### Distributions of the continuous variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig,ax = plt.subplots(5,2, figsize=(12,12), sharex=False)\naxes = ax.flatten()\nobject_bol = data_cont.dtypes == 'float'\n# print(data_cont.dtypes[object_bol].index)\nfor ax, catplot in zip(axes, data_cont.dtypes[object_bol].index):\n    sns.kdeplot(x=data_cont[catplot],ax=ax,shade = True)\n    ax.xaxis.label.set_size(20)\n    ax.yaxis.label.set_size(20)\n    ax.tick_params(axis=\"x\", labelsize=15)\n    ax.tick_params(axis=\"y\", labelsize=15)\nplt.tight_layout()  \nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale = 1)\nfig, ax = plt.subplots(figsize = (15,15))\nsns.heatmap(data_cont.corr(),ax = ax,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no variable that is highly correlated to the target variable.<br>\nHowever, Cont 8, Cont 9 and Cont 12 are highly positively correlated to Cont5."},{"metadata":{},"cell_type":"markdown","source":"### Cumulative distribution function"},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')\nfig,ax = plt.subplots(5,2, figsize=(20,20), sharex=False)\naxes = ax.flatten()\nobject_bol = data_cont.dtypes == 'float'\nfor ax, catplot in zip(axes, data_cont.dtypes[object_bol].index):\n    res = sns.ecdfplot(data=data_cont,x=catplot,ax=ax)\n    ax.xaxis.label.set_size(20)\n    ax.yaxis.label.set_size(20)\n    ax.tick_params(axis=\"x\", labelsize=15)\n    ax.tick_params(axis=\"y\", labelsize=15)\nplt.tight_layout()  \nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Boxplots"},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')\nfig,ax = plt.subplots(10,1, figsize=(25,30), sharex=False)\naxes = ax.flatten()\nobject_bol = data_cont.dtypes == 'float'\nfor ax, catplot in zip(axes, data_cont.dtypes[object_bol].index):\n    res = sns.boxplot(x=catplot,ax=ax,data=data_cont)\n    ax.xaxis.label.set_size(30)\n    ax.yaxis.label.set_size(30)\n    ax.tick_params(axis=\"x\", labelsize=20) \nplt.tight_layout()  \nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cont 0, cont 2, cont 6, cont 8 have considerable outliers"},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing "},{"metadata":{},"cell_type":"markdown","source":"### Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data = pd.read_csv(\"../input/tabular-playground-series-feb-2021/train.csv\")\ntest_data = pd.read_csv(\"../input/tabular-playground-series-feb-2021/test.csv\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#id column is unnecessary\ntraining_data.drop(['id'],axis=1,inplace=True)\ntest_data.drop(['id'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = list(filter(lambda x: 'cat' in x, training_data.columns))\ncontinuous  = list(filter(lambda x: 'cat'not in x, training_data.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(training_data.iloc[:,:-1], training_data.iloc[:,-1], test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transforms and Pipelines"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_pipeline = ColumnTransformer([('encoder', OneHotEncoder(), [idx for idx,_ in enumerate(categorical)])], remainder='passthrough')\ncont_pipeline = ColumnTransformer([('scaler', StandardScaler(), [idx+10 for idx,_ in enumerate(continuous[:-1])])], remainder='drop')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_pipeline = FeatureUnion(transformer_list = [(\"Categorical_Pipeline\",cat_pipeline),\n                                                 (\"Quantitative_Pipeline\",cont_pipeline)])\n\nx_train = cat_pipeline.fit_transform(X_train)\nx_test = cat_pipeline.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{},"cell_type":"markdown","source":"Let us train these models:\n1. Lasso Regression\n2. Random Forest\n3. Light Gradient Boosted Machines\n"},{"metadata":{},"cell_type":"markdown","source":"### **Lasso Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to get crossvalidation metrics \ndef get_score(model, x=x_train, y=Y_train, cv = 5, verbose = False, ack=False):\n    scores = cross_val_score(model, x, y, scoring = 'neg_mean_squared_error',cv=cv,n_jobs=-1)\n    mean = np.mean(-scores)\n    std = np.std(-scores)\n    if verbose:\n        print(\"Scores: {}\\nMean: {:.4f}\\nStd: {:.4f}\".format(scores,mean,std))\n    if ack:\n        return scores,mean,std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look performance of default params \nlasso_model = Lasso()\nget_score(lasso_model,cv=10,verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Funtion to find optimal alpha\ndef find_opt_params_lasso(raange, x=x_train, y=Y_train, div_fac=1):\n    alphas = []\n    errors = []\n    for alpha in raange:\n        lr_model = Lasso(alpha= (alpha/div_fac))\n        lr_model.fit(x,y)\n        alphas.append(alpha/div_fac)\n        errors.append(get_score(lr_model,ack=True)[1])\n    return alphas, errors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot errors vs alphas \nalphas, errors = find_opt_params_lasso(range(1,100),div_fac = 1000)\nplt.plot(alphas,errors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Error is lowest when alpha is 0.0001 (without std sc) and 0.001 (otherwise) "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lasso regression\nlasso_model = Lasso(alpha=0.001)\nlasso_model.fit(x_train, Y_train)\n# get_score(lasso_model,cv=10,verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluation on test set\npredictions = lasso_model.predict(x_test)\nmean_squared_error(Y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Random Forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#raw model\nrf_model = RandomForestRegressor(n_jobs=-1)\nget_score(rf_model,y=Y_train.ravel(),verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'n_estimators':range(50,300,50),'max_features':('auto','sqrt','log2')}\nsearch_rf_params = RandomizedSearchCV(rf_model, parameters, n_jobs=-1, scoring='neg_mean_squared_error',cv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_rf_params.fit(x_train,Y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_rf_params.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestRegressor(n_estimators= 250, max_features= 'sqrt',n_jobs=-1)\nrf_model.fit(x_train, Y_train.ravel()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# default predictions\npredictions = rf_model.predict(x_test)\nmean_squared_error(Y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_test = rf_model.predict(cat_pipeline.transform(test_data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More parameters can be included in the search space to see improvement in performance."},{"metadata":{},"cell_type":"markdown","source":"### Light GBM Regressor\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/josephchan524/tabularplaygroundregressor-using-lightgbm-feb2021\ndef search_best_param(X,y,cat_features):\n    \n    trainXY = lgb.Dataset(data=X, label=y,categorical_feature = cat_features,free_raw_data=False)\n    \n    # define the lightGBM cross validation\n    def lightGBM_CV(max_depth, num_leaves, n_estimators, learning_rate, subsample, colsample_bytree, \n                    lambda_l1, lambda_l2, min_child_weight):\n\n            params = {'boosting_type': 'gbdt', 'objective': 'regression', 'metric':'rmse', 'verbose': -1,\n                      'early_stopping_round':100}\n\n            params['max_depth'] = int(round(max_depth))\n            params[\"num_leaves\"] = int(round(num_leaves))\n            params[\"n_estimators\"] = int(round(n_estimators))\n            params['learning_rate'] = learning_rate\n            params['subsample'] = subsample\n            params['colsample_bytree'] = colsample_bytree\n            params['lambda_l1'] = max(lambda_l1, 0)\n            params['lambda_l2'] = max(lambda_l2, 0)\n            params['min_child_weight'] = min_child_weight\n\n            score = lgb.cv(params, trainXY, nfold=5, seed=1, stratified=False, verbose_eval =False, metrics=['rmse'])\n\n            return -np.min(score['rmse-mean']) # min or max can change best_param\n\n    \n    # use bayesian optimization to search for the best hyper-parameter combination\n    # https://github.com/fmfn/BayesianOptimization/blob/master/bayes_opt/bayesian_optimization.pyta\n    lightGBM_Bo = BayesianOptimization(lightGBM_CV, \n                                      {\n                                          'max_depth': (5, 50),\n                                          'num_leaves': (20, 100),\n                                          'n_estimators': (50, 1000),\n                                          'learning_rate': (0.01, 0.3),\n                                          'subsample': (0.7, 0.8),\n                                          'colsample_bytree' :(0.5, 0.99),\n                                          'lambda_l1': (0, 5),\n                                          'lambda_l2': (0, 3),\n                                          'min_child_weight': (2, 50) \n                                      },\n                                       random_state = 1,\n                                       verbose = -1\n                                      )\n    np.random.seed(1)\n    \n    lightGBM_Bo.maximize(init_points=5, n_iter=25) # 20 combinations \n    \n    params_set = lightGBM_Bo.max['params']\n    \n    # get the params of the maximum target     \n    max_target = -np.inf\n    for i in lightGBM_Bo.res: # loop thru all the residuals \n        if i['target'] > max_target:\n            params_set = i['params']\n            max_target = i['target']\n    \n    params_set.update({'verbose': -1})\n    params_set.update({'metric': 'rmse'})\n    params_set.update({'boosting_type': 'gbdt'})\n    params_set.update({'objective': 'regression'})\n    \n    params_set['max_depth'] = int(round(params_set['max_depth']))\n    params_set['num_leaves'] = int(round(params_set['num_leaves']))\n    params_set['n_estimators'] = int(round(params_set['n_estimators']))\n    params_set['seed'] = 1 #set seed\n    \n    return params_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn\nwarnings.filterwarnings('ignore')\nclass MultiColumnLabelEncoder(BaseEstimator):\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n    \nmult_enc_pipeline = ColumnTransformer([('encoder',MultiColumnLabelEncoder(), [idx for idx,_ in enumerate(categorical)])], remainder='passthrough')\n# trans_x = MultiColumnLabelEncoder(columns = categorical).fit_transform(training_data)\nxtrain = pd.DataFrame(mult_enc_pipeline.fit_transform(training_data.iloc[:,:-1]),columns=training_data.columns[:-1])\nytrain = pd.DataFrame(training_data.iloc[:,-1],columns=['target'])\nbest_params = search_best_param(xtrain,ytrain,categorical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def K_Fold_LightGBM(X_train, y_train , cat_features, params_set, num_folds = 5):\n    num = 0\n    models = []\n    folds = KFold(n_splits=num_folds, shuffle=True, random_state=0)\n    type(X_train)\n        # 5 times \n    for n_fold, (train_idx, valid_idx) in enumerate (folds.split(X_train, y_train)):\n        print(f\"     model{num}\")\n        train_X, train_y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n        valid_X, valid_y = X_train.iloc[valid_idx], y_train.iloc[valid_idx]\n        \n        train_data=lgb.Dataset(train_X,label=train_y, categorical_feature = cat_features,free_raw_data=False)\n        valid_data=lgb.Dataset(valid_X,label=valid_y, categorical_feature = cat_features,free_raw_data=False)\n        \n        CV_LGBM = lgb.train(params_set,\n                 train_data,\n                 num_boost_round = 2500,\n                 valid_sets = valid_data,\n                 early_stopping_rounds = 100,\n                 verbose_eval = 50\n                 )\n        # increase early_stopping_rounds can lead to overfitting \n        models.append(CV_LGBM)\n        \n        print(\"Train set RMSE:\", mean_squared_error(train_y,models[num].predict(train_X),squared = False))\n        print(\" Test set RMSE:\", mean_squared_error(valid_y,models[num].predict(valid_X),squared = False))\n        print(\"\\n\")\n        num = num + 1\n        \n    return models\n\nlgbm_models = K_Fold_LightGBM(xtrain,ytrain,categorical,best_params,5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Light GBM generalizes better than random forest on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictLGBM = lgbm_models[3].predict(mult_enc_pipeline.transform(test_data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_submission_csv(predictions_test):\n    submission_csv = pd.read_csv(\"../input/tabular-playground-series-feb-2021/sample_submission.csv\")\n    submission_csv.drop('target',axis=1)\n    submission_csv['target']=predictions_test \n    submission_csv.to_csv('Result_{}.csv'.format(datetime.now().strftime(\"%d_%m_%Y_%H_%M\")),index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_submission_csv(predictions_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}