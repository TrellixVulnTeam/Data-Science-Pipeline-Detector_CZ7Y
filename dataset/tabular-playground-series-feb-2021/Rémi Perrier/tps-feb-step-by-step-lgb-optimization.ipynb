{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Libraries\nimport numpy as np\nimport pandas as pd\n\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport optuna\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\n\nimport shap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data\ndata = pd.read_csv('../input/tabular-playground-series-feb-2021/train.csv', index_col=0)\ntest = pd.read_csv('../input/tabular-playground-series-feb-2021/test.csv', index_col=0)\n\npreds = data.columns[:-1]\ntarget = data.columns[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing\ncat_cols = [col for col in preds if 'cat' in col]\ndata[cat_cols] = data[cat_cols].astype('category')\ntest[cat_cols] = test[cat_cols].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best params\nbest_params = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optimize n_estimators"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    # Search spaces\n    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n\n    # Evaluation\n    scores = []\n\n    kf = KFold(5)\n    for i, (train_idx, test_idx) in enumerate(kf.split(data)):\n        X_train = data.iloc[train_idx][preds]\n        y_train = data.iloc[train_idx][target]\n        X_test = data.iloc[test_idx][preds]\n        y_test = data.iloc[test_idx][target]\n\n        estimator = LGBMRegressor(n_estimators=n_estimators)\n\n        estimator.fit(X_train, \n                      y_train, \n                      eval_set=(X_test, y_test), \n                      eval_metric='rmse',\n                      categorical_feature=cat_cols,\n                      verbose=0)\n\n        y_pred = estimator.predict(X_test)\n        rmse = mean_squared_error(y_test, y_pred, squared=False)\n        scores.append(rmse)\n\n    return np.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, timeout=3600*0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best score\nstudy.best_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Historic\nplot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best params\nbest_params.update(study.best_params)\nbest_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optimize Tree Properties - Num_leaves"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    hyper_params = {\n        'num_leaves': trial.suggest_int('num_leaves', 1, 63),\n    }\n\n    # Evaluation\n    scores = []\n\n    kf = KFold(5)\n    for i, (train_idx, test_idx) in enumerate(kf.split(data)):\n        X_train = data.iloc[train_idx][preds]\n        y_train = data.iloc[train_idx][target]\n        X_test = data.iloc[test_idx][preds]\n        y_test = data.iloc[test_idx][target]\n\n        hyper_params.update(best_params)\n        \n        estimator = LGBMRegressor(**hyper_params)\n\n        estimator.fit(X_train, \n                      y_train, \n                      eval_set=(X_test, y_test), \n                      eval_metric='rmse',\n                      categorical_feature=cat_cols,\n                      verbose=0)\n\n        y_pred = estimator.predict(X_test)\n        rmse = mean_squared_error(y_test, y_pred, squared=False)\n        scores.append(rmse)\n\n    return np.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, timeout=3600*1.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best score\nstudy.best_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Historic\nplot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best params\nbest_params.update(study.best_params)\nbest_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Otpimize Tree Properties - Others"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    hyper_params = {\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf ', 1, 100),\n        'max_bin': trial.suggest_int('max_bin', 15, 2043)\n    }\n\n    # Evaluation\n    scores = []\n\n    kf = KFold(5)\n    for i, (train_idx, test_idx) in enumerate(kf.split(data)):\n        X_train = data.iloc[train_idx][preds]\n        y_train = data.iloc[train_idx][target]\n        X_test = data.iloc[test_idx][preds]\n        y_test = data.iloc[test_idx][target]\n\n        hyper_params.update(best_params)\n        \n        estimator = LGBMRegressor(**hyper_params)\n\n        estimator.fit(X_train, \n                      y_train, \n                      eval_set=(X_test, y_test), \n                      eval_metric='rmse',\n                      categorical_feature=cat_cols,\n                      verbose=0)\n\n        y_pred = estimator.predict(X_test)\n        rmse = mean_squared_error(y_test, y_pred, squared=False)\n        scores.append(rmse)\n\n    return np.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, timeout=3600*1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best score\nstudy.best_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Historic\nplot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importance\nplot_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best params\nbest_params.update(study.best_params)\nbest_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optimize regulation - Fraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial): \n    hyper_params = {\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 100),\n        'bagging_fraction': trial.suggest_float('bagging_fraction ', 0, 1.0),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0, 1.0)\n    }\n\n    # Evaluation\n    scores = []\n\n    kf = KFold(5)\n    for i, (train_idx, test_idx) in enumerate(kf.split(data)):\n        X_train = data.iloc[train_idx][preds]\n        y_train = data.iloc[train_idx][target]\n        X_test = data.iloc[test_idx][preds]\n        y_test = data.iloc[test_idx][target]\n\n        hyper_params.update(best_params)\n        \n        estimator = LGBMRegressor(**hyper_params)\n\n        estimator.fit(X_train, \n                      y_train, \n                      eval_set=(X_test, y_test), \n                      eval_metric='rmse',\n                      categorical_feature=cat_cols,\n                      verbose=0)\n\n        y_pred = estimator.predict(X_test)\n        rmse = mean_squared_error(y_test, y_pred, squared=False)\n        scores.append(rmse)\n\n    return np.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, timeout=3600*2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best score\nstudy.best_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Historic\nplot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importance\nplot_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best params\nbest_params.update(study.best_params)\nbest_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optimize Regulation - L1"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial): \n    hyper_params = {\n        'lambda_l1': trial.suggest_float('lambda_l1', 1E-12, 25, log=True)\n    }\n\n    # Evaluation\n    scores = []\n\n    kf = KFold(5)\n    for i, (train_idx, test_idx) in enumerate(kf.split(data)):\n        X_train = data.iloc[train_idx][preds]\n        y_train = data.iloc[train_idx][target]\n        X_test = data.iloc[test_idx][preds]\n        y_test = data.iloc[test_idx][target]\n\n        hyper_params.update(best_params)\n        \n        estimator = LGBMRegressor(**hyper_params)\n\n        estimator.fit(X_train, \n                      y_train, \n                      eval_set=(X_test, y_test), \n                      eval_metric='rmse',\n                      categorical_feature=cat_cols,\n                      verbose=0)\n\n        y_pred = estimator.predict(X_test)\n        rmse = mean_squared_error(y_test, y_pred, squared=False)\n        scores.append(rmse)\n\n    return np.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, timeout=3600*1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best score\nstudy.best_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Historic\nplot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best params\nbest_params.update(study.best_params)\nbest_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optimize regulation - Others"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial): \n    hyper_params = {\n        'lambda_l2': trial.suggest_float('lambda_l2', 1E-12, 20, log=True),\n        'path_smooth': trial.suggest_float('path_smooth', 1E-12, 20, log=True),\n        'cat_smooth': trial.suggest_float('cat_smooth', 1E-12, 20, log=True)\n    }\n\n    # Evaluation\n    scores = []\n\n    kf = KFold(5)\n    for i, (train_idx, test_idx) in enumerate(kf.split(data)):\n        X_train = data.iloc[train_idx][preds]\n        y_train = data.iloc[train_idx][target]\n        X_test = data.iloc[test_idx][preds]\n        y_test = data.iloc[test_idx][target]\n\n        hyper_params.update(best_params)\n        \n        estimator = LGBMRegressor(**hyper_params)\n\n        estimator.fit(X_train, \n                      y_train, \n                      eval_set=(X_test, y_test), \n                      eval_metric='rmse',\n                      categorical_feature=cat_cols,\n                      verbose=0)\n\n        y_pred = estimator.predict(X_test)\n        rmse = mean_squared_error(y_test, y_pred, squared=False)\n        scores.append(rmse)\n\n    return np.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, timeout=3600*1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best score\nstudy.best_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Historic\nplot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importance\nplot_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best params\nbest_params.update(study.best_params)\nbest_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation with low learning rate"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"# Evaluation\nk = 10\ntest[target] = 0\n\nscores = []\n\nkf = KFold(k)\nfor i, (train_idx, test_idx) in enumerate(kf.split(data)):\n    X_train = data.iloc[train_idx][preds]\n    y_train = data.iloc[train_idx][target]\n    X_test = data.iloc[test_idx][preds]\n    y_test = data.iloc[test_idx][target]\n\n    \n    best_params['learning_rate'] = 0.005\n    best_params['n_estimators'] = 100000\n    \n    estimator = LGBMRegressor(**best_params)\n\n    estimator.fit(X_train, \n                  y_train, \n                  eval_set=(X_test, y_test), \n                  eval_metric='rmse',\n                  early_stopping_rounds=1000,\n                  categorical_feature=cat_cols,\n                  verbose=1000)\n\n    y_pred = estimator.predict(X_test)\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    scores.append(rmse)\n\n    test[target] += estimator.predict(test[preds]) / k\n\ntest[target].to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Expected score: {np.mean(scores)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shap values - only applied on the last estimator\nexplainer = shap.TreeExplainer(estimator)\nshap_values = explainer.shap_values(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}