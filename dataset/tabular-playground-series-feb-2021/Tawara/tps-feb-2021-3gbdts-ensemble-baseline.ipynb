{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About\n\nI reused my notebook in Tabular Playground Series - Jan 2021:  \nhttps://www.kaggle.com/ttahara/tps-jan-2021-gbdts-baseline\n\n<br>\n\n* GBDT Models baseline\n    * LightGBM, XGBoost, CatBoost\n    * each model is trained by 10 folds cross validation using 3 seeds.\n\n\n* feature engineering\n    * **label-encoding** for category features\n    * no feature engineering for continuous features\n\n\n* inference test by **weighted** averaging 3 GBDT Models(10 folds & 3 seeds averaging)\n\n<br>\n\nThere is a lot of room for improvement such as feature engineering, parameter tuning, other models, and so on. enjoy ;) \n"},{"metadata":{},"cell_type":"markdown","source":"# Prepare"},{"metadata":{},"cell_type":"markdown","source":"## import libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport time\nimport random\nimport logging\nimport typing as tp\nfrom pathlib import Path\nfrom contextlib import contextmanager\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\n\nimport category_encoders as ce\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoost, Pool\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## read data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"ROOT = Path.cwd().parent\nINPUT = ROOT / \"input\"\nDATA = INPUT / \"tabular-playground-series-feb-2021\"\nWORK = ROOT / \"working\"\n\nfor path in DATA.iterdir():\n    print(path.name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(DATA / \"train.csv\")\ntest = pd.read_csv(DATA / \"test.csv\")\nsmpl_sub = pd.read_csv(DATA / \"sample_submission.csv\")\nprint(\"train: {}, test: {}, sample sub: {}\".format(\n    train.shape, test.shape, smpl_sub.shape\n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"@contextmanager\ndef timer(logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None):\n    if prefix: format_str = str(prefix) + format_str\n    if suffix: format_str = format_str + str(suffix)\n    start = time.time()\n    yield\n    d = time.time() - start\n    out_str = format_str.format(d)\n    if logger:\n        logger.info(out_str)\n    else:\n        print(out_str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(y_true, y_pred):\n    \"\"\"\"\"\"\n    return np.sqrt(np.mean((y_true - y_pred) ** 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TreeModel:\n    \"\"\"Wrapper for LightGBM/XGBoost/CATBoost\"\"\"\n    def __init__(self, model_type: str):\n        self.model_type = model_type\n        self.trn_data = None\n        self.val_data = None\n        self.model = None\n\n    def train(self,\n              params: dict,\n              X_train: pd.DataFrame, y_train: np.ndarray,\n              X_val: pd.DataFrame, y_val: np.ndarray,\n              train_weight: tp.Optional[np.ndarray] = None,\n              val_weight: tp.Optional[np.ndarray] = None,\n              train_params: dict = None,\n              cat_cols: list = None,\n            ):\n        if self.model_type == \"lgb\":\n            self.trn_data = lgb.Dataset(X_train, label=y_train, weight=train_weight)\n            self.val_data = lgb.Dataset(X_val, label=y_val, weight=val_weight)\n            self.model = lgb.train(params=params,\n                                   train_set=self.trn_data,\n                                   valid_sets=[self.trn_data, self.val_data],\n                                   **train_params)\n        elif self.model_type == \"xgb\":\n            self.trn_data = xgb.DMatrix(X_train, y_train, weight=train_weight)\n            self.val_data = xgb.DMatrix(X_val, y_val, weight=val_weight)\n            self.model = xgb.train(params=params,\n                                   dtrain=self.trn_data,\n                                   evals=[(self.trn_data, \"train\"), (self.val_data, \"val\")],\n                                   **train_params)\n        elif self.model_type == \"cat\":\n            self.trn_data = Pool(\n                X_train, label=y_train, cat_features=cat_cols)  #, group_id=[0] * len(X_train))\n            self.val_data = Pool(\n                X_val, label=y_val, cat_features=cat_cols)  #, group_id=[0] * len(X_val))\n            self.model = CatBoost(params)\n            self.model.fit(\n                self.trn_data, eval_set=[self.val_data], use_best_model=True, **train_params)\n        else:\n            raise NotImplementedError\n\n    def predict(self, X: pd.DataFrame):\n        if self.model_type == \"lgb\":\n            return self.model.predict(\n                X, num_iteration=self.model.best_iteration)  # type: ignore\n        elif self.model_type == \"xgb\":\n            X_DM = xgb.DMatrix(X)\n            return self.model.predict(\n                X_DM, ntree_limit=self.model.best_ntree_limit)  # type: ignore\n        elif self.model_type == \"cat\":\n            return self.model.predict(X)\n        else:\n            raise NotImplementedError\n\n    @property\n    def feature_names_(self):\n        if self.model_type == \"lgb\":\n            return self.model.feature_name()\n        elif self.model_type == \"xgb\":\n            return list(self.model.get_score(importance_type=\"gain\").keys())\n        elif self.model_type == \"cat\":\n             return self.model.feature_names_\n        else:\n            raise NotImplementedError\n\n    @property\n    def feature_importances_(self):\n        if self.model_type == \"lgb\":\n            return self.model.feature_importance(importance_type=\"gain\")\n        elif self.model_type == \"xgb\":\n            return list(self.model.get_score(importance_type=\"gain\").values())\n        elif self.model_type == \"cat\":\n            return self.model.feature_importances_\n        else:\n            raise NotImplementedError","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training & Inference"},{"metadata":{},"cell_type":"markdown","source":"## Config "},{"metadata":{"trusted":true},"cell_type":"code","source":"ID_COL = \"id\"\nCAT_COLS= [f\"cat{i}\" for i in range(10)]\nCONT_COLS = [f\"cont{i}\" for i in range(14)]\nTGT_COL = \"target\"\n\nN_SPLITS = 10\nRANDOM_SEED_LIST = [\n    42, 2021, 2,\n]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"use_feat_cols = []\ntrain_feat = train[[ID_COL]].copy()\ntest_feat = test[[ID_COL]].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### for categorical features\n\napply label encoding using [`category_encoders.OrdinalEncoder`](https://contrib.scikit-learn.org/category_encoders/ordinal.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"ord_enc = ce.OrdinalEncoder(cols=CAT_COLS)\ntrain_cat_feat = ord_enc.fit_transform(train[CAT_COLS])\ntest_cat_feat = ord_enc.transform(test[CAT_COLS])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feat = pd.concat([\n    train_feat, train_cat_feat], axis=1)\ntest_feat = pd.concat([\n    test_feat, test_cat_feat], axis=1)\nuse_feat_cols.extend(train_cat_feat.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### for continuous features\n\nUse them as they are"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cont_feat = train[CONT_COLS]\ntest_cont_feat = test[CONT_COLS]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feat = pd.concat([\n    train_feat, train_cont_feat], axis=1)\ntest_feat = pd.concat([\n    test_feat, test_cont_feat], axis=1)\nuse_feat_cols.extend(CONT_COLS)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train_feat.head().T","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"test_feat.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_train_and_inference(\n    X, X_test, y, use_model, model_params, train_params, seed_list, n_splits, cat_cols=None\n):\n    \n    oof_pred_arr = np.zeros(len(X))\n    test_pred_arr = np.zeros(len(X_test))\n    feature_importances = pd.DataFrame()\n    score_list = []\n    \n    for seed in seed_list:\n        if use_model == \"cat\":\n            model_params['random_state'] = seed\n        else:\n            model_params[\"seed\"] = seed\n        kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n        tmp_oof_pred = np.zeros(len(X))\n        tmp_test_pred = np.zeros(len(X_test))\n\n        for fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n            print(\"*\" * 100)\n            print(f\"Seed: {seed} - Fold: {fold}\")\n            X_trn = X.loc[trn_idx].reset_index(drop=True)\n            X_val = X.loc[val_idx].reset_index(drop=True)\n            y_trn = y[trn_idx]\n            y_val = y[val_idx]\n\n            model = TreeModel(model_type=use_model)\n            with timer(prefix=\"Model training\"):\n                model.train(\n                    params=model_params, X_train=X_trn, y_train=y_trn,\n                    X_val=X_val, y_val=y_val, train_params=train_params, cat_cols=cat_cols\n                )\n            with timer(prefix=\"Get Feature Importance\"):\n                fi_tmp = pd.DataFrame()\n                fi_tmp[\"feature\"] = model.feature_names_\n                fi_tmp[\"importance\"] = model.feature_importances_\n                fi_tmp[\"fold\"] = fold\n                fi_tmp[\"seed\"] = seed\n                feature_importances = feature_importances.append(fi_tmp)\n\n            with timer(prefix=\"Predict Valid\"):\n                val_pred = model.predict(X_val)\n                score = mean_squared_error(y_val, val_pred, squared=False)\n                # score = rmse(y_val, val_pred)\n                print(f\"score: {score:.5f}\")\n                score_list.append([seed, fold, score])\n                tmp_oof_pred[val_idx] = val_pred\n                tmp_test_pred += model.predict(X_test)\n            \n        oof_score = mean_squared_error(y, tmp_oof_pred, squared=False)\n        # oof_score = rmse(y, tmp_oof_pred)\n        print(f\"oof score: {oof_score: 5f}\")\n        score_list.append([seed, \"oof\", oof_score])\n\n        oof_pred_arr += tmp_oof_pred\n        test_pred_arr += tmp_test_pred / n_splits\n\n    oof_pred_arr /= len(seed_list)\n    test_pred_arr /= len(seed_list)\n    \n    oof_score = mean_squared_error(y, oof_pred_arr, squared=False)\n    # oof_score = rmse(y, oof_pred_arr)\n    score_list.append([\"avg\", \"oof\", oof_score])\n    score_df = pd.DataFrame(\n        score_list, columns=[\"seed\", \"fold\", \"rmse score\"])\n    \n    return oof_pred_arr, test_pred_arr, score_df, feature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_feat[use_feat_cols]\nX_test = test_feat[use_feat_cols]\n\ny = train[TGT_COL].values\n\nprint(f\"train_feat: {X.shape}, test_feat: {X_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cat = X.copy()\nX_cat[CAT_COLS] = train[CAT_COLS]\nX_test_cat = X_test.copy()\nX_test_cat = test[CAT_COLS]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_PARAMS = {\n    \"lgb\": {\n        \"objective\": \"root_mean_squared_error\",\n        \"boosting\": \"gbdt\",\n        \"max_depth\": 8,\n        \"learning_rate\": 0.005,\n        \"colsample_bytree\": 0.2,\n        \"subsample\": 0.8,\n        \"subsample_freq\": 6,\n        \"reg_alpha\": 20,\n        \"min_data_in_leaf\": 200,\n        \"n_jobs\": 2,\n        \"seed\": RANDOM_SEED_LIST[0],\n        # \"device\": \"gpu\",\n        # \"gpu_device_id\": 0\n    },\n    \"xgb\": {\n        \"objective\": \"reg:squarederror\",\n        \"max_depth\": 8,\n        \"learning_rate\": 0.003,\n        \"colsample_bytree\": 0.2,\n        \"subsample\": 0.8,\n        \"reg_alpha\" : 6,\n        \"min_child_weight\": 200,\n        \"n_jobs\": 2,\n        \"seed\": RANDOM_SEED_LIST[0],\n        'tree_method': \"gpu_hist\",\n        \"gpu_id\": 0,\n    },\n    \"cat\": {\n        'loss_function': 'RMSE',\n        \"max_depth\": 4,\n        'learning_rate': 0.03,\n        \"bootstrap_type\": 'Poisson',\n        \"subsample\": 0.8,\n        \"border_count\": 512,\n        \"l2_leaf_reg\": 200,\n        'random_state': RANDOM_SEED_LIST[0],\n        \"thread_count\": 2,\n        \"task_type\": \"GPU\",\n        \"devices\" : \"0\",\n        'num_boost_round': 50000,\n    }\n}\nTRAIN_PARAMS = {\n    \"lgb\": {\n        \"num_boost_round\": 50000,\n        \"early_stopping_rounds\": 200,\n        \"verbose_eval\": 200,\n    },\n    \"xgb\": {\n        \"num_boost_round\": 50000,\n        \"early_stopping_rounds\": 200,\n        \"verbose_eval\":  200,\n    },\n    \"cat\": {\n        'early_stopping_rounds': 200,\n        'verbose_eval': 200,\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LightGBM"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"oof_pred_lgb, test_pred_lgb, score_lgb, feat_imps_lgb = run_train_and_inference(\n    X, X_test, y, \"lgb\", MODEL_PARAMS[\"lgb\"], TRAIN_PARAMS[\"lgb\"], RANDOM_SEED_LIST, N_SPLITS)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"score_lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_lgb.loc[score_lgb.fold == \"oof\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"order = list(feat_imps_lgb.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feat_imps_lgb, order=order)\nplt.title(\"{} importance\".format(\"lgb\"))\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"oof_pred_xgb, test_pred_xgb, score_xgb, feat_imps_xgb = run_train_and_inference(\n    X, X_test, y, \"xgb\", MODEL_PARAMS[\"xgb\"], TRAIN_PARAMS[\"xgb\"], RANDOM_SEED_LIST, N_SPLITS)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"score_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_xgb.loc[score_xgb.fold == \"oof\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"order = list(feat_imps_xgb.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feat_imps_xgb, order=order)\nplt.title(\"{} importance\".format(\"xgb\"))\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CatBoost"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"oof_pred_cat, test_pred_cat, score_cat, feat_imps_cat = run_train_and_inference(\n    X, X_test, y, \"cat\", MODEL_PARAMS[\"cat\"], TRAIN_PARAMS[\"cat\"],\n    RANDOM_SEED_LIST, N_SPLITS,)  #cat_cols=list(range(10)))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"score_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_cat.loc[score_cat.fold == \"oof\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"order = list(feat_imps_cat.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feat_imps_cat, order=order)\nplt.title(\"{} importance\".format(\"cat\"))\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensemble LGB, XGB, Cat"},{"metadata":{},"cell_type":"markdown","source":"### check correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_names = [\"lgb\", \"xgb\", \"cat\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # prediction for oof\npd.DataFrame(\n    np.corrcoef([\n        oof_pred_lgb,\n        oof_pred_xgb,\n        oof_pred_cat\n    ]),\n    columns=model_names, index=model_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # prediction for test\npd.DataFrame(\n    np.corrcoef([\n        test_pred_lgb,\n        test_pred_xgb,\n        test_pred_cat\n    ]),\n    columns=model_names, index=model_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### simple averaging"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_pred_avg = (oof_pred_lgb + oof_pred_xgb + oof_pred_cat) / 3\noof_score_avg = mean_squared_error(y, oof_pred_avg, squared=False)\n\nprint(f\"oof score avg: {oof_score_avg:.5f}\")\n\ntest_pred_avg = (test_pred_lgb + test_pred_xgb + test_pred_cat) / 3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### weighted averaging"},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = [0.5, 0.4, 0.1]\n\noof_pred_wavg = weights[0] * oof_pred_lgb + weights[1] * oof_pred_xgb + weights[2] * oof_pred_cat\noof_score_wavg = mean_squared_error(y, oof_pred_wavg, squared=False)\n\nprint(f\"oof score weighted avg: {oof_score_wavg:.6f}\")\n\ntest_pred_wavg = weights[0] * test_pred_lgb + weights[1] * test_pred_xgb + weights[2] * test_pred_cat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = smpl_sub.copy()\n# sub[TGT_COL] = test_pred_avg\nsub[TGT_COL] = test_pred_wavg\n\nsub.to_csv(\"submission.csv\", index=False)\n\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}