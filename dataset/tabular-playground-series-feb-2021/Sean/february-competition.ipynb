{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you're taking the time to look at this file, thank you! This is a synposis of the relatively basic approach I took to February's Competition. It was my first real work for a Kaggle competition, and I think I learned a lot from it. I've cleaned up my work, and this is essentially what it boiled down to. I would love to hear any comments you might have, especially if there's anything I could've done better.","metadata":{}},{"cell_type":"markdown","source":"Beginning with reading in the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(r'../input/tabular-playground-series-feb-2021/train.csv')\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's apparently 26 columns, so this will be easier than trying to look at a big table.","metadata":{}},{"cell_type":"code","source":"columns = [x for x in df]\ncolumns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for missing values in the dataset\n\ndf.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given there are 13  continuous variables and one continuous target, I want to examine what the distributions look like. Below I've created boxplots for the different variables.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, axes = plt.subplots(5,3, figsize=(9, 13), dpi=60)\nplt.subplots_adjust(left=2, bottom=2, right=3, top=3)\n\nsns.boxplot(ax=axes[0,0], x='target', data=df)\nsns.boxplot(ax=axes[0,1], x='cont0', data=df)\nsns.boxplot(ax=axes[0,2], x='cont1', data=df)\n\nsns.boxplot(ax=axes[1,0], x='cont2', data=df)\nsns.boxplot(ax=axes[1,1], x='cont3', data=df)\nsns.boxplot(ax=axes[1,2], x='cont4', data=df)\n\nsns.boxplot(ax=axes[2,0], x='cont5', data=df)\nsns.boxplot(ax=axes[2,1], x='cont6', data=df)\nsns.boxplot(ax=axes[2,2], x='cont7', data=df)\n\nsns.boxplot(ax=axes[3,0], x='cont8', data=df)\nsns.boxplot(ax=axes[3,1], x='cont9', data=df)\nsns.boxplot(ax=axes[3,2], x='cont10', data=df)\n\nsns.boxplot(ax=axes[4,0], x='cont11', data=df)\nsns.boxplot(ax=axes[4,1], x='cont12', data=df)\nsns.boxplot(ax=axes[4,2], x='cont13', data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at this data, it definitely appears this data has already been normed in some way.\n\nAfter looking at this data, I wondered if there was any correlation between target and one of the continuous variables. Below is a correlation matrix for the different variables, and while there's some correlation between the variables, there's not any linear relationship between the target variable and the continuous variables.","metadata":{}},{"cell_type":"code","source":"from matplotlib.pyplot import figure \n\n# Makes the figure larger\nfigure(num=None, figsize=(20, 15), dpi=80)\n\n# calculating the correlation values\ncorr = df.corr()\n\n# Creating a mask to eliminate the annoying doubles of a square correlation matrix\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Creating the graph with the indicated values\nsns.heatmap(corr, mask=mask, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this point, I spent a while looking at the histograms of some of these distributions in addition trying several linear models for the dataset, such as least squares, ridge regression, lasso regression, and elasticnet. Below is the pandas profile report I used to look through the variables individually before moving on to using XGBoost.","metadata":{}},{"cell_type":"code","source":"from pandas_profiling import ProfileReport\n\ndf_profile = ProfileReport(df, 'EDA')\ndf_profile","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For XGB, I one-hot encoded the categorical variables.\n\n# Categorical Data\ncat_columns = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\n\n# One-Hot encoding the categorical variables\ndf_one_hot = pd.get_dummies(df, columns=cat_columns)\n\ndf_one_hot.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I tried using Optuna and a few other functions to find the best hyperparameters, but in the end, I created a custom function to report the best parameters. The code I used is below.","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\ndef depthandlearning(learn, depth, min_child_weight, n_estimators):\n    \n    model = XGBRegressor(base_score=0.5, \n                         booster='gbtree', \n                         colsample_bylevel=1,\n                         colsample_bynode=1, \n                         colsample_bytree=1, \n                         gamma=0, \n                         importance_type='gain',\n                         learning_rate=learn, # Testing learning rate\n                         max_delta_step=0, \n                         max_depth=depth, # Testing the depth\n                         min_child_weight=min_child_weight, # Testing the child weight\n                         missing=None, \n                         n_estimators=n_estimators, #Testing the number of estimators\n                         n_jobs=1, \n                         nthread=None, \n                         objective='reg:squarederror',\n                         random_state=0, \n                         reg_alpha=0, \n                         reg_lambda=1, \n                         scale_pos_weight=1, \n                         seed=None,\n                         silent=None, \n                         subsample=1, \n                         verbosity=0) # I left it as verbose to verify results originally\n    \n    # Initialize the model on training data\n    model.fit(X_train, y_train)\n    \n    # Predict the outputs using test data\n    y_pred = model.predict(X_test)\n    \n    # Training Score\n    training_score = model.score(X_train, y_train)\n    \n    # Testing Score\n    test_score = model.score(X_test, y_test)\n\n    # RMSE -- this was the metric used for the competition\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    \n    # This is creating a list object to return the different values used\n    parameters = []\n    parameters = [test_score]\n    parameters.append(rmse)\n    parameters.append(learn)\n    parameters.append(depth)\n    parameters.append(min_child_weight)\n    parameters.append(n_estimators)\n    \n    return parameters\n\n    \nX = df_one_hot.drop(['target'], axis=1)\nY = df_one_hot['target']\n\n# 30% testing, 70% training\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n\n# Scale the data for better results -- small increase in performance\nscaler = StandardScaler()\nscaler.fit(X_train)\nX = scaler.transform(X)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n# These are some of the last values I used\n# The learning rate is very high\n# I originally was testing 0.01, but these performed better\nlearn = [0.2, 0.2125, 0.225] \ndepth = [4, 5, 6]\nmin_child_weight = [.5, .75, 1]\nn_estimators = [100, 200, 500]\n\nbest_score = 0\n\nfor i in range(len(learn)):\n    for j in range(len(depth)):\n        for k in range(len(min_child_weight)):\n            for l in range(len(n_estimators)):\n                parameters = depthandlearning(learn[i], \n                                              depth[j], \n                                              min_child_weight[k],\n                                              n_estimators[l])\n                \n                # By using an if statement, the best RMSE will end up setting the values\n                if parameters[0] > best_score:\n                    best_score = parameters[0]\n                    best_score_std = parameters[1]\n                    best_learn = parameters[2]\n                    best_depth = parameters[3]\n                    best_min_child_weight = parameters[4]\n                    best_n_estimators = parameters[5]\n        \n\nprint('Highest Score: %.4f' % best_score)\nprint('Highest Score RMSE: %.4f' % best_score_std)\nprint('Highest Score Learning: ', best_learn)\nprint('Highest Score Depth: ', best_depth)\nprint('Highest Score min_child_weight: ', best_min_child_weight)\nprint('Highest Score n_estimators: ', best_n_estimators)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Highest Score: 0.0925\n\nHighest Score RMSE: 0.8475\n\nHighest Score Learning:  0.2125\n\nHighest Score Depth:  4\n\nHighest Score min_child_weight:  0.5\n\nHighest Score n_estimators:  200\n\nAbove are the parameters that scored the highest with the training dataset and the testing dataset. I'm going to run this one more time to examine the feature importance, and then I'll train the model on all the data available before trying to make predictions about the testing dataset for this competition.","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\nmodel = XGBRegressor(base_score=0.5, \n                     booster='gbtree', \n                     colsample_bylevel=1,\n                     colsample_bynode=1, \n                     colsample_bytree=1, \n                     gamma=0, \n                     importance_type='gain',\n                     learning_rate=.2125, \n                     max_delta_step=0, \n                     max_depth=4, \n                     min_child_weight=.5,\n                     missing=None, \n                     n_estimators=200, \n                     n_jobs=1, \n                     nthread=None, \n                     objective='reg:squarederror',\n                     random_state=0, \n                     reg_alpha=0, \n                     reg_lambda=1, \n                     scale_pos_weight=1, \n                     seed=None,\n                     silent=None, \n                     subsample=1, \n                     verbosity=1)\n\nX = df_one_hot.drop(['id', 'target'], axis=1)\nY = df_one_hot['target']\n\n#30% testing, 70% training\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n\n# Scale the data for slightly better results\nscaler = StandardScaler()\nscaler.fit(X_train)\nX = scaler.transform(X)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n    \ntraining_score = model.score(X_train, y_train)\nprint('Training Score: ', training_score)\n\ntest_score = model.score(X_test, y_test)\nprint('Test Score: ', test_score)\n\n# MSE\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(\"RMSE: %.4f\" % rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's always interesting to see what the model values when making a prediction. Since we have so many columns, I'm only examining the top 15 most important variables.","metadata":{}},{"cell_type":"code","source":"from matplotlib.pyplot import figure\nimport matplotlib.pyplot as plt \n\nfigure(figsize=(8, 4), dpi=80)\n\ndef feat_imp(df, model, n_features):\n\n    d = dict(zip(df.columns, model.feature_importances_))\n    ss = sorted(d, key=d.get, reverse=True)\n    top_names = ss[0:n_features]\n\n    plt.figure(figsize=(8,4))\n    plt.title(\"Feature Importances\", fontsize=16)\n    plt.bar(range(n_features), [d[i] for i in top_names], color=\"r\", \n                                                          edgecolor='black', \n                                                          align=\"center\")\n    \n    plt.xlim(-1, n_features)\n    plt.xticks(range(n_features), top_names, rotation='vertical')\n    plt.xlabel('Features', fontsize=14)\n\nfeat_imp(df_one_hot, model, 15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, this particular dataset seems to find that several categorical values are the most important, and it values two of them significantly more than anything else.\n\nI then trained the data on the entire training set for the comepetition and submitted the predicted results.","metadata":{}}]}