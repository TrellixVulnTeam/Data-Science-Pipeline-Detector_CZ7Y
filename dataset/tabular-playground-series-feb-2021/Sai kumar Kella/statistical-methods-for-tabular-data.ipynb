{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Statistical methods for the Machine learning"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T02:39:30.33768Z","start_time":"2021-02-20T02:39:20.532035Z"},"trusted":true},"cell_type":"code","source":"## importing the libraires\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sbn","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T02:39:46.330076Z","start_time":"2021-02-20T02:39:43.092106Z"},"trusted":true},"cell_type":"code","source":"## loading the datasets\ntrain_data=pd.read_csv(\"../input/tabular-playground-series-feb-2021/train.csv\")\ntest_data=pd.read_csv(\"../input/tabular-playground-series-feb-2021/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T02:39:49.325048Z","start_time":"2021-02-20T02:39:49.080289Z"},"trusted":false},"cell_type":"code","source":"## dispaying the datasets\ndisplay(train_data.head())\ndisplay(test_data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Understanding\n- To get the insigits of the data we need to use the some statistical methods \n        - visualization.\n        - Summary statistics (if the data is not normal distribution then we will use 5 summary statistics)\n        - if the data is distributed in gaussian then we will have the parameter likw mean and standarded deviation.\n        "},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T02:39:49.640738Z","start_time":"2021-02-20T02:39:49.541815Z"},"trusted":false},"cell_type":"code","source":"print(\"Number of Rows in the training :\",train_data.shape[0])\nprint(\"Number of Rows in the testing :\",test_data.shape[0])\nprint(\"Number of Categorical columns :\",train_data.select_dtypes('object').shape[1])\nprint(\"Number of NUmerical Features :\",train_data.select_dtypes('float').shape[1]-1)\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T02:39:50.107718Z","start_time":"2021-02-20T02:39:50.08494Z"},"trusted":false},"cell_type":"code","source":"### lets understand the NUmerical Features \n## Here i am going to analyze only \nfloat_feat=train_data.select_dtypes('float').columns.tolist()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets Do Some Descriptive Stats \n\n**A Summary statistics**\n\n- Here i just assuming that the we don't know the distribution of the data.\n- Yeah if we know the distribution we can use some parameters and describe.\n- These 5 statistics will give some information about the sample but not about the population.\n"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T02:39:51.297127Z","start_time":"2021-02-20T02:39:50.937725Z"},"trusted":false},"cell_type":"code","source":"### Quantiles (Percentails of the Training dataset)\ntrain_data[float_feat].describe().T.iloc[:,3:]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T02:39:55.428551Z","start_time":"2021-02-20T02:39:51.401961Z"},"trusted":false},"cell_type":"code","source":"## Lets visualize the data \n\nplt.figure(figsize=(15,3))\nfor i in range(7):\n    plt.subplot(170+i+1)\n    sbn.kdeplot(data=train_data[float_feat[i]],shade=True,cbar=True)\nplt.show()\nplt.figure(figsize=(15,3))\nfor i in range(7):\n    plt.subplot(170+i+1)\n    sbn.kdeplot(train_data[float_feat[i+7]],shade=True,color=\"red\")\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n- Seems to be the Features are unkown distribution.\n- **We need to check the distributions if it is following the Gaussian or not**\n\n\n\n\n## Data Preparation\n\n### Many Machine Learning Algorithms Performs better when the observation in a variable is drawn from the Gaussian Distribution.\n\n- Some algorithms like linear Regressions will assume that the observations are drawn from the Gaussian distributions . So we can easily apply the parametric statistical methods.\n\n**we will follow some procedure**\n- step 1:Do Normality check \n- step 2: If the obesrvations from the Gaussian then we will do some parametric statistical models\n- Step 3: If we fails in normality check then try to transform the features again if we fails \n- Step 4: Apply Non- parametric Stastical Method for furtur steps.\n"},{"metadata":{},"cell_type":"markdown","source":"### Normality Test\n\n- There are 2 ways to Test the Normality .\n               1.By Graphical methods (for this we will use the Q-Q plot)\n               2. we can also use the Significns Test eg : k^2 Test\n               \n**It just like an decision point to use the parameter statistical methods or to use the non parametric statistical methods**"},{"metadata":{},"cell_type":"markdown","source":"#### 1. Q-Q plot \n\n- Interpret the Q_Q plots\n![image](../input/images/qqplot.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\n\nImage(filename='../input/images/qqplot.png')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T02:39:55.98312Z","start_time":"2021-02-20T02:39:55.431662Z"},"trusted":false},"cell_type":"code","source":"### loading the statsmodels\nfrom statsmodels.graphics.gofplots import qqplot","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T02:40:09.641978Z","start_time":"2021-02-20T02:39:55.986948Z"},"scrolled":false,"trusted":false},"cell_type":"code","source":"## plotting the first 7 columns in the dataset \n\nqqplot(train_data[float_feat[0]],line=\"s\",color=\"green\")\nplt.title(float_feat[0],size=15)\nplt.show()\nqqplot(train_data[float_feat[1]],line=\"s\",color=\"green\")\nplt.title(float_feat[1],size=15)\nplt.show()\nqqplot(train_data[float_feat[2]],line=\"s\",color=\"green\")\nplt.title(float_feat[2],size=15)\nplt.show()\nqqplot(train_data[float_feat[3]],line=\"s\",color=\"green\")\nplt.title(float_feat[3],size=15)\nplt.show()\nqqplot(train_data[float_feat[4]],line=\"s\",color=\"green\")\nplt.title(float_feat[4],size=15)\nplt.show()\nqqplot(train_data[float_feat[5]],line=\"s\",color=\"green\")\nplt.title(float_feat[5],size=15)\nplt.show()\nqqplot(train_data[float_feat[6]],line=\"s\",color=\"green\")\nplt.title(float_feat[6],size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- cont0 has resemblace of gaussian with deviation\n- But lot of features are exhiting the bimodels with the skew"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T02:40:25.421342Z","start_time":"2021-02-20T02:40:11.760225Z"},"scrolled":false,"trusted":false},"cell_type":"code","source":"### Plotting teh next 7 continuous columns in the dataset \nqqplot(train_data[float_feat[7]],line=\"s\",color=\"cyan\")\nplt.title(float_feat[7],size=15)\nplt.show()\n\nqqplot(train_data[float_feat[8]],line=\"s\",color=\"cyan\")\nplt.title(float_feat[8],size=15)\nplt.show()\nqqplot(train_data[float_feat[9]],line=\"s\",color=\"cyan\")\nplt.title(float_feat[9],size=15)\nplt.show()\n\nqqplot(train_data[float_feat[10]],line=\"s\",color=\"cyan\")\nplt.title(float_feat[10],size=15)\nplt.show()\n\nqqplot(train_data[float_feat[11]],line=\"s\",color=\"cyan\")\nplt.title(float_feat[11],size=15)\nplt.show()\n\nqqplot(train_data[float_feat[12]],line=\"s\",color=\"cyan\")\nplt.title(float_feat[12],size=15)\nplt.show()\n\nqqplot(train_data[float_feat[13]],line=\"s\",color=\"cyan\")\nplt.title(float_feat[13],size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- cont9, cont11,cont12 ,cont13 are exhiting the bimodels with slighter deviations and some amout of skewness.\n "},{"metadata":{},"cell_type":"markdown","source":"### Lets do Significant Test for Normality check\n\n- There are some statistical methods for the Normality checks\n                  1.Shapiro-Wilk Test\n                  2. D’Agostino’s K^2 Test\n                  3. Anderson-Darling Test\n                  \n   **Null Hypothsis : The distribution of data is normal**\n   \n   **Alternative Hypothesis : The distribution of data is not normal**"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T02:40:32.630432Z","start_time":"2021-02-20T02:40:32.62692Z"},"trusted":false},"cell_type":"code","source":"###  importing the libraries \nfrom scipy.stats import shapiro,anderson\nfrom scipy.stats import normaltest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. Shapiro-Wilk Test \n- Generally this test works for only the small samples \n- this doesn't work accurately when we have sample size greater than 5000"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T02:40:34.493692Z","start_time":"2021-02-20T02:40:34.360926Z"},"trusted":false},"cell_type":"code","source":"\nstat, p = shapiro(train_data['cont9'])\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\nelse:\n    print('Sample does not look Gaussian (reject H0)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.D'Agostino's k^2 Test\n- This test uses 2 parameters are : Skewness and Kurotisis are used to check the normality.\n- Ho : distribution is normal\n- H1 : distribution is not normal"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T02:40:36.321305Z","start_time":"2021-02-20T02:40:36.03256Z"},"scrolled":true,"trusted":false},"cell_type":"code","source":"for i in float_feat:\n    print(\"Feature : \",i)\n    print(\"=\"*50)\n    stat, p = normaltest(train_data[i])\n    print('Statistics=%.3f, p=%.3f' % (stat, p))\n    # interpreting  the results\n    alpha = 0.05\n    if p > alpha:\n        print('Sample looks Gaussian (fail to reject H0)')\n    else:\n        print('Sample does not look Gaussian (reject H0)')\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Anderson - Darling Test\n- It is used to evalute or test for many known distributions .\n- It will returns list of critical values\n- Here we will choose the critical values as a significance level"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T02:40:39.715202Z","start_time":"2021-02-20T02:40:37.831982Z"},"scrolled":false,"trusted":false},"cell_type":"code","source":"for i in float_feat:\n    print(\"Feature :\",i)\n    print(\"-\"*50)\n    result = anderson(train_data[i])\n    print('Statistic: %.3f' % result.statistic)\n    for i in range(len(result.critical_values)):\n        sl, cv = result.significance_level[i], result.critical_values[i]\n        if result.statistic < result.critical_values[i]:\n            print('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n        else:\n            print('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using these statistical significance methods there is no feature is following the normal distribution**\n\n**So For any test we need to use Non-parametric statistical Methods**"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking the distribution of Training and Testing Dataset\n- we need to ensure that the Train data and test data were came from the same distribution or not.\n- we can do this by 2 ways:\n            1. BY visualization (plotting the values) : this is better when we have large amount of data so we can approximate to the population.\n            2. Using statstical significance test \n         \n Here i will use the statistical significance test to checking disrtibution of training and testing dataset .\n \n - There will two types of test :\n             1. Parametric hypothesis test\n             2. Non parametric Hypothesis test \n             \n When the data was following the Gaussian distribution then we need to use the parametric hypothesis test otherwise we need to go for the Non parametric Hypothesis tests..\n \n Here i am using the Non-parametric Hypothesis testing:(these will work with out assuming the distribution)\n \n         1.Mann-Whitney U test (comparing the two independent data samples) \n         2.Wilcoxon Signed Rank Test (Comparing two dependent data samples)\n         3.Krushkal-Wallis H test (Comparing More than 2 indpendent data samples)\n         4.FriedMan Test (Compairng More than 2 independent data samples)"},{"metadata":{},"cell_type":"markdown","source":"### 1. Mann-Whitney U Test\n- Null Hypothesis (Ho) : data samples are drawn from same distribution\n- Alternative Hypothesis (H1) : Data samples are belongs to different distributions "},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T03:50:48.485651Z","start_time":"2021-02-20T03:50:48.467408Z"},"trusted":false},"cell_type":"code","source":"from scipy.stats import mannwhitneyu","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T04:16:05.985487Z","start_time":"2021-02-20T04:16:02.76497Z"},"trusted":false},"cell_type":"code","source":"for i in float_feat[:-1]:\n    print(\"Feature :\",i)\n    print(\"-\"*50)\n    stats,p=mannwhitneyu(train_data[i],test_data[i])\n    print(\"Statistic :{0:.3f} p-value : {1:.3f}\".format(stats,p))\n    alpha=0.05\n    if(p>alpha):\n        print(\"Same Distribution Ho accepted\")\n    else:\n        print(\"Different Distribution H1 accepted\")\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations :**\n- Here all the features of training and testing were sampled from the same distribution\n- But the Features : cont1 train and test data samples are belongs to different data samples "},{"metadata":{},"cell_type":"markdown","source":"**Solutions (These are my general assumtion not to be Sure)**\n- we can drop that feature\n- we can use covariate shifts"},{"metadata":{},"cell_type":"markdown","source":"## Lets try to transform the Features in to Gaussian \n\nGenerally the Linear regression will explicitly assume that the input variables are drawn from the Gaussian . But Non- linear algorithms doesn't have this kind of assumptions yet these algoithms perform better when we have a Gaussian Variables.\n\n- we will use power transforms to tranform the features in to Gaussian\n**Note : Generally it will works when we hava a data with the nearly Gaussian with outlier or skewed so then we can easily transform the data in to Gaussian**\n\n    The methods are :\n        - we can tranform feature by log or squar root of a variable (this will decrease the skewness of the variable)\n        - we can also apply the automatic power transforms like \n                    - Box-cox tranform\n                    - Yeo-Jhonson transform"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T05:41:48.403056Z","start_time":"2021-02-20T05:41:48.399248Z"},"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1 Box-cox Transform :\n- To apply this transform the data must be strictly positive\n- So here to get the positive numbers i am going to use Min-Max scale"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T05:46:29.157544Z","start_time":"2021-02-20T05:46:17.991465Z"},"trusted":false},"cell_type":"code","source":"min_max=MinMaxScaler(feature_range=(1,2))\npow_trans=PowerTransformer(method=\"box-cox\")\ndata=min_max.fit_transform(train_data[float_feat].values)\ndata_trans=pow_trans.fit_transform(data)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T05:49:17.592644Z","start_time":"2021-02-20T05:49:13.844463Z"},"trusted":false},"cell_type":"code","source":"## Lets visualize the data \nframe=pd.DataFrame(data_trans,columns=float_feat)\n\n\nplt.figure(figsize=(15,3))\nfor i in range(7):\n    plt.subplot(170+i+1)\n    sbn.kdeplot(data=frame[float_feat[i]],shade=True,cbar=True)\nplt.show()\nplt.figure(figsize=(15,3))\nfor i in range(7):\n    plt.subplot(170+i+1)\n    sbn.kdeplot(frame[float_feat[i+7]],shade=True,color=\"red\")\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T05:46:54.246704Z","start_time":"2021-02-20T05:46:54.230025Z"}},"cell_type":"markdown","source":"- Seems to be there is no use of applying Transforms . "},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Yeo-jhonson transform\n- No need to scale the data .it will also works for both positive or negitive values"},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T05:54:37.462434Z","start_time":"2021-02-20T05:54:26.841283Z"},"trusted":false},"cell_type":"code","source":"pow_trans=PowerTransformer(method='yeo-johnson')\ndata_trans=pow_trans.fit_transform(train_data[float_feat])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-02-20T05:55:29.274442Z","start_time":"2021-02-20T05:55:25.491978Z"},"trusted":false},"cell_type":"code","source":"## Lets visualize the data \nframe=pd.DataFrame(data_trans,columns=float_feat)\n\n\nplt.figure(figsize=(15,3))\nfor i in range(7):\n    plt.subplot(170+i+1)\n    sbn.kdeplot(data=frame[float_feat[i]],shade=True,cbar=True)\nplt.show()\nplt.figure(figsize=(15,3))\nfor i in range(7):\n    plt.subplot(170+i+1)\n    sbn.kdeplot(frame[float_feat[i+7]],shade=True,color=\"red\")\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NO much difference in these transoforms**"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}