{"cells":[{"metadata":{},"cell_type":"markdown","source":"# LightGBM on Tabular Playground Series - Feb 2021"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-feb-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-feb-2021/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_features = [\n    \"cat0\", \"cat1\", \"cat2\", \"cat3\", \"cat4\", \"cat5\", \"cat6\", \"cat7\", \n    \"cat8\", \"cat9\"\n]\n\ncontinous_features = [\n    \"cont0\", \"cont1\", \"cont2\", \"cont3\", \"cont4\",\n    \"cont5\", \"cont6\", \"cont7\", \"cont8\", \"cont9\", \"cont10\", \n    \"cont11\", \"cont12\", \"cont13\"\n]\n\nall_features = category_features + continous_features\n\n# boosted trees is not very sensitive to feature engineering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# enum of string type to categoricals of int type (lgbm can handle categoricals, but requires it in int type)\n\nfor feature in category_features:\n    encoder = LabelEncoder()\n    encoder.fit(train[feature])\n    train[feature] = pd.Series(encoder.transform(train[feature]), dtype=\"category\")\n    test[feature] = pd.Series(encoder.transform(test[feature]), dtype=\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n\nPARAMS = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting': 'gbdt',\n    # iteration\n    'num_iterations': 5000,\n    'learning_rate': 0.02,\n    # regulation\n    'num_leaves': 15,\n    'min_data_in_leaf': 1000, # scale of 300000 / num_leaves\n    'feature_fraction': 0.3,\n    # explicit regulation\n    'lambda_l2': 0.001\n}\n\n# 'gbdt' performs better than 'goss'\n# so far, num_leaves, min_data_in_leaf, and feature_fraction appears to be significant on result","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# manual parameters tuning via k-fold, ideally this should be some grid search\n\nPARAMS_KFOLD = PARAMS.copy()\nPARAMS_KFOLD.update({\n    'early_stopping_round': 500 # about 10% of num_iterations\n})\n\ndef rmse_kfold(parameters, n_fold):\n    kfold = KFold(n_splits=n_fold)\n\n    rmse_kfold = np.zeros(n_fold)\n\n    for index, (train_index, validation_index) in enumerate(kfold.split(train)):\n        X_train, X_validation = train[all_features].iloc[train_index], train[all_features].iloc[validation_index]\n        y_train, y_validation = train['target'].iloc[train_index], train['target'].iloc[validation_index]\n\n        model = LGBMRegressor(**parameters)\n\n        model.fit(X_train, y_train,\n                  eval_set = [(X_validation, y_validation)],\n                  verbose = -1)\n\n        pred_validation = model.predict(X_validation)\n\n        rmse_kfold[index] = mean_squared_error(y_validation, pred_validation, squared=False)\n\n    rmse_average = np.average(rmse_kfold)\n    \n    return rmse_average","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# a rudimental grid search for parameters tuning, however it is too slow (even on 'goss')\n\nnum_leaves_range = [2**3-1, 2**4-1, 2**5-1, 2**6-1]\nmin_data_in_leaf_range = [2000, 1000, 500, 250]\nfeature_fraction_range = [0.3]\n\nN_FOLD = 10\n\nrmse_grid_search = []\n\nfor num_leaves in num_leaves_range:\n    for min_data_in_leaf in min_data_in_leaf_range:\n        for feature_fraction in feature_fraction_range:\n            params = PARAMS_KFOLD.copy()\n            params.update({\n                'num_leaves': num_leaves,\n                'min_data_in_leaf': min_data_in_leaf,\n                'feature_fraction': feature_fraction\n            })\n            \n            rmse = rmse_kfold(params, N_FOLD)\n            \n            rmse_grid_search.append((rmse, params))\n            \n            print(\"parameters \" + str(params))\n            print(\"rmse: \" + str(rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pick parameters with the best score\n\nPARAMS_OPTIMIZED = sorted(rmse_grid_search, key=lambda o1: o1[0])[0][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the tuned parameters to train a final model\n\nPARAMS_PRED = PARAMS_OPTIMIZED.copy()\nPARAMS_PRED.pop('early_stopping_round', None)\n\nprint('parameters for final model ' + str(PARAMS_PRED))\n\nmodel = LGBMRegressor(**PARAMS_PRED)\n    \nmodel.fit(train[all_features], train['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(test[all_features])\n\nsubmission = pd.DataFrame({'id': test['id'], 'target': pred})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}