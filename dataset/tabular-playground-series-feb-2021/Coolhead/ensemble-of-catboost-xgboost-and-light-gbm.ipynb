{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's do this\n\n## Importing the libraries \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport gc\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LABEL ENCODING"},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE = \"../input/tabular-playground-series-feb-2021\"\ntest = pd.read_csv(BASE + '/test.csv')\n\ntrain = pd.read_csv(BASE + '/train.csv')\n\nsample_sub = pd.read_csv(BASE + '/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['cat4', 'cont4'], axis = 1) \ntest = test.drop(['cat4', 'cont4'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = test.columns[1:]\ncolumns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = columns[:9]\ncat_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train[feature])\n    train[feature] = le.transform(train[feature])\n    test[feature] = le.transform(test[feature])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_features = columns[9:]\ncont_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\n\ntrain_features = train[cont_features]\ntest_features = test[cont_features]\n\nct = ColumnTransformer([\n        ('somename', MinMaxScaler(), ['cont0', 'cont1', 'cont2', 'cont3', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13'])\n    ], remainder='passthrough')\nft = ColumnTransformer([\n        ('someothername', MinMaxScaler(), ['cont0', 'cont1', 'cont2', 'cont3','cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13'])\n    ], remainder='passthrough')\n\ntrain[cont_features] = ct.fit_transform(train_features)\n\ntest[cont_features] = ft.fit_transform(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.iloc[0:][columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cat_cols = [f\"cat{i}\" for i in range(10) if i is not 4]\ntrain_num_col = [f\"cont{i}\" for i in range(14) if i is not 4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_num_col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A bit of feature engineering \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's get the xgboost model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade xgboost\nimport xgboost as xgb\nxgb.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params= {\n        \"objective\": \"reg:squarederror\",\n        \"max_depth\": 6,\n        \"learning_rate\": 0.01,\n        \"colsample_bytree\": 0.4,\n        \"subsample\": 0.6,\n        \"reg_alpha\" : 6,\n        \"min_child_weight\": 100,\n        \"n_jobs\": 2,\n        \"seed\": 2021,\n        'tree_method': \"gpu_hist\",\n        \"gpu_id\": 0,\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catBoost_params = {\n    \"n_estimator\": 850,\n    \"tast_type\" : \"GPU\",\n    \"loss_function\": 'RMSE',\n    \"eval_metric\": 'RMSE',\n    \"metric_period\": 1000,\n    \"use_best_model\": True,\n    \"random_seed\": 2021\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_params = {\n    \"random_state\": 2021,\n    \"metric\": \"rmse\",\n    \"n_jobs\": -1,\n    \"cat_feature\": [x for x in range(len(train_cat_cols))],\n    \"early_stopping_round\": 150,\n    \"reg_alpha\": 6.147694913504962,\n    \"reg_lambda\": 0.002457826062076097,\n    \"colsample_bytree\": 0.3,\n    \"learning_rate\": 0.01,\n    \"max_depth\": 30,\n    \"num_leaves\": 100,\n    \"min_child_samples\": 275,\n    \"n_estimators\": 1600000,\n    \"cat_smooth\": 40.0,\n    \"max_bin\": 512,\n    \"min_data_per_group\": 100,\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": 0.7,\n    \"cat_l2\": 12.0,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_oof = np.zeros((300000,))\nlight_trainPredict = np.zeros((300000,))\ncat_trainPredict = np.zeros((300000,))\n\ncat_preds = 0\nlight_preds = 0\n\n\ntest_preds = 0\ntrain_oof.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_0 = test[columns]\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test = xgb.DMatrix(test[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n        print(f'Fold {f}')\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        train_target, val_target = target[train_ind], target[val_ind]\n        \n        ## we are going to use cat boost\n        \n        cat_model = CatBoostRegressor(learning_rate = 0.1,\n                                      iterations = 2000,\n                                      task_type=\"GPU\",\n                                      loss_function= 'RMSE',\n                                      eval_metric='RMSE',\n                                      metric_period=1000, \n                                      use_best_model=True,\n                                      random_seed=42,\n                                      allow_writing_files = False,\n                                      od_type = 'Iter',\n                                      bagging_temperature = 0.8,\n                                      depth = 6,\n                                      od_wait = 20,)\n        cat_model.fit(\n        train_df,\n        train_target,\n        eval_set=[(val_df, val_target)],\n        plot=True,\n        early_stopping_rounds=250\n        )\n        cat_valpredict = cat_model.predict(val_df)\n        cat_testPredict = cat_model.predict(test_0)\n        \n        cat_trainPredict[val_ind] = cat_valpredict\n        cat_preds += cat_testPredict/NUM_FOLDS\n        print(mean_squared_error(cat_valpredict, val_target, squared=False))\n        \n        ## we are going to use lightboost as well\n        \n        light_model = LGBMRegressor(**lgbm_params ) \n        light_model.fit(\n        train_df, \n        train_target,\n        eval_set=[(val_df, val_target)],\n        verbose=100,\n    )\n        light_valpredict = light_model.predict(val_df)\n        light_testPredict = light_model.predict(test_0)\n        \n        light_trainPredict[val_ind] = light_valpredict\n        light_preds += light_testPredict/NUM_FOLDS\n        print(mean_squared_error(light_valpredict, val_target, squared=False))\n        \n        \n        ## xgboost model with Dmatrix which is used for optimization\n        \n        train_df = xgb.DMatrix(train_df, label=train_target)\n        val_df = xgb.DMatrix(val_df, label=val_target)\n        \n        model =  xgb.train(xgb_params, train_df, 3600)\n        temp_oof = model.predict(val_df)\n        temp_test = model.predict(Test)\n\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test/NUM_FOLDS\n        \n        print(mean_squared_error(temp_oof, val_target, squared=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(train_oof, target, squared=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(light_trainPredict, target, squared=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(cat_trainPredict, target, squared=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('train_oof', train_oof)\nnp.save('test_preds', test_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model = [train_oof,light_trainPredict,cat_trainPredict]\ntest_model = [ test_preds,cat_preds, light_preds]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('light_oof', light_trainPredict)\nnp.save('cat_oof', cat_trainPredict)\nnp.save('light_test_preds', light_preds)\nnp.save('cat_test_preds', cat_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's see how the individual models perform and then we can make a ensemble according to that\n\n## CV score for \n\n### XGBOOST = 0.8422094126465101\n\n### CatBOOST = 0.8512973171285753\n\n### lightBOOST = 0.84181677324358\n\n\n## LeaderBoard Score..."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub['target'] = test_preds\nsample_sub.to_csv('xgBoost.csv', index=False)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub['target'] = light_preds\nsample_sub.to_csv('light_sub.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub['target'] = cat_preds\nsample_sub.to_csv('cat_sub.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}