{"cells":[{"metadata":{},"cell_type":"markdown","source":"This here is some essential EDA, we will:\n- check for missing values\n- plot distributions for the train and test features\n- plot distribution of the train target\n- plot correlations\n- check for outliers\n\nLet's get to it!\n\n# Import libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 25)\nimport seaborn as sns\nsns.set()\nsns.set_palette('Set2')\nfrom pathlib import Path\n\nimport os\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\n        \ninput_path = Path('/kaggle/input/tabular-playground-series-feb-2021/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read in the data files"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(input_path / 'train.csv', index_col='id')\ndisplay(df_train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(input_path / 'test.csv', index_col='id')\ndisplay(df_test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = pd.read_csv(input_path / 'sample_submission.csv', index_col='id')\ndisplay(df_submission.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check for missing values\nThere are no missing values, so we don't have to deal with them..."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Are there missing values in train set?', df_train.isnull().values.any())\nprint('Are there missing values in test set?', df_test.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot distributions\n\nTo chec the distribution of each feature and compare between the train and test set, we'll make a common DataFrame and plot histograms of the probability. For continuous features, we'll also make a box plot.\n\n## Continuous features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_copy = df_train.drop('target', axis=1)\ndf_train_copy['set'] = 'train'\ndf_test_copy = df_test.copy()\ndf_test_copy['set'] = 'test'\ndf_common = pd.concat([df_train_copy, df_test_copy])\ncat_features = [col for col in df_test.columns if col.startswith('cat')]\ncont_features = [col for col in df_test.columns if col.startswith('cont')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in cont_features:\n    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw=dict(height_ratios=(.15, .85)))\n    sns.boxplot(data=df_common, x=feature, y='set', ax=ax_box)\n    sns.histplot(data=df_common, x=feature, kde=True, hue='set', ax=ax_hist, stat='probability', common_norm=False)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot again with logarithmic y-axis to check for outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in cont_features:\n    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw=dict(height_ratios=(.15, .85)))\n    sns.boxplot(data=df_common, x=feature, y='set', ax=ax_box)\n    sns.histplot(data=df_common, x=feature, kde=True, hue='set', ax=ax_hist, stat='probability', common_norm=False)\n    ax_hist.set_yscale('log')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n- The box plots are misleading, since the features are far from being normally distributed. But I don't really see any nasty outliers, there are some weird values in cont5 below 0, but I don't consider them to be problematic for modelling.\n- The train and test sets follow the same distributions.\n- For certain models, it will be necessary to normalize the features, e.g. via quantile transformation"},{"metadata":{},"cell_type":"markdown","source":"## Try to normalize continuous features\n\nLet's see what effect the QuantileTransformer will have on the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import quantile_transform\n\ndf_train_trans = df_train[cont_features].copy()\nfor feature in cont_features:\n    df_train_trans[feature] = quantile_transform(\n        df_train_trans[feature].values.reshape(-1, 1), n_quantiles=900,\n        output_distribution='normal'\n    )\nfix, axs = plt.subplots(5, 3, figsize=(15, 25))\naxs = axs.flatten()\nfor i, feature in enumerate(cont_features):\n    sns.histplot(data=df_train_trans, x=feature, ax=axs[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical features\nPlot the distribution of the categorical features. Since there are large difference of probabilities within some features, we'll use logarithmic scaling."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(4, 3, figsize=(20, 30))\naxs = axs.flatten()\nfor i, feature in enumerate(cat_features):\n    sns.histplot(data=df_common, x=feature, hue='set', multiple=\"dodge\", shrink=.8, ax=axs[i],\n                 stat='probability', common_norm=False)\n    axs[i].set_yscale('log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of target"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax_box, ax_lin, ax_log) = plt.subplots(3, sharex=True, figsize=(10, 6))\nsns.boxplot(data=df_train, x='target', ax=ax_box)\nsns.histplot(data=df_train['target'], kde=True, stat='probability', ax=ax_lin)\nsns.histplot(data=df_train['target'], stat='probability', ax=ax_log)\nax_log.set_yscale('log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n- It's a bimodal distribution for sure. We'll try to apply a Gaussian mixture model below\n- Looks like we have some outliers - I would be tempted to remove everything below 3."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\n\ngmm = GaussianMixture(n_components=2)\ngmm.fit(df_train.target.values.reshape(-1, 1))\ndf_gmm = df_train[['target']].copy()\ndf_gmm['target_class'] = gmm.predict(df_train.target.values.reshape(-1, 1))\ndisplay(df_gmm.head())\nsns.histplot(data=df_gmm, x='target', hue='target_class', stat='probability');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also try quantile transformation here again:"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_trans = quantile_transform(\n        df_train['target'].values.reshape(-1, 1), n_quantiles=900,\n        output_distribution='normal'\n    )\nsns.histplot(data=target_trans, stat='probability')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlations\n\nNo particularly strong correlations with the target anywhere."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df_train.corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nfig, ax = plt.subplots(figsize=(14, 14))\n\n# plot heatmap\nsns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',\n            cbar_kws={\"shrink\": .8})\n# yticks\nplt.yticks(rotation=0);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline model\n\nFinally, lets make a simple baseline model with LightGBM, without any preprocessing or hyperparameter optimization. I'm using LightGBM simply because there is no need for preprocessing to get a halfway decent model. Any improvements that we come up with should lead to a better performance than **0.84523**."},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\ndf_train[cat_features] = df_train[cat_features].astype('category')\nparams = {'metrics': 'rmse',\n          'objective': 'regression'}\nd_train = lgb.Dataset(df_train.drop('target', axis=1), label=df_train.target)\nresult = lgb.cv(params, d_train, stratified=False, num_boost_round=1000, early_stopping_rounds=10,\n                return_cvbooster=True, verbose_eval=50)\nprint(f'RMSE: {result[\"rmse-mean\"][-1]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is also interesting to compare the distributions of the predictions and the target - the distribution of the prediction is much narrower for some reason."},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = result['cvbooster']\nprediction = np.array(regressor.predict(df_train.drop('target', axis=1))).mean(axis=0)\ndf = df_train.drop(cat_features + cont_features, axis=1)\ndf['prediction'] = prediction\nsns.histplot(data=df.melt(), x='value', hue='variable');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}