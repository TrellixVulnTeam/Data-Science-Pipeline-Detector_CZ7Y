{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Complete tutorial covering ML pipeline"},{"metadata":{},"cell_type":"markdown","source":"An typical machine learning pipeline consists of:\n\n1. data collection\n2. exploratory data analysis (EDA)\n3. data preprocessing\n4. data modeling\n\n  \n- Since `data collection` is done by Kaggle for you, we should focus on step 2 to step 4. \n\n- In this tutorial, we will practice popular packages that are used in those steps. \n\n- Specifically, we will pratice using following packages for each step.\n\nStep 2: `matplotlib`, `seaborn`  \nStep 3: `scikit-learn`  \nStep 4: `scikit-learn` "},{"metadata":{},"cell_type":"markdown","source":"## Setup"},{"metadata":{},"cell_type":"markdown","source":"We will first read the data using `pandas`. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/tabular-playground-series-feb-2021/'\n\ntrain_path = path + 'train.csv'\ntest_path = path + 'test.csv'\nsub_path = path + 'sample_submission.csv'\n\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)\nsub = pd.read_csv(sub_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"- Let's use `pd.DataFrame.head()` method which will show top 5 rows of each dataframe to verify that the data have been read well."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape, sub.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We have 300k samples for training and 200k samples for testing. \n\n- 300k samples is actually a huge amount. It will take quite some time for the model to train on it all. \n\n- For this tutorial, we will only use 10k samples that have been sampled randomly for training. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# tutorial on seaborn and matplotlib on progress...\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing \n\n- *on progress...*"},{"metadata":{},"cell_type":"markdown","source":"## Data Modeling"},{"metadata":{},"cell_type":"markdown","source":"- In this section, we will use a `for` loop to train several machine learning models at once. \n\n- In particular, we will train `DecisionTree`, `Random Forest`, and `LightGBM`. \n\n- Like we've stated at the `EDA` section, we will only use 10k samples from 300k total samples of training data due to time constraints. \n\n- However, it is recommended to train on all 300k samples for higher accuracy.\n\n- Let's first import the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Then, we will import `KFold` and `cross_val_score` functions in order to validate on our training data\n\n- Those two functions are necessary in order to perform `Shuffled K-fold Cross Validation`. \n\n![](http://ethen8181.github.io/machine-learning/model_selection/img/kfolds.png)\n- Image Reference: http://ethen8181.github.io/machine-learning/model_selection/model_selection.html\n\n- As shown on the image above, `Cross Validation` allows you to validate your model on your training data by holding out certain part of the training data for validation.\n\n- If you divide the whole training data into 5 parts and use each part for validating your model, it becomes `5-Fold Cross Validation`.\n\n- When dividing the training data, it is important to shuffle them in order to prevent overfitting. This can be performed with the `KFold` function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `KFold` function has following parameters.\n\n    - `n_splits`: number of folds (int)\n    - `shuffle`: whether to shuffle the data (boolean)\n    - `random_state`: seed number for reproducibility (int)\n    \n- By setting each parameters 5, True, 0 respectively, we are performing `5-Fold Cross-Validation`.\n\n- We will save the `KFold` object to `k_fold` variable, which will be passed to `cross_val_score` function."},{"metadata":{"trusted":true},"cell_type":"code","source":"k_fold = KFold(n_splits = 5, shuffle = True, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- In the cell below, we will define 3 models which will be used for modeling. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model_dict = {'DT':DecisionTreeRegressor(),\n              'RF':RandomForestRegressor(n_jobs=-1, random_state=0), \n              'LGB':lgb.LGBMRegressor()}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Next, we will define `compare_models` function which will  \n  \n    1. iterate through the models in `model_dict` and perform `5-Fold Cross Validation`\n    2. save the result to `score` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compare_models(X_train, y_train, model_dict):\n    \n    score = {}\n\n    for model_name in model_dict.keys():\n\n        model = model_dict[model_name]\n\n        score[model_name] = np.mean(cross_val_score(model, X_train.sample(frac=1, random_state=0).head(10000), y_train.sample(frac=1, random_state=0).head(10000), scoring = 'neg_mean_squared_error', cv = k_fold, n_jobs = -1))\n\n        print(f'{model_name} validation completed')\n        \n    return score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- By using `pd.DataFrame.sample()` we can randomly shuffle the dataframe and then use `head()` to extract 10k samples. \n\n- In the cell below, we have divided `train`, `test` data into `X_train`, `y_train`, and `X_test` to train the models. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.iloc[:,11:-1]\ny_train = train['target']\nX_test = test[X_train.columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By executing the cell below, it will validate 3 models in model_dict and save the validated score to the `score` variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"score = compare_models(X_train, y_train, model_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The result comes out in `negative MSE`. So the higher the score is, the better is the performance of the model. \n\n- Since `Random Forest` has highest performance of `-0.784`, we will use `Random Forest` to make inference on the test data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model_dict['RF']\n\nmodel.fit(X_train.sample(frac=1, random_state=0).head(10000), y_train.sample(frac=1, random_state=0).head(10000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- After training the model, we will make inference on `X_test` and save the result into the `sub` file."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target'] = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The result should be around `0.87` which is quite low.\n\n- However, do mind that we have only used 10k sample out of 300k training samples for educational purpose. \n\n- Since you are now familiar with the whole machine learning pipeline process, try fiddling the number of training samples and the types of models to increase your score."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}