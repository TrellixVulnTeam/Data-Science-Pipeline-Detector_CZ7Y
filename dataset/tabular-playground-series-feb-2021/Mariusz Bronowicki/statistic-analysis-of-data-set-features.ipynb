{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport scipy as sp\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold, GridSearchCV\n\nimport lightgbm as lgbm\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub = pd.read_csv(\"/kaggle/input/tabular-playground-series-feb-2021/sample_submission.csv\")\ntrain = pd.read_csv(\"/kaggle/input/tabular-playground-series-feb-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-feb-2021/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save id column for submission\nid_col = test['id']\ntrain.drop(\"id\", axis=1, inplace=True)\ntest.drop(\"id\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_feat = [x for x in train.columns if x.startswith('co')]\ncat_feat = [x for x in train.columns if x.startswith('ca')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_num_subplots(df, feat):\n    fig = plt.figure(figsize=(14, 20))\n\n    for i, col in enumerate(feat):\n        plt.subplot(12, 3, i+1)\n        sns.histplot(x=col, data=df)\n        plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_num_subplots(train, num_feat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see distribution of our numeric features is not Gaussian."},{"metadata":{"trusted":true},"cell_type":"code","source":"#x = np.cumsum(train['cont0'])\nx = train['cont0']\nplt.plot(x);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1 = (sp.stats.rankdata(x) / (len(x)+1)) *2 - 1\n#print(np.min(x1), np.max(x1))\nx1 = np.arctanh(x1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,2, figsize=(12,8))\n\nax[0,0].plot(train['cont0'])\nax[0,0].set_title(\"Original Data\")\n\n\nax[0,1].plot(x1)\nax[0,1].set_title(\"Transformed Data\")\n\nax[1,0].hist(train['cont0'], bins=40)\nax[1,0].set_title(\"Original Data\")\n\nax[1,1].hist(x1, bins=40)\nax[1,1].set_title(\"Transformed Data\")\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Non monotonic relation\nplt.plot(x, x1, 's')\nplt.xlabel(\"orginal\")\nplt.ylabel(\"transformed\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gaussian_transformer(df):\n    \n    new_df = pd.DataFrame()\n    \n    for col in df.columns:\n        z = (sp.stats.rankdata(df[col]) / (len(x)+1)) *2 - 1\n        z = np.arctanh(z)\n        new_df[col] = z\n    return new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gaussian_num_df = gaussian_transformer(train[num_feat])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gaussian_num_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_num_subplots(gaussian_num_df, num_feat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = MinMaxScaler(feature_range=(-1,1))\ngaussian_num_df[num_feat] = sc.fit_transform(gaussian_num_df[num_feat])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_num_subplots(gaussian_num_df, num_feat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"gausian_test = gaussian_transformer(test[num_feat])\ngausian_test[num_feat] = sc.transform(gausian_test[num_feat])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cat_feat:\n    diff = set(train[col]) - set(test[col])\n    print(f\"Differents between train and test set is: {diff}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_cat_subplots(df, feat):\n    \n    fig = plt.figure(figsize=(14, 20))\n    \n    for i, col in enumerate(feat):\n        plt.subplot(10, 3, i+1)\n        sns.countplot(x=df[col])\n        plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_cat_subplots(train, cat_feat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cord_df = pd.DataFrame(train[cat_feat].nunique().values,\n                       index=cat_feat, columns=['cartinality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cord_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cat_df = train[cat_feat].copy()\ntest_cat = test[cat_feat].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer, LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lb = LabelBinarizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cat_feat:\n    train_cat_df[col] = lb.fit_transform(train_cat_df[col])\n    test_cat[col] = lb.transform(test_cat[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cat_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_cat_subplots(train_cat_df, cat_feat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First create new dataframe with transformed data\nnew_train = pd.concat([train_cat_df, gaussian_num_df], axis=1)\nnew_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = new_train\ny = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_X = pd.concat([X, y], axis=1)\n\ncorr_map = new_X.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr_map, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_map, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_score = []\n\n\n \nparams ={\"objective\": \"regression\",\n         \"metric\": \"rmse\",\n         \"verbosity\": -1,\n         \"boosting_type\": \"gbdt\",\n         \"feature_fraction\": 0.5,\n         \"max_depth\": 10,\n         \"num_leaves\": 60,\n         \"lambda_l1\": 2,\n         \"lambda_l2\": 2,\n         \"learning_rate\": 0.01,\n         \"min_child_samples\":50,\n         \"bagging_fraction\": 0.7,\n         \"bagging_freq\": 1, \n         \"max_bin\": 80,}\n          #\"is_unbalance\":True,\n          #\"subsample\":0.3}\n    \n    \nlgb_train = lgbm.Dataset(X_train, y_train)\nlgb_val = lgbm.Dataset(X_val, y_val)\ngbm = lgbm.train(params,\n                 lgb_train,\n                 valid_sets=[lgb_train, lgb_val],\n                 num_boost_round=10000,\n                 verbose_eval=100,\n                 early_stopping_rounds=100,\n                 )\n    \n# Extra Boosting\nlgb_train = lgbm.Dataset(X_train, y_train)\nlgb_val = lgbm.Dataset(X_val, y_val)\nparams = {\"objective\": \"regression\",\n          \"metrics\": \"rmse\",\n          \"verbosity\": -1,\n          \"boosting_type\": \"gbdt\",\n          \"feature_fraction\": 0.5,\n          \"max_depth\": 10,\n          \"num_leaves\":200,\n          \"lambda_l1\": 2,\n          \"labmda_l2\": 2,\n          \"learning_rate\": 0.003,\n          \"min_child_samples\": 50,\n          \"max_bin\": 80,\n          #\"is_unbalance\":True,\n          #\"subsample\":0.3\n          \"bagging_fraction\": 0.7,\n          \"bagging_freq\": 1,}\n    \ngbm = lgbm.train(params,\n                 lgb_train,\n                 valid_sets = [lgb_train, lgb_val],\n                 verbose_eval = 100,\n                 num_boost_round = 10000,\n                 early_stopping_rounds=100,\n                 init_model = gbm)\n    \ny_pred = gbm.predict(X_val)\nrmse_score.append(np.sqrt(mean_squared_error(y_val, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=1)\noof = np.zeros(len(X))\nscore_list = []\nfold = 1\n\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    \n\n    #X_train = X_train.abs()\n\n    \n    y_pred_list = []\n    for seed in [1]:\n        dtrain = lgbm.Dataset(X_train, y_train)\n        dvalid = lgbm.Dataset(X_val, y_val)\n        print(seed)\n        params = {\"objective\": \"regression\",\n                  \"metric\": \"rmse\",\n                  \"verbosity\": -1,\n                  \"boosting_type\": \"gbdt\",\n                  \"feature_fraction\": 0.5,\n                  \"max_depth\": 10,\n                  \"num_leaves\": 120,\n                  \"lambda_l1\": 2,\n                  \"lambda_l2\": 2,\n                  \"learning_rate\": 0.01,\n                  \"min_child_samples\":50,\n                  #\"bagging_fraction\": 0.7,\n                  #\"bagging_freq\": 1, \n                  \"max_bin\": 80,\n                  \"is_unbalance\":True,}\n                  #\"subsample\":0.3}\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                        dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=100000,\n                        early_stopping_rounds=100\n                    )\n        \n        # Extra boosting.\n        dtrain = lgbm.Dataset(X_train, y_train)\n        dvalid = lgbm.Dataset(X_val, y_val)\n        params = {\"objective\": \"regression\",\n                  \"metric\": \"rmse\",\n                  \"verbosity\": -1,\n                  \"boosting_type\": \"gbdt\",\n                  \"feature_fraction\": 0.5,\n                  \"max_depth\": 10,\n                  \"num_leaves\": 120,\n                  \"lambda_l1\": 2,\n                  \"lambda_l2\": 2,\n                  \"learning_rate\": 0.03,\n                  \"min_child_samples\":50,\n                  #\"bagging_fraction\": 0.7,\n                  #\"bagging_freq\": 1, \n                  \"max_bin\": 80,\n                  \"is_unbalance\":True,}\n                  #\"subsample\":0.3}\n\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                            dtrain,\n                            valid_sets=[dtrain, dvalid],\n                            verbose_eval=100,\n                            num_boost_round=1000,\n                            early_stopping_rounds=100,\n                            init_model = model\n                            )\n\n    \n    \n        y_pred_list.append(model.predict(X_val))\n    \n   \n    \n    oof[test_index] = np.mean(y_pred_list,axis=0)    \n    score = np.sqrt(mean_squared_error(y_val, oof[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    fold+=1\n\nnp.mean(score_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score_list)\nprint(np.mean(score_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optuna to the rescue"},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    \n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=45)\n    dtrain = lgbm.Dataset(X_train, label=y_train)\n    \n    param = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'boosting_type': 'gbdt',\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int(\"max_depth\", 1, 100),\n        'max_bin': trial.suggest_int('max_bin', 1, 255)\n    }\n    \n    gbm = lgbm.train(param, dtrain)\n    preds = gbm.predict(X_val)\n    pred_labels = np.rint(preds)\n    rmse = np.sqrt(mean_squared_error(y_val, pred_labels))\n    return rmse\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\nprint(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial: \", study.best_trial.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_trial.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=45)\noof = np.zeros(len(X))\nscore_list = []\nfold = 1\ntest_preds = []\n\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n\n\n    y_pred_list = []\n    for seed in [1]:\n        dtrain = lgbm.Dataset(X_train, y_train)\n        dvalid = lgbm.Dataset(X_val, y_val)\n        print(seed)\n        params = {\"objective\": \"regression\",\n                  \"metric\": \"rmse\",\n                  \"verbosity\": -1,\n                  \"boosting_type\": \"gbdt\",\n                  \"feature_fraction\": 0.44071200607037225,\n                  \"max_depth\": 90,\n                  \"num_leaves\": 192,\n                  \"lambda_l1\": 1.264462581934323,\n                  \"lambda_l2\": 0.024398254447942604,\n                  \"learning_rate\": 0.01,\n                  \"min_child_samples\":91,\n                  \"bagging_fraction\": 0.9326330031125017,\n                  \"bagging_freq\": 3,\n                  \"max_bin\": 145,\n                  \"is_unbalance\":True}\n                  #\"subsample\":0.3}\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                           dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=2500,\n                        early_stopping_rounds=100\n                    )\n        \n        # Extra boosting.\n        dtrain = lgbm.Dataset(X_train, y_train)\n        dvalid = lgbm.Dataset(X_val, y_val)\n        params = {\"objective\": \"regression\",\n                  \"metric\": \"rmse\",\n                  \"verbosity\": -1,\n                  \"boosting_type\": \"gbdt\",\n                  \"feature_fraction\": 0.44071200607037225,\n                  \"max_depth\": 90,\n                  \"num_leaves\": 192,\n                  \"lambda_l1\": 1.264462581934323,\n                  \"lambda_l2\": 0.024398254447942604,\n                  \"learning_rate\": 0.0001,\n                  \"min_child_samples\":91,\n                  \"bagging_fraction\": 0.9326330031125017,\n                  \"bagging_freq\": 3,\n                  \"max_bin\": 145,\n                  \"is_unbalance\":True,}\n                 # \"subsample\":0.3}\n\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                            dtrain,\n                            valid_sets=[dtrain, dvalid],\n                            verbose_eval=100,\n                            num_boost_round=1500,\n                            early_stopping_rounds=100,\n                            init_model = model\n                            )\n\n    \n    \n        y_pred_list.append(model.predict(X_val))\n        #test_preds.append(model.predict(new_test))\n    \n   \n    \n    oof[test_index] = np.mean(y_pred_list,axis=0)    \n    score = np.sqrt(mean_squared_error(y_val, oof[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    fold+=1\n\nnp.mean(score_list)\nprint(score_list)\nprint(np.mean(score_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score_list)\nprint(np.mean(score_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_test = pd.concat([test_cat, gausian_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(new_test)\n\nsub = pd.DataFrame({\"id\":id_col,\n                    \"target\": preds})\n\nsub.to_csv(\"sub3_with_optuna.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import joblib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joblib.dump(model, \"lgbm_optuna_model.joblib\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loaded_model = joblib.load(\"./lgbm_optuna_model.joblib\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndef model_gridsearchCV(algo,param,name):\n    \"\"\"\n    Function will perform gridsearchCV for given algorithm\n    and parameter grid. Returns grid model, y_pred. Prints out \n    mean absolute error, root mean squared error, R-square score\n    \"\"\"\n    # Instatiate base model\n    model = algo()\n    \n    # Instantiate grid for a model\n    model_grid = GridSearchCV(model, \n                             param,\n                             scoring=\"r2\",\n                             verbose=2,\n                             n_jobs=-1,\n                             cv=3)\n    # Fit the grid model\n    model_grid.fit(X_train, y_train)\n    \n    # Make prediction\n    y_pred = model_grid.predict(X_val)\n    \n    # Evaluate model\n    mae = mean_absolute_error(y_val, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    r2score = r2_score(y_val, y_pred)\n    \n    # Print \n    print(f\"**{name} with GridSearchCV**\")\n    print(f\"MAE: {mae:}\")\n    print(f\"RMSE: {rmse:}\")\n    print(f\"R-squared: {r2score:.2f}%\")\n    \n    return mae, rmse, r2score, y_pred, model_grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\"loss\":[\"ls\",\"huber\",\"quantile\"],\n              \"learning_rate\": [ 0.01],\n              \"subsample\": [0.5, 0.2, 0.1],\n              \"max_depth\": [3,6,8]}\n\ngbr_grid_mae, gbr_grid_rmse, gbr_grid_r2, _ , gbr_grid = model_gridsearchCV(GradientBoostingRegressor, \n                                                                            param_grid,\n                                                                            \"GradientBoostingRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joblib.dump(gbr_grid, \"GradientBoostingRegressor_model.joblib\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\"kernel\":[\"linear\",\"rbf\",],\n              \"gamma\": [\"scale\",\"auto\"],\n              \"C\": [0.1, 0.5, 10],\n              \"epsilon\": [0.1, 0.01]}\n\nsvr_grid_mae, svr_grid_rmse, svr_grid_r2, svr_grid_y_pred, svr_grid_model = model_gridsearchCV(SVR,\n                                                                                param_grid,\n                                                                               \"SVR\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joblib.dump(gbr_grid, \"SVR_model.joblib\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\"learning_rate\":[0.01],\n              \"max_depth\":[3,4,8],\n              \"min_child_weight\":[3,5,7],\n              \"colsample_bytree\":[0.3, 0.5, 0.7]}\n\nxboost_gr_mae, xboost_gr_rmse, xboost_gr_r2, _ , xboost_gr_model = model_gridsearchCV(XGBRegressor,\n                                                                                      param_grid,\n                                                                                      \"XGBoost\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xboost_gr_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"{'colsample_bytree': 0.7,\n 'learning_rate': 0.01,\n 'max_depth': 8,\n 'min_child_weight': 7}"},{"metadata":{"trusted":true},"cell_type":"code","source":"joblib.dump(xboost_gr_model, \"XGBoost_model.joblib\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = new_train.values\ny = train['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout\nfrom keras.callbacks import EarlyStopping, TensorBoard\nfrom keras.layers import LeakyReLU\nfrom keras.optimizers import Adam, RMSprop, SGD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_model(model):\n    y_pred = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    print(f\"RMSE: {rmse}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_ann():\n    # Instantiate a model\n    model = Sequential()\n    # Add hidden layer \n    model.add(Dense(14, activation='relu'))\n    # Add hidden layer\n    model.add(Dense(24, activation='relu'))\n    model.add(Dropout(0.1))\n    # Add hidden layer\n    model.add(Dense(14, activation='relu'))\n    # Add output layer\n    model.add(Dense(1))\n    \n    # Compile the model\n    model.compile(optimizer=Adam(lr=0.001), loss=tensorflow.keras.losses.MeanSquaredError(),\n                  metrics=['mse'])\n    \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=45)\noof = np.zeros(len(X))\nscore_list = []\nfold = 1\n\ny_pred_list = []\nfor train_idx, test_idx in kf.split(X):\n    X_train, X_val = X[train_idx], X[test_idx]\n    y_train, y_val = y[train_idx], y[test_idx]\n    \n    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n    \n    ann_model = create_ann()\n    ann_model.fit(X_train, \n                  y_train, \n                  validation_data=(X_val, y_val), \n                  batch_size=32, epochs=30, \n                  callbacks=[early_stop])\n    \n    \n    y_pred_list.append(ann_model.predict(X_val))\n    \n    \n    oof[test_idx] = np.mean(y_pred_list, axis=0).reshape(len(X_val),)\n    score = np.sqrt(mean_squared_error(y_val, oof[test_idx]))\n    score_list.append(score)\n    print(f\"RMSE fold -{fold} : {score}\")\n    fold +=1\n    \nprint(f\"RMSE mean 5 folds: {np.mean(score_list)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_model.save(\"ANN_model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}