{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"This is my second notebook with February Tabular-Playgroud-Series. In the first one I've learnt how to use lightgbm algorithm. https://www.kaggle.com/godzill22/tbs-feb-2021-with-lightgbm. In this notebook I want to learn how to use neural network to solve regression model and whether ANN can improve my score."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import main libraries\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\n\n# Statistics\nfrom scipy import stats\nfrom scipy.stats import skew, kurtosis\n\n# Preprocessing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis first."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import our dataset\ntrain = pd.read_csv(\"/kaggle/input/tabular-playground-series-feb-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-feb-2021/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a list of numerical and categoricals columns\nnum_feat = [col for col in train.columns if col.startswith(\"cont\")]\ncat_feat = [col for col in train.columns if col.startswith(\"cat\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missingno.matrix(train[num_feat], figsize=(18,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save id column for submission and drop it from train/test dataset\nid_col = test['id']\ntrain.drop(\"id\", axis=1, inplace=True)\ntest.drop(\"id\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describe continues features\ntrain[num_feat].describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_feat.append(\"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train[num_feat].corr()\n\nfig = plt.figure(figsize=(14,9))\n\nsns.heatmap(corr,cmap='coolwarm', annot=True, cbar=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is some correlation between features but interestingly none of them is correlated with the target. As we can see on the plot below correlation between feature 'cont9' and the target doesn't exist."},{"metadata":{"trusted":true},"cell_type":"code","source":"target_ser = corr['target']\ntarget_ser = target_ser.drop('target').sort_values(ascending=False)\n\nplt.figure(figsize=(10,5))\nsns.barplot(x=target_ser.index, y=target_ser.values, palette='cool')\nplt.title(\"Correlation numeric features\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_feat.remove(\"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,30))\n\nfor i, col in enumerate(num_feat):\n    plt.subplot(8,2, i+1)\n    sns.kdeplot(x=train[col], color='b', shade=True)\n    plt.grid()\n    plt.tight_layout()\n    \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In most cases the distribution of continues features seems to be multimodial, except feature \"cont6\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,30))\n\nfor i, col in enumerate(num_feat):\n    plt.subplot(8,2, i+1)\n    sns.scatterplot(x=col, y=\"target\", data=train, alpha=0.3)\n    plt.tight_layout()\n    \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Discretization of continues variables\n\n#from sklearn.preprocessing import KBinsDiscretizer\n\n#est = KBinsDiscretizer(n_bins=100, encode='ordinal', strategy='quantile')\n\n#train.loc[:,num_feat] = est.fit_transform(train.loc[:,num_feat])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Discretization didn't help to improve my base model."},{"metadata":{},"cell_type":"markdown","source":"#### Skewness and Kurtosis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[num_feat].kurtosis()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[num_feat].skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def signaltonoise(a, axis=0, ddof=0):\n    a = np.asanyarray(a)\n    m = a.mean(axis)\n    sd = a.std(axis=axis, ddof=ddof)\n    return np.where(sd == 0, 0, m/sd)\n\nfor col in num_feat:\n    print(f\"Column '{col}' signal-to-noise ratio:  {signaltonoise(train[col]):2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Traget column"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt\nsns.displot(train['target'])\nplt.title(f\"skewness: {train['target'].skew()}\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our target column distribution is bimodial, and is skewed to the left."},{"metadata":{},"cell_type":"markdown","source":"### Categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cat_feat:\n    \n    set_diff = set(train[col].unique()) - set(test[col].unique())\n    print(f\"Train and Test Dataset column: {col} has different of unuque value: {set_diff}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['cat6'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['cat6'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a different in column 'cat6' and that need to be remembered when we onhotencode or using get_dummies method."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,30))\n\nfor i, col in enumerate(cat_feat):\n    plt.subplot(8,3, i+1)\n    sns.countplot(x=train[col], palette='mako_r')\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check distribution of categorical features against target feature.\nfig = plt.figure(figsize=(18, 30))\n\nfor i, col in enumerate(cat_feat):\n    plt.subplot(8, 2, i+1)\n    sns.boxenplot(x=col, y='target', data=train)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LabelEncode categorical features"},{"metadata":{},"cell_type":"markdown","source":"Normally, when converting categorical features we would be able to distinguish between nominal and ordinal but not in this case. I've notice that some of Kagglers've chosen method appropriate for ordinal transformation. I think the only reason for that would be to reduce dimensionality of the dataset as it is already quite big for computation power of out pc. For that point I will use LabelEncoder from sklearn."},{"metadata":{},"cell_type":"markdown","source":"### Preparing the dataset for machine learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfor col in cat_feat:\n    train[col] = le.fit_transform(train[col])\n    test[col] = le.transform(test[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test for skewness again\ntest.skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am going to remove column 'cat6' as it is skewed the most."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('cat6', axis=1, inplace=True)\ntest.drop('cat6', axis=1, inplace=True)\ncat_feat.remove('cat6')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating dummy variable didn't improve the model, as a matter of fact it worsen the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#def get_dummies(df):\n    #dummy_df = pd.get_dummies(df[cat_feat], drop_first=True)\n    #df.drop(cat_feat, axis=1, inplace=True)\n    #new_df = pd.concat([dummy_df, df], axis=1)\n    #return new_df\n\n#train = get_dummies(train)\n#test = get_dummies(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modeling ANN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_cols = [col for col in train.columns if col.startswith(\"c\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.iloc[:,:-1].values\ny = train.iloc[:, -1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y,\n                                                  test_size=0.1, random_state=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to normalize the dataset for tensorflow\nfrom sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val= scaler.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a simple model\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nmodel = Sequential()\n\nmodel.add(Dense(23, activation=\"relu\"))\nmodel.add(Dense(23, activation=\"relu\"))\nmodel.add(Dense(23, activation=\"relu\"))\nmodel.add(Dense(23, activation=\"relu\"))\n\n\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mae')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val) , epochs=10, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_df = pd.DataFrame(model.history.history)\nloss_df.plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model evaluation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_mae = model.evaluate(X_val, y_val, verbose=0)\nprint(f\"Base Model MAE: {base_mae}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_y = np.mean(y)\noff = 100 * (base_mae / mean_y)\nprint(f\"We are off about {off:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.evaluate(X_train, y_train, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_pred = model.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_rmse = np.sqrt(mean_squared_error(y_val, base_pred))\nprint(f\"Base model RMSE: {base_rmse}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so this is our base line. There is a lot to do in order to bit my best score from previous notebook  (0.84364). Well we see, after all we doing it for fun and to learn something new."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Dropout\nfrom keras.callbacks import EarlyStopping, TensorBoard\nfrom keras.layers import LeakyReLU\nfrom keras.optimizers import Adam, RMSprop, SGD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_ann():\n    # Instantiate a model\n    model = Sequential()\n    # Add hidden layer \n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.5))\n    # Add hidden layer\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.5))\n    # Add hidden layer\n    model.add(Dense(54, activation='relu'))\n    model.add(Dropout(0.5))\n    # Add output layer\n    model.add(Dense(1))\n    \n    # Compile the model\n    model.compile(optimizer='adam', loss=\"mae\")\n    \n    return model\n\n\ndef evaluate_model(model):\n    y_pred = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    print(f\"RMSE: {rmse}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n\nann_model = create_ann()\n\nann_model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=128, epochs=100, callbacks=[early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_df = pd.DataFrame(ann_model.history.history).head()\nloss_df.plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(ann_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's use KFold to make sure we train our model on different samples and take a mean of it."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_ann():\n    # Instantiate a model\n    model = Sequential()\n    # Add hidden layer \n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.5))\n    # Add hidden layer\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.5))\n    # Add hidden layer\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.5))\n    # Add output layer\n    model.add(Dense(1))\n    \n    # Compile the model\n    model.compile(optimizer=RMSprop(lr=0.001), loss=\"mean_squared_error\")\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=45)\noof = np.zeros(len(X))\nscore_list = []\nfold = 1\n\ny_pred_list = []\nfor train_idx, test_idx in kf.split(X):\n    X_train, X_val = X[train_idx], X[test_idx]\n    y_train, y_val = y[train_idx], y[test_idx]\n    \n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_val = sc.transform(X_val)\n    \n    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n    \n    ann_model = create_ann()\n    ann_model.fit(X_train, \n                  y_train, \n                  validation_data=(X_val, y_val), \n                  batch_size=128, epochs=30, \n                  callbacks=[early_stop])\n    \n    \n    y_pred_list.append(ann_model.predict(X_val))\n    \n    \n    oof[test_idx] = np.mean(y_pred_list, axis=0).reshape(len(X_val),)\n    score = np.sqrt(mean_squared_error(y_val, oof[test_idx]))\n    score_list.append(score)\n    print(f\"RMSE fold -{fold} : {score}\")\n    fold +=1\n    \nprint(f\"RMSE mean 5 folds: {np.mean(score_list)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"RMSE mean 5 folds: {np.mean(score_list)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submmit to Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nscaled_X = sc.fit_transform(X)\nscaled_test = sc.transform(test)\n\nann_model.fit(scaled_X, y)\nann_y_pred = ann_model.predict(scaled_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_k = pd.DataFrame({\"id\": id_col,\n                      \"target\": ann_y_pred.reshape(-1)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_k.to_csv(\"sub_tbs_feb_ann.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My score on submission was 0.87081 and as we compete in this competition in 4th and 5th decimal point this score isn't good at all.\nSo dear Kagglers can someone point me to the right direction and tell me what else can be done to improve this neural network?"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}