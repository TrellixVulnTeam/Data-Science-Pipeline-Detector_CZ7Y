{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tabular playground - February\n\n# Optimization of hyperparameters"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBRegressor\n        \ninput_path = Path('/kaggle/input/tabular-playground-series-feb-2021/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read in the data files\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(input_path / 'train.csv', index_col='id')\nprint(\"Train dataset\")\ndisplay(train.head())\ntest = pd.read_csv(input_path / 'test.csv', index_col='id')\nprint(\"Test dataset\")\ndisplay(test.head())\nsubmission = pd.read_csv(input_path / 'sample_submission.csv', index_col='id')\nprint(\"Sample submission\")\ndisplay(submission.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizations"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain.hist(bins=100,figsize=(20,15))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = [cat for cat in train.columns if train[cat].dtype =='object']\nprint(\"Categorical features: \",categorical)\nnumerical = [num for num in train.columns if train[num].dtype =='float64']\nprint(\"Numerical features: \", numerical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncorr_matrix=train.corr()\ncorr = corr_matrix\n#corr = corr_matrix.drop(['id', 'target','bt']).drop(['id', 'target','bt'], axis=1)\nplt.figure(figsize=(16,10))\nsns.heatmap(corr,  annot=True, vmin=-1, vmax=1,cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is very little correlation with the target."},{"metadata":{},"cell_type":"markdown","source":"## Handling outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.plot(figsize = (12,8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# box plot\ntrain.boxplot(figsize = (12,8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# convert outliers to quantiles\ntrain[numerical]=train[numerical].clip(lower=train[numerical].quantile(0.0001), upper=train[numerical].quantile(0.9999), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# box plot\ntrain.boxplot(figsize = (12,8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.plot(figsize = (12,8))\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing with categorical features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualization of categorical features\n\nfig, ax = plt.subplots(10,2, figsize=(15, 25))\ni=0\nfor variable, subplot in zip(categorical, ax.flatten()):\n    sns.countplot(x=train[variable], ax=ax[i,0])\n    ax[i,0].set_yscale(\"log\")\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)\n    sns.boxplot(x=variable, y='target', data=train, ax=ax[i,1])\n    ax[i,1].set_ylim([4, 9])\n    i=i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#label encoder\nfor c in train.columns:\n    if train[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(train[c].values)\n        test[c] = lbl.transform(test[c].values)\n        \ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pull out the target, and make a validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train.pop('target')\nX_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.60, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Choosing the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef check_models(X_train, y_train, X_test, y_test):\n    model_names = [ \"Random Forest\", \"XGB\"]\n\n    models = [\n        RandomForestRegressor(n_estimators=50, n_jobs=-1, random_state=42),\n        XGBRegressor(random_state=42)]\n\n    for name, model in zip(model_names, models):\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        print(name, mean_squared_error(y_test, y_pred, squared=False))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#check_models(X_train, y_train, X_test, y_test)\n\n'''Results:\nRandom Forest 0.8647820794446256\nXGB 0.8514728317890382'''\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# It look like XGBRegressor did the best. \n\n# Hyperparameter tuning with GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbreg=XGBRegressor()\n\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = [\n{'eta':[0.3, 0.4],\n 'n_estimators':[500,1000,1500],\n  'n_jobs':[3,4,5],\n 'max_depth': [ 3, 4],\n 'alpha':[2,3,4],\n 'tree_method': ['gpu_hist'],\n 'gpu_id': [0],\n 'predictor': ['gpu_predictor'],\n 'seed': [42]\n},\n]\n\n#grid_search = GridSearchCV(xgbreg, param_grid, cv=3, scoring='neg_mean_squared_error')\n#grid_search.fit(X_train, y_train)\n\n#grid_search.best_params_\n\n\n\"\"\"Results:\n\n{'alpha': 4,\n 'eta': 0.3,\n 'gpu_id': 0,\n 'max_depth': 3,\n 'n_estimators': 500,\n 'n_jobs': 3,\n 'predictor': 'gpu_predictor',\n 'seed': 42,\n 'tree_method': 'gpu_hist'}\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use the best params and train model on all dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = XGBRegressor(n_estimators=500, n_jobs=3, alpha=4, eta=0.3, gpu_id=0, max_depth=3, predictor='gpu_predictor', seed=42, tree_method='gpu_hist')\n'''model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_test, y_test)],\n             verbose=False)'''\nmodel.fit(train, target)\nsubmission['target'] = model.predict(test)\nsubmission.to_csv('XGB.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model.predict(X_test)\nprint(mean_squared_error(y_test, y_pred, squared=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}