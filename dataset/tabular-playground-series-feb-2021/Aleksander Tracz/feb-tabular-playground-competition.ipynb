{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Preparing the environment**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n        \ninput_path = Path('/kaggle/input/tabular-playground-series-feb-2021/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read in the data files"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(input_path / 'train.csv', index_col='id')\ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(input_path / 'test.csv', index_col='id')\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(input_path / 'sample_submission.csv', index_col='id')\ndisplay(submission.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pull out the target, and make a validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train.pop('target')\nX_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.65)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding the categoricals\n\nAs we are not sure whether the categorical variables are ordinal or not, the safer approach will be to use one-hot encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"obj_columns = [col for col in X_train.columns if X_train[col].dtype == 'object']\n\nohe = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\ndummies_X_train = pd.DataFrame(ohe.fit_transform(X_train[obj_columns]))\ndummies_X_test = pd.DataFrame(ohe.transform(X_test[obj_columns]))\ndummies_test = pd.DataFrame(ohe.transform(test[obj_columns]))\n\ndummies_X_train.index = X_train.index\ndummies_X_test.index = X_test.index\ndummies_test.index = test.index\n\nnum_X_train = X_train.drop(obj_columns, axis = 1)\nnum_X_test = X_test.drop(obj_columns, axis = 1)\nnum_test = test.drop(obj_columns, axis = 1)\n\nX_train = pd.concat([num_X_train, dummies_X_train], axis = 1)\nX_test = pd.concat([num_X_test, dummies_X_test], axis = 1)\ntest = pd.concat([num_test, dummies_test], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How well can we do with a completely naive model?\n\nWe'll want any of our models to do (hopefully much!) better than this."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's get a benchmark score\nmodel_dummy = DummyRegressor(strategy='median')\nmodel_dummy.fit(X_train, y_train)\ny_dummy = model_dummy.predict(X_test)\nscore_dummy = mean_squared_error(y_test, y_dummy, squared=False)\nprint(f'{score_dummy:0.5f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple Linear Regression\n\nA simple linear regression doesn't do better than our dummy regressor! (Alghouth, simple categorical encoding really doesn't make sense for this approach!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple Linear Regression\nmodel_simple_linear = LinearRegression(fit_intercept=True) # data is not centered, we need an intercept!\nmodel_simple_linear.fit(X_train, y_train)\ny_simple_linear = model_simple_linear.predict(X_test)\nscore_simple_linear = mean_squared_error(y_test, y_simple_linear, squared=False)\nprint(f'{score_simple_linear:0.5f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# This seems slow and repetative. Can we automate it a bit?"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_results(name, y, yhat, num_to_plot=10000, lims=(0,12), figsize=(6,6)):\n    plt.figure(figsize=figsize)\n    score = mean_squared_error(y, yhat, squared=False)\n    plt.scatter(y[:num_to_plot], yhat[:num_to_plot])\n    plt.plot(lims, lims)\n    plt.ylim(lims)\n    plt.xlim(lims)\n    plt.title(f'{name}: {score:0.5f}', fontsize=18)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_names = [\"Dummy Median\", \"Lasso\", \"Random Forest\", \"XGBoost\"]\n\nmodels = [\n    DummyRegressor(strategy='median'),\n    Lasso(fit_intercept=True),\n    RandomForestRegressor(n_estimators=50, n_jobs=-1),\n    xgb.XGBRegressor(objective = \"reg:linear\", learning_rate = 0.1, max_depth = 10, n_estimators = 50)]\n\nfor name, model in zip(model_names, models):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    plot_results(name, y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# It looks like XGBoost performs the best (surprise surprise:)). Now it is time to tweak some of the parameters to further decrease the error and test the algorithm on a cross-validated data set.\n\nFirst trying to assess potential ranges of parameters to grid-search for later on by tweaking parameters one-by-one."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = xgb.XGBRegressor()\n\n#model.fit(train, target)\n#submission['target'] = model.predict(test)\n#submission.to_csv('random_forest.csv')\n\nlearning_rates = [0.003, 0.01, 0.03, 0.1, 0.3]\n\nfor rate in learning_rates:\n    xgb_lrate = xgb.XGBRegressor(objective = 'reg:squarederror', learning_rate = rate)\n    xgb_lrate.fit(X_train, y_train)\n    y_pred = xgb_lrate.predict(X_test)\n    score = mean_squared_error(y_test, y_pred, squared=False)\n    print(f\"Learning rate: {rate}, score: {score}\")\n    \n# Based on the results I can reduce the learning rates span to [0.03, 0.1, 0.3]\n    \nns_estimators = [30, 50, 100, 300, 1000]\n\nfor no in ns_estimators:\n    xgb_lrate = xgb.XGBRegressor(objective = 'reg:squarederror', n_estimators = no)\n    xgb_lrate.fit(X_train, y_train)\n    y_pred = xgb_lrate.predict(X_test)\n    score = mean_squared_error(y_test, y_pred, squared=False)\n    print(f\"Number of estimators: {no}, score: {score}\")\n    \n# Based on the results I will reduce the number of estimators span to [30, 50, 100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the ranges of parameters I found suitable for further analysis, I will run grid search for optimal combination of parameters to use to train the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'objective':['reg:squarederror'],\n              'learning_rate': [0.03, 0.1, 0.3],\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [3],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.8],\n              'n_estimators': [30, 50, 100]}\n\nxgboost = GridSearchCV(model, parameters, n_jobs=3, scoring='neg_root_mean_squared_error', \n                       verbose=2, refit=True)\n\nxgboost.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now I can fit the XGBoost with optimal parameters to predict values and verify its accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final = xgboost.predict(X_test)\nscore_final = mean_squared_error(y_test, y_pred_final, squared=False)\n\nprint(f\"The optimal parameters are: {best_parameters}. \\nThey yield following RMSE value: {score_final}.\")\n\nplot_results(\"Final XGBoost\", y_test, y_pred_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## As you can see, I have not gained much improvement by grid-searching for optimal parameters for my XGBoost algorithm. Having considered differences in running time between default XGBoost and XGBoost with grid-searched parameters and their corresponding RMSE values, use of grid-search in this case is arguable."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = xgboost.predict(test)\nsubmission.to_csv('random_forest.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}