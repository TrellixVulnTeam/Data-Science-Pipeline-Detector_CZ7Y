{"cells":[{"metadata":{},"cell_type":"markdown","source":"Sometimes, there are more effective model than lightgbm or xgboost. So I tried __all regressors in scikit-learn including lightgbm, xgboost and catboost__. I didn't do any parameter tuning (too many params...). I don't know exactly about some of regressors, but I tried as many as I can. "},{"metadata":{},"cell_type":"markdown","source":"# packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nimport gc\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nfrom sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neighbors import RadiusNeighborsRegressor, KNeighborsRegressor\nfrom sklearn.linear_model import GammaRegressor, HuberRegressor, PassiveAggressiveRegressor, PoissonRegressor\nfrom sklearn.linear_model import RANSACRegressor, SGDRegressor, TheilSenRegressor, TweedieRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor as ens_ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor, RandomForestRegressor\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor \n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# loading files"},{"metadata":{"trusted":true},"cell_type":"code","source":"path= '../input/tabular-playground-series-feb-2021/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nss = pd.read_csv(path + 'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_validation_df(X_train, y_train):\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n    return X_train, X_val, y_train, y_val\n\ndef get_preprocessed_df(train_df, test_df):\n    \n    train = train_df.copy()\n    test = test_df.copy()\n    \n    cat_cols = ['cat{}'.format(i) for i in range(10)]\n    cont_cols = ['cont{}'.format(i) for i in range(14)]\n\n    X_train = train[cat_cols+cont_cols]\n    y_train = train['target']\n    X_test = test.drop('id',axis=1)\n    del train, test\n    \n    # continuous - standard scaling\n    st_scaler = StandardScaler()\n    st_scaler.fit(X_train[cont_cols])\n    X_train[cont_cols] = st_scaler.transform(X_train[cont_cols])\n    X_test[cont_cols] = st_scaler.transform(X_test[cont_cols])\n    \n    # categorical -> one-hot encoding\n    oh_enc = OneHotEncoder()\n    oh_enc.fit(X_train[cat_cols])\n    oh_cols = oh_enc.get_feature_names(cat_cols)\n\n    X_train[oh_cols] = oh_enc.transform(X_train[cat_cols]).toarray()\n    X_test[oh_cols] = oh_enc.transform(X_test[cat_cols]).toarray()    \n    \n    X_train.drop(cat_cols, axis=1, inplace=True)\n    X_test.drop(cat_cols, axis=1, inplace=True)\n    \n    X_train, X_val, y_train, y_val = get_validation_df(X_train, y_train)\n    \n    return X_train, X_val, y_train, y_val, X_test\n\ndef eval_score(y_true, pred):\n\n    print('rmse:', rmse)\n    print('mae:', mae)\n    print('r2:', r2)\n    \n    return rmse, mae, r2\n    \ndef eval_model(model, X_train, X_val, y_train, y_val, model_name, scores_dict):\n    start = time.time()\n    \n    model.fit(X_train, y_train)\n    pred = model.predict(X_val)\n    \n    rmse = round(mean_squared_error(y_val, pred, squared=False),4)\n    mae = round(mean_absolute_error(y_val, pred),4)\n    r2 = round(r2_score(y_val,pred),4)\n    print('rmse:', rmse)\n    print('mae:', mae)\n    print('r2:', r2)\n    \n    scores_dict[model_name] = [rmse, mae, r2]\n    \n    print('execution time: {:.4f}s'.format(time.time()-start))\n    \n    return scores_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val, X_test = get_preprocessed_df(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train set:', X_train.shape, y_train.shape)\nprint('Validation set:', X_val.shape, y_val.shape)\nprint('Test set:', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# making models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# making models\n\ndt_reg = DecisionTreeRegressor()\net_reg = ExtraTreeRegressor()\nmlp_reg = MLPRegressor()\n#rn_reg = RadiusNeighborsRegressor(n_jobs=-1) it raises an error\n#kn_reg = KNeighborsRegressor(n_jobs=-1) it takes so long\n#gm_reg = GammaRegressor() Some value(s) of y are out of the valid range for family GammaDistribution\nhn_reg = HuberRegressor()\npa_reg = PassiveAggressiveRegressor()\nps_reg = PoissonRegressor()\nrs_reg = RANSACRegressor() \nsgd_reg = SGDRegressor()\n#ts_reg = TheilSenRegressor() over memory \ntwd_reg = TweedieRegressor()\n#gp_reg = GaussianProcessRegressor() over memory\neet_reg = ens_ExtraTreesRegressor()\nab_reg = AdaBoostRegressor()\nbg_reg = BaggingRegressor()\ngb_reg = GradientBoostingRegressor()\nrf_reg = RandomForestRegressor()\nhgb_reg = HistGradientBoostingRegressor()\nxgb_reg = XGBRegressor()\nlgbm_reg = LGBMRegressor()\ncat_reg = CatBoostRegressor()\n\nmodels = [\n    ('DecisionTreeRegressor', dt_reg),\n    ('ExtraTreeRegressor', et_reg),\n    ('MLPRegressor', mlp_reg),\n    #('RadiusNeighborsRegressor', rn_reg)\n    #('KNeighborsRegressor', kn_reg),\n    #('GammaRegressor', gm_reg),\n    ('HuberRegressor', hn_reg),\n    ('PassiveAggressiveRegressor', pa_reg),\n    ('PoissonRegressor', ps_reg),\n    ('RANSACRegressor', rs_reg),\n    ('SGDRegressor', sgd_reg),\n    #('TheilSenRegressor', ts_reg),\n    ('TweedieRegressor', twd_reg),\n    #('GaussianProcessRegressor', gp_reg),\n    ('EnsembleExtraTreesRegressor', eet_reg),\n    ('AdaBoostRegressor', ab_reg),\n    ('BaggingRegressor', bg_reg),\n    ('GradientBoostingRegressor', gb_reg),\n    ('RandomForestRegressor', rf_reg),\n    ('HistGradientBoostingRegressor', hgb_reg),\n    ('XGBRegressor', xgb_reg),\n    ('LGBMRegressor', lgbm_reg),\n    ('CatBoostRegressor', cat_reg)\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# model fitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_dict = dict()\nfor model_name, model in models:\n    print('#',model_name)\n    scores_dict = eval_model(model, X_train, X_val, y_train, y_val, model_name, scores_dict)\n    print()\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# results"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.DataFrame.from_dict(scores_dict, orient='index', columns=['rmse','mae','r2'])\nresult.drop('RANSACRegressor',axis=0, inplace=True) # result outlier \nresult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With basic params, __Catboost__ made best score"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\ndata = result.sort_values('rmse')\nfig = px.bar(data, x=data.index, y='rmse', color=data['rmse'])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = result.sort_values('mae')\nfig = px.bar(data, x=data.index, y='mae', color=data['mae'])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"actually, __r2_score__ was meaningless in this experiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = result.sort_values('r2')\nfig = px.bar(data, x=data.index, y='r2', color=data['r2'])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss['target'] = cat_reg.predict(X_test)\nss.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}