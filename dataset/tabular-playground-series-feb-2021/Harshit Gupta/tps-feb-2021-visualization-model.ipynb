{"cells":[{"metadata":{},"cell_type":"markdown","source":"## <a>Introduction</a>\n\nWelcome to this new competition series by Kaggle. This is somewhat in between basic playground competitions and competitive featured ones. \n\nIn this competition, we are given a regression task. We will be predicting a continuous target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat9 are categorical, and the feature columns cont0 - cont13 are continuous.\n\nLet's get started."},{"metadata":{},"cell_type":"markdown","source":"## <a>Loading Packages and Data</a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os, gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport lightgbm as lgb\nimport xgboost as xgb\nimport optuna\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, GridSearchCV, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelEncoder\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH = '../input/tabular-playground-series-feb-2021/'\n\ntrain = pd.read_csv(PATH + 'train.csv')\ntest = pd.read_csv(PATH + 'test.csv')\nsample = pd.read_csv(PATH + 'sample_submission.csv')\n\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both train and test are medium sized datasets. Let's take a look at the train set.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've no missing values in the train and test sets. Let's move on to EDA."},{"metadata":{},"cell_type":"markdown","source":"## <a>EDA</a>\n\nLet's first check the distribution of target variable.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(16, 6))\nsns.distplot(train['target'], ax=ax[0])\nsns.boxplot(train['target'], ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just like the target variable distribution of TPS Jan 2021, the target variable has a bimodel distribution. Outliers are present. We'll be using LightGBM so no need for transformations for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FEATURES = train.drop(['id', 'target'], 1).columns\nFEATURES","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(7, 2, figsize=(16, 40))\n\nax = ax.flatten()\n\ncont_features = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6',\n       'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n\nfor k, i in enumerate(cont_features):\n    sns.distplot(train[i], ax=ax[k], hist=False, label='train')\n    sns.distplot(test[i], ax=ax[k], hist=False, label='test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the features are multimodal with varying number of peaks. The feature distributions from train and test set are almost same.  Let's check the categorical features now."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(10, 2, figsize=(16, 50))\nax = ax.flatten()\n\ncat_features = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9']\n\nfor k, i in enumerate(cat_features):\n    sns.countplot(train[i], ax=ax[2*k], label='train')\n    sns.countplot(test[i], ax=ax[(2*k)+1], label='test')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The countplots look same for cat0, cat1, cat2, cat4, cat5, cat6 and different for the rest. Let's look at the correlations now. We can LabelEncode the categorial variables before plotting correlation matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in cat_features:\n    le = LabelEncoder()\n    le.fit(train[i])\n    train[i] = le.transform(train[i])\n    test[i] = le.transform(test[i])\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(x, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. There is a correlation cluster from cont5 to cont12, but the highest value of correlation coefficient is 0.63, so no need to drop any features.\n2. Features are not correlated to the target.\n3. This is very similar to the datasets in TPS Jan2021."},{"metadata":{},"cell_type":"markdown","source":"## <a>Model</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=5, shuffle=True)\ncv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[FEATURES]\ny = train.target\nprint(X.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.LGBMRegressor()\nmodel\n\nNUM_BOOST_ROUNDS = 20000\nEARLY_STOPPING_ROUNDS = 500\nVERBOSE_EVAL = 0\n\noof_df = train[['id', 'target']].copy()\nfold_ = 1\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.3, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    train_set = lgb.Dataset(X_train, y_train)\n    val_set = lgb.Dataset(X_val, y_val)\n\n    param = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"verbosity\": 1,\n        \"boosting_type\": \"gbdt\",\n\n\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 0, 256),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 31),\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 10),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.4, 10),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 0.9),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 0.9),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 5, 15),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n\n    model = lgb.train(param,\n                      train_set, \n                         num_boost_round=NUM_BOOST_ROUNDS,\n                         early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                         verbose_eval=VERBOSE_EVAL,\n                         valid_sets=[train_set, val_set])\n    \n    val_preds = model.predict(X_val, num_iteration=model.best_iteration)\n    scc = math.sqrt(mean_squared_error(val_preds, y_val))\n    return -1*scc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-output":true},"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\ntrial = study.best_trial\ntrial.params['metric'] = 'rmse'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(trial.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-output":true},"cell_type":"code","source":"for train_idx, val_idx in cv.split(X, y):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    train_set = lgb.Dataset(X_train, y_train)\n    val_set = lgb.Dataset(X_val, y_val)\n\n    model = lgb.train(trial.params,\n                      train_set,\n                      num_boost_round=NUM_BOOST_ROUNDS,\n                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                      verbose_eval=-10,\n                      valid_sets=[train_set, val_set]\n                      )\n\n    val_preds = model.predict(X_val, num_iteration=model.best_iteration)\n    test_preds = model.predict(\n        test[FEATURES], num_iteration=model.best_iteration)\n\n    oof_df.loc[oof_df.iloc[val_idx].index, 'oof'] = val_preds\n    sample[f'fold{fold_}'] = test_preds\n\n    score = mean_squared_error(\n        oof_df.loc[oof_df.iloc[val_idx].index]['target'], oof_df.loc[oof_df.iloc[val_idx].index]['oof'])\n    print(math.sqrt(score))\n    fold_ += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(math.sqrt(mean_squared_error(oof_df.target, oof_df.oof)))\nsample['target'] = sample.drop(['id', 'target'], 1).mean(axis=1)\nsample[['id', 'target']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}