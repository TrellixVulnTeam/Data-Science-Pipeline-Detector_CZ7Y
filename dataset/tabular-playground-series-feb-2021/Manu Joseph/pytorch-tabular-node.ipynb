{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# install PyTorch Tabular first\n!pip install pytorch_tabular\n# This is for a custom optimizer. PyTorch Tabular is flexible enough to use custom optimizers\n!pip install torch_optimizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.figure_factory as ff\n# NODE and ML tools\nfrom sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom pytorch_tabular import TabularModel\nfrom pytorch_tabular.models import CategoryEmbeddingModelConfig, NodeConfig, TabNetModelConfig\nfrom pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig, ExperimentConfig\nfrom pytorch_tabular.categorical_encoders import CategoricalEmbeddingTransformer\nfrom torch_optimizer import QHAdam\nimport category_encoders as ce\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport time\nimport wandb\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_stratified_folds_for_regression(data_df, n_splits=5):\n    \"\"\"\n    @param data_df: training data to split in Stratified K Folds for a continous target value\n    @param n_splits: number of splits\n    @return: the training data with a column with kfold id\n    \"\"\"\n    data_df['kfold'] = -1\n    # randomize the data\n    data_df = data_df.sample(frac=1).reset_index(drop=True)\n    # calculate the optimal number of bins based on log2(data_df.shape[0])\n    num_bins = np.int(np.floor(1 + np.log2(len(data_df))))\n    print(f\"Num bins: {num_bins}\")\n    # bins value will be the equivalent of class value of target feature used by StratifiedKFold to \n    # distribute evenly the classed over each fold\n    data_df.loc[:, \"bins\"] = pd.cut(pd.to_numeric(data_df['target'], downcast=\"signed\"), bins=num_bins, labels=False)\n    kf = StratifiedKFold(n_splits=n_splits)\n    \n    # set the fold id as a new column in the train data\n    for f, (t_, v_) in enumerate(kf.split(X=data_df, y=data_df.bins.values)):\n        data_df.loc[v_, 'kfold'] = f\n    \n    # drop the bins column (no longer needed)\n    data_df = data_df.drop(\"bins\", axis=1)\n    \n    return data_df\n\ndef create_stratified_shuffle_split_for_regression(data_df):\n    \"\"\"\n    @param data_df: training data to split in Stratified K Folds for a continous target value\n    @param n_splits: number of splits\n    @return: the training data with a column with kfold id\n    \"\"\"\n    data_df['kfold'] = -1\n    # randomize the data\n    data_df = data_df.sample(frac=1).reset_index(drop=True)\n    # calculate the optimal number of bins based on log2(data_df.shape[0])\n    num_bins = np.int(np.floor(1 + np.log2(len(data_df))))\n    print(f\"Num bins: {num_bins}\")\n    # bins value will be the equivalent of class value of target feature used by StratifiedKFold to \n    # distribute evenly the classed over each fold\n    data_df.loc[:, \"bins\"] = pd.qcut(pd.to_numeric(data_df['target'], downcast=\"signed\"), q=num_bins, labels=False)\n    kf = StratifiedShuffleSplit(n_splits=1, test_size =0.2, random_state=42)\n    \n    # set the fold id as a new column in the train data\n    for f, (t_, v_) in enumerate(kf.split(X=data_df, y=data_df.bins.values)):\n        data_df.loc[v_, 'kfold'] = f\n    \n    # drop the bins column (no longer needed)\n    data_df = data_df.drop(\"bins\", axis=1)\n    \n    return data_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/train.csv', index_col='id')\ndf_test = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/test.csv', index_col='id')\nsample = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/sample_submission.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 2\ndf_train = create_stratified_shuffle_split_for_regression(df_train)\ndf_valid = df_train.loc[df_train.kfold==0]\ndf_train = df_train.loc[df_train.kfold!=0]\ndf_train.drop(columns='kfold', inplace=True)\ndf_valid.drop(columns='kfold', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_valid.shape, df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_configs(train):\n    epochs = 50\n    batch_size = 64\n    steps_per_epoch = int((len(train)//batch_size)*0.9)\n    data_config = DataConfig(\n        target=['target'], #target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented\n        continuous_cols=['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6',\n       'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13'],\n        categorical_cols=['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9'],\n        continuous_feature_transform=\"quantile_normal\"\n    )\n    trainer_config = TrainerConfig(\n        auto_lr_find=True, # Runs the LRFinder to automatically derive a learning rate\n        batch_size=batch_size,\n        max_epochs=epochs,\n        early_stopping_patience = 5,\n        gpus=1, #index of the GPU to use. 0, means CPU\n    )\n#     optimizer_config = OptimizerConfig(lr_scheduler=\"OneCycleLR\", lr_scheduler_params={\"max_lr\":0.005, \"epochs\": epochs, \"steps_per_epoch\":steps_per_epoch})\n    optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n#     model_config = CategoryEmbeddingModelConfig(\n#         task=\"regression\",\n#         layers=\"128-64-32\",  # Number of nodes in each layer\n#         activation=\"ReLU\", # Activation between each layers\n#         learning_rate = 1e-3,\n#         batch_norm_continuous_input=True,\n#         use_batch_norm =True,\n#         dropout=0.0,\n#         embedding_dropout=0.0,\n#         initialization=\"kaiming\",\n#         target_range=[(df_train[col].min(),df_train[col].max()) for col in ['target']]\n#     )\n\n    model_config = NodeConfig(\n        task=\"regression\",\n        num_layers=1, # Number of Dense Layers\n        num_trees=2048, #Number of Trees in each layer\n        depth=6, #Depth of each Tree\n        embed_categorical=True, #If True, will use a learned embedding, else it will use LeaveOneOutEncoding for categorical columns\n        learning_rate = 1e-3,\n        additional_tree_output_dim = 25,\n        target_range=[(df_train[col].min(),df_train[col].max()) for col in ['target']]\n    )\n#     model_config = TabNetModelConfig(\n#         task=\"regression\",\n#         n_d=8,\n#         n_a=8,\n#         n_steps=3,\n#         gamma=1, #btw 1 and 2\n#         n_independent = 2,\n#         n_shared= 2,\n#         virtual_batch_size=128,\n#         mask_type=\"sparsemax\",\n#         learning_rate = 1e-3,\n#         target_range=[(df_train[col].min(),df_train[col].max()) for col in ['target']]\n#     )\n    return data_config, trainer_config, optimizer_config, model_config","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(y_hat, y):\n    return mean_squared_error(y.detach().cpu().numpy(), y_hat.detach().cpu().numpy(), squared=False)\n\ndef train(train, valid):\n    data_config, trainer_config, optimizer_config, model_config = get_configs(train)\n#     exp_config = ExperimentConfig(\"Tabular Playground Feb PyTorch Tabular\", run_name=\"NODE\", log_target=\"wandb\")\n    tabular_model = TabularModel(\n        data_config=data_config,\n        model_config=model_config,\n        optimizer_config=optimizer_config,\n        trainer_config=trainer_config,\n#         experiment_config = exp_config\n    )\n    # fit model\n    tabular_model.fit(train=train, validation=valid, optimizer=QHAdam, \n                  optimizer_params={\"nus\": (0.7, 1.0), \"betas\": (0.95, 0.998), \"weight_decay\": 0})\n    result = tabular_model.evaluate(valid)\n    return tabular_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = train(df_train, df_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample['target'] = pred[\"target_prediction\"].values\nsample.to_csv('submission.csv', index=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}