{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>Tabular Playground- Feb 2021"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_validate, learning_curve\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.feature_selection import RFE\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/tabular-playground-series-feb-2021/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Dropping the ID column"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=[\"id\"],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>datatype of all the variables look good"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Visually mean and median looks to be close for most of the variables implying no presence of outliers, however further investigation is required"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>All the numeric variables are pretty much unique"},{"metadata":{},"cell_type":"markdown","source":"<h2>Storing numerical, categorical and target variables separately. This makes EDA simpler"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_features = data.select_dtypes(\"float64\").columns[:-1]\nnumeric_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = data.select_dtypes(\"object\").columns\ncategorical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'target'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Looking for % change between min and max of numeric variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[numeric_features].apply(lambda x: (np.abs((x.max() / x.min())-1) * 100),axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Few numeric variables have huge % diff between min and max value"},{"metadata":{},"cell_type":"markdown","source":"<h2>Looking for zero or near-zero variance in numeric variables by looking at std in realtion with median"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[numeric_features].apply(lambda x: ((x.std() / x.median())) * 100,axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>All the continuous variables have good standard dev (variability) relative to the median."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>No missing values"},{"metadata":{},"cell_type":"markdown","source":"<h2>Looking at cardinality of categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in categorical_features:\n    print(f'{i}\\n{(np.round((data[i].value_counts() / len(data[i]))*100,3))}\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Few categorical variables have rare labels, which can be combined together. cat4 maybe dropped as 99% of it is made by a single category"},{"metadata":{},"cell_type":"markdown","source":"<h2>Normality checks"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[numeric_features].hist(figsize=(20,20));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Visually few continuous variables seem to be Gaussian. However, we need to ensure this using Kolmogorov-Smirnov test and qq-plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in numeric_features:   \n    print(f\"Kolmogorov-Smirnov: {i} : {'Not Gaussian' if stats.kstest(data[i],'norm')[1]<0.05 else 'Gaussian'}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in numeric_features:   \n    stats.probplot(data[i],plot=plt)\n    plt.title(i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"QQ-plot and Kolmogorov-Smirnov test confirms that no continuous variable is normally distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(5,2,figsize=(15,20),sharey=False)\nrow = col = 0\nfor n,i in enumerate(categorical_features):\n    if (n % 2 == 0) & (n > 0):\n        row += 1\n        col = 0\n    sns.boxplot(x=data[i],y=data[target],ax=ax[row,col])\n    ax[row,col].set_title(f\"Target vs {i}\")\n    ax[row,col].set_xlabel(\"\")\n    col += 1\n    \n    \nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Most of the categorival variables show no relationship with the target. But few show up some sort of relationship."},{"metadata":{},"cell_type":"markdown","source":"<h2>Outlier detection"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(7,2,figsize=(15,30),sharey=False)\nrow = col = 0\nfor n,i in enumerate(numeric_features):\n    if (n % 2 == 0) & (n > 0):\n        row += 1\n        col = 0\n    sns.boxplot(y=data[i],ax=ax[row,col])\n    ax[row,col].set_title(f\"{i}\")\n    ax[row,col].set_ylabel(\"\")\n    col += 1\n    \n    \nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Few numeric variables have outliers as shown in box plots"},{"metadata":{},"cell_type":"markdown","source":"<h2>Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,12))\nsns.heatmap(data.corr(),mask=np.triu(data.corr()),annot=True,cbar=False,fmt=\".2f\",robust=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>No continuous variable is linearly correlated with the target. However, there is a fair amount of multicollinearity."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(7,2,figsize=(15,40),sharey=False)\nrow = col = 0\nfor n,i in enumerate(numeric_features):\n    if (n % 2 == 0) & (n > 0):\n        row += 1\n        col = 0\n    sns.scatterplot(x=i,y=target,data=data,ax=ax[row,col])\n    ax[row,col].set_title(f\"Target vs {i}\")\n    ax[row,col].set_xlabel(\"\")\n    col += 1\n    \n    \nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,12))\nsns.heatmap(data.corr(method='kendall'),mask=np.triu(data.corr()),annot=True,cbar=False,fmt=\".2f\",robust=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>The scatter plots, Pearson coeff and Kendall coeff show no linear or monotonic relationship between the numeric and target variables"},{"metadata":{},"cell_type":"markdown","source":"<h2>Recursively eliminating multicollinarity in numeric features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_vif  = data[numeric_features].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif =  [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\nprint(vif)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flag = True\ncorrelated_features_to_delete = []\nwhile flag == True:\n    vif =  pd.Series([variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])])\n    if vif.max() >= 10:\n        max_vif_col = pd.Series(X_vif.columns)[vif.argmax()]\n        correlated_features_to_delete.append(max_vif_col)\n        X_vif.drop(columns=max_vif_col,inplace=True)\n    else:\n        flag = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlated_features_to_delete","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Checking for multicollinearity after removing the correlated features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_vif.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif =  [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\nprint(vif)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multi_coll_features = pd.DataFrame({'feature':correlated_features_to_delete})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multi_coll_features.to_csv(\"correlated_features.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Target variable distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['target'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y=data['target']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['target'].plot(kind=\"kde\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['target'].quantile(0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['target'].quantile(0.995)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.probplot(data['target'],plot=plt);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>EDA findings</h1>\n<h3><ol>\n    <li>No strong relationship seen between numeric and target vaiables.</li>\n    <li>Box plots reveal relationship among few categorical variables and target variable.</li>\n    <li>There exists multicollinearity among numeric variables.</li>\n    <li>There are few categorical variables with rare labels.</li>\n    <li>No numeric variable has Gaussian distribution.</li>\n    <li>Few numeric variables have outliers</li>\n    <li>There are no numeric variables with zero or near zero variance relative to median</li>"},{"metadata":{},"cell_type":"markdown","source":"<h2>The above EDA shows that an extensive feature engineering is required for linear models to work on this data, since most of the assumptions like feature normality, non-multicollinearity, linear relationship b/w input features and target are not met. The relationship between the input features and target is also not looking strong and simple. Hence, trying non-linear models may be helpful. We'll build a baseline model with RandomForest."},{"metadata":{},"cell_type":"markdown","source":"<h1>Vanilla RF with cv=3 results</h1>\nTrain_RMSE: [0.32163441, 0.32111051, 0.32184251]\n<br>Test_RMSE: [0.8589247 , 0.86120012, 0.85887925]\n<br>Train_R2: [0.86871826, 0.86878779, 0.86855603]\n<br>Test_R2: [0.06125013, 0.06140248, 0.0612373 ]\n<br>Mean Test RMSE as % of Mean Target: 0.11529479488995717\n\n<br><h1>These results show that the model is extremely overfit."},{"metadata":{},"cell_type":"markdown","source":"<h1>Feature Engineering</h1>"},{"metadata":{},"cell_type":"markdown","source":"<h2>Storing the non-rare labels (labels > 5%) in a csv file would help in test set preparation."},{"metadata":{"trusted":true},"cell_type":"code","source":"non_rare = pd.DataFrame()\nfor i in categorical_features:\n    var_dist = data[i].value_counts().copy()\n    var_dist = (var_dist / var_dist.sum()).copy()\n    non_rare = pd.concat([non_rare,pd.DataFrame({i:var_dist[var_dist>0.05].index})],axis=1).copy()\n\nnon_rare.to_csv('./non_rare_categories.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_rare = pd.read_csv('./non_rare_categories.csv')\nnon_rare","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Looking at the distribution of variables before and after combining 'rare' labels would give an idea to proceed further. "},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data = data.copy()\nfor i in non_rare.columns:\n    new_data.loc[(new_data[i].isin(non_rare[i]) == False), i] = \"Rare\"\n\nfig,ax = plt.subplots(10,2,figsize=(20,40))\nrow = col = 0\nfor n,i in enumerate(non_rare.columns):\n    cat_dist = data[i].value_counts().copy()\n    cat_dist = np.round((cat_dist / cat_dist.sum()) * 100,1).copy()\n    cat_dist.plot(kind=\"bar\",ax=ax[row,0],sharey=False)\n    ax[row,0].set_title(i + \" Before Adding Rare Label\")\n    for n,j in enumerate(cat_dist.index):\n        ax[row,0].text(x=n-0.2,y=cat_dist[j]+0.1,s=str(cat_dist[j]) + \"%\")\n    \n    \n    new_cat_dist = new_data[i].value_counts().copy()\n    new_cat_dist = np.round((new_cat_dist / new_cat_dist.sum()) * 100,1).copy()\n    new_cat_dist.plot(kind=\"bar\",ax=ax[row,1])\n    ax[row,1].set_title(i + \" After Adding Rare Label\")\n    for n,j in enumerate(new_cat_dist.index):\n        ax[row,1].text(x=n-0.2,y=new_cat_dist[j]+0.1,s=str(new_cat_dist[j]) + \"%\")\n    \n    \n    row += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Looking at the above distribution plots, there is no huge change in the categorical variable distribution. Hence, the rare labels can be combined."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in non_rare.columns:\n    data.loc[(data[i].isin(non_rare[i]) == False), i] = \"Rare\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(columns=\"target\").copy()\ny = data[\"target\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target_non_outliers = y.loc[(y>=5) | (y<=10)].index\n# y = y[target_non_outliers].copy()\n# X = X.iloc[target_non_outliers,:].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Not removing correlated variables as it reduced the model performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlated_features = list(pd.read_csv('./correlated_features.csv')['feature'])\n#correlated_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X.drop(columns=correlated_features,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_col = list(X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Commenting the code, since, it takes hours for the GridSearch to complete."},{"metadata":{"trusted":true},"cell_type":"code","source":"#ct = ColumnTransformer(transformers=[['oe',OrdinalEncoder(),categorical_features]],remainder='passthrough')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pipeline = Pipeline(steps=[['ord_encoder',ct],\n#                           ['rfe',RFE(estimator=xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1))],\n#                           ['regressor',xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1)]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# param_grid = {'rfe__n_features_to_select': range(8,21,2),\n#              'regressor__n_estimators':[200,500],\n#              'regressor__max_depth':[4,7,10,12],\n#              'regressor__reg_lambda':[0.01,0.1,1,10,100]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gscv = GridSearchCV(estimator=pipeline,\n#                    param_grid=param_grid,\n#                    scoring=\"neg_root_mean_squared_error\",\n#                    cv=2,\n#                    n_jobs=-1,\n#                    return_train_score=True,\n#                    verbose=11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gscv.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gscv.best_estimator_.get_params()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"{'memory': None,\n 'steps': [('rfe',\n   RFE(estimator=XGBRegressor(base_score=None, booster=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, gamma=None, gpu_id=None,\n                              importance_type='gain', interaction_constraints=None,\n                              learning_rate=None, max_delta_step=None,\n                              max_depth=None, min_child_weight=None, missing=nan,\n                              monotone_constraints=None, n_estimators=100,\n                              n_jobs=-1, num_parallel_tree=None, random_state=11,\n                              reg_alpha=None, reg_lambda=None,\n                              scale_pos_weight=None, subsample=None,\n                              tree_method='gpu_hist', validate_parameters=None,\n                              verbosity=None),\n       n_features_to_select=18)),\n       \n       \n       \n       XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,\n                importance_type='gain', interaction_constraints='',\n                learning_rate=0.300000012, max_delta_step=0, max_depth=4,\n                min_child_weight=1, missing=nan, monotone_constraints='()',\n                n_estimators=200, n_jobs=-1, num_parallel_tree=1, random_state=11,\n                reg_alpha=0, reg_lambda=100, scale_pos_weight=1, subsample=1,\n                tree_method='gpu_hist', validate_parameters=1, verbosity=None)]],\n       \n       "},{"metadata":{"trusted":true},"cell_type":"code","source":"#gscv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pipeline(steps=[('rfe',\n                 RFE(estimator=XGBRegressor(base_score=None, booster=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=None, gamma=None,\n                                            gpu_id=None, importance_type='gain',\n                                            interaction_constraints=None,\n                                            learning_rate=None,\n                                            max_delta_step=None, max_depth=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            n_estimators=10...\n                              colsample_bytree=1, gamma=0, gpu_id=0,\n                              importance_type='gain',\n                              interaction_constraints='',\n                              learning_rate=0.300000012, max_delta_step=0,\n                              max_depth=4, min_child_weight=1, missing=nan,\n                              monotone_constraints='()', n_estimators=200,\n                              n_jobs=-1, num_parallel_tree=1, random_state=11,\n                              reg_alpha=0, reg_lambda=100, scale_pos_weight=1,\n                              subsample=1, tree_method='gpu_hist',\n                              validate_parameters=1, verbosity=None)]])"},{"metadata":{"trusted":true},"cell_type":"code","source":"#gscv.best_score_ * -1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.8540331315535704"},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = ColumnTransformer(transformers=[['oe',OrdinalEncoder(),categorical_features]],remainder='passthrough')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline(steps=[['ord_encoder',ct],\n                          ['rfe',RFE(estimator=xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1),\n                                    n_features_to_select=20)],\n                          ['regressor',xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1,\n                                                       max_depth=4,n_estimators=200,reg_lambda=100)]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Decoding the pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_after_oe = pd.Series(categorical_features)\nfeatures_after_oe = list(features_after_oe.append(pd.Series(x_col)[pd.Series(x_col).isin(features_after_oe)==False]))\nfeatures_after_oe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_selected_rfe = []\nfor n,i in enumerate(features_after_oe):\n    if pipeline[\"rfe\"].support_[n] == True:\n        features_selected_rfe.append(i)\n        \n    print(f'{i}: {pipeline[\"rfe\"].support_[n]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_selected_rfe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Finding Feature Importance For Further Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_imp = (pd.DataFrame(pipeline['regressor'].get_booster().get_score(importance_type=\"gain\"),index=[0]).T).reset_index()\nfeat_imp['index'] = feat_imp['index'].str.replace('f',\"\").astype('int')\nfeat_imp.sort_values(by=\"index\",inplace=True)\nfeat_imp['index'] = features_selected_rfe\nfeat_imp.sort_values(by=0,ascending=False,inplace=True)\nfeat_imp.columns = [\"Feature\",\"Imp\"]\nfeat_imp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'Feature':features_selected_rfe,'Imp':pipeline['regressor'].feature_importances_}).sort_values(by='Imp',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(columns=\"target\").copy()\ny = data[\"target\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target_non_outliers = y.loc[(y>=5) | (y<=10)].index\n# y = y[target_non_outliers].copy()\n# X = X.iloc[target_non_outliers,:].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlated_features = list(pd.read_csv('./correlated_features.csv')['feature'])\n#correlated_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X.drop(columns=correlated_features,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>Creating New Features By Combining The Most Important Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X['cat2p6'] = X['cat2'] + X['cat6']\nX['cat6p1'] = X['cat6'] + X['cat1']\nX['cat2p1'] = X['cat2'] + X['cat1']\n\nX['cat2p0'] = X['cat2'] + X['cat0']\nX['cat6p0'] = X['cat6'] + X['cat0']\nX['cat1p0'] = X['cat1'] + X['cat0']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_categorical_features = list(categorical_features).copy()\nnew_categorical_features.extend(['cat2p6','cat6p1','cat2p1','cat2p0','cat6p0','cat1p0'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_categorical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = ColumnTransformer(transformers=[['oe',OrdinalEncoder(),new_categorical_features]],remainder='passthrough')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline(steps=[['ord_encoder',ct],\n                          ['rfe',RFE(estimator=xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1),\n                                    n_features_to_select=22)],\n                          ['regressor',xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1,\n                                                       max_depth=4,n_estimators=200,reg_lambda=100)]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = cross_validate(estimator=pipeline,X=X,y=y,scoring='neg_root_mean_squared_error',cv=5,n_jobs=-1,return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Training RMSE"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv['train_score'] *-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(cv['train_score'] *-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>CV RMSE"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv['test_score'] *-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(cv['test_score'] *-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Learning Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size,train_scores,test_scores = learning_curve(estimator=pipeline,X=X,y=y,cv=5,scoring=\"neg_root_mean_squared_error\",random_state=42)\ntrain_scores = np.mean(-1*train_scores,axis=1)\ntest_scores = np.mean(-1*test_scores,axis=1)\nlc = pd.DataFrame({\"Training_size\":train_size,\"Training_loss\":train_scores,\"Validation_loss\":test_scores}).melt(id_vars=\"Training_size\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(data=lc,x=\"Training_size\",y=\"value\",hue=\"variable\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>The flat validation loss shows that adding additional examples will not help and training error is high and increasing, hence there is no overfitting for sure. But the high training error suggests the model is underfit."},{"metadata":{},"cell_type":"markdown","source":"<h1>Test Set Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tabular-playground-series-feb-2021/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = test[\"id\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop(columns=\"id\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_rare = pd.read_csv('./non_rare_categories.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in non_rare.columns:\n    test.loc[(test[i].isin(non_rare[i]) == False), i] = \"Rare\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlated_features = list(pd.read_csv('./correlated_features.csv')['feature'])\n#correlated_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test.drop(columns=correlated_features,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['cat2p6'] = test['cat2'] + test['cat6']\ntest['cat6p1'] = test['cat6'] + test['cat1']\ntest['cat2p1'] = test['cat2'] + test['cat1']\n\ntest['cat2p0'] = test['cat2'] + test['cat0']\ntest['cat6p0'] = test['cat6'] + test['cat0']\ntest['cat1p0'] = test['cat1'] + test['cat0']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = pipeline.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv('../input/tabular-playground-series-feb-2021/sample_submission.csv').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_ids) == len(prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id':test_ids,'target':prediction})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission_6.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>\n    Training RMSE is 0.829710734<br><br>\n    CV RMSE is 0.8454180152461298<br><br>\n    Submission score is 0.84502 (Top 53%) at the time of writing<br><br>\n    i.e. the model is not overfit, but the learning curve shows bias. The baseline model built using Random Forest had a CV RMSE of 0.859668. The final model built shows just a marginal improvement in performance compared to the baseline model.The top score in the leaderboard at the time of writing was 0.84100. The difference between my score and the top score is just 0.00402. Since the model has bias, using more complex models like catboost, LightGBM and rigorous hyperparameter tuning to reduce the bias might be helpful in increasing the score. Since, the model doesn't have high variance, the submission score and cv score are very close to each other. This might continue in the public leaderboard as well. REMOVING CORRELATED VARIABLES REDUCED THE MODEL PERFORMANCE."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}