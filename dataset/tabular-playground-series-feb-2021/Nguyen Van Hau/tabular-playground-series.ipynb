{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-23T02:00:14.160993Z","iopub.execute_input":"2021-09-23T02:00:14.161787Z","iopub.status.idle":"2021-09-23T02:00:14.308431Z","shell.execute_reply.started":"2021-09-23T02:00:14.161636Z","shell.execute_reply":"2021-09-23T02:00:14.307582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/tabular-playground-series-feb-2021/train.csv\")\ntest_data = pd.read_csv(\"../input/tabular-playground-series-feb-2021/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-23T02:00:14.313988Z","iopub.execute_input":"2021-09-23T02:00:14.316467Z","iopub.status.idle":"2021-09-23T02:00:18.712959Z","shell.execute_reply.started":"2021-09-23T02:00:14.316409Z","shell.execute_reply":"2021-09-23T02:00:18.712092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']","metadata":{"execution":{"iopub.status.busy":"2021-09-23T02:00:18.714607Z","iopub.execute_input":"2021-09-23T02:00:18.714916Z","iopub.status.idle":"2021-09-23T02:00:18.719874Z","shell.execute_reply.started":"2021-09-23T02:00:18.714877Z","shell.execute_reply":"2021-09-23T02:00:18.718799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Use as many lines of code as you need!\nOH = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nOH_cols_train = pd.DataFrame(OH.fit_transform(train_data[features]))\nOH_cols_valid = pd.DataFrame(OH.transform(test_data[features]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train_data.index\nOH_cols_valid.index = test_data.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = train_data.drop(features, axis=1)\nnum_X_valid = test_data.drop(features, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T02:00:18.721929Z","iopub.execute_input":"2021-09-23T02:00:18.722482Z","iopub.status.idle":"2021-09-23T02:00:22.618325Z","shell.execute_reply.started":"2021-09-23T02:00:18.722435Z","shell.execute_reply":"2021-09-23T02:00:22.617437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = OH_X_train.target\n\nX_train = OH_X_train.copy()\nX_train.drop(['target'], axis=1, inplace=True)\n\nX_valid = OH_X_valid.copy()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T02:00:22.620871Z","iopub.execute_input":"2021-09-23T02:00:22.621212Z","iopub.status.idle":"2021-09-23T02:00:22.902265Z","shell.execute_reply.started":"2021-09-23T02:00:22.621171Z","shell.execute_reply":"2021-09-23T02:00:22.901386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# function for comparing different approaches\nmodel = RandomForestRegressor(n_estimators=200, random_state=0)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T02:00:22.9039Z","iopub.execute_input":"2021-09-23T02:00:22.904221Z","iopub.status.idle":"2021-09-23T02:39:28.937883Z","shell.execute_reply.started":"2021-09-23T02:00:22.904178Z","shell.execute_reply":"2021-09-23T02:39:28.937094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\n# Define the model\nXGBoost_model = XGBRegressor(n_estimators=1000, learning_rate=0.05) #Here\n\n# Fit the model\nXGBoost_model.fit(X_train, y_train)\n\n# Get predictions\npredictions = XGBoost_model.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T02:39:28.939359Z","iopub.execute_input":"2021-09-23T02:39:28.93969Z","iopub.status.idle":"2021-09-23T03:07:55.335847Z","shell.execute_reply.started":"2021-09-23T02:39:28.93965Z","shell.execute_reply":"2021-09-23T03:07:55.334845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'id': test_data.id, 'target': predictions})\noutput.to_csv('submission (XGBoost).csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T03:07:55.337519Z","iopub.execute_input":"2021-09-23T03:07:55.33786Z","iopub.status.idle":"2021-09-23T03:07:55.946438Z","shell.execute_reply.started":"2021-09-23T03:07:55.337821Z","shell.execute_reply":"2021-09-23T03:07:55.945508Z"},"trusted":true},"execution_count":null,"outputs":[]}]}