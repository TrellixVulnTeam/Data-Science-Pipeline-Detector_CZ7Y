{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Imports and load data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '/kaggle/input/tabular-playground-series-feb-2021/'\ntrain_df = pd.read_csv(DATA_PATH + 'train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.cat0.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.target.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.target.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.target.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols=['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6','cat7', 'cat8', 'cat9']\nfor col in cat_cols:\n    plt.figure(figsize=(8,4))\n    train_df[col].value_counts().plot(kind='bar',color='Red', stacked=True)\n    plt.title(col)\n    plt.grid()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\ncont_features = [f'cont{i}' for i in range(14)]\ncat_features = [f'cat{i}' for i in range(10)]\nall_features = cont_features + cat_features\n\ncorr = train_df[all_features+['target']].corr()\nfig = px.imshow(corr)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Awkwardly enough, there doesn't appear to be much correlation between the dependent feature (target) and any of the independent variables. \n\nThis might be why the model baseline I trained last time didn't achieve a very good score (RMSE = about 86) "},{"metadata":{},"cell_type":"markdown","source":"### Countplot to see distribution of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(25,20))\nfor i,feature in enumerate(cat_features):\n    plt.subplot(2,5,i+1);\n    sns.countplot(train_df[feature])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['cat4','cat0', 'cat2', 'cat6', 'cat7']\nfor feature in features: \n    train_df.drop([feature], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some nice scatterplots of highly correlated variables"},{"metadata":{},"cell_type":"markdown","source":"# Training a baseline RandomForestRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.model_selection import train_test_split\n\ny = train_df['target']\nX = train_df.drop('target', axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare categorical variables by encoding them\ncat_cols = ['cat1','cat3', 'cat5', 'cat8', 'cat9']\n\nfor col in cat_cols: \n    X[col] = X[col].astype('category')\n    \nfor col in cat_cols: \n    X[col] = X[col].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=42, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_valid.shape, y_train.shape, y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train a baseline model \nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_jobs=-1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = model.predict(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = model.predict(X_valid)\nmse = mean_squared_error(y_valid, y_preds)\nrmse = np.sqrt(mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse, rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuning the Hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First look at the parameters used for the baseline Random Forest \nfrom pprint import pprint \npprint(model.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = RandomForestRegressor(n_jobs=-1, random_state=42, criterion='rmse')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Randomized search CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train[:2400]\ny_train = y_train[:2400]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reload data\ndf = pd.read_csv(DATA_PATH + 'train.csv')\n\nfeatures = ['cat4','cat0', 'cat2', 'cat6', 'cat7']\nfor feature in features: \n    df.drop([feature], axis=1, inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('target', axis=1)\ny = df['target']\n# prepare categorical variables by encoding them\ncat_cols = ['cat1','cat3', 'cat5', 'cat8', 'cat9']\n\nfor col in cat_cols: \n    X[col] = X[col].astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nrf_best = RandomForestRegressor(n_jobs=-1, n_estimators=600,\n min_samples_split=5,\n min_samples_leaf=1,\n max_features='sqrt',\n max_depth=10,\n bootstrap=True, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=42, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_best.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_best.score(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"y_preds = rf_best.predict(X_valid)\nmse = mean_squared_error(y_valid, y_preds)\nrmse = np.sqrt(mse)"},{"metadata":{"trusted":true},"cell_type":"code","source":"mse, rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating baseline model"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes =[1, 375, 750, 1500, 3000, 6000, 12000, 24000]\n\n\nfrom sklearn.model_selection import learning_curve\ntrain_sizes, train_scores, validation_scores = learning_curve(\nestimator = RandomForestRegressor(n_jobs=-1, n_estimators=600,\n min_samples_split=5,\n min_samples_leaf=1,\n max_features='sqrt',\n max_depth=10,\n bootstrap=True, random_state=42),\nX = X,\ny = y, train_sizes = train_sizes, cv = 5,\nscoring = 'neg_mean_squared_error', \nshuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training scores:\\n\\n', train_scores)\nprint('\\n', '-' * 70) # separator to make the output easy to read\nprint('\\nValidation scores:\\n\\n', validation_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scores_mean = -train_scores.mean(axis = 1)\nvalidation_scores_mean = -validation_scores.mean(axis = 1)\nprint('Mean training scores\\n\\n', pd.Series(train_scores_mean, index = train_sizes))\nprint('\\n', '-' * 20) # separator\nprint('\\nMean validation scores\\n\\n',pd.Series(validation_scores_mean, index = train_sizes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting the learning curves "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.style.use('seaborn')\nplt.plot(train_sizes, train_scores_mean, label = 'Training error')\nplt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\nplt.ylabel('MSE', fontsize = 14)\nplt.xlabel('Training set size', fontsize = 14)\nplt.title('Learning curves for a linear regression model', fontsize = 18, y = 1.03)\nplt.legend()\nplt.ylim(0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare the submission "},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(DATA_PATH + 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read in the sample submission data\nsubmission = pd.read_csv(DATA_PATH + 'sample_submission.csv', index_col='id')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reload data\ndf = pd.read_csv(DATA_PATH + 'train.csv')\nX = df.drop('target', axis=1)\ny = df['target']\n# prepare categorical variables by encoding them\ncat_cols = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7',\n       'cat8', 'cat9']\n\nfor col in cat_cols: \n    X[col] = X[col].astype('category')\n    \nfor col in cat_cols: \n    X[col] = X[col].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# prepare categorical variables by encoding them for the test set\ncat_cols = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7',\n       'cat8', 'cat9']\n\nfor col in cat_cols: \n    test[col] = test[col].astype('category')\n    \nfor col in cat_cols: \n    test[col] = test[col].cat.codes\n    \ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = rf_best\nmodel.fit(X, y)\npreds = model.predict(test)\nsubmission = pd.DataFrame(\n    {'id' : test['id'], \n     'target': preds\n    })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('random_forest.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv('./random_forest.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}