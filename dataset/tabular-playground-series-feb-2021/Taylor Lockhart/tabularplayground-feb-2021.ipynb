{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Imports and Initial Setup\n\nImporting modules/libraries and defining useful functions. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LinearRegression, SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import clone\n\n# trying out xgboost's regression \nfrom xgboost import XGBRegressor\n\n# String variable that can be used to timestamp exported objects\nfrom datetime import datetime\ncurrent_tmstmp = datetime.today().strftime('%Y%m%d')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/test.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def listCols(string_identifier, dataframe):\n    ft_cols = []\n    for field in dataframe.columns:\n        if string_identifier in field:\n            ft_cols.append(field)\n    return ft_cols\n\ndef checkForMissingCats(train, test, categorical_feat_list):\n    # take a list of categorical features from a dataset\n    # compare the distinct list of values between train and test\n    for feature in categorical_feat_list:\n        # print('Comparing categorical feature: ' + str(feature))\n        train_list = []\n        test_list = []\n        missing_train_count = 0\n        missing_test_count = 0\n        for category in train[feature].unique():\n            train_list.append(category)\n        for category in test[feature].unique():\n            test_list.append(category)\n        for val in train_list:\n            if val not in test_list:\n                # print('train', val)\n                missing_train_count += 1 \n        for val in test_list:\n            if val not in train_list:\n                # print('test', val)\n                missing_test_count += 1\n        if missing_train_count != 0 or missing_test_count != 0:\n            print(feature)\n\ndef encodeBinaryLabel(val, one_val):\n    if pd.isna(val):\n        raise ValueError('Null value found!')\n    else:\n        if val == one_val:\n            return 1\n        else:\n            return 0\n\ndef create_folds(dataframe):\n    dataframe['kfold'] = -1\n    data = dataframe.sample(frac = 1).reset_index(drop = True)\n    bin_num = int(np.floor(1 + np.log2(len(data))))\n    data.loc[:, 'bins'] = pd.cut(\n        data['target'], bins = bin_num, labels = False\n    )\n    kfold = StratifiedKFold(n_splits = 5)\n    for f, (t_, v_) in enumerate(kfold.split(X = data, y = data['bins'].values)):\n        data.loc[v_, 'kfold'] = f\n    data = data.drop(labels = ['bins'], axis = 1)\n    return data\n\ndef run_folds(dataframe, fold, drop_cols, model):\n    drop_cols.append('target')\n    df_train = dataframe[dataframe.kfold != fold].reset_index(drop = True)\n    df_val = dataframe[dataframe.kfold == fold].reset_index(drop = True)\n    x_train = df_train.drop(labels = drop_cols, axis = 1).values\n    y_train = df_train['target'].values\n    x_val = df_val.drop(labels = drop_cols, axis = 1).values\n    y_val = df_val['target'].values\n    # switch to just fit if i quit using xgboost regression\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_val)\n    mse = mean_squared_error(y_true = y_val, y_pred = y_pred)\n    sq_mse = np.sqrt(mse)\n    print('Processing fold: ' + str(fold))\n    print('Square Root of MSE: ' + str(sq_mse))\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical and Continuous Features\n\nIt seems that this month's Tabular Playground competition has continuous and categorical features. Some of the categorical columns may be binary so I'll check for those and handle those differently. "},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feats = listCols('cat', train)\ncont_feats = listCols('cont', train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imbalanced Categorical Features:\n\n- cat0\n- cat2\n- cat4\n- cat6\n- cat7"},{"metadata":{},"cell_type":"markdown","source":"It seems like some of the categorical features are not evenly distributed. I'm wondering if it might make sense to remove some of these. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize = (12,6))\n\nsns.countplot(x = 'cat0', data = train, ax = axes[0, 0])\nsns.countplot(x = 'cat2', data = train, ax = axes[0, 1])\nsns.countplot(x = 'cat4', data = train, ax = axes[1, 0])\nsns.countplot(x = 'cat6', data = train, ax = axes[1, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = 'cat7', data = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train[cont_feats].corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].plot(kind = 'hist', bins = 1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Prep"},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = 0\n\nfor col in train.columns:\n    if train[col].isnull().sum() > 0:\n        print(col, train[col].isnull().sum())\n        counter += 1\n\nif counter == 0:\n    print('There are no null values in train set.')\n\ncounter = 0\n\nfor col in test.columns:\n    if test[col].isnull().sum() > 0:\n        print(col, test[col].isnull().sum())\n        counter += 1\n\nif counter == 0:\n    print('There are no null values in the test set.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for any categories that may not exist in both the train and test csv's\n\ncheckForMissingCats(train = train, test = test, categorical_feat_list = cat_feats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since cat6 has different categories between the two datasets, I'm going to combine both dataframes into one and create a new column, train_test_id, that I'll use later to separate the train and test sets. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['train_test_id'] = 'train'\ntest['train_test_id'] = 'test'\ntest['target'] = ''\n\ntrain_and_test = [train, test]\n\ndf = pd.concat(train_and_test)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll encode the binary, categorical features into 0, 1 values and use the get_dummies method to handle the others. "},{"metadata":{"trusted":true},"cell_type":"code","source":"binary_cols = []\n\nfor col in df.columns:\n    if 'cat' in col:\n        print(col, df[col].nunique())\n        if df[col].nunique() == 2:\n            binary_cols.append(col)\n\nfor i in binary_cols:\n    print(i, df[i].unique())\n    df[i] = df[i].apply(lambda x: encodeBinaryLabel(x, 'A'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_cols = []\n\nfor ft in df.columns:\n    if 'cat' in ft and ft not in binary_cols:\n        one_hot_cols.append(ft)\n        \n\ndf = pd.get_dummies(df, columns = one_hot_cols, dummy_na=False)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df.loc[df['train_test_id'] == 'train'].drop(labels = ['train_test_id'], axis = 1)\ntest = df.loc[df['train_test_id'] == 'test'].drop(labels = ['train_test_id', 'target'], axis = 1)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features = []\n\nfor i in train.columns:\n    if 'cont' in i:\n        num_features.append(i)\n\nfor col in num_features:\n    prep = StandardScaler()\n    train[col] = prep.fit_transform(train[[col]])\n    test[col] = prep.transform(test[[col]])\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting train into training and validation sets. "},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train.drop(labels = ['id', 'target'], axis = 1).values\ny = train['target'].values\n\nseed = 7\nnp.random.seed(seed)\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.25, random_state = seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Builds\n\nAs of mid-Feb, my best score has been with a XGBoost Regression model using default params. Going forward, I'm going to try to gain more experience with XGBoost and try out differing param settings. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBRegressor()\n\nmodel.fit(x_train, y_train, eval_set = [(x_val, y_val)], early_stopping_rounds = 50, verbose = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(x_val)\nmse = mean_squared_error(y_true = y_val, y_pred = y_pred)\nnp.sqrt(mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_model = XGBRegressor(max_depth = 4, min_child_weight = 5, gamma = 0.5, alpha = 1)\n\ntuned_model.fit(x_train, y_train, eval_set = [(x_val, y_val)], early_stopping_rounds = 50, verbose = False)\n\ntuned_y_pred = tuned_model.predict(x_val)\nmse = mean_squared_error(y_true = y_val, y_pred = tuned_y_pred)\nnp.sqrt(mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prepped_test = test.drop(labels = ['id'], axis = 1).values\npredictions = tuned_model.predict(prepped_test)\n\nsub = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/sample_submission.csv')\nsub['target'] = predictions\n\nsub.to_csv('tuned_xgb_reg_predictions_{ts}.csv'.format(ts = current_tmstmp), index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A Linear Regression model scored 0.86. As of 02/06, the best score is 0.84191. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# lin_reg = LinearRegression()\n\n# lin_reg.fit(x_train, y_train)\n\n# y_pred = lin_reg.predict(x_val)\n# mse = mean_squared_error(y_true = y_val, y_pred = y_pred)\n# np.sqrt(mse)\n\n# prepped_test = test.drop(labels = ['id'], axis = 1)\n# predictions = lin_reg.predict(prepped_test)\n\n# sub = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/sample_submission.csv')\n# sub['target'] = predictions\n\n# sub.to_csv('lin_reg_predictions.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rf_reg = RandomForestRegressor()\n\n# rf_reg.fit(x_train, y_train)\n\n# y_pred = rf_reg.predict(x_val)\n# mse = mean_squared_error(y_true = y_val, y_pred = y_pred)\n# np.sqrt(mse)\n\n# prepped_test = test.drop(labels = ['id'], axis = 1)\n# predictions = rf_reg.predict(prepped_test)\n\n# sub = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/sample_submission.csv')\n# sub['target'] = predictions\n\n# sub.to_csv('rf_reg_predictions.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = create_folds(train)\n\ndf.kfold.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drops = ['id', 'kfold']\n\nmodel_dict = {}\n\nfor f in range(len(df['kfold'].unique())):\n    model_var = 'model_' + str(f)\n    mdl = run_folds(\n        dataframe = df, \n        fold = f, \n        drop_cols = drops, \n        model = XGBRegressor(max_depth = 4, min_child_weight = 5, gamma = 0.5, alpha = 1)\n    )\n    model_dict[model_var] = mdl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### End\n\nAfter the competition ended, my best submission for the XGBoost Regression model scored 0.84719 on the private leaderboard. I'll wrap up this notebook by exporting the model using joblib in case I would like to work with it again in the future. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import joblib\n\njoblib.dump(model, 'xgb_regr_tab_feb_2021.bin')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}