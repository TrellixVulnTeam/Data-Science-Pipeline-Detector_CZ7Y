{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Feb 2021 Tabular Playground:LGBM with Optuna Tuning "},{"metadata":{},"cell_type":"markdown","source":" This note book is a respomse to to the Kaggle [Tabular Playground Series - Feb 2021 competition.](http://https://www.kaggle.com/c/tabular-playground-series-feb-2021).  The Approach I have taken is as follows:\n \n* Setup including reading in the data \n* Examination of the Data \n* Evaluation of Models (spoiler LGBM wins)\n* Tuning of the model\n* Execution of tuned model\n* Submission of Results\n"},{"metadata":{},"cell_type":"markdown","source":"## Setup and Read Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\n        \ninput_path = Path('/kaggle/input/tabular-playground-series-feb-2021/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read in the data files"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(input_path / 'train.csv', index_col='id')\npd.set_option('display.max_columns', None)\ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(input_path / 'test.csv', index_col='id')\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(input_path / 'sample_submission.csv')\ndisplay(submission.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pull out the target"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train.pop('target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Identify Categorical Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = train.columns\nnum_cols = train._get_numeric_data().columns\ncat_features= list(set(cols) - set(num_cols))\ncat_features.sort()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets Look at our Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot = train.boxplot(column=num_cols.values.tolist(),\n                       figsize=(12,9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot = test.boxplot(column=num_cols.values.tolist(),\n                       figsize=(12,9))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmm, data looks very symetrical (at least on numericals) in test and train, outliers and all"},{"metadata":{},"cell_type":"markdown","source":"## Look at Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\nmatrix = np.triu(train.corr())\ncor = train.corr()\nsns.heatmap(cor, annot=True, mask=matrix,cmap= 'coolwarm', linewidths=.5, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see some strong correlations that could be explore further"},{"metadata":{},"cell_type":"markdown","source":"## Review Categorical Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor i, col in enumerate(cat_features):\n    plt.subplot(5,2,i+1)\n    sns.countplot(x=col,data=train, order=('A','B','C','D','E','F','G','H','I','J','K','L','N'))\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor i, col in enumerate(cat_features):\n    plt.subplot(5,2,i+1)\n    sns.countplot(x=col,data=test, order=('A','B','C','D','E','F','G','H','I','J','K','L','N'))\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again we see a symetry between train and test data "},{"metadata":{},"cell_type":"markdown","source":"## We need to encode the categoricals.\n\nThere are different strategies to accomplish this, and different approaches will have different performance when using different algorithms. For this starter notebook, we'll use simple encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train[feature])\n    train[feature] = le.transform(train[feature])\n    test[feature] = le.transform(test[feature])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make a validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.80)  # change 60 to 80 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.dummy import DummyRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import LassoLars\nfrom sklearn.linear_model import ARDRegression\nfrom sklearn.linear_model import PassiveAggressiveRegressor\nfrom sklearn.linear_model import TheilSenRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_results(name, y, yhat, num_to_plot=10000, lims=(0,12), figsize=(6,6)):\n    plt.figure(figsize=figsize)\n    score = mean_squared_error(y, yhat, squared=False)\n    plt.scatter(y[:num_to_plot], yhat[:num_to_plot])\n    plt.plot(lims, lims)\n    plt.ylim(lims)\n    plt.xlim(lims)\n    plt.title(f'{name}: {score:0.5f}', fontsize=18)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def FitAndScoreModel(df,name, model,X_tr,y_tr,X_tst,y_tst):\n    model.fit(X_tr,y_tr)\n    Y_pred = model.predict(X_tst)\n    score=mean_squared_error(y_tst, Y_pred, squared=False)\n    df = df.append({'Model':name, 'MSE': score},ignore_index = True) \n   # plot_results(name, y_test, Y_pred)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dResults = pd.DataFrame(columns = ['Model', 'MSE'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = [\n    DummyRegressor(strategy='median'),\n   # SVR(),\n    SGDRegressor(),\n    BayesianRidge(),\n    LassoLars(),\n    ARDRegression(),\n    LinearRegression(),\n    LGBMRegressor(),\n    RandomForestRegressor(n_estimators=50, n_jobs=-1)]\n\n \n#for item in classifiers:\n#    print(item)\n#    clf = item\n#    dResults=FitAndScoreModel(dResults,item,item,X_train,y_train,X_test,y_test) \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dResults.sort_values(by='MSE', ascending=True,inplace=True)\n#dResults.set_index('MSE',inplace=True)\n#dResults.head(dResults.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Optuna is the best out of those tested. Let's tune it."},{"metadata":{},"cell_type":"markdown","source":"## LGBM Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna\nimport sklearn\n\noptuna.logging.set_verbosity(optuna.logging.ERROR)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef objective(trial):    \n    list_bins = [25, 50, 75, 100, 125, 150, 175, 200, 225, 250,500,750,1000]   \n\n    param = {\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.006,0.008,0.01,0.014,0.017,0.02,0.05]),\n        'max_depth': trial.suggest_categorical('max_depth', [10,20,50,100]),\n        'num_leaves' : trial.suggest_int('num_leaves', 2, 1000),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 15),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 400),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 1, 256),\n        'cat_l2' : trial.suggest_int('cat_smooth', 1, 256),\n        'max_bin': trial.suggest_categorical('max_bin', list_bins)\n    }\n    \n\n    model = LGBMRegressor(**param,objective='regression',metric= 'rmse',boosting_type='gbdt',verbose=-1,random_state=42,n_estimators=20000,cat_feature= [x for x in range(len(cat_features))])\n    \n    \n    model.fit(X_train, y_train,eval_set=[(X_test,y_test)], early_stopping_rounds=150,verbose=False)\n    \n    preds = model.predict(X_test)\n    \n    rmse = mean_squared_error(y_test, preds,squared=False)\n    \n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=400)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = study.best_params\nparams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize parameter importance.\noptuna.visualization.plot_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot_optimization_history: shows the scores from all trials as well as the best score so far at each point.\noptuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nn_fold = 20\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\ntrain_columns = train.columns.values\n\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train, target.values)):\n    \n    strLog = \"fold {}\".format(fold_)\n    print(strLog)\n    \n    X_tr, X_val = train.iloc[trn_idx], train.iloc[val_idx]\n    y_tr, y_val = target.iloc[trn_idx], target.iloc[val_idx]\n\n    model = LGBMRegressor(**params, objective='regression',metric= 'rmse',boosting_type='gbdt',random_state=42,verbose=-1,n_estimators=20000,cat_feature= [x for x in range(len(cat_features))])\n   \n    model.fit(X_tr, y_tr, \n              eval_set=[(X_tr, y_tr), (X_val, y_val)], eval_metric='rmse',\n              verbose=-1, early_stopping_rounds=400)\n    \n    \n    oof[val_idx] = model.predict(X_val, num_iteration=model.best_iteration_)\n\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = train_columns\n    fold_importance_df[\"importance\"] = model.feature_importances_[:len(train_columns)]\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += model.predict(test, num_iteration=model.best_iteration_) / folds.n_splits\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:3014].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure()\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission of Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"LGBMsubmission=submission.copy()\nLGBMsubmission['target'] = predictions\nLGBMsubmission.to_csv('submission_LGBM.csv', header=True, index=False)\nLGBMsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}