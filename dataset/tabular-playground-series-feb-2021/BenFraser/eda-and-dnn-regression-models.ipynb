{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA) and Production of Various Regression Models"},{"metadata":{},"cell_type":"markdown","source":"This notebook explores the categorical and numerical features of the dataset, followed by production of a range of classical and deep neural networks to perform regression. Rather than focussing on GBMs, I've tried to put more emphasis on deep learning methods within this notebook. Hope you enjoy!"},{"metadata":{},"cell_type":"markdown","source":"\n**Table of Contents:**\n\n1. [Load Data and Analyse Overall Dataset Features](#load)\n2. [EDA](#EDA)\n3. [Data Preparation and Preprocessing](#data-preprocessing)\n4. [Creation of a Baseline Model](#baseline) \n5. [Classical Models Exploration](#classical-models)\n    - 5.1. [Basic Analysis using Random Forest](#random-forest-analysis)\n    - 5.2 [Basic Linear Regression and Visualisation of Residuals](#linear-regression)\n    - 5.3 [Exploring PCA for linear regression](#pca)\n    - 5.4 [Exploring Ridge and LASSO](#ridge-lasso)\n    - 5.5 [Bootstrapped Linear Regression](#bootstrap-linear-regression)\n    - 5.6 [More complex models - CatBoost Regressor](#catboost-model)\n6. [Deep ANN Models](#ann-models)\n    - 6.1. [Model 1 - Deep ANN Regressor with Monte Carlo Dropout and BatchNorm](#ann-model-1)\n    - 6.2 [Model 2 - Deep Self-normalising SELU Network with Monte Carlo AlphaDropout](#ann-model-2)\n7. [Test Set Predictions](#test-predictions)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport keras\nimport keras.backend as K\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom catboost import CatBoostRegressor, cv, Pool\nfrom collections import defaultdict\n\nfrom keras.layers import Dense, Embedding, Flatten, LSTM, GRU, \\\n        SpatialDropout1D, Bidirectional, Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.models import Sequential, load_model\nfrom keras import models\nfrom keras import layers\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.pipeline import Pipeline\n\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"load\"></a>\n## 1. Load Data and Analyse Overall Dataset Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"/kaggle/input/tabular-playground-series-feb-2021/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\ntest_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We seem to have a balanced mix of categorical and numerical features. From first inspection, it appears these features have already been standardised or normalised in some way."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for any null or missing values\ntrain_df.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fortunately, we have absolutely no null or missing values, which is a pleasant surprise."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"EDA\"></a>\n## 2. Basic EDA"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Analysis of Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['target'].hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# boxplot comparison\nfig = plt.figure(figsize=(4,7))\nfig.suptitle('Distribution of target variable', fontsize=16)\nax = fig.add_subplot(111)\nsns.boxplot(data=train_df[\"target\"])\nax.set_xticklabels(['Target'])\nax.set_ylabel(\"Value\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like we might have some outliers within the data, such as some zero values and those much less than the vast majority of data points."},{"metadata":{},"cell_type":"markdown","source":"Lets plot a pairplot, showing the relationship between all of our numerical columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cols = train_df.loc[:, train_df.dtypes != object].drop(columns=['id']).columns.values[:7]\nplot_cols = np.append(plot_cols, 'target')\n\nsns.pairplot(train_df.loc[:, plot_cols], plot_kws={'alpha':0.1})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its hard to make sense of any real relationships here, due to the huge number of data instances.\n\nInstead, we can find the correlation between each variable and illustrate this instead:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use all of our continuous variables this time\nplot_cols = train_df.loc[:, train_df.dtypes != object].drop(columns=['id']).columns.values\n\ncorr = train_df.loc[:, plot_cols].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use this correlation matrix to plot a heatmap of correlations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.heatmap(corr, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a few strong correlations between our variables, for instance, 'cont5' and 'cont8', 'cont9', 'cont10', 'cont11', and 'cont12'. Its possible that these share a lot of the same correlations, and therefore it could be worth trying to eliminate some of these, in order to make our model simpler and to avoid redundancy. \n\nAll things considered, they do not resemble anywhere near perfect correlation (1.0), and so keeping them in should not do too much harm in this case.\n\nIn looking at the target variable, it appears that no columns on their own are particularly correlated. This suggests a more complex relationship between our input data and output variable (assuming of course that such a relationship does in fact exist)."},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Analysis of Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_countplot(data_df, col_name, ax=None):\n    \"\"\" Plot seaborn countplot for selected dataframe col \"\"\"\n    c_plot = sns.countplot(x=col_name, data=data_df, ax=ax)\n    for g in c_plot.patches:\n        c_plot.annotate(f\"{g.get_height()}\",\n                        (g.get_x()+g.get_width()/3,\n                         g.get_height()+60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_countplot(data_df=train_df, col_name='cat0')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets do similar plots for all of our categorical columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = train_df.loc[:, train_df.dtypes == object].columns\nn = len(cat_cols)\n\nfig, axs = plt.subplots(2, 5, figsize=(20,10))\naxs = axs.flatten()\n\n# iterate through each col and plot\nfor i, col_name in enumerate(cat_cols):\n    custom_countplot(train_df, col_name, ax=axs[i])\n    axs[i].set_xlabel(f\"{col_name}\", weight = 'bold')\n    axs[i].set_ylabel('Count', weight='bold')\n    \n    if (i != 0 and i != 5):\n        axs[i].set_ylabel('')\n        \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of our categorical columns have variables that occur a very low number of times, and therefore we have some imbalanced data in this case.\n\nWe can overcome this through many techniques, one of which is to combine our minority categories into composite ones. We'll look at some simple examples for this next.\n\nFirst of all, we can automatically select the minority categories to combine within each of our features like so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_minority_cats(cat_cols, data_df, composite_category='z', threshold=0.01):\n    \"\"\" Find minority categories for each feature column, and create a \n        dictionary that maps those to selected composite category \"\"\"\n    minority_col_dict = {}\n    minority_mapping_dict = {}\n    \n    # find all feature categories with less than 5% proportion\n    for feature in cat_cols:\n        minority_col_dict[feature] = []\n        minority_mapping_dict[feature] = {}\n        \n        for category, proportion in data_df[feature].value_counts(normalize=True).iteritems():\n            if proportion < threshold:\n                minority_col_dict[feature].append(category)\n                \n                # map those minority cats to chosen composite feature\n                minority_mapping_dict[feature] = { x : composite_category for x \n                                                  in minority_col_dict[feature]}\n                \n    return minority_mapping_dict, minority_col_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_min_mappings, minority_cols = find_minority_cats(cat_cols, train_df)\n\nminority_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our function has returned a mapping dict, so that we can automatically update our dataset with the desired composite value:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_min_mappings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With all of these minority categories, we have simply combined them all into composite categories for each feature. This should give us a greater balance throughout the dataset, at the expense of loosing some information about many data instances. \n\nAlthough we're loosing information about our data, its likely this will actually benefit our models in the long-run, since it makes them simpler and less prone to the issues of unbalanced data. The issues of having unbalanced data would likely be more impactful anyway.\n\nUsing our dictionary created above, we can simply transform each of our features like so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_tx = train_df.copy()\n\nfor feat in cat_cols:\n    train_df_tx[feat] = train_df[feat].replace(cat_min_mappings[feat])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets have a look and make sure these are now updated:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 5, figsize=(20,10))\naxs = axs.flatten()\n\n# iterate through each col and plot\nfor i, col_name in enumerate(cat_cols):\n    custom_countplot(train_df_tx, col_name, ax=axs[i])\n    axs[i].set_xlabel(f\"{col_name}\", weight = 'bold')\n    axs[i].set_ylabel('Count', weight='bold')\n    \n    # only plot ylabels on lhs\n    if (i != 0 and i != 5):\n        axs[i].set_ylabel('')\n        \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df_tx\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We still have many minority categories, but its certainly much better than it was originally!"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data-preprocessing\"></a>\n## 3. Data Preprocessing: Creation of a data loader and preprocessor"},{"metadata":{},"cell_type":"markdown","source":"Its important that we handle our numerical and categorical features appropriately prior to producing our models.\n\nWe'll put together some preprocessing functions to encode our categorical features and standardise our numerical features. Whilst doing this, we'll also add support for combining some of the minority categories within our data features (since some are very imbalanced), and add support for producing additional dimensionality-reduced features (using PCA) to our dataset.\n\nThese extra features will allow us to experiment and tune to find the best combinations of feature engineering to perform for this problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataProcessor(object):\n    def __init__(self):\n        self.encoder = None\n        self.standard_scaler = None\n        self.num_cols = None\n        self.cat_cols = None\n        \n    def preprocess(self, data_df, train=True, one_hot_encode=False,\n                   combine_min_cats=False, add_pca_feats=False):\n        \"\"\" Preprocess train / test as required \"\"\"\n        \n        # if training, fit our transformers\n        if train:\n            self.train_ids = data_df.loc[:, 'id']\n            train_cats = data_df.loc[:, data_df.dtypes == object]\n            self.cat_cols = train_cats.columns\n            \n            # if selected, combine minority categorical feats\n            if combine_min_cats:\n                self._find_minority_cats(train_cats)\n                train_cats = self._combine_minority_feats(train_cats)\n            \n            # if selected, one hot encode our cat features\n            if one_hot_encode:\n                self.encoder = OneHotEncoder(handle_unknown='ignore')\n                oh_enc = self.encoder.fit_transform(train_cats).toarray()\n                train_cats_enc = pd.DataFrame(oh_enc, columns=self.encoder.get_feature_names())\n                self.final_cat_cols = list(train_cats_enc.columns)\n            \n            # otherwise just encode our cat feats with ints\n            else:\n                # encode all of our categorical variables\n                self.encoder = defaultdict(LabelEncoder)\n                train_cats_enc = train_cats.apply(lambda x: \n                                                  self.encoder[x.name].fit_transform(x))\n                self.final_cat_cols = list(self.cat_cols)\n            \n            \n            # standardise all numerical columns\n            train_num = data_df.loc[:, data_df.dtypes != object].drop(columns=['target', 'id'])\n            self.num_cols = train_num.columns\n            self.standard_scaler = StandardScaler()\n            train_num_std = self.standard_scaler.fit_transform(train_num)\n            \n            # add pca reduced num feats if selected, else just combine num + cat feats\n            if add_pca_feats:\n                pca_feats = self._return_num_pca(train_num_std)\n                self.final_num_feats = list(self.num_cols)+list(self.pca_cols)\n                \n                \n                X = pd.DataFrame(np.hstack((train_cats_enc, train_num_std, pca_feats)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols)+list(self.pca_cols))\n            else:   \n                self.final_num_feats = list(self.num_cols)\n                X = pd.DataFrame(np.hstack((train_cats_enc, train_num_std)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols))\n        \n        # otherwise, treat as test data\n        else:\n            # transform categorical and numerical data\n            self.test_ids = data_df.loc[:, 'id']\n            cat_data = data_df.loc[:, self.cat_cols]\n            if combine_min_cats:\n                cat_data = self._combine_minority_feats(cat_data)\n        \n            if one_hot_encode:\n                oh_enc = self.encoder.transform(cat_data).toarray()\n                cats_enc = pd.DataFrame(oh_enc, columns=self.encoder.get_feature_names())\n            else:\n                cats_enc = cat_data.apply(lambda x: self.encoder[x.name].transform(x))\n                \n            # transform test numerical data\n            num_data = data_df.loc[:, self.num_cols]\n            num_std = self.standard_scaler.transform(num_data)\n            \n            if add_pca_feats:\n                pca_feats = self._return_num_pca(num_std, train=False)\n                \n                X = pd.DataFrame(np.hstack((cats_enc, num_std, pca_feats)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols)+list(self.pca_cols))\n            \n            else:\n                X = pd.DataFrame(np.hstack((cats_enc, num_std)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols)) \n        return X\n    \n    \n    def _find_minority_cats(self, data_df, composite_category='z', threshold=0.05):\n        \"\"\" Find minority categories for each feature column, and create a \n            dictionary that maps those to selected composite category \"\"\"\n        self.min_col_dict = {}\n        self.min_cat_mappings = {}\n    \n        # find all feature categories with less than 5% proportion\n        for feature in self.cat_cols:\n            self.min_col_dict[feature] = []\n            self.min_cat_mappings[feature] = {}\n        \n            for category, proportion in data_df[feature].value_counts(normalize=True).iteritems():\n                if proportion < threshold:\n                    self.min_col_dict[feature].append(category)\n                \n                    # map those minority cats to chosen composite feature\n                    self.min_cat_mappings[feature] = {x : composite_category for x \n                                                    in self.min_col_dict[feature]}\n    \n    \n    def _combine_minority_feats(self, data_df, replace=False):\n        \"\"\" Combine minority categories into composite for each cat feature \"\"\"\n        new_df = data_df.copy()\n        for feat in self.cat_cols:\n            col_label = f\"{feat}\" if replace else f\"{feat}_new\"\n            new_df[feat] = new_df[feat].replace(self.min_cat_mappings[feat])\n        return new_df\n    \n    \n    def _return_num_pca(self, num_df, n_components=0.85, train=True):\n        \"\"\" return dim reduced numerical features using PCA \"\"\"\n        if train:\n            self.pca = PCA(n_components=n_components)\n            num_rd = self.pca.fit_transform(num_df)\n            \n            # create new col names for our reduced features\n            self.pca_cols = [f\"pca_{x}\" for x in range(num_rd.shape[1])]\n            \n        else:\n            num_rd = self.pca.transform(num_df)\n        \n        return pd.DataFrame(num_rd, columns=self.pca_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets transform our initial data into our total training and test sets. For this we'll one hot encode our categorical variables, and standardise our numerical data. We'll also add some additional numerical features using PCA."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_proc = DataProcessor()\nX = data_proc.preprocess(train_df, add_pca_feats=True, one_hot_encode=True)\ny = train_df.loc[:, 'target']\nX_test = data_proc.preprocess(test_df, train=False, add_pca_feats=True, one_hot_encode=True)\n\nprint(f\"X: {X.shape} \\ny: {y.shape} \\nX_test: {X_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets convert our data into a more efficient set of dtypes, to avoid any memory issues:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert all of our categorical columns to ints before using GBMs\ncat_feat_dtype_dict = { x : \"int\" for x in data_proc.final_cat_cols}\nnum_feat_dtype_dict = { x : \"float32\" for x in data_proc.final_num_feats }\ndf_map_dict = {**cat_feat_dtype_dict, **num_feat_dtype_dict}\nX = X.astype(df_map_dict)\nX_test = X_test.astype(df_map_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that from above, we only fit our label encoder and standard scaler transformers to our training set, and then use this to transform (and not fit) to our test data. \n\nWe next need to break this down into training and validation splits. Our training split allows us to train each of our models through the optimisation of our objective function, which is specific to the model used. Our validation split allows us to analyse the estimate performance of our trained models and lets us make refinements to improve and maximise their performance.\n\nThroughout this entire process, we should not touch our test set until the very end, at which point we make predictions using our final model and submitting these to the competition. \n\nThis is how it should work in practice, however what we often find is people simply make continued attempts at making predictions on the competition test set and maximise this over time. The problem is this often results in a model that has become over-fitted specifically to the test set, and thus might not generalise well beyond this. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=13)\nprint(f\"X_train: {X_train.shape} \\ny_train: {y_train.shape} \\nX_val: {X_val.shape}, \\ny_val: {y_val.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These splits of data are still very large, and so in practice we might reduce these significantly into much smaller sub-sets. These can then be used to quickly train and evaluate a range of models, which can be iteratively improved and then tested on the full splits we produced above."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"baseline\"></a>\n## 4. Creation of a baseline"},{"metadata":{},"cell_type":"markdown","source":"In almost any data science project, we should make use of a simple baseline model from which we can gauge the utility and performance of any intelligent models we produce.\n\nThis is important, since if we jump straight in with a complex model we have no idea if its actually performing well. If it performs worse than a simple mathematical technique (such as the mean or median target value), then its not useful.\n\nWe'll make a baseline that simply predicts the average of the dependent output, so that can gauge how well this performs."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_baseline = pd.DataFrame(y_train.values, columns=['y_train'])\ny_baseline['y_train_avg'] = y_train.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_baseline['y_train'], y_baseline['y_train_avg'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is our score on the training data, and we could do this similarly for the validation data in order to get a gauge of how well the baseline performs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_baseline = pd.DataFrame(y_val.values, columns=['y_val'])\ny_baseline['y_train_avg'] = y_train.mean()\n\nmean_squared_error(y_baseline['y_val'], y_baseline['y_train_avg'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems this MSE is roughly consistent with our validation set MSE. Let's now make a set of baseline predictions on the test set, using exactly the same technique. We'll then submit this as a **basic baseline submission**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use average value for all predictions on test set\n#submission_df = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\n#submission_df['target'] = y.mean()\n#submission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del y_baseline\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This extremely simple baseline scores 0.88498 on the test set, which isn't bad when considering the lack of effort put in."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"classical-models\"></a>\n## 5. Classical Model Exploration"},{"metadata":{},"cell_type":"markdown","source":"Lets take 10% of the total data as a sample for experimenting with models and training"},{"metadata":{"trusted":true},"cell_type":"code","source":"_, X_sub, _, y_sub = train_test_split(X, y, test_size=0.05)\nX_sub.shape, y_sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(X_sub, y_sub, test_size=0.2)\nX_train_sub.shape, X_val_sub.shape, y_train_sub.shape, y_val_sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clear unnecessary variables\ndel X_sub\ndel y_sub\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"random-forest-analysis\"></a>\n### 5.1 Basic Analysis using a Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_reg = RandomForestRegressor(n_jobs=-1)\nrf_reg.fit(X_train_sub, y_train_sub)\nrf_preds = rf_reg.predict(X_val_sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mse(preds, true):\n    \"\"\" Return mean squared error score \"\"\"\n    return np.square(preds - true).mean()\n\nprint(f\"Validation MSE: {mse(rf_preds, y_val_sub)}\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_preds_cv = cross_val_predict(rf_reg, X_train_sub, y_train_sub, cv=3)\nprint(f\"Cross-validation MSE: {mse(rf_preds_cv, y_train_sub)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_score(rf_preds, y_val_sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del rf_preds\ndel rf_preds_cv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case we are obtaining a negative $ R^{2} $ score, which is bad. This means we are doing worse than simply prediction the average, which is represented by an $ R^{2} $ of zero."},{"metadata":{},"cell_type":"markdown","source":"#### Feature importances using our random forest"},{"metadata":{},"cell_type":"markdown","source":"Lets also look at our feature importances, as obtained from the Random Forest Regressor:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(rf_reg.feature_importances_, index=X.columns).plot.bar(figsize=(12,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on our smaller sample and random forest model, it doesnt look like many of our categorical features are important relative to the majority of numerical features in our data.\n\nWe could try iteratively removing the lowest of these, and seeing how it impacts our performance. For now, we'll simply leave them all in."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"linear-regression\"></a>\n### 5.2 Basic Linear Regression and Visualisation of Residuals"},{"metadata":{},"cell_type":"markdown","source":"A basic linear regressor is convenient due to its extreme simplicity and high-speed. Although its not going to perform amazingly in general, it can still be useful for analysing our problem from a starting perspective."},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_train, y_train)\ny_train_preds = lr.predict(X_train)\ny_val_preds = lr.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse(y_val_preds, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fortunately, our basic linear regression model is better than our simple baseline, which scored 0.7871 on the validation split.\n\nWe can also visualise these predictions and how far they deviate through the use of a residual plot:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def residual_plot(train_labels, train_preds, test_labels=None, test_preds=None, \n                  title=\"Residual Plot\", figsize=(9,6), xlim=[6.5, 9.5]):\n    \"\"\" Residual plot to evaluate performance of our simple linear regressor \"\"\"\n    plt.figure(figsize=figsize)\n    plt.scatter(train_preds, train_preds - train_labels, c='blue', alpha=0.1,\n                marker='o', edgecolors='white', label='Training')\n    \n    if test_labels is not None:\n        plt.scatter(test_preds, test_preds - test_labels, c='red', alpha=0.1,\n                    marker='^', edgecolors='white', label='Test')\n    plt.xlabel('Predicted values')\n    plt.ylabel('Residuals')\n    plt.hlines(y=0, xmin=xlim[0], xmax=xlim[1], color='black', lw=2)\n    plt.xlim(xlim)\n    if test_labels is not None:\n        plt.legend(loc='best')\n    plt.title(title)\n    plt.show()\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"residual_plot(y_train, y_train_preds, y_val, y_val_preds, title=\"Linear Regressor Residual Plot\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want our residual plot points to be randomly dispersed close to the horizontal line. Due to the vast number of predictions, this is quite difficult to interpret effectively from just one residual plot. We'll plot training and test residuals seperately to make this more clear below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"residual_plot(y_train, y_train_preds, title=\"Lin Reg Residual Plot - Training Set\", figsize=(6,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"residual_plot(y_val, y_val_preds, title=\"Lin Reg Residual Plot - Validation Set\", figsize=(6,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our residuals look very similar in this case. These plots will be useful to compare to more complex models later on.\n\nLets apply dimensionality reduction and see whether it improves or impedes our basic linear model."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"pca\"></a>\n### 5.3 Exploring PCA and linear regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_99 = PCA(n_components=0.99)\nX_train_rd = pca_99.fit_transform(X_train)\nX_val_rd = pca_99.transform(X_val)\nX_train_rd.shape, X_val_rd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train_rd, y_train)\ny_train_preds = lr.predict(X_train_rd)\ny_val_preds = lr.predict(X_val_rd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse(y_val_preds, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_range = np.arange(1, 25)\nmse_scores = []\n\nlin_reg = LinearRegression()\n\nfor n in n_range:\n    pca = PCA(n_components=n)\n    lr_model = Pipeline(steps=[('pca', pca), ('linear regression', lin_reg)])\n    \n    # evaluate using cross-validation\n    lr_cv_preds = cross_val_predict(lr_model, X, y, cv=5)\n\n    # in order to effective work out log loss, we need to flatten both arrays before computing log loss\n    cv_mse = mean_squared_error(y, lr_cv_preds)\n    \n    mse_scores.append(cv_mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.lineplot(x=n_range, y=mse_scores)\nplt.ylabel(\"Average Cross-Val MSE\", weight='bold')\nplt.xlabel(\"PCA n components\", weight='bold')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly our model performs best when all of the features are included. Removing even a few of these results in severe impacts to the MSE score, as shown above. Thus, we'll not waste our time exploring PCA any further for this problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete existing vars to save memory later\ndel X_train_rd\ndel X_val_rd\ndel y_train_preds\ndel y_val_preds\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ridge-lasso\"></a>\n### 5.4 LASSO and Ridge Regression Exploration"},{"metadata":{},"cell_type":"markdown","source":"#### Exploring Ridge Hyper-parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha_values = [0, 0.01, 0.1, 0.5, 1.0, 1.5, 3, 10, 20, 50, 100, \n                200, 250, 500, 750, 1000, 2000, 3000, 4000, 5000]\n\nridge_mse_scores = []\nmean_ridge_mse_scores = []\n\nfor alpha in tqdm(alpha_values):\n    ridge_reg = Ridge(alpha=alpha, fit_intercept=True)\n    ridge_scores = cross_val_score(ridge_reg, X, y, \n                                   scoring='neg_mean_squared_error', cv=5)\n    mse_scores = -ridge_scores\n    ridge_mse_scores.append(mse_scores)\n    mean_ridge_mse_scores.append(mse_scores.mean())\n\nridge_mse_scores = np.array(ridge_mse_scores)\nmean_ridge_mse_scores = np.array(mean_ridge_mse_scores)\n\nprint(f\"Ridge MSE: {mean_ridge_mse_scores.mean():.5f} +/- {mean_ridge_mse_scores.std():.5f}\")\n\n# calculate standard deviation to annotate on our plot\nridge_mse_scores_std = ridge_mse_scores.std(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot mean and standard deviation of cross-val scores\nplt.figure(figsize=(18,6))\nsns.lineplot(x=alpha_values, y=mean_ridge_mse_scores)\nplt.fill_between(alpha_values, mean_ridge_mse_scores - ridge_mse_scores_std, \n                 mean_ridge_mse_scores + ridge_mse_scores_std, \n                 color='tab:blue', alpha=0.2)\nplt.semilogx(alpha_values, mean_ridge_mse_scores + ridge_mse_scores_std, 'b--')\nplt.semilogx(alpha_values, mean_ridge_mse_scores - ridge_mse_scores_std, 'b--')\nplt.ylabel(\"Cross-Validation MSE (Average)\", weight='bold', size=14)\nplt.xlabel(\"Ridge Alpha Hyper-Parameter\", weight='bold', size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exploring LASSO Hyper-parameters"},{"metadata":{},"cell_type":"markdown","source":"LASSO performs automatic feature reduction by setting the coefficient of low-importance features to zero throughout training. This is performed in relation to the hyper-parameter values we select at model creation. We'll explore a range of these similarly to how we did with Ridge regression above:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# range of alpha values between 0 and 1\nalpha_values = np.logspace(-4, 1, 20)\n\nlasso_mse_scores = []\nmean_lasso_mse_scores = []\n\nfor alpha in tqdm(alpha_values):\n    lasso_reg = Lasso(alpha=alpha, fit_intercept=True, max_iter=1000, tol=0.5)\n    lasso_scores = cross_val_score(lasso_reg, X, y,\n                                   scoring='neg_mean_squared_error', cv=5)\n    mse_scores = -lasso_scores\n    lasso_mse_scores.append(mse_scores)\n    mean_lasso_mse_scores.append(mse_scores.mean())\n\nlasso_mse_scores = np.array(lasso_mse_scores)\nmean_lasso_mse_scores = np.array(mean_lasso_mse_scores)\n\nprint(f\"LASSO MSE: {mean_lasso_mse_scores.mean():.5f} +/- {mean_lasso_mse_scores.std():.5f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate standard deviation to annotate on our plot\nlasso_mse_scores_std = lasso_mse_scores.std(axis=1)\n\n# plot mean and standard deviation of cross-val scores\nplt.figure(figsize=(16,6))\nsns.lineplot(x=alpha_values, y=mean_lasso_mse_scores)\nplt.fill_between(alpha_values, mean_lasso_mse_scores - lasso_mse_scores_std, \n                 mean_lasso_mse_scores + lasso_mse_scores_std, \n                 color='tab:blue', alpha=0.2)\nplt.semilogx(alpha_values, mean_lasso_mse_scores + lasso_mse_scores_std, 'b--')\nplt.semilogx(alpha_values, mean_lasso_mse_scores - lasso_mse_scores_std, 'b--')\nplt.ylabel(\"Cross-Validation MSE (Average)\", weight='bold', size=14)\nplt.xlabel(\"LASSO Alpha Hyper-Parameter\", weight='bold', size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears for this problem neither LASSO nor Ridge perform that well in comparison to a basic linear regressor. They all perform uniformly relative to one-another. \n\nTo perform well on this dataset, it appears we need more complex models that can better characterise the relations between our input data and dependent target variable."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"bootstrap-linear-regression\"></a>\n### 5.5 Bootstrapping a basic linear regression model to obtain a simple ensemble:"},{"metadata":{},"cell_type":"markdown","source":"Before moving on to more complex models, we'll just investigate how well a simple linear regression model can perform when using bootstrapping techniques:"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_bag_reg = BaggingRegressor(LinearRegression(), n_estimators=5, bootstrap=True, \n                              max_samples=0.75, max_features=0.75)\n\nlr_bag_mse_scores = []\nmean_lr_bag_mse_scores = []\n\n# times to repeat the cross validation\nrepeats = 2\n\nfor i in tqdm(range(repeats)):\n    lr_bag_scores = cross_val_score(lr_bag_reg, X, y,\n                                    scoring='neg_mean_squared_error', cv=5)\n    mse_scores = -lr_bag_scores\n    lr_bag_mse_scores.append(mse_scores)\n    mean_lr_bag_mse_scores.append(mse_scores.mean())\n\nlr_bag_mse_scores = np.array(lr_bag_mse_scores)\nmean_lr_bag_mse_scores = np.array(mean_lr_bag_mse_scores)\n\nprint(f\"Lin Reg Bagging MSE: {mean_lr_bag_mse_scores.mean():.5f} +/- {mean_lr_bag_mse_scores.std():.5f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"catboost-model\"></a>\n### 5.6 More complex models - CatBoost Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# if training on GPU:\n#cb_reg = CatBoostRegressor(task_type='GPU', random_seed=13, verbose=400)\n\n#cb_reg = CatBoostRegressor(random_seed=13, verbose=400)\n#cb_reg.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True, plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_preds = cb_reg.predict(X_train)\n#val_preds = cb_reg.predict(X_val)\n\n# calculate mean squared error on val sub-set preds\n#cat_rmse = np.sqrt(mean_squared_error(val_preds, y_val))\n\n#print(\"CatBoost Regressor RMSE: {cat_rmse}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets have a quick look at our feature importances for this model for insight:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#feat_importances = cb_reg.get_feature_importance(prettified=True)\n\n#plt.figure(figsize=(12, 12))\n#sns.barplot(x=\"Importances\", y=\"Feature Id\", data=feat_importances)\n#plt.title('CatBoost features importance:')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly, lets see how well our residuals look compared to the previous simple linear regression models:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#residual_plot(y_train[:10000], train_preds[:10000], \n#              y_val[:10000], val_preds[:10000], \n#              title=\"CatBoost Residual Plot\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete existing vars to save memory later\n#del train_preds\n#del val_preds\n#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets make some predictions on the test set, and save them for later should we need them:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#cb_test_preds = cb_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ann-models\"></a>\n## 6. Production of various Deep ANN Models"},{"metadata":{},"cell_type":"markdown","source":"We'll try some different approaches in this section through the production of various Deep Neural Networks. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ann-model-1\"></a>\n### 6.1 Model 1 - Deep ANN Regressor with Monte Carlo Dropout and Batch Norm"},{"metadata":{},"cell_type":"markdown","source":"We'll produce a model that uses monte carlo dropout to provide a form of ensembling for our final model predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"class MonteCarloDropout(layers.Dropout):\n    \"\"\" Class that overrides default call function used by standard \n        dropout to keep dropout active during inference \"\"\"\n    def call(self, inputs):\n        return super().call(inputs, training=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred_mc_dropout(model, test_inputs, n_samples=50):\n    \"\"\" Make a large number of predictions (equal to n_samples) using the \n        passed model and input features \"\"\"\n    pred_probs = [model.predict(test_inputs) for samples in range(n_samples)]\n    return np.mean(pred_probs, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ann_model(monte_carlo_dropout=False, dropout_val=0.45, lr=2e-3, \n              input_feat_dim=X_train.shape[1]):\n    \"\"\" Create a Deep NN for regression that supports monte carlo dropout\n        and uses Batch Norm \"\"\"\n    model = models.Sequential()\n    \n    model.add(layers.Dense(200, activation='elu', input_shape=(input_feat_dim,), \n                           kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    if monte_carlo_dropout:\n        model.add(MonteCarloDropout(dropout_val))\n    else:\n        model.add(layers.Dropout(dropout_val))\n    model.add(layers.Dense(100, activation='elu', kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    if monte_carlo_dropout:\n        model.add(MonteCarloDropout(dropout_val))\n    else:\n        model.add(layers.Dropout(dropout_val))\n    model.add(layers.Dense(50, activation='elu', kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    if monte_carlo_dropout:\n        model.add(MonteCarloDropout(dropout_val))\n    else:\n        model.add(layers.Dropout(dropout_val))\n        \n    # regression output layer - no activation\n    model.add(layers.Dense(1))\n        \n    model.compile(optimizer=keras.optimizers.Nadam(lr=lr), \n                  loss='mse', metrics=['mse'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1 = ann_model(monte_carlo_dropout=True, lr=1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def schedule_lr_rate(epoch, lr):\n    \"\"\" Use initial learning rate for 20 epochs and then\n        decrease it exponentially \"\"\"\n    if epoch < 20:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create our lr scheduler - use reduceLRonPlat below - better performance\n#lr_scheduler = tf.keras.callbacks.LearningRateScheduler(schedule_lr_rate)\n\n# create learning rate scheduler\n#lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, \n#                                                   patience=5, verbose=0, \n#                                                   min_delta=0.0001, mode='min')\n\n# create an early stopper callback\nearly_stopper = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n\n# list of callbacks to use\ntrg_callbacks = [early_stopper]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_1.fit(X_train, y_train, epochs=50, \n                      batch_size=256, validation_data=(X_val, y_val), \n                      callbacks=trg_callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history_results(history, metric='mse', figsize=(12,5)):\n    \"\"\" Helper function for plotting history from keras model \"\"\"\n    \n    # gather desired features\n    trg_loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(trg_loss) + 1)\n\n    # plot losses and accuracies for training and validation \n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_subplot(1, 1, 1)\n    plt.plot(epochs, trg_loss, marker='o', label='Training Loss')\n    plt.plot(epochs, val_loss, marker='x', label='Validation Loss')\n    plt.title(\"Training / Validation Loss\")\n    ax.set_ylabel(\"Loss\")\n    ax.set_xlabel(\"Epochs\")\n    plt.legend(loc='best')\n    plt.tight_layout()\n    plt.ylim(0.0, 2.0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history_results(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#val_preds = model_1.predict(X_val)\n#mse = mean_squared_error(y_val, val_preds)\n#print(f'Validation MSE: {mse:.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that making one set of predictions with Monte Carlo Dropout enabled is not sufficient - we must make a large number of these due to the diversified nature of the predictions with dropout layers enabled during inference:"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = pred_mc_dropout(model_1, X_val, n_samples=25)\nmse = mean_squared_error(y_val, val_preds)\nprint(f'Validation MSE with Monte Carlo Preds: {mse:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete existing vars to save memory later\ndel val_preds\ngc.collect()\n\n# clear keras session\nK.clear_session()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The downside of Monte Carlo is the long time for inference, however the benefits in performance can be good as seen above. Therefore, it is a compromise between the two."},{"metadata":{},"cell_type":"markdown","source":"Let's use this model to make a set of predictions on the test set to use as a submission. First, we'll retrain on the entire training set, rather than just the sub-set of training we reserved initially. We'll only train for a maximum of 40 epochs though, since our model began to saturate training above."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1 = ann_model(monte_carlo_dropout=True, lr=1e-3)\n\n# create an early stopper callback\nearly_stopper = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n\n# list of callbacks to use\ntrg_callbacks = [early_stopper]\n\n# fit on full set of training data\nhistory = model_1.fit(X, y, epochs=40, batch_size=256, callbacks=trg_callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_1 = pred_mc_dropout(model_1, X_test, n_samples=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets combine these predictions with the CatBoost ones we made earlier as a simple set of ensembled predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"final_preds = test_preds_1.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_preds_1\n#del cb_test_preds\ngc.collect()\nK.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save submission in csv format\n#submission_df = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\n#submission_df['target'] = final_preds\n#submission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ann-model-2\"></a>\n## Model 2 - Deep Self-Normalising SELU Network"},{"metadata":{},"cell_type":"markdown","source":"With traditional neep neural networks we start having large problems when we train a network that is very deep, which is due to the occurrence of exploding and vanishing gradients. Certain network architectures overcome these issues, with one being a self-normalising SELU network.\n\nWe'll produce a deep network (with around 20 hidden layers) and see how well it performs on the given regression challenge.\n\nAn important consideration for SELU networks is that our data is correctly standardised before feeding it into our network. We also need to ensure we modify our the way our dropout works on the network, if we want to implement dropout as a regularisation technique. For this we can apply AlphaDropout. Like before, we'll also make use of monte carlo dropout for our model, modifying for alpha dropout accordingly."},{"metadata":{"trusted":true},"cell_type":"code","source":"class MCAlphaDropout(keras.layers.AlphaDropout):\n    \"\"\" Custom Monte Carlo Alpha Dropout layer \"\"\"\n    def call(self, inputs):\n        return super().call(inputs, training=True)\n    \n    \ndef pred_mc_dropout(model, test_inputs, n_samples=50):\n    \"\"\" Make a large number of predictions (equal to n_samples) using the \n        passed model and input features \"\"\"\n    pred_probs = [model.predict(test_inputs) for samples in range(n_samples)]\n    return np.mean(pred_probs, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dnn_selu(layers=20, neurons=100, dropout_rate=0.4):\n    \"\"\" Deeply stacked SELU network with self-normalisation and Monte\n        Carlo AlphaDropout \n    \"\"\"\n    model = keras.models.Sequential()\n    \n    # stack SELU layers with alpha dropout\n    for layer in range(layers):\n        model.add(keras.layers.Dense(neurons, activation='selu',\n                               kernel_initializer='lecun_normal'))\n        \n        # add alpha dropout to our last layers\n        if layer >= (layers - 2):\n            model.add(MCAlphaDropout(rate=dropout_rate))\n    \n    # final regression layer\n    model.add(keras.layers.Dense(1))\n    \n    model.compile(loss='mse', \n                  optimizer=keras.optimizers.Nadam(lr=1e-4), \n                  metrics=['mse'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_2 = dnn_selu()\n\n# create an early stopper callback and a checkpoint callback\n#early_stopper = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n\n#checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dnn_selu_dropout.h5\", save_best_only=True)\n\n# list of callbacks to use\n#trg_callbacks = [early_stopper, checkpoint_cb]\n\n# train our model and store the results\n#history = model_2.fit(X_train, y_train, epochs=1, \n#                      batch_size=256, validation_data=(X_val, y_val), \n#                      callbacks=trg_callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot_history_results(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#val_preds = pred_mc_dropout(model_2, X_val, n_samples=1)\n#mse = mean_squared_error(y_val, val_preds)\n#print(f'Validation MSE with SELU Model & Monte Carlo Preds: {mse:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_preds_2 = pred_mc_dropout(model_2, X_test, n_samples=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission_df = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\n#submission_df['target'] = test_preds_2\n#submission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"test-predictions\"></a>\n## 7. Final Test Predictions"},{"metadata":{},"cell_type":"markdown","source":"Finally, we'll create a mix of predictions using some of our previous models on the test set, and submit these accordingly."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\nsubmission_df['target'] = final_preds\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Many thanks for following along - I hope you enjoyed this work!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}