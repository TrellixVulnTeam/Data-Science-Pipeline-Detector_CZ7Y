{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Training of a DNN Tabular model with categorical embeddings using FastAI"},{"metadata":{},"cell_type":"markdown","source":"FastAI provides a huge number of convenient functions on top of PyTorch for Deep Learning tasks. Within this notebook, I'll demonstrate just how extremely easy it is to get a Deep Learning Tabular model up and running."},{"metadata":{"trusted":true},"cell_type":"code","source":"import fastai\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\n\nfrom fastai.tabular.all import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Load our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"/kaggle/input/tabular-playground-series-feb-2021/\"\ntrain_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\ntest_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data preprocessing and creation of dataloaders"},{"metadata":{},"cell_type":"markdown","source":"Lets preprocess our data into a suitable form for training. We'll encode categorical variables, standardise numerical features, and fill missing values (if there are any) within the dataset. We can do this extremely easily using the TabulerPandas class, like so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"processing_funcs = [Categorify, FillMissing, Normalize]\ncat_cols = [x for x in train_df.columns.values if x.startswith('cat')]\nnum_cols = [x for x in train_df.columns.values if x.startswith('cont')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_df = TabularPandas(train_df, cat_names=cat_cols, cont_names=num_cols, procs=processing_funcs, y_names='target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# turn our tabular data into a dataloader, batch size 1024\ntrain_dl = nn_df.dataloaders(1024)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preview some of our data from the dataloader\ntrain_dl.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its so easy you almost feel like you've cheated somehow! \n\nI must admit, this is something that put me off using FastAI initially, however after the pain and effort of doing all of this manually many times with Keras, Tensorflow and PyTorch imeplementations, the ease of this method is highly appreciated.\n\nWe could also have performed exactly the same as above, but straight from TabularDataLoaders, like so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dls = TabularDataLoaders.from_df(train_df, path='.', y_names=\"target\",  \n                                 cat_names = cat_cols, \n                                 cont_names = num_cols, \n                                 procs=processing_funcs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Production of our DNN model"},{"metadata":{},"cell_type":"markdown","source":"Since we're performing regression, we need to provide our model with the possible bounds of the output. We need to do this since fastai uses a sigmoid activation on the final layer, rather than a dense layer with no activation. In general across many regression problems, we find that sigmoid tends to outperm just a raw dense layer, provided we precisely know the maximum and minimum outputs of our regression model.\n\nWe can do this and find the maximum / minimum output values based on our training data like so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = dls.train.y\ny.min(), y.max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we've got the basics working with our dataloader and preprocessers produced, we can get on to model training:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_learn = tabular_learner(dls, y_range=(0, 11), layers=[500, 250], n_out=1, metrics=rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can view the architecture of our model like so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_learn.model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before starting training, its helpful to find an appropriate learning rate for our model. This is as simple in FastAI as calling the lr_find() function, like so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot, a learning rate of around 1e-3 should work well in this case.\n\nLets train our model for 5 epochs, and see how well it performs."},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_learn.fit_one_cycle(5, lr_max=1e-2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_learn.recorder.plot_loss()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Test set predictions"},{"metadata":{},"cell_type":"markdown","source":"Preprocess our test set and make predictions using our trained model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dl = tab_learn.dls.test_dl(test_df)\ntest_dl.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds, test_labels = tab_learn.get_preds(dl=test_dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! Lets submit these to the competition and see how well the predictions perform:"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_preds = preds.numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\nsubmission_df['target'] = final_preds\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall, its remarkable how easy this process is, especially when compared to doing all of the low-level features yourself. I think going through the process of doing these low-level implementations is extremely important for learning, and is essential when you need to perform something a bit more specific for a data science problem.\n\nHowever, once you've been through this process, and can appreciate what is going on under the hood, FastAI becomes hugely convenient and an asset for quickly experimenting on different data-based problems. Tabular data is just one tiny aspect of this, as done simply within this notebook.\n\nTo expand nicely from this work, we could extract the learned embeddings from our model above for each categorical feature, and feed these into a gradient booasting model, such as CatBoost for even better performance on this competition.\n\nI hope you enjoyed this short piece of work anyway - thanks for reading!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}