{"cells":[{"metadata":{},"cell_type":"markdown","source":"# DNN with categorical embeddings"},{"metadata":{},"cell_type":"markdown","source":"This notebook demonstrates a method of producing categorical embeddings for a neural network model when working with tabular data.\n\nThe categorical and numerical features of the data are preprocessed and formed into seperate input layers for the model, which are then concatenated into a composite model that performs regression for the target variable.\n\nThis is performed using keras and the functional API, but could equally be performed in PyTorch using a similar process.\n\nThis process of creating categorical embeddings can be very useful in practice, and can help provide additional inputs into simpler models as a form of pre-training, such as gradient boosting machines.\n\nHopefully you can find this notebook useful!"},{"metadata":{},"cell_type":"markdown","source":"**Table of Contents:**\n\n1. [Load Data and Analyse Overall Dataset Features](#load)\n2. [Data Preparation and Preprocessing](#data-preprocessing) \n3. [Deep ANN Models](#ann-models)\n4. [Improving our DNN model using Monte Carlo Dropout](#ann-model-2)\n5. [Test Set Predictions](#test-predictions)\n6. [Examining our categorical embeddings](#categorical-embeddings)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport keras.backend as K\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom collections import defaultdict\n\nfrom keras.layers import Dense, Embedding, Flatten, LSTM, GRU, \\\n        SpatialDropout1D, Bidirectional, Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.models import Sequential, load_model\nfrom keras import models\nfrom keras import layers\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.pipeline import Pipeline\n\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"load\"></a>\n## 1. Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"/kaggle/input/tabular-playground-series-feb-2021/\"\ntrain_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\ntest_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data-preprocessing\"></a>\n## 2. Data Preprocessing: Creation of a data loader and preprocessor"},{"metadata":{},"cell_type":"markdown","source":"It's important that we handle our numerical and categorical features appropriately prior to producing our models.\n\nWe'll put together some preprocessing functions to encode our categorical features and standardise our numerical features. Whilst doing this, we'll also add support for combining some of the minority categories within our data features (since some are very imbalanced), and add support for producing additional dimensionality-reduced features (using PCA) to our dataset.\n\nThese extra features will allow us to experiment and tune to find the best combinations of feature engineering to perform for this problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataProcessor(object):\n    def __init__(self):\n        self.enc_dict = None\n        self.standard_scaler = None\n        self.num_cols = None\n        self.cat_cols = None\n        \n    def preprocess(self, data_df, train=True, \n                   combine_min_cats=False, add_pca_feats=False):\n        \"\"\" Preprocess train / test as required \"\"\"\n        \n        # if training, fit our transformers\n        if train:\n            self.train_ids = data_df.loc[:, 'id']\n            train_cats = data_df.loc[:, data_df.dtypes == object]\n            self.cat_cols = train_cats.columns\n            \n            # if selected, combine minority categorical feats\n            if combine_min_cats:\n                self._find_minority_cats(train_cats)\n                train_cats = self._combine_minority_feats(train_cats)\n            \n            # encode all of our categorical variables\n            self.enc_dict = defaultdict(LabelEncoder)\n            train_cats_enc = train_cats.apply(lambda x: self.enc_dict[x.name].fit_transform(x))\n            \n            # standardise all numerical columns\n            train_num = data_df.loc[:, data_df.dtypes != object].drop(columns=['target', 'id'])\n            self.num_cols = train_num.columns\n            self.standard_scaler = StandardScaler()\n            train_num_std = self.standard_scaler.fit_transform(train_num)\n            \n            # add pca reduced num feats if selected, else just combine num + cat feats\n            if add_pca_feats:\n                pca_feats = self._return_num_pca(train_num_std)\n                self.final_num_feats = list(self.num_cols)+list(self.pca_cols)\n                \n                X = pd.DataFrame(np.hstack((train_cats_enc, train_num_std, pca_feats)), \n                        columns=list(self.cat_cols)+self.final_num_feats)\n            else:\n                # set final list of all num cols and form final combined df\n                self.final_num_feats = list(self.num_cols)\n                X = pd.DataFrame(np.hstack((train_cats_enc, train_num_std)), \n                        columns=list(self.cat_cols)+list(self.num_cols))\n        \n        # otherwise, treat as test data\n        else:\n            # transform categorical and numerical data\n            self.test_ids = data_df.loc[:, 'id']\n            cat_data = data_df.loc[:, self.cat_cols]\n            if combine_min_cats:\n                cat_data = self._combine_minority_feats(cat_data)\n            cats_enc = cat_data.apply(lambda x: self.enc_dict[x.name].transform(x))\n            num_data = data_df.loc[:, self.num_cols]\n            num_std = self.standard_scaler.transform(num_data)\n            \n            if add_pca_feats:\n                pca_feats = self._return_num_pca(num_std, train=False)\n                \n                X = pd.DataFrame(np.hstack((cats_enc, num_std, pca_feats)), \n                        columns=list(self.cat_cols) + self.final_num_feats)\n            \n            else:\n                X = pd.DataFrame(np.hstack((cats_enc, num_std)), \n                        columns=list(self.cat_cols)+list(self.num_cols)) \n        return X\n    \n    \n    def _find_minority_cats(self, data_df, composite_category='z', threshold=0.05):\n        \"\"\" Find minority categories for each feature column, and create a \n            dictionary that maps those to selected composite category \"\"\"\n        self.min_col_dict = {}\n        self.min_cat_mappings = {}\n    \n        # find all feature categories with less than 5% proportion\n        for feature in self.cat_cols:\n            self.min_col_dict[feature] = []\n            self.min_cat_mappings[feature] = {}\n        \n            for category, proportion in data_df[feature].value_counts(normalize=True).iteritems():\n                if proportion < threshold:\n                    self.min_col_dict[feature].append(category)\n                \n                    # map those minority cats to chosen composite feature\n                    self.min_cat_mappings[feature] = {x : composite_category for x \n                                                    in self.min_col_dict[feature]}\n    \n    \n    def _combine_minority_feats(self, data_df, replace=False):\n        \"\"\" Combine minority categories into composite for each cat feature \"\"\"\n        new_df = data_df.copy()\n        for feat in self.cat_cols:\n            col_label = f\"{feat}\" if replace else f\"{feat}_new\"\n            new_df[feat] = new_df[feat].replace(self.min_cat_mappings[feat])\n        return new_df\n    \n    \n    def _return_num_pca(self, num_df, n_components=0.85, train=True):\n        \"\"\" return dim reduced numerical features using PCA \"\"\"\n        if train:\n            self.pca = PCA(n_components=n_components)\n            num_rd = self.pca.fit_transform(num_df)\n            \n            # create new col names for our reduced features\n            self.pca_cols = [f\"pca_{x}\" for x in range(num_rd.shape[1])]\n            \n        else:\n            num_rd = self.pca.transform(num_df)\n        \n        return pd.DataFrame(num_rd, columns=self.pca_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although we've added support for minority category modifications and addition of PCA dimensionality reduced features, we'll keep it simple for this example, and just use our training and test data with basic categorical encoding and numerical standardisation."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_proc = DataProcessor()\nX = data_proc.preprocess(train_df, add_pca_feats=True)\ny = train_df.loc[:, 'target']\nX_test = data_proc.preprocess(test_df, train=False, add_pca_feats=True)\n\nprint(f\"X: {X.shape} \\ny: {y.shape} \\nX_test: {X_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our preprocessing provides us with a list of the final numerical columns (including original standardised features + pca reduced features if chosen):"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_proc.final_num_feats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that from above, we only fit our label encoder and standard scaler transformers to our training set, and then use this to transform (and not fit) to our test data.\n\nWe next need to break this down into training and validation splits. Our training split allows us to train each of our models through the optimisation of our objective function, which is specific to the model used. Our validation split allows us to analyse the estimate performance of our trained models and lets us make refinements to improve and maximise their performance.\n\nThroughout this entire process, we should not touch our test set until the very end, at which point we make predictions using our final model and submit these to the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=13)\nprint(f\"X_train: {X_train.shape} \\ny_train: {y_train.shape} \\nX_val: {X_val.shape}, \\ny_val: {y_val.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These splits of data are still very large, and so in practice we might reduce these significantly into much smaller sub-sets. These can then be used to quickly train and evaluate a range of models, which can be iteratively improved and then tested on the full splits we produced above.\n\nFor this example we'll just use the large sub-sets obtained above."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ann-models\"></a>\n## 3. Production of a DNN model with categorical embeddings and numerical inputs"},{"metadata":{},"cell_type":"markdown","source":"There are lots of ways of creating embedding and numerical input layers for a neural network model. For this, we'll keep it relatively simple and create arrays to store each of our categorical and numerical feature inputs respectively.\n\nWith each of these arrays containing the categorical and numerical input layers, we can concatanate these together and produce a composite model that exploits both categorical embeddings and numerical features. We'll use the keras functional API for this."},{"metadata":{"trusted":true},"cell_type":"code","source":"# arrays to store individual input layers for each cat & num feature\ncategorical_inputs = []\nnumerical_inputs = []\n\n# arrays to store all our categorical embeddings & layer names\ncat_embeddings = []\nemb_layer_names = []\n\n# embedding dimension - this is a hyper-parameter that can be tuned\nemb_n = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create embeddings for each of our categorical features\nfor cat_col in data_proc.cat_cols:\n    _in = layers.Input(shape=[1], name=cat_col)\n    _emb = layers.Embedding(int(X_train[cat_col].max()) + 1, \n                            emb_n, name=cat_col + '_emb')(_in)\n    categorical_inputs.append(_in)\n    cat_embeddings.append(_emb)\n    emb_layer_names.append(cat_col + '_emb')\n    \n# input layers for the numeric features\nfor num_col in data_proc.final_num_feats: \n    numeric_input = layers.Input(shape=(1,), name=num_col)\n    numerical_inputs.append(numeric_input)\n    \n# merge all our numeric inputs into one layer\ncombined_num_inputs = layers.concatenate(numerical_inputs)\n\n# Merge embedding layers, apply dropout for regularisation, and flatten\nmerged_inputs = layers.concatenate(cat_embeddings)\nspatial_dropout = layers.SpatialDropout1D(0.2)(merged_inputs)\nflat_embed = layers.Flatten()(spatial_dropout)\n\n# concatenate all of our categorical and numerical features\ncomposite_feats = layers.concatenate([flat_embed, combined_num_inputs])\n\n# custom DNN for regression\nx = layers.Dropout(0.3)(layers.Dense(200, activation='elu', \n                                      kernel_initializer='he_normal')(composite_feats))\nx = layers.Dropout(0.3)(layers.Dense(100, activation='elu', \n                                      kernel_initializer='he_normal')(x))\nx = layers.Dropout(0.3)(layers.Dense(50, activation='elu', \n                                      kernel_initializer='he_normal')(x))\n\n# define model inputs and outputs\noutput = layers.Dense(1)(x)\nmodel = models.Model(inputs=categorical_inputs + numerical_inputs, outputs=output)\nmodel.compile(loss='mse', optimizer='adam', metrics=['mse'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With our model compiled, we can view it, along with all of the input layers and embeddings we created, like so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll create an early stopper callback so that if we start over-fitting our model automatically stops at an appropriate point:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an early stopper callback\nearly_stopper = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n\n# list of callbacks to use\ntrg_callbacks = [early_stopper]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we're using the keras functional api, with a range of input layers as shown in the model summary above, we need to pass our data into the keras model in a more specific way compared to usual. We can do this using a dictionary, which contains the feature names as the keys, and input features for each as the values, like so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_keras_dataset(df):\n    \"\"\" Return a dictionary of feature names and associated arrays for\n        input into our functional model \"\"\"\n    X = {str(col) : np.array(df[col]) for col in df.columns}\n    return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we're ready to train our model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x=get_keras_dataset(X_train), y=y_train, \n                    epochs=30, \n                    batch_size=512, \n                    validation_data=(get_keras_dataset(X_val), y_val), \n                    callbacks=trg_callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history_results(history, metric='mse', figsize=(12,5)):\n    \"\"\" Helper function for plotting history from keras model \"\"\"\n    \n    # gather desired features\n    trg_loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(trg_loss) + 1)\n\n    # plot losses and accuracies for training and validation \n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_subplot(1, 1, 1)\n    plt.plot(epochs, trg_loss, marker='o', label='Training Loss')\n    plt.plot(epochs, val_loss, marker='x', label='Validation Loss')\n    plt.title(\"Training / Validation Loss\")\n    ax.set_ylabel(\"Loss\")\n    ax.set_xlabel(\"Epochs\")\n    plt.legend(loc='best')\n    plt.tight_layout()\n    plt.ylim(0.0, 2.0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history_results(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = model.predict(get_keras_dataset(X_val))\nmse = mean_squared_error(y_val, val_preds)\nprint(f'Validation MSE: {mse:.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.7432 with pca reduced features."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ann-model-2\"></a>\n## 4. Improving our DNN model with Monte Carlo Dropout"},{"metadata":{},"cell_type":"markdown","source":"Lets add monte carlo dropout to this model, and use this as a means of making ensembled predictions with our model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MonteCarloDropout(layers.Dropout):\n    \"\"\" Class that overrides default call function used by standard \n        dropout to keep dropout active during inference \"\"\"\n    def call(self, inputs):\n        return super().call(inputs, training=True)\n    \ndef pred_mc_dropout(model, test_inputs, n_samples=50):\n    \"\"\" Make a large number of predictions (equal to n_samples) using the \n        passed model and input features \"\"\"\n    pred_probs = [model.predict(test_inputs) for samples in range(n_samples)]\n    return np.mean(pred_probs, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We simply produce the model in exactly the same way as before, except replacing dropout with MonteCarloDropout instead, like so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tabular_dnn(X_df, cat_cols, num_cols, embedding_dim=3, dropout=0.3):\n    \"\"\" Tabular DNN model that supports both categorical and numerical variables. \n        Categorical embeddings are used for categorical embeddings.\"\"\"\n    # arrays to store individual input layers for each cat & num feature\n    categorical_inputs = []\n    numerical_inputs = []\n\n    # arrays to store all our categorical embeddings & layer names\n    cat_embeddings = []\n    emb_layer_names = []\n\n    # embedding dimension - this is a hyper-parameter that can be tuned\n    emb_n = embedding_dim\n    \n    # create embeddings for each of our categorical features\n    for cat_col in cat_cols:\n        _in = layers.Input(shape=[1], name=cat_col)\n        _emb = layers.Embedding(int(X_df[cat_col].max()) + 1, \n                            emb_n, name=f\"{cat_col}_emb\")(_in)\n        categorical_inputs.append(_in)\n        cat_embeddings.append(_emb)\n        emb_layer_names.append(f\"{cat_col}_emb\")\n    \n    # input layers for the numeric features\n    for num_col in num_cols: \n        numeric_input = layers.Input(shape=(1,), name=num_col)\n        numerical_inputs.append(numeric_input)\n    \n    # merge all our numeric inputs into one layer\n    combined_num_inputs = layers.concatenate(numerical_inputs)\n\n    # Merge embedding layers, apply dropout for regularisation, and flatten\n    merged_inputs = layers.concatenate(cat_embeddings)\n    spatial_dropout = layers.SpatialDropout1D(0.2)(merged_inputs)\n    flat_embed = layers.Flatten()(spatial_dropout)\n\n    # concatenate all of our categorical and numerical features\n    composite_feats = layers.concatenate([flat_embed, combined_num_inputs])\n\n    # custom DNN for regression\n    x = MonteCarloDropout(dropout)(layers.Dense(200, activation='elu', \n                                      kernel_initializer='he_normal')(composite_feats))\n    x = MonteCarloDropout(dropout)(layers.Dense(100, activation='elu', \n                                      kernel_initializer='he_normal')(x))\n    x = MonteCarloDropout(dropout)(layers.Dense(50, activation='elu', \n                                      kernel_initializer='he_normal')(x))\n\n    # define model inputs and outputs\n    output = layers.Dense(1)(x)\n    model = models.Model(inputs=categorical_inputs + numerical_inputs, outputs=output)\n    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tabular_dnn(X_train, data_proc.cat_cols, data_proc.final_num_feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an early stopper callback\nearly_stopper = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n\n# list of callbacks to use\ntrg_callbacks = [early_stopper]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x=get_keras_dataset(X_train), y=y_train, \n                    epochs=40, \n                    batch_size=512, \n                    validation_data=(get_keras_dataset(X_val), y_val), \n                    callbacks=trg_callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history_results(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we make predictions with this model, we need to call out monte carlo predict function above, rather than the native model prediction. This is because we want to call model.predict a large number of times, which provides us with ensembled predictions from which we average to obtain our overall final predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = pred_mc_dropout(model, get_keras_dataset(X_val), n_samples=25)\nmse = mean_squared_error(y_val, val_preds)\nprint(f'Validation MSE: {mse:.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** This takes much longer than it normally would for inference, since our monte carlo prediction function is calling model.predict a large number of times (equal to n_samples argument above). Thus, we have a compromise that we need to make between inference time, and the number of ensembled predictions we want to make.\n\nGenerally, we can obtain reasonable performance with 25-50 samples of predictions of monte carlo dropout models, but this varies depending on the complexity of the underlying model. The more diverse and varying our neural network models are (proportional to the number of monte carlo dropout layers throughout the model) then the more samples we will need to produce for our ensembled predictions accordingly."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"test-predictions\"></a>\n## 5. Final Training and Test Set Predictions"},{"metadata":{},"cell_type":"markdown","source":"Using the model defined above, lets train on the entire training set and make a set of final predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tabular_dnn(X, data_proc.cat_cols, data_proc.final_num_feats)\n\n# create an early stopper callback\nearly_stopper = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n\n# list of callbacks to use\ntrg_callbacks = [early_stopper]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x=get_keras_dataset(X), y=y, \n                    epochs=40, \n                    batch_size=512,  \n                    callbacks=trg_callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = pred_mc_dropout(model, get_keras_dataset(X_test), n_samples=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save submission in csv format\nsubmission_df = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\nsubmission_df['target'] = test_preds\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"categorical-embeddings\"></a>\n## 6. Using our categorical embeddings from the trained model"},{"metadata":{},"cell_type":"markdown","source":"We can view the trained embeddings for each of our input features quite easily:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for embedding in cat_embeddings:\n    print(embedding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.layers[14].embeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['cat4'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown, we have embeddings of the selected dimension size for each unique category within each feature. When using a large number of categories this can be very useful, and we can do things like extract these embeddings for use in classical models and ensemble-tree based models for greater performance.\n\nFor this dataset, we likely do not have a large enough cardinality (number of unique categories for each feature) for this to be useful. In practice, one hot encoding is easier and more convenient for this challenge."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}