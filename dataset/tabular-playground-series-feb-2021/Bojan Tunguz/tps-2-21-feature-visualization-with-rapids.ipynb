{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://developer.nvidia.com/sites/default/files/pictures/2018/rapids/rapids-logo.png\"/>"},{"metadata":{},"cell_type":"markdown","source":"In this notebook we'll do dimensionality reduction and visualization of the features. We will make this visualization notebook with the Rapids library. [Rapids](https://rapids.ai) is an open-source GPU accelerated Data Sceince and Machine Learning library, developed and mainatained by [Nvidia](https://www.nvidia.com). It is designed to be compatible with many existing CPU tools, such as Pandas, scikit-learn, numpy, etc. It enables **massive** acceleration of many data-science and machine learning tasks, oftentimes by a factor fo 100X, or even more. \n\nRapids is still undergoing developemnt, and only recently has it become possible to use RAPIDS natively in the Kaggle Docker environment. If you are interested in installing and riunning Rapids locally on your own machine, then you should [refer to the followong instructions](https://rapids.ai/start.html)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import cudf\nimport cuml\nimport cupy as cp\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nfrom scipy.interpolate import interp1d\nimport gc\nfrom cuml.linear_model import Ridge\nfrom cuml.neighbors import KNeighborsRegressor\nfrom cuml.svm import SVR\nfrom cuml.ensemble import RandomForestRegressor\nfrom cuml.preprocessing.TargetEncoder import TargetEncoder\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom cuml.metrics import mean_squared_error\nfrom cuml.manifold import TSNE, UMAP\n\nimport soundfile as sf\n# Librosa Libraries\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.metrics import roc_auc_score, label_ranking_average_precision_score\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = cudf.read_csv(\"/kaggle/input/tabular-playground-series-feb-2021/train.csv\")\ntest = cudf.read_csv(\"/kaggle/input/tabular-playground-series-feb-2021/test.csv\")\nsample_submission = cudf.read_csv('../input/tabular-playground-series-feb-2021/sample_submission.csv')\n\ntarget = train['target'].values\ncolumns = test.columns[1:]\ncat_features = columns[:10]\ncat_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset contains 10 categorical features. These featrues cannot be used as they are for the dimensionality reduction, so we'll have to convert them into numerical values. We'll do this by target encoding them. Target encoding can be tricky, and the most rigorous way of doing it is by using some kind of cross-validation scheme. However, as we are only interested in visualizing the features, and not necessarily in getting good modeling features, we'll use a simpler approach to target encoding. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nFOLDS = 10\nSMOOTH = 0.001\nSPLIT = 'interleaved'\nfor col in cat_features:\n\n    encoder = TargetEncoder(n_folds=FOLDS, smooth=SMOOTH, split_method=SPLIT)\n    train[col] = encoder.fit_transform(train[col], train['target'])\n    test[col] = encoder.transform(test[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test = cp.vstack([train[columns].values, test[columns].values])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntsne = TSNE(n_components=2)\ntrain_test_2D = tsne.fit_transform(train_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_2D = cp.asnumpy(train_test_2D)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now let's take a look at the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(train_test_2D[:,0], train_test_2D[:,1], s = 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems there are some interesting groupings in the data.\n\nNow let's look at what the dataset looks with UMAP dimensionality reduction."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\numap = UMAP(n_components=2)\ntrain_test_2D = umap.fit_transform(train_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_2D = cp.asnumpy(train_test_2D)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(train_test_2D[:,0], train_test_2D[:,1], s = 0.5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That looks even more interesting."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}