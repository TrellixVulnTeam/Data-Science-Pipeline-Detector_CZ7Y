{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Robustly Encoding categorical features\n\nThe categorical features in the tabular playground series are stored as strings. They need to be encoded to something else to be used in machine learning models. I would like to build some encoding that is robust to having new categories that have not yet been seen in the training data."},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-feb-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-feb-2021/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = [x for x in train.columns if x.startswith('cat')]\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test sample with new categories to validate encodings"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_new_cats = pd.DataFrame(columns=test.columns, data = [[1]+['Z']*len(cat_cols)+[0]*14])\ntest_new_cats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using the LabelEncoder\nThe sklearn LabelEncoder can encode strings to some integer label."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nle.fit(train['cat8'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le.transform(['A'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The LabelEncoder is meant to encode the target variable, not the features. It cannot deal with not previously seen categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"le.transform(['Z'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using pandas CategoricalDtype"},{"metadata":{},"cell_type":"markdown","source":"The CategoricalDtype from pandas can be used in many machine learning models, including LightGBM and CatBoost. I created a simple transformer that 'learns' the available categories from the training data and encodes strings to categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom pandas.api.types import CategoricalDtype\n\nclass CategoricalTransform(BaseEstimator, TransformerMixin):\n    def __init__(self, cat_cols):\n        self.cat_cols = cat_cols\n        \n    def _transform_column(self, col, col_name):\n        return col.astype(self.cat_type[col_name]) \n        \n    def transform(self, df, **transform_params):\n        df_cat = df.copy()\n        for col in self.cat_cols:\n            df_cat[col] = self._transform_column(df_cat[col], col)\n        return df_cat\n        \n    def fit(self, X, y=None, **fit_params):\n        self.cat_type = dict()\n        for col in self.cat_cols:\n            self.cat_type[col] = CategoricalDtype(X[col].unique())\n        return self","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = CategoricalTransform(cat_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = ct.fit_transform(train)\nt.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Non-existing categories are encoded as NaN:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_new_cats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct.transform(test_new_cats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The transformer can be embedded in a sklearn Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom lightgbm.sklearn import LGBMRegressor\np = Pipeline([('cat_trans', CategoricalTransform(cat_cols)), \n              ('lgbm', LGBMRegressor(n_jobs=-2))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train.drop(columns=['target','id'])\ny_train = train['target']\np.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p.predict(test.drop(columns=['id']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\nscores = cross_validate(p, X=x_train, y=y_train, cv=5, return_train_score = True,\n                         scoring='neg_root_mean_squared_error')\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores['test_score'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"prediction with new categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"p.predict(test_new_cats.drop(columns=['id']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transform to integer value\nIf you need to transform to an integer value, for example to train an embedding in tensorflow, you can use the codes from the categorical feature instead. Below is a small transformer to do so."},{"metadata":{"trusted":true},"cell_type":"code","source":"class IntegerCategoricalTransform(CategoricalTransform):\n    def _transform_column(self, col, col_name):\n        return super()._transform_column(col, col_name).values.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = IntegerCategoricalTransform(cat_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = ct.fit_transform(train)\nt.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing values are encoded as -1"},{"metadata":{"trusted":true},"cell_type":"code","source":"ct.transform(test_new_cats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using this in a sklearn Pipeline\nHere I'm using CatBoost, as it is not trivial to use integer encoded features in the sklearn API of LightGBM."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom catboost import CatBoostRegressor\np = Pipeline([('cat_trans', IntegerCategoricalTransform(cat_cols)), \n              ('cb', CatBoostRegressor(iterations=50, thread_count=3, cat_features=cat_cols))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train.drop(columns=['target','id'])\ny_train = train['target']\np.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p.predict(test.drop(columns=['id']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\nscores = cross_validate(p, X=x_train, y=y_train, cv=5, return_train_score = True,\n                         scoring='neg_root_mean_squared_error')\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores['test_score'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p.predict(test_new_cats.drop(columns=['id']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One hot encoding\nRobust One hot encoding can be achieved by chaining the categorical transformer with the OneHotTransform below."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass OneHotTransform(BaseEstimator, TransformerMixin):\n    def transform(self, df, **transform_params):\n        return pd.get_dummies(df)\n    \n    def fit(self, X, y=None, **fit_params):\n        return self","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oh_pipe = Pipeline([('cat_trans', CategoricalTransform(cat_cols)),\n                    ('oh_trans', OneHotTransform())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_oh = oh_pipe.fit_transform(train)\ntrain_oh.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat5_cols = [col for col in train_oh.columns if col.startswith('cat5')]\ncat5_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_oh[cat5_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## New categories are encoded as zeros for each category column."},{"metadata":{"trusted":true},"cell_type":"code","source":"oh_pipe.transform(test_new_cats)[cat5_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using this in a sklearn Pipeline\nThe sklearn Randomforestregressor does not support categorical variables, so in this example I use one hot encoding for the categorical features. Note that the one hot pipeline defined before can be used as an element in the new pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\np = Pipeline([('oh_trans', oh_pipe), \n              ('rf', RandomForestRegressor(n_jobs=-2))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train.drop(columns=['target','id'])\ny_train = train['target']\np.fit(x_train.head(10000), y_train.head(10000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p.predict(test.drop(columns=['id']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\nscores = cross_validate(p, X=x_train.head(10000), y=y_train.head(10000), cv=5, return_train_score = True,\n                         scoring='neg_root_mean_squared_error')\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores['test_score'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p.predict(test_new_cats.drop(columns=['id']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}