{"cells":[{"metadata":{},"cell_type":"markdown","source":"# シンプルな11位の解法【詳細】\n# very simple 11th place solution【Details】"},{"metadata":{},"cell_type":"markdown","source":"このNotebookではPseudo Labeling(疑似ラベリング)について説明しています。11位の解法の概要についてはこちらのNotebookを参照してください。  \n【日本語&English】TPS Feb 11th place solution  \nhttps://www.kaggle.com/maostack/english-tps-feb-11th-place-solution\n  \nThis Notebook describes Pseudo Labeling, see this Notebook for an overview of the 11th solution.  \n【日本語&English】TPS Feb 11th place solution  \nhttps://www.kaggle.com/maostack/english-tps-feb-11th-place-solution"},{"metadata":{},"cell_type":"markdown","source":"## 疑似ラベリング / Pseudo Labeling\n半教師あり学習の手法の一つ / One of the methods of semi-supervised learning  \n  \n疑似ラベリングは2段階の構成になっている。  \nまず、何らかの予測モデルを用意する（今回はLightGBM）。  \n第1段階では、モデルを学習させた後、普通にテストデータに対して予測を行う。その予測値をテストデータに対する疑似ラベルとする。つまり、テストデータに対する予測値を疑似的に目的変数(label・target)として扱う。  \n第2段階では、\"もともとの学習データにテストデータを合体させたもの\"を学習データとして用いて、テストデータに対する予測を行う。  \n  \nThe pseudo labeling consists of two steps.  \nFirst, prepare some kind of prediction model (LightGBM in this case).  \nIn the 1st stage, we train the model and make a prediction for the test data. The predicted value is used as a pseudo-label for the test data. In other words, the predicted value for the test data is treated as a pseudo target variable (label/target).  \nIn the 2nd stage, predictions are made for the test data using the \"original training data combined with the test data\" as the training data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"別のNotebookで既に前処理をしたデータをtrain, testとして読み込んでいます。  \n前処理として行ったことは、  \n・targetが外れ値の行を除外(targetが4より小さい行を除外)  \n・変数\"cat6\"について\"G\"は学習データにしか存在しない(テストデータで値がGをとるデータが存在しない)ので、cat6の値がGの行を除外  \n・カテゴリ変数に対するLabel Encoding  \n・cont列に対するRankGauss変換  \nです。最後のRankGauss変換は、決定木系のモデルには影響を与えないのでしなくてもいいのですが一応しておきました。  \n除外後の学習データのデータ数は299963になりました。37行減った。テストデータの数は変わっていない。  \n  \nThe data that has already been preprocessed in another Notebook is loaded as train and test.  \nWhat we did as preprocessing was  \n・Exclude rows where target is an outlier (exclude rows where target is less than 4)  \n・For the variable \"cat6\", \"G\" exists only in the training data (there is no data that takes the value G in the test data), so the line with the value G in cat6 is excluded.  \n・Label Encoding for categorical variables  \n・RankGauss transform for cont columns  \nThe RankGauss transformation is not necessary because it does not affect the decision tree model, but I did it just in case.  \nAfter preprocessing, the number of data in the training data is now 299963. 37 rows have been reduced. The number of test data has not changed."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/tps-feb-eda-fe/train_data.csv\")\ntest = pd.read_csv(\"../input/tps-feb-eda-fe/test_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_columns = [f\"cat{i}\" for i in range(10)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop([\"target\"], axis=1)\nX_test = test\ny = train.target\n\nprint(X.shape)\nprint(X_test.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nSEED = 8970365","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# パラメータの値は他の方のNotebookを参考にしました。感謝します。\n# The value of the parameter was taken from another person's Notebook.\n# I appreciate it.\n\nparams_lgb = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.003899156646724397,\n    \"num_leaves\": 63,\n    \"max_depth\": 99,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.8805303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 9.562925363678952,\n    \"reg_lambda\": 9.355810045480153,\n    \"max_bin\": 882,\n    \"min_data_per_group\": 127,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 19,\n    \"verbosity\": -1,\n    \"bagging_seed\": SEED,\n    \"feature_fraction_seed\": SEED,\n    \"seed\": SEED\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"まず普通にテストデータに対して予測を行う。  \nFirst, make a prediction for the test data as usual."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 予測値を格納するdf\n# df to store the predicted value\npreds_lgb = pd.DataFrame()\n\nX[cat_columns] = X[cat_columns].astype(\"category\")\nX_test[cat_columns] = X_test[cat_columns].astype(\"category\")\n\nfor k, (tr_id, vl_id) in enumerate(kf.split(X, y)):\n    print(\"=\"*50)\n    print(f\"               KFold{k+1}\")\n    print(\"=\"*50)\n    \n    X_train, X_val = X.iloc[tr_id, :], X.iloc[vl_id, :]\n    y_train, y_val = y.iloc[tr_id], y.iloc[vl_id]\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_val = lgb.Dataset(X_val, y_val)\n    \n    model_lgb = lgb.train(params=params_lgb,\n                          train_set=lgb_train,\n                          valid_sets=lgb_val,\n                          num_boost_round=100000,\n                          early_stopping_rounds=200,\n                          verbose_eval=1000)\n    \n    pred_lgb = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n    pred_lgb = pd.DataFrame(pred_lgb)\n    \n    # 予測値を横に連結していく\n    # Concatenate the predictions horizontally\n    preds_lgb = pd.concat([preds_lgb, pred_lgb], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 平均を計算して、テストデータに対する疑似ラベルとする\n# Calculate the mean and use it as a pseudo labels for the test data\n\nlabel = preds_lgb.mean(axis=1)\nlabel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# もともとの学習データX, yにテストデータと疑似ラベルを縦に連結する。\n# これを新たな学習データとする\n# Concatenate the test data and pseudo labels to the original training data X, y.\n# Make this the new training data.\n\nX = pd.concat([X, X_test], axis=0).reset_index(drop=True)\ny = pd.concat([y, label], axis=0).reset_index(drop=True)\n\nprint(\"X.shape: \", X.shape)\nprint(\"y.shape: \", y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 最終予測値を格納するdf\n# df to store the final prediction\npreds_lgb = pd.DataFrame()\n\nX[cat_columns] = X[cat_columns].astype(\"category\")\nX_test[cat_columns] = X_test[cat_columns].astype(\"category\")\n\nfor k, (tr_id, vl_id) in enumerate(kf.split(X, y)):\n    print(\"=\"*50)\n    print(f\"               KFold{k+1}\")\n    print(\"=\"*50)\n    \n    X_train, X_val = X.iloc[tr_id, :], X.iloc[vl_id, :]\n    y_train, y_val = y.iloc[tr_id], y.iloc[vl_id]\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_val = lgb.Dataset(X_val, y_val)\n    \n    model_lgb = lgb.train(params=params_lgb,\n                          train_set=lgb_train,\n                          valid_sets=lgb_val,\n                          num_boost_round=100000,\n                          early_stopping_rounds=200,\n                          verbose_eval=1000)\n    \n    pred_lgb = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n    pred_lgb = pd.DataFrame(pred_lgb)\n    preds_lgb = pd.concat([preds_lgb, pred_lgb], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/tabular-playground-series-feb-2021/sample_submission.csv\")\n\n#　予測値の平均を計算して、最終的な予測値とする\n# Calculate the average of the predictions to get the final prediction.\npred = preds_lgb.mean(axis=1)\nsubmission.target = pred\n\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission_pseudo_lgb_5.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"このNotebookと同じ予測方法で、パラメータの値を変えたり、シード値を変えたり、early stopping roundsを変えたりなどをしながら、複数の予測提出ファイルを作る。最後にそれらをアンサンブルする。  \n以上が11位の解法です。\n  \nUsing the same prediction method as in this Notebook, create multiple prediction submission files, changing the paramters, seed value and early stopping rounds etc.. Finally, ensemble them.  \nThis is the 11th solution."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}