{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#standard\nimport time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import ceil\nfrom tqdm.notebook import tqdm\nfrom pathlib import Path\n\n#sklearn data_preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n#sklearn categorical encoding\nimport category_encoders as ce\n\n#sklearn modelling\nfrom sklearn.model_selection import KFold\n\n# boosting library\nimport lightgbm as lgb\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndata = pd.read_csv(\"../input/tabular-playground-series-feb-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-feb-2021/test.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROOT = Path.cwd().parent\nINPUT = ROOT / \"input\"\nDATA = INPUT / \"tabular-playground-series-feb-2021\"\nWORK = ROOT / \"working\"\n\nfor path in DATA.iterdir():\n    print(path.name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(DATA / \"train.csv\")\ntest = pd.read_csv(DATA / \"test.csv\")\nsmpl_sub = pd.read_csv(DATA / \"sample_submission.csv\")\nprint(\"train: {}, test: {}, sample sub: {}\".format(\n    train.shape, test.shape, smpl_sub.shape\n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe(include = \"all\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set id as as index\ndata.set_index(\"id\",inplace=True)\ntest.set_index(\"id\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we have 1 id, 10 categorical variables, 14 continuous variables\ncat_feats = data.iloc[:,0:10].columns\nnumeric_feats = data.iloc[:,10:-1].columns\ntrain = data.iloc[:,:-1]\ntarget = data.iloc[:,-1].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Function defining**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(y_true, y_pred):\n    \"\"\"calculate rmse\"\"\"\n    return np.sqrt(np.mean((y_true - y_pred) ** 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def enc_scl_pipe(X_train, y_train, X_test, enc_method, scaler = StandardScaler()): \n    X_train_encoded = X_train.copy()\n    X_test_encoded= X_test.copy()\n    # Set up feature to encode\n    feature_to_encode = X_train.columns[X_train.dtypes == 'O'].tolist()\n    \n    if enc_method == 'label':\n        for feat in feature_to_encode:\n            # Initia the encoder model\n            lbEncoder = LabelEncoder()\n            # fit the train data\n            lbEncoder.fit(X_train[feat])\n\n            # transform training set\n            X_train_encoded[feat] = lbEncoder.transform(X_train[feat])\n            # transform test set\n            X_test_encoded[feat] = lbEncoder.transform(X_test[feat])\n            \n    elif enc_method == 'glmm':\n        # Initia the encoder model\n        GLMMEncoder = ce.glmm.GLMMEncoder(verbose =0 ,binomial_target=False)\n        # fit the train data\n        GLMMEncoder.fit(X_train[feature_to_encode],y_train)\n        # transform training set\n        X_train_encoded[feature_to_encode] = GLMMEncoder.transform(X_train[feature_to_encode])\n        # transform test set\n        X_test_encoded[feature_to_encode] = GLMMEncoder.transform(X_test[feature_to_encode])\n    else:\n        raise 'No encoding method stated'\n        \n    # fit the scaler                    \n    scaler.fit(X_train_encoded)\n    # transform training set\n    X_train_scaled = pd.DataFrame(scaler.transform(X_train_encoded), columns=X_train_encoded.columns, index=X_train_encoded.index)\n    # transform test set\n    X_test_scaled = pd.DataFrame(scaler.transform(X_test_encoded), columns=X_test_encoded.columns, index=X_test_encoded.index)\n    \n    return X_train_scaled, X_test_scaled, feature_to_encode","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kfold_CV_pipe(x_train, y_train, test, model, columns,enc_method, n_splits = 10):\n    all_scores = []\n    # set up k-fold\n    kf = KFold(n_splits=n_splits)\n    # all data\n    _, test,__ = enc_scl_pipe(x_train,y_train, test,enc_method = enc_method)\n    \n    train_oof_preds = np.zeros((300000,))\n    test_preds = 0\n    for  f, (trn_idx, val_idx) in tqdm(enumerate(kf.split(x_train, y_train))):\n        # set up the splitted data\n        train       , val        = x_train.iloc[trn_idx][columns], x_train.iloc[val_idx][columns]\n        train_target, val_target = y_train[trn_idx]              , y_train[val_idx]\n        \n        # encode\n        # k-fold data\n        train, val, categorical_feats = enc_scl_pipe(train,train_target, val, enc_method = enc_method)        \n        # model fitting\n      \n        model.fit(train, train_target, eval_set=[(val, val_target)],\n                          eval_metric=['rmse'],\n                          early_stopping_rounds=2000,                        \n                          verbose=0)\n\n        # get predicted values for oof data and whole test set\n        temp_oof = model.predict(val)\n        temp_test = model.predict(test[columns])\n        # get predicted values for whole data set aggregate from each fold iter\n        train_oof_preds[val_idx] = temp_oof\n        test_preds += temp_test/n_splits\n        \n        fold_score = rmse(val_target,temp_oof)\n        all_scores.append(fold_score)\n        print(fold_score)        \n    return  train_oof_preds, test_preds, all_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LightGBM base\nlgb_params_base = {'subsample': 0.8,\n             'learning_rate': 0.5,\n             'max_depth': 30,\n             'num_leaves': 80,\n             'min_child_samples': 100,\n             'random_state': 22,\n             'n_estimators': 100,\n             'metric': 'rmse',\n             'max_bin': 600, \n             'cat_l2': 4,\n             'cat_smooth': 79}\n\n# LightGBM params\nlgb_params = {'reg_alpha': 6.15,\n             'reg_lambda': 0.0025,\n             'colsample_bytree': 0.3,\n             'subsample': 0.8,\n             'learning_rate': 0.004,\n             'max_depth': 15,\n             'num_leaves': 80,\n             'min_child_samples': 250,\n             'random_state': 22,\n             'n_estimators': 20000,\n             'metric': 'rmse',\n             'max_bin': 600, \n             'cat_smooth': 50}\n# LightGBM params 2\nlgb_params_2 = {'max_depth': 16, \n                'subsample': 0.8032697250789377, \n                'colsample_bytree': 0.21067140508531404, \n                'learning_rate': 0.003,\n                'reg_lambda': 5.25, \n                'reg_alpha': 8.2914, \n                'min_child_samples': 31, \n                'num_leaves': 320, \n                'max_bin': 522, \n                'cat_smooth': 81, \n                'cat_l2': 0.029690334194270022, \n                'metric': 'rmse', \n                'n_jobs': -1, \n                'n_estimators': 20000}\n\n# xgboost\nxgb_params = {\n                'booster':'gbtree',\n                'n_estimators':20000,\n                'max_depth':11, \n                \"learning_rate\": 0.009,\n                'gamma':3.5,\n                'objective':'reg:squarederror',\n                'verbosity':0,\n                'subsample':0.65,\n                'colsample_bytree':0.3,\n                'reg_lambda':0.5,\n                'reg_alpha':8,\n                'scale_pos_weight':1,\n                'objective':'reg:squarederror',\n                'eval_metric':'rmse',\n                'seed': 22,\n                'tree_method':'gpu_hist',\n                'gpu_id':0\n                }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(data):\n    new_data = data.copy()\n    new_data['cat2p6'] = new_data['cat2'] + new_data['cat6']\n    new_data['cat6p1'] = new_data['cat6'] + new_data['cat1']\n    new_data['cat2p1'] = new_data['cat2'] + new_data['cat1']\n    \n    new_data['cont0p8'] = new_data['cont0']*new_data['cont8']\n    new_data['cont0p5'] = new_data['cont0']*new_data['cont5']\n    new_data['cont11p8'] = new_data['cont11']*new_data['cont8']\n    return new_data\n\nnew_train = feature_engineering(train)\nnew_test = feature_engineering(test)\n\nprint(new_train.shape)\nprint(new_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lgb\nprint(\"lgb base CV scores label\")\nmodel = lgb.LGBMRegressor(**lgb_params_base)\nlgb_train_oof, lgb_test_preds, lgb_all_scores = kfold_CV_pipe(train, target, test, model,enc_method = 'label', columns = train.columns)\noof_score = rmse(target, lgb_train_oof)\nprint(f\"lgb oof score: {oof_score:.6f}\")\nprint(\"---------------------------------------------------------------\")\n# lgb\nprint(\"lgb base CV scores glmm\")\nmodel = lgb.LGBMRegressor(**lgb_params_base)\nlgb_train_oof, lgb_test_preds, lgb_all_scores = kfold_CV_pipe(train, target, test, model,enc_method = 'glmm' ,columns = train.columns)\noof_score = rmse(target, lgb_train_oof)\nprint(f\"lgb oof score: {oof_score:.6f}\")\nprint(\"---------------------------------------------------------------\")\n# lgb\nprint(\"lgb base new features CV scores\")\nmodel = lgb.LGBMRegressor(**lgb_params_base)\nlgb_train_oof, lgb_test_preds, lgb_all_scores = kfold_CV_pipe(new_train, target, new_test, model,enc_method = 'glmm', columns = new_train.columns)\noof_score = rmse(target, lgb_train_oof)\nprint(f\"lgb oof score: {oof_score:.6f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"---------------------------------------------------------------\")\nprint(\"===============================================================\")\nprint(\"---------------------------------------------------------------\")\n# lgb\nprint(\"lgb1 CV scores\")\nmodel = lgb.LGBMRegressor(**lgb_params)\nlgb1_train_oof, lgb1_test_preds, lgb1_all_scores = kfold_CV_pipe(train, target, test, model,enc_method = 'glmm', columns = train.columns)\noof_score = rmse(target, lgb1_train_oof)\nprint(f\"lgb1 oof score: {oof_score:.6f}\")\n\nprint(\"---------------------------------------------------------------\")\nprint(\"===============================================================\")\nprint(\"---------------------------------------------------------------\")\n# lgb\nprint(\"lgb2 CV scores\")\nmodel = lgb.LGBMRegressor(**lgb_params_2)\nlgb2_train_oof, lgb2_test_preds, lgb2_all_scores = kfold_CV_pipe(train, target, test, model,enc_method = 'glmm', columns = train.columns)\noof_score = rmse(target, lgb2_train_oof)\nprint(f\"lgb2 oof score: {oof_score:.6f}\")\n\nprint(\"---------------------------------------------------------------\")\nprint(\"===============================================================\")\nprint(\"---------------------------------------------------------------\")\nprint(\"xgb CV scores\")\nmodel =  xgb.XGBRegressor(**xgb_params)\nxgb_train_oof, xgb_test_preds, xgb_all_scores = kfold_CV_pipe(train, target, test, model,enc_method = 'glmm', columns = train.columns)\noof_score = rmse(target, xgb_train_oof)\nprint(f\"xgb oof score: {oof_score:.6f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = smpl_sub.copy()\n# sub[TGT_COL] = test_pred_avg\nsub['target'] = lgb1_test_preds\n\nsub.to_csv(\"lgb1_submission.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = smpl_sub.copy()\n# sub[TGT_COL] = test_pred_avg\nsub['target'] = lgb2_test_preds\n\nsub.to_csv(\"lgb2_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = smpl_sub.copy()\n# sub[TGT_COL] = test_pred_avg\nsub['target'] = xgb_test_preds\n\nsub.to_csv(\"xgb_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = [0.2, 0.4, 0.4]\n\noof_pred_wavg = weights[0]*lgb1_train_oof + weights[1]*lgb2_train_oof + weights[2]*xgb_train_oof\noof_score_wavg = rmse(target, oof_pred_wavg)\n\nprint(f\"oof score weighted avg: {oof_score_wavg:.6f}\")\n\ntest_pred_wavg = weights[0]*lgb1_test_preds + weights[1]*lgb2_test_preds + weights[2]*xgb_test_preds\n\nsub = smpl_sub.copy()\n# sub[TGT_COL] = test_pred_avg\nsub['target'] = test_pred_wavg\n\nsub.to_csv(\"wavg_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}