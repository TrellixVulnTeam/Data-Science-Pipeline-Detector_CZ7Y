{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fast.ai on Feb Playground\n\nThis notebook uses fast.ai to use a neural network on the February Playground data.\nAll data analysis is done in a separate notebook."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import packages and load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n\nfrom fastai import *\nfrom fastai.tabular.all import *\nimport torch\n\nsns.set_theme(style=\"white\")\ncmap_div = sns.diverging_palette(230, 20, as_cmap=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fixing seed\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED) # gpu vars\ntorch.backends.cudnn.deterministic = True  #needed\ntorch.backends.cudnn.benchmark = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/tabular-playground-series-feb-2021","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dpath = Path('../input/tabular-playground-series-feb-2021')\nsample_sub = pd.read_csv(dpath / 'sample_submission.csv')\ntest_raw = pd.read_csv(dpath / 'test.csv')\ntrain_raw = pd.read_csv(dpath / 'train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA\n\nJust a very brief look to avoid nonsense."},{"metadata":{"trusted":true},"cell_type":"code","source":"# crate full DataFrame for easier overview of the data\nfull = pd.concat([train_raw.copy(deep = True), test_raw.copy(deep =True)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quite a few cont. features are strongly correlated\nsns.heatmap(full.drop(columns=['id']).corr(), cmap=cmap_div);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# luckely no columns have NA values\nfull.isna().sum().to_frame(name='NA').query('NA > 0')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no duplicated entries\nfull.drop(columns=['id', 'target']).duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TODOs\n* look for temporal correlation\n* ...\n"},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\nFor fast.ai no (or almost no) feature engineering is done."},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot_encoding(df):\n    # pandas take care of one-hot encoding of categorial features\n    df = pd.get_dummies(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_data(df):\n    df = df.reset_index(drop=True) # this makes the index going from 0 .. n-1 independently of any transformation before\n    id = df['id']\n    df = df.drop(columns=['id'])\n    \n    if 'target' in df.columns:\n        x = df.drop(columns=['target'])\n        y = df['target']\n    else:\n        x = df\n        y = None\n    \n    return (x, y, id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_data_fastai(df):\n    df = df.reset_index(drop=True) # this makes the index going from 0 .. n-1 independently of any transformation before\n    id = df['id']\n    df = df.drop(columns=['id'])\n    \n    return (df, id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_all(df, funs, debug=False):\n    \"\"\"Helper function to apply a series of functions onto a DataFrame\"\"\"\n    for fun in funs:\n        if debug:\n            print(f'Apply {fun.__name__}')\n        df = fun(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Model\n\nHere I calculate the performance of a naive model, which just applies the median."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_raw.copy(deep = True)\nprep_triv = lambda x: apply_all(x, [one_hot_encoding, split_data])\nx, y, id = prep_triv(train)\n\ntriv_model = DummyRegressor(strategy='median')\n\nscores = cross_validate(triv_model, x, y, cv=5,\n                        scoring=('neg_root_mean_squared_error'),\n                        n_jobs=-1)\ntriv_model.fit(x, y)\n\n\nprint(f'triv_model - RMSE: {np.mean(scores[\"test_score\"]*-1)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## fast.ai\n\nSince the dataset is relativly large, the employed model can also quite big.\nThe data contains no missing data, so there is no need for much preprocessing here.\nI've used a small dropout probability for the embedding and linear layers. Setting the dropout to zero, leads to very unstable training."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_raw.copy(deep = True)\nprep_nn1 = lambda x: apply_all(x, [split_data_fastai])\ntrain, train_ids = prep_nn1(train)\n\ntorch.device('cuda') # enable cuda, (activate GPU usage)\n\ncont_names = [f'cont{i}' for i in range(14)] # set the continous variables\ncat_names = [f'cat{i}' for i in range(10)] # set the categoriall variables\nprocs = [Categorify, Normalize] # different fast.ai preprocessing steps\ndep_var = 'target' # our target variable\n\nsplits = RandomSplitter(valid_pct=0.25, seed=42)(train.index) # to validate the results we use randomly 20% of the training set\n\ncfg = tabular_config(embed_p=0.10, ps=0.10)\n\ndls = TabularPandas(train,\n                    cont_names=cont_names,\n                    cat_names=cat_names,\n                    procs=procs,\n                    y_names=dep_var,\n                    splits=splits).dataloaders(bs=2056)\n\n\ncallbacks = [SaveModelCallback(min_delta=0.001, monitor='_rmse', comp=np.less, fname='model_triv_best')]\n\nlearn = tabular_learner(dls, layers=[2000,500], metrics=[rmse], config=cfg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(45, lr_max=2e-2, cbs=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(45, lr_max=slice(1e-6, 1e-6, 1e-2), cbs=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_loss()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final result is not much better than the trivial model. Nevertheless I submit this first results."},{"metadata":{"trusted":true},"cell_type":"code","source":"test, test_id = prep_nn1(test_raw.copy(deep=True))\n\ntest_dl = learn.dls.test_dl(test)\n\npreds, _ = learn.get_preds(dl=test_dl)\npreds = preds.numpy().T[0]\n\nsubmission = pd.DataFrame(\n    {'id': test_id,\n     'target': preds}\n)\nsubmission.to_csv('submission_trivial_nn.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, run the model also on the full training dataset and store the results of the training and test set for later usage."},{"metadata":{"trusted":true},"cell_type":"code","source":"full_train_dl = learn.dls.test_dl(train)\n\npreds, _ = learn.get_preds(dl=full_train_dl)\npreds = preds.numpy().T[0]\n\nfull_train_results = pd.DataFrame(\n    {'id': train_ids,\n     'target': preds}\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p '/kaggle/working/Feb2021Playground/FastAi'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('/kaggle/working/Feb2021Playground/FastAi/test_results_fastai.csv', index=False)\nfull_train_results.to_csv('/kaggle/working/Feb2021Playground/FastAi/train_results_fastai.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}