{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Using Optuna for Fast.ai on Feb Playground\n\nThis notebooks show how to use pptuna to tune the hyperparamter of a Neural Network which is trained on the Februrary Playgorund data from kaggle.\n\nA first notebook using fast.ai without optimizes hyperparameter Optimization can be found here: https://www.kaggle.com/martinmarenz/first-pred-feb-tabular-playground-with-fast-ai\n\nJust to mention to save you a little bit time. By no means I expect that the resulting neural network rank high in the competition. If you are looking for something like that, you have to find it somewhere else.\n"},{"metadata":{},"cell_type":"markdown","source":"## Import packages and load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nfrom pathlib import Path\n\nimport joblib\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nimport optuna\nfrom optuna.integration import FastAIV2PruningCallback\nfrom fastai import *\nfrom fastai.tabular.all import *\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# autocompletaion works better this way\n%config Completer.use_jedi = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fixing seed\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED) # gpu vars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/tabular-playground-series-feb-2021","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dpath = Path('../input/tabular-playground-series-feb-2021')\nsample_sub = pd.read_csv(dpath / 'sample_submission.csv')\ntest_raw = pd.read_csv(dpath / 'test.csv')\ntrain_raw = pd.read_csv(dpath / 'train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare data"},{"metadata":{},"cell_type":"markdown","source":"I use the apply method to easily add later any kind of preprocessing. Honestly, I just copy this approach around different notebooks, since it allows to play around with different approaches and preprocessing steps.\n\nNevertheless, for now only the `id` column has to be dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_data_fastai(df):\n    df = df.reset_index(drop=True) # this makes the index going from 0 .. n-1 independently of any transformation before\n    id = df['id']\n    df = df.drop(columns=['id'])\n    \n    return (df, id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_all(df, funs, debug=False):\n    \"\"\"Helper function to apply a series of functions onto a DataFrame\"\"\"\n    for fun in funs:\n        if debug:\n            print(f'Apply {fun.__name__}')\n        df = fun(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_raw.copy(deep = True)\nprep_nn1 = lambda x: apply_all(x, [split_data_fastai])\ntrain, train_ids = prep_nn1(train)\ntorch.device('cuda') # enable cuda, (activate GPU usage)\n\ncont_names = [f'cont{i}' for i in range(14)] # set the continous variables\ncat_names = [f'cat{i}' for i in range(10)] # set the categoriall variables\nprocs = [Categorify, Normalize] # different fast.ai preprocessing steps\ndep_var = 'target' # our target variable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting up optuna"},{"metadata":{},"cell_type":"markdown","source":"Optuna's API is stunningly easy, you just have to wrap your normal training loop around a trial and let optuna create suggestion\nfor all hyperparameters you usally set manually. This to tune every aspect of your model. Nevertheless, the following code just\ndo standard stuff.\n\nFirstly, via `num_layers = trial.suggest_int('n_layers', 2, 7)` the number of hidden layers can be between 2 and 7 layers, the final layer is added automatically by fast.ai.\nFor each of this layer the size will be drawn from the corresponding level of `pot_layers`. I do this to nudge the network architecture into a funnel.\nOut of interest I have done before this optimization a run where the architecture was randomly sampled. However, the best trials where networks with shrinking layer sizes.\n\nThe rest is a standard training loop for fast.ai, where different hyperparamter values are created via optuna trials.\nThe `FastAIV2PruningCallback` allows optuna to stop trials which are not promising, and thus sample the search space much faster.\nThe other callback, `SaveModelCallback`  is used within each training loop to save the best model during this trial.\nThen later, the model of the best trial is stored with the help of another callback.\n\nWhen starting the study I set `n_warmup_steps=5` and `interval_steps=5` for two reasons. Firstly, I want to avoid that Optuna prune some trials right in the beginning, only because their loss is very high\nat this moment. That could be just some random effects.\nThe interval_steps is set to 5 to reduce the effect of random fluctions in the loss a bit. Having the posibilty to define something like `patience` would be\nbetter but for now this approach must be sufficient. (see: https://github.com/optuna/optuna/issues/1447)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dpath = Path('/kaggle/working/Feb2021Playground/OptunaFastAi')\ndpath.mkdir(exist_ok=True, parents=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner = None\n\nlpath = Path(dpath/\"best_learner.pkl\")\n\nif lpath.exists():\n    best_learner = joblib.load(lpath)\nelse:\n    best_learner = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pot_layers = [\n    [500, 1000, 1500, 2000, 2500,  3000, 3500, 4000, 5000],\n    [100, 250, 500, 750, 1000, 1500, 2000, 2500, 3000],\n    [50, 100, 200, 300, 400, 500, 750, 1000, 1500, 2000],\n    [50, 100, 150, 200, 300, 400, 500, 750, 1000],\n    [50, 100, 200, 300, 400, 500],\n    [50, 100, 200, 300, 400],\n    [50, 100, 200, 300]\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial: optuna.Trial):\n    num_layers = trial.suggest_int('n_layers', 2, 7)\n    num_layers += 1\n    layers, ps = [], []\n    \n    # shrinking sizes for deeper layers\n    pot_layers = [\n        [500, 1000, 1500, 2000, 2500,  3000, 3500, 4000, 5000],\n        [100, 250, 500, 750, 1000, 1500, 2000, 2500, 3000],\n        [50, 100, 200, 300, 400, 500, 750, 1000, 1500, 2000],\n        [50, 100, 150, 200, 300, 400, 500, 750, 1000],\n        [50, 100, 200, 300, 400, 500],\n        [50, 100, 200, 300, 400],\n        [50, 100, 200, 300]\n    ]\n\n    # size of last layer is choosen automatically by fast.ai\n    for i in range(num_layers - 1):\n        num_units = trial.suggest_categorical(f'num_units_{i}', pot_layers[i])\n        \n        # although my inital intuition would be to reduce the dropout for deeper\n        # layers, the optimization showd that this does not lead to the best results\n        p = trial.suggest_uniform(f'ps_{i}', 0, 0.5)\n        \n        layers.append(num_units)\n        ps.append(p)\n    \n\n     # to validate the results we use randomly 20% of the training set\n    splits = RandomSplitter(valid_pct=0.25, seed=42)(train.index)\n\n    dls = TabularPandas(\n        train,\n        cont_names=cont_names,\n        cat_names=cat_names,\n        procs=procs,\n        y_names=dep_var,\n        splits=splits\n    ).dataloaders(bs=8224)\n\n\n    callbacks = [\n        SaveModelCallback(min_delta=0.0005, monitor='_rmse', comp=np.less, fname='model_triv_best'),\n        FastAIV2PruningCallback(trial, monitor='_rmse')\n    ]\n\n    # I leave the automically size for the embedding layers in place, but trial on the embbeding droppout\n    emb_drop = trial.suggest_uniform('emb_drop', 0, 0.35)\n    \n    cfg = tabular_config(embed_p=emb_drop, ps=ps)\n    global learner\n    learner = tabular_learner(dls, layers=layers, metrics=[rmse], config=cfg)\n    \n    # could be improved by an automatic learning rate finder, fast.ai brings the capabilities for that,\n    # when doing so, the learning rate would fit to the size of the network\n    \n    \n    lr_max = trial.suggest_uniform('lr_max', 0.01, 0.2)\n    weight_decay = trial.suggest_uniform('weight_decay', 0.02, 0.25)\n    learner.fit_one_cycle(55, lr_max=lr_max, wd=weight_decay, cbs=callbacks)\n\n    return learner.validate()[-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This callback stores the best model in the global variable `best_learner`, so I can use it later without retraining."},{"metadata":{"trusted":true},"cell_type":"code","source":"def saveBestModelCallback(study, trial):\n    global best_learner\n    if study.best_trial == trial:\n        best_learner = learner","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reload an existing study, if existing; this allows to rerun the notebook and get better results\nspath = dpath / \"study.pkl\"\nif spath.exists():\n    study = joblib.load(spath)\nelse:\n    study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=4, interval_steps=4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"study.optimize(objective, timeout=60*60*4, callbacks=[saveBestModelCallback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# store the study for further optimization\njoblib.dump(study, spath)\njoblib.dump(best_learner, lpath)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization of the Hyperparameter\n\nMaybe the coolest stuff optuna offers, is the ability to visualize the hyperparameters.\nOne can have a look on the interdependence of the different hyperparameter, see what has worked in many trials and where understand what worked for the problem at hand."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trials = study.trials_dataframe() \\\n    .sort_values(by=['value']) \\\n    .drop(columns=['datetime_start', 'datetime_complete', 'number', 'state']) # drop uninteresting columns\n\n\ndf_trials['duration'] = df_trials['duration'].dt.total_seconds()/60.0\n# shrik some colum names for better overview\npar_cols = df_trials.columns[df_trials.columns.str.startswith('params')]\ndf_trials = df_trials.rename(columns={col: col[7:] for col in par_cols})\n\nlayer_cols = [f'num_units_{i}' for i in range(5)]\npdrop_cols = [f'ps_{i}' for i in range(5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_edf(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_contour(study,\n                                  params=['lr_max',\n                                          'n_layers',\n                                          'weight_decay',\n                                          'emb_drop'\n                                         ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_contour(study,\n                                  params=['num_units_0',\n                                          'num_units_1',\n                                          'num_units_2',\n                                          'num_units_3',\n                                         ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_trials = df_trials.nsmallest(n=20, columns=['value'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_trials","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# more layers seems to be better :)\nbest_trials['n_layers'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclussion from the best trials: (I hope they are still valid after the save run :))\n\n* None of the trials beat optimized LightGBM, XGBoost or similiar approaches\n* best number of layers is consistently 4\n* the size of the layer fluctuates quite a lot, but in general a funnel network emerges\n* dropout fluctuates quite a bit, but to seems highest in second layer, and does NOT go down to 0\n* rest is quite standard:\n    * lr_max [0.10, 0.13]\n    * weight_decay [0.08, 0.11]\n    * emb_drop [0.15, 0.25]"},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create the Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test, test_id = prep_nn1(test_raw.copy(deep=True))\n\ntest_dl = best_learner.dls.test_dl(test)\n\npreds, _ = best_learner.get_preds(dl=test_dl)\npreds = preds.numpy().T[0]\n\nsubmission = pd.DataFrame(\n    {'id': test_id,\n     'target': preds}\n)\nsubmission.to_csv('submission_trivial_nn.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# next to the submission I also store the results on the full training set somewhere\nfull_train_dl = best_learner.dls.test_dl(train)\n\npreds, _ = best_learner.get_preds(dl=full_train_dl)\npreds = preds.numpy().T[0]\n\nfull_train_results = pd.DataFrame(\n    {'id': train_ids,\n     'target': preds}\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p '/kaggle/working/Feb2021Playground/OptunaFastAi'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('/kaggle/working/Feb2021Playground/OptunaFastAi/test_results_fastai.csv', index=False)\nfull_train_results.to_csv('/kaggle/working/Feb2021Playground/OptunaFastAi/train_results_fastai.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}