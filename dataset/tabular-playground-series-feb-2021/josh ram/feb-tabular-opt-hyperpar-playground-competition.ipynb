{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, you will learn how to make your first submission to the [Tabular Playground Series - Feb 2021 competition.](http://https://www.kaggle.com/c/tabular-playground-series-feb-2021)\n\n# Make the most of this notebook!\n\nYou can use the \"Copy and Edit\" button in the upper right of the page to create your own copy of this notebook and experiment with different models. You can run it as is and then see if you can make improvements."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\n        \ninput_path = Path('/kaggle/input/tabular-playground-series-feb-2021/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read in the data files"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(input_path / 'train.csv', index_col='id')\ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(input_path / 'test.csv', index_col='id')\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(input_path / 'sample_submission.csv', index_col='id')\ndisplay(submission.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We need to encode the categoricals.\n\nThere are different strategies to accomplish this, and different approaches will have different performance when using different algorithms. For this starter notebook, we'll use simple encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in train.columns:\n    if train[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(train[c].values)\n        test[c] = lbl.transform(test[c].values)\n        \ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pull out the target, and make a validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train.pop('target')\nX_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How well can we do with a completely naive model?\n\nWe'll want any of our models to do (hopefully much!) better than this."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's get a benchmark score\nmodel_dummy = DummyRegressor(strategy='median')\nmodel_dummy.fit(X_train, y_train)\ny_dummy = model_dummy.predict(X_test)\nscore_dummy = mean_squared_error(y_test, y_dummy, squared=False)\nprint(f'{score_dummy:0.5f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple Linear Regression\n\nA simple linear regression doesn't do better than our dummy regressor! (Alghouth, simple categorical encoding really doesn't make sense for this approach!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple Linear Regression\nmodel_simple_linear = LinearRegression(fit_intercept=False) # data is not centered, don't fit intercept\nmodel_simple_linear.fit(X_train, y_train)\ny_simple_linear = model_simple_linear.predict(X_test)\nscore_simple_linear = mean_squared_error(y_test, y_simple_linear, squared=False)\nprint(f'{score_simple_linear:0.5f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# This seems slow and repetative. Can we automate it a bit?"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_results(name, y, yhat, num_to_plot=10000, lims=(0,12), figsize=(6,6)):\n    plt.figure(figsize=figsize)\n    score = mean_squared_error(y, yhat, squared=False)\n    plt.scatter(y[:num_to_plot], yhat[:num_to_plot])\n    plt.plot(lims, lims)\n    plt.ylim(lims)\n    plt.xlim(lims)\n    plt.title(f'{name}: {score:0.5f}', fontsize=18)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#help(RandomForestRegressor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_names = [\"Dummy Median\", \"Linear\",  \"Lasso\", \"Random Forest\"]\n\nmodels = [\n    DummyRegressor(strategy='median'),\n    LinearRegression(fit_intercept=False),\n    Lasso(fit_intercept=False),\n    RandomForestRegressor(n_estimators=1000, n_jobs=-1,\n                         max_depth=2)]\n\nfor name, model in zip(model_names, models):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    plot_results(name, y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# It look like RandomForest did the best. Let's train it on all the data and make a submission!"},{"metadata":{},"cell_type":"markdown","source":"Hyperparameter Study"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\nfrom sklearn.datasets import make_classification\nfrom functools import partial\nfrom xgboost import XGBRegressor\nimport math\nimport plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### A function to calculate errors ####\ndef root_mean_squared_log_error(y_valid, y_preds):\n    \"\"\"Calculate root mean squared error of log(y_true) and log(y_pred)\"\"\"\n    if len(y_preds)!=len(y_valid): return 'error_mismatch'\n    y_preds_new = [math.log(x) for x in y_preds]\n    y_valid_new = [math.log(x) for x in y_valid]\n    return mean_squared_error(y_valid_new, y_preds_new, squared=False)\n#########################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"space = [\n#    Real(0.9, 1.0, name=\"colsample_bylevel\"),\n#    Real(0.4, 0.6, name=\"colsample_bytree\"),\n#    Real(0.0, 0.0001, name=\"gamma\"),\n    Real(0.001, 0.03, name=\"learning_rate\"),\n#    Real(0., 0.1, name=\"max_delta_step\"),\n    Integer(0, 5, name=\"max_depth\"), ## Influencer parameter\n#    Real(0, 0.1, name=\"min_child_weight\"),\n    Integer(1200, 1400, name=\"n_estimators\"),\n#    Real(0., 0.001, name=\"reg_alpha\"),\n#    Real(0.1, 1, name=\"reg_lambda\"),\n#    Real(0.79, 0.8, name=\"subsample\"),\n#    Real(0.4, 0.5, name=\"base_score\"),\n]\n\n#########\n#########\n# function to fit the model and return the performance of the model\ndef return_model_assessment(args, X_train, y_train, X_test, X_valid):\n    global models, train_scores, test_scores, curr_model_hyper_params\n    params = {curr_model_hyper_params[i]: args[i] for i, j in enumerate(curr_model_hyper_params)}\n    #print('Mira los params: ',params)\n    model = XGBRegressor(random_state=0,\n#                          gpu_id=-1,\n#                          booster='gbtree', \n#                          monotone_constraints='()',\n                          n_jobs=0, num_parallel_tree=1,\n                          colsample_bytree=0.6,\n                          max_depth=2,\n                          learning_rate=0.02, max_delta_step=0,\n                          subsample=0.8,\n#                          scale_pos_weight=1,\n#                          tree_method='exact', validate_parameters=1,\n                          verbosity=None)                    \n    model.set_params(**params)\n    #model = Pipeline(steps=[('feature_preprocessor', feature_preprocessor2),('feature_model', model1)])\n    fitted_model = model.fit(X_train, y_train)\n    models.append(fitted_model)\n    train_predictions = model.predict(X_test)\n    #test_predictions = model.predict(X_test)\n    train_score = root_mean_squared_log_error(y_test, train_predictions)\n    #test_score = root_mean_squared_log_error(feature_y_valid2, test_predictions)\n    train_scores.append(train_score)\n    #test_scores.append(test_score)\n    return train_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# collecting the fitted models and model performance\nmodels = []\ntrain_scores = []\ntest_scores = []\ncurr_model_hyper_params = ['learning_rate','max_depth','n_estimators']  \n#    'colsample_bylevel', 'colsample_bytree',\n#                           'gamma', 'learning_rate', 'max_delta_step',\n#                           'max_depth', 'min_child_weight', \n#                           'n_estimators', 'reg_alpha', 'reg_lambda',\n#                           'subsample','base_score']\n\nobjective_function = partial(return_model_assessment,\n                             X_train=X_train, \n                             y_train=y_train, \n                             X_test=X_test, \n                             X_valid=X_test)\n\n# running the algorithm\nn_calls = 20 # number of times you want to train your model\nresults = gp_minimize(objective_function, space, base_estimator=None,\n                      n_calls=n_calls,n_random_starts=n_calls-1,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics = pd.DataFrame(train_scores)\nmetrics.loc[:,'dataset'] = [\"train_score\"]*n_calls\nmetrics.loc[:,'Iteration Number'] = list(range(1,n_calls+1))\nmetrics.columns = [\"MSLE\", \"dataset\", \"Iteration Number\"]\nfig = px.line(metrics, x=\"Iteration Number\", y=\"MSLE\", color=\"dataset\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the best model, based in the mean_squared_log_error\nIndexMin=train_scores.index(min(train_scores))\n# The model -\nbestModel=models[IndexMin]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We write the file for submission here"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = bestModel.predict(test)\nsubmission.to_csv('random_forest.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now you should save your Notebook (blue button in the upper right), and then when that's complete go to the notebook viewer and make a submission to the competition. :-)\n\n## There's lots of room for improvement. What things can you try to get a better score?"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}