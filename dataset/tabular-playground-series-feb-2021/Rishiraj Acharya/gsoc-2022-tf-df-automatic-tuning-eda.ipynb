{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"A lot of the code here was inspired from Paul Mooney's [notebook on TF-DF](https://www.kaggle.com/code/paultimothymooney/getting-started-with-tensorflow-decision-forests). Make sure to check that out as well.","metadata":{}},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"A learning algorithm trains a machine learning model on a training dataset. The parameters of a learning algorithm–called \"hyper-parameters\"–control how the model is trained and impact its quality. Therefore, finding the best hyper-parameters is an important stage of modeling.\n\nAutomated tuning algorithms work by generating and evaluating a large number of hyper-parameter values. Each of those iterations is called a \"trial\". The evaluation of a trial is expensive as it requires to train a new model each time. At the end of the tuning, the hyper-parameter with the best evaluation is used.\n\nTo demonstrate automated hyper-parameter tuning in TF-DF we'll be working with the Tabular Playground Series Feb 2021 Kaggle Dataset. It is a tabular dataset with 300,000 rows and 26 columns in training (93.66 MiB .CSV training dataset + 58.85 MiB .CSV test set) that is suitable for training algorithms to solve regression problems.\n\nWe'll be predicting a continuous target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat9 are categorical, and the feature columns cont0 - cont13 are continuous.\n\nBy studying this tutorial you will learn how to quickly and automatically tune hyper-parameters of a GradientBoostedTrees model to perform a regression task using tabular data.","metadata":{}},{"cell_type":"markdown","source":"# Installing TensorFlow Decision Forests","metadata":{}},{"cell_type":"code","source":"# Display only the messages with ERROR, CRITICAL log levels\n!pip install tensorflow_decision_forests -U -qq","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"# Import Python packages\nimport os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\nprint(\"TensorFlow Decision Forests v\" + tfdf.__version__)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions","metadata":{}},{"cell_type":"code","source":"# Define helper functions for plotting training evaluation curves\n\ndef plot_tfdf_model_training_curves(model):\n    # This function was adapted from the following tutorial:\n    # https://www.tensorflow.org/decision_forests/tutorials/beginner_colab\n    logs = model.make_inspector().training_logs()\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    # Plot RMSE vs number of trees\n    plt.plot([log.num_trees for log in logs], [log.evaluation.rmse for log in logs])\n    plt.xlabel(\"Number of trees\")\n    plt.ylabel(\"RMSE (out-of-bag)\")\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print list of all data and files attached to this notebook\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the dataset and convert it in a tf.Dataset","metadata":{}},{"cell_type":"markdown","source":"The dataset contains a mix of numerical (e.g. cont0 - cont13) and categorical (e.g. cat0 - cat9) features. TF-DF supports all these feature types natively (differently than NN based models), therefore there is no need for preprocessing in the form of one-hot encoding or normalization. Also by default the task is set to Classification in TF-DF, we'll change that to Regression.","metadata":{}},{"cell_type":"code","source":"# load to pandas dataframe (for data exploration)\ntrain_df = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/test.csv')\n\n# load to tensorflow dataset (for model training)\ntrain_tfds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"target\", task=tfdf.keras.Task.REGRESSION)\ntest_tfds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df, task=tfdf.keras.Task.REGRESSION)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory data analysis","metadata":{}},{"cell_type":"code","source":"# print column names\nprint(train_df.columns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preview first few rows of data\ntrain_df.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print basic summary statistics\ntrain_df.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for missing values\nsns.heatmap(train_df.isnull(), cbar=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Automatic hyper-parameter tuning","metadata":{}},{"cell_type":"markdown","source":"Hyper-paramter tuning is enabled by specifying the tuner constructor argument of the model. The tuner object contains all the configuration of the tuner (search space, optimizer, trial and objective).","metadata":{}},{"cell_type":"code","source":"# This code block was adapted from the following tutorial:\n# https://www.tensorflow.org/decision_forests/tutorials/automatic_tuning_colab\n\n# Configure the tuner.\n\n# Create a Random Search tuner with 10 trials for demo purpose, tune with\n# atleast 50 trials in practice.\ntuner = tfdf.tuner.RandomSearch(num_trials=10)\n\n# Define the search space.\n#\n# Adding more parameters generaly improve the quality of the model, but make\n# the tuning last longer.\n\ntuner.choice(\"min_examples\", [2, 5, 7, 10])\ntuner.choice(\"categorical_algorithm\", [\"CART\", \"RANDOM\"])\n\n# Some hyper-parameters are only valid for specific values of other\n# hyper-parameters. For example, the \"max_depth\" parameter is mostly useful when\n# \"growing_strategy=LOCAL\" while \"max_num_nodes\" is better suited when\n# \"growing_strategy=BEST_FIRST_GLOBAL\".\n\nlocal_search_space = tuner.choice(\"growing_strategy\", [\"LOCAL\"])\nlocal_search_space.choice(\"max_depth\", [3, 4, 5, 6, 8])\n\n# merge=True indicates that the parameter (here \"growing_strategy\") is already\n# defined, and that new values are added to it.\nglobal_search_space = tuner.choice(\"growing_strategy\", [\"BEST_FIRST_GLOBAL\"], merge=True)\nglobal_search_space.choice(\"max_num_nodes\", [16, 32, 64, 128, 256])\n\n# Regression tasks currently does not support hessian optimization in TF-DF\n# https://github.com/tensorflow/decision-forests/issues/116\n# tuner.choice(\"use_hessian_gain\", [True, False])\ntuner.choice(\"shrinkage\", [0.02, 0.05, 0.10, 0.15])\ntuner.choice(\"num_candidate_attributes_ratio\", [0.2, 0.5, 0.9, 1.0])\ntuner.choice(\"split_axis\", [\"AXIS_ALIGNED\"])\n\noblique_space = tuner.choice(\"split_axis\", [\"SPARSE_OBLIQUE\"], merge=True)\noblique_space.choice(\"sparse_oblique_normalization\",\n                     [\"NONE\", \"STANDARD_DEVIATION\", \"MIN_MAX\"])\noblique_space.choice(\"sparse_oblique_weights\", [\"BINARY\", \"CONTINUOUS\"])\noblique_space.choice(\"sparse_oblique_num_projections_exponent\", [1.0, 1.5])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In addition to having a default set of hyper-parameters, TF-DF also provides you with a list of additional hyper-parameter choices to consider.","metadata":{}},{"cell_type":"code","source":"print(tfdf.keras.GradientBoostedTreesModel.predefined_hyperparameters())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, here we'll not be using the default hyper-parameter choices and tune them to get the best results instead. Also by default the task is set to Classification in TF-DF, we'll change that to Regression.","metadata":{}},{"cell_type":"code","source":"# Tune the model. Notice the `tuner=tuner`.\ngb_model = tfdf.keras.GradientBoostedTreesModel(tuner=tuner, task=tfdf.keras.Task.REGRESSION)\ngb_model.fit(x=train_tfds, verbose=2)","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the tuning logs.\ntuning_logs = gb_model.make_inspector().tuning_logs()\ntuning_logs.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best hyper-parameters.\ntuning_logs[tuning_logs.best].iloc[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot the model","metadata":{}},{"cell_type":"markdown","source":"We plot the evoluation of the best score during the training and then the tuning.","metadata":{}},{"cell_type":"code","source":"plot_tfdf_model_training_curves(gb_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.plot(tuning_logs[\"score\"], label=\"current trial\")\nplt.plot(tuning_logs[\"score\"].cummax(), label=\"best trial\")\nplt.xlabel(\"Tuning step\")\nplt.ylabel(\"Tuning score\")\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate the model","metadata":{}},{"cell_type":"markdown","source":"For this dataset, submissions are scored on the root mean squared error. Hence we evaluate the model on that metrics.","metadata":{}},{"cell_type":"code","source":"gb_model.compile(metrics=[tf.keras.metrics.RootMeanSquaredError()])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inspector = gb_model.make_inspector()\ninspector.evaluation()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb_model.evaluate(train_tfds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Model type:\", inspector.model_type())\nprint(\"Objective:\", inspector.objective())\nprint(\"Evaluation:\", inspector.evaluation())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Variable importances generally indicates how much a variable contributes to the model predictions or quality. Variable importance SUM_SCORE is sum of the split scores using a specific feature. The larger, the most important.","metadata":{}},{"cell_type":"code","source":"inspector.variable_importances()[\"SUM_SCORE\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make submission","metadata":{}},{"cell_type":"code","source":"sample_submission_df = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/sample_submission.csv')\nsample_submission_df['target'] = gb_model.predict(test_tfds)\nsample_submission_df.to_csv('/kaggle/working/submission.csv', index=False)\nsample_submission_df.head()","metadata":{},"execution_count":null,"outputs":[]}]}